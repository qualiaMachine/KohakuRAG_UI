{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Local HF Pipeline - End-to-End Test\n",
    "\n",
    "This notebook validates that the **fully local** KohakuRAG pipeline works\n",
    "without any network calls. It tests:\n",
    "\n",
    "1. Local embeddings (`LocalHFEmbeddingModel` via sentence-transformers)\n",
    "2. Local LLM chat (`HuggingFaceLocalChatModel` via transformers)\n",
    "3. Full RAG pipeline: index documents, retrieve, and answer\n",
    "\n",
    "**Prerequisites:**\n",
    "- Kernel: `kohaku-gb10` (or your project venv)\n",
    "- Dependencies installed: `pip install -r local_requirements.txt`\n",
    "- Vendored packages installed: `pip install -e vendor/KohakuVault && pip install -e vendor/KohakuRAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 - Verify imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "\n",
    "print(f\"torch:                {torch.__version__}\")\n",
    "print(f\"transformers:         {transformers.__version__}\")\n",
    "print(f\"sentence-transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"CUDA available:       {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:                  {torch.cuda.get_device_name(0)}\")\n",
    "print()\n",
    "\n",
    "import kohakurag\n",
    "import kohakuvault\n",
    "print(f\"kohakurag:  {kohakurag.__file__}\")\n",
    "print(f\"kohakuvault: {kohakuvault.__file__}\")\n",
    "print(\"\\nAll imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 - Test local embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.embeddings import LocalHFEmbeddingModel\n",
    "\n",
    "# Use a small, fast model for testing\n",
    "embedder = LocalHFEmbeddingModel(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding model loaded: BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding dimension:    {embedder.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_texts = [\n",
    "    \"Solar panels convert sunlight into electricity.\",\n",
    "    \"Photovoltaic cells generate power from solar radiation.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "]\n",
    "\n",
    "vecs = await embedder.embed(test_texts)\n",
    "print(f\"Embedding shape: {vecs.shape}\")\n",
    "print(f\"Dtype:           {vecs.dtype}\")\n",
    "\n",
    "# Cosine similarity (vectors are already normalized)\n",
    "sim_01 = float(np.dot(vecs[0], vecs[1]))\n",
    "sim_02 = float(np.dot(vecs[0], vecs[2]))\n",
    "print(f\"\\nSimilarity (solar vs photovoltaic): {sim_01:.4f}  (should be high)\")\n",
    "print(f\"Similarity (solar vs Paris):         {sim_02:.4f}  (should be low)\")\n",
    "assert sim_01 > sim_02, \"Semantic similarity check failed!\"\n",
    "print(\"\\nEmbedding sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 - Test local LLM chat\n",
    "\n",
    "This loads a local HF model for generation. The default is `Qwen/Qwen2.5-7B-Instruct`.\n",
    "\n",
    "**Note:** If this is too large for your GPU, change `LLM_MODEL_ID` to a smaller model\n",
    "like `Qwen/Qwen2.5-1.5B-Instruct` or `TinyLlama/TinyLlama-1.1B-Chat-v1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM model - adjust if needed for your hardware\n",
    "LLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # change to smaller model if OOM\n",
    "LLM_DTYPE = \"bf16\"  # \"bf16\", \"fp16\", or \"auto\"\n",
    "\n",
    "print(f\"Will load: {LLM_MODEL_ID} ({LLM_DTYPE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.llm import HuggingFaceLocalChatModel\n",
    "\n",
    "chat = HuggingFaceLocalChatModel(\n",
    "    model=LLM_MODEL_ID,\n",
    "    dtype=LLM_DTYPE,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,  # greedy for reproducibility\n",
    ")\n",
    "print(f\"LLM loaded: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await chat.complete(\n",
    "    \"What is 2 + 2? Answer with just the number.\",\n",
    "    system_prompt=\"You are a helpful assistant. Be concise.\",\n",
    ")\n",
    "print(f\"LLM response: {response!r}\")\n",
    "assert \"4\" in response, f\"Expected '4' in response, got: {response}\"\n",
    "print(\"LLM sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": "## Step 4 - Full RAG pipeline with train_QA.csv\n\nThis step loads real WattBot questions from `data/train_QA.csv`, creates\na small document corpus from our sample data, indexes it with proper\nhierarchy (document -> paragraph -> sentence), and runs retrieval + QA."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-docs",
   "metadata": {},
   "outputs": [],
   "source": "import csv\nfrom pathlib import Path\n\n# Load train_QA.csv\nqa_path = Path(\"../data/train_QA.csv\")\nif not qa_path.exists():\n    qa_path = Path(\"data/train_QA.csv\")  # fallback if running from repo root\n\nwith qa_path.open(newline=\"\", encoding=\"utf-8-sig\") as f:\n    reader = csv.DictReader(f)\n    qa_rows = list(reader)\n\nprint(f\"Loaded {len(qa_rows)} questions from {qa_path.name}\")\nprint(f\"Columns: {list(qa_rows[0].keys())}\")\nprint(f\"\\nFirst 5 questions:\")\nfor row in qa_rows[:5]:\n    print(f\"  [{row['id']}] {row['question'][:90]}...\")\n    print(f\"         expected: {row['answer_value']} ({row.get('answer_unit', '')})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-index",
   "metadata": {},
   "outputs": [],
   "source": "from kohakurag.types import NodeKind, StoredNode\nfrom kohakurag.embeddings import average_embeddings\nfrom kohakurag.pipeline import RAGPipeline\nfrom kohakurag.datastore import InMemoryNodeStore\n\n# Sample documents (sustainable AI topics that overlap with train_QA questions)\ndocuments = [\n    {\n        \"id\": \"patterson2021\",\n        \"title\": \"Carbon Emissions and Large Neural Networks\",\n        \"sentences\": [\n            \"Training GPT-3 (175B parameters) was estimated to emit approximately 552 tonnes of CO2.\",\n            \"Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.\",\n            \"Smaller models like Llama-2-7B require roughly 30x less compute.\",\n            \"Techniques such as mixed-precision training and gradient checkpointing can further reduce energy consumption by 20-30%.\",\n        ],\n    },\n    {\n        \"id\": \"wu2021b\",\n        \"title\": \"Sustainable AI and Data Center Efficiency\",\n        \"sentences\": [\n            \"Modern data centers consume approximately 1-2% of global electricity.\",\n            \"Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\",\n            \"Google reported a PUE (Power Usage Effectiveness) of 1.10 across its fleet in 2023.\",\n            \"Liquid cooling systems can reduce energy usage by up to 40% compared to traditional air cooling.\",\n        ],\n    },\n    {\n        \"id\": \"li2025b\",\n        \"title\": \"Water Consumption of AI Systems\",\n        \"sentences\": [\n            \"GPT-3 needs to drink a 500ml bottle of water for roughly 10 to 50 medium-length responses.\",\n            \"The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers was 5.439 million liters.\",\n            \"Microsoft committed to being carbon negative by 2030.\",\n            \"Azure data centers in Sweden run on 100% renewable energy.\",\n        ],\n    },\n    {\n        \"id\": \"strubell2019\",\n        \"title\": \"Energy and Policy Considerations for Deep Learning\",\n        \"sentences\": [\n            \"Authors should report training time and computational resources required for reproducibility.\",\n            \"Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\",\n            \"The financial cost of training a large transformer model can exceed $1 million.\",\n        ],\n    },\n]\n\n# Build hierarchical nodes: document -> paragraph -> sentence\n# The pipeline expects parent nodes to exist when walking the hierarchy\nnodes = []\n\nfor doc in documents:\n    doc_id = doc[\"id\"]\n    \n    # Embed all sentences at once\n    sent_vecs = await embedder.embed(doc[\"sentences\"])\n    \n    # Create sentence nodes\n    sent_node_ids = []\n    for s_idx, (sent, vec) in enumerate(zip(doc[\"sentences\"], sent_vecs)):\n        sent_id = f\"{doc_id}:p0:s{s_idx}\"\n        sent_node_ids.append(sent_id)\n        nodes.append(StoredNode(\n            node_id=sent_id,\n            parent_id=f\"{doc_id}:p0\",\n            kind=NodeKind.SENTENCE,\n            title=doc[\"title\"],\n            text=sent,\n            metadata={\"document_id\": doc_id},\n            embedding=vec,\n            child_ids=[],\n        ))\n    \n    # Create paragraph node (parent of sentences) with averaged embedding\n    para_vec = average_embeddings([v for v in sent_vecs])\n    nodes.append(StoredNode(\n        node_id=f\"{doc_id}:p0\",\n        parent_id=doc_id,\n        kind=NodeKind.PARAGRAPH,\n        title=doc[\"title\"],\n        text=\" \".join(doc[\"sentences\"]),\n        metadata={\"document_id\": doc_id},\n        embedding=para_vec,\n        child_ids=sent_node_ids,\n    ))\n    \n    # Create document node (root) with averaged embedding\n    nodes.append(StoredNode(\n        node_id=doc_id,\n        parent_id=None,\n        kind=NodeKind.DOCUMENT,\n        title=doc[\"title\"],\n        text=doc[\"title\"],\n        metadata={\"document_id\": doc_id},\n        embedding=para_vec,  # same as paragraph for single-paragraph docs\n        child_ids=[f\"{doc_id}:p0\"],\n    ))\n\n# Create in-memory store and index\nstore = InMemoryNodeStore()\nawait store.upsert_nodes(nodes)\nprint(f\"Indexed {len(nodes)} nodes ({len(documents)} docs) into in-memory store\")\nprint(f\"  Hierarchy: document -> paragraph -> sentences\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble pipeline with local components\n",
    "pipeline = RAGPipeline(\n",
    "    store=store,\n",
    "    embedder=embedder,\n",
    "    chat_model=chat,\n",
    "    top_k=3,\n",
    ")\n",
    "print(\"Pipeline assembled (local embedder + local LLM + in-memory store)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-retrieve",
   "metadata": {},
   "outputs": [],
   "source": "# Test retrieval with a real WattBot question\nquestion = qa_rows[0][\"question\"]  # First question from train_QA.csv\n\nresult = await pipeline.retrieve(question, top_k=3)\nprint(f\"Question: {question}\")\nprint(f\"Retrieved {len(result.matches)} matches:\\n\")\nfor i, match in enumerate(result.matches):\n    print(f\"  [{i+1}] score={match.score:.4f}  node={match.node.node_id}\")\n    print(f\"      {match.node.text[:120]}...\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-answer",
   "metadata": {},
   "outputs": [],
   "source": "# Test full QA (retrieve + generate)\nanswer = await pipeline.answer(question)\n\nprint(f\"Question: {answer['question']}\\n\")\nprint(f\"Expected: {qa_rows[0]['answer_value']}\")\nprint(f\"\\nResponse:\\n{answer['response']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 - Structured QA (JSON output)\n",
    "\n",
    "This tests the `run_qa` method with the same prompt templates used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nsystem_prompt = (\n    \"You must answer strictly based on the provided context snippets. \"\n    \"Do NOT use external knowledge. If the context does not support an answer, \"\n    \"output 'is_blank' for answer_value. Respond in JSON with keys: \"\n    \"explanation, answer, answer_value, ref_id.\"\n)\n\nuser_template = \"\"\"Question: {question}\n\nContext:\n{context}\n\nAdditional info: {additional_info_json}\n\nReturn STRICT JSON:\n- explanation: 1-2 sentences\n- answer: short answer\n- answer_value: numeric/categorical value or \"is_blank\"\n- ref_id: list of document ids used\n\nJSON Answer:\"\"\"\n\n# Use a question from train_QA that should match our sample docs\n# q009: \"What were the net CO2e emissions from training the GShard-600B model?\"\ngshard_row = next(r for r in qa_rows if \"GShard\" in r[\"question\"])\n\nstructured_result = await pipeline.run_qa(\n    question=gshard_row[\"question\"],\n    system_prompt=system_prompt,\n    user_template=user_template,\n    additional_info={\"answer_unit\": gshard_row.get(\"answer_unit\", \"\")},\n    top_k=3,\n)\n\nprint(f\"Question:     {gshard_row['question']}\")\nprint(f\"Expected:     {gshard_row['answer_value']} ({gshard_row.get('answer_unit', '')})\")\nprint(f\"Answer:       {structured_result.answer.answer}\")\nprint(f\"Answer value: {structured_result.answer.answer_value}\")\nprint(f\"Ref IDs:      {structured_result.answer.ref_id}\")\nprint(f\"Explanation:  {structured_result.answer.explanation}\")\nprint(f\"\\nRaw LLM output:\\n{structured_result.raw_response[:500]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6 - Offline validation\n",
    "\n",
    "Confirm no network calls were made by unsetting API keys and re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clear any API keys to prove we're fully local\n",
    "for key in [\"OPENROUTER_API_KEY\", \"OPENAI_API_KEY\", \"JINA_API_KEY\"]:\n",
    "    os.environ.pop(key, None)\n",
    "\n",
    "# Re-run a query - should work without any API keys\n",
    "offline_answer = await pipeline.answer(\n",
    "    \"What percentage of global electricity do data centers use?\"\n",
    ")\n",
    "print(f\"Offline response:\\n{offline_answer['response']}\")\n",
    "print(\"\\nOFFLINE VALIDATION PASSED - no API keys needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIf all cells above ran successfully, your local HF pipeline is working:\n\n| Component | Provider | Model |\n|-----------|----------|-------|\n| Embeddings | `LocalHFEmbeddingModel` | `BAAI/bge-base-en-v1.5` |\n| LLM | `HuggingFaceLocalChatModel` | Configured above |\n| Vector store | `InMemoryNodeStore` | (in-memory, no DB needed) |\n\n**What was tested:**\n- Steps 1-3: Individual component verification (imports, embeddings, LLM)\n- Step 4: Full RAG pipeline with synthetic documents\n- Step 5: Structured JSON QA (production format)\n- Step 6: Offline validation (no API keys)\n- Step 7: Real WattBot questions from `data/train_QA.csv`\n\nTo use with the full production pipeline (KVaultNodeStore + pre-indexed docs),\nset `llm_provider = \"hf_local\"` and `embedding_model = \"hf_local\"` in your config."
  },
  {
   "cell_type": "markdown",
   "id": "wkpaqv0z0dp",
   "source": "## Step 7 - Batch test with multiple WattBot questions\n\nRun several questions from `train_QA.csv` through the pipeline, including\nones that should match our sample docs and ones that won't (testing \"is_blank\").",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "olphmjbycc",
   "source": "# Pick questions that test different scenarios\nsample_questions = [\n    # Should match: hyperscale data centers (wu2021b)\n    next(r for r in qa_rows if \"Hyperscale\" in r[\"question\"]),\n    # Should match: water consumption (li2025b)\n    next(r for r in qa_rows if \"water consumption\" in r[\"question\"].lower() and \"training GPT-3\" in r[\"question\"]),\n    # Should match: tracking runtime (strubell2019)\n    next(r for r in qa_rows if \"runtime\" in r[\"question\"].lower() and \"training job\" in r[\"question\"].lower()),\n    # Should NOT match: elephant (tests is_blank)\n    next(r for r in qa_rows if \"elephant\" in r[\"question\"].lower()),\n]\n\nprint(f\"Running {len(sample_questions)} WattBot questions through local pipeline...\\n\")\nprint(\"=\" * 70)\n\nfor row in sample_questions:\n    qid = row[\"id\"]\n    question = row[\"question\"]\n    expected = row[\"answer_value\"]\n    unit = row.get(\"answer_unit\", \"\")\n\n    result = await pipeline.run_qa(\n        question=question,\n        system_prompt=system_prompt,\n        user_template=user_template,\n        additional_info={\"answer_unit\": unit},\n        top_k=3,\n    )\n\n    print(f\"\\n[{qid}] {question[:85]}...\")\n    print(f\"  Expected:  {expected} ({unit})\")\n    print(f\"  Got:       {result.answer.answer_value}\")\n    print(f\"  Answer:    {result.answer.answer}\")\n    print(f\"  Ref IDs:   {result.answer.ref_id}\")\n    print(\"-\" * 70)\n\nprint(\"\\nBatch WattBot test completed (pipeline ran without errors)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohaku-gb10",
   "language": "python",
   "name": "kohaku-gb10"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}