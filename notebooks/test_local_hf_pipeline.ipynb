{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Local HF Pipeline - End-to-End Test\n",
    "\n",
    "This notebook validates that the **fully local** KohakuRAG pipeline works\n",
    "without any network calls. It tests:\n",
    "\n",
    "1. Local embeddings (`LocalHFEmbeddingModel` via sentence-transformers)\n",
    "2. Local LLM chat (`HuggingFaceLocalChatModel` via transformers)\n",
    "3. Full RAG pipeline: index documents, retrieve, and answer\n",
    "\n",
    "**Prerequisites:**\n",
    "- Kernel: `kohaku-gb10` (or your project venv)\n",
    "- Dependencies installed: `pip install -r local_requirements.txt`\n",
    "- Vendored packages installed: `pip install -e vendor/KohakuVault && pip install -e vendor/KohakuRAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 - Verify imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "\n",
    "print(f\"torch:                {torch.__version__}\")\n",
    "print(f\"transformers:         {transformers.__version__}\")\n",
    "print(f\"sentence-transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"CUDA available:       {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:                  {torch.cuda.get_device_name(0)}\")\n",
    "print()\n",
    "\n",
    "import kohakurag\n",
    "import kohakuvault\n",
    "print(f\"kohakurag:  {kohakurag.__file__}\")\n",
    "print(f\"kohakuvault: {kohakuvault.__file__}\")\n",
    "print(\"\\nAll imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 - Test local embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.embeddings import LocalHFEmbeddingModel\n",
    "\n",
    "# Use a small, fast model for testing\n",
    "embedder = LocalHFEmbeddingModel(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding model loaded: BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding dimension:    {embedder.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_texts = [\n",
    "    \"Solar panels convert sunlight into electricity.\",\n",
    "    \"Photovoltaic cells generate power from solar radiation.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "]\n",
    "\n",
    "vecs = await embedder.embed(test_texts)\n",
    "print(f\"Embedding shape: {vecs.shape}\")\n",
    "print(f\"Dtype:           {vecs.dtype}\")\n",
    "\n",
    "# Cosine similarity (vectors are already normalized)\n",
    "sim_01 = float(np.dot(vecs[0], vecs[1]))\n",
    "sim_02 = float(np.dot(vecs[0], vecs[2]))\n",
    "print(f\"\\nSimilarity (solar vs photovoltaic): {sim_01:.4f}  (should be high)\")\n",
    "print(f\"Similarity (solar vs Paris):         {sim_02:.4f}  (should be low)\")\n",
    "assert sim_01 > sim_02, \"Semantic similarity check failed!\"\n",
    "print(\"\\nEmbedding sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 - Test local LLM chat\n",
    "\n",
    "This loads a local HF model for generation. The default is `Qwen/Qwen2.5-7B-Instruct`.\n",
    "\n",
    "**Note:** If this is too large for your GPU, change `LLM_MODEL_ID` to a smaller model\n",
    "like `Qwen/Qwen2.5-1.5B-Instruct` or `TinyLlama/TinyLlama-1.1B-Chat-v1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM model - adjust if needed for your hardware\n",
    "LLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # change to smaller model if OOM\n",
    "LLM_DTYPE = \"bf16\"  # \"bf16\", \"fp16\", or \"auto\"\n",
    "\n",
    "print(f\"Will load: {LLM_MODEL_ID} ({LLM_DTYPE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.llm import HuggingFaceLocalChatModel\n",
    "\n",
    "chat = HuggingFaceLocalChatModel(\n",
    "    model=LLM_MODEL_ID,\n",
    "    dtype=LLM_DTYPE,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,  # greedy for reproducibility\n",
    ")\n",
    "print(f\"LLM loaded: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await chat.complete(\n",
    "    \"What is 2 + 2? Answer with just the number.\",\n",
    "    system_prompt=\"You are a helpful assistant. Be concise.\",\n",
    ")\n",
    "print(f\"LLM response: {response!r}\")\n",
    "assert \"4\" in response, f\"Expected '4' in response, got: {response}\"\n",
    "print(\"LLM sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": "## Step 4 - Full RAG pipeline with train_QA.csv\n\nThis step loads real WattBot questions from `data/train_QA.csv`, creates\na small document corpus from our sample data, indexes it with proper\nhierarchy (document -> paragraph -> sentence), and runs retrieval + QA."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-docs",
   "metadata": {},
   "outputs": [],
   "source": "import csv\nfrom pathlib import Path\n\n# Load train_QA.csv\nqa_path = Path(\"../data/train_QA.csv\")\nif not qa_path.exists():\n    qa_path = Path(\"data/train_QA.csv\")  # fallback if running from repo root\n\nwith qa_path.open(newline=\"\", encoding=\"utf-8-sig\") as f:\n    reader = csv.DictReader(f)\n    qa_rows = list(reader)\n\nprint(f\"Loaded {len(qa_rows)} questions from {qa_path.name}\")\nprint(f\"Columns: {list(qa_rows[0].keys())}\")\nprint(f\"\\nFirst 5 questions:\")\nfor row in qa_rows[:5]:\n    print(f\"  [{row['id']}] {row['question'][:90]}...\")\n    print(f\"         expected: {row['answer_value']} ({row.get('answer_unit', '')})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-index",
   "metadata": {},
   "outputs": [],
   "source": "from kohakurag.types import NodeKind, StoredNode\nfrom kohakurag.embeddings import average_embeddings\nfrom kohakurag.pipeline import RAGPipeline\nfrom kohakurag.datastore import InMemoryNodeStore\n\n# Sample documents (sustainable AI topics that overlap with train_QA questions)\ndocuments = [\n    {\n        \"id\": \"patterson2021\",\n        \"title\": \"Carbon Emissions and Large Neural Networks\",\n        \"sentences\": [\n            \"Training GPT-3 (175B parameters) was estimated to emit approximately 552 tonnes of CO2.\",\n            \"Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.\",\n            \"Smaller models like Llama-2-7B require roughly 30x less compute.\",\n            \"Techniques such as mixed-precision training and gradient checkpointing can further reduce energy consumption by 20-30%.\",\n        ],\n    },\n    {\n        \"id\": \"wu2021b\",\n        \"title\": \"Sustainable AI and Data Center Efficiency\",\n        \"sentences\": [\n            \"Modern data centers consume approximately 1-2% of global electricity.\",\n            \"Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\",\n            \"Google reported a PUE (Power Usage Effectiveness) of 1.10 across its fleet in 2023.\",\n            \"Liquid cooling systems can reduce energy usage by up to 40% compared to traditional air cooling.\",\n        ],\n    },\n    {\n        \"id\": \"li2025b\",\n        \"title\": \"Water Consumption of AI Systems\",\n        \"sentences\": [\n            \"GPT-3 needs to drink a 500ml bottle of water for roughly 10 to 50 medium-length responses.\",\n            \"The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers was 5.439 million liters.\",\n            \"Microsoft committed to being carbon negative by 2030.\",\n            \"Azure data centers in Sweden run on 100% renewable energy.\",\n        ],\n    },\n    {\n        \"id\": \"strubell2019\",\n        \"title\": \"Energy and Policy Considerations for Deep Learning\",\n        \"sentences\": [\n            \"Authors should report training time and computational resources required for reproducibility.\",\n            \"Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\",\n            \"The financial cost of training a large transformer model can exceed $1 million.\",\n        ],\n    },\n]\n\n# Build hierarchical nodes: document -> paragraph -> sentence\n# The pipeline expects parent nodes to exist when walking the hierarchy\nnodes = []\n\nfor doc in documents:\n    doc_id = doc[\"id\"]\n    \n    # Embed all sentences at once\n    sent_vecs = await embedder.embed(doc[\"sentences\"])\n    \n    # Create sentence nodes\n    sent_node_ids = []\n    for s_idx, (sent, vec) in enumerate(zip(doc[\"sentences\"], sent_vecs)):\n        sent_id = f\"{doc_id}:p0:s{s_idx}\"\n        sent_node_ids.append(sent_id)\n        nodes.append(StoredNode(\n            node_id=sent_id,\n            parent_id=f\"{doc_id}:p0\",\n            kind=NodeKind.SENTENCE,\n            title=doc[\"title\"],\n            text=sent,\n            metadata={\"document_id\": doc_id},\n            embedding=vec,\n            child_ids=[],\n        ))\n    \n    # Create paragraph node (parent of sentences) with averaged embedding\n    para_vec = average_embeddings([v for v in sent_vecs])\n    nodes.append(StoredNode(\n        node_id=f\"{doc_id}:p0\",\n        parent_id=doc_id,\n        kind=NodeKind.PARAGRAPH,\n        title=doc[\"title\"],\n        text=\" \".join(doc[\"sentences\"]),\n        metadata={\"document_id\": doc_id},\n        embedding=para_vec,\n        child_ids=sent_node_ids,\n    ))\n    \n    # Create document node (root) with averaged embedding\n    nodes.append(StoredNode(\n        node_id=doc_id,\n        parent_id=None,\n        kind=NodeKind.DOCUMENT,\n        title=doc[\"title\"],\n        text=doc[\"title\"],\n        metadata={\"document_id\": doc_id},\n        embedding=para_vec,  # same as paragraph for single-paragraph docs\n        child_ids=[f\"{doc_id}:p0\"],\n    ))\n\n# Create in-memory store and index\nstore = InMemoryNodeStore()\nawait store.upsert_nodes(nodes)\nprint(f\"Indexed {len(nodes)} nodes ({len(documents)} docs) into in-memory store\")\nprint(f\"  Hierarchy: document -> paragraph -> sentences\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble pipeline with local components\n",
    "pipeline = RAGPipeline(\n",
    "    store=store,\n",
    "    embedder=embedder,\n",
    "    chat_model=chat,\n",
    "    top_k=3,\n",
    ")\n",
    "print(\"Pipeline assembled (local embedder + local LLM + in-memory store)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-retrieve",
   "metadata": {},
   "outputs": [],
   "source": "# Test retrieval with a real WattBot question\nquestion = qa_rows[0][\"question\"]  # First question from train_QA.csv\n\nresult = await pipeline.retrieve(question, top_k=3)\nprint(f\"Question: {question}\")\nprint(f\"Retrieved {len(result.matches)} matches:\\n\")\nfor i, match in enumerate(result.matches):\n    print(f\"  [{i+1}] score={match.score:.4f}  node={match.node.node_id}\")\n    print(f\"      {match.node.text[:120]}...\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-answer",
   "metadata": {},
   "outputs": [],
   "source": "# Test full QA (retrieve + generate)\nanswer = await pipeline.answer(question)\n\nprint(f\"Question: {answer['question']}\\n\")\nprint(f\"Expected: {qa_rows[0]['answer_value']}\")\nprint(f\"\\nResponse:\\n{answer['response']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 - Structured QA (JSON output)\n",
    "\n",
    "This tests the `run_qa` method with the same prompt templates used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nsystem_prompt = (\n    \"You must answer strictly based on the provided context snippets. \"\n    \"Do NOT use external knowledge. If the context does not support an answer, \"\n    \"output 'is_blank' for answer_value. Respond in JSON with keys: \"\n    \"explanation, answer, answer_value, ref_id.\"\n)\n\nuser_template = \"\"\"Question: {question}\n\nContext:\n{context}\n\nAdditional info: {additional_info_json}\n\nReturn STRICT JSON:\n- explanation: 1-2 sentences\n- answer: short answer\n- answer_value: numeric/categorical value or \"is_blank\"\n- ref_id: list of document ids used\n\nJSON Answer:\"\"\"\n\n# Use a question from train_QA that should match our sample docs\n# q009: \"What were the net CO2e emissions from training the GShard-600B model?\"\ngshard_row = next(r for r in qa_rows if \"GShard\" in r[\"question\"])\n\nstructured_result = await pipeline.run_qa(\n    question=gshard_row[\"question\"],\n    system_prompt=system_prompt,\n    user_template=user_template,\n    additional_info={\"answer_unit\": gshard_row.get(\"answer_unit\", \"\")},\n    top_k=3,\n)\n\nprint(f\"Question:     {gshard_row['question']}\")\nprint(f\"Expected:     {gshard_row['answer_value']} ({gshard_row.get('answer_unit', '')})\")\nprint(f\"Answer:       {structured_result.answer.answer}\")\nprint(f\"Answer value: {structured_result.answer.answer_value}\")\nprint(f\"Ref IDs:      {structured_result.answer.ref_id}\")\nprint(f\"Explanation:  {structured_result.answer.explanation}\")\nprint(f\"\\nRaw LLM output:\\n{structured_result.raw_response[:500]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6 - Offline validation\n",
    "\n",
    "Confirm no network calls were made by unsetting API keys and re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clear any API keys to prove we're fully local\n",
    "for key in [\"OPENROUTER_API_KEY\", \"OPENAI_API_KEY\", \"JINA_API_KEY\"]:\n",
    "    os.environ.pop(key, None)\n",
    "\n",
    "# Re-run a query - should work without any API keys\n",
    "offline_answer = await pipeline.answer(\n",
    "    \"What percentage of global electricity do data centers use?\"\n",
    ")\n",
    "print(f\"Offline response:\\n{offline_answer['response']}\")\n",
    "print(\"\\nOFFLINE VALIDATION PASSED - no API keys needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIf all cells above ran successfully, your local HF pipeline is working:\n\n| Component | Provider | Model |\n|-----------|----------|-------|\n| Embeddings | `LocalHFEmbeddingModel` | `BAAI/bge-base-en-v1.5` |\n| LLM | `HuggingFaceLocalChatModel` | Configured above |\n| Vector store | `InMemoryNodeStore` | (in-memory, no DB needed) |\n\n**What was tested:**\n- Steps 1-3: Individual component verification (imports, embeddings, LLM)\n- Step 4: Full RAG pipeline with train_QA.csv questions\n- Step 5: Structured JSON QA (production format)\n- Step 6: Offline validation (no API keys)\n- Step 7: Batch test with multiple WattBot questions\n- Step 8a: Serial ensemble voting (single GPU)\n- Step 8b: Parallel ensemble voting (multi-GPU with `CUDA_VISIBLE_DEVICES`)\n- Step 8c: Batch ensemble across multiple questions\n\n**Ensemble modes:**\n- `ref_mode=\"union\"` — vote on answer, union ref_ids from agreeing runs\n- `ref_mode=\"intersection\"` — vote on answer, intersect ref_ids\n- `ref_mode=\"independent\"` — vote answer and ref_id separately\n- `ignore_blank=True` — filter out `is_blank` before voting (default)\n\nTo use with the full production pipeline (KVaultNodeStore + pre-indexed docs),\nset `llm_provider = \"hf_local\"` and `embedding_model = \"hf_local\"` in your config."
  },
  {
   "cell_type": "markdown",
   "id": "wkpaqv0z0dp",
   "source": "## Step 7 - Batch test with multiple WattBot questions\n\nRun several questions from `train_QA.csv` through the pipeline, including\nones that should match our sample docs and ones that won't (testing \"is_blank\").",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "olphmjbycc",
   "source": "# Pick questions that test different scenarios\nsample_questions = [\n    # Should match: hyperscale data centers (wu2021b)\n    next(r for r in qa_rows if \"Hyperscale\" in r[\"question\"]),\n    # Should match: water consumption (li2025b)\n    next(r for r in qa_rows if \"water consumption\" in r[\"question\"].lower() and \"training GPT-3\" in r[\"question\"]),\n    # Should match: tracking runtime (strubell2019)\n    next(r for r in qa_rows if \"runtime\" in r[\"question\"].lower() and \"training job\" in r[\"question\"].lower()),\n    # Should NOT match: elephant (tests is_blank)\n    next(r for r in qa_rows if \"elephant\" in r[\"question\"].lower()),\n]\n\nprint(f\"Running {len(sample_questions)} WattBot questions through local pipeline...\\n\")\nprint(\"=\" * 70)\n\nfor row in sample_questions:\n    qid = row[\"id\"]\n    question = row[\"question\"]\n    expected = row[\"answer_value\"]\n    unit = row.get(\"answer_unit\", \"\")\n\n    result = await pipeline.run_qa(\n        question=question,\n        system_prompt=system_prompt,\n        user_template=user_template,\n        additional_info={\"answer_unit\": unit},\n        top_k=3,\n    )\n\n    print(f\"\\n[{qid}] {question[:85]}...\")\n    print(f\"  Expected:  {expected} ({unit})\")\n    print(f\"  Got:       {result.answer.answer_value}\")\n    print(f\"  Answer:    {result.answer.answer}\")\n    print(f\"  Ref IDs:   {result.answer.ref_id}\")\n    print(\"-\" * 70)\n\nprint(\"\\nBatch WattBot test completed (pipeline ran without errors)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "avampgwqku8",
   "source": "## Step 8 - Ensemble voting with local HF models\n\nKohakuRAG's competition-winning strategy uses **ensemble voting**: run the\nsame question through the pipeline N times (with temperature > 0 so outputs\nvary), then pick the answer that appears most often via majority vote.\n\nThis step demonstrates two modes:\n- **Serial** (single GPU) — runs N passes sequentially, safe for any setup\n- **Parallel** (multi-GPU) — spawns workers across GPUs via `CUDA_VISIBLE_DEVICES`\n\nThe voting logic uses the same `wattbot_aggregate.py` strategies as production:\n`union`, `intersection`, `independent`, `ref_priority`, `answer_priority`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "agt6vg9c457",
   "source": "import asyncio\nimport json\nimport os\nimport time\nfrom collections import Counter\nfrom concurrent.futures import ProcessPoolExecutor\nfrom dataclasses import dataclass\n\n\n# ---------------------------------------------------------------------------\n# Voting / aggregation helpers (same logic as wattbot_aggregate.py)\n# ---------------------------------------------------------------------------\n\ndef majority_vote(values: list[str], ignore_blank: bool = True) -> str:\n    \"\"\"Pick the most-common value; first occurrence breaks ties.\n    \n    Args:\n        values: candidate answers from N runs\n        ignore_blank: drop 'is_blank' entries before voting (if non-blanks exist)\n    \"\"\"\n    if not values:\n        return \"is_blank\"\n    if ignore_blank:\n        non_blank = [v for v in values if v != \"is_blank\"]\n        if non_blank:\n            values = non_blank\n    counter = Counter(values)\n    max_count = counter.most_common(1)[0][1]\n    # First-occurrence tiebreak\n    for v in values:\n        if counter[v] == max_count:\n            return v\n    return values[0]\n\n\ndef aggregate_ensemble(\n    all_run_answers: list[dict],\n    ref_mode: str = \"union\",\n    ignore_blank: bool = True,\n) -> dict:\n    \"\"\"Aggregate N answers for a single question.\n    \n    Args:\n        all_run_answers: list of dicts, each with keys\n            answer_value, answer, ref_id, explanation\n        ref_mode: \"union\" | \"intersection\" | \"independent\"\n        ignore_blank: filter is_blank before voting\n        \n    Returns:\n        dict with voted answer_value, answer, ref_ids, per-run details\n    \"\"\"\n    answer_values = [r.get(\"answer_value\", \"is_blank\") for r in all_run_answers]\n    best_value = majority_vote(answer_values, ignore_blank)\n\n    # Collect ref_ids from runs that agree with the winning answer\n    matching = [r for r in all_run_answers if r.get(\"answer_value\") == best_value]\n    if not matching:\n        matching = all_run_answers  # fallback\n\n    if ref_mode == \"union\":\n        all_refs = set()\n        for r in matching:\n            refs = r.get(\"ref_id\", [])\n            if isinstance(refs, list):\n                all_refs.update(refs)\n            elif isinstance(refs, str) and refs != \"is_blank\":\n                all_refs.add(refs)\n        voted_refs = sorted(all_refs) if all_refs else [\"is_blank\"]\n    elif ref_mode == \"intersection\":\n        ref_sets = []\n        for r in matching:\n            refs = r.get(\"ref_id\", [])\n            if isinstance(refs, list):\n                ref_sets.append(set(refs))\n        if ref_sets:\n            voted_refs = sorted(ref_sets[0].intersection(*ref_sets[1:]))\n        else:\n            voted_refs = [\"is_blank\"]\n        if not voted_refs:\n            voted_refs = [\"is_blank\"]\n    else:  # independent\n        ref_strs = [str(r.get(\"ref_id\", \"is_blank\")) for r in all_run_answers]\n        voted_refs = [majority_vote(ref_strs, ignore_blank)]\n\n    # Use explanation from the first matching run\n    best_answer = matching[0].get(\"answer\", best_value)\n    best_explanation = matching[0].get(\"explanation\", \"\")\n\n    return {\n        \"answer_value\": best_value,\n        \"answer\": best_answer,\n        \"ref_id\": voted_refs,\n        \"explanation\": best_explanation,\n        \"vote_counts\": dict(Counter(answer_values)),\n        \"n_runs\": len(all_run_answers),\n        \"agreement\": Counter(answer_values).most_common(1)[0][1] / len(all_run_answers),\n    }\n\n\nprint(\"Ensemble voting helpers loaded\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bhxzwpvcaz4",
   "source": "### 8a) Serial ensemble (single GPU)\n\nRuns the pipeline N times with `temperature > 0` so each pass produces\nslightly different outputs. Then aggregates with majority voting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8v1onjzron",
   "source": "# ---------------------------------------------------------------------------\n# Serial ensemble: N runs on the same GPU, one after another\n# ---------------------------------------------------------------------------\n\nENSEMBLE_SIZE = 5         # Number of runs per question\nENSEMBLE_TEMPERATURE = 0.6  # Needs > 0 for diversity across runs\n\n# Build a pipeline with temperature > 0 for the ensemble\n# (reuses the same embedder + store from Step 4)\nchat_ensemble = HuggingFaceLocalChatModel(\n    model=LLM_MODEL_ID,\n    dtype=LLM_DTYPE,\n    max_new_tokens=256,\n    temperature=ENSEMBLE_TEMPERATURE,\n)\n\npipeline_ensemble = RAGPipeline(\n    store=store,\n    embedder=embedder,\n    chat_model=chat_ensemble,\n    top_k=3,\n)\n\n# Pick a question from train_QA\ntest_row = next(r for r in qa_rows if \"GShard\" in r[\"question\"])\ntest_question = test_row[\"question\"]\ntest_expected = test_row[\"answer_value\"]\ntest_unit = test_row.get(\"answer_unit\", \"\")\n\nprint(f\"Question:  {test_question}\")\nprint(f\"Expected:  {test_expected} ({test_unit})\")\nprint(f\"Ensemble:  {ENSEMBLE_SIZE} runs @ temperature={ENSEMBLE_TEMPERATURE}\")\nprint(f\"\\nRunning serial ensemble...\")\nprint(\"-\" * 60)\n\nrun_answers = []\nt0 = time.time()\n\nfor i in range(ENSEMBLE_SIZE):\n    result = await pipeline_ensemble.run_qa(\n        question=test_question,\n        system_prompt=system_prompt,\n        user_template=user_template,\n        additional_info={\"answer_unit\": test_unit},\n        top_k=3,\n    )\n    answer_dict = {\n        \"answer_value\": result.answer.answer_value,\n        \"answer\": result.answer.answer,\n        \"ref_id\": result.answer.ref_id,\n        \"explanation\": result.answer.explanation,\n    }\n    run_answers.append(answer_dict)\n    print(f\"  Run {i+1}/{ENSEMBLE_SIZE}: answer_value={result.answer.answer_value!r}\")\n\nelapsed = time.time() - t0\nprint(f\"\\n{ENSEMBLE_SIZE} runs completed in {elapsed:.1f}s ({elapsed/ENSEMBLE_SIZE:.1f}s/run)\")\n\n# Aggregate with majority voting\nvoted = aggregate_ensemble(run_answers, ref_mode=\"union\", ignore_blank=True)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"ENSEMBLE RESULT (majority vote, ref_mode=union)\")\nprint(f\"{'=' * 60}\")\nprint(f\"  Voted answer:  {voted['answer_value']}\")\nprint(f\"  Expected:      {test_expected}\")\nprint(f\"  Agreement:     {voted['agreement']:.0%} ({voted['n_runs']} runs)\")\nprint(f\"  Vote counts:   {voted['vote_counts']}\")\nprint(f\"  Ref IDs:       {voted['ref_id']}\")\nprint(f\"  Explanation:   {voted['explanation'][:200]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "btczljucwa9",
   "source": "### 8b) Parallel ensemble (multi-GPU)\n\nWhen you have multiple GPUs (e.g., PowerEdge with 2x A100), each worker\ngets its own GPU via `CUDA_VISIBLE_DEVICES`. Each process loads its own\nmodel copy and runs independently — no VRAM contention.\n\n**How it works:**\n1. Spawns N worker **processes** (one per GPU, round-robin)\n2. Each process sets `CUDA_VISIBLE_DEVICES` to its assigned GPU\n3. Loads the model fresh (no shared memory between processes)\n4. Runs the pipeline and returns the answer dict\n5. Main process collects all results and runs majority vote\n\n> **Tip:** On a 2-GPU server, set `N_GPUS=2` and `ENSEMBLE_SIZE=6`\n> to run 3 passes per GPU in parallel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7yzyugidsfs",
   "source": "# ---------------------------------------------------------------------------\n# Parallel ensemble worker function (runs in a subprocess)\n# ---------------------------------------------------------------------------\n\ndef _ensemble_worker(args: dict) -> dict:\n    \"\"\"Run a single ensemble pass in a subprocess with a specific GPU.\n    \n    This function is called by ProcessPoolExecutor. Each invocation:\n    1. Pins to a specific GPU via CUDA_VISIBLE_DEVICES\n    2. Loads embedder + LLM from scratch (subprocess has its own memory)\n    3. Rebuilds the in-memory store + pipeline\n    4. Runs run_qa and returns the answer dict\n    \n    Args:\n        args: dict with keys:\n            gpu_id, run_id, question, system_prompt, user_template,\n            additional_info, llm_model_id, llm_dtype, temperature,\n            embed_model_name, documents, top_k\n    \n    Returns:\n        dict with answer_value, answer, ref_id, explanation, run_id, gpu_id\n    \"\"\"\n    import os\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args[\"gpu_id\"])\n    \n    import asyncio\n    import numpy as np\n    \n    from kohakurag.embeddings import LocalHFEmbeddingModel, average_embeddings\n    from kohakurag.llm import HuggingFaceLocalChatModel\n    from kohakurag.pipeline import RAGPipeline\n    from kohakurag.datastore import InMemoryNodeStore\n    from kohakurag.types import NodeKind, StoredNode\n    \n    async def _run():\n        # Load models on this GPU\n        embedder = LocalHFEmbeddingModel(model_name=args[\"embed_model_name\"])\n        chat = HuggingFaceLocalChatModel(\n            model=args[\"llm_model_id\"],\n            dtype=args[\"llm_dtype\"],\n            max_new_tokens=256,\n            temperature=args[\"temperature\"],\n        )\n        \n        # Rebuild the store (each subprocess needs its own)\n        nodes = []\n        for doc in args[\"documents\"]:\n            doc_id = doc[\"id\"]\n            sent_vecs = await embedder.embed(doc[\"sentences\"])\n            \n            sent_node_ids = []\n            for s_idx, (sent, vec) in enumerate(zip(doc[\"sentences\"], sent_vecs)):\n                sent_id = f\"{doc_id}:p0:s{s_idx}\"\n                sent_node_ids.append(sent_id)\n                nodes.append(StoredNode(\n                    node_id=sent_id,\n                    parent_id=f\"{doc_id}:p0\",\n                    kind=NodeKind.SENTENCE,\n                    title=doc[\"title\"],\n                    text=sent,\n                    metadata={\"document_id\": doc_id},\n                    embedding=vec,\n                    child_ids=[],\n                ))\n            \n            para_vec = average_embeddings([v for v in sent_vecs])\n            nodes.append(StoredNode(\n                node_id=f\"{doc_id}:p0\",\n                parent_id=doc_id,\n                kind=NodeKind.PARAGRAPH,\n                title=doc[\"title\"],\n                text=\" \".join(doc[\"sentences\"]),\n                metadata={\"document_id\": doc_id},\n                embedding=para_vec,\n                child_ids=sent_node_ids,\n            ))\n            nodes.append(StoredNode(\n                node_id=doc_id,\n                parent_id=None,\n                kind=NodeKind.DOCUMENT,\n                title=doc[\"title\"],\n                text=doc[\"title\"],\n                metadata={\"document_id\": doc_id},\n                embedding=para_vec,\n                child_ids=[f\"{doc_id}:p0\"],\n            ))\n        \n        store = InMemoryNodeStore()\n        await store.upsert_nodes(nodes)\n        \n        pipeline = RAGPipeline(\n            store=store,\n            embedder=embedder,\n            chat_model=chat,\n            top_k=args[\"top_k\"],\n        )\n        \n        result = await pipeline.run_qa(\n            question=args[\"question\"],\n            system_prompt=args[\"system_prompt\"],\n            user_template=args[\"user_template\"],\n            additional_info=args[\"additional_info\"],\n            top_k=args[\"top_k\"],\n        )\n        \n        return {\n            \"answer_value\": result.answer.answer_value,\n            \"answer\": result.answer.answer,\n            \"ref_id\": result.answer.ref_id,\n            \"explanation\": result.answer.explanation,\n            \"run_id\": args[\"run_id\"],\n            \"gpu_id\": args[\"gpu_id\"],\n        }\n    \n    return asyncio.run(_run())\n\n\nprint(\"Parallel ensemble worker defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "594fvzfpv3e",
   "source": "# ---------------------------------------------------------------------------\n# Run the parallel ensemble\n# ---------------------------------------------------------------------------\n\n# Configuration — adjust for your hardware\nN_GPUS = torch.cuda.device_count()  # auto-detect\nPARALLEL_ENSEMBLE_SIZE = 4           # total runs (distributed across GPUs)\n\nprint(f\"Detected {N_GPUS} GPU(s)\")\n\nif N_GPUS < 2:\n    print(\n        \"\\nSkipping parallel ensemble (needs 2+ GPUs).\\n\"\n        \"The serial ensemble in Step 8a works on single-GPU setups.\\n\"\n        \"On your PowerEdge with 2 GPUs, this will run automatically.\"\n    )\nelse:\n    print(f\"Running {PARALLEL_ENSEMBLE_SIZE} passes across {N_GPUS} GPUs...\")\n    \n    # Build worker args — round-robin GPU assignment\n    worker_args = []\n    for run_id in range(PARALLEL_ENSEMBLE_SIZE):\n        gpu_id = run_id % N_GPUS\n        worker_args.append({\n            \"gpu_id\": gpu_id,\n            \"run_id\": run_id,\n            \"question\": test_question,\n            \"system_prompt\": system_prompt,\n            \"user_template\": user_template,\n            \"additional_info\": {\"answer_unit\": test_unit},\n            \"llm_model_id\": LLM_MODEL_ID,\n            \"llm_dtype\": LLM_DTYPE,\n            \"temperature\": ENSEMBLE_TEMPERATURE,\n            \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n            \"documents\": documents,  # serializable list of dicts\n            \"top_k\": 3,\n        })\n    \n    t0 = time.time()\n    \n    # Use ProcessPoolExecutor — each process gets its own GPU\n    # max_workers = N_GPUS ensures one model per GPU at a time\n    with ProcessPoolExecutor(max_workers=N_GPUS) as executor:\n        parallel_results = list(executor.map(_ensemble_worker, worker_args))\n    \n    elapsed = time.time() - t0\n    \n    print(f\"\\n{PARALLEL_ENSEMBLE_SIZE} runs completed in {elapsed:.1f}s \"\n          f\"({elapsed/PARALLEL_ENSEMBLE_SIZE:.1f}s/run effective, \"\n          f\"{N_GPUS} GPUs)\")\n    \n    for r in parallel_results:\n        print(f\"  Run {r['run_id']} (GPU {r['gpu_id']}): {r['answer_value']!r}\")\n    \n    # Aggregate\n    voted_parallel = aggregate_ensemble(parallel_results, ref_mode=\"union\")\n    \n    print(f\"\\n{'=' * 60}\")\n    print(f\"PARALLEL ENSEMBLE RESULT ({N_GPUS} GPUs)\")\n    print(f\"{'=' * 60}\")\n    print(f\"  Voted answer:  {voted_parallel['answer_value']}\")\n    print(f\"  Expected:      {test_expected}\")\n    print(f\"  Agreement:     {voted_parallel['agreement']:.0%}\")\n    print(f\"  Vote counts:   {voted_parallel['vote_counts']}\")\n    print(f\"  Ref IDs:       {voted_parallel['ref_id']}\")\n    print(f\"  Speedup:       ~{N_GPUS}x vs serial\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9qf0vi3zvn",
   "source": "### 8c) Batch ensemble over multiple questions\n\nRun the serial ensemble across several train_QA questions and compare\nvoted answers to expected values.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qbzfgeir5bc",
   "source": "# ---------------------------------------------------------------------------\n# Batch ensemble: run N passes for each of several questions\n# ---------------------------------------------------------------------------\n\nBATCH_ENSEMBLE_SIZE = 3  # fewer runs per question to keep this fast\n\nbatch_questions = [\n    next(r for r in qa_rows if \"GShard\" in r[\"question\"]),\n    next(r for r in qa_rows if \"Hyperscale\" in r[\"question\"]),\n    next(r for r in qa_rows if \"water consumption\" in r[\"question\"].lower() and \"training GPT-3\" in r[\"question\"]),\n    next(r for r in qa_rows if \"elephant\" in r[\"question\"].lower()),\n]\n\nprint(f\"Batch ensemble: {len(batch_questions)} questions x {BATCH_ENSEMBLE_SIZE} runs each\\n\")\nprint(f\"{'ID':<8} {'Expected':<20} {'Voted':<20} {'Agree':>6}  Question\")\nprint(\"-\" * 100)\n\nfor row in batch_questions:\n    qid = row[\"id\"]\n    question = row[\"question\"]\n    expected = row[\"answer_value\"]\n    unit = row.get(\"answer_unit\", \"\")\n    \n    run_answers = []\n    for _ in range(BATCH_ENSEMBLE_SIZE):\n        result = await pipeline_ensemble.run_qa(\n            question=question,\n            system_prompt=system_prompt,\n            user_template=user_template,\n            additional_info={\"answer_unit\": unit},\n            top_k=3,\n        )\n        run_answers.append({\n            \"answer_value\": result.answer.answer_value,\n            \"answer\": result.answer.answer,\n            \"ref_id\": result.answer.ref_id,\n            \"explanation\": result.answer.explanation,\n        })\n    \n    voted = aggregate_ensemble(run_answers, ref_mode=\"union\")\n    match = \"Y\" if voted[\"answer_value\"] == expected else \" \"\n    \n    print(\n        f\"{qid:<8} {expected:<20} {voted['answer_value']:<20} \"\n        f\"{voted['agreement']:>5.0%}  {question[:50]}...\"\n    )\n\nprint(f\"\\nBatch ensemble complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohaku-gb10",
   "language": "python",
   "name": "kohaku-gb10"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}