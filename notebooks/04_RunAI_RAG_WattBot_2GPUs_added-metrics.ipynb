{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d140fb-6dc2-4bc2-af7e-6154937a237e",
   "metadata": {},
   "source": [
    "### Setup env. \n",
    "\n",
    "In a terminal in Jupyter lab, run the following commands:\n",
    "\n",
    "```bash\n",
    "pwd\n",
    "```\n",
    "\n",
    "```output\n",
    "/workspace2\n",
    "```\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/qualiaMachine/GB10_Tests.git\n",
    "```\n",
    "\n",
    "Cd to RAG directory\n",
    "```bash\n",
    "cd GB10_Tests/WattBot\n",
    "```\n",
    "\n",
    "Create cache and temp directories on /workspace2 (the large disk) and tell uv/pip to use them instead of the small home volume. \n",
    "```bash\n",
    "# create cache and temp directories on /workspace2 (large disk)\n",
    "mkdir -p \\\n",
    "  /workspace2/.cache/uv \\\n",
    "  /workspace2/.cache/pip \\\n",
    "  /workspace2/.cache/huggingface \\\n",
    "  /workspace2/.cache/torch \\\n",
    "  /workspace2/.tmp \\\n",
    "  /workspace2/.tmp/uv \\\n",
    "  /workspace2/.tmp/hf\n",
    "\n",
    "# -------------------------\n",
    "# System-wide temp (many libs respect these)\n",
    "# -------------------------\n",
    "export TMPDIR=/workspace2/.tmp\n",
    "export TEMP=/workspace2/.tmp\n",
    "export TMP=/workspace2/.tmp\n",
    "\n",
    "# -------------------------\n",
    "# Python / package managers\n",
    "# -------------------------\n",
    "export UV_CACHE_DIR=/workspace2/.cache/uv\n",
    "export PIP_CACHE_DIR=/workspace2/.cache/pip\n",
    "\n",
    "# (optional) uv can also use its own temp dir if you want separation\n",
    "# export TMPDIR=/workspace2/.tmp/uv\n",
    "\n",
    "# -------------------------\n",
    "# Hugging Face / PyTorch\n",
    "# -------------------------\n",
    "export HF_HOME=/workspace2/.cache/huggingface\n",
    "export TRANSFORMERS_CACHE=/workspace2/.cache/huggingface\n",
    "export HF_DATASETS_CACHE=/workspace2/.cache/huggingface\n",
    "export TORCH_HOME=/workspace2/.cache/torch\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Install UV\n",
    "```bash\n",
    "pip install uv\n",
    "```\n",
    "\n",
    "Create venv\n",
    "```bash\n",
    "uv venv # creates .venv folder\n",
    "```\n",
    "\n",
    "Activate venv\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Install requirements\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Add venv as named kernel in Jupyter lab\n",
    "```bash\n",
    "python -m ipykernel install \\\n",
    "  --user \\\n",
    "  --name wattbot \\\n",
    "  --display-name \"wattbot\"\n",
    "```\n",
    "\n",
    "\n",
    "After waiting a minute, you can select the \"wattbot\" kernel in Jupyter lab notebooks to use this environment and run the `GB10_Tests/WattBot/02_RunAI_RAG_WattBot.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a067113-4b07-4ea5-a9e3-4ce73cae5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /workspace2/.cache/huggingface/* # so we don't fill our full 1TB when we re-run this code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d97444b-2aa7-4403-a0ba-e4d880d4f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /workspace2/.cache/huggingface/hub # add folder back in so we don't install models into home by accident\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540d899e-8103-4b47-9672-9d8e6fda9ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set TMPDIR = /workspace2/.tmp\n",
      "Set HF_HOME = /workspace2/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WS = Path(\"/workspace2\")\n",
    "TMP_BASE = WS / \".tmp\"\n",
    "CACHE_BASE = WS / \".cache\"\n",
    "\n",
    "# Make sure directories exist\n",
    "for p in [\n",
    "    TMP_BASE,\n",
    "    TMP_BASE / \"uv\",\n",
    "    TMP_BASE / \"hf\",\n",
    "    CACHE_BASE / \"uv\",\n",
    "    CACHE_BASE / \"pip\",\n",
    "    CACHE_BASE / \"huggingface\",\n",
    "    CACHE_BASE / \"torch\",\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# System-wide temp\n",
    "os.environ[\"TMPDIR\"] = str(TMP_BASE)\n",
    "os.environ[\"TEMP\"]  = str(TMP_BASE)\n",
    "os.environ[\"TMP\"]   = str(TMP_BASE)\n",
    "\n",
    "# uv / pip caches\n",
    "os.environ[\"UV_CACHE_DIR\"]  = str(CACHE_BASE / \"uv\")\n",
    "os.environ[\"PIP_CACHE_DIR\"] = str(CACHE_BASE / \"pip\")\n",
    "\n",
    "# Hugging Face / PyTorch caches\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_BASE / \"huggingface\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(CACHE_BASE / \"huggingface\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"]  = str(CACHE_BASE / \"huggingface\")\n",
    "os.environ[\"TORCH_HOME\"] = str(CACHE_BASE / \"torch\")\n",
    "\n",
    "# Optional: general cache root for other libs\n",
    "os.environ.setdefault(\"XDG_CACHE_HOME\", str(CACHE_BASE))\n",
    "\n",
    "print(\"Set TMPDIR =\", os.environ[\"TMPDIR\"])\n",
    "print(\"Set HF_HOME =\", os.environ[\"HF_HOME\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7677-bb22-4cbb-963a-e58e0d54904b",
   "metadata": {},
   "source": [
    "Run the below cell to make sure the temp dirs are set to live within /workspace2 (checks setup above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929b088e-b135-4164-b631-938d4b4cedcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMPDIR               = /workspace2/.tmp\n",
      "TEMP                 = /workspace2/.tmp\n",
      "TMP                  = /workspace2/.tmp\n",
      "UV_CACHE_DIR         = /workspace2/.cache/uv\n",
      "PIP_CACHE_DIR        = /workspace2/.cache/pip\n",
      "HF_HOME              = /workspace2/.cache/huggingface\n",
      "TRANSFORMERS_CACHE   = /workspace2/.cache/huggingface\n",
      "HF_DATASETS_CACHE    = /workspace2/.cache/huggingface\n",
      "TORCH_HOME           = /workspace2/.cache/torch\n",
      "XDG_CACHE_HOME       = /workspace2/.cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "keys = [\n",
    "    \"TMPDIR\", \"TEMP\", \"TMP\",\n",
    "    \"UV_CACHE_DIR\", \"PIP_CACHE_DIR\",\n",
    "    \"HF_HOME\", \"TRANSFORMERS_CACHE\", \"HF_DATASETS_CACHE\",\n",
    "    \"TORCH_HOME\", \"XDG_CACHE_HOME\",\n",
    "]\n",
    "\n",
    "for k in keys:\n",
    "    print(f\"{k:20} =\", os.environ.get(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c81ff",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with a Notebook GPU\"\n",
    "teaching: 30\n",
    "exercises: 15\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How can we run a basic Retrieval-Augmented Generation (RAG) pipeline entirely from a single GPU-backed SageMaker notebook?\n",
    "- How do we go from raw PDFs and CSV files to a searchable embedding space for WattBot documents?\n",
    "- How can we generate WattBot-style answers (including citations and evidence) that follow the competition’s scoring conventions?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Verify that our SageMaker notebook instance has a working GPU and compatible Python environment.\n",
    "- Load the WattBot metadata and question–answer files from local storage and inspect their structure.\n",
    "- Download all referenced PDFs from `metadata.csv` and turn them into a collection of text pages with useful metadata attached.\n",
    "- Implement a simple, explicit “from scratch” text-chunking and embedding pipeline without relying on FAISS or production vector DBs.\n",
    "- Build a small retrieval helper that finds the most relevant chunks for a question using cosine similarity in embedding space.\n",
    "- Wire the retriever to a local Qwen 7B-style generator to produce WattBot-format answers (including `answer`, `ref_id`, `ref_url`, and `supporting_materials`).\n",
    "- Add a second LLM pass that generates short explanations and marks whether the evidence comes from text, figures, tables, or a combination.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "\n",
    "## Working with AWS for RAG Experiments \n",
    "\n",
    "In the previous episode, we briefly introduced several approaches for implementing RAG in AWS. Here, we are simply selecting a good GPU instance that can handle whatever RAG system we want to build. This approach is:\n",
    "\n",
    "- Very easy to understand core on the AWS side of things (just select GPU instance and you're good to move on)\n",
    "- Ideal for learning retrieval and generation steps  \n",
    "- Great for experimentation and debugging  \n",
    "\n",
    "However, it is **not the most cost‑efficient method**. In upcoming episodes we will introduce more efficient and production‑aligned GPU strategies, including:\n",
    "\n",
    "- On-demand GPU tasks  \n",
    "- Fully managed asynchronous jobs  \n",
    "- Serverless or streaming LLM inference  \n",
    "- SageMaker batch transform & RAG pipelines  \n",
    "- Embedding jobs that run only when needed  \n",
    "\n",
    "Those techniques bring you closer to best practice for scalable and budget‑friendly research computing.\n",
    "\n",
    "**Remember to Shut Down Your AWS Instance**: GPU notebook instances continue billing **even when idle**.  Always:\n",
    "\n",
    "- Save your work  \n",
    "- Shut down or stop the instance when not in use\n",
    "- Verify the status in the AWS console  \n",
    "\n",
    "This habit prevents accidental ongoing GPU charges.\n",
    "\n",
    "\n",
    "## Overview: WattBot RAG on a single notebook GPU\n",
    "\n",
    "In this episode we build a **minimal but realistic RAG pipeline** from the [WattBot 2025](https://www.kaggle.com/competitions/WattBot2025/overview) challenge that runs entirely from a single GPU-backed SageMaker notebook.\n",
    "\n",
    "In this episode we will:\n",
    "\n",
    "1. **Work directly with the WattBot data.**\n",
    "   - Use `train_QA.csv` and `metadata.csv` from the competition dataset.\n",
    "   - Download all referenced PDFs (our RAG corpus) using the URLs in `metadata.csv`.\n",
    "2. **Implement the core RAG steps explicitly in code.**\n",
    "   - Read PDFs, extract per-page text, and attach document metadata.\n",
    "   - Chunk text into overlapping segments suitable for embedding.\n",
    "   - Embed chunks with a sentence-transformer (`thenlper/gte-base`)\n",
    "   - Implement cosine-similarity search over the embedding matrix.\n",
    "3. **Connect to a local Qwen-style generator.**\n",
    "   - Use a quantized 7B model on a GPU-backed instance (e.g., `ml.g5.xlarge`).\n",
    "   - Construct WattBot-style answers that we can compare against `train_QA.csv`.\n",
    "4. **Add an explanation pass.**\n",
    "   - Use an LLM to look at the retrieved evidence, the answer, and citations.\n",
    "   - Generate a short explanation and label the **evidence type**: `[Quote]`, `[Table]`, `[Figure]`, or `[Mixed]`.\n",
    "\n",
    "\n",
    "## Notebook + dataset setup\n",
    "\n",
    "For this episode, we assume you are running on an AWS SageMaker notebook instance with a GPU, such as:\n",
    "\n",
    "- `ml.g5.xlarge` (recommended) or\n",
    "- `ml.g4dn.xlarge` (may work with smaller models / more aggressive quantization).\n",
    "\n",
    "See [Instances for ML](https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/instances-for-ML.html) for further guidance.\n",
    "\n",
    "\n",
    "### Step 1 – Download `data.zip` locally\n",
    "\n",
    "We’ll use the **WattBot 2025** dataset. Download the workshop data archive to your laptop or desktop:\n",
    "\n",
    "- Open this link in your browser: https://github.com/carpentries-incubator/ML_with_AWS_SageMaker/blob/main/data/data.zip\n",
    "- Save `data.zip` somewhere you can find it easily and unzip the folder contents\n",
    "\n",
    "This archive should include a `data/wattbot/` folder containing:\n",
    "\n",
    "- `metadata.csv` – index of all WattBot papers.\n",
    "- `train_QA.csv` – labeled questions + ground truth answers.\n",
    "\n",
    "### Step 2 – Create a WattBot S3 bucket\n",
    "\n",
    "In the AWS console:\n",
    "\n",
    "1. Go to **S3**.\n",
    "2. Create a new bucket named something like:  \n",
    "   `teamname-yourname-wattbot`\n",
    "3. Keep **Block all public access** enabled.\n",
    "4. (Optional, but recommended) Add tags so we can track costs:  \n",
    "   - `Project = your-team-name`  \n",
    "   - `Name = your-name`  \n",
    "   - `Purpose = RAG-demo`\n",
    "\n",
    "### Step 3 – Upload the WattBot files to S3\n",
    "\n",
    "1. In your new bucket, click **Upload**.\n",
    "2. Drag the `data/wattbot/` folder contents from `data.zip` into the upload dialog.\n",
    "3. Upload it so that your bucket contains paths like:\n",
    "\n",
    "   - `metadata.csv`\n",
    "   - `train_QA.csv`\n",
    "\n",
    "We’ll pull these files from S3 into the notebook in the next steps.\n",
    "\n",
    "\n",
    "###  Verify GPU and basic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b18742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 23 18:01:03 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX PRO 6000 Blac...    On  |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   50C    P8             34W /  450W |       3MiB /  97887MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX PRO 6000 Blac...    On  |   00000001:C5:00.0 Off |                    0 |\n",
      "| N/A   34C    P8             30W /  450W |       3MiB /  97887MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi || echo \"No GPU detected – please switch to a GPU-backed instance (e.g., ml.g5.xlarge) before running this notebook.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af41ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda available: True\n",
      "num gpus: 2\n"
     ]
    }
   ],
   "source": [
    "# also verify you've selected teh conda_pytorch_p310 kernel\n",
    "import torch\n",
    "print(\"torch cuda available:\", torch.cuda.is_available())\n",
    "print(\"num gpus:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41da5c7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc683f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d149f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local data dir: ./data\n"
     ]
    }
   ],
   "source": [
    "# Local working directory in the notebook instance\n",
    "local_data_dir = \"./data\"\n",
    "\n",
    "print(\"Local data dir:\", local_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5766bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download metadata.csv and train_QA.csv\n",
    "metadata_key = \"metadata.csv\"\n",
    "train_qa_key = \"train_QA.csv\"\n",
    "\n",
    "metadata_path = os.path.join(local_data_dir, metadata_key)\n",
    "train_qa_path = os.path.join(local_data_dir, train_qa_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3c018",
   "metadata": {},
   "source": [
    "## Step 1 – Imports, paths, and safe CSV loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f78f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace2/GB10_Tests/WattBot/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import zipfile\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d33e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_QA.csv columns: ['id', 'question', 'answer', 'answer_value', 'answer_unit', 'ref_id', 'ref_url', 'supporting_materials', 'explanation']\n",
      "metadata.csv columns: ['id', 'type', 'title', 'year', 'citation', 'url']\n",
      "\n",
      "Number of training QAs: 41\n",
      "Number of metadata rows: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>We present the ML.ENERGY Benchmark, a benchmar...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3 tCO2e</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>\"Training GShard-600B used 24 MWh and produced...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>64.7 GB</td>\n",
       "      <td>64.7</td>\n",
       "      <td>GB</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>Table 3: Large language models used for evalua...</td>\n",
       "      <td>Table 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>MWh</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['wu2021b','patterson2021']</td>\n",
       "      <td>['https://arxiv.org/abs/2108.06738','https://a...</td>\n",
       "      <td>Wu 2021, body text near Fig. 1: \"…between trad...</td>\n",
       "      <td>The &gt;40% statement is explicit in Wu. Patterso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q078</td>\n",
       "      <td>For every medium-length GPT-3 completion (prom...</td>\n",
       "      <td>0.02 to 0.1 bottles</td>\n",
       "      <td>[0.02,0.1]</td>\n",
       "      <td>500 mL bottles</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>\"Additionally, GPT-3 needs to -drink- (i.e., c...</td>\n",
       "      <td>The paper states that one 500ml bottle is cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q091</td>\n",
       "      <td>From a sample of 60 papers from top AI confere...</td>\n",
       "      <td>55%</td>\n",
       "      <td>55</td>\n",
       "      <td>percent</td>\n",
       "      <td>['schwartz2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1907.10597']</td>\n",
       "      <td>\"A large majority of the papers target accurac...</td>\n",
       "      <td>Requires calculation (75-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q102</td>\n",
       "      <td>True or False: The AI Act makes energy consump...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>0</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Section 4.3 Transparency: 'Where the Act does ...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q105</td>\n",
       "      <td>What is the projected maximum batch size (in s...</td>\n",
       "      <td>28 samples per batch</td>\n",
       "      <td>28</td>\n",
       "      <td>samples</td>\n",
       "      <td>['xia2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2408.04693']</td>\n",
       "      <td>Figure 13</td>\n",
       "      <td>Figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q106</td>\n",
       "      <td>What was the approximate speedup in inference ...</td>\n",
       "      <td>2x</td>\n",
       "      <td>2</td>\n",
       "      <td>multiplier</td>\n",
       "      <td>['samsi2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2310.03003']</td>\n",
       "      <td>\"anywhere from a 2 times (7B) … increase … on ...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q124</td>\n",
       "      <td>What is the estimated total operational water ...</td>\n",
       "      <td>5.439 million liters</td>\n",
       "      <td>5439000</td>\n",
       "      <td>liters</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>Table 1</td>\n",
       "      <td>Table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q135</td>\n",
       "      <td>True or False: The authors propose that sustai...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Section 5.4 Sustainability Impact Assessments:...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q139</td>\n",
       "      <td>As of 2023, what was the water use effectivene...</td>\n",
       "      <td>0.18 L/kWh</td>\n",
       "      <td>0.18</td>\n",
       "      <td>L/kWh</td>\n",
       "      <td>['amazon2023']</td>\n",
       "      <td>['https://sustainability.aboutamazon.com/2023-...</td>\n",
       "      <td>0.18 Liters of water per kilowatt-hour (L/kWh)...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q146</td>\n",
       "      <td>True or False: Local inference was emphasized ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['khan2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2504.06307']</td>\n",
       "      <td>Section III.B.1: 'local inference allows model...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q153</td>\n",
       "      <td>True or False: Tracking the runtime of a train...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['strubell2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>\"Authors should report training time and sensi...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0   q003  What is the name of the benchmark suite presen...   \n",
       "1   q009  What were the net CO2e emissions from training...   \n",
       "2   q054  What is the model size in gigabytes (GB) for t...   \n",
       "3   q062  What was the total electricity consumption of ...   \n",
       "4   q075  True or False: Hyperscale data centers in 2020...   \n",
       "5   q078  For every medium-length GPT-3 completion (prom...   \n",
       "6   q091  From a sample of 60 papers from top AI confere...   \n",
       "7   q102  True or False: The AI Act makes energy consump...   \n",
       "8   q105  What is the projected maximum batch size (in s...   \n",
       "9   q106  What was the approximate speedup in inference ...   \n",
       "10  q124  What is the estimated total operational water ...   \n",
       "11  q135  True or False: The authors propose that sustai...   \n",
       "12  q139  As of 2023, what was the water use effectivene...   \n",
       "13  q146  True or False: Local inference was emphasized ...   \n",
       "14  q153  True or False: Tracking the runtime of a train...   \n",
       "\n",
       "                                               answer         answer_value  \\\n",
       "0                             The ML.ENERGY Benchmark  ML.ENERGY Benchmark   \n",
       "1                                           4.3 tCO2e                  4.3   \n",
       "2                                             64.7 GB                 64.7   \n",
       "3   Unable to answer with confidence based on the ...             is_blank   \n",
       "4                                                TRUE                    1   \n",
       "5                                 0.02 to 0.1 bottles           [0.02,0.1]   \n",
       "6                                                 55%                   55   \n",
       "7                                               FALSE                    0   \n",
       "8                                28 samples per batch                   28   \n",
       "9                                                  2x                    2   \n",
       "10                               5.439 million liters              5439000   \n",
       "11                                               TRUE                    1   \n",
       "12                                         0.18 L/kWh                 0.18   \n",
       "13                                               TRUE                    1   \n",
       "14                                               TRUE                    1   \n",
       "\n",
       "       answer_unit                       ref_id  \\\n",
       "0         is_blank                ['chung2025']   \n",
       "1            tCO2e            ['patterson2021']   \n",
       "2               GB                 ['chen2024']   \n",
       "3              MWh                     is_blank   \n",
       "4         is_blank  ['wu2021b','patterson2021']   \n",
       "5   500 mL bottles                  ['li2025b']   \n",
       "6          percent             ['schwartz2019']   \n",
       "7         is_blank                ['ebert2024']   \n",
       "8          samples                  ['xia2024']   \n",
       "9       multiplier                ['samsi2024']   \n",
       "10          liters                  ['li2025b']   \n",
       "11        is_blank                ['ebert2024']   \n",
       "12           L/kWh               ['amazon2023']   \n",
       "13        is_blank                 ['khan2025']   \n",
       "14        is_blank             ['strubell2019']   \n",
       "\n",
       "                                              ref_url  \\\n",
       "0                ['https://arxiv.org/pdf/2505.06371']   \n",
       "1                ['https://arxiv.org/pdf/2104.10350']   \n",
       "2                ['https://arxiv.org/pdf/2405.01814']   \n",
       "3                                            is_blank   \n",
       "4   ['https://arxiv.org/abs/2108.06738','https://a...   \n",
       "5                ['https://arxiv.org/pdf/2304.03271']   \n",
       "6                ['https://arxiv.org/pdf/1907.10597']   \n",
       "7                ['https://arxiv.org/pdf/2410.06681']   \n",
       "8                ['https://arxiv.org/pdf/2408.04693']   \n",
       "9                ['https://arxiv.org/pdf/2310.03003']   \n",
       "10               ['https://arxiv.org/pdf/2304.03271']   \n",
       "11               ['https://arxiv.org/pdf/2410.06681']   \n",
       "12  ['https://sustainability.aboutamazon.com/2023-...   \n",
       "13               ['https://arxiv.org/pdf/2504.06307']   \n",
       "14               ['https://arxiv.org/pdf/1906.02243']   \n",
       "\n",
       "                                 supporting_materials  \\\n",
       "0   We present the ML.ENERGY Benchmark, a benchmar...   \n",
       "1   \"Training GShard-600B used 24 MWh and produced...   \n",
       "2   Table 3: Large language models used for evalua...   \n",
       "3                                            is_blank   \n",
       "4   Wu 2021, body text near Fig. 1: \"…between trad...   \n",
       "5   \"Additionally, GPT-3 needs to -drink- (i.e., c...   \n",
       "6   \"A large majority of the papers target accurac...   \n",
       "7   Section 4.3 Transparency: 'Where the Act does ...   \n",
       "8                                           Figure 13   \n",
       "9   \"anywhere from a 2 times (7B) … increase … on ...   \n",
       "10                                            Table 1   \n",
       "11  Section 5.4 Sustainability Impact Assessments:...   \n",
       "12  0.18 Liters of water per kilowatt-hour (L/kWh)...   \n",
       "13  Section III.B.1: 'local inference allows model...   \n",
       "14  \"Authors should report training time and sensi...   \n",
       "\n",
       "                                          explanation  \n",
       "0                                               Quote  \n",
       "1                                               Quote  \n",
       "2                                             Table 3  \n",
       "3                                            is_blank  \n",
       "4   The >40% statement is explicit in Wu. Patterso...  \n",
       "5   The paper states that one 500ml bottle is cons...  \n",
       "6                        Requires calculation (75-20)  \n",
       "7                                               Quote  \n",
       "8                                              Figure  \n",
       "9                                               Quote  \n",
       "10                                              Table  \n",
       "11                                              Quote  \n",
       "12                                              Quote  \n",
       "13                                              Quote  \n",
       "14                                              Quote  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smart_read_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Try several encodings when reading a CSV file.\n",
    "\n",
    "    Some CSVs (especially those with special characters in author names or titles)\n",
    "    may not be valid UTF-8. This helper rotates through common encodings and raises\n",
    "    the last error only if all fail.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "    last_error = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "    if last_error is not None:\n",
    "        raise last_error\n",
    "    raise RuntimeError(f\"Unable to read CSV at {path}\")\n",
    "\n",
    "\n",
    "train_df = smart_read_csv(train_qa_path)\n",
    "metadata_df = smart_read_csv(metadata_path)\n",
    "\n",
    "print(\"train_QA.csv columns:\", train_df.columns.tolist())\n",
    "print(\"metadata.csv columns:\", metadata_df.columns.tolist())\n",
    "print(\"\\nNumber of training QAs:\", len(train_df))\n",
    "print(\"Number of metadata rows:\", len(metadata_df))\n",
    "\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b7603",
   "metadata": {},
   "source": [
    "## Step 2 – Download all PDFs from `metadata.csv`\n",
    "\n",
    "Next we will...\n",
    "\n",
    "1. Read the `url` column from `metadata.csv`.\n",
    "2. Download each PDF via HTTP and save it locally as `<id>.pdf` under `pdfs/`.\n",
    "3. Report any failures (e.g., missing or malformed URLs) at the end.\n",
    "4. Upload zipped version of corpus to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b991cb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving PDFs to: ./data/pdfs\n",
      "\n",
      "Skipping amazon2023: already exists\n",
      "Skipping chen2024: already exists\n",
      "Skipping chung2025: already exists\n",
      "Skipping cottier2024: already exists\n",
      "Skipping dodge2022: already exists\n",
      "Skipping ebert2024: already exists\n",
      "Skipping erben2023: already exists\n",
      "Skipping fernandez2025: already exists\n",
      "Skipping griggs2024: already exists\n",
      "Skipping han2024: already exists\n",
      "Skipping jegham2025: already exists\n",
      "Skipping khan2025: already exists\n",
      "Skipping kim2025: already exists\n",
      "Skipping li2025a: already exists\n",
      "Skipping li2025b: already exists\n",
      "Skipping luccioni2023: already exists\n",
      "Skipping luccioni2024: already exists\n",
      "Skipping luccioni2025a: already exists\n",
      "Skipping luccioni2025b: already exists\n",
      "Skipping luccioni2025c: already exists\n",
      "Skipping morrison2025: already exists\n",
      "Skipping patterson2021: already exists\n",
      "Skipping rubei2025: already exists\n",
      "Skipping samsi2024: already exists\n",
      "Skipping schwartz2019: already exists\n",
      "Skipping shen2024: already exists\n",
      "Skipping stone2022: already exists\n",
      "Skipping strubell2019: already exists\n",
      "Skipping wu2021a: already exists\n",
      "Skipping wu2021b: already exists\n",
      "Skipping xia2024: already exists\n",
      "Skipping zschache2025: already exists\n",
      "\n",
      "All PDFs downloaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "PDF_DIR = os.path.join(local_data_dir, \"pdfs\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "def download_all_pdfs_from_urls(\n",
    "    metadata: pd.DataFrame,\n",
    "    local_pdf_dir: str,\n",
    "    url_col: str = \"url\",\n",
    "    id_col: str = \"id\",\n",
    "    timeout: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"Download all PDFs referenced in `metadata` using their URLs.\n",
    "\n",
    "    - Saves each file as `<id>.pdf` in `local_pdf_dir`.\n",
    "    - Skips download if the file already exists.\n",
    "    - Strips whitespace from the URL.\n",
    "    - Skips rows with missing or non-HTTP URLs.\n",
    "    - Prints a short summary of any failures.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_pdf_dir, exist_ok=True)\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "\n",
    "    print(f\"Saving PDFs to: {local_pdf_dir}\\n\")\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[id_col]).strip()\n",
    "        local_path = os.path.join(local_pdf_dir, f\"{doc_id}.pdf\")\n",
    "\n",
    "        # Skip if file already exists\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"Skipping {doc_id}: already exists\")\n",
    "            continue\n",
    "\n",
    "        raw_url = row.get(url_col, None)\n",
    "        if not isinstance(raw_url, str):\n",
    "            errors.append((doc_id, \"URL is not a string\"))\n",
    "            continue\n",
    "\n",
    "        pdf_url = raw_url.strip()\n",
    "        if not pdf_url.startswith(\"http\"):\n",
    "            errors.append((doc_id, f\"Invalid URL: {pdf_url!r}\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {doc_id} from {pdf_url} ...\")\n",
    "            resp = requests.get(pdf_url, timeout=timeout, allow_redirects=True)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "            if \"pdf\" not in content_type.lower() and not pdf_url.lower().endswith(\".pdf\"):\n",
    "                print(f\"  Warning: Content-Type does not look like PDF ({content_type})\")\n",
    "\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> FAILED for {doc_id}: {e}\")\n",
    "            errors.append((doc_id, str(e)))\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nSome PDFs could not be downloaded:\")\n",
    "        for doc_id, err in errors:\n",
    "            print(f\"  {doc_id}: {err}\")\n",
    "    else:\n",
    "        print(\"\\nAll PDFs downloaded successfully!\")\n",
    "\n",
    "\n",
    "download_all_pdfs_from_urls(\n",
    "    metadata_df,\n",
    "    PDF_DIR,\n",
    "    url_col=\"url\",\n",
    "    id_col=\"id\",\n",
    "    timeout=20,\n",
    ")\n",
    "\n",
    "len(os.listdir(PDF_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2afa1f",
   "metadata": {},
   "source": [
    "## Step 3 – Turn PDFs into page-level “documents”\n",
    "\n",
    "Next, we convert each PDF into a list of **page-level records**. Each record stores:\n",
    "\n",
    "- `text`: page text (as extracted by `pypdf`).\n",
    "- `doc_id`: short ID from `metadata.csv` (e.g., `strubell2019`).\n",
    "- `title`: title of the document.\n",
    "- `url`: original PDF URL.\n",
    "- `page_num`: zero-based page index.\n",
    "- `page_label`: label used inside the PDF (often 1-based).\n",
    "\n",
    "Later, we will **chunk these pages** into smaller overlapping segments for embedding.\n",
    "\n",
    "### Why we page-chunk first\n",
    "\n",
    "We split the PDF into **pages before chunking** because pages give us a stable, easy-to-interpret unit.  \n",
    "This helps with:\n",
    "\n",
    "- **Keeping metadata** (doc ID, URL, page labels) tied to the text.  \n",
    "- **Debugging retrieval** — it’s much easier to understand what the model saw if we know which page(s) were used.  \n",
    "- **Cleaning text** before making smaller overlapping chunks.  \n",
    "- **Flexibility later** — once pages are structured, we can try different chunk sizes or strategies without re-extracting the PDF.\n",
    "\n",
    "In short: **pages first → then chunks** keeps the workflow cleaner and easier to reason about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "060b1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a0e8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract text from patterson2021 page 1: 'bbox'\n",
      "Failed to extract text from patterson2021 page 4: 'bbox'\n",
      "Loaded 639 page-level records from 32 PDFs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Amazon \\nSustainability \\nReport\\n2023',\n",
       " 'doc_id': 'amazon2023',\n",
       " 'title': '2023 Amazon Sustainability Report',\n",
       " 'url': 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       " 'page_num': 0,\n",
       " 'page_label': '1',\n",
       " 'total_pages': 98}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def pdfs_to_page_docs(metadata: pd.DataFrame, pdf_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load each PDF into a list of page-level dictionaries.\n",
    "\n",
    "    Each dict has keys: text, doc_id, title, url, page_num, page_label, total_pages.\n",
    "    \"\"\"\n",
    "    page_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[\"id\"]).strip()\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{doc_id}.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Missing PDF for {doc_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        total_pages = len(reader.pages)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract text from {doc_id} page {i}: {e}\")\n",
    "                text = \"\"\n",
    "\n",
    "            text = text.strip()\n",
    "            if not text:\n",
    "                # Still keep the page so we know it exists, but mark it as empty\n",
    "                text = \"[[EMPTY PAGE TEXT – see original PDF for tables/figures]]\"\n",
    "\n",
    "            page_docs.append(\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"page_num\": i,\n",
    "                    \"page_label\": str(i + 1),\n",
    "                    \"total_pages\": total_pages,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return page_docs\n",
    "\n",
    "\n",
    "page_docs = pdfs_to_page_docs(metadata_df, PDF_DIR)\n",
    "print(f\"Loaded {len(page_docs)} page-level records from {len(metadata_df)} PDFs.\")\n",
    "page_docs[0] if page_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c06ed",
   "metadata": {},
   "source": [
    "## Step 4 – Simple, explicit text chunking\n",
    "\n",
    "RAG systems typically break documents into **chunks** so that:\n",
    "\n",
    "- Each chunk is long enough to carry meaningful context.\n",
    "- No chunk is so long that it blows up the embedding/LLM context window.\n",
    "\n",
    "For this workshop we will implement a **simple sliding-window chunker** that operates on characters:\n",
    "\n",
    "- `chunk_size_chars`: maximum characters per chunk (e.g., 1,000–1,500).\n",
    "- `chunk_overlap_chars`: overlap between consecutive chunks (e.g., 200).\n",
    "\n",
    "In our own work, you may wish to plug in more sophisticated *semantic chunking*  methods(e.g., splitting on headings, section titles, or sentence boundaries). For now, we'll keep the implementation explicit and easy to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0448a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(\n",
    "    text: str,\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"Split `text` into overlapping character-based chunks.\n",
    "\n",
    "    This is a simple baseline; more advanced versions might:\n",
    "    - split on sentence boundaries, or\n",
    "    - merge short paragraphs and respect section headings.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while start < text_len:\n",
    "        end = min(start + chunk_size_chars, text_len)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == text_len:\n",
    "            break\n",
    "        # Move the window forward, keeping some overlap\n",
    "        start = end - chunk_overlap_chars\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def make_chunked_docs(\n",
    "    page_docs: List[Dict[str, Any]],\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Turn page-level records into smaller overlapping text chunks.\n",
    "\n",
    "    Each chunk keeps a pointer back to its document and page metadata.\n",
    "    \"\"\"\n",
    "    chunked: List[Dict[str, Any]] = []\n",
    "    for page in page_docs:\n",
    "        page_text = page[\"text\"]\n",
    "        chunks = split_text_into_chunks(\n",
    "            page_text,\n",
    "            chunk_size_chars=chunk_size_chars,\n",
    "            chunk_overlap_chars=chunk_overlap_chars,\n",
    "        )\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            chunked.append(\n",
    "                {\n",
    "                    \"text\": chunk_text,\n",
    "                    \"doc_id\": page[\"doc_id\"],\n",
    "                    \"title\": page[\"title\"],\n",
    "                    \"url\": page[\"url\"],\n",
    "                    \"page_num\": page[\"page_num\"],\n",
    "                    \"page_label\": page[\"page_label\"],\n",
    "                    \"total_pages\": page[\"total_pages\"],\n",
    "                    \"chunk_idx_in_page\": idx,\n",
    "                }\n",
    "            )\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ed7c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing chunk file: ./data/chunks.jsonl\n",
      "Loaded chunked docs: 2874\n",
      "Raw pages: 639\n",
      "Chunked docs: 2874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Amazon \\nSustainability \\nReport\\n2023',\n",
       " 'doc_id': 'amazon2023',\n",
       " 'title': '2023 Amazon Sustainability Report',\n",
       " 'url': 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       " 'page_num': 0,\n",
       " 'page_label': '1',\n",
       " 'total_pages': 98,\n",
       " 'chunk_idx_in_page': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "chunks_s3_key = 'chunks.jsonl'\n",
    "chunks_jsonl_path = os.path.join(local_data_dir, chunks_s3_key)\n",
    "\n",
    "def save_chunked_docs_jsonl(path, chunks):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in chunks:\n",
    "            json.dump(rec, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_chunked_docs_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Cached chunking logic\n",
    "# -------------------------------------------------------------------\n",
    "if os.path.exists(chunks_jsonl_path):\n",
    "    print(f\"Found existing chunk file: {chunks_jsonl_path}\")\n",
    "    chunked_docs = load_chunked_docs_jsonl(chunks_jsonl_path)\n",
    "    print(\"Loaded chunked docs:\", len(chunked_docs))\n",
    "else:\n",
    "    print(\"No chunk file found. Running chunking step...\")\n",
    "    chunked_docs = make_chunked_docs(page_docs)\n",
    "    save_chunked_docs_jsonl(chunks_jsonl_path, chunked_docs)\n",
    "    print(f\"Saved chunked docs to {chunks_jsonl_path}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(\"Raw pages:\", len(page_docs))\n",
    "print(\"Chunked docs:\", len(chunked_docs))\n",
    "chunked_docs[0] if chunked_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f23f6",
   "metadata": {},
   "source": [
    "## Step 5 – Build an embedding matrix\n",
    "\n",
    "Now we embed each chunk into a vector using a **sentence-transformer** model. For WattBot, a strong and relatively efficient choice is:\n",
    "\n",
    "### `thenlper/gte-large` (Recommended baseline embedder)\n",
    "\n",
    "- Size / parameters:  ~335M parameters, roughly 1.3–1.4 GB in BF16/FP16 when loaded on GPU. Fits cleanly on T4 (16 GB), L4, A10G, A10, A100, and all g5.* instances.  Offers noticeably better retrieval quality than smaller 100M–150M models without requiring high-end GPU memory. Runs comfortably on g4dn.xlarge, g5.xlarge, or g5.2xlarge during workshops. Lets participants see meaningful improvements from chunking and retrieval methods without excessive compute cost.\n",
    "\n",
    "- Intended use:  General-purpose retrieval and semantic search across academic PDFs, sustainability reports, and mixed-domain long-form documents. Stronger semantic coherence than gte-base or MiniLM, but still lightweight enough for workshop hardware.\n",
    "\n",
    "- Throughput expectations:\n",
    "  - CPU only: workable for small corpora (<2k chunks) but slow for anything larger.  \n",
    "  - GPU (T4, L4, A10G, A100) with batch sizes around 64–128:  \n",
    "    - 20k–40k chunks/min on L4 or A10G  \n",
    "    - 10k–15k chunks/min on T4  \n",
    "    - 50k+ chunks/min on A100  \n",
    "      \n",
    "We will:\n",
    "\n",
    "1. Load the embedding model on GPU if available.\n",
    "2. Encode all chunks in batches.\n",
    "3. Store the resulting matrix as a `torch.Tensor` or `numpy.ndarray` along with the original `chunked_docs` list.\n",
    "\n",
    "Later, we’ll implement a small retrieval helper that does cosine-similarity search over this matrix—no additional indexing library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "047c4979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available for embeddings: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# We'll use a stronger embedding model now that we have a GPU.\n",
    "# This model has ~335M parameters and benefits from GPU acceleration,\n",
    "# but is still reasonable to run on a single 24 GB GPU.\n",
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "\n",
    "use_cuda_for_embeddings = torch.cuda.is_available()\n",
    "print(\"CUDA available for embeddings:\", use_cuda_for_embeddings)\n",
    "\n",
    "# Single shared embedder object that we can pass around.\n",
    "embedder = SentenceTransformer(\n",
    "    embedding_model_id,\n",
    "    device=\"cuda\" if use_cuda_for_embeddings else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc328a95-883f-48f7-9fc3-4ad5327ebf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "capability: (12, 0)\n",
      "name: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "matmul ok, seconds: 0.023830278776586056 mean: -0.0036396931391209364\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"capability:\", torch.cuda.get_device_capability(0))\n",
    "print(\"name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.randn(4096, 4096, device=\"cuda\")\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "y = x @ x\n",
    "torch.cuda.synchronize()\n",
    "print(\"matmul ok, seconds:\", time.perf_counter()-t0, \"mean:\", y.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c34538-ae8d-46db-b069-4d18c4fec9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(embedder.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e0a03e-cb63-4944-8e69-e63f7ee7f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120', 'compute_120']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_arch_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c74d4db2-bf05-4fee-bf28-f97a7ed120a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv ok: -0.003982101567089558 sec: 0.038530409801751375\n",
      "layernorm ok: -1.6916601452976465e-10 sec: 0.0034753610379993916\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# conv (cuDNN)\n",
    "x = torch.randn(16, 3, 224, 224, device=\"cuda\")\n",
    "w = torch.randn(64, 3, 7, 7, device=\"cuda\")\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "y = torch.nn.functional.conv2d(x, w, stride=2, padding=3)\n",
    "torch.cuda.synchronize()\n",
    "print(\"conv ok:\", y.mean().item(), \"sec:\", time.perf_counter()-t0)\n",
    "\n",
    "# layernorm\n",
    "x = torch.randn(1024, 4096, device=\"cuda\")\n",
    "ln = torch.nn.LayerNorm(4096).cuda()\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "y = ln(x)\n",
    "torch.cuda.synchronize()\n",
    "print(\"layernorm ok:\", y.mean().item(), \"sec:\", time.perf_counter()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "343afead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(embedder, docs, batch_size: int = 128) -> np.ndarray:\n",
    "    \"\"\"Embed all chunk texts into a dense matrix of shape (N, D).\"\"\"\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    all_embeddings = []\n",
    "    start = time.time()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        emb = embedder.encode(\n",
    "            batch,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        all_embeddings.append(emb)\n",
    "    embeddings = np.vstack(all_embeddings) if all_embeddings else np.zeros((0, 768))\n",
    "    print(f\"Computed embeddings for {len(texts)} chunks in {time.time() - start:.1f}s\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fbbff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 2874 chunks in 9.2s\n",
      "chunk_embeddings shape: (2874, 1024)\n",
      "Embedding time: 9.17 seconds\n",
      "Docs/sec: 313.36\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Make sure any previous GPU work is finished\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "chunk_embeddings = embed_texts(embedder, chunked_docs)\n",
    "\n",
    "# Synchronize again so timing includes actual GPU compute\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "dt = time.perf_counter() - t0\n",
    "\n",
    "# Save for later metrics export\n",
    "timing_embedding_s = dt\n",
    "embedding_batch_size = 128  # matches embed_texts default in this notebook\n",
    "\n",
    "print(\"chunk_embeddings shape:\", chunk_embeddings.shape)\n",
    "print(f\"Embedding time: {dt:.2f} seconds\")\n",
    "print(f\"Docs/sec: {len(chunked_docs) / dt:.2f}\")\n",
    "\n",
    "# Computed embeddings for 2874 chunks in 76.4s\n",
    "# chunk_embeddings shape: (2874, 1024)\n",
    "# Embedding time: 76.41 seconds\n",
    "# Docs/sec: 37.61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0db35e",
   "metadata": {},
   "source": [
    "### 6. Build a simple retrieval step (cosine similarity)\n",
    "\n",
    "We are **not** using a heavy vector database in this first episode.\n",
    "\n",
    "Instead, we:\n",
    "\n",
    "1. Embed each chunk with `thenlper/gte-large` (done above).\n",
    "2. Embed each question.\n",
    "3. Compute cosine similarity between the question embedding and all chunk embeddings.\n",
    "4. Take the top–k most similar chunks as our retrieved context.\n",
    "\n",
    "This keeps the retrieval logic completely transparent for teaching, while still matching the *spirit* of\n",
    "production systems that use FAISS, Chroma, Weaviate, etc.\n",
    "\n",
    "#### When might FAISS or a vector database be worth exploring?\n",
    "\n",
    "For small–to–medium experiments (a few thousand to maybe tens of thousands of chunks), this \"plain NumPy + cosine\n",
    "similarity\" approach is usually enough. You might consider FAISS or a full vector DB when:\n",
    "\n",
    "- **Your corpus gets big**  \n",
    "  Once you’re in the hundreds of thousands to millions of chunks, brute-force similarity search can become slow\n",
    "  and memory-hungry. FAISS and friends provide *approximate nearest neighbor* search that scales much better.\n",
    "\n",
    "- **You need low-latency, repeated queries**  \n",
    "  If many users (or a web app) will hit your RAG system concurrently, you’ll want:\n",
    "  - fast indexing,\n",
    "  - efficient caching, and\n",
    "  - sub-second query latency.  \n",
    "  Vector DBs are designed for this use case.\n",
    "\n",
    "- **You need rich filtering or metadata search**  \n",
    "  Vector DBs often support:\n",
    "  - filtering by metadata (e.g., `paper = \"chung2025\"`, `year > 2021`),\n",
    "  - combining keyword + vector search (“hybrid search”),\n",
    "  - role-based access control and multi-tenant setups.\n",
    "\n",
    "- **You want to share an index across services**  \n",
    "  If multiple notebooks, microservices, or teams need to reuse the **same embedding index**, a shared FAISS index or\n",
    "  hosted vector DB is much easier to manage than passing around `.npy` files.\n",
    "\n",
    "- **You need GPU-accelerated or distributed search**  \n",
    "  FAISS can use GPUs and sharding to speed up search on very large embedding collections. This is overkill for our\n",
    "  teaching demo (and the Wattbot project in general), but very relevant for production-scale systems.\n",
    "\n",
    "In this episode we deliberately stick with a simple in-memory index so the retrieval step is easy to inspect and\n",
    "debug. In later episodes (or your own projects), you can **swap out the retrieval layer** for FAISS or a vector DB\n",
    "without changing the overall RAG architecture: the model still sees “top–k retrieved chunks” as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f91139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute cosine similarity between rows of a and rows of b.\"\"\"\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return top-k most similar chunks for a query embedding.\"\"\"\n",
    "    if chunk_embeddings.shape[0] == 0:\n",
    "        return []\n",
    "\n",
    "    # query_embedding is 1D (D,)\n",
    "    sims = cosine_similarity_matrix(query_embedding.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for idx in top_idx:\n",
    "        doc = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"page_num\": doc[\"page_num\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae12aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "Top 3 retrieved chunks:\n",
      "- score=0.922 | doc_id=chung2025 | page=0 | snippet=The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization Jae-Won Chung Jeff J. Ma Ruofan Wu Jiachen Liu Oh Jun Kweon Yuxuan Xia Z...\n",
      "- score=0.921 | doc_id=chung2025 | page=9 | snippet=e generalizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the first inference energy benchmark for modern generative AI mo...\n",
      "- score=0.917 | doc_id=chung2025 | page=9 | snippet=ially, requires direct access to the system under test to physically install the power analyzer, which significantly limits who can run the benchmarks (Section ...\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check for `retrieve_top_k` on the first training question\n",
    "first_row = train_df.iloc[0]\n",
    "test_question = first_row[\"question\"]\n",
    "print(\"Sample question:\", test_question)\n",
    "\n",
    "test_q_emb = embedder.encode(\n",
    "    [test_question],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")[0]\n",
    "\n",
    "test_retrieved = retrieve_top_k(\n",
    "    query_embedding=test_q_emb,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    chunked_docs=chunked_docs,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "print(f\"Top {len(test_retrieved)} retrieved chunks:\")\n",
    "for r in test_retrieved:\n",
    "    snippet = r[\"text\"].replace(\"\\n\", \" \")\n",
    "    if len(snippet) > 160:\n",
    "        snippet = snippet[:160] + \"...\"\n",
    "    print(f\"- score={r['score']:.3f} | doc_id={r['doc_id']} | page={r['page_num']} | snippet={snippet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a943ea4",
   "metadata": {},
   "source": [
    "### 7. Load the Qwen model for answer generation\n",
    "\n",
    "For this episode we use **Qwen2.5-7B-Instruct** via the Hugging Face `transformers` library.\n",
    "\n",
    "- Parameter count: ~7 billion.\n",
    "- VRAM needs: ~14–16 GB in bfloat16 / 4-bit; fine for `ml.g5.xlarge` or a similar single-GPU instance.\n",
    "- Intended use here: short, grounded answers plus a normalized `answer_value`.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Call Qwen once to propose an answer and supporting evidence.\n",
    "2. Call Qwen a **second time** with a smaller prompt to generate a short explanation (<= 100 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "385c98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    \"fp16\": torch.float16,\n",
    "    \"bf16\": torch.bfloat16,\n",
    "    \"fp32\": torch.float32,\n",
    "}\n",
    "\n",
    "def load_textgen_model(\n",
    "    model_id: str,\n",
    "    quant: str | None = None,          # None | \"8bit\" | \"4bit\"\n",
    "    dtype: str = \"bf16\",\n",
    "    device_map: str | None = None,     # None | \"auto\"  (kept for compatibility)\n",
    "    trust_remote_code: bool = True,\n",
    "    force_gpu: bool = False,           # NEW: force everything onto cuda:0\n",
    "):\n",
    "    dtype = DTYPE_MAP[dtype]\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "\n",
    "    quant_cfg = None\n",
    "    if quant == \"8bit\":\n",
    "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    elif quant == \"4bit\":\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_torch_dtype=dtype,\n",
    "        )\n",
    "    elif quant is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"quant must be None, '8bit', or '4bit'\")\n",
    "\n",
    "    # Decide device_map behavior\n",
    "    # - force_gpu=True: prevent CPU/disk dispatch (fixes your 72B bnb error)\n",
    "    # - otherwise: honor device_map param (defaulting to \"auto\" for quantized loads)\n",
    "    if force_gpu:\n",
    "        dm = {\"\": 0}  # put the entire model on cuda:0\n",
    "    else:\n",
    "        if quant is not None:\n",
    "            dm = device_map or \"auto\"\n",
    "        else:\n",
    "            dm = device_map  # usually None\n",
    "\n",
    "    # Load model\n",
    "    if dm is not None:\n",
    "        # accelerate/device_map path → DO NOT pass device= to pipeline\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=dm,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        gen_pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "    else:\n",
    "        # simple single-GPU path\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            quantization_config=quant_cfg,\n",
    "            device_map=None,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).to(\"cuda\")\n",
    "        gen_pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device=0)\n",
    "\n",
    "    return tok, model, gen_pipe\n",
    "\n",
    "def call_chat(tok, gen_pipe, system_prompt: str, user_prompt: str, max_new_tokens: int = 384) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    prompt_text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    out = gen_pipe(prompt_text, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    full = out[0][\"generated_text\"]\n",
    "    return full[len(prompt_text):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5345b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = None\n",
      "torch.cuda.device_count() = 2\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n",
    "assert torch.cuda.device_count() == 2, \"Expected 2 GPUs visible (check Run:AI GPU allocation and env).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b50ab-6dc3-42f8-806b-ed75e2386746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae5e198d-4d57-49ec-95ac-937eaaf5efb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db1dceb22514cf78d6fda47ff58464a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 18.07 seconds\n",
      "RAG stands for \"Retrieval-Augmented Generation,\" a technique that combines information retrieval with language generation to improve the accuracy and relevance of generated text.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "quant=\"4bit\"\n",
    "t_load_start = time.perf_counter()\n",
    "tok, model, gen_pipe = load_textgen_model(\n",
    "    model_id,\n",
    "    quant=quant,\n",
    "    dtype=\"bf16\",       \n",
    "    device_map=\"balanced\",\n",
    "    force_gpu=False,\n",
    ")\n",
    "torch.cuda.synchronize()\n",
    "t_load = time.perf_counter() - t_load_start\n",
    "print(f\"Model load time: {t_load:.2f} seconds\")\n",
    "# Save for later metrics export\n",
    "timing_model_load_s = t_load\n",
    "run_meta = {\n",
    "    \"model_id\": model_id,\n",
    "    \"quant\": quant,\n",
    "    \"dtype\": \"bf16\",\n",
    "    \"device_map\": \"balanced\",\n",
    "    \"force_gpu\": False,\n",
    "}\n",
    "\n",
    "\n",
    "print(call_chat(tok, gen_pipe, \"You are helpful.\", \"One sentence: what is RAG?\", max_new_tokens=64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e30cb7f-1517-4d8b-8927-6a0f171370bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen2.5-72B-Instruct'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d50629e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mem after load: {'0': {'allocated_gb': 26.983323648, 'reserved_gb': 50.832867328}, '1': {'allocated_gb': 15.64194048, 'reserved_gb': 25.541214208}}\n"
     ]
    }
   ],
   "source": [
    "# --- NEW: model footprint helpers (disk + memory) ---\n",
    "import os, sys, subprocess, threading\n",
    "from pathlib import Path\n",
    "\n",
    "def _dir_size_bytes(path: Path) -> int:\n",
    "    total = 0\n",
    "    for p in path.rglob(\"*\"):\n",
    "        try:\n",
    "            if p.is_file():\n",
    "                total += p.stat().st_size\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    return total\n",
    "\n",
    "def _fmt_bytes(n: int) -> str:\n",
    "    # human readable\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    f = float(n)\n",
    "    for u in units:\n",
    "        if f < 1024 or u == units[-1]:\n",
    "            return f\"{f:.2f} {u}\"\n",
    "        f /= 1024\n",
    "\n",
    "# def hf_repo_disk_bytes(repo_id: str) -> int | None:\n",
    "#     \"\"\"Best-effort: compute on-disk footprint for a HF repo already present in cache.\n",
    "\n",
    "#     Returns bytes or None if we can't locate it.\n",
    "#     \"\"\"\n",
    "#     # Respect HF_HOME if set; otherwise default cache locations.\n",
    "#     hf_home = Path(os.environ.get(\"HF_HOME\", Path.home() / \".cache\" / \"huggingface\"))\n",
    "#     hub_dir = hf_home / \"hub\"\n",
    "#     if not hub_dir.exists():\n",
    "#         return None\n",
    "\n",
    "#     # HF cache names repos like: models--org--name\n",
    "#     safe = repo_id.replace(\"/\", \"--\")\n",
    "#     repo_dir = hub_dir / f\"models--{safe}\"\n",
    "#     if not repo_dir.exists():\n",
    "#         # Could be a dataset, or different cache layout\n",
    "#         return None\n",
    "\n",
    "#     # The bulk is usually in snapshots/<hash>/\n",
    "#     # We'll sum the whole repo folder to include refs + blobs + snapshots.\n",
    "#     return _dir_size_bytes(repo_dir)\n",
    "\n",
    "def gpu_mem_snapshot_gb() -> dict:\n",
    "    \"\"\"Current allocated/reserved on each visible CUDA device.\"\"\"\n",
    "    out = {}\n",
    "    if not torch.cuda.is_available():\n",
    "        return out\n",
    "    for d in range(torch.cuda.device_count()):\n",
    "        out[str(d)] = {\n",
    "            \"allocated_gb\": float(torch.cuda.memory_allocated(d) / 1e9),\n",
    "            \"reserved_gb\": float(torch.cuda.memory_reserved(d) / 1e9),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# Record model disk footprint (after from_pretrained has pulled weights)\n",
    "# model_disk_bytes = hf_repo_disk_bytes(model_id)\n",
    "# print(\"Model on-disk footprint (HF cache):\", None if model_disk_bytes is None else _fmt_bytes(model_disk_bytes))\n",
    "\n",
    "\n",
    "# Record baseline GPU memory right after load\n",
    "model_gpu_mem_after_load = gpu_mem_snapshot_gb()\n",
    "print(\"GPU mem after load:\", model_gpu_mem_after_load)\n",
    "\n",
    "\n",
    "run_meta[\"gpu_mem_after_load\"] = model_gpu_mem_after_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be64d2aa-6650-4039-8b18-cef2a7eee2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-72B-Instruct\n",
      "Canonical params (from docs): 72,700,000,000\n",
      "Bytes per param on disk (assumed): 2\n",
      "Estimated disk size (weights only): 135.41 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DISK SIZE (WEIGHTS) — CANONICAL QWEN 2.5 ONLY (NO FILE LOOKUPS)\n",
    "\n",
    "What this code does:\n",
    "- Estimates on-disk weight size using a hard-coded map of canonical parameter counts\n",
    "  from the Qwen 2.5 model cards.\n",
    "\n",
    "What this code explicitly does NOT do (because it failed / is unreliable in our environment):\n",
    "- It does NOT locate Hugging Face cache snapshots.\n",
    "- It does NOT sum actual .safetensors/.bin files on disk.\n",
    "- It does NOT rely on snapshot_download(..., local_files_only=True), because we hit\n",
    "  LocalEntryNotFoundError / couldn't reliably resolve the cached snapshot folder.\n",
    "\n",
    "Why we do it this way:\n",
    "- For standard Hugging Face Qwen repos, the cached weights are typically stored as FP16/BF16.\n",
    "- bitsandbytes quantization (e.g., '4bit') affects runtime memory, not the canonical files on disk.\n",
    "- Given we could not reliably find the cached files, the most defensible option is:\n",
    "      disk_weight_bytes_est = canonical_params_from_docs * bytes_per_param_on_disk\n",
    "\n",
    "Important caveats:\n",
    "- This is an estimate of weights only (not tokenizer/config, not multiple snapshots).\n",
    "- If you use a pre-quantized repo (GPTQ/AWQ), this estimate may overstate disk usage.\n",
    "\"\"\"\n",
    "\n",
    "# Canonical parameter counts from Qwen 2.5 docs (model cards)\n",
    "QWEN25_CANONICAL_PARAMS = {\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\":  3_090_000_000,   # 3.09B\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\":  7_610_000_000,   # 7.61B\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\": 32_500_000_000,  # 32.5B\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\": 72_700_000_000,  # 72.7B\n",
    "}\n",
    "\n",
    "def fmt_bytes(n: int) -> str:\n",
    "    f = float(n)\n",
    "    for u in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n",
    "        if f < 1024:\n",
    "            return f\"{f:.2f} {u}\"\n",
    "        f /= 1024\n",
    "    return f\"{f:.2f} EB\"\n",
    "\n",
    "# Standard HF Qwen repos store weights on disk as FP16/BF16\n",
    "BYTES_PER_PARAM_ON_DISK = 2  # set to 4 only if you know the repo stores FP32 weights\n",
    "\n",
    "canonical_n_params = QWEN25_CANONICAL_PARAMS[model_id]\n",
    "disk_bytes_est = canonical_n_params * BYTES_PER_PARAM_ON_DISK\n",
    "\n",
    "print(\"Model:\", model_id)\n",
    "print(\"Canonical params (from docs):\", f\"{canonical_n_params:,}\")\n",
    "print(\"Bytes per param on disk (assumed):\", BYTES_PER_PARAM_ON_DISK)\n",
    "print(\"Estimated disk size (weights only):\", fmt_bytes(disk_bytes_est))\n",
    "\n",
    "run_meta[\"canonical_n_params\"] = canonical_n_params\n",
    "run_meta[\"disk_bytes_est\"] = disk_bytes_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c55b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check for `call_qwen_chat`\n",
    "test_system_prompt = \"You are a concise assistant who answers simple questions clearly.\"\n",
    "test_user_prompt = \"What is 2 + 2? Answer in one short sentence.\"\n",
    "print(call_chat(tok, gen_pipe, test_system_prompt, test_user_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef95fc",
   "metadata": {},
   "source": [
    "### 8. Build prompts for answers and explanations\n",
    "\n",
    "We keep the prompts **very explicit**:\n",
    "\n",
    "- The first call asks Qwen to return JSON with:\n",
    "  - `answer` (short text),\n",
    "  - `answer_value` (normalized scalar or category),\n",
    "  - `ref_id` (comma‑separated doc ids, e.g. `\"jegham2025\"`),\n",
    "  - `supporting_material` (short quote or paraphrase).\n",
    "\n",
    "- The second call asks Qwen to generate a **single sentence explanation** (<= 100 characters).\n",
    "  We will prepend an evidence type tag (e.g. `[text]` or `[text+table]`) in code rather than\n",
    "  asking the model to output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f676b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_prompt(retrieved_chunks):\n",
    "    \"\"\"Format retrieved chunks so the LLM can see where text came from.\"\"\"\n",
    "    blocks = []\n",
    "    for r in retrieved_chunks:\n",
    "        header = f\"[DOC {r['doc_id']} | page {r['page_num']} | score {r['score']:.3f}]\"\n",
    "        blocks.append(header + \"\\n\" + r[\"text\"])\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "explanation_system_prompt = (\n",
    "    \"You are helping annotate how an answer is supported by a research paper. \"\n",
    "    \"You will see a question, an answer, and the supporting text used. \"\n",
    "    \"Your job is to (1) choose the MAIN type of evidence and \"\n",
    "    \"(2) give a VERY short explanation (<= 100 characters). \"\n",
    "    \"Valid evidence types are: text, figure, table, text+figure, table+figure, etc. \"\n",
    "    \"Respond in the strict format: evidence_type: explanation\"\n",
    ")\n",
    "\n",
    "def build_explanation_prompt(question, answer, supporting_materials, ref_id_list):\n",
    "    ref_str = \", \".join(ref_id_list) if ref_id_list else \"unknown\"\n",
    "    return f\"\"\"Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Supporting materials:\n",
    "{supporting_materials}\n",
    "\n",
    "Cited document ids: {ref_str}\n",
    "\n",
    "Remember:\n",
    "- evidence_type in [text, figure, table, text+figure, table+figure, etc.]\n",
    "- explanation <= 100 characters\n",
    "- Format: evidence_type: explanation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdc25a",
   "metadata": {},
   "source": [
    "### 9. Run over the full WattBot training set\n",
    "\n",
    "Now we:\n",
    "\n",
    "1. Iterate over **all** questions in `train_QA.csv`.\n",
    "2. Retrieve the top-\\(k\\) chunks for each question.\n",
    "3. Ask Qwen for an answer proposal (JSON).\n",
    "4. Derive:\n",
    "   - `answer` and `answer_value` from the JSON,\n",
    "   - `answer_unit` **copied directly from the ground truth** (never guessed),\n",
    "   - `ref_id` from the JSON,\n",
    "   - `ref_url` by mapping `ref_id` to `metadata.csv`,\n",
    "   - `supporting_material` from the JSON,\n",
    "   - `evidence_type` from the supporting text,\n",
    "   - `explanation` via a second Qwen call, prefixed with `[evidence_type]`.\n",
    "5. Save `wattbot_solutions.csv` in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb20b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from decimal import Decimal\n",
    "\n",
    "def normalize_answer_value(raw_answer_value, answer_text, answer_unit, is_blank):\n",
    "    \"\"\"\n",
    "    Normalize answer_value into the conventions used by train_QA:\n",
    "      - 'is_blank' for unanswerable questions\n",
    "      - plain numeric strings without units, commas, or scientific notation\n",
    "      - booleans as 1/0\n",
    "      - categorical strings (e.g., 'ML.ENERGY Benchmark') unchanged\n",
    "      - ranges like '[0.02,0.1]' preserved as-is\n",
    "    \"\"\"\n",
    "    s = str(raw_answer_value).strip()\n",
    "    if is_blank:\n",
    "        return \"is_blank\"\n",
    "    if not s or s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # Preserve ranges like [0.02,0.1]\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    lower = s.lower()\n",
    "\n",
    "    # Booleans -> 1/0\n",
    "    if lower in {\"true\", \"false\"}:\n",
    "        return \"1\" if lower == \"true\" else \"0\"\n",
    "\n",
    "    # Pure categorical (no digits) -> leave as-is\n",
    "    if not any(ch.isdigit() for ch in s):\n",
    "        return s\n",
    "\n",
    "    # Try to extract the first numeric token from either the raw string or the answer text\n",
    "    txt_candidates = [s, str(answer_text)]\n",
    "    match = None\n",
    "    for txt in txt_candidates:\n",
    "        if not txt:\n",
    "            continue\n",
    "        match = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", str(txt).replace(\",\", \"\"))\n",
    "        if match:\n",
    "            break\n",
    "\n",
    "    if not match:\n",
    "        # Fallback: strip obvious formatting characters\n",
    "        cleaned = s.replace(\",\", \"\").replace(\"%\", \"\").strip()\n",
    "        return cleaned or \"is_blank\"\n",
    "\n",
    "    num_str = match.group(0)\n",
    "\n",
    "    # Format without scientific notation, trim trailing zeros\n",
    "    try:\n",
    "        d = Decimal(num_str)\n",
    "        normalized = format(d.normalize(), \"f\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            f = float(num_str)\n",
    "            normalized = (\"%.15f\" % f).rstrip(\"0\").rstrip(\".\")\n",
    "        except Exception:\n",
    "            normalized = num_str\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19194c62",
   "metadata": {},
   "source": [
    "### Running the full RAG pipeline for one question\n",
    "\n",
    "At this point we have all the building blocks we need:\n",
    "\n",
    "- an **embedder** to turn questions into vectors  \n",
    "- a **retriever** (`retrieve_top_k`) to grab the most relevant text chunks  \n",
    "- a **chat helper** (`call_qwen_chat`) to talk to Qwen and get JSON back  \n",
    "- a small post-processing helper (`normalize_answer_value`) to clean numbers\n",
    "\n",
    "In the next few cells we tie these pieces together. We keep the code split into\n",
    "small helper functions so learners can follow each step:\n",
    "\n",
    "1. Retrieve context for a question.  \n",
    "2. Ask the LLM for an answer, references, and a quote.  \n",
    "3. Clean and normalize the structured fields (answer_value, ref_id, is_blank).  \n",
    "4. Ask a second LLM call for a short explanation and evidence type.\n",
    "\n",
    "\n",
    "### 🔍 Retrieving Relevant Context\n",
    "This function embeds the question and fetches the top‐K most relevant text chunks. It’s the first step of the RAG pipeline and determines what evidence the LLM can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa3831ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup from document id -> URL using metadata\n",
    "docid_to_url = {\n",
    "    str(row[\"id\"]).strip(): row[\"url\"]\n",
    "    for _, row in metadata_df.iterrows()\n",
    "    if isinstance(row.get(\"url\", None), str)\n",
    "}\n",
    "\n",
    "def retrieve_context_for_question(question, embedder, chunk_embeddings, chunked_docs, top_k: int = 8):\n",
    "    \"\"\"Embed the question and retrieve the top-k most similar chunks.\"\"\"\n",
    "    q_emb = embedder.encode(\n",
    "        [question],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )[0]\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    context = format_context_for_prompt(retrieved)\n",
    "    return retrieved, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183f0d8",
   "metadata": {},
   "source": [
    "### First LLM Step: Producing an Answer\n",
    "Here we prompt the model to:\n",
    "- Decide if the question is answerable\n",
    "- Extract a numeric/categorical answer\n",
    "- Identify supporting evidence\n",
    "- Select relevant document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dbbb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_phase_for_question(qid, question, answer_unit, context):\n",
    "    \"\"\"\n",
    "    First LLM call: ask Qwen for an answer, answer_value, is_blank, ref_ids,\n",
    "    and a short supporting quote. Then normalize these fields.\n",
    "    \"\"\"\n",
    "    # System prompt: what role Qwen should play\n",
    "    system_prompt_answer = (\n",
    "        \"You answer questions about AI energy, carbon, and water from academic papers.\\n\"\n",
    "        \"You are given:\\n\"\n",
    "        \"- a question\\n\"\n",
    "        \"- retrieved text chunks from the relevant paper(s)\\n\\n\"\n",
    "        \"You must:\\n\"\n",
    "        \"1. Decide if the question can be answered from the provided context.\\n\"\n",
    "        \"2. If answerable, extract a concise numeric or short-text answer_value.\\n\"\n",
    "        \"3. Use the provided answer_unit EXACTLY as given (do NOT invent units).\\n\"\n",
    "        \"4. Select one or more document ids as ref_id from the supplied chunks.\\n\"\n",
    "        \"5. Copy a short supporting quote (<= 300 chars) into supporting_materials.\\n\"\n",
    "        \"6. If the context is insufficient, mark is_blank = true and set all\\n\"\n",
    "        \"   other fields to 'is_blank' except answer_unit (keep it as provided).\\n\"\n",
    "        \"Return a JSON object with fields:\\n\"\n",
    "        \"  answer (string)\\n\"\n",
    "        \"  answer_value (string)\\n\"\n",
    "        \"  is_blank (true or false)\\n\"\n",
    "        \"  ref_id (list of doc_id strings)\\n\"\n",
    "        \"  supporting_materials (string)\\n\"\n",
    "    )\n",
    "\n",
    "    context_block = context if context.strip() else \"[NO CONTEXT FOUND]\"\n",
    "\n",
    "    # User prompt: question + unit hint + retrieved context\n",
    "    user_prompt_answer = f\"\"\"Question: {question}\n",
    "Expected answer unit: {answer_unit}\n",
    "\n",
    "Retrieved context:\n",
    "{context_block}\n",
    "\n",
    "Return JSON ONLY with keys:\n",
    "  answer (string)\n",
    "  answer_value (string)\n",
    "  is_blank (true or false)\n",
    "  ref_id (list of doc_id strings)\n",
    "  supporting_materials (string)\n",
    "\"\"\"\n",
    "\n",
    "    raw_answer = call_chat(tok, gen_pipe, system_prompt_answer, user_prompt_answer, max_new_tokens=384)\n",
    "\n",
    "    # Try to parse JSON from the model output\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"is_blank\": True,\n",
    "        \"ref_id\": [],\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "        candidate = json.loads(json_str)\n",
    "        if isinstance(candidate, dict):\n",
    "            parsed.update(candidate)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}: {e}\")\n",
    "        # fall back to defaults in `parsed`\n",
    "\n",
    "    # Normalize parsed fields\n",
    "    is_blank = bool(parsed.get(\"is_blank\", False))\n",
    "    ref_ids = parsed.get(\"ref_id\") or []\n",
    "    if isinstance(ref_ids, str):\n",
    "        ref_ids = [ref_ids]\n",
    "    ref_ids = [str(r).strip() for r in ref_ids if str(r).strip()]\n",
    "\n",
    "    answer = str(parsed.get(\"answer\", \"\")).strip()\n",
    "    answer_value = str(parsed.get(\"answer_value\", \"\")).strip() or \"is_blank\"\n",
    "    answer_value = normalize_answer_value(\n",
    "        raw_answer_value=answer_value,\n",
    "        answer_text=answer,\n",
    "        answer_unit=answer_unit,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "    supporting_materials = str(parsed.get(\"supporting_materials\", \"\")).strip()\n",
    "\n",
    "    # If context is empty, force is_blank behaviour but keep a useful diagnostic supporting_materials.\n",
    "    if not context.strip():\n",
    "        is_blank = True\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        supporting_materials = \"[NO CONTEXT FOUND]\"\n",
    "\n",
    "    # String formatting for ref_id / ref_url to match training style\n",
    "    if not ref_ids:\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "    else:\n",
    "        ref_id_str = str(ref_ids)\n",
    "\n",
    "        # Resolve ref_url via metadata\n",
    "        ref_url = \"is_blank\"\n",
    "        for rid in ref_ids:\n",
    "            if rid in docid_to_url:\n",
    "                ref_url = docid_to_url[rid]\n",
    "                break\n",
    "        if not ref_url:\n",
    "            ref_url = \"is_blank\"\n",
    "        ref_url_str = str([ref_url])\n",
    "\n",
    "    return answer, answer_value, is_blank, ref_ids, supporting_materials, ref_id_str, ref_url_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b80d5",
   "metadata": {},
   "source": [
    "### Second LLM Step: Explanation and Evidence Type\n",
    "Now that we have an answer, we produce a short explanation and classify the evidence type. This step matches WattBot’s expected metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df19d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_phase_for_question(question, answer, supporting_materials, ref_ids, is_blank):\n",
    "    \"\"\"\n",
    "    Second LLM call: ask Qwen to label an evidence_type and provide a short\n",
    "    explanation tying the answer back to the supporting materials.\n",
    "    \"\"\"\n",
    "    # Even when is_blank=True, we still generate an explanation describing why the\n",
    "    # supporting materials do not answer the question with confidence.\n",
    "    expl_user_prompt = build_explanation_prompt(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_id_list=ref_ids,\n",
    "    )\n",
    "    raw_expl = call_chat(\n",
    "        tok, \n",
    "        gen_pipe, \n",
    "        explanation_system_prompt,\n",
    "        expl_user_prompt,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Take the first non-empty line (we expect something like \"text: short reason\")\n",
    "    first_line = \"\"\n",
    "    for line in raw_expl.splitlines():\n",
    "        if line.strip():\n",
    "            first_line = line.strip()\n",
    "            break\n",
    "\n",
    "    if \":\" in first_line:\n",
    "        etype, expl = first_line.split(\":\", 1)\n",
    "        evidence_type = etype.strip().lower() or \"other\"\n",
    "        explanation = expl.strip()\n",
    "    else:\n",
    "        evidence_type = \"other\"\n",
    "        explanation = first_line.strip() or \"short justification\"\n",
    "\n",
    "    # Keep explanations short for the CSV\n",
    "    if len(explanation) > 100:\n",
    "        explanation = explanation[:100]\n",
    "\n",
    "    return evidence_type, explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf6d39",
   "metadata": {},
   "source": [
    "###  Orchestration: `run_single_qa`\n",
    "This high‐level function ties together retrieval, answering, normalization, and explanation into one full pass over a single question.\n",
    "\n",
    "\n",
    "\n",
    "### Handling unanswerable questions\n",
    "\n",
    "Some WattBot questions truly **cannot** be answered from the retrieved papers.  \n",
    "We use a simple hybrid rule to detect these cases:\n",
    "\n",
    "- We look at the **top retrieval score** (cosine similarity).  \n",
    "- We also use the LLM's own `is_blank` flag from the first JSON response.  \n",
    "\n",
    "If **either** of these says the evidence is too weak, we mark the question as unanswerable\n",
    "and set all relevant fields (`answer_value`, `ref_id`, `supporting_materials`) to `is_blank`.\n",
    "\n",
    "The `THRESHOLD` inside `run_single_qa` controls how strict this behaviour is:\n",
    "\n",
    "- lower values → fewer questions marked unanswerable  \n",
    "- higher values → more questions marked unanswerable  \n",
    "\n",
    "You can change `THRESHOLD` and then re-run the notebook and `Score.py` to see\n",
    "how this trade-off affects your final WattBot score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f0c459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_qa(\n",
    "    row,\n",
    "    embedder,\n",
    "    chunk_embeddings,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"Run retrieval + Qwen for a single training QA row.\n",
    "\n",
    "    This is the high-level orchestration function that calls three helpers:\n",
    "\n",
    "    1. retrieve_context_for_question  -> get relevant text chunks\n",
    "    2. answer_phase_for_question      -> generate answer from context, returning citations and supporting materials\n",
    "    3. explanation_phase_for_question -> evidence type + short explanation\n",
    "    \"\"\"\n",
    "\n",
    "    # Confidence threshold for retrieval.\n",
    "    # If the top similarity score is below this value, we treat the question\n",
    "    # as unanswerable, even if the LLM tried to produce an answer.\n",
    "    THRESHOLD = 0.1\n",
    "\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 1. Retrieval step\n",
    "    retrieved, context = retrieve_context_for_question(\n",
    "        question=question,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    # 2. Answer + refs + supporting materials (LLM's view)\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "        ref_id_str,\n",
    "        ref_url_str,\n",
    "    ) = answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        answer_unit=answer_unit,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    # Hybrid is_blank decision:\n",
    "    # - if retrieval is weak (top_score < THRESHOLD)\n",
    "    # - OR the LLM marks is_blank = true\n",
    "    # then we treat the question as unanswerable.\n",
    "    is_blank = bool(is_blank_llm) or (top_score < THRESHOLD)\n",
    "\n",
    "    if is_blank:\n",
    "        # Always keep a diagnostic trail (retrieved refs + supporting materials + explanation)\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "\n",
    "        # If the answer phase did not return usable refs/materials, fall back to retrieval output.\n",
    "        if (not ref_ids) and retrieved:\n",
    "            ref_ids = []\n",
    "            for r in retrieved:\n",
    "                rid = str(r.get(\"doc_id\", \"\")).strip()\n",
    "                if rid and rid not in ref_ids:\n",
    "                    ref_ids.append(rid)\n",
    "        if (not supporting_materials) or (supporting_materials == \"is_blank\"):\n",
    "            if retrieved:\n",
    "                supporting_materials = \"\\n\".join([f\"{r.get('doc_id','')}: {str(r.get('text','')).strip()[:300]}\" for r in retrieved[:3]])\n",
    "            else:\n",
    "                supporting_materials = \"[NO SUPPORTING MATERIALS FOUND]\"\n",
    "\n",
    "        # String formatting for ref_id / ref_url\n",
    "        if not ref_ids:\n",
    "            ref_id_str = \"[]\"\n",
    "            ref_url_str = \"[]\"\n",
    "        else:\n",
    "            ref_id_str = str(ref_ids)\n",
    "            ref_url = \"is_blank\"\n",
    "            for rid in ref_ids:\n",
    "                if rid in docid_to_url:\n",
    "                    ref_url = docid_to_url[rid]\n",
    "                    break\n",
    "            if not ref_url:\n",
    "                ref_url = \"is_blank\"\n",
    "            ref_url_str = str([ref_url])\n",
    "\n",
    "    # Always copy answer_unit from train_QA.csv (do NOT let the LLM invent it)\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 3. Explanation + evidence_type\n",
    "    evidence_type, explanation = explanation_phase_for_question(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_ids=ref_ids,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"is_blank\": \"true\" if is_blank else \"false\",\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"evidence_type\": evidence_type,\n",
    "        \"explanation\": explanation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f661a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitors ready. Note: GPU energy is best-effort; some GPUs/drivers don't expose an energy counter.\n"
     ]
    }
   ],
   "source": [
    "# --- NEW: runtime monitors (CPU RSS, GPU peak memory, GPU energy/power) ---\n",
    "import time, subprocess, threading\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    !pip -q install psutil\n",
    "    import psutil\n",
    "\n",
    "@dataclass\n",
    "class MonitorResults:\n",
    "    wall_s: float\n",
    "    cpu_rss_peak_bytes: int | None\n",
    "    gpu_peak_mem: dict\n",
    "    gpu_energy_wh: dict | None\n",
    "    gpu_power_samples: dict | None\n",
    "\n",
    "class CPURSSMonitor:\n",
    "    def __init__(self, interval_s: float = 0.25):\n",
    "        self.interval_s = interval_s\n",
    "        self._stop = threading.Event()\n",
    "        self._thr = None\n",
    "        self.peak_rss = 0\n",
    "        self._proc = psutil.Process()\n",
    "\n",
    "    def _loop(self):\n",
    "        while not self._stop.is_set():\n",
    "            try:\n",
    "                rss = self._proc.memory_info().rss\n",
    "                if rss > self.peak_rss:\n",
    "                    self.peak_rss = rss\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(self.interval_s)\n",
    "\n",
    "    def start(self):\n",
    "        self.peak_rss = 0\n",
    "        self._stop.clear()\n",
    "        self._thr = threading.Thread(target=self._loop, daemon=True)\n",
    "        self._thr.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._thr is not None:\n",
    "            self._thr.join(timeout=2.0)\n",
    "        return self.peak_rss\n",
    "\n",
    "def _nvidia_smi_query(fields: list[str]) -> str:\n",
    "    cmd = [\n",
    "        \"nvidia-smi\",\n",
    "        f\"--query-gpu={','.join(fields)}\",\n",
    "        \"--format=csv,noheader,nounits\",\n",
    "    ]\n",
    "    return subprocess.check_output(cmd, text=True).strip()\n",
    "\n",
    "class GPUPowerSampler:\n",
    "    \"\"\"Samples GPU power.draw (W) and integrates to Wh. Works even when energy counters are unavailable.\"\"\"\n",
    "    def __init__(self, interval_s: float = 0.5):\n",
    "        self.interval_s = interval_s\n",
    "        self._stop = threading.Event()\n",
    "        self._thr = None\n",
    "        self.samples = {}  # gpu_index -> list[(t, watts)]\n",
    "\n",
    "    def _loop(self):\n",
    "        # Initialize sample lists\n",
    "        n = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "        self.samples = {str(i): [] for i in range(n)}\n",
    "        t0 = time.time()\n",
    "        last_t = t0\n",
    "        while not self._stop.is_set():\n",
    "            try:\n",
    "                # one line per GPU in index order\n",
    "                out = _nvidia_smi_query([\"index\", \"power.draw\"])\n",
    "                now = time.time()\n",
    "                for line in out.splitlines():\n",
    "                    idx_s, p_s = [x.strip() for x in line.split(\",\")]\n",
    "                    self.samples.setdefault(idx_s, []).append((now, float(p_s)))\n",
    "                last_t = now\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(self.interval_s)\n",
    "\n",
    "    def start(self):\n",
    "        self._stop.clear()\n",
    "        self._thr = threading.Thread(target=self._loop, daemon=True)\n",
    "        self._thr.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._thr is not None:\n",
    "            self._thr.join(timeout=2.0)\n",
    "        # Integrate to Wh per GPU\n",
    "        energy_wh = {}\n",
    "        for idx, s in self.samples.items():\n",
    "            if len(s) < 2:\n",
    "                continue\n",
    "            e_ws = 0.0\n",
    "            for (t1, p1), (t2, p2) in zip(s[:-1], s[1:]):\n",
    "                dt = max(0.0, t2 - t1)\n",
    "                # trapezoid integration\n",
    "                e_ws += (p1 + p2) / 2.0 * dt\n",
    "            energy_wh[idx] = e_ws / 3600.0\n",
    "        return energy_wh, self.samples\n",
    "\n",
    "class NVMLTotalEnergy:\n",
    "    \"\"\"Uses NVML total_energy_consumption counter if supported (best).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ok = False\n",
    "        self._handles = []\n",
    "        self._start_mj = {}\n",
    "        try:\n",
    "            import pynvml\n",
    "            self.pynvml = pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            n = pynvml.nvmlDeviceGetCount()\n",
    "            self._handles = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(n)]\n",
    "            # Probe support\n",
    "            for i, h in enumerate(self._handles):\n",
    "                _ = pynvml.nvmlDeviceGetTotalEnergyConsumption(h)  # mJ\n",
    "            self.ok = True\n",
    "        except Exception:\n",
    "            self.ok = False\n",
    "\n",
    "    def start(self):\n",
    "        if not self.ok:\n",
    "            return\n",
    "        self._start_mj = {}\n",
    "        for i, h in enumerate(self._handles):\n",
    "            self._start_mj[str(i)] = float(self.pynvml.nvmlDeviceGetTotalEnergyConsumption(h))\n",
    "\n",
    "    def stop(self):\n",
    "        if not self.ok:\n",
    "            return None\n",
    "        out_wh = {}\n",
    "        for i, h in enumerate(self._handles):\n",
    "            end_mj = float(self.pynvml.nvmlDeviceGetTotalEnergyConsumption(h))\n",
    "            start_mj = self._start_mj.get(str(i), end_mj)\n",
    "            delta_mj = max(0.0, end_mj - start_mj)\n",
    "            out_wh[str(i)] = (delta_mj / 1000.0) / 3600.0  # mJ->J->Wh\n",
    "        return out_wh\n",
    "\n",
    "def start_run_monitors():\n",
    "    # reset GPU peak memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        for d in range(torch.cuda.device_count()):\n",
    "            torch.cuda.reset_peak_memory_stats(d)\n",
    "\n",
    "    cpu_mon = CPURSSMonitor(interval_s=0.25)\n",
    "    cpu_mon.start()\n",
    "\n",
    "    nvml_energy = NVMLTotalEnergy()\n",
    "    sampler = None\n",
    "    if nvml_energy.ok:\n",
    "        nvml_energy.start()\n",
    "    else:\n",
    "        sampler = GPUPowerSampler(interval_s=0.5)\n",
    "        sampler.start()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    return t0, cpu_mon, nvml_energy, sampler\n",
    "\n",
    "def stop_run_monitors(t0, cpu_mon, nvml_energy, sampler):\n",
    "    wall_s = time.perf_counter() - t0\n",
    "\n",
    "    # peak GPU memory (allocated + reserved) per device\n",
    "    gpu_peak = {}\n",
    "    if torch.cuda.is_available():\n",
    "        for d in range(torch.cuda.device_count()):\n",
    "            gpu_peak[str(d)] = {\n",
    "                \"max_allocated_gb\": float(torch.cuda.max_memory_allocated(d) / 1e9),\n",
    "                \"max_reserved_gb\": float(torch.cuda.max_memory_reserved(d) / 1e9),\n",
    "            }\n",
    "\n",
    "    cpu_peak = cpu_mon.stop()\n",
    "\n",
    "    if nvml_energy.ok:\n",
    "        energy_wh = nvml_energy.stop()\n",
    "        samples = None\n",
    "    else:\n",
    "        energy_wh, samples = sampler.stop() if sampler is not None else (None, None)\n",
    "\n",
    "    return MonitorResults(\n",
    "        wall_s=wall_s,\n",
    "        cpu_rss_peak_bytes=cpu_peak,\n",
    "        gpu_peak_mem=gpu_peak,\n",
    "        gpu_energy_wh=energy_wh,\n",
    "        gpu_power_samples=samples,\n",
    "    )\n",
    "\n",
    "def _maybe_fmt_bytes(n):\n",
    "    return None if n is None else _fmt_bytes(int(n))\n",
    "\n",
    "print(\"Monitors ready. Note: GPU energy is best-effort; some GPUs/drivers don't expose an energy counter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58ab10aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################\n",
      "QUESTION 1: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "EXPECTED answer_value: ML.ENERGY Benchmark\n",
      "PREDICTED answer_value: ML.ENERGY Benchmark\n",
      "EXPECTED ref_id(s): ['chung2025']\n",
      "RECOVERED ref_id(s): ['chung2025']\n",
      "EXPLANATION: Directly stated in the supporting text.\n",
      "ANSWER (raw): ML.ENERGY Benchmark\n",
      "########################################################################################################\n",
      "QUESTION 2: What were the net CO2e emissions from training the GShard-600B model?\n",
      "EXPECTED answer_value: 4.3\n",
      "PREDICTED answer_value: 4.3\n",
      "EXPECTED ref_id(s): ['patterson2021']\n",
      "RECOVERED ref_id(s): ['patterson2021']\n",
      "EXPLANATION: Table 4 shows GShard-600B's emissions as 4.3 tCO2e.\n",
      "ANSWER (raw): 4.3 tCO2e\n",
      "########################################################################################################\n",
      "QUESTION 3: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "EXPECTED answer_value: 64.7\n",
      "PREDICTED answer_value: is_blank\n",
      "EXPECTED ref_id(s): ['chen2024']\n",
      "RECOVERED ref_id(s): ['is_blank']\n",
      "EXPLANATION: TABLE II provides hardware requirements for LLaMA models, including 7B, 13B, and 65B.\n",
      "ANSWER (raw): \n",
      "########################################################################################################\n",
      "QUESTION 4: What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED answer_value: is_blank\n",
      "PREDICTED answer_value: is_blank\n",
      "EXPECTED ref_id(s): is_blank\n",
      "RECOVERED ref_id(s): ['is_blank']\n",
      "EXPLANATION: The provided text does not contain the specific information required to answer the question.\n",
      "ANSWER (raw): \n",
      "########################################################################################################\n",
      "QUESTION 5: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['wu2021b','patterson2021']\n",
      "RECOVERED ref_id(s): ['wu2021b']\n",
      "EXPLANATION: Explanation states hyperscale data centers have over 40% higher efficiency compared to traditional o\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 6: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "EXPECTED answer_value: [0.02,0.1]\n",
      "PREDICTED answer_value: 10\n",
      "EXPECTED ref_id(s): ['li2025b']\n",
      "RECOVERED ref_id(s): ['li2025b']\n",
      "EXPLANATION: Explanation given directly in the supporting text.\n",
      "ANSWER (raw): 10 - 50\n",
      "########################################################################################################\n",
      "QUESTION 7: From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?\n",
      "EXPECTED answer_value: 55\n",
      "PREDICTED answer_value: 55\n",
      "EXPECTED ref_id(s): ['schwartz2019']\n",
      "RECOVERED ref_id(s): ['schwartz2019']\n",
      "EXPLANATION: Figure 2 shows 75% of CVPR papers target accuracy, 20% target efficiency. Difference is 55%.\n",
      "ANSWER (raw): 55\n",
      "########################################################################################################\n",
      "QUESTION 8: True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): ['ebert2024']\n",
      "RECOVERED ref_id(s): ['ebert2024']\n",
      "EXPLANATION: Energy consumption data is restricted to authorities, not public, due to confidentiality clauses.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 9: What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?\n",
      "EXPECTED answer_value: 28\n",
      "PREDICTED answer_value: 28\n",
      "EXPECTED ref_id(s): ['xia2024']\n",
      "RECOVERED ref_id(s): ['xia2024']\n",
      "EXPLANATION: Explanation from text for 100GB GPU memory capacity.\n",
      "ANSWER (raw): 28\n",
      "########################################################################################################\n",
      "QUESTION 10: What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?\n",
      "EXPECTED answer_value: 2\n",
      "PREDICTED answer_value: 2\n",
      "EXPECTED ref_id(s): ['samsi2024']\n",
      "RECOVERED ref_id(s): ['samsi2024']\n",
      "EXPLANATION: The text states a 2 times increase in inference latency for LLaMA-7B on A100 vs V100.\n",
      "ANSWER (raw): 2\n",
      "########################################################################################################\n",
      "QUESTION 11: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "EXPECTED answer_value: 5439000\n",
      "PREDICTED answer_value: 16.904\n",
      "EXPECTED ref_id(s): ['li2025b']\n",
      "RECOVERED ref_id(s): ['li2025b']\n",
      "EXPLANATION: U.S. average total water for training listed in table\n",
      "ANSWER (raw): 16.904 million liters\n",
      "########################################################################################################\n",
      "QUESTION 12: True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['ebert2024']\n",
      "RECOVERED ref_id(s): ['ebert2024']\n",
      "EXPLANATION: Explanation supports applying SIAs to all AI systems, not just high-risk ones.\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 13: As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?\n",
      "EXPECTED answer_value: 0.18\n",
      "PREDICTED answer_value: 0.18\n",
      "EXPECTED ref_id(s): ['amazon2023']\n",
      "RECOVERED ref_id(s): ['amazon2023']\n",
      "EXPLANATION: Directly stated in the text as 0.18 L/kWh for AWS data centers.\n",
      "ANSWER (raw): 0.18\n",
      "########################################################################################################\n",
      "QUESTION 14: True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['khan2025']\n",
      "RECOVERED ref_id(s): ['khan2025']\n",
      "EXPLANATION: Local inference reduces environmental impact by lowering network overhead and carbon footprint.\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 15: True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['strubell2019']\n",
      "RECOVERED ref_id(s): ['strubell2019']\n",
      "EXPLANATION: Authors report training time for cost estimation, showing runtime tracking's importance.\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 16: For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?\n",
      "EXPECTED answer_value: 13.2\n",
      "PREDICTED answer_value: 13.2\n",
      "EXPECTED ref_id(s): ['chen2024']\n",
      "RECOVERED ref_id(s): ['chen2024']\n",
      "EXPLANATION: Figure 14 shows up to 13.2% latency reduction for LLaMA-65B.\n",
      "ANSWER (raw): 13.2%\n",
      "########################################################################################################\n",
      "QUESTION 17: How much does an elephant weigh?\n",
      "EXPECTED answer_value: is_blank\n",
      "PREDICTED answer_value: is_blank\n",
      "EXPECTED ref_id(s): is_blank\n",
      "RECOVERED ref_id(s): ['is_blank']\n",
      "EXPLANATION: The provided materials do not contain any information about elephant weights.\n",
      "ANSWER (raw): \n",
      "########################################################################################################\n",
      "QUESTION 18: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "EXPECTED answer_value: GPT-3\n",
      "PREDICTED answer_value: 1287\n",
      "EXPECTED ref_id(s): ['patterson2021']\n",
      "RECOVERED ref_id(s): ['patterson2021']\n",
      "EXPLANATION: GPT-3 has 175B parameters and an estimated energy consumption of 1287 MWh, the highest among the lis\n",
      "ANSWER (raw): GPT-3\n",
      "########################################################################################################\n",
      "QUESTION 19: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "EXPECTED answer_value: 14.4\n",
      "PREDICTED answer_value: 1.5\n",
      "EXPECTED ref_id(s): ['strubell2019']\n",
      "RECOVERED ref_id(s): ['strubell2019']\n",
      "EXPLANATION: Calculation based on CO2 emissions for American life and BERT training.\n",
      "ANSWER (raw): 1.5\n",
      "########################################################################################################\n",
      "QUESTION 20: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): ['patterson2021']\n",
      "RECOVERED ref_id(s): ['patterson2021 | page 7 | score 0.920', 'patterson2021 | page 8 | score 0.910']\n",
      "EXPLANATION: Figure 4 shows Evolved Transformer outperforming vanilla Transformer on WMT EN-DE task.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 21: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "EXPECTED answer_value: Financial Sentiment Analysis\n",
      "PREDICTED answer_value: is_blank\n",
      "EXPECTED ref_id(s): ['khan2025']\n",
      "RECOVERED ref_id(s): ['is_blank']\n",
      "EXPLANATION: The paper mentions a dataset of 5,842 labeled entries used for testing in the financial domain.\n",
      "ANSWER (raw): \n",
      "########################################################################################################\n",
      "QUESTION 22: True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['erben2023']\n",
      "RECOVERED ref_id(s): ['erben2023']\n",
      "EXPLANATION: Eight T4 spot instances are more cost-efficient than a DGX-2 for distributed training.\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 23: True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): luccioni2025b\n",
      "RECOVERED ref_id(s): ['luccioni2025b']\n",
      "EXPLANATION: The 2023 US Executive Order on AI did not mention greenhouse gas emissions or energy usage.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 24: True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['ebert2024']\n",
      "RECOVERED ref_id(s): ['ebert2024']\n",
      "EXPLANATION: Data centers must increase renewable energy use to 100% by 1 Jan 2027 (Sec. 11).\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 25: Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?\n",
      "EXPECTED answer_value: 2\n",
      "PREDICTED answer_value: 6\n",
      "EXPECTED ref_id(s): ['schwartz2019']\n",
      "RECOVERED ref_id(s): ['schwartz2019']\n",
      "EXPLANATION: 10% of 60 papers from ACL targeted efficiency, and all likely aimed at accuracy.\n",
      "ANSWER (raw): 6\n",
      "########################################################################################################\n",
      "QUESTION 26: According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?\n",
      "EXPECTED answer_value: 90\n",
      "PREDICTED answer_value: 90\n",
      "EXPECTED ref_id(s): ['jegham2025']\n",
      "RECOVERED ref_id(s): ['jegham2025']\n",
      "EXPLANATION: Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use\n",
      "ANSWER (raw): 90\n",
      "########################################################################################################\n",
      "QUESTION 27: True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): ['ebert2024']\n",
      "RECOVERED ref_id(s): ['ebert2024']\n",
      "EXPLANATION: AI Act mandates reporting only for development phase, not inference.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 28: True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): ['ebert2024']\n",
      "RECOVERED ref_id(s): ['ebert2024']\n",
      "EXPLANATION: The AI Act mandates reporting for the development phase, not the inference phase.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 29: True or False: New AI data centers often rely on air cooling due to high server power densities.\n",
      "EXPECTED answer_value: 0\n",
      "PREDICTED answer_value: 0\n",
      "EXPECTED ref_id(s): ['li2025b']\n",
      "RECOVERED ref_id(s): ['li2025b']\n",
      "EXPLANATION: Water-based cooling methods are more common for high-power density servers.\n",
      "ANSWER (raw): False\n",
      "########################################################################################################\n",
      "QUESTION 30: By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?\n",
      "EXPECTED answer_value: 6.7\n",
      "PREDICTED answer_value: 6.7\n",
      "EXPECTED ref_id(s): ['wu2021a']\n",
      "RECOVERED ref_id(s): ['wu2021a']\n",
      "EXPLANATION: Explanation from text describing power efficiency improvement due to caching.\n",
      "ANSWER (raw): 6.7\n",
      "########################################################################################################\n",
      "QUESTION 31: What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?\n",
      "EXPECTED answer_value: 1438\n",
      "PREDICTED answer_value: 1438\n",
      "EXPECTED ref_id(s): ['strubell2019']\n",
      "RECOVERED ref_id(s): ['strubell2019']\n",
      "EXPLANATION: Table shows CO2 emissions for BERT base model training.\n",
      "ANSWER (raw): 1438\n",
      "########################################################################################################\n",
      "QUESTION 32: According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?\n",
      "EXPECTED answer_value: [80,90]\n",
      "PREDICTED answer_value: 90\n",
      "EXPECTED ref_id(s): ['chung2025']\n",
      "RECOVERED ref_id(s): ['patterson2021']\n",
      "EXPLANATION: NVIDIA and AWS estimates cited in the paper support the 90% figure.\n",
      "ANSWER (raw): 90\n",
      "########################################################################################################\n",
      "QUESTION 33: How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?\n",
      "EXPECTED answer_value: 1.3\n",
      "PREDICTED answer_value: 6.1\n",
      "EXPECTED ref_id(s): ['dodge2022','strubell2019']\n",
      "RECOVERED ref_id(s): ['morrison2025']\n",
      "EXPLANATION: Llama 2 7B model's energy consumption equivalent to 6 years, 1 month of U.S. household electricity u\n",
      "ANSWER (raw): 6 years, 1 month\n",
      "########################################################################################################\n",
      "QUESTION 34: True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.\n",
      "EXPECTED answer_value: 1\n",
      "PREDICTED answer_value: 1\n",
      "EXPECTED ref_id(s): ['erben2023']\n",
      "RECOVERED ref_id(s): ['DOC erben2023 | page 7 | score 0.911']\n",
      "EXPLANATION: Egress cost is $4.329/h, over 90% of total VM cost ($4.804/h).\n",
      "ANSWER (raw): True\n",
      "########################################################################################################\n",
      "QUESTION 35: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "EXPECTED answer_value: 13\n",
      "PREDICTED answer_value: is_blank\n",
      "EXPECTED ref_id(s): ['shen2024']\n",
      "RECOVERED ref_id(s): ['is_blank']\n",
      "EXPLANATION: Median training time of 33 days used for unknown values.\n",
      "ANSWER (raw): \n",
      "########################################################################################################\n",
      "QUESTION 36: What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?\n",
      "EXPECTED answer_value: Water consumption\n",
      "PREDICTED answer_value: Water consumption\n",
      "EXPECTED ref_id(s): ['li2025b']\n",
      "RECOVERED ref_id(s): ['DOC li2025b']\n",
      "EXPLANATION: Definition provided directly in the text.\n",
      "ANSWER (raw): Water consumption\n",
      "########################################################################################################\n",
      "QUESTION 37: What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?\n",
      "EXPECTED answer_value: [300,1000]\n",
      "PREDICTED answer_value: 300\n",
      "EXPECTED ref_id(s): ['samsi2024']\n",
      "RECOVERED ref_id(s): ['samsi2024']\n",
      "EXPLANATION: Explanation given directly in the text.\n",
      "ANSWER (raw): 300 to 1000\n",
      "########################################################################################################\n",
      "QUESTION 38: When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?\n",
      "EXPECTED answer_value: 8.720430108\n",
      "PREDICTED answer_value: 7.02\n",
      "EXPECTED ref_id(s): ['zschache2025']\n",
      "RECOVERED ref_id(s): ['luccioni2024']\n",
      "EXPLANATION: Energy consumption values for BLOOMz-7B and BLOOMz-11B provided in Table 3.\n",
      "ANSWER (raw): The 72B version consumed approximately 7.02 times more energy than the 7B version.\n",
      "########################################################################################################\n",
      "QUESTION 39: By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?\n",
      "EXPECTED answer_value: 55.6\n",
      "PREDICTED answer_value: 45\n",
      "EXPECTED ref_id(s): ['khan2025']\n",
      "RECOVERED ref_id(s): ['khan2025']\n",
      "EXPLANATION: Table III shows up to 45% reduction in carbon emissions after optimization.\n",
      "ANSWER (raw): 45%\n",
      "########################################################################################################\n",
      "QUESTION 40: How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?\n",
      "EXPECTED answer_value: 40\n",
      "PREDICTED answer_value: 40\n",
      "EXPECTED ref_id(s): ['chung2025']\n",
      "RECOVERED ref_id(s): ['chung2025']\n",
      "EXPLANATION: 40 widely used model architectures across 6 different tasks were measured.\n",
      "ANSWER (raw): 40\n",
      "########################################################################################################\n",
      "QUESTION 41: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "EXPECTED answer_value: 2510000\n",
      "PREDICTED answer_value: 2500000\n",
      "EXPECTED ref_id(s): ['han2024']\n",
      "RECOVERED ref_id(s): ['han2024']\n",
      "EXPLANATION: Explanation given directly in the text for Iowa's health cost.\n",
      "ANSWER (raw): 2.5 million\n",
      "Generation time (total): 542.72 seconds\n",
      "Questions/minute: 4.5327\n",
      "Saved solutions for 41 questions to: ./data/train_solutions_NVIDIA2_4bit_Qwen__Qwen2.5-72B-Instruct.csv\n",
      "Saved run metrics to: ./data/train_solutions_NVIDIA2_4bit_Qwen__Qwen2.5-72B-Instruct_run_metrics.json\n",
      "Number of questions with errors (filled as blank): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>is_blank</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>evidence_type</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>We present the ML.ENERGY Benchmark, a benchmar...</td>\n",
       "      <td>text</td>\n",
       "      <td>Directly stated in the supporting text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3 tCO2e</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>false</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>GShard-600B’s emissions (Table 4) are 4.3 tCO2...</td>\n",
       "      <td>table</td>\n",
       "      <td>Table 4 shows GShard-600B's emissions as 4.3 t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>GB</td>\n",
       "      <td>true</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>samsi2024: generation\\nlength of 256. The 7B m...</td>\n",
       "      <td>table</td>\n",
       "      <td>TABLE II provides hardware requirements for LL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>MWh</td>\n",
       "      <td>true</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>patterson2021: 46&nbsp;&nbsp; for&nbsp;&nbsp; TPU&nbsp;&nbsp; v2,&nbsp;&nbsp; 123&nbsp;&nbsp; fo...</td>\n",
       "      <td>text</td>\n",
       "      <td>The provided text does not contain the specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['wu2021b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2108.06738']</td>\n",
       "      <td>Furthermore, between traditional and highly op...</td>\n",
       "      <td>text</td>\n",
       "      <td>Explanation states hyperscale data centers hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q078</td>\n",
       "      <td>For every medium-length GPT-3 completion (prom...</td>\n",
       "      <td>10 - 50</td>\n",
       "      <td>10</td>\n",
       "      <td>500 mL bottles</td>\n",
       "      <td>false</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>GPT-3 needs to 'drink' (i.e., consume) a 500ml...</td>\n",
       "      <td>text</td>\n",
       "      <td>Explanation given directly in the supporting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q091</td>\n",
       "      <td>From a sample of 60 papers from top AI confere...</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>percent</td>\n",
       "      <td>false</td>\n",
       "      <td>['schwartz2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1907.10597']</td>\n",
       "      <td>As shown in Figure 2, in all conferences we co...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Figure 2 shows 75% of CVPR papers target accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q102</td>\n",
       "      <td>True or False: The AI Act makes energy consump...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Where the Act does mandate the disclosure of e...</td>\n",
       "      <td>text</td>\n",
       "      <td>Energy consumption data is restricted to autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q105</td>\n",
       "      <td>What is the projected maximum batch size (in s...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>samples</td>\n",
       "      <td>false</td>\n",
       "      <td>['xia2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2408.04693']</td>\n",
       "      <td>For GPU memory capacities of 100GB and 120GB, ...</td>\n",
       "      <td>text</td>\n",
       "      <td>Explanation from text for 100GB GPU memory cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q106</td>\n",
       "      <td>What was the approximate speedup in inference ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>multiplier</td>\n",
       "      <td>false</td>\n",
       "      <td>['samsi2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2310.03003']</td>\n",
       "      <td>we see anywhere from a 2 times (7B) to a 1.25 ...</td>\n",
       "      <td>text</td>\n",
       "      <td>The text states a 2 times increase in inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q124</td>\n",
       "      <td>What is the estimated total operational water ...</td>\n",
       "      <td>16.904 million liters</td>\n",
       "      <td>16.904</td>\n",
       "      <td>liters</td>\n",
       "      <td>false</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>U.S. Average: Total Water for Training: 16.904...</td>\n",
       "      <td>table</td>\n",
       "      <td>U.S. average total water for training listed i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q135</td>\n",
       "      <td>True or False: The authors propose that sustai...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Importantly, these assessments should not be l...</td>\n",
       "      <td>text</td>\n",
       "      <td>Explanation supports applying SIAs to all AI s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q139</td>\n",
       "      <td>As of 2023, what was the water use effectivene...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>L/kWh</td>\n",
       "      <td>false</td>\n",
       "      <td>['amazon2023']</td>\n",
       "      <td>['https://sustainability.aboutamazon.com/2023-...</td>\n",
       "      <td>0.18 liters of water per kilowatt-hour (L/kWh)...</td>\n",
       "      <td>text</td>\n",
       "      <td>Directly stated in the text as 0.18 L/kWh for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q146</td>\n",
       "      <td>True or False: Local inference was emphasized ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['khan2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2504.06307']</td>\n",
       "      <td>To address these concerns, this study proposes...</td>\n",
       "      <td>text</td>\n",
       "      <td>Local inference reduces environmental impact b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q153</td>\n",
       "      <td>True or False: Tracking the runtime of a train...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['strubell2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>The authors report training time and use it to...</td>\n",
       "      <td>text</td>\n",
       "      <td>Authors report training time for cost estimati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q158</td>\n",
       "      <td>For the LLaMA-65B model, what was the maximum ...</td>\n",
       "      <td>13.2%</td>\n",
       "      <td>13.2</td>\n",
       "      <td>percent</td>\n",
       "      <td>false</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>As illustrated in Figure 14, the LLaMA-65B mod...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Figure 14 shows up to 13.2% latency reduction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q164</td>\n",
       "      <td>How much does an elephant weigh?</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>lbs</td>\n",
       "      <td>true</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>morrison2025: .16 53.1 0.238 100.58 4.83 bil.\\...</td>\n",
       "      <td>none</td>\n",
       "      <td>The provided materials do not contain any info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q166</td>\n",
       "      <td>Which of the following five large NLP DNNs has...</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>1287</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>GPT-3 is an autoregressive language model with...</td>\n",
       "      <td>text</td>\n",
       "      <td>GPT-3 has 175B parameters and an estimated ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>q170</td>\n",
       "      <td>How many days of CO₂ emissions from an average...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>days</td>\n",
       "      <td>false</td>\n",
       "      <td>['strubell2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>One year of an average American life emits 36,...</td>\n",
       "      <td>text</td>\n",
       "      <td>Calculation based on CO2 emissions for America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>q200</td>\n",
       "      <td>True or False: The Transformer architecture ev...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['patterson2021 | page 7 | score 0.920', 'patt...</td>\n",
       "      <td>['is_blank']</td>\n",
       "      <td>Figure 4 shows that the Evolved Transformer, f...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Figure 4 shows Evolved Transformer outperformi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0   q003  What is the name of the benchmark suite presen...   \n",
       "1   q009  What were the net CO2e emissions from training...   \n",
       "2   q054  What is the model size in gigabytes (GB) for t...   \n",
       "3   q062  What was the total electricity consumption of ...   \n",
       "4   q075  True or False: Hyperscale data centers in 2020...   \n",
       "5   q078  For every medium-length GPT-3 completion (prom...   \n",
       "6   q091  From a sample of 60 papers from top AI confere...   \n",
       "7   q102  True or False: The AI Act makes energy consump...   \n",
       "8   q105  What is the projected maximum batch size (in s...   \n",
       "9   q106  What was the approximate speedup in inference ...   \n",
       "10  q124  What is the estimated total operational water ...   \n",
       "11  q135  True or False: The authors propose that sustai...   \n",
       "12  q139  As of 2023, what was the water use effectivene...   \n",
       "13  q146  True or False: Local inference was emphasized ...   \n",
       "14  q153  True or False: Tracking the runtime of a train...   \n",
       "15  q158  For the LLaMA-65B model, what was the maximum ...   \n",
       "16  q164                   How much does an elephant weigh?   \n",
       "17  q166  Which of the following five large NLP DNNs has...   \n",
       "18  q170  How many days of CO₂ emissions from an average...   \n",
       "19  q200  True or False: The Transformer architecture ev...   \n",
       "\n",
       "                   answer         answer_value     answer_unit is_blank  \\\n",
       "0     ML.ENERGY Benchmark  ML.ENERGY Benchmark        is_blank    false   \n",
       "1               4.3 tCO2e                  4.3           tCO2e    false   \n",
       "2                                     is_blank              GB     true   \n",
       "3                                     is_blank             MWh     true   \n",
       "4                    True                    1        is_blank    false   \n",
       "5                 10 - 50                   10  500 mL bottles    false   \n",
       "6                      55                   55         percent    false   \n",
       "7                   False                    0        is_blank    false   \n",
       "8                      28                   28         samples    false   \n",
       "9                       2                    2      multiplier    false   \n",
       "10  16.904 million liters               16.904          liters    false   \n",
       "11                   True                    1        is_blank    false   \n",
       "12                   0.18                 0.18           L/kWh    false   \n",
       "13                   True                    1        is_blank    false   \n",
       "14                   True                    1        is_blank    false   \n",
       "15                  13.2%                 13.2         percent    false   \n",
       "16                                    is_blank             lbs     true   \n",
       "17                  GPT-3                 1287        is_blank    false   \n",
       "18                    1.5                  1.5            days    false   \n",
       "19                  False                    0        is_blank    false   \n",
       "\n",
       "                                               ref_id  \\\n",
       "0                                       ['chung2025']   \n",
       "1                                   ['patterson2021']   \n",
       "2                                        ['is_blank']   \n",
       "3                                        ['is_blank']   \n",
       "4                                         ['wu2021b']   \n",
       "5                                         ['li2025b']   \n",
       "6                                    ['schwartz2019']   \n",
       "7                                       ['ebert2024']   \n",
       "8                                         ['xia2024']   \n",
       "9                                       ['samsi2024']   \n",
       "10                                        ['li2025b']   \n",
       "11                                      ['ebert2024']   \n",
       "12                                     ['amazon2023']   \n",
       "13                                       ['khan2025']   \n",
       "14                                   ['strubell2019']   \n",
       "15                                       ['chen2024']   \n",
       "16                                       ['is_blank']   \n",
       "17                                  ['patterson2021']   \n",
       "18                                   ['strubell2019']   \n",
       "19  ['patterson2021 | page 7 | score 0.920', 'patt...   \n",
       "\n",
       "                                              ref_url  \\\n",
       "0                ['https://arxiv.org/pdf/2505.06371']   \n",
       "1                ['https://arxiv.org/pdf/2104.10350']   \n",
       "2                                        ['is_blank']   \n",
       "3                                        ['is_blank']   \n",
       "4                ['https://arxiv.org/pdf/2108.06738']   \n",
       "5                ['https://arxiv.org/pdf/2304.03271']   \n",
       "6                ['https://arxiv.org/pdf/1907.10597']   \n",
       "7                ['https://arxiv.org/pdf/2410.06681']   \n",
       "8                ['https://arxiv.org/pdf/2408.04693']   \n",
       "9                ['https://arxiv.org/pdf/2310.03003']   \n",
       "10               ['https://arxiv.org/pdf/2304.03271']   \n",
       "11               ['https://arxiv.org/pdf/2410.06681']   \n",
       "12  ['https://sustainability.aboutamazon.com/2023-...   \n",
       "13               ['https://arxiv.org/pdf/2504.06307']   \n",
       "14               ['https://arxiv.org/pdf/1906.02243']   \n",
       "15               ['https://arxiv.org/pdf/2405.01814']   \n",
       "16                                       ['is_blank']   \n",
       "17               ['https://arxiv.org/pdf/2104.10350']   \n",
       "18               ['https://arxiv.org/pdf/1906.02243']   \n",
       "19                                       ['is_blank']   \n",
       "\n",
       "                                 supporting_materials evidence_type  \\\n",
       "0   We present the ML.ENERGY Benchmark, a benchmar...          text   \n",
       "1   GShard-600B’s emissions (Table 4) are 4.3 tCO2...         table   \n",
       "2   samsi2024: generation\\nlength of 256. The 7B m...         table   \n",
       "3   patterson2021: 46   for   TPU   v2,   123   fo...          text   \n",
       "4   Furthermore, between traditional and highly op...          text   \n",
       "5   GPT-3 needs to 'drink' (i.e., consume) a 500ml...          text   \n",
       "6   As shown in Figure 2, in all conferences we co...        figure   \n",
       "7   Where the Act does mandate the disclosure of e...          text   \n",
       "8   For GPU memory capacities of 100GB and 120GB, ...          text   \n",
       "9   we see anywhere from a 2 times (7B) to a 1.25 ...          text   \n",
       "10  U.S. Average: Total Water for Training: 16.904...         table   \n",
       "11  Importantly, these assessments should not be l...          text   \n",
       "12  0.18 liters of water per kilowatt-hour (L/kWh)...          text   \n",
       "13  To address these concerns, this study proposes...          text   \n",
       "14  The authors report training time and use it to...          text   \n",
       "15  As illustrated in Figure 14, the LLaMA-65B mod...        figure   \n",
       "16  morrison2025: .16 53.1 0.238 100.58 4.83 bil.\\...          none   \n",
       "17  GPT-3 is an autoregressive language model with...          text   \n",
       "18  One year of an average American life emits 36,...          text   \n",
       "19  Figure 4 shows that the Evolved Transformer, f...        figure   \n",
       "\n",
       "                                          explanation  \n",
       "0             Directly stated in the supporting text.  \n",
       "1   Table 4 shows GShard-600B's emissions as 4.3 t...  \n",
       "2   TABLE II provides hardware requirements for LL...  \n",
       "3   The provided text does not contain the specifi...  \n",
       "4   Explanation states hyperscale data centers hav...  \n",
       "5   Explanation given directly in the supporting t...  \n",
       "6   Figure 2 shows 75% of CVPR papers target accur...  \n",
       "7   Energy consumption data is restricted to autho...  \n",
       "8   Explanation from text for 100GB GPU memory cap...  \n",
       "9   The text states a 2 times increase in inferenc...  \n",
       "10  U.S. average total water for training listed i...  \n",
       "11  Explanation supports applying SIAs to all AI s...  \n",
       "12  Directly stated in the text as 0.18 L/kWh for ...  \n",
       "13  Local inference reduces environmental impact b...  \n",
       "14  Authors report training time for cost estimati...  \n",
       "15  Figure 14 shows up to 13.2% latency reduction ...  \n",
       "16  The provided materials do not contain any info...  \n",
       "17  GPT-3 has 175B parameters and an estimated ene...  \n",
       "18  Calculation based on CO2 emissions for America...  \n",
       "19  Figure 4 shows Evolved Transformer outperformi...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run over max_N training questions (this can take a while!)\n",
    "# -------------------------------------------------------------------\n",
    "all_results = []\n",
    "error_log = []\n",
    "max_N = np.inf\n",
    "\n",
    "# Start monitors for this full RAG run (wall time, CPU RSS peak, GPU peak mem, GPU energy)\n",
    "t0, cpu_mon, nvml_energy, power_sampler = start_run_monitors()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    if idx >= max_N:\n",
    "        break\n",
    "\n",
    "    qnum = idx + 1\n",
    "    question = row[\"question\"]\n",
    "    expected_value = row.get(\"answer_value\", \"\")\n",
    "    expected_refids = row.get(\"ref_id\", \"\")\n",
    "\n",
    "    print(f\"########################################################################################################\")\n",
    "    print(f\"QUESTION {qnum}: {question}\")\n",
    "\n",
    "    res = run_single_qa(\n",
    "        row=row,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=8,\n",
    "    )\n",
    "\n",
    "    answer = res[\"answer\"]\n",
    "    answer_value = res[\"answer_value\"]\n",
    "    ref_ids = res[\"ref_id\"]\n",
    "    explanation = res[\"explanation\"]\n",
    "    print(f\"EXPECTED answer_value: {expected_value}\")\n",
    "    print(f\"PREDICTED answer_value: {answer_value}\")\n",
    "    print(f\"EXPECTED ref_id(s): {expected_refids}\")\n",
    "    print(f\"RECOVERED ref_id(s): {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "    print(f\"ANSWER (raw): {answer}\")\n",
    "\n",
    "    all_results.append(res)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "mon = stop_run_monitors(t0, cpu_mon, nvml_energy, power_sampler)\n",
    "# --- determine energy method used ---\n",
    "if mon.gpu_energy_wh is None:\n",
    "    gpu_energy_method = \"none\"\n",
    "elif mon.gpu_power_samples is None:\n",
    "    gpu_energy_method = \"nvml_total_energy\"\n",
    "else:\n",
    "    gpu_energy_method = \"power_sampling\"\n",
    "\n",
    "# --- save to metadata ---\n",
    "run_meta[\"gpu_energy_wh\"] = mon.gpu_energy_wh\n",
    "run_meta[\"gpu_energy_method\"] = gpu_energy_method\n",
    "# Save for later metrics export\n",
    "timing_generation_s = float(mon.wall_s)\n",
    "print(f\"Generation time (total): {timing_generation_s:.2f} seconds\")\n",
    "print(f\"Questions/minute: {len(all_results) / (timing_generation_s/60):.4f}\")\n",
    "\n",
    "solutions_df = pd.DataFrame(all_results)\n",
    "\n",
    "_model_tag = str(model_id).replace(\"/\", \"__\")\n",
    "\n",
    "solutions_path = os.path.join(local_data_dir, f\"train_solutions_NVIDIA{torch.cuda.device_count()}_{quant}_{_model_tag}.csv\")\n",
    "solutions_df.to_csv(solutions_path, index=False)\n",
    "\n",
    "print(f\"Saved solutions for {len(solutions_df)} questions to: {solutions_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Save run metadata + timing summary (for cross-machine comparisons)\n",
    "# -------------------------------------------------------------------\n",
    "import json, sys, platform\n",
    "import transformers, sentence_transformers\n",
    "\n",
    "timings_summary = {\n",
    "    \"timing_model_load_s\": float(globals().get(\"timing_model_load_s\", float(\"nan\"))),\n",
    "    \"timing_embedding_s\": float(globals().get(\"timing_embedding_s\", float(\"nan\"))),\n",
    "    \"timing_generation_s\": float(globals().get(\"timing_generation_s\", float(\"nan\"))),\n",
    "    \"num_chunks\": int(len(chunked_docs)),\n",
    "    \"num_questions\": int(len(all_results)),\n",
    "    \"chunks_per_s\": float(len(chunked_docs) / globals().get(\"timing_embedding_s\", float(\"nan\"))),\n",
    "    \"questions_per_s\": float(len(all_results) / globals().get(\"timing_generation_s\", float(\"nan\"))) if globals().get(\"timing_generation_s\", 0) else None,\n",
    "\n",
    "    # --- NEW: footprint + energy ---\n",
    "    \"cpu_rss_peak_bytes\": int(mon.cpu_rss_peak_bytes) if mon.cpu_rss_peak_bytes is not None else None,\n",
    "    \"cpu_rss_peak_human\": _maybe_fmt_bytes(mon.cpu_rss_peak_bytes),\n",
    "    \"gpu_peak_mem\": mon.gpu_peak_mem,                 # per GPU: max_allocated_gb / max_reserved_gb\n",
    "    \"gpu_energy_wh\": mon.gpu_energy_wh,               # per GPU (best-effort)\n",
    "}\n",
    "\n",
    "run_metadata = {\n",
    "    **globals().get(\"run_meta\", {}),\n",
    "    \"embedding_model_id\": globals().get(\"embedding_model_id\", None),\n",
    "    \"embedding_batch_size\": int(globals().get(\"embedding_batch_size\", 128)),\n",
    "    \"top_k\": 8,\n",
    "    \"answer_max_new_tokens\": 384,\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"torch_cuda\": torch.version.cuda,\n",
    "    \"cuda_capability\": str(torch.cuda.get_device_capability(0)) if torch.cuda.is_available() else None,\n",
    "    \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    \"transformers\": transformers.__version__,\n",
    "    \"sentence_transformers\": sentence_transformers.__version__,\n",
    "}\n",
    "\n",
    "metrics_out = {\n",
    "    \"run_metadata\": run_metadata,\n",
    "    \"timings\": timings_summary,\n",
    "}\n",
    "\n",
    "timings_path = solutions_path.replace(\".csv\", \"_run_metrics.json\")\n",
    "with open(timings_path, \"w\") as f:\n",
    "    json.dump(metrics_out, f, indent=2)\n",
    "\n",
    "print(f\"Saved run metrics to: {timings_path}\")\n",
    "print(f\"Number of questions with errors (filled as blank): {len(error_log)}\")\n",
    "\n",
    "solutions_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0708a502-018d-4a94-8ba2-950366be7a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen__Qwen2.5-72B-Instruct'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8f71bee-1588-42e8-bae6-04920465c453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'Qwen/Qwen2.5-72B-Instruct',\n",
       " 'quant': '4bit',\n",
       " 'dtype': 'bf16',\n",
       " 'device_map': 'balanced',\n",
       " 'force_gpu': False,\n",
       " 'gpu_mem_after_load': {'0': {'allocated_gb': 26.983323648,\n",
       "   'reserved_gb': 50.832867328},\n",
       "  '1': {'allocated_gb': 15.64194048, 'reserved_gb': 25.541214208}},\n",
       " 'canonical_n_params': 72700000000,\n",
       " 'disk_bytes_est': 145400000000,\n",
       " 'gpu_energy_wh': {'0': 39.95516601621872, '1': 28.312784113056484},\n",
       " 'gpu_energy_method': 'power_sampling',\n",
       " 'embedding_model_id': 'thenlper/gte-large',\n",
       " 'embedding_batch_size': 128,\n",
       " 'top_k': 8,\n",
       " 'answer_max_new_tokens': 384,\n",
       " 'python': '3.12.3',\n",
       " 'platform': 'Linux-5.15.0-164-generic-x86_64-with-glibc2.39',\n",
       " 'torch': '2.9.1+cu130',\n",
       " 'torch_cuda': '13.0',\n",
       " 'cuda_capability': '(12, 0)',\n",
       " 'gpu_name': 'NVIDIA RTX PRO 6000 Blackwell Server Edition',\n",
       " 'transformers': '4.57.6',\n",
       " 'sentence_transformers': '5.2.0'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66ca5d",
   "metadata": {},
   "source": [
    "### Compare answers to ground truth\n",
    "\n",
    "WattBot evaluates each prediction using a weighted score that combines three components. Most of the credit (0.75) comes from the `answer_value`, which must match the ground truth after normalization (numeric answers must be within ±0.1% relative tolerance; categorical values must match exactly). An additional 0.15 comes from `ref_id`, where partial credit is given based on the Jaccard overlap between your cited document IDs and the ground-truth set. The final 0.10 comes from correctly marking unanswerable questions: if a question is truly unanswerable, you must set `answer_value`, `ref_id`, and `supporting_materials` to `is_blank`. Any other combination scores zero for this component.\n",
    "\n",
    "| Component      | Weight | What counts as correct |\n",
    "|----------------|--------|------------------------|\n",
    "| answer_value   | 0.75   | Numeric within ±0.1% relative tolerance; categorical exact match; `is_blank` if unanswerable |\n",
    "| ref_id         | 0.15   | Jaccard overlap with the ground-truth reference set (case-insensitive) |\n",
    "| is_NA          | 0.10   | If the truly unanswerable questions, how many were correctly identified as unanswerable? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ec7e5ea-9c92-4ec9-a141-4c01bd8d2ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen2.5-72B-Instruct'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac0322e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []  # will be NaN for non-NA ground truth rows\n",
    "\n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "\n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "\n",
    "        # 1) answer_value component (unchanged)\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "\n",
    "        # 2) ref_id Jaccard (unchanged)\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "\n",
    "        # 3) NA component: only score on GT-unanswerable rows\n",
    "        #    \"Out of the unanswerables, did you successfully NOT answer?\"\n",
    "        if gt_is_na:\n",
    "            na_scores.append(1.0 if pred_is_na else 0.0)\n",
    "        else:\n",
    "            na_scores.append(np.nan)\n",
    "\n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores  # NaN for answerable rows\n",
    "\n",
    "    # NA recall over ONLY the GT-unanswerable subset\n",
    "    na_recall = merged[\"is_NA_score\"].mean()  # mean ignores NaN by default in pandas\n",
    "\n",
    "    # Overall score uses NA recall (global), not per-row matching\n",
    "    overall_score = (\n",
    "        0.75 * merged[\"answer_score\"].mean()\n",
    "        + 0.15 * merged[\"ref_id_score\"].mean()\n",
    "        + 0.10 * (0.0 if pd.isna(na_recall) else na_recall)\n",
    "    )\n",
    "\n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"NA recall (GT NA only):  {(0.0 if pd.isna(na_recall) else na_recall):.4f}\")\n",
    "    print(f\"Overall WattBot score:   {overall_score:.4f}\")\n",
    "\n",
    "    # If you still want a per-row wattbot_score column for debugging,\n",
    "    # you can broadcast NA recall to all rows:\n",
    "    merged[\"wattbot_score\"] = overall_score\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd6e1ec1-0f22-488b-9f54-b170ed4bda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 41\n",
      "Mean answer_value score: 0.6585\n",
      "Mean ref_id score:       0.6829\n",
      "NA recall (GT NA only):  1.0000\n",
      "Overall WattBot score:   0.6963\n",
      "\n",
      "Examples of incorrect / partially correct responses (up to 10 rows):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "id: q054\n",
      "Question: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "GT answer_value:   64.7\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['chen2024']\n",
      "Pred ref_id:       ['is_blank']\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q062\n",
      "Question: What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?\n",
      "GT answer_value:   is_blank\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         is_blank\n",
      "Pred ref_id:       ['is_blank']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: 1.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q075\n",
      "Question: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "GT answer_value:   1\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['wu2021b','patterson2021']\n",
      "Pred ref_id:       ['wu2021b']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q078\n",
      "Question: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "GT answer_value:   [0.02,0.1]\n",
      "Pred answer_value: 10\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       ['li2025b']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q124\n",
      "Question: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "GT answer_value:   5439000\n",
      "Pred answer_value: 16.904\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       ['li2025b']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q164\n",
      "Question: How much does an elephant weigh?\n",
      "GT answer_value:   is_blank\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         is_blank\n",
      "Pred ref_id:       ['is_blank']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: 1.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q166\n",
      "Question: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "GT answer_value:   GPT-3\n",
      "Pred answer_value: 1287\n",
      "GT ref_id:         ['patterson2021']\n",
      "Pred ref_id:       ['patterson2021']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q170\n",
      "Question: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "GT answer_value:   14.4\n",
      "Pred answer_value: 1.5\n",
      "GT ref_id:         ['strubell2019']\n",
      "Pred ref_id:       ['strubell2019']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q200\n",
      "Question: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "GT answer_value:   0\n",
      "Pred answer_value: 0\n",
      "GT ref_id:         ['patterson2021']\n",
      "Pred ref_id:       ['patterson2021 | page 7 | score 0.920', 'patterson2021 | page 8 | score 0.910']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: nan\n",
      "--------------------------------------------------------------------------------\n",
      "id: q202\n",
      "Question: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "GT answer_value:   Financial Sentiment Analysis\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['khan2025']\n",
      "Pred ref_id:       ['is_blank']\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=solutions_path,\n",
    "    gt_is_na_col=\"is_blank\",   # or \"is_blank\" / None depending on how you mark NAs\n",
    "    n_examples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf38cc",
   "metadata": {},
   "source": [
    "## Recap and next steps\n",
    "\n",
    "In this episode, we:\n",
    "\n",
    "- Loaded a small corpus of AI / ML energy papers into our notebook environment.\n",
    "- Split long documents into manageable chunks and cached those chunks to disk so we don’t have to re-run the chunking step every time.\n",
    "- Created vector embeddings for each chunk and used similarity search to retrieve relevant context for a given question.\n",
    "- Used an LLM to generate answers from retrieved context and wrote results out to a CSV for later scoring and analysis.\n",
    "- Handled unanswerable questions with an `is_blank` flag so the system can explicitly say “I don’t know” when the evidence isn’t there.\n",
    "\n",
    "This is just a first pass at a RAG pipeline: it works, but there’s a lot of headroom to improve both accuracy and robustness. Some natural next steps:\n",
    "\n",
    "- **Increase the size/quality of models used for embedding and generation**: Try stronger embedding models (e.g., larger sentence-transformers or domain-tuned embeddings) and more capable LLMs for answer generation, especially if you have GPU budget.\n",
    "\n",
    "- **Add a reranking step**: Instead of sending the top-k raw nearest neighbors directly to the LLM, use a cross-encoder or reranker model to re-score those candidates and send only the best ones.\n",
    "\n",
    "- **Handle figures and tables more carefully**: Many key numbers live in tables, figure captions, or plots. Consider:\n",
    "  - OCR / table-parsing tools (e.g., `pytesseract`, table extractors, PDF parsers).\n",
    "  - Multimodal models that can embed or interpret figures and diagrams, not just text.\n",
    "  - Separate chunking strategies for captions, tables, and main text.\n",
    "\n",
    "- **Enrich chunks with metadata**: Attach metadata like section headings (e.g., *Methods*, *Results*), paper ID, year, or paragraph type. You can:\n",
    "  - Filter or boost chunks by metadata at retrieval time.\n",
    "  - Use metadata in the prompt so the LLM knows where evidence is coming from.\n",
    "\n",
    "- **Look for LLMs tuned for scientific literature**: Experiment with models that are explicitly trained or finetuned on scientific text (e.g., arXiv / PubMed) so they:\n",
    "  - Parse equations and technical language more reliably.\n",
    "  - Are less likely to hallucinate when reading dense scientific prose.\n",
    "\n",
    "As you iterate, the goal is to treat this notebook as a baseline RAG “workbench”: you can swap in better models, smarter retrieval strategies, and richer document preprocessing without changing the overall pipeline structure.\n",
    "\n",
    "In the next episodes, we will repeat largely the same exact RAG pipeline using slightly different approaches on AWS (processing jobs and Bedrock).\n",
    "\n",
    "\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- **Notebook setup**: Start by provisioning a GPU-backed notebook instance\n",
    "  (e.g., `ml.g5.xlarge`) so that both the embedding model and Qwen2.5-7B\n",
    "  can run comfortably.\n",
    "- **Local-first RAG**: For teaching (and small corpora), we avoid an external vector database\n",
    "  and instead perform cosine similarity search over in-memory embeddings.\n",
    "- **Ground-truth units**: The `answer_unit` column is always copied directly\n",
    "  from `train_QA.csv`, never guessed by the LLM.\n",
    "- **Two-stage LLM use**: One call focuses on *answering and citing*; a second,\n",
    "  lighter call produces a short explanation tagged with an evidence type.\n",
    "- **WattBot conventions**: We respect the Kaggle competition format,\n",
    "  using `is_blank` for unanswerable questions and for missing fields.\n",
    "- **Scalability path**: The same logic can later be swapped to FAISS/Chroma\n",
    "  and larger models, while preserving the interface used here.\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26c88aec-d2b7-4fa1-add8-3954260749b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu130\n",
      "13.0\n",
      "PyTorch version: 2.9.1+cu130\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 13.0\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 24.04.1 LTS (x86_64)\n",
      "GCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.31.0\n",
      "Libc version: glibc-2.39\n",
      "\n",
      "Python version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64-bit runtime)\n",
      "Python platform: Linux-5.15.0-164-generic-x86_64-with-glibc2.39\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.6.85\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "GPU 1: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "\n",
      "Nvidia driver version: 580.82.07\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.1\n",
      "Is XPU available: False\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                            x86_64\n",
      "CPU op-mode(s):                          32-bit, 64-bit\n",
      "Address sizes:                           52 bits physical, 57 bits virtual\n",
      "Byte Order:                              Little Endian\n",
      "CPU(s):                                  128\n",
      "On-line CPU(s) list:                     0-127\n",
      "Vendor ID:                               AuthenticAMD\n",
      "Model name:                              AMD EPYC 9555 64-Core Processor\n",
      "CPU family:                              26\n",
      "Model:                                   2\n",
      "Thread(s) per core:                      1\n",
      "Core(s) per socket:                      64\n",
      "Socket(s):                               2\n",
      "Stepping:                                1\n",
      "BogoMIPS:                                6390.71\n",
      "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx_vnni avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect movdiri movdir64b overflow_recov succor smca avx512_vp2intersect flush_l1d\n",
      "Virtualization:                          AMD-V\n",
      "L1d cache:                               6 MiB (128 instances)\n",
      "L1i cache:                               4 MiB (128 instances)\n",
      "L2 cache:                                128 MiB (128 instances)\n",
      "L3 cache:                                512 MiB (16 instances)\n",
      "NUMA node(s):                            8\n",
      "NUMA node0 CPU(s):                       0-15\n",
      "NUMA node1 CPU(s):                       16-31\n",
      "NUMA node2 CPU(s):                       32-47\n",
      "NUMA node3 CPU(s):                       48-63\n",
      "NUMA node4 CPU(s):                       64-79\n",
      "NUMA node5 CPU(s):                       80-95\n",
      "NUMA node6 CPU(s):                       96-111\n",
      "NUMA node7 CPU(s):                       112-127\n",
      "Vulnerability Gather data sampling:      Not affected\n",
      "Vulnerability Indirect target selection: Not affected\n",
      "Vulnerability Itlb multihit:             Not affected\n",
      "Vulnerability L1tf:                      Not affected\n",
      "Vulnerability Mds:                       Not affected\n",
      "Vulnerability Meltdown:                  Not affected\n",
      "Vulnerability Mmio stale data:           Not affected\n",
      "Vulnerability Reg file data sampling:    Not affected\n",
      "Vulnerability Retbleed:                  Not affected\n",
      "Vulnerability Spec rstack overflow:      Not affected\n",
      "Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
      "Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; PBRSB-eIBRS Not affected; BHI Not affected\n",
      "Vulnerability Srbds:                     Not affected\n",
      "Vulnerability Tsa:                       Not affected\n",
      "Vulnerability Tsx async abort:           Not affected\n",
      "Vulnerability Vmscape:                   Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] Could not collect\n",
      "[conda] Could not collect\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.utils.collect_env.get_pretty_env_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d87b3b-99b1-483c-9875-010f4efb3982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ffe1836-02bf-4001-b48f-8c96596c7af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wattbot",
   "language": "python",
   "name": "wattbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
