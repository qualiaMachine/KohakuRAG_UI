# Sample environment variables for KohakuRAG_UI
# Copy this file to .env and fill in your values
#
# This project supports multiple LLM providers:
#   - bedrock:    AWS Bedrock API (Claude, Llama, Nova, Mistral, DeepSeek)
#   - hf_local:   Local HuggingFace Transformers (no network required)
#   - openrouter:  OpenRouter API (300+ models)
#   - openai:     OpenAI API
#
# Provider selection is done per-config file in vendor/KohakuRAG/configs/

# =============================================================================
# AWS Bedrock Configuration
# =============================================================================

# Option 1: AWS SSO profile (recommended)
# After running `aws configure sso`, set your profile name here
# AWS_PROFILE=your-sso-profile-name

# Option 2: Direct credentials (not recommended for shared environments)
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_SESSION_TOKEN=your-session-token

# AWS Region (default: us-east-2, which has most Bedrock models)
AWS_REGION=us-east-2

# =============================================================================
# OpenRouter Configuration (optional — for OpenRouter-based experiments)
# =============================================================================

# OPENROUTER_API_KEY=your-openrouter-key

# =============================================================================
# Jina Embeddings (required for indexing and retrieval)
# =============================================================================

JINA_API_KEY=your-jina-key

# =============================================================================
# HuggingFace Local Settings (for fully offline inference)
# =============================================================================

# HuggingFace local LLM model
# HF_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
# HF_DTYPE=bf16

# Local embedding model (sentence-transformers)
# EMBED_MODEL_ID=BAAI/bge-base-en-v1.5

# Cache locations (optional — control where models are downloaded)
# HF_HOME=/path/to/cache
# TRANSFORMERS_CACHE=/path/to/cache

# =============================================================================
# Streamlit App Settings
# =============================================================================

# LLM Provider: "bedrock", "openrouter", "openai", or "hf_local"
LLM_PROVIDER=bedrock

# STREAMLIT_SERVER_PORT=8501
