Research Cyberinfrastructure Exploration: WattBot (RAG) User Interface
Overview
This project builds a long-running chatbot/RAG app using the WattBot corpus and Q&A set—a limited set of energy and sustainability research papers—where the main goal is to provide accurate answers with clear citations about the environmental impacts of AI.
This project develops a long-running chatbot and RAG application using the WattBot corpus and Q&A set to answer questions about the environmental impacts of AI, emphasizing accuracy and transparent citation of energy and sustainability research.
This project will use KohakuRAG (#1 solution from the 2025 WattBot Challenge), largely as-is to perform hierarchical retrieval (document → section → paragraph → sentence) and mixture-of-expert generation. We will expose the pipeline through a minimal Streamlit interface so collaborators can query the corpus without writing code (gated access to control costs at first).
The same workload is used to compare deployment options:
on-demand AWS setup using Bedrock for LLM access, and
self-hosted/on-prem options (e.g., GB10 and NVIDIA GPU cluster) 
The result is a small, understandable reference system that supports real questions about sustainable AI and infrastructure trade-offs, while also serving as a concrete example for running and evaluating long-running RAG applications.
Expanded Goals
Create a lightweight user interface (Streamlit) that allows non-technical collaborators to interact with the WattBot RAG system without writing code.
Demonstrate an on-demand, pay-for-use hosted RAG architecture on AWS, avoiding always-on model services and minimizing idle cost.
Evaluate managed LLM usage via AWS Bedrock for planning and answer generation, including cost, latency, governance, and operational trade-offs.
Compare the AWS-hosted deployment against a self-hosted/on-prem alternative (e.g., GB10 or campus GPU cluster) using the same workload and evaluation criteria.
Compare customized RAG solution to out-of-the-box RAG solutions offered by Google, AWS, and Microsoft. The WattBot dataset includes > 200 questions and answers so we can evaluate performance easily. 
Use this project as a reference deployment and learning artifact that can be reused in future RCI consultations, workshops, and guidance around long-running GenAI/RAG applications.
Enable the RAG system itself to support internal decision-making by answering questions about sustainable AI practices, infrastructure trade-offs, and policy considerations based on published literature.
Keep the system:
access-controlled (so costs don’t explode when using APIs for generation)
low-cost at idle
understandable and extensible for collaborators
explicit about assumptions and limitations
Guiding principles
Reuse first: core RAG logic comes directly from KohakuRAG
Thin glue, not thick systems: new code focuses on wiring and deployment
Cheap at idle: expensive components run only on demand
Infrastructure is part of the research question
What already exists (upstream)
KohakuRAG already provides the RAG engine:
Hierarchical document parsing
Tree-structured indexing down to sentence level
Sentence and parent-level embeddings
Single-file SQLite vector store
Multi-query planning, deduplication, and reranking
Evidence-grounded answer generation
Retry and rate-limit handling
What this project adds
Streamlit-based chat interface
Private access for a limited user group
AWS-hosted reference deployment
On-prem GPU comparison using GB10 hardware
High-level architecture
At a bird’s-eye level:
Indexing happens offline using KohakuRAG and produces a versioned SQLite artifact (placed in S3 bucket)
A long-running Streamlit app provides the chat-style user interface
On-demand RAG execution wraps the KohakuRAG pipeline
Managed foundation models are called at request time via AWS Bedrock
This design avoids always-on model hosting and makes managed cloud services explicit in the pipeline.
Core system components
Streamlit application: user interaction and result rendering
Authentication: private access at the application layer
RAG runtime: thin wrapper around KohakuRAG
RAG index: reused upstream logic, loaded read-only
Embeddings: Jina v3 baseline
LLM usage: query planning and answer synthesis via Bedrock 

Links
AWS Access (SageMaker, Bedrock, S3, etc.) https://uw-madison-dlt3.awsapps.com/start/#/console?account_id=183295408236&role_name=ml-bedrock-183295408236sagemaker 
Development repo: https://github.com/matteso1/KohakuRAG_UI/tree/main 
References
KohakuRAG code: https://github.com/KohakuBlueleaf/KohakuRAG/tree/main 
KohakuRAG paper: https://drive.google.com/file/d/16cuDubYSbolzZyhu8UqXxvg7jLJUycjS/view?usp=sharing 
Meeting notes: WattBot_KohakuRAG_UI

Ongoing Meeting Notes
1/13/2026 - Week 1 
Intros
TODO
Read over paper covering KohakuRAG solution: https://drive.google.com/file/d/16cuDubYSbolzZyhu8UqXxvg7jLJUycjS/view?usp=sharing 
Explore code repo: https://github.com/KohakuBlueleaf/KohakuRAG/tree/main 
Local branch (Blaise)
Get a local Chatbot interface working in streamlit
Use as much of the KohakuRAG code as possible. 
Substitute API calls with local LLM calls. Use a single local model (< 1B) in the ensemble.
Set ensemble to use just 1 model
Bedrock branch (Nils)
Begin planning conversion to Bedrock. Use as much of the KohakuRAG code as possible. Substitute OpenRouter calls with Bedrock calls
Diagram/sketch workflow
Chris: Give both access to Bedrock
How should the UI look?
Something similar to chatGPT interface, but we don’t necessarily need all of those features
Chat history is good
Provide references and explanations
Resources
Building genAI apps with streamlit
Ignore the snowflake stuff (not using): https://www.coursera.org/learn/fast-prototyping-of-genai-apps-with-streamlit#modules 
Intro to SageMaker and Bedrock
https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/index.html 

1/20/2026 - Week 2
Check-in with Blaise - local development branch 
Blaise will attempt to get local code working this weekend
Chris
Write some instructions for accessing GB10 remotely
Nils
Make progress towards AWS development branch

Gated Access Chatbot (with Streamlit as UI, Bedrock powering LLMs)
Launch one EC2 instance
 This is the single server that will run the app.


Attach an IAM role to the EC2 instance
 The role allows the server (not users) to call Bedrock and read from S3.


Run Streamlit on the EC2 instance
 Streamlit hosts the WattBot UI and runs KohakuRAG locally.


Install a small auth proxy in front of Streamlit
 This proxy sits on the same server and controls who can access Streamlit.


Configure the proxy to use UW NetID login (SSO)
 Users are redirected to normal UW login and then sent back.


Add an email allowlist to the proxy
 Only emails on the list are allowed through; everyone else is blocked.


Forward allowed traffic from the proxy to Streamlit
 If the email is allowed, the proxy passes the request to Streamlit.


Store the vector database in S3, copy it locally on startup
 Streamlit reads the SQLite DB from disk; S3 is just storage.


Streamlit calls Bedrock using the EC2 role
 All LLM calls go through the server’s identity, not the user.


Add ensemble/MoE inside the app
 Do multiple Bedrock calls in parallel and vote or route in Python.

