{"document_id": "shen2024", "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars", "text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen ∗\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo∗\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n∗Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024\n\nJetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n• Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n• Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n• Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2\n\n[Image page=2 idx=1 name=Im1.png] Size: 751x1056, Data: 195838 bytes\n\nJetMoE\nmodules. Of these, we select the top k experts. When k < N , we are using a Sparse Mixture\nof Experts (SMoE, Shazeer et al. 2017). In this JetMoE, we use a linear layer to model the\nrouter\ns = Wrtrx, (1)\ng(e | x) =\n\u001a\nsoftmax (Topk (s))i , si ∈ Topk (s)\n0, si /∈ Topk (s) (2)\nwhere Wrtr is the expert embedding matrix of shape (N, Demb), Topk is the operator that\nselect the top k logits from s. The final output of the SMoE is then given by\ny =\nN\n∑\ne=1\ng(e | x) · fe(x) (3)\nWhen g(e | x) = 0, fe(x) will not need to be evaluated, thus reducing computation cost\nduring training and inference.\nFollowing the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-\nattention and Feed-forward layers (FFD) with SMoE layer. This is different from most\nopensource MoE models (Dai et al., 2024; Xue et al., 2024), that only replace FFD layers.\n2.2 FeedFoward Expert\nEach FFD expert is a standard 2-layer MLP with hidden state size Dffd:\nfmlp (x) = Wout σ (Winx) (4)\nWhere Wout is the output projection matrix of shape(Demb, D f f d), Win in the input projection\nmatrix of shape (2D f f d, Demb ), σ is the SwiGLU activation function.\n2.3 Attention Expert\nZhang et al. (2022) propose the Mixture of Attention heads (MoA), which extends SMOEs to\nattention mechanisms. We adapt MoA for our purposes, generalizing it to allow for multiple\nheads per expert and introducing RoPE relative positioning into the attention computation.\nIn JetMoE, each attention expert e is composed of four RDemb ×Datt matrix: We\nq, Wk, Wv, We\no,\nwhere Datt = H × Dhead , H is the number of attention head inside each attention experts,\nDhead is the dimension of each attention head. Among these matrices, We\nq and We\no are\nowned by each expert, but Wk and Wv are shared across experts to improve the training\nand inference efficiency.\nGiven an input vector sequence x, we first projected it to key vectors k and value vectors v\nusing the shared key and value projection matrices:\nk = Wkx (5)\nv = Wvx (6)\nInside expert e, we project x into the query vectors qe, apply standard multi-head attention\nwith RoPE (Su et al., 2024), and project the attention output back to the input space:\nqe = We\nqx (7)\nae = MHA (qe, k, v) (8)\noe = We\noa (9)\nBy introducing the MoA, we can scale up the attention layer with more attention experts\nwhile maintaining the same amount of computation. Such that the attention layer will not\nbecome a performance bottleneck, while we scale up the MLP layers.\n3\n\nJetMoE\n2.4 Load Balancing during Pretraining\nTo avoid the SMoE repeatedly using the same module and wasting the extra capacity in\nthe other modules, it requires various load balancing losses to regulate the training of\nthe router (Shazeer et al., 2017; Fedus et al., 2021). In the training of JetMoE, we use the\nfrequency-based auxiliary loss introduced in Fedus et al. (2021)\nloss b = N\nN\n∑\ni=1\nfiPi (10)\nwhere N is the number of experts, fi is the fraction of tokens dispatched to expert i, and Pi is\nthe fraction of the router probability allocated for expert i. To improve the training stability,\nwe also use the router z-loss introduced in Zoph et al. (2022):\nloss z = 1\nB\nB\n∑\ni=1\n\nlog\nN\n∑\nj=1\nexp(xi\nj)\n!2\n(11)\nwhere B is the number of tokens, x is the logits given by router. The final training loss will\nbe the weighted sum of three losses:\nloss = loss lm + αloss b + βloss z (12)\nwhere α is the weight for load balancing loss and β is the weight for z-loss.\n3 Pretraining Datasets\n3.1 Real-world Datasets\nRefinedWebis a high-quality web dataset, which contains 5 trillion tokens extracted from\nCommonCrawl 1 using the MacroData Refinement (MDR) pipeline to improve data qual-\nity (Penedo et al., 2023). We use the 600 billion token extract of RefinedWeb publicly\navailable.\nStarCoder training data is sourced from The Stack v1.2 with code from GitHub spanning\n86 programming languages (Li et al., 2023b). The data is preprocessed through visual\ninspection, filtering, deduplication, and reweighting low-data languages. A new version of\nthe dataset has been recently released (Lozhkov et al., 2024).\nDolma is a large, open, diverse English text corpus contains 3 trillion tokens sampled from\n7 sources, including web pages from Common Crawl, code from The Stack, curated web\ndata from C4 (Raffel et al., 2020), social media conversations from Reddit, academic papers\nfrom PeS2o, public domain books from Project Gutenberg, and encyclopedic content from\nWikipedia and Wikibooks (Soldaini et al., 2024).\nThe Pile is an 825 GB open-source English text corpus for training large language mod-\nels (Gao et al., 2020). It includes 22 diverse, publicly available datasets such as Wikipedia,\nNIH exPORTER, ArXiv, Books3, BookCorpus2, OpenSubtitles, YTSubtitles, and Enron\nEmails.\n3.1.1 Miscellaneous\n◦ Proof-Pile-2 is a 55 billion token dataset of mathematical and scientific docu-\nments (Azerbayev et al., 2023). We use the algebraic-stack (11B tokens) subset in-\ncluding numerical computing, computer algebra, and formal mathematics.\n◦ OpenWebMath is a large, high-quality, open dataset containing 14.7 billion tokens of\nEnglish mathematical web text (Paster et al., 2023).\n1http://commoncrawl.org/\n4\n\nJetMoE\n◦ StackMathQA is a meticulously curated collection of 2 million mathematical questions\nand answers, sourced from various Stack Exchange sites (Zhang, 2024).\n◦ OpenAssistant is a human-generated, human-annotated assistant-style conversation\ncorpus in 35 different languages. The corpus is a product of a worldwide crowd-\nsourcing effort involving over 13,500 volunteers (LAION-AI, 2023).\n◦ xP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts and\ndatasets spanning 277 languages and 16 NLP tasks (Muennighoff et al., 2023b).\n◦ CommitPackFT is a 2GB filtered version of CommitPack to contain only high-quality\ncommit messages on public Github repos that resemble natural language instruc-\ntions (Muennighoff et al., 2023a).\n3.2 Synthetic Datasets\nOpenHermes 2.5 is a large-scale, diverse, high-quality compilation of open-source and\ncustom synthetic datasets (Teknium, 2023). It contains 1 million primarily synthetically\ngenerated instruction and chat samples, following a ShareGPT structure. The dataset is\ncompiled from sources including Airoboros 2.2 (Durbin, 2023), CamelAI domain expert\ndatasets (Li et al., 2023a), ChatBot Arena (GPT-4 Only) (Zheng et al., 2024a), Collective\nCognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol\nInstruct 70K and 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-\nLLM (Peng et al., 2023), GPTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023),\nMetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023; Mukherjee et al., 2023;\nLian et al., 2023), Platypus (Lee et al., 2024; Lightman et al., 2023; Wang et al., 2023b),\nShareGPT (GPT4-Only) (lm sys, 2023), and Unnatural Instructions GPT4 (Peng et al., 2023).\nUltraTextbooks is a comprehensive collection of high-quality synthetic and human-\nwritten textbooks (Locutusque, 2024). The composition of the dataset incorporating\nmultiple sources such as nampdn-ai/mini-peS2o, open-phi/programming books llama,\nopen-phi/textbooks, nampdn-ai/tiny-strange-textbooks, and a select high-quality web\ncollection from math-ai/AutoMathText.\nUltraChat 200k is a filtered subset of the UltraChat dataset, which consists of 1.4M dialogues\ngenerated by ChatGPT (Ding et al., 2023; Tunstall et al., 2023b). The subset was created by\nselecting a smaller portion of the data, truecasing the text to fix grammatical errors, and\nremoving dialogues where the assistant inappropriately claims to lack emotions or opinions.\n3.2.1 Miscellaneous\n◦ TemplateGSM dataset is a novel and extensive collection containing over 7 mil-\nlion grade school math problems with code solutions and natural language solu-\ntions (Zhang et al., 2024b).\n◦ Magicoder-Evol-110K and Magicoder-OSS-75K datasets are generated using the\nOSS-INSTRUCT approach, which leverages a LLM to automatically create new coding\nproblems by drawing inspiration from random code snippets collected from open\nsource projects (Wei et al., 2023).\n◦ Evol-Code Alpaca is an open-sourced implementation of Evol-Instruct adapted for\ncode instructions by streamlining, simplifying, and adding code-specific evolutionary\ninstructions (Luo et al., 2023).\n◦ Code-290k-ShareGPT is a dataset in the ShareGPT format, consisting of approximately\n290,000 sets of conversations (ajibawa 2023, 2024). Code-290k-ShareGPT is built upon\nthe existing datasets Python-Code-23k-ShareGPT and Code-74k-ShareGPT.\n5\n\nJetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\n\n\ns\nW ∗ η, s < W\nη, W < s < S\nf (s − S) ∗ η, S < s < S + D\n(13)\nwhere 0 < f (s − S) ≤ 1 is a decreasing function of s, and η is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n• Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6\n\nJetMoE\n• Phase 2 (decay learning rate): We include additional high-quality data to further\nimprove the model’s performance.\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note\nthat given the limited computing budget available, our data mixture might not be ideal.\nHowever, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.\nFigure 2: Pretraining data mixture\nCategory Dataset Percentage\nNL pretraining data\nRefinedweb 39.8%\nPile Wikipedia 6.7%\nPile StackExchange 4.8%\nPile arXiv 1.0%\nPile remaining 5.1%\nDolma peS2o 1.0%\nNL SFT data xP3x, OpenAssistant, OpenHermes 7.3%UltraChat, Oasst-octopack\nTextbook UltraTextbooks 4.8%\nCode pretraining data Starcoder Github 19.6%\nCode SFT data\nMagicoder-OSS, Magicoder-Evol\n3.8% Code-290k-ShareGPT, CommitPackFT\nEvol-Code Alpaca\nMath data Open-web-math, algebraic-stack 5.8%TemplateGSM, StackMathQA\nTable 2: Detailed data mixture for Phase 2\n5 Model Alignment\n5.1 Distilled Supervised Fine-Tuning (dSFT)\nThe dSFT process involves training a student language model for replying to user prompts,\nwith data generated by a teacher model (such as GPT-4 or Claude) (Wang et al., 2022; Taori\net al., 2023; Chiang et al., 2023; Tunstall et al., 2023b). The key steps are as follows:\n7\n\n[Image page=7 idx=1 name=Im2.png] Size: 2484x2342, Data: 278571 bytes\n\n[Image page=7 idx=2 name=Im3.png] Size: 2200x2332, Data: 237358 bytes\n\nJetMoE\n1. Data Distillation: For a set of seed prompts {x0\nj }J\nj=1, generate responses y0\nj using the\nteacher model πT, and refine instructions to obtain C = {(xj, yj)}J\nj=1.\n2. Instruction T uning: The student model πdSFT is trained by maximizing the likelihood\nof the responses given the instructions:\nπdSFT = arg max\nπ\n∑\n(x,y)∈C\nlog π(y|x). (14)\nNote that the expectation for the likelihood function is approximated by using the\narithmetic mean over a batch of training samples.\n5.2 Distilled Direct Preference Optimization (dDPO)\ndDPO refines the dSFT model by incorporating preferences from an aligned teacher model\ninto the training process. It optimizes a reward function that reflects these preferences,\naiming to align the student model’s outputs with the desired outcomes based on the static\npreference dataset.\n1. KL-Constrained Optimization: The foundation of dDPO lies in the KL-constrained\noptimization, which derives the optimal policy π∗\nr that maximizes expected rewards\nwhile minimizing divergence from a baseline policy π0 (Wang et al., 2023a):\nπ∗\nr (y|x) := arg max\nπ\nEx∼d0\nh\nEy∼π(·|x)[r(x, y)] − ηKL(π(·|x)∥π0(·|x))\ni\n(15)\nwhere η is a regularization parameter that balances maximizing the reward function\nr(x, y) and adhering to the baseline policy π0.\n2. Preference-Driven Reward Function : dDPO incorporates a reward function that\nreflects preferences from an aligned teacher model:\nr∗(x, y) = η log\n\u0012 π∗(y|x)\nπdSFT(y|x)\n\u0013\n+ η log Z(x), (16)\nquantifying the preference for producing response y given input x relative to the\ndSFT model’s baseline probability.η scales the reward’s influence, andZ(x) ensures\nnormalization.\n3. Optimization Objective : The objective for aligning πθ with the teacher model’s\npreferences is:\nπθ = arg max\nπ\n∑\n(x,yw,yl )∈D\nlog σ\n\u0012\nη log π(yw|x)\nπdSFT(yw|x) − η log π(yl|x)\nπdSFT(yl|x)\n\u0013\n, (17)\nwhere D comprises instruction-response pairs, with yw and yl indicating preferred\nand less preferred responses respectively, scored by the teacher model.\nOffline DPO (Rafailov et al., 2023) directly optimizes language model policies using static\npreference data, providing stable learning and simpler tuning compared to Reinforcement\nlearning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2023). How-\never, it faces challenges with distribution shifts between the dataset and the evolving policy.\nOnline and iterative DPO variants address this issue at the cost of increased computational\ncomplexity (Xu et al., 2023b; Guo et al., 2024b; Xiong et al., 2024).\n5.3 Alignment details\nOur alginment framework is based on Alignment Handbook (Tunstall et al., 2023a) using\nPytorch 2 (He & Yu, 2023; Ansel et al., 2024) with DeepSpeed ZeRO-3 (Rajbhandari et al.,\n2020). We finetune the JetMoE-8B base model using dSFT on a combination of the following\ndatasets: UltraChat 200k (Ding et al., 2023; Tunstall et al., 2023b), Airoboros-3.2 (Durbin,\n8\n\nJetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B’s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: ”Writing” and ”Roleplay” tasks have a temperature\nof 0.7, indicating higher creativity; ”Extraction”, ”Math”, ”Coding”, and ”Reasoning” tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9\n\nJetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and ”STEM” and ”Humanities” have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10\n\n[Image page=10 idx=1 name=Im4.png] Size: 1312x746, Data: 158582 bytes\n\nJetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432–7439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11\n\nJetMoE\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P . Xing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nURL https://lmsys.org/blog/2023-03-30-vicuna/ .\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences, 2023.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers\nto solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nCogStack. OpenGPT: A framework for creating grounded instruction based datasets and\ntraining conversational domain expert Large Language Models (LLMs). https://github.\ncom/CogStack/OpenGPT, 2023.\nCollectiveCognition. Collective cognition chatgpt conversations, 2023. URL https://\nhuggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22 .\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,\nZhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-\nquality feedback, 2023.\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi\nLi, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert\nspecialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066,\n2024.\nLuigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse\nmulti-turn conversations for effecient llm training. arXiv preprint arXiv:(coming soon), 2023.\nURL https://huggingface.co/datasets/LDJnr/Capybara.\nDatabricks. Dbrx: Resources and code examples. https://github.com/databricks/dbrx,\n2024.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,\nMaosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality\ninstructional conversations, 2023.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\nlanguage models with mixture-of-experts. In International Conference on Machine Learning,\npp. 5547–5569. PMLR, 2022.\nJon Durbin. airoboros: Customizable implementation of the self-instruct paper. https:\n//github.com/jondurbin/airoboros, 2023.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity, 2021.\n12\n\nJetMoE\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient\nsparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5,\n2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\nhttps://github.com/openlm-research/open_llama.\nglaiveai. Glaive-code-assistant, 2023. URL https://huggingface.co/datasets/glaiveai/\nglaive-code-assistant.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,\nXiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:\nWhen the large language model meets programming – the rise of code intelligence, 2024a.\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares,\nAlexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu\nBlondel. Direct language model alignment from online ai feedback, 2024b.\nHorace He and Shangdi Yu. Transcending runtime-memory tradeoffs in checkpointing by\nbeing fusion aware. Proceedings of Machine Learning and Systems, 5, 2023.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei\nFang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang,\nChongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding,\nChao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling\nthe potential of small language models with scalable training strategies, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Th ´eophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth´ee Lacroix, and William El Sayed. Mixtral of experts, 2024.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551, 2017.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural\nquestions: a benchmark for question answering research. Transactions of the Association for\nComputational Linguistics, 7:453–466, 2019.\nLAION-AI. Open-Assistant: A chat-based assistant that understands tasks, can interact\nwith third-party systems, and retrieve information dynamically. https://github.com/\nLAION-AI/Open-Assistant, 2023.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful\nrefinement of llms, 2024.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for ”mind” exploration of large language model\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.\n13\n\nJetMoE\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi\nBhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-\nnakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜noz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you!, 2023b.\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand ”Teknium”. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces,\nwith verification, 2023. URL https://https://huggingface.co/Open-Orca/SlimOrca.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step.\npreprint arXiv:2305.20050, 2023.\nlm sys. FastChat: An open platform for training, serving, and evaluating large language\nmodel based chatbots. https://github.com/lm-sys/FastChat, 2023.\nLocutusque. Ultratextbooks, 2024. URL https://huggingface.co/datasets/Locutusque/\nUltraTextbooks.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V . Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing\ndata and methods for effective instruction tuning, 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,\nNouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2\nand the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao,\nJing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language\nmodels with evol-instruct, 2023.\nArindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math:\nUnlocking the potential of slms in grade school math, 2024.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,\nSwayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack:\nInstruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023a.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-\ngru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid\nAlyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization\nthrough multitask finetuning, 2023b.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi,\nand Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of\ngpt-4, 2023.\n14\n\nJetMoE\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-\nwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, et al. Efficient large-scale language model training on gpu clusters using\nmegatron-lm. In Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pp. 1–15, 2021.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions\nwith human feedback, 2022.\nBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin\nRaffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of\nmixture-of-experts language models, 2024.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The\nlambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031, 2016.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An\nopen dataset of high-quality mathematical web text, 2023.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and\nweb data only. arXiv preprint arXiv:2306.01116, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction\ntuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\nURL http://jmlr.org/papers/v21/20-074.html.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory\noptimizations toward training trillion parameter models, 2020.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J´er´emy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis\nMartin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open\nfoundation models for code, 2024.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\n2021.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:\nCommonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n15\n\nJetMoE\nYikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.\nModuleformer: Learning modular large language models from uncurated data. arXiv\npreprint arXiv:2306.04640, 2023.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism. arXiv preprint arXiv:1909.08053, 2019.\nQingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical\nstudy of instruction-tuning large language models in chinese, 2023.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,\nAnanya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,\nJacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Pe-\nters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant\nSubramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh\nHajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus\nof three trillion tokens for language model pretraining research, 2024.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology. arXiv preprint\narXiv:2403.08295, 2024.\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.\nTeknium1. GPTeacher: A collection of modular datasets generated by GPT-4. https:\n//github.com/teknium1/GPTeacher, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang,\nKashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https:\n//github.com/huggingface/alignment-handbook, 2023a.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Cl ´ementine Fourrier, Nathan Habib,\nNathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr:\nDirect distillation of lm alignment, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nChaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl:\nGeneralizing direct preference optimization with diverse divergence constraints, 2023a.\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,\nArjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating\ncollege-level scientific problem-solving abilities of large language models, 2023b.\n16", "metadata": {"url": "https://arxiv.org/pdf/2404.07413", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen ∗\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo∗\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n∗Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024", "sentences": [{"text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen ∗\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo∗\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence.", "metadata": {}}, {"text": "This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours.", "metadata": {}}, {"text": "Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model.", "metadata": {}}, {"text": "These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought.", "metadata": {}}, {"text": "JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts.", "metadata": {}}, {"text": "Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B.", "metadata": {}}, {"text": "Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code.", "metadata": {}}, {"text": "All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models.", "metadata": {}}, {"text": "This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs.", "metadata": {}}, {"text": "The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.", "metadata": {}}, {"text": "1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.", "metadata": {}}, {"text": "Although modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible.", "metadata": {}}, {"text": "Most LLMs (e.g., Llama, Touvron et al.", "metadata": {}}, {"text": "2023;", "metadata": {}}, {"text": "Pythia, Biderman\net al.", "metadata": {}}, {"text": "2023;", "metadata": {}}, {"text": "GPT-3, Brown et al.", "metadata": {}}, {"text": "2020;", "metadata": {}}, {"text": "Mistral, Jiang et al.", "metadata": {}}, {"text": "2023) use all of their parameters\nduring inference and training, which are referred to as dense models.", "metadata": {}}, {"text": "Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012;", "metadata": {}}, {"text": "Shazeer\net al., 2017;", "metadata": {}}, {"text": "Du et al., 2022;", "metadata": {}}, {"text": "Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest.", "metadata": {}}, {"text": "Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024).", "metadata": {}}, {"text": "However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations.", "metadata": {}}, {"text": "The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).", "metadata": {}}, {"text": "∗Equal contribution.", "metadata": {}}, {"text": "1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n• Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n• Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n• Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2", "sentences": [{"text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers.", "metadata": {}}, {"text": "Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.", "metadata": {}}, {"text": "Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours.", "metadata": {}}, {"text": "Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought.", "metadata": {}}, {"text": "In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.", "metadata": {}}, {"text": "The key advantages of JetMoE-8B include:\n• Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.", "metadata": {}}, {"text": "The model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).", "metadata": {}}, {"text": "• Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs.", "metadata": {}}, {"text": "We also propose to share the kv projection in\nattention experts to improve training stability.", "metadata": {}}, {"text": "• Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.", "metadata": {}}, {"text": "These innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community.", "metadata": {}}, {"text": "To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.", "metadata": {}}, {"text": "2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "., fN and a router g(e | x).", "metadata": {}}, {"text": "Given an input x to the MoE layer, the router predicts a probability distribution over the N\n2", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.png] Size: 751x1056, Data: 195838 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.png] Size: 751x1056, Data: 195838 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.png", "image_width": 751, "image_height": 1056, "attachment_type": "image", "has_image_data": true, "image_data_size": 195838}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "JetMoE\nmodules. Of these, we select the top k experts. When k < N , we are using a Sparse Mixture\nof Experts (SMoE, Shazeer et al. 2017). In this JetMoE, we use a linear layer to model the\nrouter\ns = Wrtrx, (1)\ng(e | x) =\n\u001a\nsoftmax (Topk (s))i , si ∈ Topk (s)\n0, si /∈ Topk (s) (2)\nwhere Wrtr is the expert embedding matrix of shape (N, Demb), Topk is the operator that\nselect the top k logits from s. The final output of the SMoE is then given by\ny =\nN\n∑\ne=1\ng(e | x) · fe(x) (3)\nWhen g(e | x) = 0, fe(x) will not need to be evaluated, thus reducing computation cost\nduring training and inference.\nFollowing the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-\nattention and Feed-forward layers (FFD) with SMoE layer. This is different from most\nopensource MoE models (Dai et al., 2024; Xue et al., 2024), that only replace FFD layers.\n2.2 FeedFoward Expert\nEach FFD expert is a standard 2-layer MLP with hidden state size Dffd:\nfmlp (x) = Wout σ (Winx) (4)\nWhere Wout is the output projection matrix of shape(Demb, D f f d), Win in the input projection\nmatrix of shape (2D f f d, Demb ), σ is the SwiGLU activation function.\n2.3 Attention Expert\nZhang et al. (2022) propose the Mixture of Attention heads (MoA), which extends SMOEs to\nattention mechanisms. We adapt MoA for our purposes, generalizing it to allow for multiple\nheads per expert and introducing RoPE relative positioning into the attention computation.\nIn JetMoE, each attention expert e is composed of four RDemb ×Datt matrix: We\nq, Wk, Wv, We\no,\nwhere Datt = H × Dhead , H is the number of attention head inside each attention experts,\nDhead is the dimension of each attention head. Among these matrices, We\nq and We\no are\nowned by each expert, but Wk and Wv are shared across experts to improve the training\nand inference efficiency.\nGiven an input vector sequence x, we first projected it to key vectors k and value vectors v\nusing the shared key and value projection matrices:\nk = Wkx (5)\nv = Wvx (6)\nInside expert e, we project x into the query vectors qe, apply standard multi-head attention\nwith RoPE (Su et al., 2024), and project the attention output back to the input space:\nqe = We\nqx (7)\nae = MHA (qe, k, v) (8)\noe = We\noa (9)\nBy introducing the MoA, we can scale up the attention layer with more attention experts\nwhile maintaining the same amount of computation. Such that the attention layer will not\nbecome a performance bottleneck, while we scale up the MLP layers.\n3", "sentences": [{"text": "JetMoE\nmodules.", "metadata": {}}, {"text": "Of these, we select the top k experts.", "metadata": {}}, {"text": "When k < N , we are using a Sparse Mixture\nof Experts (SMoE, Shazeer et al.", "metadata": {}}, {"text": "2017).", "metadata": {}}, {"text": "In this JetMoE, we use a linear layer to model the\nrouter\ns = Wrtrx, (1)\ng(e | x) =\n\u001a\nsoftmax (Topk (s))i , si ∈ Topk (s)\n0, si /∈ Topk (s) (2)\nwhere Wrtr is the expert embedding matrix of shape (N, Demb), Topk is the operator that\nselect the top k logits from s.", "metadata": {}}, {"text": "The final output of the SMoE is then given by\ny =\nN\n∑\ne=1\ng(e | x) · fe(x) (3)\nWhen g(e | x) = 0, fe(x) will not need to be evaluated, thus reducing computation cost\nduring training and inference.", "metadata": {}}, {"text": "Following the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-\nattention and Feed-forward layers (FFD) with SMoE layer.", "metadata": {}}, {"text": "This is different from most\nopensource MoE models (Dai et al., 2024;", "metadata": {}}, {"text": "Xue et al., 2024), that only replace FFD layers.", "metadata": {}}, {"text": "2.2 FeedFoward Expert\nEach FFD expert is a standard 2-layer MLP with hidden state size Dffd:\nfmlp (x) = Wout σ (Winx) (4)\nWhere Wout is the output projection matrix of shape(Demb, D f f d), Win in the input projection\nmatrix of shape (2D f f d, Demb ), σ is the SwiGLU activation function.", "metadata": {}}, {"text": "2.3 Attention Expert\nZhang et al.", "metadata": {}}, {"text": "(2022) propose the Mixture of Attention heads (MoA), which extends SMOEs to\nattention mechanisms.", "metadata": {}}, {"text": "We adapt MoA for our purposes, generalizing it to allow for multiple\nheads per expert and introducing RoPE relative positioning into the attention computation.", "metadata": {}}, {"text": "In JetMoE, each attention expert e is composed of four RDemb ×Datt matrix: We\nq, Wk, Wv, We\no,\nwhere Datt = H × Dhead , H is the number of attention head inside each attention experts,\nDhead is the dimension of each attention head.", "metadata": {}}, {"text": "Among these matrices, We\nq and We\no are\nowned by each expert, but Wk and Wv are shared across experts to improve the training\nand inference efficiency.", "metadata": {}}, {"text": "Given an input vector sequence x, we first projected it to key vectors k and value vectors v\nusing the shared key and value projection matrices:\nk = Wkx (5)\nv = Wvx (6)\nInside expert e, we project x into the query vectors qe, apply standard multi-head attention\nwith RoPE (Su et al., 2024), and project the attention output back to the input space:\nqe = We\nqx (7)\nae = MHA (qe, k, v) (8)\noe = We\noa (9)\nBy introducing the MoA, we can scale up the attention layer with more attention experts\nwhile maintaining the same amount of computation.", "metadata": {}}, {"text": "Such that the attention layer will not\nbecome a performance bottleneck, while we scale up the MLP layers.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "JetMoE\n2.4 Load Balancing during Pretraining\nTo avoid the SMoE repeatedly using the same module and wasting the extra capacity in\nthe other modules, it requires various load balancing losses to regulate the training of\nthe router (Shazeer et al., 2017; Fedus et al., 2021). In the training of JetMoE, we use the\nfrequency-based auxiliary loss introduced in Fedus et al. (2021)\nloss b = N\nN\n∑\ni=1\nfiPi (10)\nwhere N is the number of experts, fi is the fraction of tokens dispatched to expert i, and Pi is\nthe fraction of the router probability allocated for expert i. To improve the training stability,\nwe also use the router z-loss introduced in Zoph et al. (2022):\nloss z = 1\nB\nB\n∑\ni=1", "sentences": [{"text": "JetMoE\n2.4 Load Balancing during Pretraining\nTo avoid the SMoE repeatedly using the same module and wasting the extra capacity in\nthe other modules, it requires various load balancing losses to regulate the training of\nthe router (Shazeer et al., 2017;", "metadata": {}}, {"text": "Fedus et al., 2021).", "metadata": {}}, {"text": "In the training of JetMoE, we use the\nfrequency-based auxiliary loss introduced in Fedus et al.", "metadata": {}}, {"text": "(2021)\nloss b = N\nN\n∑\ni=1\nfiPi (10)\nwhere N is the number of experts, fi is the fraction of tokens dispatched to expert i, and Pi is\nthe fraction of the router probability allocated for expert i.", "metadata": {}}, {"text": "To improve the training stability,\nwe also use the router z-loss introduced in Zoph et al.", "metadata": {}}, {"text": "(2022):\nloss z = 1\nB\nB\n∑\ni=1", "metadata": {}}], "metadata": {"page": 4}}, {"text": "log\nN\n∑\nj=1\nexp(xi\nj)\n!2\n(11)\nwhere B is the number of tokens, x is the logits given by router. The final training loss will\nbe the weighted sum of three losses:\nloss = loss lm + αloss b + βloss z (12)\nwhere α is the weight for load balancing loss and β is the weight for z-loss.\n3 Pretraining Datasets\n3.1 Real-world Datasets\nRefinedWebis a high-quality web dataset, which contains 5 trillion tokens extracted from\nCommonCrawl 1 using the MacroData Refinement (MDR) pipeline to improve data qual-\nity (Penedo et al., 2023). We use the 600 billion token extract of RefinedWeb publicly\navailable.\nStarCoder training data is sourced from The Stack v1.2 with code from GitHub spanning\n86 programming languages (Li et al., 2023b). The data is preprocessed through visual\ninspection, filtering, deduplication, and reweighting low-data languages. A new version of\nthe dataset has been recently released (Lozhkov et al., 2024).\nDolma is a large, open, diverse English text corpus contains 3 trillion tokens sampled from\n7 sources, including web pages from Common Crawl, code from The Stack, curated web\ndata from C4 (Raffel et al., 2020), social media conversations from Reddit, academic papers\nfrom PeS2o, public domain books from Project Gutenberg, and encyclopedic content from\nWikipedia and Wikibooks (Soldaini et al., 2024).\nThe Pile is an 825 GB open-source English text corpus for training large language mod-\nels (Gao et al., 2020). It includes 22 diverse, publicly available datasets such as Wikipedia,\nNIH exPORTER, ArXiv, Books3, BookCorpus2, OpenSubtitles, YTSubtitles, and Enron\nEmails.\n3.1.1 Miscellaneous\n◦ Proof-Pile-2 is a 55 billion token dataset of mathematical and scientific docu-\nments (Azerbayev et al., 2023). We use the algebraic-stack (11B tokens) subset in-\ncluding numerical computing, computer algebra, and formal mathematics.\n◦ OpenWebMath is a large, high-quality, open dataset containing 14.7 billion tokens of\nEnglish mathematical web text (Paster et al., 2023).\n1http://commoncrawl.org/\n4", "sentences": [{"text": "log\nN\n∑\nj=1\nexp(xi\nj)\n!2\n(11)\nwhere B is the number of tokens, x is the logits given by router.", "metadata": {}}, {"text": "The final training loss will\nbe the weighted sum of three losses:\nloss = loss lm + αloss b + βloss z (12)\nwhere α is the weight for load balancing loss and β is the weight for z-loss.", "metadata": {}}, {"text": "3 Pretraining Datasets\n3.1 Real-world Datasets\nRefinedWebis a high-quality web dataset, which contains 5 trillion tokens extracted from\nCommonCrawl 1 using the MacroData Refinement (MDR) pipeline to improve data qual-\nity (Penedo et al., 2023).", "metadata": {}}, {"text": "We use the 600 billion token extract of RefinedWeb publicly\navailable.", "metadata": {}}, {"text": "StarCoder training data is sourced from The Stack v1.2 with code from GitHub spanning\n86 programming languages (Li et al., 2023b).", "metadata": {}}, {"text": "The data is preprocessed through visual\ninspection, filtering, deduplication, and reweighting low-data languages.", "metadata": {}}, {"text": "A new version of\nthe dataset has been recently released (Lozhkov et al., 2024).", "metadata": {}}, {"text": "Dolma is a large, open, diverse English text corpus contains 3 trillion tokens sampled from\n7 sources, including web pages from Common Crawl, code from The Stack, curated web\ndata from C4 (Raffel et al., 2020), social media conversations from Reddit, academic papers\nfrom PeS2o, public domain books from Project Gutenberg, and encyclopedic content from\nWikipedia and Wikibooks (Soldaini et al., 2024).", "metadata": {}}, {"text": "The Pile is an 825 GB open-source English text corpus for training large language mod-\nels (Gao et al., 2020).", "metadata": {}}, {"text": "It includes 22 diverse, publicly available datasets such as Wikipedia,\nNIH exPORTER, ArXiv, Books3, BookCorpus2, OpenSubtitles, YTSubtitles, and Enron\nEmails.", "metadata": {}}, {"text": "3.1.1 Miscellaneous\n◦ Proof-Pile-2 is a 55 billion token dataset of mathematical and scientific docu-\nments (Azerbayev et al., 2023).", "metadata": {}}, {"text": "We use the algebraic-stack (11B tokens) subset in-\ncluding numerical computing, computer algebra, and formal mathematics.", "metadata": {}}, {"text": "◦ OpenWebMath is a large, high-quality, open dataset containing 14.7 billion tokens of\nEnglish mathematical web text (Paster et al., 2023).", "metadata": {}}, {"text": "1http://commoncrawl.org/\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "JetMoE\n◦ StackMathQA is a meticulously curated collection of 2 million mathematical questions\nand answers, sourced from various Stack Exchange sites (Zhang, 2024).\n◦ OpenAssistant is a human-generated, human-annotated assistant-style conversation\ncorpus in 35 different languages. The corpus is a product of a worldwide crowd-\nsourcing effort involving over 13,500 volunteers (LAION-AI, 2023).\n◦ xP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts and\ndatasets spanning 277 languages and 16 NLP tasks (Muennighoff et al., 2023b).\n◦ CommitPackFT is a 2GB filtered version of CommitPack to contain only high-quality\ncommit messages on public Github repos that resemble natural language instruc-\ntions (Muennighoff et al., 2023a).\n3.2 Synthetic Datasets\nOpenHermes 2.5 is a large-scale, diverse, high-quality compilation of open-source and\ncustom synthetic datasets (Teknium, 2023). It contains 1 million primarily synthetically\ngenerated instruction and chat samples, following a ShareGPT structure. The dataset is\ncompiled from sources including Airoboros 2.2 (Durbin, 2023), CamelAI domain expert\ndatasets (Li et al., 2023a), ChatBot Arena (GPT-4 Only) (Zheng et al., 2024a), Collective\nCognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol\nInstruct 70K and 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-\nLLM (Peng et al., 2023), GPTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023),\nMetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023; Mukherjee et al., 2023;\nLian et al., 2023), Platypus (Lee et al., 2024; Lightman et al., 2023; Wang et al., 2023b),\nShareGPT (GPT4-Only) (lm sys, 2023), and Unnatural Instructions GPT4 (Peng et al., 2023).\nUltraTextbooks is a comprehensive collection of high-quality synthetic and human-\nwritten textbooks (Locutusque, 2024). The composition of the dataset incorporating\nmultiple sources such as nampdn-ai/mini-peS2o, open-phi/programming books llama,\nopen-phi/textbooks, nampdn-ai/tiny-strange-textbooks, and a select high-quality web\ncollection from math-ai/AutoMathText.\nUltraChat 200k is a filtered subset of the UltraChat dataset, which consists of 1.4M dialogues\ngenerated by ChatGPT (Ding et al., 2023; Tunstall et al., 2023b). The subset was created by\nselecting a smaller portion of the data, truecasing the text to fix grammatical errors, and\nremoving dialogues where the assistant inappropriately claims to lack emotions or opinions.\n3.2.1 Miscellaneous\n◦ TemplateGSM dataset is a novel and extensive collection containing over 7 mil-\nlion grade school math problems with code solutions and natural language solu-\ntions (Zhang et al., 2024b).\n◦ Magicoder-Evol-110K and Magicoder-OSS-75K datasets are generated using the\nOSS-INSTRUCT approach, which leverages a LLM to automatically create new coding\nproblems by drawing inspiration from random code snippets collected from open\nsource projects (Wei et al., 2023).\n◦ Evol-Code Alpaca is an open-sourced implementation of Evol-Instruct adapted for\ncode instructions by streamlining, simplifying, and adding code-specific evolutionary\ninstructions (Luo et al., 2023).\n◦ Code-290k-ShareGPT is a dataset in the ShareGPT format, consisting of approximately\n290,000 sets of conversations (ajibawa 2023, 2024). Code-290k-ShareGPT is built upon\nthe existing datasets Python-Code-23k-ShareGPT and Code-74k-ShareGPT.\n5", "sentences": [{"text": "JetMoE\n◦ StackMathQA is a meticulously curated collection of 2 million mathematical questions\nand answers, sourced from various Stack Exchange sites (Zhang, 2024).", "metadata": {}}, {"text": "◦ OpenAssistant is a human-generated, human-annotated assistant-style conversation\ncorpus in 35 different languages.", "metadata": {}}, {"text": "The corpus is a product of a worldwide crowd-\nsourcing effort involving over 13,500 volunteers (LAION-AI, 2023).", "metadata": {}}, {"text": "◦ xP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts and\ndatasets spanning 277 languages and 16 NLP tasks (Muennighoff et al., 2023b).", "metadata": {}}, {"text": "◦ CommitPackFT is a 2GB filtered version of CommitPack to contain only high-quality\ncommit messages on public Github repos that resemble natural language instruc-\ntions (Muennighoff et al., 2023a).", "metadata": {}}, {"text": "3.2 Synthetic Datasets\nOpenHermes 2.5 is a large-scale, diverse, high-quality compilation of open-source and\ncustom synthetic datasets (Teknium, 2023).", "metadata": {}}, {"text": "It contains 1 million primarily synthetically\ngenerated instruction and chat samples, following a ShareGPT structure.", "metadata": {}}, {"text": "The dataset is\ncompiled from sources including Airoboros 2.2 (Durbin, 2023), CamelAI domain expert\ndatasets (Li et al., 2023a), ChatBot Arena (GPT-4 Only) (Zheng et al., 2024a), Collective\nCognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol\nInstruct 70K and 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-\nLLM (Peng et al., 2023), GPTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023),\nMetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023;", "metadata": {}}, {"text": "Mukherjee et al., 2023;", "metadata": {}}, {"text": "Lian et al., 2023), Platypus (Lee et al., 2024;", "metadata": {}}, {"text": "Lightman et al., 2023;", "metadata": {}}, {"text": "Wang et al., 2023b),\nShareGPT (GPT4-Only) (lm sys, 2023), and Unnatural Instructions GPT4 (Peng et al., 2023).", "metadata": {}}, {"text": "UltraTextbooks is a comprehensive collection of high-quality synthetic and human-\nwritten textbooks (Locutusque, 2024).", "metadata": {}}, {"text": "The composition of the dataset incorporating\nmultiple sources such as nampdn-ai/mini-peS2o, open-phi/programming books llama,\nopen-phi/textbooks, nampdn-ai/tiny-strange-textbooks, and a select high-quality web\ncollection from math-ai/AutoMathText.", "metadata": {}}, {"text": "UltraChat 200k is a filtered subset of the UltraChat dataset, which consists of 1.4M dialogues\ngenerated by ChatGPT (Ding et al., 2023;", "metadata": {}}, {"text": "Tunstall et al., 2023b).", "metadata": {}}, {"text": "The subset was created by\nselecting a smaller portion of the data, truecasing the text to fix grammatical errors, and\nremoving dialogues where the assistant inappropriately claims to lack emotions or opinions.", "metadata": {}}, {"text": "3.2.1 Miscellaneous\n◦ TemplateGSM dataset is a novel and extensive collection containing over 7 mil-\nlion grade school math problems with code solutions and natural language solu-\ntions (Zhang et al., 2024b).", "metadata": {}}, {"text": "◦ Magicoder-Evol-110K and Magicoder-OSS-75K datasets are generated using the\nOSS-INSTRUCT approach, which leverages a LLM to automatically create new coding\nproblems by drawing inspiration from random code snippets collected from open\nsource projects (Wei et al., 2023).", "metadata": {}}, {"text": "◦ Evol-Code Alpaca is an open-sourced implementation of Evol-Instruct adapted for\ncode instructions by streamlining, simplifying, and adding code-specific evolutionary\ninstructions (Luo et al., 2023).", "metadata": {}}, {"text": "◦ Code-290k-ShareGPT is a dataset in the ShareGPT format, consisting of approximately\n290,000 sets of conversations (ajibawa 2023, 2024).", "metadata": {}}, {"text": "Code-290k-ShareGPT is built upon\nthe existing datasets Python-Code-23k-ShareGPT and Code-74k-ShareGPT.", "metadata": {}}, {"text": "5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\n\n\ns\nW ∗ η, s < W\nη, W < s < S\nf (s − S) ∗ η, S < s < S + D\n(13)\nwhere 0 < f (s − S) ≤ 1 is a decreasing function of s, and η is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n• Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6", "sentences": [{"text": "JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support.", "metadata": {}}, {"text": "We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4).", "metadata": {}}, {"text": "Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training.", "metadata": {}}, {"text": "This is mainly due to two reasons.", "metadata": {}}, {"text": "First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels.", "metadata": {}}, {"text": "Thus, the communication cost for pipeline parallelism is smaller.", "metadata": {}}, {"text": "Second, we use\nthe dropless MoE schema introduced in Gale et al.", "metadata": {}}, {"text": "(2023);", "metadata": {}}, {"text": "Shen et al.", "metadata": {}}, {"text": "(2023), which could\ncause load unbalance across experts.", "metadata": {}}, {"text": "Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training.", "metadata": {}}, {"text": "Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device.", "metadata": {}}, {"text": "We con-\nduct training on a cluster containing 12 nodes and 96 H100s.", "metadata": {}}, {"text": "Inside each node, gpus are\nconnected via NVLinks.", "metadata": {}}, {"text": "Infiniband is used for fast communication between nodes.", "metadata": {}}, {"text": "4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.", "metadata": {}}, {"text": "The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model.", "metadata": {}}, {"text": "We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE.", "metadata": {}}, {"text": "Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer.", "metadata": {}}, {"text": "Such that the model has approximately two times the computation compared to a 1B\nmodel.", "metadata": {}}, {"text": "Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively.", "metadata": {}}, {"text": "Table 1 shows the key hyperparameters in JetMoE-8B.", "metadata": {}}, {"text": "JetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096.", "metadata": {}}, {"text": "We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.", "metadata": {}}, {"text": "(2024).", "metadata": {}}, {"text": "This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\n\n\ns\nW ∗ η, s < W\nη, W < s < S\nf (s − S) ∗ η, S < s < S + D\n(13)\nwhere 0 < f (s − S) ≤ 1 is a decreasing function of s, and η is the maximum learning rate.", "metadata": {}}, {"text": "In our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens.", "metadata": {}}, {"text": "The initial and final learning rates are set to 10% of the maximum learning\nrate.", "metadata": {}}, {"text": "A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.", "metadata": {}}, {"text": "4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code.", "metadata": {}}, {"text": "Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase.", "metadata": {}}, {"text": "The training process is divided into two phases:\n• Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.", "metadata": {}}, {"text": "6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "JetMoE\n• Phase 2 (decay learning rate): We include additional high-quality data to further\nimprove the model’s performance.\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note\nthat given the limited computing budget available, our data mixture might not be ideal.\nHowever, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.\nFigure 2: Pretraining data mixture\nCategory Dataset Percentage\nNL pretraining data\nRefinedweb 39.8%\nPile Wikipedia 6.7%\nPile StackExchange 4.8%\nPile arXiv 1.0%\nPile remaining 5.1%\nDolma peS2o 1.0%\nNL SFT data xP3x, OpenAssistant, OpenHermes 7.3%UltraChat, Oasst-octopack\nTextbook UltraTextbooks 4.8%\nCode pretraining data Starcoder Github 19.6%\nCode SFT data\nMagicoder-OSS, Magicoder-Evol\n3.8% Code-290k-ShareGPT, CommitPackFT\nEvol-Code Alpaca\nMath data Open-web-math, algebraic-stack 5.8%TemplateGSM, StackMathQA\nTable 2: Detailed data mixture for Phase 2\n5 Model Alignment\n5.1 Distilled Supervised Fine-Tuning (dSFT)\nThe dSFT process involves training a student language model for replying to user prompts,\nwith data generated by a teacher model (such as GPT-4 or Claude) (Wang et al., 2022; Taori\net al., 2023; Chiang et al., 2023; Tunstall et al., 2023b). The key steps are as follows:\n7", "sentences": [{"text": "JetMoE\n• Phase 2 (decay learning rate): We include additional high-quality data to further\nimprove the model’s performance.", "metadata": {}}, {"text": "The detailed data mixture can be found in Figure 2 and Table 2.", "metadata": {}}, {"text": "It is important to note\nthat given the limited computing budget available, our data mixture might not be ideal.", "metadata": {}}, {"text": "However, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.", "metadata": {}}, {"text": "Figure 2: Pretraining data mixture\nCategory Dataset Percentage\nNL pretraining data\nRefinedweb 39.8%\nPile Wikipedia 6.7%\nPile StackExchange 4.8%\nPile arXiv 1.0%\nPile remaining 5.1%\nDolma peS2o 1.0%\nNL SFT data xP3x, OpenAssistant, OpenHermes 7.3%UltraChat, Oasst-octopack\nTextbook UltraTextbooks 4.8%\nCode pretraining data Starcoder Github 19.6%\nCode SFT data\nMagicoder-OSS, Magicoder-Evol\n3.8% Code-290k-ShareGPT, CommitPackFT\nEvol-Code Alpaca\nMath data Open-web-math, algebraic-stack 5.8%TemplateGSM, StackMathQA\nTable 2: Detailed data mixture for Phase 2\n5 Model Alignment\n5.1 Distilled Supervised Fine-Tuning (dSFT)\nThe dSFT process involves training a student language model for replying to user prompts,\nwith data generated by a teacher model (such as GPT-4 or Claude) (Wang et al., 2022;", "metadata": {}}, {"text": "Taori\net al., 2023;", "metadata": {}}, {"text": "Chiang et al., 2023;", "metadata": {}}, {"text": "Tunstall et al., 2023b).", "metadata": {}}, {"text": "The key steps are as follows:\n7", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=Im2.png] Size: 2484x2342, Data: 278571 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=Im2.png] Size: 2484x2342, Data: 278571 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "Im2.png", "image_width": 2484, "image_height": 2342, "attachment_type": "image", "has_image_data": true, "image_data_size": 278571}}, {"text": "[Image page=7 idx=2 name=Im3.png] Size: 2200x2332, Data: 237358 bytes", "sentences": [{"text": "[Image page=7 idx=2 name=Im3.png] Size: 2200x2332, Data: 237358 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 2, "image_name": "Im3.png", "image_width": 2200, "image_height": 2332, "attachment_type": "image", "has_image_data": true, "image_data_size": 237358}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "JetMoE\n1. Data Distillation: For a set of seed prompts {x0\nj }J\nj=1, generate responses y0\nj using the\nteacher model πT, and refine instructions to obtain C = {(xj, yj)}J\nj=1.\n2. Instruction T uning: The student model πdSFT is trained by maximizing the likelihood\nof the responses given the instructions:\nπdSFT = arg max\nπ\n∑\n(x,y)∈C\nlog π(y|x). (14)\nNote that the expectation for the likelihood function is approximated by using the\narithmetic mean over a batch of training samples.\n5.2 Distilled Direct Preference Optimization (dDPO)\ndDPO refines the dSFT model by incorporating preferences from an aligned teacher model\ninto the training process. It optimizes a reward function that reflects these preferences,\naiming to align the student model’s outputs with the desired outcomes based on the static\npreference dataset.\n1. KL-Constrained Optimization: The foundation of dDPO lies in the KL-constrained\noptimization, which derives the optimal policy π∗\nr that maximizes expected rewards\nwhile minimizing divergence from a baseline policy π0 (Wang et al., 2023a):\nπ∗\nr (y|x) := arg max\nπ\nEx∼d0\nh\nEy∼π(·|x)[r(x, y)] − ηKL(π(·|x)∥π0(·|x))\ni\n(15)\nwhere η is a regularization parameter that balances maximizing the reward function\nr(x, y) and adhering to the baseline policy π0.\n2. Preference-Driven Reward Function : dDPO incorporates a reward function that\nreflects preferences from an aligned teacher model:\nr∗(x, y) = η log\n\u0012 π∗(y|x)\nπdSFT(y|x)\n\u0013\n+ η log Z(x), (16)\nquantifying the preference for producing response y given input x relative to the\ndSFT model’s baseline probability.η scales the reward’s influence, andZ(x) ensures\nnormalization.\n3. Optimization Objective : The objective for aligning πθ with the teacher model’s\npreferences is:\nπθ = arg max\nπ\n∑\n(x,yw,yl )∈D\nlog σ\n\u0012\nη log π(yw|x)\nπdSFT(yw|x) − η log π(yl|x)\nπdSFT(yl|x)\n\u0013\n, (17)\nwhere D comprises instruction-response pairs, with yw and yl indicating preferred\nand less preferred responses respectively, scored by the teacher model.\nOffline DPO (Rafailov et al., 2023) directly optimizes language model policies using static\npreference data, providing stable learning and simpler tuning compared to Reinforcement\nlearning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2023). How-\never, it faces challenges with distribution shifts between the dataset and the evolving policy.\nOnline and iterative DPO variants address this issue at the cost of increased computational\ncomplexity (Xu et al., 2023b; Guo et al., 2024b; Xiong et al., 2024).\n5.3 Alignment details\nOur alginment framework is based on Alignment Handbook (Tunstall et al., 2023a) using\nPytorch 2 (He & Yu, 2023; Ansel et al., 2024) with DeepSpeed ZeRO-3 (Rajbhandari et al.,\n2020). We finetune the JetMoE-8B base model using dSFT on a combination of the following\ndatasets: UltraChat 200k (Ding et al., 2023; Tunstall et al., 2023b), Airoboros-3.2 (Durbin,\n8", "sentences": [{"text": "JetMoE\n1.", "metadata": {}}, {"text": "Data Distillation: For a set of seed prompts {x0\nj }J\nj=1, generate responses y0\nj using the\nteacher model πT, and refine instructions to obtain C = {(xj, yj)}J\nj=1.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Instruction T uning: The student model πdSFT is trained by maximizing the likelihood\nof the responses given the instructions:\nπdSFT = arg max\nπ\n∑\n(x,y)∈C\nlog π(y|x).", "metadata": {}}, {"text": "(14)\nNote that the expectation for the likelihood function is approximated by using the\narithmetic mean over a batch of training samples.", "metadata": {}}, {"text": "5.2 Distilled Direct Preference Optimization (dDPO)\ndDPO refines the dSFT model by incorporating preferences from an aligned teacher model\ninto the training process.", "metadata": {}}, {"text": "It optimizes a reward function that reflects these preferences,\naiming to align the student model’s outputs with the desired outcomes based on the static\npreference dataset.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "KL-Constrained Optimization: The foundation of dDPO lies in the KL-constrained\noptimization, which derives the optimal policy π∗\nr that maximizes expected rewards\nwhile minimizing divergence from a baseline policy π0 (Wang et al., 2023a):\nπ∗\nr (y|x) := arg max\nπ\nEx∼d0\nh\nEy∼π(·|x)[r(x, y)] − ηKL(π(·|x)∥π0(·|x))\ni\n(15)\nwhere η is a regularization parameter that balances maximizing the reward function\nr(x, y) and adhering to the baseline policy π0.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Preference-Driven Reward Function : dDPO incorporates a reward function that\nreflects preferences from an aligned teacher model:\nr∗(x, y) = η log\n\u0012 π∗(y|x)\nπdSFT(y|x)\n\u0013\n+ η log Z(x), (16)\nquantifying the preference for producing response y given input x relative to the\ndSFT model’s baseline probability.η scales the reward’s influence, andZ(x) ensures\nnormalization.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Optimization Objective : The objective for aligning πθ with the teacher model’s\npreferences is:\nπθ = arg max\nπ\n∑\n(x,yw,yl )∈D\nlog σ\n\u0012\nη log π(yw|x)\nπdSFT(yw|x) − η log π(yl|x)\nπdSFT(yl|x)\n\u0013\n, (17)\nwhere D comprises instruction-response pairs, with yw and yl indicating preferred\nand less preferred responses respectively, scored by the teacher model.", "metadata": {}}, {"text": "Offline DPO (Rafailov et al., 2023) directly optimizes language model policies using static\npreference data, providing stable learning and simpler tuning compared to Reinforcement\nlearning from Human Feedback (RLHF) (Ouyang et al., 2022;", "metadata": {}}, {"text": "Christiano et al., 2023).", "metadata": {}}, {"text": "How-\never, it faces challenges with distribution shifts between the dataset and the evolving policy.", "metadata": {}}, {"text": "Online and iterative DPO variants address this issue at the cost of increased computational\ncomplexity (Xu et al., 2023b;", "metadata": {}}, {"text": "Guo et al., 2024b;", "metadata": {}}, {"text": "Xiong et al., 2024).", "metadata": {}}, {"text": "5.3 Alignment details\nOur alginment framework is based on Alignment Handbook (Tunstall et al., 2023a) using\nPytorch 2 (He & Yu, 2023;", "metadata": {}}, {"text": "Ansel et al., 2024) with DeepSpeed ZeRO-3 (Rajbhandari et al.,\n2020).", "metadata": {}}, {"text": "We finetune the JetMoE-8B base model using dSFT on a combination of the following\ndatasets: UltraChat 200k (Ding et al., 2023;", "metadata": {}}, {"text": "Tunstall et al., 2023b), Airoboros-3.2 (Durbin,\n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B’s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: ”Writing” and ”Roleplay” tasks have a temperature\nof 0.7, indicating higher creativity; ”Extraction”, ”Math”, ”Coding”, and ”Reasoning” tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9", "sentences": [{"text": "JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).", "metadata": {}}, {"text": "Chat template is the same as Zephyr-7b-beta.", "metadata": {}}, {"text": "The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.", "metadata": {}}, {"text": "We further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options.", "metadata": {}}, {"text": "The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch.", "metadata": {}}, {"text": "This fine-tuning process results in the\nJetMoE-8B-Chat model.", "metadata": {}}, {"text": "The entire alignment process takes 60 H100 GPU hours.", "metadata": {}}, {"text": "6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg.", "metadata": {}}, {"text": "51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg.", "metadata": {}}, {"text": "45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.", "metadata": {}}, {"text": "We measure JetMoE-8B’s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019;", "metadata": {}}, {"text": "Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more.", "metadata": {}}, {"text": "For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models..", "metadata": {}}, {"text": "We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.", "metadata": {}}, {"text": "In addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models.", "metadata": {}}, {"text": "Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024;", "metadata": {}}, {"text": "Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.", "metadata": {}}, {"text": "Table 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels.", "metadata": {}}, {"text": "JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.", "metadata": {}}, {"text": "Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming.", "metadata": {}}, {"text": "We also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint).", "metadata": {}}, {"text": "The temperature configuration, following the official FastChat\nimplementation, is defined as follows: ”Writing” and ”Roleplay” tasks have a temperature\nof 0.7, indicating higher creativity;", "metadata": {}}, {"text": "”Extraction”, ”Math”, ”Coding”, and ”Reasoning” tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and ”STEM” and ”Humanities” have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10", "sentences": [{"text": "JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness;", "metadata": {}}, {"text": "and ”STEM” and ”Humanities” have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.", "metadata": {}}, {"text": "JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance.", "metadata": {}}, {"text": "However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo.", "metadata": {}}, {"text": "This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks.", "metadata": {}}, {"text": "Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.", "metadata": {}}, {"text": "7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture.", "metadata": {}}, {"text": "The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023;", "metadata": {}}, {"text": "Zoph et al., 2022;", "metadata": {}}, {"text": "Hu et al., 2024).", "metadata": {}}, {"text": "In the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.", "metadata": {}}, {"text": "8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency.", "metadata": {}}, {"text": "By leveraging sparse\n10", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=Im4.png] Size: 1312x746, Data: 158582 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=Im4.png] Size: 1312x746, Data: 158582 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "Im4.png", "image_width": 1312, "image_height": 746, "attachment_type": "image", "has_image_data": true, "image_data_size": 158582}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432–7439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11", "sentences": [{"text": "JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.", "metadata": {}}, {"text": "Trained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard.", "metadata": {}}, {"text": "In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.", "metadata": {}}, {"text": "We provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work.", "metadata": {}}, {"text": "JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.", "metadata": {}}, {"text": "Acknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture.", "metadata": {}}, {"text": "We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.", "metadata": {}}, {"text": "References\nabacusai.", "metadata": {}}, {"text": "Systemchat, 2024.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/abacusai/\nSystemChat.", "metadata": {}}, {"text": "ajibawa 2023.", "metadata": {}}, {"text": "Code-290k-sharegpt, 2024.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .", "metadata": {}}, {"text": "Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al.", "metadata": {}}, {"text": "Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.", "metadata": {}}, {"text": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.", "metadata": {}}, {"text": "Program synthesis with\nlarge language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2108.07732, 2021.", "metadata": {}}, {"text": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q.", "metadata": {}}, {"text": "Jiang, Jia Deng, Stella Biderman, and Sean Welleck.", "metadata": {}}, {"text": "Llemma: An open language\nmodel for mathematics, 2023.", "metadata": {}}, {"text": "Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra.", "metadata": {}}, {"text": "A framework for the evaluation of code generation models.", "metadata": {}}, {"text": "https://github.", "metadata": {}}, {"text": "com/bigcode-project/bigcode-evaluation-harness , 2022.", "metadata": {}}, {"text": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al.", "metadata": {}}, {"text": "Pythia: A suite for analyzing large language models across training and scaling.", "metadata": {}}, {"text": "arXiv preprint arXiv:2304.01373, 2023.", "metadata": {}}, {"text": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.", "metadata": {}}, {"text": "Piqa: Reasoning about physical\ncommonsense in natural language.", "metadata": {}}, {"text": "In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp.", "metadata": {}}, {"text": "7432–7439, 2020.", "metadata": {}}, {"text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.", "metadata": {}}, {"text": "Language models are few-shot learners.", "metadata": {}}, {"text": "Advances in neural information processing systems,\n33:1877–1901, 2020.", "metadata": {}}, {"text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "JetMoE\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P . Xing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nURL https://lmsys.org/blog/2023-03-30-vicuna/ .\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences, 2023.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers\nto solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nCogStack. OpenGPT: A framework for creating grounded instruction based datasets and\ntraining conversational domain expert Large Language Models (LLMs). https://github.\ncom/CogStack/OpenGPT, 2023.\nCollectiveCognition. Collective cognition chatgpt conversations, 2023. URL https://\nhuggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22 .\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,\nZhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-\nquality feedback, 2023.\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi\nLi, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert\nspecialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066,\n2024.\nLuigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse\nmulti-turn conversations for effecient llm training. arXiv preprint arXiv:(coming soon), 2023.\nURL https://huggingface.co/datasets/LDJnr/Capybara.\nDatabricks. Dbrx: Resources and code examples. https://github.com/databricks/dbrx,\n2024.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,\nMaosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality\ninstructional conversations, 2023.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\nlanguage models with mixture-of-experts. In International Conference on Machine Learning,\npp. 5547–5569. PMLR, 2022.\nJon Durbin. airoboros: Customizable implementation of the self-instruct paper. https:\n//github.com/jondurbin/airoboros, 2023.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity, 2021.\n12", "sentences": [{"text": "JetMoE\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N.", "metadata": {}}, {"text": "Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba.", "metadata": {}}, {"text": "Evaluating large language models trained on code,\n2021.", "metadata": {}}, {"text": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.", "metadata": {}}, {"text": "Gonzalez, Ion Stoica, and Eric P .", "metadata": {}}, {"text": "Xing.", "metadata": {}}, {"text": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.", "metadata": {}}, {"text": "URL https://lmsys.org/blog/2023-03-30-vicuna/ .", "metadata": {}}, {"text": "Paul Christiano, Jan Leike, Tom B.", "metadata": {}}, {"text": "Brown, Miljan Martic, Shane Legg, and Dario Amodei.", "metadata": {}}, {"text": "Deep reinforcement learning from human preferences, 2023.", "metadata": {}}, {"text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova.", "metadata": {}}, {"text": "Boolq: Exploring the surprising difficulty of natural yes/no questions.", "metadata": {}}, {"text": "arXiv preprint arXiv:1905.10044, 2019.", "metadata": {}}, {"text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.", "metadata": {}}, {"text": "Training verifiers\nto solve math word problems.", "metadata": {}}, {"text": "arXiv preprint arXiv:2110.14168, 2021.", "metadata": {}}, {"text": "CogStack.", "metadata": {}}, {"text": "OpenGPT: A framework for creating grounded instruction based datasets and\ntraining conversational domain expert Large Language Models (LLMs).", "metadata": {}}, {"text": "https://github.", "metadata": {}}, {"text": "com/CogStack/OpenGPT, 2023.", "metadata": {}}, {"text": "CollectiveCognition.", "metadata": {}}, {"text": "Collective cognition chatgpt conversations, 2023.", "metadata": {}}, {"text": "URL https://\nhuggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22 .", "metadata": {}}, {"text": "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,\nZhiyuan Liu, and Maosong Sun.", "metadata": {}}, {"text": "Ultrafeedback: Boosting language models with high-\nquality feedback, 2023.", "metadata": {}}, {"text": "Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi\nLi, Wangding Zeng, Xingkai Yu, Y Wu, et al.", "metadata": {}}, {"text": "Deepseekmoe: Towards ultimate expert\nspecialization in mixture-of-experts language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2401.06066,\n2024.", "metadata": {}}, {"text": "Luigi Daniele and Suphavadeeprasit.", "metadata": {}}, {"text": "Amplify-instruct: Synthetically generated diverse\nmulti-turn conversations for effecient llm training.", "metadata": {}}, {"text": "arXiv preprint arXiv:(coming soon), 2023.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/LDJnr/Capybara.", "metadata": {}}, {"text": "Databricks.", "metadata": {}}, {"text": "Dbrx: Resources and code examples.", "metadata": {}}, {"text": "https://github.com/databricks/dbrx,\n2024.", "metadata": {}}, {"text": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,\nMaosong Sun, and Bowen Zhou.", "metadata": {}}, {"text": "Enhancing chat language models by scaling high-quality\ninstructional conversations, 2023.", "metadata": {}}, {"text": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al.", "metadata": {}}, {"text": "Glam: Efficient scaling of\nlanguage models with mixture-of-experts.", "metadata": {}}, {"text": "In International Conference on Machine Learning,\npp.", "metadata": {}}, {"text": "5547–5569.", "metadata": {}}, {"text": "PMLR, 2022.", "metadata": {}}, {"text": "Jon Durbin.", "metadata": {}}, {"text": "airoboros: Customizable implementation of the self-instruct paper.", "metadata": {}}, {"text": "https:\n//github.com/jondurbin/airoboros, 2023.", "metadata": {}}, {"text": "William Fedus, Barret Zoph, and Noam Shazeer.", "metadata": {}}, {"text": "Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity, 2021.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "JetMoE\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient\nsparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5,\n2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\nhttps://github.com/openlm-research/open_llama.\nglaiveai. Glaive-code-assistant, 2023. URL https://huggingface.co/datasets/glaiveai/\nglaive-code-assistant.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,\nXiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:\nWhen the large language model meets programming – the rise of code intelligence, 2024a.\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares,\nAlexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu\nBlondel. Direct language model alignment from online ai feedback, 2024b.\nHorace He and Shangdi Yu. Transcending runtime-memory tradeoffs in checkpointing by\nbeing fusion aware. Proceedings of Machine Learning and Systems, 5, 2023.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei\nFang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang,\nChongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding,\nChao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling\nthe potential of small language models with scalable training strategies, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Th ´eophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth´ee Lacroix, and William El Sayed. Mixtral of experts, 2024.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551, 2017.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural\nquestions: a benchmark for question answering research. Transactions of the Association for\nComputational Linguistics, 7:453–466, 2019.\nLAION-AI. Open-Assistant: A chat-based assistant that understands tasks, can interact\nwith third-party systems, and retrieve information dynamically. https://github.com/\nLAION-AI/Open-Assistant, 2023.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful\nrefinement of llms, 2024.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for ”mind” exploration of large language model\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.\n13", "sentences": [{"text": "JetMoE\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia.", "metadata": {}}, {"text": "Megablocks: Efficient\nsparse training with mixture-of-experts.", "metadata": {}}, {"text": "Proceedings of Machine Learning and Systems, 5,\n2023.", "metadata": {}}, {"text": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al.", "metadata": {}}, {"text": "The pile: An 800gb dataset of\ndiverse text for language modeling.", "metadata": {}}, {"text": "arXiv preprint arXiv:2101.00027, 2020.", "metadata": {}}, {"text": "Xinyang Geng and Hao Liu.", "metadata": {}}, {"text": "Openllama: An open reproduction of llama, May 2023.", "metadata": {}}, {"text": "URL\nhttps://github.com/openlm-research/open_llama.", "metadata": {}}, {"text": "glaiveai.", "metadata": {}}, {"text": "Glaive-code-assistant, 2023.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/glaiveai/\nglaive-code-assistant.", "metadata": {}}, {"text": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,\nXiao Bi, Y.", "metadata": {}}, {"text": "Wu, Y.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.", "metadata": {}}, {"text": "Deepseek-coder:\nWhen the large language model meets programming – the rise of code intelligence, 2024a.", "metadata": {}}, {"text": "Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares,\nAlexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu\nBlondel.", "metadata": {}}, {"text": "Direct language model alignment from online ai feedback, 2024b.", "metadata": {}}, {"text": "Horace He and Shangdi Yu.", "metadata": {}}, {"text": "Transcending runtime-memory tradeoffs in checkpointing by\nbeing fusion aware.", "metadata": {}}, {"text": "Proceedings of Machine Learning and Systems, 5, 2023.", "metadata": {}}, {"text": "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei\nFang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang,\nChongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding,\nChao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun.", "metadata": {}}, {"text": "Minicpm: Unveiling\nthe potential of small language models with scalable training strategies, 2024.", "metadata": {}}, {"text": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al.", "metadata": {}}, {"text": "Mistral 7b.", "metadata": {}}, {"text": "arXiv preprint arXiv:2310.06825, 2023.", "metadata": {}}, {"text": "Albert Q.", "metadata": {}}, {"text": "Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Th ´eophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth´ee Lacroix, and William El Sayed.", "metadata": {}}, {"text": "Mixtral of experts, 2024.", "metadata": {}}, {"text": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.", "metadata": {}}, {"text": "Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1705.03551, 2017.", "metadata": {}}, {"text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.", "metadata": {}}, {"text": "Natural\nquestions: a benchmark for question answering research.", "metadata": {}}, {"text": "Transactions of the Association for\nComputational Linguistics, 7:453–466, 2019.", "metadata": {}}, {"text": "LAION-AI.", "metadata": {}}, {"text": "Open-Assistant: A chat-based assistant that understands tasks, can interact\nwith third-party systems, and retrieve information dynamically.", "metadata": {}}, {"text": "https://github.com/\nLAION-AI/Open-Assistant, 2023.", "metadata": {}}, {"text": "Ariel N.", "metadata": {}}, {"text": "Lee, Cole J.", "metadata": {}}, {"text": "Hunter, and Nataniel Ruiz.", "metadata": {}}, {"text": "Platypus: Quick, cheap, and powerful\nrefinement of llms, 2024.", "metadata": {}}, {"text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem.", "metadata": {}}, {"text": "Camel: Communicative agents for ”mind” exploration of large language model\nsociety.", "metadata": {}}, {"text": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "JetMoE\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi\nBhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-\nnakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜noz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you!, 2023b.\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand ”Teknium”. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces,\nwith verification, 2023. URL https://https://huggingface.co/Open-Orca/SlimOrca.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step.\npreprint arXiv:2305.20050, 2023.\nlm sys. FastChat: An open platform for training, serving, and evaluating large language\nmodel based chatbots. https://github.com/lm-sys/FastChat, 2023.\nLocutusque. Ultratextbooks, 2024. URL https://huggingface.co/datasets/Locutusque/\nUltraTextbooks.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V . Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing\ndata and methods for effective instruction tuning, 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,\nNouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2\nand the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao,\nJing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language\nmodels with evol-instruct, 2023.\nArindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math:\nUnlocking the potential of slms in grade school math, 2024.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,\nSwayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack:\nInstruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023a.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-\ngru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid\nAlyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization\nthrough multitask finetuning, 2023b.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi,\nand Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of\ngpt-4, 2023.\n14", "sentences": [{"text": "JetMoE\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi\nBhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-\nnakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜noz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.", "metadata": {}}, {"text": "Starcoder: may the\nsource be with you!, 2023b.", "metadata": {}}, {"text": "Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand ”Teknium”.", "metadata": {}}, {"text": "Slimorca: An open dataset of gpt-4 augmented flan reasoning traces,\nwith verification, 2023.", "metadata": {}}, {"text": "URL https://https://huggingface.co/Open-Orca/SlimOrca.", "metadata": {}}, {"text": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.", "metadata": {}}, {"text": "Let’s verify step by step.", "metadata": {}}, {"text": "preprint arXiv:2305.20050, 2023.", "metadata": {}}, {"text": "lm sys.", "metadata": {}}, {"text": "FastChat: An open platform for training, serving, and evaluating large language\nmodel based chatbots.", "metadata": {}}, {"text": "https://github.com/lm-sys/FastChat, 2023.", "metadata": {}}, {"text": "Locutusque.", "metadata": {}}, {"text": "Ultratextbooks, 2024.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/Locutusque/\nUltraTextbooks.", "metadata": {}}, {"text": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V .", "metadata": {}}, {"text": "Le, Barret Zoph, Jason Wei, and Adam Roberts.", "metadata": {}}, {"text": "The flan collection: Designing\ndata and methods for effective instruction tuning, 2023.", "metadata": {}}, {"text": "Ilya Loshchilov and Frank Hutter.", "metadata": {}}, {"text": "Decoupled weight decay regularization.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1711.05101, 2017.", "metadata": {}}, {"text": "Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,\nNouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al.", "metadata": {}}, {"text": "Starcoder 2\nand the stack v2: The next generation.", "metadata": {}}, {"text": "arXiv preprint arXiv:2402.19173, 2024.", "metadata": {}}, {"text": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao,\nJing Ma, Qingwei Lin, and Daxin Jiang.", "metadata": {}}, {"text": "Wizardcoder: Empowering code large language\nmodels with evol-instruct, 2023.", "metadata": {}}, {"text": "Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah.", "metadata": {}}, {"text": "Orca-math:\nUnlocking the potential of slms in grade school math, 2024.", "metadata": {}}, {"text": "Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,\nSwayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre.", "metadata": {}}, {"text": "Octopack:\nInstruction tuning code large language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2308.07124, 2023a.", "metadata": {}}, {"text": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-\ngru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid\nAlyafeai, Albert Webson, Edward Raff, and Colin Raffel.", "metadata": {}}, {"text": "Crosslingual generalization\nthrough multitask finetuning, 2023b.", "metadata": {}}, {"text": "Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi,\nand Ahmed Awadallah.", "metadata": {}}, {"text": "Orca: Progressive learning from complex explanation traces of\ngpt-4, 2023.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "JetMoE\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-\nwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, et al. Efficient large-scale language model training on gpu clusters using\nmegatron-lm. In Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pp. 1–15, 2021.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions\nwith human feedback, 2022.\nBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin\nRaffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of\nmixture-of-experts language models, 2024.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The\nlambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031, 2016.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An\nopen dataset of high-quality mathematical web text, 2023.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and\nweb data only. arXiv preprint arXiv:2306.01116, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction\ntuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\nURL http://jmlr.org/papers/v21/20-074.html.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory\noptimizations toward training trillion parameter models, 2020.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J´er´emy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis\nMartin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open\nfoundation models for code, 2024.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\n2021.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:\nCommonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n15", "sentences": [{"text": "JetMoE\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-\nwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, et al.", "metadata": {}}, {"text": "Efficient large-scale language model training on gpu clusters using\nmegatron-lm.", "metadata": {}}, {"text": "In Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pp.", "metadata": {}}, {"text": "1–15, 2021.", "metadata": {}}, {"text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.", "metadata": {}}, {"text": "Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe.", "metadata": {}}, {"text": "Training language models to follow instructions\nwith human feedback, 2022.", "metadata": {}}, {"text": "Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin\nRaffel, and Rameswar Panda.", "metadata": {}}, {"text": "Dense training, sparse inference: Rethinking training of\nmixture-of-experts language models, 2024.", "metadata": {}}, {"text": "Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez.", "metadata": {}}, {"text": "The\nlambada dataset: Word prediction requiring a broad discourse context.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1606.06031, 2016.", "metadata": {}}, {"text": "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba.", "metadata": {}}, {"text": "Openwebmath: An\nopen dataset of high-quality mathematical web text, 2023.", "metadata": {}}, {"text": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.", "metadata": {}}, {"text": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and\nweb data only.", "metadata": {}}, {"text": "arXiv preprint arXiv:2306.01116, 2023.", "metadata": {}}, {"text": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.", "metadata": {}}, {"text": "Instruction\ntuning with gpt-4.", "metadata": {}}, {"text": "arXiv preprint arXiv:2304.03277, 2023.", "metadata": {}}, {"text": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D.", "metadata": {}}, {"text": "Manning, and\nChelsea Finn.", "metadata": {}}, {"text": "Direct preference optimization: Your language model is secretly a reward\nmodel, 2023.", "metadata": {}}, {"text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J.", "metadata": {}}, {"text": "Liu.", "metadata": {}}, {"text": "Exploring the limits of transfer learning with a\nunified text-to-text transformer.", "metadata": {}}, {"text": "Journal of Machine Learning Research, 21(140):1–67, 2020.", "metadata": {}}, {"text": "URL http://jmlr.org/papers/v21/20-074.html.", "metadata": {}}, {"text": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.", "metadata": {}}, {"text": "Zero: Memory\noptimizations toward training trillion parameter models, 2020.", "metadata": {}}, {"text": "Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J´er´emy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis\nMartin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.", "metadata": {}}, {"text": "Code llama: Open\nfoundation models for code, 2024.", "metadata": {}}, {"text": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.", "metadata": {}}, {"text": "Winogrande: An\nadversarial winograd schema challenge at scale.", "metadata": {}}, {"text": "Communications of the ACM, 64(9):99–106,\n2021.", "metadata": {}}, {"text": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.", "metadata": {}}, {"text": "Socialiqa:\nCommonsense reasoning about social interactions.", "metadata": {}}, {"text": "arXiv preprint arXiv:1904.09728, 2019.", "metadata": {}}, {"text": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\nHinton, and Jeff Dean.", "metadata": {}}, {"text": "Outrageously large neural networks: The sparsely-gated mixture-\nof-experts layer.", "metadata": {}}, {"text": "arXiv preprint arXiv:1701.06538, 2017.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "JetMoE\nYikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.\nModuleformer: Learning modular large language models from uncurated data. arXiv\npreprint arXiv:2306.04640, 2023.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism. arXiv preprint arXiv:1909.08053, 2019.\nQingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical\nstudy of instruction-tuning large language models in chinese, 2023.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,\nAnanya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,\nJacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Pe-\nters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant\nSubramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh\nHajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus\nof three trillion tokens for language model pretraining research, 2024.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology. arXiv preprint\narXiv:2403.08295, 2024.\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.\nTeknium1. GPTeacher: A collection of modular datasets generated by GPT-4. https:\n//github.com/teknium1/GPTeacher, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang,\nKashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https:\n//github.com/huggingface/alignment-handbook, 2023a.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Cl ´ementine Fourrier, Nathan Habib,\nNathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr:\nDirect distillation of lm alignment, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nChaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl:\nGeneralizing direct preference optimization with diverse divergence constraints, 2023a.\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,\nArjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating\ncollege-level scientific problem-solving abilities of large language models, 2023b.\n16", "sentences": [{"text": "JetMoE\nYikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.", "metadata": {}}, {"text": "Moduleformer: Learning modular large language models from uncurated data.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2306.04640, 2023.", "metadata": {}}, {"text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro.", "metadata": {}}, {"text": "Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism.", "metadata": {}}, {"text": "arXiv preprint arXiv:1909.08053, 2019.", "metadata": {}}, {"text": "Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang.", "metadata": {}}, {"text": "An empirical\nstudy of instruction-tuning large language models in chinese, 2023.", "metadata": {}}, {"text": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,\nAnanya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,\nJacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E.", "metadata": {}}, {"text": "Pe-\nters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant\nSubramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A.", "metadata": {}}, {"text": "Smith, Hannaneh\nHajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.", "metadata": {}}, {"text": "Dolma: an open corpus\nof three trillion tokens for language model pretraining research, 2024.", "metadata": {}}, {"text": "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.", "metadata": {}}, {"text": "Roformer:\nEnhanced transformer with rotary position embedding.", "metadata": {}}, {"text": "Neurocomputing, 568:127063, 2024.", "metadata": {}}, {"text": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B.", "metadata": {}}, {"text": "Hashimoto.", "metadata": {}}, {"text": "Stanford alpaca: An instruction-following\nllama model.", "metadata": {}}, {"text": "https://github.com/tatsu-lab/stanford_alpaca, 2023.", "metadata": {}}, {"text": "Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, et al.", "metadata": {}}, {"text": "Gemma: Open models based on gemini research and technology.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2403.08295, 2024.", "metadata": {}}, {"text": "Teknium.", "metadata": {}}, {"text": "Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\n2023.", "metadata": {}}, {"text": "URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.", "metadata": {}}, {"text": "Teknium1.", "metadata": {}}, {"text": "GPTeacher: A collection of modular datasets generated by GPT-4.", "metadata": {}}, {"text": "https:\n//github.com/teknium1/GPTeacher, 2023.", "metadata": {}}, {"text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.", "metadata": {}}, {"text": "Llama: Open and efficient foundation language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.13971,\n2023.", "metadata": {}}, {"text": "Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang,\nKashif Rasul, Alexander M.", "metadata": {}}, {"text": "Rush, and Thomas Wolf.", "metadata": {}}, {"text": "The alignment handbook.", "metadata": {}}, {"text": "https:\n//github.com/huggingface/alignment-handbook, 2023a.", "metadata": {}}, {"text": "Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Cl ´ementine Fourrier, Nathan Habib,\nNathan Sarrazin, Omar Sanseviero, Alexander M.", "metadata": {}}, {"text": "Rush, and Thomas Wolf.", "metadata": {}}, {"text": "Zephyr:\nDirect distillation of lm alignment, 2023b.", "metadata": {}}, {"text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin.", "metadata": {}}, {"text": "Attention is all you need.", "metadata": {}}, {"text": "Advances in neural informa-\ntion processing systems, 30, 2017.", "metadata": {}}, {"text": "Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen.", "metadata": {}}, {"text": "Beyond reverse kl:\nGeneralizing direct preference optimization with diverse divergence constraints, 2023a.", "metadata": {}}, {"text": "Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,\nArjun R.", "metadata": {}}, {"text": "Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang.", "metadata": {}}, {"text": "Scibench: Evaluating\ncollege-level scientific problem-solving abilities of large language models, 2023b.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}]}