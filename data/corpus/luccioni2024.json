{"document_id": "luccioni2024", "title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?", "text": "Power Hungry Processing:\n Watts\n Driving the Cost of AI Deployment?\nALEXANDRA SASHA LUCCIONI and YACINE JERNITE, Hugging Face, Canada/USA\nEMMA STRUBELL, Carnegie Mellon University, Allen Institute for AI, USA\nFig. 1. The tasks examined in our study and the average quantity of carbon emissions they produced (in g of ùê∂ùëÇ 2ùëíùëû) for 1,000 queries.\nN.B. The y axis is in logarithmic scale.\nRecent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising\na unified approach to building machine learning (ML) models into technology. However, this ambition of ‚Äúgenerality‚Äù comes at a steep\ncost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we\npropose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific\n(i.e. finetuned models that carry out a single task) and ‚Äògeneral-purpose‚Äô models, (i.e. those trained for multiple tasks). We measure\ndeployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using\nthese models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems\nfor a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current\ntrend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against\nincreased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out\nfurther exploration and analysis.\nCCS Concepts: ‚Ä¢Computing methodologies ‚Üí Machine learning; Neural networks; ‚Ä¢Hardware ‚Üí Impact on the environ-\nment; Power estimation and optimization .\nACM Reference Format:\nAlexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2024. Power Hungry Processing:\n Watts\n Driving the Cost of AI\nDeployment?. In ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT ‚Äô24), June 3‚Äì6, 2024, Rio de Janeiro, Brazil.\nACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/3630106.3658542\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party\ncomponents of this work must be honored. For all other uses, contact the owner/author(s).\n¬© 2024 Copyright held by the owner/author(s).\nManuscript submitted to ACM\n1\narXiv:2311.16863v3  [cs.LG]  15 Oct 2024\n\n[Image page=1 idx=1 name=Im2.png] Size: 838x407, Data: 52874 bytes\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n1 INTRODUCTION\nUnderstanding the environmental impacts of different industries is an important first step towards developing effective\nstrategies to mitigate those impacts. For newer industries such as information and communication technologies (ICT)\nof which Artificial Intelligence (AI) and Machine Learning (ML) are considered to be a part of, more work is needed to\nunderstand the extent of their environmental impacts and the factors that influence it. Between 2017 and 2021, the\nelectricity used by Meta, Amazon, Microsoft, and Google, the main providers of commercially-available cloud compute,\nmore than doubled [22]. According to the most recent figures available, global data centre electricity consumption\nhas grown by 20-40% annually in recent years, reaching 1-1.3% of global electricity demand and contributing 1% of\nenergy-related greenhouse gas emissions in 2022 [21]. However the contribution of the AI sector specifically towards\nthese figures is unclear.\nRecent work documenting the environmental impacts of ML has focused largely on quantifying the operational\nenergy and carbon required to perform the training phase of the ML model life cycle [12, 30, 41, 49] due to the relative\nease of measuring per-model energy use for that phase and the impressive quantity of energy required to perform\na single training run [ 41, 49]. Yet, other phases of the ML model life cycle, such as inference, stand to impact the\nenvironment just as much, or more, than training due to the computational resources required to deploy modern\nmodels at scale. While inference on a single example requires much less computation than that required to train the\nsame model, inference happens far more frequently than model training ‚Äî as many as billions of times a day for a\nmodel powering a popular user-facing product such as Google Translate.1 Yet, in-depth work quantifying the costs of\nmodel inference and deployment is limited and their environmental impacts, in terms of energy and carbon as well as\nwater and mining of rare earth minerals, have yet to be estimated. According to AWS, the largest global cloud provider,\ninference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by\nMeta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the\nremainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed\n60% of its ML energy use to inference, compared to 40% for training [40]. Given the increasing ubiquity of AI model\ndeployment, it is crucial to go beyond these high-level statistics to get a better idea of the energy requirements and\ncarbon emissions of model inference for different models and tasks. In particular, looking at inference rather than\ntraining leads to drastically different conclusions when considering the multi-purpose (or ‚Äúgeneral-purpose‚Äù) aspect\nspecifically. Training a single model for multiple tasks can indeed be more energy-efficient when considering training\ncosts only, but these gains can easily be lost and even reversed over the course of the model‚Äôs lifetime, given how much\ninference is carried out when these models are deployed in user-facing applications like chat and web search.\nTo help shed light on this issue, we perform an extensive study measuring the amount of energy required to deploy\nvarious ML models and architectures, including large language models (LLMs)- as such, our study is, to our knowledge,\nthe first to focus solely on the inference phase of the ML model life cycle. We study 88 models across 10 tasks and 30\ndatasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality,\nmodel size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency. We\nidentify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and\ntasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and\nensuing carbon emissions. By painting a more detailed picture of widely varying energy requirements for ML model\n1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion\nqueries to the model per day. Source: https://blog.google/products/translate/ten-years-of-google-translate/\n2\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ninference, we hope this study can be useful for practitioners to better understand accuracy-efficiency trade-offs across\ntasks and models, as well as enabling better estimates, and projections and policy decisions at the sector level.\n2 PREVIOUS WORK\nEstimating the energy and emissions of ML models has remains a relatively under-explored topic, albeit one that\nhas been gathering traction since Strubell et al‚Äôs seminal article quantifying the energy and carbon emissions of a\nvariety of then-large NLP models [2019]. Since then, most studies have focused on estimating the energy consumed and\ncarbon emitted during the training phase of neural networks ‚Äì this includes studies by Patterson et al. [2022, 2021],\nwho compared different models and analyzed factors influencing their emissions. There have also been studies of\nspecific model architectures, e.g. BLOOM [ 31] and Nour [27], which carried out in-depth analyses of the different\nsteps in the models‚Äô life cycle and their relative contribution towards the final quantity of carbon emissions. Given the\nincreasing deployment of ML models in the cloud, several studies have therefore looked at cloud-specific ways to reduce\nthe emissions of ML models such as delayed scheduling, workload elasticity and choosing the least carbon-intensive\nelectricity available Chien et al. [6], Dodge et al. [12], Hanafy et al. [19].\nDespite these empirical studies, there is currently a lack of standardized methodology for quantifying and comparing\nthe energy consumption and carbon emissions of ML models. There are several tools that exist, such as Code Carbon [47],\nMLCO2 [26] and LLMCarbon [13], all of which adopt different approaches and output different results (see [ 1] for\na detailed comparison). It is therefore difficult to systematically compare the carbon footprints of different models.\nExisting tools and studies have also largely focused on the dynamic power consumption (i.e. the electricity necessary for\npowering hardware) and its resulting emissions. However, there have been several proposals to also take into account\nthe embodied emissions of ML models (i.e. the emissions that can be attributed to the manufacturing of computing\nequipment) into carbon emissions estimates. This has been impeded by a lack of transparency from the designers\nof common computing hardware such as GPUs, although recent estimates have revealed that the embodied carbon\nfootprint of an LLM trained and deployed on Meta‚Äôs compute cluster constitutes up to 50% of its carbon footprint [57].\nWhile the majority of existing work has been focused on ML model training given that it is a more tractable part of\nthe model life cycle (i.e. it is most often carried out over a set period of time on a specific compute instance), model\ninference has started to also become the subject of scholarship [6, 11]. Luccioni et al. ‚Äôs study of BLOOM was the first\nof its kind to look at the specific energy costs related to deploying an LLM [ 31] and found that, over time, this can\nrepresent a significant portion of a model‚Äôs overall carbon footprint.\nThe current study further pursues this line of work, delving deeper into the inference stage of ML models, the energy\nit consumes and the carbon it emits. By testing a variety of architectures on different tasks and datasets, we aim to gain\na better understanding of the degree of variance that can be observed and how seemingly small user choices can result\nin large differences in model‚Äôs environmental impacts.\n3 METHODOLOGY\nAs stated above, our study focuses on the inference (i.e. deployment) stage in the model life cycle, aiming to address the\nknowledge gaps that currently exist with regards to its energy consumption and ensuing emissions. We describe how\nwe chose the tasks, datasets and models in the sections below, and present the results of our analysis in Section 4.\n3\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n3.1 Task and dataset selection\nAs the starting point of our study, we chose 10 ML tasks from 5 different modalities:Text-to-category (text classifica-\ntion, token classification, extractive question answering), Text-to-text (masked language modeling, text generation,\nsummarization), Image-to-category (image classification, object detection), Image-to-text (image captioning) and\nText-to-image (image generation). These tasks were chosen because they are common in both Natural Language\nProcessing and Computer Vision, allowing us to explore multiple modalities, and include several multimodal tasks (i.e.\nimage captioning and image generation), allowing us to explore the nexus between several modalities as well. To test\neach of the tasks listed above, we chose three of the most downloaded datasets from the Hugging Face Hub. We present\nthe tasks and their corresponding datasets in Table 1.\nTask Datasets Task Datasets\nimage\nclassification\nCIFAR 10 [25]\nCIFAR 100 [25]\nImageNet 1K [45]\nquestion\nanswering\nSQuAD[44]\nSQuAD v2 [43]\nSciQ [23]\nimage\ncaptioning\nVisual Genome [24]\nRedCaps [10]\nCOCO [29]\nsummarization\nSAMSum [15]\nCNN-Daily Mail [20]\nXSum [35]\nimage\ngeneration\nDiffusionDB [54]\nImageReward [58]\nSD Prompts [46]\ntext\nclassification\nIMDB [32]\nRotten Tomatoes [39]\nSST 2 [48]\nmasked\nlanguage\nmodeling\nBookCorpus [59]\nC4 [42]\nOSCAR [37]\ntext\ngeneration\nWikiText [33]\nBookCorpus [59]\nOSCAR [37]\nobject\ndetection\nVisual Genome [24]\nCPPE-5 [9]\nCOCO [29]\ntoken\nclassification\nReCoRD [53]\nWikiANN [38]\nCoNLL 2003 [50]\nTable 1. A list of the tasks and datasets used in our study.\n3.2 Models\nTo be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained\nor finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or\nmulti-task models, to allow comparisons both for different architectures on a given task and between tasks for the same\narchitecture.\nTask-specific Models. For all of the tasks listed above, we selected the 8 most popular models from the HuggingFace\nHub (by number of downloads) 2 - we present the full list of model identifiers in Table 6 in the Supplementary Materials.\nFor each model, we ran 1,000 inferences for each of the 3 datasets from the task it was trained for (listed in Table 1),\nusing the Transformers [55] library. We ran each set of inferences 10 times to ensure statistical significance of our\nmeasurements. We set up the inferences sequentially ‚Äì i.e., without batching ‚Äì in order to reflect the variability of\nmodel deployment in situ, which can make it difficult to batch model inputs.\n2We were obliged to discard some models, e.g. if they were trained on another language or if the specific task they were fine-tuned for was not compatible\nwith any of the datasets selected.\n4\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nMulti-Purpose Models. In addition to the task-specific models listed above, we also selected 8 multi-purpose models\nto analyze on different tasks ‚Äì models that were specifically trained to perform well in various different application\nsettings. We chose 4 sequence-to-sequence models of different sizes from the Flan-T5 family [8] (base, large, xl and\nxxl) and 4 decoder-only models from the BLOOMz family [34]: BLOOMz-560M, BLOOMz-1B, BLOOMz-3B and BLOOMz-7B.\nWe tested these on a subset of the tasks to allow a comparison of multi-purpose generative models with individual\ntask-specific systems in terms of their energy consumption and emissions: question answering, text classification and\nsummarization. We selected these three tasks because we were able to find a set of models that were capable of carrying\nthem out with a unified model architecture (which wasn‚Äôt possible for all tasks, especially ones that involved multiple\nmodalities.) We prompted these 8 models in a zero-shot setting that was constant across models, e.g. \"Summarize the\nfollowing text: [text]. Summary:\" on the same 1,000 samples as the fine-tuned models, also repeating each experiment\nten times to measure the significance of results.\nWe ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and\nused the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference 3.\nGiven that all of our experiments were run in the same compute region (AWS‚Äôsus-west-2), which is based in Oregon\nand has an average carbon intensity of 297.6 grams of ùê∂ùëÇ2ùëíùëû per kWh4, this means that both the energy consumed\nduring inference and the carbon emitted are correlated; we will therefore plot one or the other depending on which\naspect of our results we are discussing. While the energy consumed during inference will remain similar for models\ndeployed on A100 GPUs in other compute regions, the carbon emissions will vary depending on the source of energy\nused in the region ‚Äì it is therefore helpful to report both energy and carbon separately to allow for meaningful\ncomparisons across regions and hardware. We provide all the code used for our experiments in our GitHub repository,\nalongside the logs produced by Code Carbon, which not only provides the total energy consumed but also a more\nfine-grained breakdown by hardware component (GPU, CPU and RAM), which can be used to carry out further analyses.\nIn total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg\nof ùê∂ùëÇ2ùëíùëû.\n4 RESULTS\nWe present our results in the subsections below: in Section 4.1, we analyze the range of energy used and carbon emitted\nfor each task for task-specific models. In Section 4.2, we shift our focus to multi-purpose (i.e. ‚Äòzero-shot‚Äò models), looking\nat the variation between different sizes and architectures of multi-purpose models and the difference in the energy\nconsumption and emissions between task-specific and multi-purpose models. In Section 4.3, we carry out a comparison\nbetween model training and inference costs for models of different sizes, calculating when parity is reached.\n4.1 Task-specific model analysis\nWe start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety\nof tasks. Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for\n1,000 inferences and its standard deviation. We can see that classification tasks for both images and text are on the\nlower end of the spectrum in terms of emissions (ranging between 0.002 and 0.007 kWh for 1,000 inferences), whereas\n3While all of our experiments were run on a single GPU, the idle power usage of the other GPUs is also reflected in the numbers that we report in our\nresults.\n4The carbon intensity of an energy grid is measured in ùê∂ùëÇ 2ùëíùëû, and not in ùê∂ùëÇ 2 specifically, because the different greenhouse gases that are generated\nduring electricity generation are reduced to a common denominator, that of carbon dioxide, or ùê∂ùëÇ 2. For a more in-depth discussion of how this is done,\nsee Luccioni and Hernandez-Garcia [2023].\n5\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\ngenerative tasks such as text generation and summarization use, on average, over 10 times more energy for the same\nnumber of inferences (around 0.05 kWh for 1,000 inferences), and multimodal tasks such as image captioning and image\ngeneration are on the highest end of the spectrum (0.06-2.9 kWh for 1,000 inferences). Text-based tasks are, all things\nconsidered, more energy-efficient than image-based tasks, with image classification requiring less energy (median\nof 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH)\nrequiring more than text classification (0.0023 kWh). For comparison, charging the average smartphone requires 0.022\nkWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full\nsmartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as\n522 smartphone charges (11.49 kWh), or around half a charge per image generation 5, although there is also a large\nvariation between image generation models, depending on the size of image that they generate.\ninference energy (kWh)\ntask mean std\ntext classification 0.002 0.001\nextractive QA 0.003 0.001\nmasked language modeling 0.003 0.001\ntoken classification 0.004 0.002\nimage classification 0.007 0.001\nobject detection 0.038 0.02\ntext generation 0.047 0.03\nsummarization 0.049 0.01\nimage captioning 0.063 0.02\nimage generation 2.907 3.31\nTable 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.\nWe can also observe that there is a large variation in the amount of energy used, from the least energy-intensive\ntask, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one,\nimage generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study\ncan vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences. Intuitively,\nthis is coherent given the decision space that different types of models have - from a binary classification task such as\nsentiment analysis (which can only output, for instance, a 0 for negative sentiment and a 1 for positive) to an entire\nvocabulary for text generation and summarization models. The length of text generated also impacts energy usage: on\naverage, text generation uses 15 times more energy than masked language modeling, which makes sense given that the\nmasked language modeling task only generates a single token, whereas in our setup the text generation task generates\n10 new tokens for each input text, with the length of the input text rising as new tokens are generated, since each\nsequence of tokens gets fed back into the model to generate subsequent tokens. Finally, for image-based tasks, the level\nof abstraction is lower and the decision space is larger given that they generate raw pixels as opposed to tokens for text,\nmaking image-based tasks more energy intensive than text based ones, e.g. image classification uses over 3 times more\nenergy than text classification (0.007 vs. 0.002 kWh) and image generation uses, on average, over 60 times more energy\nthan text generation (0.047 vs. 2.9 kWh).\n5Before January 2024, the EPA website estimated a smartphone charge to consume 0.012 kWh of energy, which was the number used for comparisons in\nan earlier version of this study.\n6\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig. 2. The 5 modalities examined in our study, with the number of parameters of each model on the x axis and the average amount\nof carbon emitted for 1000 inferences on the y axis. NB: Both axes are in logarithmic scale.\nNext, we examine the respective influences of model size and task structure on model emissions. Figure 2 shows the\nrelationship between model emissions (in grams of ùê∂ùëÇ2ùëíùëû per 1,000 inferences) and sizes (in terms of the number of\nparameters) across the task categories listed in Section 3.1. We do observe a relationship between model size and quantity\nof emissions produced during inference, with differing progressions for each modality ‚Äì however, the task structure ac-\ncounts for more of the variation than the model size does. We can observe once again that text-to-image is by far the most\ncarbon- and energy-intensive task, with smaller image generation models such as segmind/tiny-sd that have around\n500M parameters producing magnitudes more carbon than text-to-category models (100g vs. 0.6g of ùê∂ùëÇ2ùëíùëû per 1,000\ninferences). Within the text-to-text tasks, we see two separate sets of models: the masked language modeling task follow-\ning a lower trend, producing emissions akin to text-to-category models, compared to text generation and summarization\ntasks, which produce similar amounts of carbon to the image captioning models with a similar number of parameters.\nFor context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594\ngrams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered\npassenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) gener-\nates as much carbon as 0.0006 miles driven by a similar vehicle, i.e. 6,833 times less. This can add up quickly when\nimage generation models such as Dall¬∑E and MidJourney are deployed in user-facing applications and used by millions\nof users globally (we discuss this point further in Section 5).\nThe (high-level) takeaway of this analysis is that even for models specifically trained to carry out a single task,\nthere is a large level of variation both within each task and an even larger one between tasks from different modalities.\nIn essence, tasks that map both image and text inputs to categorical outputs are less energy- and carbon-intensive\nthan those that generate text or images. Making these distinctions can help inform policies seeking to mitigate the\nenvironmental impacts of AI, given that it is important to be aware of this variation, which can sometimes reach several\norders of magnitude. In the next section, we delve deeper into multi-purpose systems, which are meant to carry out\nseveral tasks concurrently, to better understand their environmental impacts and how they compare to task-specific\nmodels.\n7\n\n[Image page=7 idx=1 name=Im3.png] Size: 1000x360, Data: 39959 bytes\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2 The environmental cost of multi-purpose systems\nThe second part of our analysis examines multi-task models of two types: decoder only, from the BLOOMz family,\nand sequence-to-sequence models from the FLAN-T5 family, with the goal of comparing energy intensity and carbon\nemissions of models with differing numbers of parameters when applied to different tasks. To address this question,\nwe selected a subset of 3 tasks ‚Äì text classification, extractive question answering, and summarization ‚Äì given their\ndiversity and broad applicability in a variety of settings, and compare the 8 zero-shot models of different sizes, based on\nthe same 3 datasets per task as described in Table 1.\n4.2.1 Emissions of task-specific and multi-task architectures.\nTo start our analysis, we examined how the choice of model and architecture type impacts emissions given a specific\ntask and dataset. For this analysis, we took the same 8 task-specific models described in Section 3.2 and compared their\nemissions to the 8 multi-purpose models described above.\nFig. 3. Model emissions (measured in g ùê∂ùëÇ 2ùëíùëû) and architecture type for each of the datasets from our analysis. The y axis is in\nlogarithmic scale, dot size is proportional to model size.\nIn Figure 3, we plot the mean query emissions for each model on a dataset-by-dataset basis. We can see that for\nthe two discriminative tasks, sentiment analysis (which includes SST 2, Rotten Tomatoes and IMDB datasets) and\nquestion answering (which encompasses SciQ, SQuAD and SQuAD v2) there is a clear distinction between task-specific\ndiscriminative models (in blue), which have less emissions than both multi-purpose sequence-to-sequence (in yellow)\nand decoder-only generative models (in green). Given that the y axis in Figure 3 is in logarithmic scale, this indicates that\nthe difference is several orders of magnitude - e.g. with the most efficient task-specific models emit 0.3g ofùê∂ùëÇ2ùëíùëû per\n1,000 inferences for extractive question answering on a dataset like SciQ, multi-purpose models emit 10g for the same\ntask. This result follows intuitions derived from the model structures: while a task-specific model trained on binary text\nclassification will carry out a softmax on a two-category vector to predict a class, a multi-purpose model will generate\n‚Äòpositive‚Äô or ‚Äònegative‚Äô, which logically requires more energy because the prediction is based on the model‚Äôs entire\nvocabulary. For the generative task, summarization (represented by the SAMsum, XSum and CNN-Daily Mail datasets),\nthe task-specific and multi-purpose models are closer in terms of emissions: task-specific sequence-to-sequence models\n8\n\n[Image page=8 idx=1 name=Im4.png] Size: 1000x400, Data: 62665 bytes\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ngenerate 4-10g of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, while multi-purpose models emit 20-30g for the same task. The difference\nappears to mostly come from model size ‚Äì all of the task-specific summarization models we looked at were 600 million\nparameters at most, compared to the larger multi-purpose architectures, which attained the 11 billion parameters.\nWe also carry out an evaluation of both the task-specific and multi-purpose models examined in our study to\nensure that they have comparable performance. For task-specific models, we used the evaluate library [52] and the\nLM Evaluation Harness [14] for zero-shot models. Fundamentally speaking, it is hard to compare task-specific and\nmulti-purpose models using the same metrics, given that task-specific models have a much more constrained decision\nspace (e.g. two classes in the case of binary text classification), whereas multi-purpose models have a large output\nvocabulary to choose from, and are dependent upon the prompt schema and prompting strategy used. However, by\nutilizing two standardized packages (evaluate and lm-evaluation-harness) and keeping the prompting approach\nstable across zero-shot models, we endeavor to standardize our evaluation approach as much as possible.\nFig. 4. Model size, measured in number of parameters (x axis, logarithmic scale) and text classification accuracy (y axis), with dot size\nindicating the quantity of emissions (logarithmic scale).\nWe hone in on one specific task, text classification, in Figure 4, which illustrates the relationship between model\nsize (x axis, in logarithmic scale), accuracy (y axis) and emissions (dot size, in logarithmic scale). Among task-specific\nencoder models, we observe that accuracy varies more widely, i.e. there are several smaller models of similar size and\ncomparably small amounts of carbon emissions, with widely varying levels of accuracy. The multi-purpose models\nvary less in terms of accuracy, having higher average accuracy overall. Both sequence-to-sequence and decoder-only\nmodels produce comparable amounts of emissions (several orders of magnitude more than task-specific models).We can\nsee that mid-size multi-purpose models (in the 3B parameter range) may have slightly better accuracy compared to\nboth larger and smaller models. However, given the many caveats and specificities involved in multi-purpose LLM\nevaluation, this difference may not be significant. We present the full results of our evaluation, which include the other\n2 tasks, in Section B in the Supplementary Materials.\n9\n\n[Image page=9 idx=1 name=Im5.png] Size: 993x246, Data: 50628 bytes\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2.2 Differences within multi-purpose architectures.\nBeyond the differences between task-specific and multi-purpose models generally, we also observed variation\nwithin the multi-purpose models that we examined. We present our results in Table 3; in it, we can observe that\non a per-architecture basis (i.e. within the family of decoder-only models and the family of sequence-to-sequence\nmodels), size and emissions are correlated, with smaller models emitting less carbon and using less energy. However,\nsequence-to-sequence models are more efficient than their decoder-only counterparts when models of the same size\nare compared: for instance, Flan-T5-XL and BLOOMz-3B are both of a similar size (around 3B parameters), but the\nformer generates, on average, 2 grams of emissions less for 1,000 inferences than the latter. This difference holds when\ncomparing Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we\ntested (11 billion), yet it has lower emissions (11.48g on average) compared to the smaller BLOOMz-7B. Comparing the\nmodels on a per-task basis in Figure 5, we can see the same pattern for zero-shot models as for task-specific ones, with\ntext classification a less carbon-intensive task compared to question answering, and summarization the most intensive\none of the three. The spread between the tasks is smaller for sequence-to-sequence models (indicated with dots in\nFigure 5), whereas for decoder-only models (indicated with crosses), the difference between the different tasks is more\nsignificant.\nseq2seq models decoder-only models\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nFlan-T5-base 222M 3.67 0.026 BLOOMz-560M 559M 7.5 0.054\nFlan-T5-large 750M 7.68 0.055 BLOOMz-1B 1.7B 8.66 0.062\nFlan-T5-xl 2.8B 8.08 0.058 BLOOMz-3B 3B 10.17 0.073\nFlan-T5-xxl 11B 11.48 0.083 BLOOMz-7B 7B 14.46 0.104\nTable 3. Zero-shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of\nemissions (in g of ùê∂ùëÇ 2ùëíùëû) and average energy usage (in kWh) for 1,000 inferences.\nWe can analyse the relationship between sequence-to-sequence and decoder-only models noted in Table 3: whereas\nfor tasks such as summarization, decoder models do generate more emissions than sequence-to-sequence models of\na similar size, for question answering and text classification, the two architectures have similar emissions. This can\nagain be explained by the differences in the model structures, specifically the attention mechanism: while sequence-to-\nsequence models only attend to the last layer of the input when producing their answers, decoder-only architectures\nattend to all layers for the full sequence ‚Äì leading to a stronger dependency on the output length for the number of\noperations, resulting in more emissions for tasks with longer outputs.\nWe further verify this intuition in Table 4 and Figure 6: while there is some variation between models and datasets in\nTable 4, the distribution of output lengths is consistent with our expectations for the different task categories: tasks with\nlonger outputs result in more emissions, especially for decoder-only models. Figure 6 delves further into the relationship\nbetween average output length, carbon emissions, and model structures for the different summarization datasets. It\nshows a clear correlation between output length and measured emissions, with a higher slope for the decoder-only\narchitectures (the BLOOMz family of models) than for the sequence-to-sequence architectures (the Flan-T5 family).\nAs we have observed in the current section, there is no ‚Äòone-size-fits-all‚Äô pattern for multi-purpose models either ‚Äì\nthey too exhibit variation in terms of their emissions and energy usage, which can be attributed to different factors,\n10\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig. 5. A plot of the total emissions (in grams of ùê∂ùëÇ 2ùëíùëû) for 1,000 inferences for all multi-purpose models.\nBLOOMz\n560M\nBLOOMz\n1B\nBLOOMz\n3B\nBLOOMz\n7B\nFlan-T5\nbase\nFlan-T5\nlarge\nFlan-T5\nxl\nFlan-T5\nxxl\ndataset input output output output output output output output output\nIMDB 58.73 1.64 2.61 1.72 1.53 1.00 1.00 1.00 1.00\nRotten\nTomatoes 30.08 1.00 0.99 1.03 1.00 1.00 1.00 1.00 1.00\nSST 2 28.35 0.98 0.99 1.01 1.02 1.00 1.00 1.00 1.00\nSciQ 113.12 1.28 1.25 1.10 1.10 2.03 5.41 3.12 2.42\nSQuAD 134.00 1.93 1.96 2.02 1.95 2.01 2.15 2.16 2.13\nSQuAD 2 115.85 2.33 2.54 2.58 2.41 2.28 2.74 2.71 2.58\nCNN 54.00 12.05 11.91 11.73 10.34 8.52 11.34 11.34 10.68\nSamSUM 47.82 9.54 9.41 9.75 9.85 10.56 11.05 10.18 10.57\nXSum 53.85 11.53 12.22 11.94 11.92 12.95 13.62 13.49 13.09\nTable 4. Average input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study.\nThe darker the cell, the more carbon was output by the model for the task.\nFig. 6. A plot of the output length (X axis) and carbon emissions (Y axis) for the summarization task. The symbol refers to the type of\narchitecture (BLOOMz vs Flan-T5), symbol size references the relative model size (in terms of the number of parameters), and color\nthe input length.\nincluding model size and output length. This would indicate that more careful consideration is needed when making\n11\n\n[Image page=11 idx=1 name=Im6.png] Size: 1000x300, Data: 57124 bytes\n\n[Image page=11 idx=2 name=Im7.png] Size: 1260x360, Data: 41012 bytes\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4√ó 10‚àí5 for the\nsmallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 √ó 10‚àí4 7.3 √ó 10‚àí5 6.2 √ó 10‚àí5 5.4 √ó 10‚àí5\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used ‚Äì while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nby the open-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded\n606,096 times, BLOOMz-3B has been downloaded 357,368 times, BLOOMz-1B has been downloaded 61,757 times and\nBLOOMz-560m has been downloaded 498,601 times. They have also been finetuned for a number of downstream\ntasks, such as chat, and deployed in HuggingFace Spaces, interactive interfaces for model interaction. While this\nanalysis represents a relatively small sample of models, analyses such as this are vital for estimating the relative energy\nconsumption (and ensuing emissions) of different stages of the ML training and deployment cycle, understanding\ntrade-offs between training and inference emissions patterns, and characterizing the lifetime emissions of ML models,\nand we hope that others will be possible in the future, which would require more transparency from model creators\nregarding both the up front (i.e. training) and downstream (i.e. inference) costs of ML models. We discuss the importance\nof transparency and other important actions that members of the community can take in the next, and final, section.\n5 DISCUSSION\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due\nto its distributed nature ‚Äî compared to the relatively time- and location-constrained nature of training ‚Äî making it\ndifficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to\nkeep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We\nprovide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore\nthe results we present here. We also highlight the main high-level takeaways of our study below:\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the\nmost energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image\ncaptioning, and image generation.\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically,\ntasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving\ngenerating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a\nsimilar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that\nmore computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for\ntasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as\nother tasks and datasets.\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model\ndeployment, this parity can be reached quickly for many popular models.\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these\nsame tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question\nanswering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large,\nand less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference\n13\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nbetween multi-purpose models and task-specific models is amplified as the length of output gets longer.\nWe find this last point to be the most compelling takeaway of our study, given the current paradigm shift away\nfrom smaller models finetuned for a specific task towards models that are meant to carry out a multitude of tasks\nat once, deployed to respond to a barrage of user queries in real time. This transition has been happening both\nin ML research since the advent of GPT-3 [ 5], which illustrated the potential for few- and zero-shot learning with\nlanguage models, as well as in consumer settings, with LLMs such as GPT-4 and PaLM being deployed in user-facing\nproducts such as web search [4, 18], email, and navigation [17], where smaller, task-specific versions of models such\nas BERT were previously used [ 3, 16]. While it is hard to quantify the environmental impacts of this transition\ngiven the lack of transparency of technology companies regarding both the number of parameters, architecture\nand carbon emissions of their products, we can make a comparison based on the experiments carried out in the\npresent study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering\n(bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70gùê∂ùëÇ2ùëíùëû\nper 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for\nBLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification\nwith the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of\nùê∂ùëÇ2ùëíùëû per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM\nmodel, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see\nthe benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see\nconvincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web\nsearch and navigation, given these models‚Äô energy requirements.\nFinally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon\nemissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between\ntraining, finetuning and inference energy requirements carried out in Section 4.3 is, to our knowledge, the first\ncomparison of its kind, and paves the way to a better understanding of how the different stages of an ML model‚Äôs\nlifecycle add up in terms of energy use. These are important data points that can help inform both our fellow AI\nresearchers and practitioners, as well as policy-makers who are working towards estimating and regulating the\nenvironmental impacts of AI models and ICT in general. We recognize that our study is not representative of all\ndeployment contexts and constraints ‚Äì our intent is to establish a set of initial data points and to set the stage for testing\nand comparing other models. In fact, our study highlights many potential avenues for future research aimed towards a\nbetter understanding of the myriad factors that influence the efficiency of inference, including the choice of architecture,\nthe usage of techniques such as distillation, the number of parameters, the choice of hardware and the numerical (i.e.\nfloating point) precision of model parameters. While we encourage continued work analysing open-source models,\nwe note that the growing lack of transparency in model architecture and training details makes this line of work,\nalongside many branches relating to fairness and accountability in machine learning, increasingly difficult to carry\nout. Given our findings and the increased deployment of generative, multi-purpose AI models, we hope that both ML\nresearchers and practitioners will practice transparency regarding the nature and impacts of their models, to enable\nbetter understanding of their environmental impacts.\n8The exact number of parameters of GPT-4 and PaLM 2 have not been publicly shared.\n14\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nETHICAL CONSIDERATIONS STATEMENT\nThe main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon\nemissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical\nsignificance of our measurements. In total, for all of model experimentation and evaluation, we used a total of 754.66\nkWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû. In order to reduce our impacts as much as possible, we did all up-front\nexperimentations on smaller portions of the dataset (to reduce wasted resources).\nRESEARCHER POSITIONALITY STATEMENT\nThe authors of this paper have backgrounds in theoretical and applied machine learning and work in institutions\nbased in North America. We therefore recognize that our way of planning and running experiments is not necessarily\nreflective of other institutions from other regions, or the constraints faced by researchers from institutions with more\nlimited access to compute.\nADVERSE IMPACTS STATEMENT\nWe recognize that our work can be perceived as a critique of ML deployment in general, given the analysis that we\nprovide of its environmental impacts. This could be used as an argument to stop pursuing ML research and development,\nor as a way of targeting specific companies or organizations. Our intention, however, is to shed additional light on the\nenvironmental impacts of ML, in order to help model developers and researchers make more informed choices as a\nfunction of their environmental footprint or energy usage.\nACKNOWLEDGMENTS\nWe thank Will Alpine, Nima Boscarino, Priya Donti, R√©gis Pierrard, David Rolnick, Roy Schwartz and Rajiv Shah for\ntheir useful feedback and suggestions.\n15\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nREFERENCES\n[1] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools. In EMNLP, Workshop SustaiNLP.\n[2] Jeff Barr. 2019. Amazon ec2 update‚Äìinf1 instances with AWS inferentia chips for high performance cost-effective inferencing. https://aws.amazon.\ncom/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n[3] Bing. 2019. Bing delivers its largest improvement in search experience using Azure GPUs. https://azure.microsoft.com/en-us/blog/bing-delivers-\nits-largest-improvement-in-search-experience-using-azure-gpus/\n[4] Bing. 2023. Confirmed: the new Bing runs on OpenAI‚Äôs GPT-4. https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-\nOpenAI%E2%80%99s-GPT-4\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.\n[6] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of\nGenerative AI Inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems . 1‚Äì7.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,\nAlbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu,\nVincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. https://doi.org/10.48550/ARXIV.2210.11416\n[9] Rishit Dagli and Ali Mustufa Shaikh. 2021. CPPE-5: Medical Personal Protective Equipment Dataset. arXiv:2112.09569 [cs.CV]\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. 2021. RedCaps: web-curated image-text data created by the people, for the people.\narXiv:2111.11431 [cs.CV]\n[11] Radosvet Desislavov, Fernando Mart√≠nez-Plumed, and Jos√© Hern√°ndez-Orallo. 2021. Compute and energy consumption trends in deep learning\ninference. arXiv preprint arXiv:2109.05472 (2021).\n[12] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency. 1877‚Äì1894.\n[13] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang. 2023. LLMCarbon: Modeling the end-to-end Carbon\nFootprint of Large Language Models. arXiv preprint arXiv:2309.14393 (2023).\n[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas\nMuennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.A framework for few-shot language\nmodel evaluation. https://doi.org/10.5281/zenodo.5371628\n[15] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive\nSummarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization . Association for Computational Linguistics, Hong Kong,\nChina, 70‚Äì79. https://doi.org/10.18653/v1/D19-5409\n[16] Google. 2019. Understanding searches better than ever before. https://blog.google/products/search/search-language-understanding-bert/\n[17] Google. 2023. Bard can now connect to your Google apps and services. https://blog.google/products/bard/google-bard-new-features-update-sept-\n2023/\n[18] Google. 2023. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/\n[19] Walid A Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant Shenoy. 2023. CarbonScaler: Leveraging Cloud Workload Elasticity for\nOptimizing Carbon-Efficiency. arXiv preprint arXiv:2302.08681 (2023).\n[20] Karl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching\nMachines to Read and Comprehend. In NeurIPS. 1693‚Äì1701. http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend\n[21] Ralph Hintemann and Simon Hinterholzer. 2022. Cloud computing drives the growth of the data center industry and its energy consumption. Data\ncenters 2022. ResearchGate (2022).\n[22] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-\nand-data-transmission-networks\n[23] Matt Gardner Johannes Welbl, Nelson F. Liu. 2017. Crowdsourcing Multiple Choice Science Questions. arXiv:1707.06209v1.\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.\nInternational Journal of Computer Vision 123 (2017), 32‚Äì73. https://doi.org/10.1007/s11263-016-0981-7\n[25] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images . Technical Report.\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700 (2019).\n16\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\n[27] Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, and Julien Launay. 2022. A Holistic Assessment of the Carbon Footprint\nof Noor, a Very Large Arabic Language Model. In Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large\nLanguage Models. Association for Computational Linguistics, virtual+Dublin, 84‚Äì94. https://doi.org/10.18653/v1/2022.bigscience-1.8\n[28] George Leopold. 2019. AWS to Offer NVIDIA‚Äôs T4 GPUs for AI Inferencing. www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll‚Äôar, and C Lawrence Zitnick. 2014. Microsoft\nCOCO: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13. Springer, 740‚Äì755.\n[30] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning.\narXiv preprint arXiv:2302.08476 (2023).\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n[32] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment\nAnalysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies . Association for\nComputational Linguistics, Portland, Oregon, USA, 142‚Äì150. http://www.aclweb.org/anthology/P11-1015\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong,\nHailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 (2022).\n[35] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don‚Äôt Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural\nNetworks for Extreme Summarization. ArXiv abs/1808.08745 (2018).\n[36] Will Oremus. 2023. AI chatbots lose money every time you use them. That is a problem. Washington Post (2023). https://www.washingtonpost.com/\ntechnology/2023/06/05/chatgpt-hidden-cost-gpu-compute/\n[37] Pedro Javier Ortiz Su‚Äôarez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource\ninfrastructures (Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019) , Piotr Ba≈Ñski,\nAdrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L\"ungen, and Caroline Iliadi (Eds.). Leibniz-Institut f\"ur\nDeutsche Sprache, Mannheim, 9 ‚Äì 16. https://doi.org/10.14618/ids-pub-9021\n[38] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual Name Tagging and Linking for 282\nLanguages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics, Vancouver, Canada, 1946‚Äì1958. https://doi.org/10.18653/v1/P17-1178\n[39] Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of\nthe ACL.\n[40] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. https://doi.org/10.48550/ARXIV.2204.05149\n[41] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer.arXiv e-prints (2019). arXiv:1910.10683\n[43] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don‚Äôt Know: Unanswerable Questions for SQuAD. arXiv:1806.03822 [cs.CL]\n[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text.\narXiv:1606.05250 (2016). arXiv:1606.05250\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\n(IJCV) 115, 3 (2015), 211‚Äì252. https://doi.org/10.1007/s11263-015-0816-y\n[46] Gustavo Santana. 2023. Stable Diffusion Prompts. https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts\n[47] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, Seattle, Washington, USA, 1631‚Äì1642. https://www.aclweb.org/anthology/D13-1170\n[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n[50] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.\nIn Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 . 142‚Äì147. https://www.aclweb.org/anthology/W03-0419\n[51] US Environmental Protection Agencyy. 2024. Greenhouse Gases Equivalencies Calculator - Calculations and References. https://www.epa.gov/\nenergy/greenhouse-gases-equivalencies-calculator-calculations-and-references\n17\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n[52] Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani,\nVictor Mustar, Helen Ngo, et al. 2022. Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement. arXiv preprint\narXiv:2210.01970 (2022).\n[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint arXiv:1905.00537 (2019).\n[54] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: A Large-Scale Prompt\nGallery Dataset for Text-to-Image Generative Models. arXiv:2210.14896 [cs] (2022). https://arxiv.org/abs/2210.14896\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan\nFuntowicz, et al. 2019. Huggingface‚Äôs transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n[56] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha\nLuccioni, Fran√ßois Yvon, et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n[57] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. ImageReward: Learning and Evaluating\nHuman Preferences for Text-to-Image Generation. arXiv:2304.05977 [cs.CV]\n[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies:\nTowards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV) .\n18\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nA FULL LIST OF TASK-SPECIFIC MODELS TESTED\nTask Models Task Models\nimage\nclassification\nmicrosoft/resnet-50\nmicrosoft/beit-base-patch16-224\ngoogle/vit-base-patch16-384\nfacebook/convnextv2-tiny-22k-384\nmicrosoft/resnet-18\ngoogle/mobilenet_v1_0.75_192\nfacebook/convnextv2-tiny-1k-224\ngoogle/vit-base-patch16-224\nquestion\nanswering\ndistilbert-base-uncased-distilled-squad\ndistilbert-base-cased-distilled-squad\ndeepset/roberta-base-squad2\nbert-large-uncased-whole-word-masking-finetuned-squad\ntimpal0l/mdeberta-v3-base-squad2\ndeepset/tinyroberta-squad2\ndeepset/electra-base-squad2\ndeepset/bert-large-uncased-whole-word-masking-squad2\nimage\ncaptioning\nnlpconnect/vit-gpt2-image-captioning\nSalesforce/blip-image-captioning-large\nSalesforce/blip-image-captioning-base\nmicrosoft/git-large-coco\nSalesforce/blip2-flan-t5-xl\nSalesforce/blip2-opt-2.7b\nydshieh/vit-gpt2-coco-en\nmicrosoft/git-base\nsummarization\nsshleifer/distilbart-xsum-12-6\nsshleifer/distilbart-cnn-12-6\npszemraj/led-large-book-summary\ngoogle/pegasus-xsum\ngoogle/pegasus-large\ngoogle/pegasus-multi_news\nfacebook/bart-large-cnn\nainize/bart-base-cnn\nimage\ngeneration\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1\nstabilityai/stable-diffusion-xl-base-1.0\nCompVis/stable-diffusion-v1-4\nprompthero/openjourney\ndreamlike-art/dreamlike-photoreal-2.0\nnota-ai/bk-sdm-tiny\nsegmind/tiny-sd\ntext\nclassification\ndistilbert-base-uncased-finetuned-sst-2-english\nnlptown/bert-base-multilingual-uncased-sentiment\ntwitter-roberta-base-sentiment-latest\ncardiffnlp/twitter-xlm-roberta-base-sentiment\nlvwerra/distilbert-imdb\nsiebert/sentiment-roberta-large-english\nfiniteautomata/bertweet-base-sentiment-analysis\nsbcBI/sentiment_analysis_mode\nmasked\nlanguage\nmodeling\nbert-base-uncased\nxlm-roberta-base\ndistilbert-base-uncased\nroberta-base\nalbert-base-v2\nbert-base-cased\nmicrosoft/deberta-base\nbert-base-multilingual-cased\ntext\ngeneration\ngpt2\nbigscience/bloom-560m\ndistilgpt2\nfacebook/opt-6.7b\nEleutherAI/gpt-neo-125m\ngpt2-medium\nfacebook/opt-1.3b\ngpt2-xl\nobject\ndetection\nfacebook/detr-resnet-50\nhustvl/yolos-tiny\njozhang97/deta-swin-large\nfacebook/detr-resnet-101\nhustvl/yolos-small\nSenseTime/deformable-detr\npolejowska/detr-r50-cd45rb-8ah-6l\npolejowska/detr-r50-cd45rb-1ah-6l\ntoken\nclassification\nQCRI/bert-base-multilingual-cased-pos-english\ndslim/bert-base-NER\ndslim/bert-large-NER\nJean-Baptiste/roberta-large-ner-english\noliverguhr/fullstop-punctuation-multilang-large\nBabelscape/wikineural-multilingual-ner\nml6team/keyphrase-extraction-distilbert-inspec\nobi/deid_roberta_i2b2\nTable 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.\n19\n\nACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nB MODEL EVALUATION\nFig. 7. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and summarization accuracy (y axis),\nwith dot size indicating the quantity of emissions.\nFig. 8. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and question answering accuracy (y axis),\nwith dot size indicating the quantity of emissions.\n20\n\n[Image page=20 idx=1 name=Im8.png] Size: 800x400, Data: 61696 bytes\n\n[Image page=20 idx=2 name=Im9.png] Size: 800x400, Data: 51502 bytes\n\nPower Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nmodel SST 2\n(acc)\nIMDB\n(acc)\nRotten\nTomatoes\n(acc)\nSciQ\n(acc)\nSQuAD\n(F1)\nSQuAD v2\n(F1, has\nanswer)\nSamSUM\n(ROUGE)\nXSum\n(ROUGE)\nCNN\n(ROUGE)\nbloomz-560m 0.92 0.94 0.85 0.92 0.43 0.21 0.23 0.15 0.10\nbloomz-1b7 0.94 0.97 0.93 0.96 0.50 0.25 0.26 0.16 0.18\nbloomz-3b 0.95 0.98 0.95 0.97 0.53 0.26 0.28 0.17 0.21\nbloomz-7b1 0.94 0.98 0.95 0.97 0.54 0.27 0.32 0.21 0.09\nflan-t5-xxl 0.96 0.97 0.92 0.72 0.98 0.49 0.30 0.37 0.23\nflan-t5-xl 0.96 0.97 0.93 0.66 0.97 0.49 0.49 0.38 0.24\nflan-t5-large 0.94 0.96 0.92 0.53 0.97 0.50 0.45 0.30 0.24\nflan-t5-base 0.93 0.95 0.88 0.61 0.95 0.48 0.46 0.32 0.23\ndistilbert-base-uncased\n-distilled-squad 0.44 0.87 0.86\ndistilbert-base-cased-\ndistilled-squad 0.46 0.87 0.87\ndeepset/roberta-base-squad2 0.48 0.93 0.83\nbert-large-uncased-whole-\nword-masking-finetuned-squad 0.48 0.93 0.84\ntimpal0l/mdeberta-v3-\nbase-squad2 0.46 0.91 0.90\ndeepset/tinyroberta-squad2 0.45 0.98 0.91\ndeepset/electra-base-squad2 0.48 0.89 0.82\ndeepset/bert-large-uncased-\nwhole-word-masking-squad2 0.46 0.92 0.92\nsshleifer/distilbart-xsum-12-6 0.20 0.45 0.23\nsshleifer/distilbart-cnn-12-6 0.29 0.21 0.44\npszemraj/led-large-\nbook-summary 0.33 0.16 0.33\npegasus-xsum 0.22 0.22 0.22\npegasus-large 0.27 0.17 0.34\npegasus-multi_news 0.12 0.16 0.29\nfacebook/bart-large-cnn 0.32 0.21 0.44\nainize/bart-base-cnn 0.27 0.16 0.26\ndistilbert-base-uncased-\nfinetuned-sst-2-english 0.99 0.88 0.90\nnlptown/bert-base-\nmultilingual-uncased-sentiment 0.75 0.85 0.73\ntwitter-roberta-base-\nsentiment-latest 0.82 0.80 0.77\ncardiffnlp/twitter-xlm-roberta-\nbase-sentiment 0.79 0.71 0.74\nlvwerra/distilbert-imdb 0.88 0.93 0.82\nsiebert/sentiment-roberta-\nlarge-english 0.92 0.92 0.92\nfiniteautomata/bertweet-\nbase-sentiment-analysis 0.82 0.72 0.77\nsbcBI/sentiment_analysis_model 0.81 0.75 0.76\nTable 7. Full performance metrics for the 32 models (24 finetuned, 8 multi-purpose) that we evaluated as part of our study.\n21", "metadata": {"url": "https://arxiv.org/pdf/2311.16863", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Power Hungry Processing:\n Watts\n Driving the Cost of AI Deployment?\nALEXANDRA SASHA LUCCIONI and YACINE JERNITE, Hugging Face, Canada/USA\nEMMA STRUBELL, Carnegie Mellon University, Allen Institute for AI, USA\nFig. 1. The tasks examined in our study and the average quantity of carbon emissions they produced (in g of ùê∂ùëÇ 2ùëíùëû) for 1,000 queries.\nN.B. The y axis is in logarithmic scale.\nRecent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising\na unified approach to building machine learning (ML) models into technology. However, this ambition of ‚Äúgenerality‚Äù comes at a steep\ncost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we\npropose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific\n(i.e. finetuned models that carry out a single task) and ‚Äògeneral-purpose‚Äô models, (i.e. those trained for multiple tasks). We measure\ndeployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using\nthese models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems\nfor a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current\ntrend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against\nincreased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out\nfurther exploration and analysis.\nCCS Concepts: ‚Ä¢Computing methodologies ‚Üí Machine learning; Neural networks; ‚Ä¢Hardware ‚Üí Impact on the environ-\nment; Power estimation and optimization .\nACM Reference Format:\nAlexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2024. Power Hungry Processing:\n Watts\n Driving the Cost of AI\nDeployment?. In ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT ‚Äô24), June 3‚Äì6, 2024, Rio de Janeiro, Brazil.\nACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/3630106.3658542\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party\ncomponents of this work must be honored. For all other uses, contact the owner/author(s).\n¬© 2024 Copyright held by the owner/author(s).\nManuscript submitted to ACM\n1\narXiv:2311.16863v3  [cs.LG]  15 Oct 2024", "sentences": [{"text": "Power Hungry Processing:\n Watts\n Driving the Cost of AI Deployment?", "metadata": {}}, {"text": "ALEXANDRA SASHA LUCCIONI and YACINE JERNITE, Hugging Face, Canada/USA\nEMMA STRUBELL, Carnegie Mellon University, Allen Institute for AI, USA\nFig.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "The tasks examined in our study and the average quantity of carbon emissions they produced (in g of ùê∂ùëÇ 2ùëíùëû) for 1,000 queries.", "metadata": {}}, {"text": "N.B.", "metadata": {}}, {"text": "The y axis is in logarithmic scale.", "metadata": {}}, {"text": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising\na unified approach to building machine learning (ML) models into technology.", "metadata": {}}, {"text": "However, this ambition of ‚Äúgenerality‚Äù comes at a steep\ncost to the environment, given the amount of energy these systems require and the amount of carbon that they emit.", "metadata": {}}, {"text": "In this work, we\npropose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific\n(i.e.", "metadata": {}}, {"text": "finetuned models that carry out a single task) and ‚Äògeneral-purpose‚Äô models, (i.e.", "metadata": {}}, {"text": "those trained for multiple tasks).", "metadata": {}}, {"text": "We measure\ndeployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using\nthese models.", "metadata": {}}, {"text": "We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems\nfor a variety of tasks, even when controlling for the number of model parameters.", "metadata": {}}, {"text": "We conclude with a discussion around the current\ntrend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against\nincreased costs in terms of energy and emissions.", "metadata": {}}, {"text": "All the data from our study can be accessed via an interactive demo to carry out\nfurther exploration and analysis.", "metadata": {}}, {"text": "CCS Concepts: ‚Ä¢Computing methodologies ‚Üí Machine learning;", "metadata": {}}, {"text": "Neural networks;", "metadata": {}}, {"text": "‚Ä¢Hardware ‚Üí Impact on the environ-\nment;", "metadata": {}}, {"text": "Power estimation and optimization .", "metadata": {}}, {"text": "ACM Reference Format:\nAlexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Power Hungry Processing:\n Watts\n Driving the Cost of AI\nDeployment?.", "metadata": {}}, {"text": "In ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT ‚Äô24), June 3‚Äì6, 2024, Rio de Janeiro, Brazil.", "metadata": {}}, {"text": "ACM, New York, NY, USA, 21 pages.", "metadata": {}}, {"text": "https://doi.org/10.1145/3630106.3658542\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.", "metadata": {}}, {"text": "Copyrights for third-party\ncomponents of this work must be honored.", "metadata": {}}, {"text": "For all other uses, contact the owner/author(s).", "metadata": {}}, {"text": "¬© 2024 Copyright held by the owner/author(s).", "metadata": {}}, {"text": "Manuscript submitted to ACM\n1\narXiv:2311.16863v3  [cs.LG]  15 Oct 2024", "metadata": {}}], "metadata": {"page": 1}}, {"text": "[Image page=1 idx=1 name=Im2.png] Size: 838x407, Data: 52874 bytes", "sentences": [{"text": "[Image page=1 idx=1 name=Im2.png] Size: 838x407, Data: 52874 bytes", "metadata": {}}], "metadata": {"page": 1, "image_index": 1, "image_name": "Im2.png", "image_width": 838, "image_height": 407, "attachment_type": "image", "has_image_data": true, "image_data_size": 52874}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n1 INTRODUCTION\nUnderstanding the environmental impacts of different industries is an important first step towards developing effective\nstrategies to mitigate those impacts. For newer industries such as information and communication technologies (ICT)\nof which Artificial Intelligence (AI) and Machine Learning (ML) are considered to be a part of, more work is needed to\nunderstand the extent of their environmental impacts and the factors that influence it. Between 2017 and 2021, the\nelectricity used by Meta, Amazon, Microsoft, and Google, the main providers of commercially-available cloud compute,\nmore than doubled [22]. According to the most recent figures available, global data centre electricity consumption\nhas grown by 20-40% annually in recent years, reaching 1-1.3% of global electricity demand and contributing 1% of\nenergy-related greenhouse gas emissions in 2022 [21]. However the contribution of the AI sector specifically towards\nthese figures is unclear.\nRecent work documenting the environmental impacts of ML has focused largely on quantifying the operational\nenergy and carbon required to perform the training phase of the ML model life cycle [12, 30, 41, 49] due to the relative\nease of measuring per-model energy use for that phase and the impressive quantity of energy required to perform\na single training run [ 41, 49]. Yet, other phases of the ML model life cycle, such as inference, stand to impact the\nenvironment just as much, or more, than training due to the computational resources required to deploy modern\nmodels at scale. While inference on a single example requires much less computation than that required to train the\nsame model, inference happens far more frequently than model training ‚Äî as many as billions of times a day for a\nmodel powering a popular user-facing product such as Google Translate.1 Yet, in-depth work quantifying the costs of\nmodel inference and deployment is limited and their environmental impacts, in terms of energy and carbon as well as\nwater and mining of rare earth minerals, have yet to be estimated. According to AWS, the largest global cloud provider,\ninference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by\nMeta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the\nremainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed\n60% of its ML energy use to inference, compared to 40% for training [40]. Given the increasing ubiquity of AI model\ndeployment, it is crucial to go beyond these high-level statistics to get a better idea of the energy requirements and\ncarbon emissions of model inference for different models and tasks. In particular, looking at inference rather than\ntraining leads to drastically different conclusions when considering the multi-purpose (or ‚Äúgeneral-purpose‚Äù) aspect\nspecifically. Training a single model for multiple tasks can indeed be more energy-efficient when considering training\ncosts only, but these gains can easily be lost and even reversed over the course of the model‚Äôs lifetime, given how much\ninference is carried out when these models are deployed in user-facing applications like chat and web search.\nTo help shed light on this issue, we perform an extensive study measuring the amount of energy required to deploy\nvarious ML models and architectures, including large language models (LLMs)- as such, our study is, to our knowledge,\nthe first to focus solely on the inference phase of the ML model life cycle. We study 88 models across 10 tasks and 30\ndatasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality,\nmodel size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency. We\nidentify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and\ntasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and\nensuing carbon emissions. By painting a more detailed picture of widely varying energy requirements for ML model\n1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion\nqueries to the model per day. Source: https://blog.google/products/translate/ten-years-of-google-translate/\n2", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n1 INTRODUCTION\nUnderstanding the environmental impacts of different industries is an important first step towards developing effective\nstrategies to mitigate those impacts.", "metadata": {}}, {"text": "For newer industries such as information and communication technologies (ICT)\nof which Artificial Intelligence (AI) and Machine Learning (ML) are considered to be a part of, more work is needed to\nunderstand the extent of their environmental impacts and the factors that influence it.", "metadata": {}}, {"text": "Between 2017 and 2021, the\nelectricity used by Meta, Amazon, Microsoft, and Google, the main providers of commercially-available cloud compute,\nmore than doubled [22].", "metadata": {}}, {"text": "According to the most recent figures available, global data centre electricity consumption\nhas grown by 20-40% annually in recent years, reaching 1-1.3% of global electricity demand and contributing 1% of\nenergy-related greenhouse gas emissions in 2022 [21].", "metadata": {}}, {"text": "However the contribution of the AI sector specifically towards\nthese figures is unclear.", "metadata": {}}, {"text": "Recent work documenting the environmental impacts of ML has focused largely on quantifying the operational\nenergy and carbon required to perform the training phase of the ML model life cycle [12, 30, 41, 49] due to the relative\nease of measuring per-model energy use for that phase and the impressive quantity of energy required to perform\na single training run [ 41, 49].", "metadata": {}}, {"text": "Yet, other phases of the ML model life cycle, such as inference, stand to impact the\nenvironment just as much, or more, than training due to the computational resources required to deploy modern\nmodels at scale.", "metadata": {}}, {"text": "While inference on a single example requires much less computation than that required to train the\nsame model, inference happens far more frequently than model training ‚Äî as many as billions of times a day for a\nmodel powering a popular user-facing product such as Google Translate.1 Yet, in-depth work quantifying the costs of\nmodel inference and deployment is limited and their environmental impacts, in terms of energy and carbon as well as\nwater and mining of rare earth minerals, have yet to be estimated.", "metadata": {}}, {"text": "According to AWS, the largest global cloud provider,\ninference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by\nMeta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the\nremainder produced by data management, storage, and training [57];", "metadata": {}}, {"text": "similarly, a 2022 study from Google attributed\n60% of its ML energy use to inference, compared to 40% for training [40].", "metadata": {}}, {"text": "Given the increasing ubiquity of AI model\ndeployment, it is crucial to go beyond these high-level statistics to get a better idea of the energy requirements and\ncarbon emissions of model inference for different models and tasks.", "metadata": {}}, {"text": "In particular, looking at inference rather than\ntraining leads to drastically different conclusions when considering the multi-purpose (or ‚Äúgeneral-purpose‚Äù) aspect\nspecifically.", "metadata": {}}, {"text": "Training a single model for multiple tasks can indeed be more energy-efficient when considering training\ncosts only, but these gains can easily be lost and even reversed over the course of the model‚Äôs lifetime, given how much\ninference is carried out when these models are deployed in user-facing applications like chat and web search.", "metadata": {}}, {"text": "To help shed light on this issue, we perform an extensive study measuring the amount of energy required to deploy\nvarious ML models and architectures, including large language models (LLMs)- as such, our study is, to our knowledge,\nthe first to focus solely on the inference phase of the ML model life cycle.", "metadata": {}}, {"text": "We study 88 models across 10 tasks and 30\ndatasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality,\nmodel size, architecture, and learning paradigm (i.e.", "metadata": {}}, {"text": "task-specific or multi-task/multi-purpose) on energy efficiency.", "metadata": {}}, {"text": "We\nidentify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and\ntasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and\nensuing carbon emissions.", "metadata": {}}, {"text": "By painting a more detailed picture of widely varying energy requirements for ML model\n1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion\nqueries to the model per day.", "metadata": {}}, {"text": "Source: https://blog.google/products/translate/ten-years-of-google-translate/\n2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ninference, we hope this study can be useful for practitioners to better understand accuracy-efficiency trade-offs across\ntasks and models, as well as enabling better estimates, and projections and policy decisions at the sector level.\n2 PREVIOUS WORK\nEstimating the energy and emissions of ML models has remains a relatively under-explored topic, albeit one that\nhas been gathering traction since Strubell et al‚Äôs seminal article quantifying the energy and carbon emissions of a\nvariety of then-large NLP models [2019]. Since then, most studies have focused on estimating the energy consumed and\ncarbon emitted during the training phase of neural networks ‚Äì this includes studies by Patterson et al. [2022, 2021],\nwho compared different models and analyzed factors influencing their emissions. There have also been studies of\nspecific model architectures, e.g. BLOOM [ 31] and Nour [27], which carried out in-depth analyses of the different\nsteps in the models‚Äô life cycle and their relative contribution towards the final quantity of carbon emissions. Given the\nincreasing deployment of ML models in the cloud, several studies have therefore looked at cloud-specific ways to reduce\nthe emissions of ML models such as delayed scheduling, workload elasticity and choosing the least carbon-intensive\nelectricity available Chien et al. [6], Dodge et al. [12], Hanafy et al. [19].\nDespite these empirical studies, there is currently a lack of standardized methodology for quantifying and comparing\nthe energy consumption and carbon emissions of ML models. There are several tools that exist, such as Code Carbon [47],\nMLCO2 [26] and LLMCarbon [13], all of which adopt different approaches and output different results (see [ 1] for\na detailed comparison). It is therefore difficult to systematically compare the carbon footprints of different models.\nExisting tools and studies have also largely focused on the dynamic power consumption (i.e. the electricity necessary for\npowering hardware) and its resulting emissions. However, there have been several proposals to also take into account\nthe embodied emissions of ML models (i.e. the emissions that can be attributed to the manufacturing of computing\nequipment) into carbon emissions estimates. This has been impeded by a lack of transparency from the designers\nof common computing hardware such as GPUs, although recent estimates have revealed that the embodied carbon\nfootprint of an LLM trained and deployed on Meta‚Äôs compute cluster constitutes up to 50% of its carbon footprint [57].\nWhile the majority of existing work has been focused on ML model training given that it is a more tractable part of\nthe model life cycle (i.e. it is most often carried out over a set period of time on a specific compute instance), model\ninference has started to also become the subject of scholarship [6, 11]. Luccioni et al. ‚Äôs study of BLOOM was the first\nof its kind to look at the specific energy costs related to deploying an LLM [ 31] and found that, over time, this can\nrepresent a significant portion of a model‚Äôs overall carbon footprint.\nThe current study further pursues this line of work, delving deeper into the inference stage of ML models, the energy\nit consumes and the carbon it emits. By testing a variety of architectures on different tasks and datasets, we aim to gain\na better understanding of the degree of variance that can be observed and how seemingly small user choices can result\nin large differences in model‚Äôs environmental impacts.\n3 METHODOLOGY\nAs stated above, our study focuses on the inference (i.e. deployment) stage in the model life cycle, aiming to address the\nknowledge gaps that currently exist with regards to its energy consumption and ensuing emissions. We describe how\nwe chose the tasks, datasets and models in the sections below, and present the results of our analysis in Section 4.\n3", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ninference, we hope this study can be useful for practitioners to better understand accuracy-efficiency trade-offs across\ntasks and models, as well as enabling better estimates, and projections and policy decisions at the sector level.", "metadata": {}}, {"text": "2 PREVIOUS WORK\nEstimating the energy and emissions of ML models has remains a relatively under-explored topic, albeit one that\nhas been gathering traction since Strubell et al‚Äôs seminal article quantifying the energy and carbon emissions of a\nvariety of then-large NLP models [2019].", "metadata": {}}, {"text": "Since then, most studies have focused on estimating the energy consumed and\ncarbon emitted during the training phase of neural networks ‚Äì this includes studies by Patterson et al.", "metadata": {}}, {"text": "[2022, 2021],\nwho compared different models and analyzed factors influencing their emissions.", "metadata": {}}, {"text": "There have also been studies of\nspecific model architectures, e.g.", "metadata": {}}, {"text": "BLOOM [ 31] and Nour [27], which carried out in-depth analyses of the different\nsteps in the models‚Äô life cycle and their relative contribution towards the final quantity of carbon emissions.", "metadata": {}}, {"text": "Given the\nincreasing deployment of ML models in the cloud, several studies have therefore looked at cloud-specific ways to reduce\nthe emissions of ML models such as delayed scheduling, workload elasticity and choosing the least carbon-intensive\nelectricity available Chien et al.", "metadata": {}}, {"text": "[6], Dodge et al.", "metadata": {}}, {"text": "[12], Hanafy et al.", "metadata": {}}, {"text": "[19].", "metadata": {}}, {"text": "Despite these empirical studies, there is currently a lack of standardized methodology for quantifying and comparing\nthe energy consumption and carbon emissions of ML models.", "metadata": {}}, {"text": "There are several tools that exist, such as Code Carbon [47],\nMLCO2 [26] and LLMCarbon [13], all of which adopt different approaches and output different results (see [ 1] for\na detailed comparison).", "metadata": {}}, {"text": "It is therefore difficult to systematically compare the carbon footprints of different models.", "metadata": {}}, {"text": "Existing tools and studies have also largely focused on the dynamic power consumption (i.e.", "metadata": {}}, {"text": "the electricity necessary for\npowering hardware) and its resulting emissions.", "metadata": {}}, {"text": "However, there have been several proposals to also take into account\nthe embodied emissions of ML models (i.e.", "metadata": {}}, {"text": "the emissions that can be attributed to the manufacturing of computing\nequipment) into carbon emissions estimates.", "metadata": {}}, {"text": "This has been impeded by a lack of transparency from the designers\nof common computing hardware such as GPUs, although recent estimates have revealed that the embodied carbon\nfootprint of an LLM trained and deployed on Meta‚Äôs compute cluster constitutes up to 50% of its carbon footprint [57].", "metadata": {}}, {"text": "While the majority of existing work has been focused on ML model training given that it is a more tractable part of\nthe model life cycle (i.e.", "metadata": {}}, {"text": "it is most often carried out over a set period of time on a specific compute instance), model\ninference has started to also become the subject of scholarship [6, 11].", "metadata": {}}, {"text": "Luccioni et al.", "metadata": {}}, {"text": "‚Äôs study of BLOOM was the first\nof its kind to look at the specific energy costs related to deploying an LLM [ 31] and found that, over time, this can\nrepresent a significant portion of a model‚Äôs overall carbon footprint.", "metadata": {}}, {"text": "The current study further pursues this line of work, delving deeper into the inference stage of ML models, the energy\nit consumes and the carbon it emits.", "metadata": {}}, {"text": "By testing a variety of architectures on different tasks and datasets, we aim to gain\na better understanding of the degree of variance that can be observed and how seemingly small user choices can result\nin large differences in model‚Äôs environmental impacts.", "metadata": {}}, {"text": "3 METHODOLOGY\nAs stated above, our study focuses on the inference (i.e.", "metadata": {}}, {"text": "deployment) stage in the model life cycle, aiming to address the\nknowledge gaps that currently exist with regards to its energy consumption and ensuing emissions.", "metadata": {}}, {"text": "We describe how\nwe chose the tasks, datasets and models in the sections below, and present the results of our analysis in Section 4.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n3.1 Task and dataset selection\nAs the starting point of our study, we chose 10 ML tasks from 5 different modalities:Text-to-category (text classifica-\ntion, token classification, extractive question answering), Text-to-text (masked language modeling, text generation,\nsummarization), Image-to-category (image classification, object detection), Image-to-text (image captioning) and\nText-to-image (image generation). These tasks were chosen because they are common in both Natural Language\nProcessing and Computer Vision, allowing us to explore multiple modalities, and include several multimodal tasks (i.e.\nimage captioning and image generation), allowing us to explore the nexus between several modalities as well. To test\neach of the tasks listed above, we chose three of the most downloaded datasets from the Hugging Face Hub. We present\nthe tasks and their corresponding datasets in Table 1.\nTask Datasets Task Datasets\nimage\nclassification\nCIFAR 10 [25]\nCIFAR 100 [25]\nImageNet 1K [45]\nquestion\nanswering\nSQuAD[44]\nSQuAD v2 [43]\nSciQ [23]\nimage\ncaptioning\nVisual Genome [24]\nRedCaps [10]\nCOCO [29]\nsummarization\nSAMSum [15]\nCNN-Daily Mail [20]\nXSum [35]\nimage\ngeneration\nDiffusionDB [54]\nImageReward [58]\nSD Prompts [46]\ntext\nclassification\nIMDB [32]\nRotten Tomatoes [39]\nSST 2 [48]\nmasked\nlanguage\nmodeling\nBookCorpus [59]\nC4 [42]\nOSCAR [37]\ntext\ngeneration\nWikiText [33]\nBookCorpus [59]\nOSCAR [37]\nobject\ndetection\nVisual Genome [24]\nCPPE-5 [9]\nCOCO [29]\ntoken\nclassification\nReCoRD [53]\nWikiANN [38]\nCoNLL 2003 [50]\nTable 1. A list of the tasks and datasets used in our study.\n3.2 Models\nTo be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained\nor finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or\nmulti-task models, to allow comparisons both for different architectures on a given task and between tasks for the same\narchitecture.\nTask-specific Models. For all of the tasks listed above, we selected the 8 most popular models from the HuggingFace\nHub (by number of downloads) 2 - we present the full list of model identifiers in Table 6 in the Supplementary Materials.\nFor each model, we ran 1,000 inferences for each of the 3 datasets from the task it was trained for (listed in Table 1),\nusing the Transformers [55] library. We ran each set of inferences 10 times to ensure statistical significance of our\nmeasurements. We set up the inferences sequentially ‚Äì i.e., without batching ‚Äì in order to reflect the variability of\nmodel deployment in situ, which can make it difficult to batch model inputs.\n2We were obliged to discard some models, e.g. if they were trained on another language or if the specific task they were fine-tuned for was not compatible\nwith any of the datasets selected.\n4", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n3.1 Task and dataset selection\nAs the starting point of our study, we chose 10 ML tasks from 5 different modalities:Text-to-category (text classifica-\ntion, token classification, extractive question answering), Text-to-text (masked language modeling, text generation,\nsummarization), Image-to-category (image classification, object detection), Image-to-text (image captioning) and\nText-to-image (image generation).", "metadata": {}}, {"text": "These tasks were chosen because they are common in both Natural Language\nProcessing and Computer Vision, allowing us to explore multiple modalities, and include several multimodal tasks (i.e.", "metadata": {}}, {"text": "image captioning and image generation), allowing us to explore the nexus between several modalities as well.", "metadata": {}}, {"text": "To test\neach of the tasks listed above, we chose three of the most downloaded datasets from the Hugging Face Hub.", "metadata": {}}, {"text": "We present\nthe tasks and their corresponding datasets in Table 1.", "metadata": {}}, {"text": "Task Datasets Task Datasets\nimage\nclassification\nCIFAR 10 [25]\nCIFAR 100 [25]\nImageNet 1K [45]\nquestion\nanswering\nSQuAD[44]\nSQuAD v2 [43]\nSciQ [23]\nimage\ncaptioning\nVisual Genome [24]\nRedCaps [10]\nCOCO [29]\nsummarization\nSAMSum [15]\nCNN-Daily Mail [20]\nXSum [35]\nimage\ngeneration\nDiffusionDB [54]\nImageReward [58]\nSD Prompts [46]\ntext\nclassification\nIMDB [32]\nRotten Tomatoes [39]\nSST 2 [48]\nmasked\nlanguage\nmodeling\nBookCorpus [59]\nC4 [42]\nOSCAR [37]\ntext\ngeneration\nWikiText [33]\nBookCorpus [59]\nOSCAR [37]\nobject\ndetection\nVisual Genome [24]\nCPPE-5 [9]\nCOCO [29]\ntoken\nclassification\nReCoRD [53]\nWikiANN [38]\nCoNLL 2003 [50]\nTable 1.", "metadata": {}}, {"text": "A list of the tasks and datasets used in our study.", "metadata": {}}, {"text": "3.2 Models\nTo be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained\nor finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or\nmulti-task models, to allow comparisons both for different architectures on a given task and between tasks for the same\narchitecture.", "metadata": {}}, {"text": "Task-specific Models.", "metadata": {}}, {"text": "For all of the tasks listed above, we selected the 8 most popular models from the HuggingFace\nHub (by number of downloads) 2 - we present the full list of model identifiers in Table 6 in the Supplementary Materials.", "metadata": {}}, {"text": "For each model, we ran 1,000 inferences for each of the 3 datasets from the task it was trained for (listed in Table 1),\nusing the Transformers [55] library.", "metadata": {}}, {"text": "We ran each set of inferences 10 times to ensure statistical significance of our\nmeasurements.", "metadata": {}}, {"text": "We set up the inferences sequentially ‚Äì i.e., without batching ‚Äì in order to reflect the variability of\nmodel deployment in situ, which can make it difficult to batch model inputs.", "metadata": {}}, {"text": "2We were obliged to discard some models, e.g.", "metadata": {}}, {"text": "if they were trained on another language or if the specific task they were fine-tuned for was not compatible\nwith any of the datasets selected.", "metadata": {}}, {"text": "4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nMulti-Purpose Models. In addition to the task-specific models listed above, we also selected 8 multi-purpose models\nto analyze on different tasks ‚Äì models that were specifically trained to perform well in various different application\nsettings. We chose 4 sequence-to-sequence models of different sizes from the Flan-T5 family [8] (base, large, xl and\nxxl) and 4 decoder-only models from the BLOOMz family [34]: BLOOMz-560M, BLOOMz-1B, BLOOMz-3B and BLOOMz-7B.\nWe tested these on a subset of the tasks to allow a comparison of multi-purpose generative models with individual\ntask-specific systems in terms of their energy consumption and emissions: question answering, text classification and\nsummarization. We selected these three tasks because we were able to find a set of models that were capable of carrying\nthem out with a unified model architecture (which wasn‚Äôt possible for all tasks, especially ones that involved multiple\nmodalities.) We prompted these 8 models in a zero-shot setting that was constant across models, e.g. \"Summarize the\nfollowing text: [text]. Summary:\" on the same 1,000 samples as the fine-tuned models, also repeating each experiment\nten times to measure the significance of results.\nWe ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and\nused the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference 3.\nGiven that all of our experiments were run in the same compute region (AWS‚Äôsus-west-2), which is based in Oregon\nand has an average carbon intensity of 297.6 grams of ùê∂ùëÇ2ùëíùëû per kWh4, this means that both the energy consumed\nduring inference and the carbon emitted are correlated; we will therefore plot one or the other depending on which\naspect of our results we are discussing. While the energy consumed during inference will remain similar for models\ndeployed on A100 GPUs in other compute regions, the carbon emissions will vary depending on the source of energy\nused in the region ‚Äì it is therefore helpful to report both energy and carbon separately to allow for meaningful\ncomparisons across regions and hardware. We provide all the code used for our experiments in our GitHub repository,\nalongside the logs produced by Code Carbon, which not only provides the total energy consumed but also a more\nfine-grained breakdown by hardware component (GPU, CPU and RAM), which can be used to carry out further analyses.\nIn total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg\nof ùê∂ùëÇ2ùëíùëû.\n4 RESULTS\nWe present our results in the subsections below: in Section 4.1, we analyze the range of energy used and carbon emitted\nfor each task for task-specific models. In Section 4.2, we shift our focus to multi-purpose (i.e. ‚Äòzero-shot‚Äò models), looking\nat the variation between different sizes and architectures of multi-purpose models and the difference in the energy\nconsumption and emissions between task-specific and multi-purpose models. In Section 4.3, we carry out a comparison\nbetween model training and inference costs for models of different sizes, calculating when parity is reached.\n4.1 Task-specific model analysis\nWe start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety\nof tasks. Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for\n1,000 inferences and its standard deviation. We can see that classification tasks for both images and text are on the\nlower end of the spectrum in terms of emissions (ranging between 0.002 and 0.007 kWh for 1,000 inferences), whereas\n3While all of our experiments were run on a single GPU, the idle power usage of the other GPUs is also reflected in the numbers that we report in our\nresults.\n4The carbon intensity of an energy grid is measured in ùê∂ùëÇ 2ùëíùëû, and not in ùê∂ùëÇ 2 specifically, because the different greenhouse gases that are generated\nduring electricity generation are reduced to a common denominator, that of carbon dioxide, or ùê∂ùëÇ 2. For a more in-depth discussion of how this is done,\nsee Luccioni and Hernandez-Garcia [2023].\n5", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nMulti-Purpose Models.", "metadata": {}}, {"text": "In addition to the task-specific models listed above, we also selected 8 multi-purpose models\nto analyze on different tasks ‚Äì models that were specifically trained to perform well in various different application\nsettings.", "metadata": {}}, {"text": "We chose 4 sequence-to-sequence models of different sizes from the Flan-T5 family [8] (base, large, xl and\nxxl) and 4 decoder-only models from the BLOOMz family [34]: BLOOMz-560M, BLOOMz-1B, BLOOMz-3B and BLOOMz-7B.", "metadata": {}}, {"text": "We tested these on a subset of the tasks to allow a comparison of multi-purpose generative models with individual\ntask-specific systems in terms of their energy consumption and emissions: question answering, text classification and\nsummarization.", "metadata": {}}, {"text": "We selected these three tasks because we were able to find a set of models that were capable of carrying\nthem out with a unified model architecture (which wasn‚Äôt possible for all tasks, especially ones that involved multiple\nmodalities.) We prompted these 8 models in a zero-shot setting that was constant across models, e.g.", "metadata": {}}, {"text": "\"Summarize the\nfollowing text: [text].", "metadata": {}}, {"text": "Summary:\" on the same 1,000 samples as the fine-tuned models, also repeating each experiment\nten times to measure the significance of results.", "metadata": {}}, {"text": "We ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and\nused the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference 3.", "metadata": {}}, {"text": "Given that all of our experiments were run in the same compute region (AWS‚Äôsus-west-2), which is based in Oregon\nand has an average carbon intensity of 297.6 grams of ùê∂ùëÇ2ùëíùëû per kWh4, this means that both the energy consumed\nduring inference and the carbon emitted are correlated;", "metadata": {}}, {"text": "we will therefore plot one or the other depending on which\naspect of our results we are discussing.", "metadata": {}}, {"text": "While the energy consumed during inference will remain similar for models\ndeployed on A100 GPUs in other compute regions, the carbon emissions will vary depending on the source of energy\nused in the region ‚Äì it is therefore helpful to report both energy and carbon separately to allow for meaningful\ncomparisons across regions and hardware.", "metadata": {}}, {"text": "We provide all the code used for our experiments in our GitHub repository,\nalongside the logs produced by Code Carbon, which not only provides the total energy consumed but also a more\nfine-grained breakdown by hardware component (GPU, CPU and RAM), which can be used to carry out further analyses.", "metadata": {}}, {"text": "In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg\nof ùê∂ùëÇ2ùëíùëû.", "metadata": {}}, {"text": "4 RESULTS\nWe present our results in the subsections below: in Section 4.1, we analyze the range of energy used and carbon emitted\nfor each task for task-specific models.", "metadata": {}}, {"text": "In Section 4.2, we shift our focus to multi-purpose (i.e.", "metadata": {}}, {"text": "‚Äòzero-shot‚Äò models), looking\nat the variation between different sizes and architectures of multi-purpose models and the difference in the energy\nconsumption and emissions between task-specific and multi-purpose models.", "metadata": {}}, {"text": "In Section 4.3, we carry out a comparison\nbetween model training and inference costs for models of different sizes, calculating when parity is reached.", "metadata": {}}, {"text": "4.1 Task-specific model analysis\nWe start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety\nof tasks.", "metadata": {}}, {"text": "Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for\n1,000 inferences and its standard deviation.", "metadata": {}}, {"text": "We can see that classification tasks for both images and text are on the\nlower end of the spectrum in terms of emissions (ranging between 0.002 and 0.007 kWh for 1,000 inferences), whereas\n3While all of our experiments were run on a single GPU, the idle power usage of the other GPUs is also reflected in the numbers that we report in our\nresults.", "metadata": {}}, {"text": "4The carbon intensity of an energy grid is measured in ùê∂ùëÇ 2ùëíùëû, and not in ùê∂ùëÇ 2 specifically, because the different greenhouse gases that are generated\nduring electricity generation are reduced to a common denominator, that of carbon dioxide, or ùê∂ùëÇ 2.", "metadata": {}}, {"text": "For a more in-depth discussion of how this is done,\nsee Luccioni and Hernandez-Garcia [2023].", "metadata": {}}, {"text": "5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\ngenerative tasks such as text generation and summarization use, on average, over 10 times more energy for the same\nnumber of inferences (around 0.05 kWh for 1,000 inferences), and multimodal tasks such as image captioning and image\ngeneration are on the highest end of the spectrum (0.06-2.9 kWh for 1,000 inferences). Text-based tasks are, all things\nconsidered, more energy-efficient than image-based tasks, with image classification requiring less energy (median\nof 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH)\nrequiring more than text classification (0.0023 kWh). For comparison, charging the average smartphone requires 0.022\nkWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full\nsmartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as\n522 smartphone charges (11.49 kWh), or around half a charge per image generation 5, although there is also a large\nvariation between image generation models, depending on the size of image that they generate.\ninference energy (kWh)\ntask mean std\ntext classification 0.002 0.001\nextractive QA 0.003 0.001\nmasked language modeling 0.003 0.001\ntoken classification 0.004 0.002\nimage classification 0.007 0.001\nobject detection 0.038 0.02\ntext generation 0.047 0.03\nsummarization 0.049 0.01\nimage captioning 0.063 0.02\nimage generation 2.907 3.31\nTable 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.\nWe can also observe that there is a large variation in the amount of energy used, from the least energy-intensive\ntask, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one,\nimage generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study\ncan vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences. Intuitively,\nthis is coherent given the decision space that different types of models have - from a binary classification task such as\nsentiment analysis (which can only output, for instance, a 0 for negative sentiment and a 1 for positive) to an entire\nvocabulary for text generation and summarization models. The length of text generated also impacts energy usage: on\naverage, text generation uses 15 times more energy than masked language modeling, which makes sense given that the\nmasked language modeling task only generates a single token, whereas in our setup the text generation task generates\n10 new tokens for each input text, with the length of the input text rising as new tokens are generated, since each\nsequence of tokens gets fed back into the model to generate subsequent tokens. Finally, for image-based tasks, the level\nof abstraction is lower and the decision space is larger given that they generate raw pixels as opposed to tokens for text,\nmaking image-based tasks more energy intensive than text based ones, e.g. image classification uses over 3 times more\nenergy than text classification (0.007 vs. 0.002 kWh) and image generation uses, on average, over 60 times more energy\nthan text generation (0.047 vs. 2.9 kWh).\n5Before January 2024, the EPA website estimated a smartphone charge to consume 0.012 kWh of energy, which was the number used for comparisons in\nan earlier version of this study.\n6", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\ngenerative tasks such as text generation and summarization use, on average, over 10 times more energy for the same\nnumber of inferences (around 0.05 kWh for 1,000 inferences), and multimodal tasks such as image captioning and image\ngeneration are on the highest end of the spectrum (0.06-2.9 kWh for 1,000 inferences).", "metadata": {}}, {"text": "Text-based tasks are, all things\nconsidered, more energy-efficient than image-based tasks, with image classification requiring less energy (median\nof 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH)\nrequiring more than text classification (0.0023 kWh).", "metadata": {}}, {"text": "For comparison, charging the average smartphone requires 0.022\nkWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full\nsmartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as\n522 smartphone charges (11.49 kWh), or around half a charge per image generation 5, although there is also a large\nvariation between image generation models, depending on the size of image that they generate.", "metadata": {}}, {"text": "inference energy (kWh)\ntask mean std\ntext classification 0.002 0.001\nextractive QA 0.003 0.001\nmasked language modeling 0.003 0.001\ntoken classification 0.004 0.002\nimage classification 0.007 0.001\nobject detection 0.038 0.02\ntext generation 0.047 0.03\nsummarization 0.049 0.01\nimage captioning 0.063 0.02\nimage generation 2.907 3.31\nTable 2.", "metadata": {}}, {"text": "Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.", "metadata": {}}, {"text": "We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive\ntask, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one,\nimage generation, whose mean consumption is 2.9kWh.", "metadata": {}}, {"text": "This means that the different models examined in our study\ncan vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.", "metadata": {}}, {"text": "Intuitively,\nthis is coherent given the decision space that different types of models have - from a binary classification task such as\nsentiment analysis (which can only output, for instance, a 0 for negative sentiment and a 1 for positive) to an entire\nvocabulary for text generation and summarization models.", "metadata": {}}, {"text": "The length of text generated also impacts energy usage: on\naverage, text generation uses 15 times more energy than masked language modeling, which makes sense given that the\nmasked language modeling task only generates a single token, whereas in our setup the text generation task generates\n10 new tokens for each input text, with the length of the input text rising as new tokens are generated, since each\nsequence of tokens gets fed back into the model to generate subsequent tokens.", "metadata": {}}, {"text": "Finally, for image-based tasks, the level\nof abstraction is lower and the decision space is larger given that they generate raw pixels as opposed to tokens for text,\nmaking image-based tasks more energy intensive than text based ones, e.g.", "metadata": {}}, {"text": "image classification uses over 3 times more\nenergy than text classification (0.007 vs.", "metadata": {}}, {"text": "0.002 kWh) and image generation uses, on average, over 60 times more energy\nthan text generation (0.047 vs.", "metadata": {}}, {"text": "2.9 kWh).", "metadata": {}}, {"text": "5Before January 2024, the EPA website estimated a smartphone charge to consume 0.012 kWh of energy, which was the number used for comparisons in\nan earlier version of this study.", "metadata": {}}, {"text": "6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig. 2. The 5 modalities examined in our study, with the number of parameters of each model on the x axis and the average amount\nof carbon emitted for 1000 inferences on the y axis. NB: Both axes are in logarithmic scale.\nNext, we examine the respective influences of model size and task structure on model emissions. Figure 2 shows the\nrelationship between model emissions (in grams of ùê∂ùëÇ2ùëíùëû per 1,000 inferences) and sizes (in terms of the number of\nparameters) across the task categories listed in Section 3.1. We do observe a relationship between model size and quantity\nof emissions produced during inference, with differing progressions for each modality ‚Äì however, the task structure ac-\ncounts for more of the variation than the model size does. We can observe once again that text-to-image is by far the most\ncarbon- and energy-intensive task, with smaller image generation models such as segmind/tiny-sd that have around\n500M parameters producing magnitudes more carbon than text-to-category models (100g vs. 0.6g of ùê∂ùëÇ2ùëíùëû per 1,000\ninferences). Within the text-to-text tasks, we see two separate sets of models: the masked language modeling task follow-\ning a lower trend, producing emissions akin to text-to-category models, compared to text generation and summarization\ntasks, which produce similar amounts of carbon to the image captioning models with a similar number of parameters.\nFor context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594\ngrams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered\npassenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) gener-\nates as much carbon as 0.0006 miles driven by a similar vehicle, i.e. 6,833 times less. This can add up quickly when\nimage generation models such as Dall¬∑E and MidJourney are deployed in user-facing applications and used by millions\nof users globally (we discuss this point further in Section 5).\nThe (high-level) takeaway of this analysis is that even for models specifically trained to carry out a single task,\nthere is a large level of variation both within each task and an even larger one between tasks from different modalities.\nIn essence, tasks that map both image and text inputs to categorical outputs are less energy- and carbon-intensive\nthan those that generate text or images. Making these distinctions can help inform policies seeking to mitigate the\nenvironmental impacts of AI, given that it is important to be aware of this variation, which can sometimes reach several\norders of magnitude. In the next section, we delve deeper into multi-purpose systems, which are meant to carry out\nseveral tasks concurrently, to better understand their environmental impacts and how they compare to task-specific\nmodels.\n7", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "The 5 modalities examined in our study, with the number of parameters of each model on the x axis and the average amount\nof carbon emitted for 1000 inferences on the y axis.", "metadata": {}}, {"text": "NB: Both axes are in logarithmic scale.", "metadata": {}}, {"text": "Next, we examine the respective influences of model size and task structure on model emissions.", "metadata": {}}, {"text": "Figure 2 shows the\nrelationship between model emissions (in grams of ùê∂ùëÇ2ùëíùëû per 1,000 inferences) and sizes (in terms of the number of\nparameters) across the task categories listed in Section 3.1.", "metadata": {}}, {"text": "We do observe a relationship between model size and quantity\nof emissions produced during inference, with differing progressions for each modality ‚Äì however, the task structure ac-\ncounts for more of the variation than the model size does.", "metadata": {}}, {"text": "We can observe once again that text-to-image is by far the most\ncarbon- and energy-intensive task, with smaller image generation models such as segmind/tiny-sd that have around\n500M parameters producing magnitudes more carbon than text-to-category models (100g vs.", "metadata": {}}, {"text": "0.6g of ùê∂ùëÇ2ùëíùëû per 1,000\ninferences).", "metadata": {}}, {"text": "Within the text-to-text tasks, we see two separate sets of models: the masked language modeling task follow-\ning a lower trend, producing emissions akin to text-to-category models, compared to text generation and summarization\ntasks, which produce similar amounts of carbon to the image captioning models with a similar number of parameters.", "metadata": {}}, {"text": "For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594\ngrams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered\npassenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) gener-\nates as much carbon as 0.0006 miles driven by a similar vehicle, i.e.", "metadata": {}}, {"text": "6,833 times less.", "metadata": {}}, {"text": "This can add up quickly when\nimage generation models such as Dall¬∑E and MidJourney are deployed in user-facing applications and used by millions\nof users globally (we discuss this point further in Section 5).", "metadata": {}}, {"text": "The (high-level) takeaway of this analysis is that even for models specifically trained to carry out a single task,\nthere is a large level of variation both within each task and an even larger one between tasks from different modalities.", "metadata": {}}, {"text": "In essence, tasks that map both image and text inputs to categorical outputs are less energy- and carbon-intensive\nthan those that generate text or images.", "metadata": {}}, {"text": "Making these distinctions can help inform policies seeking to mitigate the\nenvironmental impacts of AI, given that it is important to be aware of this variation, which can sometimes reach several\norders of magnitude.", "metadata": {}}, {"text": "In the next section, we delve deeper into multi-purpose systems, which are meant to carry out\nseveral tasks concurrently, to better understand their environmental impacts and how they compare to task-specific\nmodels.", "metadata": {}}, {"text": "7", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=Im3.png] Size: 1000x360, Data: 39959 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=Im3.png] Size: 1000x360, Data: 39959 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "Im3.png", "image_width": 1000, "image_height": 360, "attachment_type": "image", "has_image_data": true, "image_data_size": 39959}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2 The environmental cost of multi-purpose systems\nThe second part of our analysis examines multi-task models of two types: decoder only, from the BLOOMz family,\nand sequence-to-sequence models from the FLAN-T5 family, with the goal of comparing energy intensity and carbon\nemissions of models with differing numbers of parameters when applied to different tasks. To address this question,\nwe selected a subset of 3 tasks ‚Äì text classification, extractive question answering, and summarization ‚Äì given their\ndiversity and broad applicability in a variety of settings, and compare the 8 zero-shot models of different sizes, based on\nthe same 3 datasets per task as described in Table 1.\n4.2.1 Emissions of task-specific and multi-task architectures.\nTo start our analysis, we examined how the choice of model and architecture type impacts emissions given a specific\ntask and dataset. For this analysis, we took the same 8 task-specific models described in Section 3.2 and compared their\nemissions to the 8 multi-purpose models described above.\nFig. 3. Model emissions (measured in g ùê∂ùëÇ 2ùëíùëû) and architecture type for each of the datasets from our analysis. The y axis is in\nlogarithmic scale, dot size is proportional to model size.\nIn Figure 3, we plot the mean query emissions for each model on a dataset-by-dataset basis. We can see that for\nthe two discriminative tasks, sentiment analysis (which includes SST 2, Rotten Tomatoes and IMDB datasets) and\nquestion answering (which encompasses SciQ, SQuAD and SQuAD v2) there is a clear distinction between task-specific\ndiscriminative models (in blue), which have less emissions than both multi-purpose sequence-to-sequence (in yellow)\nand decoder-only generative models (in green). Given that the y axis in Figure 3 is in logarithmic scale, this indicates that\nthe difference is several orders of magnitude - e.g. with the most efficient task-specific models emit 0.3g ofùê∂ùëÇ2ùëíùëû per\n1,000 inferences for extractive question answering on a dataset like SciQ, multi-purpose models emit 10g for the same\ntask. This result follows intuitions derived from the model structures: while a task-specific model trained on binary text\nclassification will carry out a softmax on a two-category vector to predict a class, a multi-purpose model will generate\n‚Äòpositive‚Äô or ‚Äònegative‚Äô, which logically requires more energy because the prediction is based on the model‚Äôs entire\nvocabulary. For the generative task, summarization (represented by the SAMsum, XSum and CNN-Daily Mail datasets),\nthe task-specific and multi-purpose models are closer in terms of emissions: task-specific sequence-to-sequence models\n8", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2 The environmental cost of multi-purpose systems\nThe second part of our analysis examines multi-task models of two types: decoder only, from the BLOOMz family,\nand sequence-to-sequence models from the FLAN-T5 family, with the goal of comparing energy intensity and carbon\nemissions of models with differing numbers of parameters when applied to different tasks.", "metadata": {}}, {"text": "To address this question,\nwe selected a subset of 3 tasks ‚Äì text classification, extractive question answering, and summarization ‚Äì given their\ndiversity and broad applicability in a variety of settings, and compare the 8 zero-shot models of different sizes, based on\nthe same 3 datasets per task as described in Table 1.", "metadata": {}}, {"text": "4.2.1 Emissions of task-specific and multi-task architectures.", "metadata": {}}, {"text": "To start our analysis, we examined how the choice of model and architecture type impacts emissions given a specific\ntask and dataset.", "metadata": {}}, {"text": "For this analysis, we took the same 8 task-specific models described in Section 3.2 and compared their\nemissions to the 8 multi-purpose models described above.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Model emissions (measured in g ùê∂ùëÇ 2ùëíùëû) and architecture type for each of the datasets from our analysis.", "metadata": {}}, {"text": "The y axis is in\nlogarithmic scale, dot size is proportional to model size.", "metadata": {}}, {"text": "In Figure 3, we plot the mean query emissions for each model on a dataset-by-dataset basis.", "metadata": {}}, {"text": "We can see that for\nthe two discriminative tasks, sentiment analysis (which includes SST 2, Rotten Tomatoes and IMDB datasets) and\nquestion answering (which encompasses SciQ, SQuAD and SQuAD v2) there is a clear distinction between task-specific\ndiscriminative models (in blue), which have less emissions than both multi-purpose sequence-to-sequence (in yellow)\nand decoder-only generative models (in green).", "metadata": {}}, {"text": "Given that the y axis in Figure 3 is in logarithmic scale, this indicates that\nthe difference is several orders of magnitude - e.g.", "metadata": {}}, {"text": "with the most efficient task-specific models emit 0.3g ofùê∂ùëÇ2ùëíùëû per\n1,000 inferences for extractive question answering on a dataset like SciQ, multi-purpose models emit 10g for the same\ntask.", "metadata": {}}, {"text": "This result follows intuitions derived from the model structures: while a task-specific model trained on binary text\nclassification will carry out a softmax on a two-category vector to predict a class, a multi-purpose model will generate\n‚Äòpositive‚Äô or ‚Äònegative‚Äô, which logically requires more energy because the prediction is based on the model‚Äôs entire\nvocabulary.", "metadata": {}}, {"text": "For the generative task, summarization (represented by the SAMsum, XSum and CNN-Daily Mail datasets),\nthe task-specific and multi-purpose models are closer in terms of emissions: task-specific sequence-to-sequence models\n8", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im4.png] Size: 1000x400, Data: 62665 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im4.png] Size: 1000x400, Data: 62665 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im4.png", "image_width": 1000, "image_height": 400, "attachment_type": "image", "has_image_data": true, "image_data_size": 62665}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ngenerate 4-10g of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, while multi-purpose models emit 20-30g for the same task. The difference\nappears to mostly come from model size ‚Äì all of the task-specific summarization models we looked at were 600 million\nparameters at most, compared to the larger multi-purpose architectures, which attained the 11 billion parameters.\nWe also carry out an evaluation of both the task-specific and multi-purpose models examined in our study to\nensure that they have comparable performance. For task-specific models, we used the evaluate library [52] and the\nLM Evaluation Harness [14] for zero-shot models. Fundamentally speaking, it is hard to compare task-specific and\nmulti-purpose models using the same metrics, given that task-specific models have a much more constrained decision\nspace (e.g. two classes in the case of binary text classification), whereas multi-purpose models have a large output\nvocabulary to choose from, and are dependent upon the prompt schema and prompting strategy used. However, by\nutilizing two standardized packages (evaluate and lm-evaluation-harness) and keeping the prompting approach\nstable across zero-shot models, we endeavor to standardize our evaluation approach as much as possible.\nFig. 4. Model size, measured in number of parameters (x axis, logarithmic scale) and text classification accuracy (y axis), with dot size\nindicating the quantity of emissions (logarithmic scale).\nWe hone in on one specific task, text classification, in Figure 4, which illustrates the relationship between model\nsize (x axis, in logarithmic scale), accuracy (y axis) and emissions (dot size, in logarithmic scale). Among task-specific\nencoder models, we observe that accuracy varies more widely, i.e. there are several smaller models of similar size and\ncomparably small amounts of carbon emissions, with widely varying levels of accuracy. The multi-purpose models\nvary less in terms of accuracy, having higher average accuracy overall. Both sequence-to-sequence and decoder-only\nmodels produce comparable amounts of emissions (several orders of magnitude more than task-specific models).We can\nsee that mid-size multi-purpose models (in the 3B parameter range) may have slightly better accuracy compared to\nboth larger and smaller models. However, given the many caveats and specificities involved in multi-purpose LLM\nevaluation, this difference may not be significant. We present the full results of our evaluation, which include the other\n2 tasks, in Section B in the Supplementary Materials.\n9", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\ngenerate 4-10g of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, while multi-purpose models emit 20-30g for the same task.", "metadata": {}}, {"text": "The difference\nappears to mostly come from model size ‚Äì all of the task-specific summarization models we looked at were 600 million\nparameters at most, compared to the larger multi-purpose architectures, which attained the 11 billion parameters.", "metadata": {}}, {"text": "We also carry out an evaluation of both the task-specific and multi-purpose models examined in our study to\nensure that they have comparable performance.", "metadata": {}}, {"text": "For task-specific models, we used the evaluate library [52] and the\nLM Evaluation Harness [14] for zero-shot models.", "metadata": {}}, {"text": "Fundamentally speaking, it is hard to compare task-specific and\nmulti-purpose models using the same metrics, given that task-specific models have a much more constrained decision\nspace (e.g.", "metadata": {}}, {"text": "two classes in the case of binary text classification), whereas multi-purpose models have a large output\nvocabulary to choose from, and are dependent upon the prompt schema and prompting strategy used.", "metadata": {}}, {"text": "However, by\nutilizing two standardized packages (evaluate and lm-evaluation-harness) and keeping the prompting approach\nstable across zero-shot models, we endeavor to standardize our evaluation approach as much as possible.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "Model size, measured in number of parameters (x axis, logarithmic scale) and text classification accuracy (y axis), with dot size\nindicating the quantity of emissions (logarithmic scale).", "metadata": {}}, {"text": "We hone in on one specific task, text classification, in Figure 4, which illustrates the relationship between model\nsize (x axis, in logarithmic scale), accuracy (y axis) and emissions (dot size, in logarithmic scale).", "metadata": {}}, {"text": "Among task-specific\nencoder models, we observe that accuracy varies more widely, i.e.", "metadata": {}}, {"text": "there are several smaller models of similar size and\ncomparably small amounts of carbon emissions, with widely varying levels of accuracy.", "metadata": {}}, {"text": "The multi-purpose models\nvary less in terms of accuracy, having higher average accuracy overall.", "metadata": {}}, {"text": "Both sequence-to-sequence and decoder-only\nmodels produce comparable amounts of emissions (several orders of magnitude more than task-specific models).We can\nsee that mid-size multi-purpose models (in the 3B parameter range) may have slightly better accuracy compared to\nboth larger and smaller models.", "metadata": {}}, {"text": "However, given the many caveats and specificities involved in multi-purpose LLM\nevaluation, this difference may not be significant.", "metadata": {}}, {"text": "We present the full results of our evaluation, which include the other\n2 tasks, in Section B in the Supplementary Materials.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}, {"text": "[Image page=9 idx=1 name=Im5.png] Size: 993x246, Data: 50628 bytes", "sentences": [{"text": "[Image page=9 idx=1 name=Im5.png] Size: 993x246, Data: 50628 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 1, "image_name": "Im5.png", "image_width": 993, "image_height": 246, "attachment_type": "image", "has_image_data": true, "image_data_size": 50628}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2.2 Differences within multi-purpose architectures.\nBeyond the differences between task-specific and multi-purpose models generally, we also observed variation\nwithin the multi-purpose models that we examined. We present our results in Table 3; in it, we can observe that\non a per-architecture basis (i.e. within the family of decoder-only models and the family of sequence-to-sequence\nmodels), size and emissions are correlated, with smaller models emitting less carbon and using less energy. However,\nsequence-to-sequence models are more efficient than their decoder-only counterparts when models of the same size\nare compared: for instance, Flan-T5-XL and BLOOMz-3B are both of a similar size (around 3B parameters), but the\nformer generates, on average, 2 grams of emissions less for 1,000 inferences than the latter. This difference holds when\ncomparing Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we\ntested (11 billion), yet it has lower emissions (11.48g on average) compared to the smaller BLOOMz-7B. Comparing the\nmodels on a per-task basis in Figure 5, we can see the same pattern for zero-shot models as for task-specific ones, with\ntext classification a less carbon-intensive task compared to question answering, and summarization the most intensive\none of the three. The spread between the tasks is smaller for sequence-to-sequence models (indicated with dots in\nFigure 5), whereas for decoder-only models (indicated with crosses), the difference between the different tasks is more\nsignificant.\nseq2seq models decoder-only models\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nFlan-T5-base 222M 3.67 0.026 BLOOMz-560M 559M 7.5 0.054\nFlan-T5-large 750M 7.68 0.055 BLOOMz-1B 1.7B 8.66 0.062\nFlan-T5-xl 2.8B 8.08 0.058 BLOOMz-3B 3B 10.17 0.073\nFlan-T5-xxl 11B 11.48 0.083 BLOOMz-7B 7B 14.46 0.104\nTable 3. Zero-shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of\nemissions (in g of ùê∂ùëÇ 2ùëíùëû) and average energy usage (in kWh) for 1,000 inferences.\nWe can analyse the relationship between sequence-to-sequence and decoder-only models noted in Table 3: whereas\nfor tasks such as summarization, decoder models do generate more emissions than sequence-to-sequence models of\na similar size, for question answering and text classification, the two architectures have similar emissions. This can\nagain be explained by the differences in the model structures, specifically the attention mechanism: while sequence-to-\nsequence models only attend to the last layer of the input when producing their answers, decoder-only architectures\nattend to all layers for the full sequence ‚Äì leading to a stronger dependency on the output length for the number of\noperations, resulting in more emissions for tasks with longer outputs.\nWe further verify this intuition in Table 4 and Figure 6: while there is some variation between models and datasets in\nTable 4, the distribution of output lengths is consistent with our expectations for the different task categories: tasks with\nlonger outputs result in more emissions, especially for decoder-only models. Figure 6 delves further into the relationship\nbetween average output length, carbon emissions, and model structures for the different summarization datasets. It\nshows a clear correlation between output length and measured emissions, with a higher slope for the decoder-only\narchitectures (the BLOOMz family of models) than for the sequence-to-sequence architectures (the Flan-T5 family).\nAs we have observed in the current section, there is no ‚Äòone-size-fits-all‚Äô pattern for multi-purpose models either ‚Äì\nthey too exhibit variation in terms of their emissions and energy usage, which can be attributed to different factors,\n10", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n4.2.2 Differences within multi-purpose architectures.", "metadata": {}}, {"text": "Beyond the differences between task-specific and multi-purpose models generally, we also observed variation\nwithin the multi-purpose models that we examined.", "metadata": {}}, {"text": "We present our results in Table 3;", "metadata": {}}, {"text": "in it, we can observe that\non a per-architecture basis (i.e.", "metadata": {}}, {"text": "within the family of decoder-only models and the family of sequence-to-sequence\nmodels), size and emissions are correlated, with smaller models emitting less carbon and using less energy.", "metadata": {}}, {"text": "However,\nsequence-to-sequence models are more efficient than their decoder-only counterparts when models of the same size\nare compared: for instance, Flan-T5-XL and BLOOMz-3B are both of a similar size (around 3B parameters), but the\nformer generates, on average, 2 grams of emissions less for 1,000 inferences than the latter.", "metadata": {}}, {"text": "This difference holds when\ncomparing Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we\ntested (11 billion), yet it has lower emissions (11.48g on average) compared to the smaller BLOOMz-7B.", "metadata": {}}, {"text": "Comparing the\nmodels on a per-task basis in Figure 5, we can see the same pattern for zero-shot models as for task-specific ones, with\ntext classification a less carbon-intensive task compared to question answering, and summarization the most intensive\none of the three.", "metadata": {}}, {"text": "The spread between the tasks is smaller for sequence-to-sequence models (indicated with dots in\nFigure 5), whereas for decoder-only models (indicated with crosses), the difference between the different tasks is more\nsignificant.", "metadata": {}}, {"text": "seq2seq models decoder-only models\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nmodel\nname\nnumber of\nparameters\nemissions\n(g ùê∂ùëÇ2ùëíùëû)\nenergy\n(kWh)\nFlan-T5-base 222M 3.67 0.026 BLOOMz-560M 559M 7.5 0.054\nFlan-T5-large 750M 7.68 0.055 BLOOMz-1B 1.7B 8.66 0.062\nFlan-T5-xl 2.8B 8.08 0.058 BLOOMz-3B 3B 10.17 0.073\nFlan-T5-xxl 11B 11.48 0.083 BLOOMz-7B 7B 14.46 0.104\nTable 3.", "metadata": {}}, {"text": "Zero-shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of\nemissions (in g of ùê∂ùëÇ 2ùëíùëû) and average energy usage (in kWh) for 1,000 inferences.", "metadata": {}}, {"text": "We can analyse the relationship between sequence-to-sequence and decoder-only models noted in Table 3: whereas\nfor tasks such as summarization, decoder models do generate more emissions than sequence-to-sequence models of\na similar size, for question answering and text classification, the two architectures have similar emissions.", "metadata": {}}, {"text": "This can\nagain be explained by the differences in the model structures, specifically the attention mechanism: while sequence-to-\nsequence models only attend to the last layer of the input when producing their answers, decoder-only architectures\nattend to all layers for the full sequence ‚Äì leading to a stronger dependency on the output length for the number of\noperations, resulting in more emissions for tasks with longer outputs.", "metadata": {}}, {"text": "We further verify this intuition in Table 4 and Figure 6: while there is some variation between models and datasets in\nTable 4, the distribution of output lengths is consistent with our expectations for the different task categories: tasks with\nlonger outputs result in more emissions, especially for decoder-only models.", "metadata": {}}, {"text": "Figure 6 delves further into the relationship\nbetween average output length, carbon emissions, and model structures for the different summarization datasets.", "metadata": {}}, {"text": "It\nshows a clear correlation between output length and measured emissions, with a higher slope for the decoder-only\narchitectures (the BLOOMz family of models) than for the sequence-to-sequence architectures (the Flan-T5 family).", "metadata": {}}, {"text": "As we have observed in the current section, there is no ‚Äòone-size-fits-all‚Äô pattern for multi-purpose models either ‚Äì\nthey too exhibit variation in terms of their emissions and energy usage, which can be attributed to different factors,\n10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig. 5. A plot of the total emissions (in grams of ùê∂ùëÇ 2ùëíùëû) for 1,000 inferences for all multi-purpose models.\nBLOOMz\n560M\nBLOOMz\n1B\nBLOOMz\n3B\nBLOOMz\n7B\nFlan-T5\nbase\nFlan-T5\nlarge\nFlan-T5\nxl\nFlan-T5\nxxl\ndataset input output output output output output output output output\nIMDB 58.73 1.64 2.61 1.72 1.53 1.00 1.00 1.00 1.00\nRotten\nTomatoes 30.08 1.00 0.99 1.03 1.00 1.00 1.00 1.00 1.00\nSST 2 28.35 0.98 0.99 1.01 1.02 1.00 1.00 1.00 1.00\nSciQ 113.12 1.28 1.25 1.10 1.10 2.03 5.41 3.12 2.42\nSQuAD 134.00 1.93 1.96 2.02 1.95 2.01 2.15 2.16 2.13\nSQuAD 2 115.85 2.33 2.54 2.58 2.41 2.28 2.74 2.71 2.58\nCNN 54.00 12.05 11.91 11.73 10.34 8.52 11.34 11.34 10.68\nSamSUM 47.82 9.54 9.41 9.75 9.85 10.56 11.05 10.18 10.57\nXSum 53.85 11.53 12.22 11.94 11.92 12.95 13.62 13.49 13.09\nTable 4. Average input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study.\nThe darker the cell, the more carbon was output by the model for the task.\nFig. 6. A plot of the output length (X axis) and carbon emissions (Y axis) for the summarization task. The symbol refers to the type of\narchitecture (BLOOMz vs Flan-T5), symbol size references the relative model size (in terms of the number of parameters), and color\nthe input length.\nincluding model size and output length. This would indicate that more careful consideration is needed when making\n11", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nFig.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "A plot of the total emissions (in grams of ùê∂ùëÇ 2ùëíùëû) for 1,000 inferences for all multi-purpose models.", "metadata": {}}, {"text": "BLOOMz\n560M\nBLOOMz\n1B\nBLOOMz\n3B\nBLOOMz\n7B\nFlan-T5\nbase\nFlan-T5\nlarge\nFlan-T5\nxl\nFlan-T5\nxxl\ndataset input output output output output output output output output\nIMDB 58.73 1.64 2.61 1.72 1.53 1.00 1.00 1.00 1.00\nRotten\nTomatoes 30.08 1.00 0.99 1.03 1.00 1.00 1.00 1.00 1.00\nSST 2 28.35 0.98 0.99 1.01 1.02 1.00 1.00 1.00 1.00\nSciQ 113.12 1.28 1.25 1.10 1.10 2.03 5.41 3.12 2.42\nSQuAD 134.00 1.93 1.96 2.02 1.95 2.01 2.15 2.16 2.13\nSQuAD 2 115.85 2.33 2.54 2.58 2.41 2.28 2.74 2.71 2.58\nCNN 54.00 12.05 11.91 11.73 10.34 8.52 11.34 11.34 10.68\nSamSUM 47.82 9.54 9.41 9.75 9.85 10.56 11.05 10.18 10.57\nXSum 53.85 11.53 12.22 11.94 11.92 12.95 13.62 13.49 13.09\nTable 4.", "metadata": {}}, {"text": "Average input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study.", "metadata": {}}, {"text": "The darker the cell, the more carbon was output by the model for the task.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "A plot of the output length (X axis) and carbon emissions (Y axis) for the summarization task.", "metadata": {}}, {"text": "The symbol refers to the type of\narchitecture (BLOOMz vs Flan-T5), symbol size references the relative model size (in terms of the number of parameters), and color\nthe input length.", "metadata": {}}, {"text": "including model size and output length.", "metadata": {}}, {"text": "This would indicate that more careful consideration is needed when making\n11", "metadata": {}}], "metadata": {"page": 11}}, {"text": "[Image page=11 idx=1 name=Im6.png] Size: 1000x300, Data: 57124 bytes", "sentences": [{"text": "[Image page=11 idx=1 name=Im6.png] Size: 1000x300, Data: 57124 bytes", "metadata": {}}], "metadata": {"page": 11, "image_index": 1, "image_name": "Im6.png", "image_width": 1000, "image_height": 300, "attachment_type": "image", "has_image_data": true, "image_data_size": 57124}}, {"text": "[Image page=11 idx=2 name=Im7.png] Size: 1260x360, Data: 41012 bytes", "sentences": [{"text": "[Image page=11 idx=2 name=Im7.png] Size: 1260x360, Data: 41012 bytes", "metadata": {}}], "metadata": {"page": 11, "image_index": 2, "image_name": "Im7.png", "image_width": 1260, "image_height": 360, "attachment_type": "image", "has_image_data": true, "image_data_size": 41012}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4√ó 10‚àí5 for the\nsmallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 √ó 10‚àí4 7.3 √ó 10‚àí5 6.2 √ó 10‚àí5 5.4 √ó 10‚àí5\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used ‚Äì while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios.", "metadata": {}}, {"text": "We further discuss our\nresults and further avenues of research in the next and final section.", "metadata": {}}, {"text": "4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e.", "metadata": {}}, {"text": "when does the deployment of models use as much energy as\ntheir initial training?", "metadata": {}}, {"text": "This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available.", "metadata": {}}, {"text": "Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models.", "metadata": {}}, {"text": "However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31].", "metadata": {}}, {"text": "We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6.", "metadata": {}}, {"text": "We present these numbers, alongside the average energy consumption\nper inference, in Table 5.", "metadata": {}}, {"text": "We can see that the amount of energy required per inference varies from 5.4√ó 10‚àí5 for the\nsmallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one, BLOOMz-7B.", "metadata": {}}, {"text": "This is coherent to the numbers\nreported by Luccioni et al.", "metadata": {}}, {"text": "for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.", "metadata": {}}, {"text": "BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 √ó 10‚àí4 7.3 √ó 10‚àí5 6.2 √ó 10‚àí5 5.4 √ó 10‚àí5\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5.", "metadata": {}}, {"text": "The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.", "metadata": {}}, {"text": "If we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training.", "metadata": {}}, {"text": "As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B.", "metadata": {}}, {"text": "This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel.", "metadata": {}}, {"text": "For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36];", "metadata": {}}, {"text": "the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7.", "metadata": {}}, {"text": "Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.", "metadata": {}}, {"text": "While the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used ‚Äì while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.", "metadata": {}}, {"text": "7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nby the open-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded\n606,096 times, BLOOMz-3B has been downloaded 357,368 times, BLOOMz-1B has been downloaded 61,757 times and\nBLOOMz-560m has been downloaded 498,601 times. They have also been finetuned for a number of downstream\ntasks, such as chat, and deployed in HuggingFace Spaces, interactive interfaces for model interaction. While this\nanalysis represents a relatively small sample of models, analyses such as this are vital for estimating the relative energy\nconsumption (and ensuing emissions) of different stages of the ML training and deployment cycle, understanding\ntrade-offs between training and inference emissions patterns, and characterizing the lifetime emissions of ML models,\nand we hope that others will be possible in the future, which would require more transparency from model creators\nregarding both the up front (i.e. training) and downstream (i.e. inference) costs of ML models. We discuss the importance\nof transparency and other important actions that members of the community can take in the next, and final, section.\n5 DISCUSSION\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due\nto its distributed nature ‚Äî compared to the relatively time- and location-constrained nature of training ‚Äî making it\ndifficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to\nkeep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We\nprovide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore\nthe results we present here. We also highlight the main high-level takeaways of our study below:\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the\nmost energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image\ncaptioning, and image generation.\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically,\ntasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving\ngenerating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a\nsimilar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that\nmore computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for\ntasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as\nother tasks and datasets.\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model\ndeployment, this parity can be reached quickly for many popular models.\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these\nsame tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question\nanswering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large,\nand less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference\n13", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nby the open-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded\n606,096 times, BLOOMz-3B has been downloaded 357,368 times, BLOOMz-1B has been downloaded 61,757 times and\nBLOOMz-560m has been downloaded 498,601 times.", "metadata": {}}, {"text": "They have also been finetuned for a number of downstream\ntasks, such as chat, and deployed in HuggingFace Spaces, interactive interfaces for model interaction.", "metadata": {}}, {"text": "While this\nanalysis represents a relatively small sample of models, analyses such as this are vital for estimating the relative energy\nconsumption (and ensuing emissions) of different stages of the ML training and deployment cycle, understanding\ntrade-offs between training and inference emissions patterns, and characterizing the lifetime emissions of ML models,\nand we hope that others will be possible in the future, which would require more transparency from model creators\nregarding both the up front (i.e.", "metadata": {}}, {"text": "training) and downstream (i.e.", "metadata": {}}, {"text": "inference) costs of ML models.", "metadata": {}}, {"text": "We discuss the importance\nof transparency and other important actions that members of the community can take in the next, and final, section.", "metadata": {}}, {"text": "5 DISCUSSION\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due\nto its distributed nature ‚Äî compared to the relatively time- and location-constrained nature of training ‚Äî making it\ndifficult to make meaningful comparisons between different models and tasks.", "metadata": {}}, {"text": "In this work, we have endeavored to\nkeep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library.", "metadata": {}}, {"text": "We\nprovide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore\nthe results we present here.", "metadata": {}}, {"text": "We also highlight the main high-level takeaways of our study below:\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks.", "metadata": {}}, {"text": "As shown in Figure 1, the\nmost energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image\ncaptioning, and image generation.", "metadata": {}}, {"text": "Tasks involving images are more energy- and carbon-intensive compared to those involving text alone.", "metadata": {}}, {"text": "More specifically,\ntasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving\ngenerating images (e.g.", "metadata": {}}, {"text": "text-to-image), with those involving text between the two (see Figure 2).", "metadata": {}}, {"text": "Decoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a\nsimilar size and applied to the same tasks.", "metadata": {}}, {"text": "The findings we present in Table 3, Figure 3, and Figure 6 would indicate that\nmore computation (i.e.", "metadata": {}}, {"text": "energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for\ntasks with longer outputs.", "metadata": {}}, {"text": "This observation is worth verifying for other architectures from both categories, and well as\nother tasks and datasets.", "metadata": {}}, {"text": "Training remains orders of magnitude more energy- and carbon- intensive than inference.", "metadata": {}}, {"text": "We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size.", "metadata": {}}, {"text": "While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model\ndeployment, this parity can be reached quickly for many popular models.", "metadata": {}}, {"text": "Using multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these\nsame tasks.", "metadata": {}}, {"text": "This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question\nanswering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large,\nand less so for summarization (for CNN-Daily Mail, SamSUM and XSum).", "metadata": {}}, {"text": "As can be seen in Table 4, the difference\n13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nbetween multi-purpose models and task-specific models is amplified as the length of output gets longer.\nWe find this last point to be the most compelling takeaway of our study, given the current paradigm shift away\nfrom smaller models finetuned for a specific task towards models that are meant to carry out a multitude of tasks\nat once, deployed to respond to a barrage of user queries in real time. This transition has been happening both\nin ML research since the advent of GPT-3 [ 5], which illustrated the potential for few- and zero-shot learning with\nlanguage models, as well as in consumer settings, with LLMs such as GPT-4 and PaLM being deployed in user-facing\nproducts such as web search [4, 18], email, and navigation [17], where smaller, task-specific versions of models such\nas BERT were previously used [ 3, 16]. While it is hard to quantify the environmental impacts of this transition\ngiven the lack of transparency of technology companies regarding both the number of parameters, architecture\nand carbon emissions of their products, we can make a comparison based on the experiments carried out in the\npresent study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering\n(bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70gùê∂ùëÇ2ùëíùëû\nper 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for\nBLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification\nwith the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of\nùê∂ùëÇ2ùëíùëû per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM\nmodel, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see\nthe benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see\nconvincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web\nsearch and navigation, given these models‚Äô energy requirements.\nFinally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon\nemissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between\ntraining, finetuning and inference energy requirements carried out in Section 4.3 is, to our knowledge, the first\ncomparison of its kind, and paves the way to a better understanding of how the different stages of an ML model‚Äôs\nlifecycle add up in terms of energy use. These are important data points that can help inform both our fellow AI\nresearchers and practitioners, as well as policy-makers who are working towards estimating and regulating the\nenvironmental impacts of AI models and ICT in general. We recognize that our study is not representative of all\ndeployment contexts and constraints ‚Äì our intent is to establish a set of initial data points and to set the stage for testing\nand comparing other models. In fact, our study highlights many potential avenues for future research aimed towards a\nbetter understanding of the myriad factors that influence the efficiency of inference, including the choice of architecture,\nthe usage of techniques such as distillation, the number of parameters, the choice of hardware and the numerical (i.e.\nfloating point) precision of model parameters. While we encourage continued work analysing open-source models,\nwe note that the growing lack of transparency in model architecture and training details makes this line of work,\nalongside many branches relating to fairness and accountability in machine learning, increasingly difficult to carry\nout. Given our findings and the increased deployment of generative, multi-purpose AI models, we hope that both ML\nresearchers and practitioners will practice transparency regarding the nature and impacts of their models, to enable\nbetter understanding of their environmental impacts.\n8The exact number of parameters of GPT-4 and PaLM 2 have not been publicly shared.\n14", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nbetween multi-purpose models and task-specific models is amplified as the length of output gets longer.", "metadata": {}}, {"text": "We find this last point to be the most compelling takeaway of our study, given the current paradigm shift away\nfrom smaller models finetuned for a specific task towards models that are meant to carry out a multitude of tasks\nat once, deployed to respond to a barrage of user queries in real time.", "metadata": {}}, {"text": "This transition has been happening both\nin ML research since the advent of GPT-3 [ 5], which illustrated the potential for few- and zero-shot learning with\nlanguage models, as well as in consumer settings, with LLMs such as GPT-4 and PaLM being deployed in user-facing\nproducts such as web search [4, 18], email, and navigation [17], where smaller, task-specific versions of models such\nas BERT were previously used [ 3, 16].", "metadata": {}}, {"text": "While it is hard to quantify the environmental impacts of this transition\ngiven the lack of transparency of technology companies regarding both the number of parameters, architecture\nand carbon emissions of their products, we can make a comparison based on the experiments carried out in the\npresent study.", "metadata": {}}, {"text": "For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering\n(bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70gùê∂ùëÇ2ùëíùëû\nper 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for\nBLOOMz-560M).", "metadata": {}}, {"text": "The difference is much more drastic if comparing BERT-based models for tasks such as text classification\nwith the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of\nùê∂ùëÇ2ùëíùëû per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.", "metadata": {}}, {"text": "For comparison, the first PaLM\nmodel, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8.", "metadata": {}}, {"text": "While we see\nthe benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see\nconvincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web\nsearch and navigation, given these models‚Äô energy requirements.", "metadata": {}}, {"text": "Finally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon\nemissions of the final, often overlooked, step in the ML model life cycle: model deployment.", "metadata": {}}, {"text": "The comparison between\ntraining, finetuning and inference energy requirements carried out in Section 4.3 is, to our knowledge, the first\ncomparison of its kind, and paves the way to a better understanding of how the different stages of an ML model‚Äôs\nlifecycle add up in terms of energy use.", "metadata": {}}, {"text": "These are important data points that can help inform both our fellow AI\nresearchers and practitioners, as well as policy-makers who are working towards estimating and regulating the\nenvironmental impacts of AI models and ICT in general.", "metadata": {}}, {"text": "We recognize that our study is not representative of all\ndeployment contexts and constraints ‚Äì our intent is to establish a set of initial data points and to set the stage for testing\nand comparing other models.", "metadata": {}}, {"text": "In fact, our study highlights many potential avenues for future research aimed towards a\nbetter understanding of the myriad factors that influence the efficiency of inference, including the choice of architecture,\nthe usage of techniques such as distillation, the number of parameters, the choice of hardware and the numerical (i.e.", "metadata": {}}, {"text": "floating point) precision of model parameters.", "metadata": {}}, {"text": "While we encourage continued work analysing open-source models,\nwe note that the growing lack of transparency in model architecture and training details makes this line of work,\nalongside many branches relating to fairness and accountability in machine learning, increasingly difficult to carry\nout.", "metadata": {}}, {"text": "Given our findings and the increased deployment of generative, multi-purpose AI models, we hope that both ML\nresearchers and practitioners will practice transparency regarding the nature and impacts of their models, to enable\nbetter understanding of their environmental impacts.", "metadata": {}}, {"text": "8The exact number of parameters of GPT-4 and PaLM 2 have not been publicly shared.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nETHICAL CONSIDERATIONS STATEMENT\nThe main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon\nemissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical\nsignificance of our measurements. In total, for all of model experimentation and evaluation, we used a total of 754.66\nkWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû. In order to reduce our impacts as much as possible, we did all up-front\nexperimentations on smaller portions of the dataset (to reduce wasted resources).\nRESEARCHER POSITIONALITY STATEMENT\nThe authors of this paper have backgrounds in theoretical and applied machine learning and work in institutions\nbased in North America. We therefore recognize that our way of planning and running experiments is not necessarily\nreflective of other institutions from other regions, or the constraints faced by researchers from institutions with more\nlimited access to compute.\nADVERSE IMPACTS STATEMENT\nWe recognize that our work can be perceived as a critique of ML deployment in general, given the analysis that we\nprovide of its environmental impacts. This could be used as an argument to stop pursuing ML research and development,\nor as a way of targeting specific companies or organizations. Our intention, however, is to shed additional light on the\nenvironmental impacts of ML, in order to help model developers and researchers make more informed choices as a\nfunction of their environmental footprint or energy usage.\nACKNOWLEDGMENTS\nWe thank Will Alpine, Nima Boscarino, Priya Donti, R√©gis Pierrard, David Rolnick, Roy Schwartz and Rajiv Shah for\ntheir useful feedback and suggestions.\n15", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nETHICAL CONSIDERATIONS STATEMENT\nThe main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon\nemissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical\nsignificance of our measurements.", "metadata": {}}, {"text": "In total, for all of model experimentation and evaluation, we used a total of 754.66\nkWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.", "metadata": {}}, {"text": "In order to reduce our impacts as much as possible, we did all up-front\nexperimentations on smaller portions of the dataset (to reduce wasted resources).", "metadata": {}}, {"text": "RESEARCHER POSITIONALITY STATEMENT\nThe authors of this paper have backgrounds in theoretical and applied machine learning and work in institutions\nbased in North America.", "metadata": {}}, {"text": "We therefore recognize that our way of planning and running experiments is not necessarily\nreflective of other institutions from other regions, or the constraints faced by researchers from institutions with more\nlimited access to compute.", "metadata": {}}, {"text": "ADVERSE IMPACTS STATEMENT\nWe recognize that our work can be perceived as a critique of ML deployment in general, given the analysis that we\nprovide of its environmental impacts.", "metadata": {}}, {"text": "This could be used as an argument to stop pursuing ML research and development,\nor as a way of targeting specific companies or organizations.", "metadata": {}}, {"text": "Our intention, however, is to shed additional light on the\nenvironmental impacts of ML, in order to help model developers and researchers make more informed choices as a\nfunction of their environmental footprint or energy usage.", "metadata": {}}, {"text": "ACKNOWLEDGMENTS\nWe thank Will Alpine, Nima Boscarino, Priya Donti, R√©gis Pierrard, David Rolnick, Roy Schwartz and Rajiv Shah for\ntheir useful feedback and suggestions.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nREFERENCES\n[1] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools. In EMNLP, Workshop SustaiNLP.\n[2] Jeff Barr. 2019. Amazon ec2 update‚Äìinf1 instances with AWS inferentia chips for high performance cost-effective inferencing. https://aws.amazon.\ncom/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n[3] Bing. 2019. Bing delivers its largest improvement in search experience using Azure GPUs. https://azure.microsoft.com/en-us/blog/bing-delivers-\nits-largest-improvement-in-search-experience-using-azure-gpus/\n[4] Bing. 2023. Confirmed: the new Bing runs on OpenAI‚Äôs GPT-4. https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-\nOpenAI%E2%80%99s-GPT-4\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.\n[6] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of\nGenerative AI Inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems . 1‚Äì7.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,\nAlbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu,\nVincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. https://doi.org/10.48550/ARXIV.2210.11416\n[9] Rishit Dagli and Ali Mustufa Shaikh. 2021. CPPE-5: Medical Personal Protective Equipment Dataset. arXiv:2112.09569 [cs.CV]\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. 2021. RedCaps: web-curated image-text data created by the people, for the people.\narXiv:2111.11431 [cs.CV]\n[11] Radosvet Desislavov, Fernando Mart√≠nez-Plumed, and Jos√© Hern√°ndez-Orallo. 2021. Compute and energy consumption trends in deep learning\ninference. arXiv preprint arXiv:2109.05472 (2021).\n[12] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency. 1877‚Äì1894.\n[13] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang. 2023. LLMCarbon: Modeling the end-to-end Carbon\nFootprint of Large Language Models. arXiv preprint arXiv:2309.14393 (2023).\n[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas\nMuennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.A framework for few-shot language\nmodel evaluation. https://doi.org/10.5281/zenodo.5371628\n[15] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive\nSummarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization . Association for Computational Linguistics, Hong Kong,\nChina, 70‚Äì79. https://doi.org/10.18653/v1/D19-5409\n[16] Google. 2019. Understanding searches better than ever before. https://blog.google/products/search/search-language-understanding-bert/\n[17] Google. 2023. Bard can now connect to your Google apps and services. https://blog.google/products/bard/google-bard-new-features-update-sept-\n2023/\n[18] Google. 2023. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/\n[19] Walid A Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant Shenoy. 2023. CarbonScaler: Leveraging Cloud Workload Elasticity for\nOptimizing Carbon-Efficiency. arXiv preprint arXiv:2302.08681 (2023).\n[20] Karl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching\nMachines to Read and Comprehend. In NeurIPS. 1693‚Äì1701. http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend\n[21] Ralph Hintemann and Simon Hinterholzer. 2022. Cloud computing drives the growth of the data center industry and its energy consumption. Data\ncenters 2022. ResearchGate (2022).\n[22] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-\nand-data-transmission-networks\n[23] Matt Gardner Johannes Welbl, Nelson F. Liu. 2017. Crowdsourcing Multiple Choice Science Questions. arXiv:1707.06209v1.\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.\nInternational Journal of Computer Vision 123 (2017), 32‚Äì73. https://doi.org/10.1007/s11263-016-0981-7\n[25] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images . Technical Report.\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700 (2019).\n16", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nREFERENCES\n[1] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools.", "metadata": {}}, {"text": "In EMNLP, Workshop SustaiNLP.", "metadata": {}}, {"text": "[2] Jeff Barr.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Amazon ec2 update‚Äìinf1 instances with AWS inferentia chips for high performance cost-effective inferencing.", "metadata": {}}, {"text": "https://aws.amazon.", "metadata": {}}, {"text": "com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n[3] Bing.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Bing delivers its largest improvement in search experience using Azure GPUs.", "metadata": {}}, {"text": "https://azure.microsoft.com/en-us/blog/bing-delivers-\nits-largest-improvement-in-search-experience-using-azure-gpus/\n[4] Bing.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Confirmed: the new Bing runs on OpenAI‚Äôs GPT-4.", "metadata": {}}, {"text": "https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-\nOpenAI%E2%80%99s-GPT-4\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Language models are few-shot learners.", "metadata": {}}, {"text": "Advances in neural information processing systems 33 (2020), 1877‚Äì1901.", "metadata": {}}, {"text": "[6] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Reducing the Carbon Impact of\nGenerative AI Inference (today and in 2035).", "metadata": {}}, {"text": "In Proceedings of the 2nd Workshop on Sustainable Computer Systems .", "metadata": {}}, {"text": "1‚Äì7.", "metadata": {}}, {"text": "[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton, Sebastian Gehrmann, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Palm: Scaling language modeling with pathways.", "metadata": {}}, {"text": "arXiv preprint arXiv:2204.02311 (2022).", "metadata": {}}, {"text": "[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,\nAlbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu,\nVincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H.", "metadata": {}}, {"text": "Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V.", "metadata": {}}, {"text": "Le,\nand Jason Wei.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Scaling Instruction-Finetuned Language Models.", "metadata": {}}, {"text": "https://doi.org/10.48550/ARXIV.2210.11416\n[9] Rishit Dagli and Ali Mustufa Shaikh.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "CPPE-5: Medical Personal Protective Equipment Dataset.", "metadata": {}}, {"text": "arXiv:2112.09569 [cs.CV]\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "RedCaps: web-curated image-text data created by the people, for the people.", "metadata": {}}, {"text": "arXiv:2111.11431 [cs.CV]\n[11] Radosvet Desislavov, Fernando Mart√≠nez-Plumed, and Jos√© Hern√°ndez-Orallo.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Compute and energy consumption trends in deep learning\ninference.", "metadata": {}}, {"text": "arXiv preprint arXiv:2109.05472 (2021).", "metadata": {}}, {"text": "[12] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Measuring the carbon intensity of AI in cloud instances.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency.", "metadata": {}}, {"text": "1877‚Äì1894.", "metadata": {}}, {"text": "[13] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "LLMCarbon: Modeling the end-to-end Carbon\nFootprint of Large Language Models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2309.14393 (2023).", "metadata": {}}, {"text": "[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas\nMuennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.", "metadata": {}}, {"text": "2021.A framework for few-shot language\nmodel evaluation.", "metadata": {}}, {"text": "https://doi.org/10.5281/zenodo.5371628\n[15] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive\nSummarization.", "metadata": {}}, {"text": "In Proceedings of the 2nd Workshop on New Frontiers in Summarization .", "metadata": {}}, {"text": "Association for Computational Linguistics, Hong Kong,\nChina, 70‚Äì79.", "metadata": {}}, {"text": "https://doi.org/10.18653/v1/D19-5409\n[16] Google.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Understanding searches better than ever before.", "metadata": {}}, {"text": "https://blog.google/products/search/search-language-understanding-bert/\n[17] Google.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Bard can now connect to your Google apps and services.", "metadata": {}}, {"text": "https://blog.google/products/bard/google-bard-new-features-update-sept-\n2023/\n[18] Google.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "An important next step on our AI journey.", "metadata": {}}, {"text": "https://blog.google/technology/ai/bard-google-ai-search-updates/\n[19] Walid A Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant Shenoy.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "CarbonScaler: Leveraging Cloud Workload Elasticity for\nOptimizing Carbon-Efficiency.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.08681 (2023).", "metadata": {}}, {"text": "[20] Karl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Teaching\nMachines to Read and Comprehend.", "metadata": {}}, {"text": "In NeurIPS.", "metadata": {}}, {"text": "1693‚Äì1701.", "metadata": {}}, {"text": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend\n[21] Ralph Hintemann and Simon Hinterholzer.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Cloud computing drives the growth of the data center industry and its energy consumption.", "metadata": {}}, {"text": "Data\ncenters 2022.", "metadata": {}}, {"text": "ResearchGate (2022).", "metadata": {}}, {"text": "[22] International Energy Authority.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Data Centres and Data Transmission Networks.", "metadata": {}}, {"text": "https://www.iea.org/energy-system/buildings/data-centres-\nand-data-transmission-networks\n[23] Matt Gardner Johannes Welbl, Nelson F.", "metadata": {}}, {"text": "Liu.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Crowdsourcing Multiple Choice Science Questions.", "metadata": {}}, {"text": "arXiv:1707.06209v1.", "metadata": {}}, {"text": "[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.", "metadata": {}}, {"text": "Shamma,\nMichael S.", "metadata": {}}, {"text": "Bernstein, and Li Fei-Fei.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.", "metadata": {}}, {"text": "International Journal of Computer Vision 123 (2017), 32‚Äì73.", "metadata": {}}, {"text": "https://doi.org/10.1007/s11263-016-0981-7\n[25] Alex Krizhevsky.", "metadata": {}}, {"text": "2009.", "metadata": {}}, {"text": "Learning multiple layers of features from tiny images .", "metadata": {}}, {"text": "Technical Report.", "metadata": {}}, {"text": "[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Quantifying the carbon emissions of machine learning.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:1910.09700 (2019).", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\n[27] Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, and Julien Launay. 2022. A Holistic Assessment of the Carbon Footprint\nof Noor, a Very Large Arabic Language Model. In Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large\nLanguage Models. Association for Computational Linguistics, virtual+Dublin, 84‚Äì94. https://doi.org/10.18653/v1/2022.bigscience-1.8\n[28] George Leopold. 2019. AWS to Offer NVIDIA‚Äôs T4 GPUs for AI Inferencing. www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll‚Äôar, and C Lawrence Zitnick. 2014. Microsoft\nCOCO: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13. Springer, 740‚Äì755.\n[30] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning.\narXiv preprint arXiv:2302.08476 (2023).\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n[32] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment\nAnalysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies . Association for\nComputational Linguistics, Portland, Oregon, USA, 142‚Äì150. http://www.aclweb.org/anthology/P11-1015\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong,\nHailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 (2022).\n[35] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don‚Äôt Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural\nNetworks for Extreme Summarization. ArXiv abs/1808.08745 (2018).\n[36] Will Oremus. 2023. AI chatbots lose money every time you use them. That is a problem. Washington Post (2023). https://www.washingtonpost.com/\ntechnology/2023/06/05/chatgpt-hidden-cost-gpu-compute/\n[37] Pedro Javier Ortiz Su‚Äôarez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource\ninfrastructures (Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019) , Piotr Ba≈Ñski,\nAdrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L\"ungen, and Caroline Iliadi (Eds.). Leibniz-Institut f\"ur\nDeutsche Sprache, Mannheim, 9 ‚Äì 16. https://doi.org/10.14618/ids-pub-9021\n[38] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual Name Tagging and Linking for 282\nLanguages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics, Vancouver, Canada, 1946‚Äì1958. https://doi.org/10.18653/v1/P17-1178\n[39] Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of\nthe ACL.\n[40] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. https://doi.org/10.48550/ARXIV.2204.05149\n[41] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer.arXiv e-prints (2019). arXiv:1910.10683\n[43] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don‚Äôt Know: Unanswerable Questions for SQuAD. arXiv:1806.03822 [cs.CL]\n[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text.\narXiv:1606.05250 (2016). arXiv:1606.05250\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\n(IJCV) 115, 3 (2015), 211‚Äì252. https://doi.org/10.1007/s11263-015-0816-y\n[46] Gustavo Santana. 2023. Stable Diffusion Prompts. https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts\n[47] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, Seattle, Washington, USA, 1631‚Äì1642. https://www.aclweb.org/anthology/D13-1170\n[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n[50] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.\nIn Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 . 142‚Äì147. https://www.aclweb.org/anthology/W03-0419\n[51] US Environmental Protection Agencyy. 2024. Greenhouse Gases Equivalencies Calculator - Calculations and References. https://www.epa.gov/\nenergy/greenhouse-gases-equivalencies-calculator-calculations-and-references\n17", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\n[27] Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, and Julien Launay.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "A Holistic Assessment of the Carbon Footprint\nof Noor, a Very Large Arabic Language Model.", "metadata": {}}, {"text": "In Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large\nLanguage Models.", "metadata": {}}, {"text": "Association for Computational Linguistics, virtual+Dublin, 84‚Äì94.", "metadata": {}}, {"text": "https://doi.org/10.18653/v1/2022.bigscience-1.8\n[28] George Leopold.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "AWS to Offer NVIDIA‚Äôs T4 GPUs for AI Inferencing.", "metadata": {}}, {"text": "www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll‚Äôar, and C Lawrence Zitnick.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Microsoft\nCOCO: Common objects in context.", "metadata": {}}, {"text": "In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13.", "metadata": {}}, {"text": "Springer, 740‚Äì755.", "metadata": {}}, {"text": "[30] Alexandra Sasha Luccioni and Alex Hernandez-Garcia.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Counting carbon: A survey of factors influencing the emissions of machine learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.08476 (2023).", "metadata": {}}, {"text": "[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.02001 (2022).", "metadata": {}}, {"text": "[32] Andrew L.", "metadata": {}}, {"text": "Maas, Raymond E.", "metadata": {}}, {"text": "Daly, Peter T.", "metadata": {}}, {"text": "Pham, Dan Huang, Andrew Y.", "metadata": {}}, {"text": "Ng, and Christopher Potts.", "metadata": {}}, {"text": "2011.", "metadata": {}}, {"text": "Learning Word Vectors for Sentiment\nAnalysis.", "metadata": {}}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies .", "metadata": {}}, {"text": "Association for\nComputational Linguistics, Portland, Oregon, USA, 142‚Äì150.", "metadata": {}}, {"text": "http://www.aclweb.org/anthology/P11-1015\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Pointer Sentinel Mixture Models.", "metadata": {}}, {"text": "arXiv:1609.07843 [cs.CL]\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong,\nHailey Schoelkopf, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Crosslingual generalization through multitask finetuning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.01786 (2022).", "metadata": {}}, {"text": "[35] Shashi Narayan, Shay B.", "metadata": {}}, {"text": "Cohen, and Mirella Lapata.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Don‚Äôt Give Me the Details, Just the Summary!", "metadata": {}}, {"text": "Topic-Aware Convolutional Neural\nNetworks for Extreme Summarization.", "metadata": {}}, {"text": "ArXiv abs/1808.08745 (2018).", "metadata": {}}, {"text": "[36] Will Oremus.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "AI chatbots lose money every time you use them.", "metadata": {}}, {"text": "That is a problem.", "metadata": {}}, {"text": "Washington Post (2023).", "metadata": {}}, {"text": "https://www.washingtonpost.com/\ntechnology/2023/06/05/chatgpt-hidden-cost-gpu-compute/\n[37] Pedro Javier Ortiz Su‚Äôarez, Benoit Sagot, and Laurent Romary.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Asynchronous pipelines for processing huge corpora on medium to low resource\ninfrastructures (Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019.", "metadata": {}}, {"text": "Cardiff, 22nd July 2019) , Piotr Ba≈Ñski,\nAdrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L\"ungen, and Caroline Iliadi (Eds.).", "metadata": {}}, {"text": "Leibniz-Institut f\"ur\nDeutsche Sprache, Mannheim, 9 ‚Äì 16.", "metadata": {}}, {"text": "https://doi.org/10.14618/ids-pub-9021\n[38] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Cross-lingual Name Tagging and Linking for 282\nLanguages.", "metadata": {}}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .", "metadata": {}}, {"text": "Association for\nComputational Linguistics, Vancouver, Canada, 1946‚Äì1958.", "metadata": {}}, {"text": "https://doi.org/10.18653/v1/P17-1178\n[39] Bo Pang and Lillian Lee.", "metadata": {}}, {"text": "2005.", "metadata": {}}, {"text": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.", "metadata": {}}, {"text": "In Proceedings of\nthe ACL.", "metadata": {}}, {"text": "[40] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.", "metadata": {}}, {"text": "https://doi.org/10.48550/ARXIV.2204.05149\n[41] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.10350 (2021).", "metadata": {}}, {"text": "[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.", "metadata": {}}, {"text": "Liu.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer.arXiv e-prints (2019).", "metadata": {}}, {"text": "arXiv:1910.10683\n[43] Pranav Rajpurkar, Robin Jia, and Percy Liang.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Know What You Don‚Äôt Know: Unanswerable Questions for SQuAD.", "metadata": {}}, {"text": "arXiv:1806.03822 [cs.CL]\n[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text.", "metadata": {}}, {"text": "arXiv:1606.05250 (2016).", "metadata": {}}, {"text": "arXiv:1606.05250\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, Alexander C.", "metadata": {}}, {"text": "Berg, and Li Fei-Fei.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "ImageNet Large Scale Visual Recognition Challenge.", "metadata": {}}, {"text": "International Journal of Computer Vision\n(IJCV) 115, 3 (2015), 211‚Äì252.", "metadata": {}}, {"text": "https://doi.org/10.1007/s11263-015-0816-y\n[46] Gustavo Santana.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Stable Diffusion Prompts.", "metadata": {}}, {"text": "https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts\n[47] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.", "metadata": {}}, {"text": "[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.", "metadata": {}}, {"text": "Manning, Andrew Ng, and Christopher Potts.", "metadata": {}}, {"text": "2013.", "metadata": {}}, {"text": "Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank.", "metadata": {}}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing.", "metadata": {}}, {"text": "Association for Computational Linguistics, Seattle, Washington, USA, 1631‚Äì1642.", "metadata": {}}, {"text": "https://www.aclweb.org/anthology/D13-1170\n[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Energy and policy considerations for deep learning in NLP.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1906.02243 (2019).", "metadata": {}}, {"text": "[50] Erik F.", "metadata": {}}, {"text": "Tjong Kim Sang and Fien De Meulder.", "metadata": {}}, {"text": "2003.", "metadata": {}}, {"text": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.", "metadata": {}}, {"text": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 .", "metadata": {}}, {"text": "142‚Äì147.", "metadata": {}}, {"text": "https://www.aclweb.org/anthology/W03-0419\n[51] US Environmental Protection Agencyy.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Greenhouse Gases Equivalencies Calculator - Calculations and References.", "metadata": {}}, {"text": "https://www.epa.gov/\nenergy/greenhouse-gases-equivalencies-calculator-calculations-and-references\n17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n[52] Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani,\nVictor Mustar, Helen Ngo, et al. 2022. Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement. arXiv preprint\narXiv:2210.01970 (2022).\n[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint arXiv:1905.00537 (2019).\n[54] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: A Large-Scale Prompt\nGallery Dataset for Text-to-Image Generative Models. arXiv:2210.14896 [cs] (2022). https://arxiv.org/abs/2210.14896\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan\nFuntowicz, et al. 2019. Huggingface‚Äôs transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n[56] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha\nLuccioni, Fran√ßois Yvon, et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n[57] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. ImageReward: Learning and Evaluating\nHuman Preferences for Text-to-Image Generation. arXiv:2304.05977 [cs.CV]\n[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies:\nTowards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV) .\n18", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\n[52] Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani,\nVictor Mustar, Helen Ngo, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2210.01970 (2022).", "metadata": {}}, {"text": "[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Understanding Systems.", "metadata": {}}, {"text": "arXiv preprint arXiv:1905.00537 (2019).", "metadata": {}}, {"text": "[54] Zijie J.", "metadata": {}}, {"text": "Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "DiffusionDB: A Large-Scale Prompt\nGallery Dataset for Text-to-Image Generative Models.", "metadata": {}}, {"text": "arXiv:2210.14896 [cs] (2022).", "metadata": {}}, {"text": "https://arxiv.org/abs/2210.14896\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan\nFuntowicz, et al.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Huggingface‚Äôs transformers: State-of-the-art natural language processing.", "metadata": {}}, {"text": "arXiv preprint arXiv:1910.03771 (2019).", "metadata": {}}, {"text": "[56] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha\nLuccioni, Fran√ßois Yvon, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "BLOOM: A 176B-parameter open-access multilingual language model.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.05100 (2022).", "metadata": {}}, {"text": "[57] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Sustainable AI: Environmental Implications, Challenges and Opportunities.", "metadata": {}}, {"text": "arXiv preprint arXiv:2111.00364 (2021).", "metadata": {}}, {"text": "[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "ImageReward: Learning and Evaluating\nHuman Preferences for Text-to-Image Generation.", "metadata": {}}, {"text": "arXiv:2304.05977 [cs.CV]\n[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Aligning Books and Movies:\nTowards Story-Like Visual Explanations by Watching Movies and Reading Books.", "metadata": {}}, {"text": "In The IEEE International Conference on Computer Vision (ICCV) .", "metadata": {}}, {"text": "18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nA FULL LIST OF TASK-SPECIFIC MODELS TESTED\nTask Models Task Models\nimage\nclassification\nmicrosoft/resnet-50\nmicrosoft/beit-base-patch16-224\ngoogle/vit-base-patch16-384\nfacebook/convnextv2-tiny-22k-384\nmicrosoft/resnet-18\ngoogle/mobilenet_v1_0.75_192\nfacebook/convnextv2-tiny-1k-224\ngoogle/vit-base-patch16-224\nquestion\nanswering\ndistilbert-base-uncased-distilled-squad\ndistilbert-base-cased-distilled-squad\ndeepset/roberta-base-squad2\nbert-large-uncased-whole-word-masking-finetuned-squad\ntimpal0l/mdeberta-v3-base-squad2\ndeepset/tinyroberta-squad2\ndeepset/electra-base-squad2\ndeepset/bert-large-uncased-whole-word-masking-squad2\nimage\ncaptioning\nnlpconnect/vit-gpt2-image-captioning\nSalesforce/blip-image-captioning-large\nSalesforce/blip-image-captioning-base\nmicrosoft/git-large-coco\nSalesforce/blip2-flan-t5-xl\nSalesforce/blip2-opt-2.7b\nydshieh/vit-gpt2-coco-en\nmicrosoft/git-base\nsummarization\nsshleifer/distilbart-xsum-12-6\nsshleifer/distilbart-cnn-12-6\npszemraj/led-large-book-summary\ngoogle/pegasus-xsum\ngoogle/pegasus-large\ngoogle/pegasus-multi_news\nfacebook/bart-large-cnn\nainize/bart-base-cnn\nimage\ngeneration\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1\nstabilityai/stable-diffusion-xl-base-1.0\nCompVis/stable-diffusion-v1-4\nprompthero/openjourney\ndreamlike-art/dreamlike-photoreal-2.0\nnota-ai/bk-sdm-tiny\nsegmind/tiny-sd\ntext\nclassification\ndistilbert-base-uncased-finetuned-sst-2-english\nnlptown/bert-base-multilingual-uncased-sentiment\ntwitter-roberta-base-sentiment-latest\ncardiffnlp/twitter-xlm-roberta-base-sentiment\nlvwerra/distilbert-imdb\nsiebert/sentiment-roberta-large-english\nfiniteautomata/bertweet-base-sentiment-analysis\nsbcBI/sentiment_analysis_mode\nmasked\nlanguage\nmodeling\nbert-base-uncased\nxlm-roberta-base\ndistilbert-base-uncased\nroberta-base\nalbert-base-v2\nbert-base-cased\nmicrosoft/deberta-base\nbert-base-multilingual-cased\ntext\ngeneration\ngpt2\nbigscience/bloom-560m\ndistilgpt2\nfacebook/opt-6.7b\nEleutherAI/gpt-neo-125m\ngpt2-medium\nfacebook/opt-1.3b\ngpt2-xl\nobject\ndetection\nfacebook/detr-resnet-50\nhustvl/yolos-tiny\njozhang97/deta-swin-large\nfacebook/detr-resnet-101\nhustvl/yolos-small\nSenseTime/deformable-detr\npolejowska/detr-r50-cd45rb-8ah-6l\npolejowska/detr-r50-cd45rb-1ah-6l\ntoken\nclassification\nQCRI/bert-base-multilingual-cased-pos-english\ndslim/bert-base-NER\ndslim/bert-large-NER\nJean-Baptiste/roberta-large-ner-english\noliverguhr/fullstop-punctuation-multilang-large\nBabelscape/wikineural-multilingual-ner\nml6team/keyphrase-extraction-distilbert-inspec\nobi/deid_roberta_i2b2\nTable 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.\n19", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nA FULL LIST OF TASK-SPECIFIC MODELS TESTED\nTask Models Task Models\nimage\nclassification\nmicrosoft/resnet-50\nmicrosoft/beit-base-patch16-224\ngoogle/vit-base-patch16-384\nfacebook/convnextv2-tiny-22k-384\nmicrosoft/resnet-18\ngoogle/mobilenet_v1_0.75_192\nfacebook/convnextv2-tiny-1k-224\ngoogle/vit-base-patch16-224\nquestion\nanswering\ndistilbert-base-uncased-distilled-squad\ndistilbert-base-cased-distilled-squad\ndeepset/roberta-base-squad2\nbert-large-uncased-whole-word-masking-finetuned-squad\ntimpal0l/mdeberta-v3-base-squad2\ndeepset/tinyroberta-squad2\ndeepset/electra-base-squad2\ndeepset/bert-large-uncased-whole-word-masking-squad2\nimage\ncaptioning\nnlpconnect/vit-gpt2-image-captioning\nSalesforce/blip-image-captioning-large\nSalesforce/blip-image-captioning-base\nmicrosoft/git-large-coco\nSalesforce/blip2-flan-t5-xl\nSalesforce/blip2-opt-2.7b\nydshieh/vit-gpt2-coco-en\nmicrosoft/git-base\nsummarization\nsshleifer/distilbart-xsum-12-6\nsshleifer/distilbart-cnn-12-6\npszemraj/led-large-book-summary\ngoogle/pegasus-xsum\ngoogle/pegasus-large\ngoogle/pegasus-multi_news\nfacebook/bart-large-cnn\nainize/bart-base-cnn\nimage\ngeneration\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1\nstabilityai/stable-diffusion-xl-base-1.0\nCompVis/stable-diffusion-v1-4\nprompthero/openjourney\ndreamlike-art/dreamlike-photoreal-2.0\nnota-ai/bk-sdm-tiny\nsegmind/tiny-sd\ntext\nclassification\ndistilbert-base-uncased-finetuned-sst-2-english\nnlptown/bert-base-multilingual-uncased-sentiment\ntwitter-roberta-base-sentiment-latest\ncardiffnlp/twitter-xlm-roberta-base-sentiment\nlvwerra/distilbert-imdb\nsiebert/sentiment-roberta-large-english\nfiniteautomata/bertweet-base-sentiment-analysis\nsbcBI/sentiment_analysis_mode\nmasked\nlanguage\nmodeling\nbert-base-uncased\nxlm-roberta-base\ndistilbert-base-uncased\nroberta-base\nalbert-base-v2\nbert-base-cased\nmicrosoft/deberta-base\nbert-base-multilingual-cased\ntext\ngeneration\ngpt2\nbigscience/bloom-560m\ndistilgpt2\nfacebook/opt-6.7b\nEleutherAI/gpt-neo-125m\ngpt2-medium\nfacebook/opt-1.3b\ngpt2-xl\nobject\ndetection\nfacebook/detr-resnet-50\nhustvl/yolos-tiny\njozhang97/deta-swin-large\nfacebook/detr-resnet-101\nhustvl/yolos-small\nSenseTime/deformable-detr\npolejowska/detr-r50-cd45rb-8ah-6l\npolejowska/detr-r50-cd45rb-1ah-6l\ntoken\nclassification\nQCRI/bert-base-multilingual-cased-pos-english\ndslim/bert-base-NER\ndslim/bert-large-NER\nJean-Baptiste/roberta-large-ner-english\noliverguhr/fullstop-punctuation-multilang-large\nBabelscape/wikineural-multilingual-ner\nml6team/keyphrase-extraction-distilbert-inspec\nobi/deid_roberta_i2b2\nTable 6.", "metadata": {}}, {"text": "The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nB MODEL EVALUATION\nFig. 7. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and summarization accuracy (y axis),\nwith dot size indicating the quantity of emissions.\nFig. 8. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and question answering accuracy (y axis),\nwith dot size indicating the quantity of emissions.\n20", "sentences": [{"text": "ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil Luccioni et al\nB MODEL EVALUATION\nFig.", "metadata": {}}, {"text": "7.", "metadata": {}}, {"text": "A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and summarization accuracy (y axis),\nwith dot size indicating the quantity of emissions.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and question answering accuracy (y axis),\nwith dot size indicating the quantity of emissions.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}, {"text": "[Image page=20 idx=1 name=Im8.png] Size: 800x400, Data: 61696 bytes", "sentences": [{"text": "[Image page=20 idx=1 name=Im8.png] Size: 800x400, Data: 61696 bytes", "metadata": {}}], "metadata": {"page": 20, "image_index": 1, "image_name": "Im8.png", "image_width": 800, "image_height": 400, "attachment_type": "image", "has_image_data": true, "image_data_size": 61696}}, {"text": "[Image page=20 idx=2 name=Im9.png] Size: 800x400, Data: 51502 bytes", "sentences": [{"text": "[Image page=20 idx=2 name=Im9.png] Size: 800x400, Data: 51502 bytes", "metadata": {}}], "metadata": {"page": 20, "image_index": 2, "image_name": "Im9.png", "image_width": 800, "image_height": 400, "attachment_type": "image", "has_image_data": true, "image_data_size": 51502}}], "metadata": {"page": 20}}, {"title": "Page 21", "paragraphs": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nmodel SST 2\n(acc)\nIMDB\n(acc)\nRotten\nTomatoes\n(acc)\nSciQ\n(acc)\nSQuAD\n(F1)\nSQuAD v2\n(F1, has\nanswer)\nSamSUM\n(ROUGE)\nXSum\n(ROUGE)\nCNN\n(ROUGE)\nbloomz-560m 0.92 0.94 0.85 0.92 0.43 0.21 0.23 0.15 0.10\nbloomz-1b7 0.94 0.97 0.93 0.96 0.50 0.25 0.26 0.16 0.18\nbloomz-3b 0.95 0.98 0.95 0.97 0.53 0.26 0.28 0.17 0.21\nbloomz-7b1 0.94 0.98 0.95 0.97 0.54 0.27 0.32 0.21 0.09\nflan-t5-xxl 0.96 0.97 0.92 0.72 0.98 0.49 0.30 0.37 0.23\nflan-t5-xl 0.96 0.97 0.93 0.66 0.97 0.49 0.49 0.38 0.24\nflan-t5-large 0.94 0.96 0.92 0.53 0.97 0.50 0.45 0.30 0.24\nflan-t5-base 0.93 0.95 0.88 0.61 0.95 0.48 0.46 0.32 0.23\ndistilbert-base-uncased\n-distilled-squad 0.44 0.87 0.86\ndistilbert-base-cased-\ndistilled-squad 0.46 0.87 0.87\ndeepset/roberta-base-squad2 0.48 0.93 0.83\nbert-large-uncased-whole-\nword-masking-finetuned-squad 0.48 0.93 0.84\ntimpal0l/mdeberta-v3-\nbase-squad2 0.46 0.91 0.90\ndeepset/tinyroberta-squad2 0.45 0.98 0.91\ndeepset/electra-base-squad2 0.48 0.89 0.82\ndeepset/bert-large-uncased-\nwhole-word-masking-squad2 0.46 0.92 0.92\nsshleifer/distilbart-xsum-12-6 0.20 0.45 0.23\nsshleifer/distilbart-cnn-12-6 0.29 0.21 0.44\npszemraj/led-large-\nbook-summary 0.33 0.16 0.33\npegasus-xsum 0.22 0.22 0.22\npegasus-large 0.27 0.17 0.34\npegasus-multi_news 0.12 0.16 0.29\nfacebook/bart-large-cnn 0.32 0.21 0.44\nainize/bart-base-cnn 0.27 0.16 0.26\ndistilbert-base-uncased-\nfinetuned-sst-2-english 0.99 0.88 0.90\nnlptown/bert-base-\nmultilingual-uncased-sentiment 0.75 0.85 0.73\ntwitter-roberta-base-\nsentiment-latest 0.82 0.80 0.77\ncardiffnlp/twitter-xlm-roberta-\nbase-sentiment 0.79 0.71 0.74\nlvwerra/distilbert-imdb 0.88 0.93 0.82\nsiebert/sentiment-roberta-\nlarge-english 0.92 0.92 0.92\nfiniteautomata/bertweet-\nbase-sentiment-analysis 0.82 0.72 0.77\nsbcBI/sentiment_analysis_model 0.81 0.75 0.76\nTable 7. Full performance metrics for the 32 models (24 finetuned, 8 multi-purpose) that we evaluated as part of our study.\n21", "sentences": [{"text": "Power Hungry Processing ACM FAccT ‚Äô24, June 3‚Äì6, 2024, Rio de Janeiro, Brazil\nmodel SST 2\n(acc)\nIMDB\n(acc)\nRotten\nTomatoes\n(acc)\nSciQ\n(acc)\nSQuAD\n(F1)\nSQuAD v2\n(F1, has\nanswer)\nSamSUM\n(ROUGE)\nXSum\n(ROUGE)\nCNN\n(ROUGE)\nbloomz-560m 0.92 0.94 0.85 0.92 0.43 0.21 0.23 0.15 0.10\nbloomz-1b7 0.94 0.97 0.93 0.96 0.50 0.25 0.26 0.16 0.18\nbloomz-3b 0.95 0.98 0.95 0.97 0.53 0.26 0.28 0.17 0.21\nbloomz-7b1 0.94 0.98 0.95 0.97 0.54 0.27 0.32 0.21 0.09\nflan-t5-xxl 0.96 0.97 0.92 0.72 0.98 0.49 0.30 0.37 0.23\nflan-t5-xl 0.96 0.97 0.93 0.66 0.97 0.49 0.49 0.38 0.24\nflan-t5-large 0.94 0.96 0.92 0.53 0.97 0.50 0.45 0.30 0.24\nflan-t5-base 0.93 0.95 0.88 0.61 0.95 0.48 0.46 0.32 0.23\ndistilbert-base-uncased\n-distilled-squad 0.44 0.87 0.86\ndistilbert-base-cased-\ndistilled-squad 0.46 0.87 0.87\ndeepset/roberta-base-squad2 0.48 0.93 0.83\nbert-large-uncased-whole-\nword-masking-finetuned-squad 0.48 0.93 0.84\ntimpal0l/mdeberta-v3-\nbase-squad2 0.46 0.91 0.90\ndeepset/tinyroberta-squad2 0.45 0.98 0.91\ndeepset/electra-base-squad2 0.48 0.89 0.82\ndeepset/bert-large-uncased-\nwhole-word-masking-squad2 0.46 0.92 0.92\nsshleifer/distilbart-xsum-12-6 0.20 0.45 0.23\nsshleifer/distilbart-cnn-12-6 0.29 0.21 0.44\npszemraj/led-large-\nbook-summary 0.33 0.16 0.33\npegasus-xsum 0.22 0.22 0.22\npegasus-large 0.27 0.17 0.34\npegasus-multi_news 0.12 0.16 0.29\nfacebook/bart-large-cnn 0.32 0.21 0.44\nainize/bart-base-cnn 0.27 0.16 0.26\ndistilbert-base-uncased-\nfinetuned-sst-2-english 0.99 0.88 0.90\nnlptown/bert-base-\nmultilingual-uncased-sentiment 0.75 0.85 0.73\ntwitter-roberta-base-\nsentiment-latest 0.82 0.80 0.77\ncardiffnlp/twitter-xlm-roberta-\nbase-sentiment 0.79 0.71 0.74\nlvwerra/distilbert-imdb 0.88 0.93 0.82\nsiebert/sentiment-roberta-\nlarge-english 0.92 0.92 0.92\nfiniteautomata/bertweet-\nbase-sentiment-analysis 0.82 0.72 0.77\nsbcBI/sentiment_analysis_model 0.81 0.75 0.76\nTable 7.", "metadata": {}}, {"text": "Full performance metrics for the 32 models (24 finetuned, 8 multi-purpose) that we evaluated as part of our study.", "metadata": {}}, {"text": "21", "metadata": {}}], "metadata": {"page": 21}}], "metadata": {"page": 21}}]}