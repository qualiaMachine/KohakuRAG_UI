{"document_id": "patterson2021", "title": "Carbon Emissions and Large Neural Network Training", "text": "Carbon   Emissions   and   Large   Neural   Network   Training     \nDavid   Patterson 1 , 2 ,   Joseph   Gonzalez 2 ,   Quoc   Le 1 ,   Chen   Liang 1 ,   Lluis-Miquel   Munguia 1 ,     \nDaniel   Rothchild 2 ,   David   So 1 ,   Maud   Texier 1 ,   and   Jeff   Dean 1   \n{davidpatterson,   qvl,   crazydonkey,   llmunguia,   davidso,   maudt,   jeff}@google.com,     \n{pattrsn,   jegonzal,   drothchild}@berkeley.edu\n\nAbstract:    The   computation   demand   for   machine   learning   (ML)    has   grown   rapidly    recently,   which   comes   with   a   \nnumber   of   costs.   Estimating   the   energy   cost   helps   measure   its   environmental   impact   and   finding   greener   \nstrategies,   yet   it   is    challenging   without   detailed   information .   \nWe   calculate   the   energy   use   and   carbon   footprint   of   several   recent   large   models— T5 ,    Meena ,    GShard ,   \nSwitch   Transformer ,   and    GPT-3 —and   refine   earlier   estimates   for   the   neural   architecture   search   that   found   \nEvolved   Transformer .   \nWe   highlight   the   following   opportunities   to   improve   energy   efficiency   and    CO 2    equivalent   emissions    ( CO 2 e ):   \n● Large   but   sparsely   activated   DNNs   can   consume   <1/10th   the   energy   of   large,   dense   DNNs   without   \nsacrificing   accuracy   despite   using   as   many   or   even   more   parameters.   \n● Geographic   location   matters   for   ML   workload   scheduling   since   the   fraction   of   carbon-free   energy   and   \nresulting   CO 2 e   vary   ~5X-10X,   even   within   the   same   country   and   the   same   organization.   We   are   now   \noptimizing   where   and   when   large   models   are   trained.   \n● Specific   datacenter   infrastructure   matters,   as   Cloud   datacenters   can   be   ~1.4-2X   more   energy   efficient   \nthan   typical   datacenters,   and   the   ML-oriented   accelerators   inside   them   can   be   ~2-5X   more   effective   \nthan   off-the-shelf   systems.   \nRemarkably,   the   choice   of   DNN,   datacenter,   and   processor   can   reduce   the   carbon   footprint   up   to   ~100-1000X.     \nThese   large   factors   also   make   retroactive   estimates   of   energy   cost   difficult.   To   avoid   miscalculations,   we   \nbelieve   ML   papers   requiring   large   computational   resources   should   make   energy   consumption   and   CO 2 e   \nexplicit   when   practical.   We   are   working   to   be   more   transparent   about   energy   use   and   CO 2 e   in   our   future   \nresearch.   To   help   reduce   the   carbon   footprint   of   ML,   we   believe   energy   usage   and   CO 2 e   should   be   a   key   \nmetric   in   evaluating   models,   and   we   are   collaborating   with   MLPerf   developers   to   include   energy   usage   during   \ntraining   and   inference   in   this   industry   standard   benchmark.   \n1.Introduction   \nAs   ML   models   increase   in   scale,   a   general   trend   is   that   they   become   more   accurate   and   more   capable.   \nHowever,   larger   models   translate   to   greater   computing   demands   and,   by   extension,   greater   energy   demands.   \nWe   focus   on   natural   language   processing   (NLP)   because   it   is   important   in   Google   products   and   because   of   the   \nrecent   development   of   many   large   NLP   models,   e.g.,   T5   [Raf19],   Meena   [Adi20],   GShard   [Lep20],   Switch   \nTransformer   [Fed21],   and   GPT-3   [Bro20].    Recent   studies    attempt   to   evaluate   the   environmental   impact   of   this   \ntrend   in   NLP,   which   is   difficult   [Str19].   Here   we   investigate   and   share   the   estimates   of   the   energy   consumed   \nand   CO 2 e 3    of   these   recent   and   large   NLP   models.   We   also   reduce   by   88X   an   earlier   estimate   of   the   CO 2 e   for   \nthe   neural   architecture   search   for   Evolved   Transformer   [So19,   Str19]   by   characterizing   the   actual   search   \nprocess   on   the   hardware   and   datacenter   on   which   it   was   performed   (see   Appendices   C   and   D).   \n      Our   investigation   into   CO 2 e   revealed   surprises   and   misunderstandings   about   the   full   Deep   Neural   Network  \n(DNN)   lifecycle,   the   datacenters   and   hardware   that   run   them,   the   variations   in   energy   mix,   and   the   difficulty   of   \nassessing   CO 2 e   accurately.   Note   that   we   are   evaluating   the   CO 2 e   of    operating    computers   and   datacenters,   but   \nnot   fabricating   and   recycling   them   (see   [Gup20]   for   the   latter   topic).   \nTo   make   it   easier   for   the   ML   community   to   understand   the   real   impact   of   training   and   how   to   reduce   it,   we   \nendorse   prior   calls   for   new   publication   norms   for   computationally   intensive   ML   models:   \n1  Google   \n2  University   of   California,   Berkeley   \n3  “CO 2 e”   means   CO 2     equivalent   emissions ,   accounting   for   carbon   dioxide   and   all   the   other   greenhouse   gases   as   well:   \nmethane,   nitrous   oxide,   ...   (calculated   from   Equation   A-1   in    40   Code   of   Federal   Regulations   98 ).   “CO 2    emissions”   is   only   \ncarbon   dioxide.    tCO 2 e    stands   for   1000   kg   (metric   ton)   of   CO 2     equivalent   emissions .   \n1\n\n1. We   must   assess   CO 2 e   correctly,   but   it   is   hard   to   quantify   precisely   in   part   because   all   the   required   \ninformation   is   rarely   reported   or   publicly   available   (e.g.,   datacenter,   hardware,   energy   mix)   and   in   part   \nbecause   it   is   hard   to   uncover   important   details   afterwards   (see   Section   4.1).   To   make   the   carbon   costs   \nof   training   transparent,   we   encourage   more   researchers   to   measure   energy   usage   and   CO 2 e—or   to   get   \na   rough   estimate   using   a   tool   like   ML   Emissions   Calculator   [Lac19]   (Section   4.3)—and   publish   the   data.     \n2. We   agree   with   [Str19,Sch20,Hen20]   that   efficiency   should   be   an   evaluation   criterion   for   publishing   ML   \nresearch   on   computationally   intensive   models   besides   accuracy   and   related   measures,   since   we   need   \nto   encourage   advances   across   the   board   as    the   most   sustainable   energy   is   the   energy   you   don’t   use .     \n3. And   even   if   we   could   bring   CO 2 e   to   zero   in   cloud   datacenters,   reducing   training   time   matters,   both   \nbecause   “time   is   money,”   and   because   cheaper   training   lets   more   people   participate.   Hence,   we   also   \nsecond   the   recommendation   of   [Str19]   for   more   researchers   to   publish   the   number   of   accelerators   and   \ntheir   time   to   train   computationally   intensive   models   to   inspire   progress   in   reducing   training   costs.   \nWe   believe   such   new   incentives   could   lead   to   a   virtuous   cycle   where   ML   practitioners   compete   to   increase   \naccuracy   while   lowering   energy   consumption   and   CO 2 e   that   could   bend   the   curve   of   ML   carbon   footprint   \ngrowth   for   computationally   intensive   NLP   models.   \nThe   following   sections   summarize   the   findings   that   led   to   these   recommendations.   They   also   document   our   \nCO 2 e   estimates,   highlight   recent   advances   that   curb   the   CO 2 e   of   ML,   and   estimate   the   CO 2 e   from   training   the   \nfive   recent   large   NLP   models   mentioned   above.   We   end   by   updating   the   results   of   [Str19]   on   the   emissions   of   \nthe   Evolved   Transformer   neural   architecture   search   and   discussing   common   misperceptions.     \nWe   start   with   an   overview   of   the   carbon   footprint   over   the   DNN   lifecycle   and   show   ways   to   improve   a   \nconcrete   example   by   nearly   two   orders   of   magnitude.\n\n2.Energy   Consumption   and   Carbon   Footprint   of   an   NLP   Model   \nElectricity   required   to   run   an   ML   model   is   a   function   of   the   algorithm,   the   program   that   implements   it,   the   \nnumber   of   processors   that   run   the   program,   the   speed   and   power   of   those   processors,   a   datecenter’s   \nefficiency   in   delivering   power   and   cooling   the   processors,   and   the   energy   supply   mix   (renewable,   gas,   coal,   \netc.).   A   simplified   formula   for   the   carbon   footprint   of   an   ML   model   that   takes   these   factors   into   account   is:   \n ootprint  electrical energy ueries  ) KWh F =( train  +q ×electrical energyinference ×CO2e \ndatacenter/  \nMost   companies   spend   more   energy   on   serving   a   DNN   model   (performing   inference)   than   on   training   it.   \nFor   example,   NVIDIA   estimated   that   80–90%   of   the   ML   workload   is   inference   processing   [Leo19].   Similarly,   \nAmazon   Web   services   claimed   that    90%    of   the   ML   demand   in   the   cloud   is   for   inference   [Bar19].   Given   its   \nsubstantial   role   in   the   ML   model   lifecycle,   Alibaba,   Amazon,   Google,   and   NVIDIA   designed   ML   accelerators   \nsolely   for   inference.   If   the   total   ML   energy   is   split   10%   on   training   and   90%   on   serving,   then   even   if   a   given   ML   \nmodel   required   double   the   energy   cost   of   training,   it   could   reduce   overall   total   carbon   emissions   if   that   model   \nalso   cut   serving   energy   by   20%.   Because   energy   usage   during   training   is   more   isolated   and   thus   easier   to   \ninvestigate   than   inference,   we   focus   on   it   in   this   paper,   but   keep   in   mind   that   the   carbon   footprint   of   inference   is   \nsignificant.   \nAn   ML   practitioner   is   often   improving   the   quality   of   an   existing   model   rather   than   starting   from   scratch.   We   \nwill   use   as   a   running   example   (found   in   [Str19])   the   CO 2 e   impact   of   going   from   training   a   Transformer   model   \nusing   off-the-shelf   hardware   in   an   average   datacenter   to   training   an   Evolved   Transformer   model   on   Google’s   \ncustom   hardware   for   DNNs   in   Google’s   energy   optimized   datacenters.   The   large   impact   of   each   factor   in   this  \nexample   demonstrates   why   we   suggest   that   the   trainers   of   a   model   be   involved   in   the   calculation   of   its   costs.    \nTable   1   shows   the   CO 2 e   breakdown,   which   we   explain   further   in   the   next   subsections   along   with   the   \nbusiness   rationale   for   these   improvements,   demonstrating   the   cross-cutting   incentives   for   more   efficient   ML.   \nFigure   1   illustrates   the   gains   per   step;   the   overall   improvement   in   CO 2 e   is   57X.   This   large   gain   demonstrates   \nwhy   the   selection   of   the   DNN   model,   processor,   datacenter,   and   geographic   location   are   critical   to   improve   \nCO 2 e.   Table   2   shows   the   units   for   CO 2 e   and   a   running   example   that   puts   these   units   into   perspective.   \nWe   next   go   over   the   four   factors   in   more   detail   that   contribute   to   the   carbon   footprint   of   training.\n\n2\n\nTable   1.   See   Appendix   A   for   more   detail 4 .   Estimates   of   CO 2 e   for   Transformer   and   Evolved   Transformer   \nfor   P100   and   TPU   v2   are   based   on   power   measurements. 5    Evolved   Transformer   (Medium)   reached   the   \nsame   accuracy   as   Transformer   (Big)   in   [So19].   CO 2 e     is   shown   both   before   (“gross”)   and   after   (“net”)   \naccounting   for   24/7   reduction   via   real   time,   local   carbon   free   energy   purchases   (Appendix   B).   To   help   \nput   the   CO 2 e   numbers   in   perspective,   a   single   passenger   round   trip   SF-NY   is   ~1.2t   CO 2 e   (Table   2).\n\nFigure   1.   Improvement   in   CO 2 e   over   Transformer   (Big)   on   P100   GPUs   in   an   average   US   datacenter   \nversus   Evolved   Transformer   (Medium)   on   TPU   v2s   in   the   Google   Iowa   datacenter.\n\nTable   2.   Small   and   large   units   for   energy   and   carbon   footprint   in   this   paper,   plus   airline   travel   CO 2 e   \nused   for   perspective   on   the   relative   size   of   ML   emissions   compared   to   other   activities   (Section   4.8).   \n4  The   peak   TeraFLOPS/second   is   19   for   P100   and   46   for   TPU   v2.   \n5  Training   on   TPU   v3   instead   of   TPU   v2   takes   Transformer   (Big)   0.44   days   (averaging   61   TFLOPS/s)   and   0.37   days   (47   \nTFLOPS/s)   for   Evolved   Transformer   (Medium).   For   TPU   v4,   the   respective   numbers   are   0.25   days   (93   TFLOPS/s)    and   \n0.19   days   (73   TFLOPS/s).   TPU   v3   shrinks   energy   consumed   and   gross   and   net   CO 2 e   from   TPU   v2   by   ~1.4X   for   \nTransformer   and   by   ~1.3X   for   Evolved   Transformer.     \n3   \nModel   Transformer   (Big)   \nEvolved   \nTransformer   \n(Medium)   \nTransformer   \n(Big)   \nEvolved   \nTransformer   \n(Medium)   \nNumber   of   Parameters   (B)   0.21   0.13   0.21   0.13   \nDatacenter   US   Average  Google   Iowa   Council   Bluffs   \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh)   2020   \n(Section   2.4   and   Appendix   D)   0.429   0.478   \nDatacenter   Net   CO 2 e/KWh   (kg/KWh)   2020   (Section   \n2.4   and   Appendix   D)   0.429   0.080   \nDatacenter   PUE   (Latest   quarter   2020)   1.59   1.11   \nProcessor   P100   TPU   v2   \nChip   Thermal   Design   Power   (TDP   in   Watts)   300   280   \nMeasured   System   Average   Power   including   \nmemory,   network   interface,   fans,   host   CPU   (Watts)   296   271   229   227   \nMeasured   Performance   (TFLOPS/s) 5   6.7   4.7   28.8   24.0   \nNumber   of   Chips   8   \nTraining   time   to   accuracy   goal   (days)   3.5   3.2   0.81   0.62   \nTotal   Computation   (floating   point   operations)   1.61E+19   1.03E+19   1.61E+19   1.03E+19   \nEnergy   consumption   (KWh)   316   221   185   40   30   \nGross   CO 2 e   for   Model   Training   (metric   ton)   (Section  \n2.4   and   Appendix   D)   0.1357   0.1055   0.0883   0.0189   0.0143   \nNet   CO 2 e   for   Model   Training   (metric   ton)   (Section   \n2.4   and   Appendix   D)   0.1357   0.0177   0.0148   0.0032   0.0024   \n%   24/7   net   carbon   free   energy   (CY   2019)   N/A   78%   \n  Small   Unit   Large   Unit   \nEnergy   Consumption   Kilowatt   hours   (KWh)   Megawatt   hours   (MWh   =   1000   KWh)   \nCarbon   Footprint   (CO 2 e   or   CO 2 )   Kilograms   (kg)   Metric   ton   (t   =   1000   kg)   \nPerspective   (see   Appendix   A)   Single   passenger   round   \ntrip   SF-NY   (1.2t   CO 2 e)   Passenger   jet   plane   round   trip   SF-NY   (180t   CO 2 e)\n\n[Image page=3 idx=1 name=X39.png] Size: 1386x522, Data: 45512 bytes\n\n2.1   Algorithm/program   improvement   \nThe   Evolved   Transformer   (Medium)   model   discovered   by   So   et   al.   [So19]   using   neural   architecture   search   \n(see   Section   4.1)   uses   1.6X   fewer   FLOPS   and   1.1X–1.3X   less   time   than   Transformer   (Big)   at   slightly   higher   \naccuracy   (see   Table   1   and   Appendix   A) 6 .   \nBusiness   Rationale .   Training   faster   saves   ML   researchers   time   as   well   as   saves   their   organizations   money   \nand   reduces   CO 2 e.   \n2.2   Processor   improvement   \nGoogle’s   custom   TPU   v2   processor   runs   Transformer   (Big)   4.3X   faster   than   P100   GPUs   and   Evolved   \nTransformer   (Medium)   5.2X   faster. 7    TPU   v2   also   uses   less   power:   1.3X   less   for   Transformer   and   1.2X   less   for   \nEvolved   Transformer.   The   net   gain   in   performance/Watt   is   5.6X   and   6.2X,   respectively.   \nBusiness   Rationale .   The   substantial   increase   in   the   scope   and   scale   of   deep   learning   over   the   past   decade   \nhas   created   the   opportunity   to   build   customized   hardware   that   is   tailored   to   the   kinds   of   computations   involved   \nin   training   and   serving   DNN   models.   Instead   of   using   GPUs   like   many   other   organizations,   over   the   past   seven   \nyears   Google   has   designed,   built,   and   deployed   four   generations   of   custom   Tensor   Processing   Unit   (TPU)   \nhardware   for   DNNs   to   accelerate   model   training   and   serving   [Jou21].   To   get   a   better   return   on   their   investment,   \ncloud   companies   actually   aim   for   improved    cost-performance,    as   opposed   to   simply   performance.   Cost   here   \nmeans    Total   Cost   of   Ownership    ( TCO ),   which   includes   the   annual   operating   costs   such   as   electricity   consumed   \nand   amortization   of   capital   expenditures   for   the   computer,   cooling,   power   distribution,   and   the   building.   Jouppi   \net   al .   show   that   power   consumption   is   nearly   perfectly   linearly   correlated   with   TCO 8    [Jou21],   so   \nperformance/TCO   gains   also   help   performance/Watt,   saving   money   and   reducing   CO 2 e.     \n2.3   Datacenter   improvement   \nA   useful   quantitative   metric   of   datacenter   efficiency   is   the   energy   overhead   above   and   beyond   what   directly   \npowers   the   computing   equipment   inside   the   datacenters.   If   the   overhead   were   50%,   the    Power   Usage   \nEffectiveness    ( PUE )   would   be   1.50.   The   US   national   datacenter   average   in   2018   was   1.58,   which   is   the   value   \n[Str19]     used ;    In   2020,   it   was   1.59 .   Google    publishes   its   datacenter   PUE   online   every   quarter .   The   PUE   for   the   \nIowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,   a   factor   of   1.4X   better.   Cloud   datacenters   are   \nroughly   2X   as   energy   efficient   as   a   typical   enterprise   datacenter   due   to   other   factors   like   server   utilization   (see  \n[Höl20]),   but   we’ll   limit   the   quantitative   improvement   in   this   paper   to   the   easy-to-measure   PUE.   \nMore   broadly,   since   cloud   datacenters   are   much   more   energy   efficient,   the   long-feared   explosion   of   \ndatacenter   energy   usage   has   not   materialized.   A   recent   paper   in    Science    [Mas20]   found   that   global   datacenter   \nenergy   consumption   increased   by   only   6%   compared   with   2010,   despite   computing   capacity   increasing   by   \n550%   over   the   same   time   period   [Mas21].   \nBusiness   Rationale .   Cloud   companies   strive   for   energy   efficient   datacenters   since   it   saves   money   and   \nlowers   emissions.   Perhaps   we   should   add   “energy   is   money”   to   Ben   Franklin’s   “time   is   money”   advice?     \n2.4   Energy   mix   improvement   \n  The   gross   carbon   intensity   of   energy   according   to   the   U.S.   average   mix   is    0.429    kg   of   CO 2 e/KWh   [USE21].   \nAfter   matching   Google’s   clean   energy   purchase   per   its   24/7   carbon-free   energy   framework   (see   Appendix   B),   \nthe   net   CO 2 e   drops   to   0.080   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer,   which   is   5.4X   better.   \nBusiness   Rationale .   Transmitting   electricity   long   distances   is   more   expensive   and   less   efficient   than   \nsending   information   as   photons   over   optical   fibers   [Arm10].   Cloud   computing   allows   companies   like   Google   to   \nhave   a   global   portfolio   of   datacenters,   many   of   which   are   placed   where   the   grid   is   cleaner   (e.g.,   Finland)   or   \nwhere   companies   can   purchase   clean   energy   directly   (e.g.,   Iowa).   In   2020   Google   announced   a   new   objective   \nin   its   energy   strategy:   by   2030,   it   aims   to   run   all   Google   datacenters   and   offices   on   carbon-free   energy   24/7.   \nFor   our   24/7   carbon-free   energy   accounting    (see   Appendix   B),   we   deduct   from   the   hourly   consumption   all   \n6  Their   neural   architecture   search   also   found   another   version   that   had   the   same   performance   but   better   accuracy.   \n7  [Str19]   used   P100s,   which   are   contemporary   GPUs   to   TPU   v2s.   \n8  The   correlation   coefficient   R   between   TCO   and   TDP   is   0.99   out   of   1.00   across   four   generations   of   TPUs.   \n4\n\nclean   energy   purchased   on   that   same   geographically   local   grid   and   the   same   hour,   which   results   in   the   net   \nCO 2 e/KWh   value.   As   Iowa   has   strong   nighttime   winds,   Google’s   wind   portfolio   lowered   Iowa's   datacenter    gross   \naverage   CO 2 e/KWh   in   December   2020   by   6X,   from   the   local   grid’s   0.478   kg   to   a    net    average   of   0.080   kg.     \n2.5   Summary:   Formulas   for   energy   consumption   and   carbon   footprint   of   training   \nReducing   CO 2 e   is   not   only   a   moral   obligation   but   ultimately   sound   business.   To   decrease   the   footprint   of   \ntraining,   an   ML   researcher   should   pick   the   DNN   model,   the   processor,   and   the   datacenter   carefully. 9    Cutting   \nenergy   saves   money   and   CO 2 e   and   improving   the   energy   mix   reduces   CO 2 e.   We   refactor   the   equation   above   \nfor   training   into   energy   consumption   and   its   carbon   footprint   (tCO 2 e     means   metric   tons   of   CO 2 e):   \n ours to train  umber of Processors verage Power per Processor UE  KWh  =H ×N ×A ×P ÷1000   \n CO2e Wh g CO2e per KWh 000 t =K ×k ÷1  \nWe   believe   it   is   straightforward   for   ML   practitioners   to   calculate   energy   consumption.   They   already   know   \nhours   to   train   and   number   of   processors.   Google   and   Facebook   publish   PUE   of   their   datacenters,   so   that   is   \neasy   to   look   up   for   those   clouds.   If   cloud   providers   don’t   share   PUE,   use   the   US   average   PUE   as   in   [Str19].   \nWe   measured   the   power   of   the   processors   during   training,   which   is   ideal,   but   using   the   average   of   the   training   \nof   several   similar   models   is   probably   sufficient   and   much   easier. 10    Table   3   shows   the   average   power   and   \nstandard   deviation   for   the   processors   and   DNNs   that   we   measured   in   this   paper.     \nThe   final   piece   is   the   CO 2 e   of   the   datacenter   at   the   time   the   model   was   run.   Google   calculates   the   average   \nper   month,   which   is   close   enough,   and   it   is   now   available   for   Google   employees   to   look   up.   Without   access   to   \nsuch   a   dashboard,   use   the   ML   Emissions   Calculator   [Lac19]   or   Green   Algorithms   tool   [Lan20]   that   estimate   the   \nCO 2 e   mix   by   region   (see   Figure   6   below) 11 .   While   not   absolutely   necessary,   we   hope   the   ML   community   will   \nlobby   all   cloud   providers   to   reveal   the   actual   energy   mix,   since   it   can   vary   within   a   region.    For   example,    to   let   \ncustomers   pick   the   datacenter   based   on   CO 2 e,    Google   Cloud   recently   released   the   percentage   of   carbon-free   \nenergy   and   gross   CO 2 e   of   its   datacenters    and   committed   to   publishing   updated   figures   going   forward.   \nWe   next   show   the   impact   of   these   three   choices   on   much   larger   NLP   models.   \nTable   3.   Average   system   power   per   processor   and   standard   deviation   for   DNNs   in   this   paper.   We   \nmeasured   the   Google   DNNs   (see   Tables   1   and   4).    OpenAI   measured   GPT-3   in   a   Microsoft   Azure   \ndatacenter   [Sut21].     \n3.Energy   Usage   and   CO 2 e   Emissions    of   Five   Recent   Large   NLP   Models   \nA   natural   question   that   follows   is   what   about   the   training   CO 2 e   of   much   larger   NLP   models?   Table   4   and   \nAppendix   A   show   a   CO 2 e   calculation 11    for   five   of   them:   T5,   Meena,   GShard,   and   Switch   Transformer   from   \nGoogle   plus   GPT-3   from   Open   AI   that   runs   on   Microsoft   Azure   Cloud:  \n● T5    is   a   pre-trained   language   model   that   casts   all   NLP   problems   in   a   unified   text-to-text   format   to   enable   \napplication   of   transfer   learning   techniques   to   reduce   the   cost   of   training   [Raf19].   The   largest   size   has   \n11B   parameters,   and   training   used   86   MWh   and   produced   47   tCO 2 e.     \n● Meena    is   a   multi-turn   open-domain   chatbot   [Adi20].   This   2.6B   parameter   DNN   is   trained   to   minimize   \nperplexity   of   the   next   token.   The   year-old   companion   paper   has   ~150   citations.   Training   Meena   used   \n9  PUE   and   kg   CO 2 e   per   KWh   are   functions   of   the   datacenter   where   the   model   is   run.   \n10  The   ML   Emissions   Calculator   [Lac19]   also   estimates   power   per   processor.   It   now   uses   the   values   in   Table   3   for   TPU   v2   \nand   TPU   v3   [Luc21].   At   the   time   of   this   writing,   the   calculator   shows   CO 2 e   produced   but   not   the   estimated   power   per   \nprocessor,   energy   consumed,   or   CO 2 e/KWh.   \n11   The   Google   models   happen   to   be   run   in   datacenters   where   the   gross   and   net   CO 2 e   were   the   same   or   close.   \n5   \nProcessor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power   \nTPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nTPU   v3   283   10%   T5,   Meena,   Gshard,   Switch   Transformer   \nP100   GPU   271   11%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nV100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]\n\n232   MWh   and   emissions   was   96   tCO 2 e.   As   Evolved   Transformer   saved   48   tCO 2 e   alone   for   the   single   \nuse   case   of   developing   Meena   (see   Table   4),   the   3.2   net   tCO 2 e   cost   for   its   development   returned   15:1.   \n● GShard    is   composed   of   a   set   of   lightweight   annotation   APIs   that   provide   an   elegant   way   to   express   a   \nwide   range   of   parallel   computation   patterns   with   minimal   changes   to   the   existing   model   code   [Lep20].   It   \nenabled   scaling   up   of   a   multilingual   neural   machine   translation   Transformer   model   with   sparsely   gated   \nmixture-of-experts   (MoE)   [Sha17]   using   automatic   sharding.   The   GShard-600B   model   is   a   particular   \nuse   of   that   framework   for   training   a   multi-lingual   translation   model   with   600B   total   parameters.   Sparse   \nmodels   can   have   many   model   parameters   while   requiring   much   less   computation   than   dense   models.   \nTraining   GShard-600B   used   24   MWh   and   produced   4.3   net   tCO 2 e.   \n● Switch   Transformer    simplifies   the   Mixture   of   Expert   (MoE)   routing   algorithm   to   design   intuitive   improved   \nmodels   with   reduced   communication   and   computational   costs   [Fed21].   The   authors   show   large   sparse   \nmodels—1500B   parameters   but   only   0.1%   activated   per   token—can   deliver   up   to   7x   increases   in   \npre-training   speed   with   the   same   computational   resources.   We   estimated   it   used   179   MWh   and   \nproduced   59   net   tCO 2 e.   \nTable   4.   CO 2 e   for   NLP   models   (see   Appendix   A) 12 .   V100’s   TDP   is   closer   to   average   power   due   to    Turbo   \nmode    and    DVFS .   TPUs   don’t   offer   them,   so   their   TDP   is   much   higher   than   their   average   power.     \n12  The   peak   TeraFLOPS/second   is   46   for   TPU   v2,   123   for   TPU   v3,   and   125   for   V100.   \n6   \nModel   \nEvolved   \nTrans-   \nformer   \nNAS   \nT5   Meena   Gshard   \n-600B   \nSwitch   \nTrans-   \nformer   \nGPT-3   \nNumber   of   Parameters   (B)   0.064   per  \nmodel  11  2.6  619  1500  175  \nPercent   of   model   activated   on   every   token   100%  100%  100%  0.25%  0.10%  100%  \nDeveloper   Google   OpenAI   \nDatacenter   of   original   experiment   Google   \nGeorgia   \nGoogle   \nTaiwan   \nGoogle   \nGeorgia   \nGoogle   \nNorth   \nCarolina   \nGoogle   \nGeorgia   Microsoft   \nWhen   model   ran   Dec   2018  Sep   2019  Dec   2019  Apr   2020  Oct   2020  2020  \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.201  0.403  0.429  \nDatacenter   Net   CO2e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.177  0.330  0.429  \nDatacenter   PUE   (when   it   was   run)   1.10  1.12  1.09  1.09  1.10  1.10  \nProcessor   TPU   v2  TPU   v3   V100  \nChip   Thermal   Design   Power   (TDP   in   Watts)   280  450   300  \nMeasured   System   Average   Power   per   Accelerator,   \nincluding   memory,   network   interface,   fans,   host   CPU   (W)  208  310  289  288  245  330  \nMeasured   Performance   (TFLOPS/s) 12   24.8  45.6  42.3  48.0  34.4  24.6  \nNumber   of   Chips   200  512  1024  1024  1024  10,000  \nTraining   time   (days)   6.8  20  30  3.1  27  14.8  \nTotal   Computation   (floating   point   operations)   2.91E+21  4.05E+22  1.12E+23  1.33E+22  8.22E+22  3.14E+23  \nEnergy   Consumption   (MWh)   7.5  85.7  232  24.1  179  1,287  \n%   of   Google   2019   total   energy   consumption   (12.2   TWh  \n=   12,200,000   MWh)   [Goo20]   0.00006%  0.00070%  0.00190%  0.00020%  0.00147%  0.01055%  \nGross   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.8  72.2  552.1  \nNet   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.3  59.1  552.1  \nFraction   of   NAS   Estimate   in   [Str19]   (284   tCO2e)   0.011  0.164  0.340  0.015  0.208  1.944  \nFraction   of   equivalent   jet   plane   CO 2 e   round   trip   San   \nFrancisco   ↔   New   York   (~180   t;   see   Ap.   A)   0.018  0.258  0.533  0.024  0.327  3.054  \ntCO 2 e   savings   by   Meena   using   Evolved   Transformer   --  --  48.5  --  --  --  \n%   24/x7   carbon   free   energy   (when   run)   31%  19%  30%  73%  43%  N/A\n\n● GPT-3    is   an   autoregressive   language   model   with   175B   parameters,   10x   more   than   any   non-sparse   \nlanguage   model   at   the   time   [Bro20].   It   achieves   strong   performance   on   many   NLP   datasets.   A   winner   of   \nthe   best   paper   award   at   NeurIPS   2020,   this   8-month-old   paper   already   has   ~700   citations   and    made   \nmainstream   media   headlines . 13    It   is   now   available   for   commercial   use.   One   potential   energy   benefit   of   a   \nlarge   language   model   like   GPT-3   is   that   they   exhibit    few-shot   generalization ,   which   means   that   they   \ndon’t   need   to   be   retrained   for   every   new   task   like   smaller   models   [Wan20].    Its   estimated   carbon   \nemissions   due   to   training   are   552   tCO 2 e   and   its   energy   consumption   is   1287   MWh. 14   \nTable   4   also   lists   the   neural   architecture   search   for   Evolved   Transformer,   discussed   shortly.\n\nFigure   2.   Total   FLOPS   versus   number   of   parameters   relative   to   Transformer   (Big)   in   a   log-log   graph   \n(Table   1).   While   all   are   not   doing   the   same   tasks,   a   reason   T5   has   relatively   lower   FLOPS   relative   to   its   \nnumber   of   parameters   is   that   it   trains   until   the   accuracy   is   good   enough   instead   of   to   the   best   possible   \naccuracy.   [Kap20]   notes   that   some   architectures   have   a   much   lower   footprint   than   others   at   equivalent   \naccuracy   and   suggests   that   significant   power   might   be   saved   by   revisiting   accuracy   requirements.\n\nFigure   3.   Accelerator   years   of   computation,   energy   consumption,   and   CO 2 e   for   five   large   NLP   DNNs.   \n13  Metz,   C.,   Meet   GPT-3.   It   Has   Learned   to   Code   (and   Blog   and   Argue),   November   24,   2020,    New   York   Times .   \n14  We   measured   all   the   data   for   Google   models.   OpenAI   measured   V100   performance,   V100   power,   total   FLOPS,   and   \nPUE   for   GPT-3.   We   used   the   US   average   CO 2 e/KWh   for   GPT-3   at   Microsoft   Azure   (see   Appendix   A).   \n7\n\n[Image page=7 idx=1 name=X66.png] Size: 1200x958, Data: 52353 bytes\n\n[Image page=7 idx=2 name=X67.png] Size: 1260x850, Data: 48697 bytes\n\nFigures   2   and   3   present   the   same   data   graphically.   Figure   2   plots   the   number   of   parameters   on   the   X   axis   \nand   number   of   total   FLOPS   on   the   Y   axis   relative   to   Transformer   (Big)   [So19]   using   a   log-log   graph.   Sparsely  \nactivated   models   use   many   more   parameters   with   much   lower   total   FLOPS.   Since   performance   is   not   \nnecessarily   linear   in   FLOPS   (see   [Li21]),   Figure   3   shows   computation   in   processor   years   along   with   their   \nenergy   consumption   and   carbon   footprint.   Compared   to   the   dense   GPT-3,   sparsely   activated   Gshard   needs  \n~45X   fewer   processor   years,   uses   ~55X   less   energy,   and   reduces   gross   CO 2 e   ~115X   and   net   CO 2 e   ~130X.     \n4.   Discussion   \nIn   this   section,   we   address   the   additional   factors   relating   to   carbon   emissions   due   to   training   NLP   models.   We   \nstart   by   revisiting   the   estimate   of   neural   architecture   search   in   [Str19]   and   end   with   example   benefits   of   some   \nNLP   models.   \n4.1   Estimating   the   cost   of   neural   architecture   search   (NAS)   \nThe   Evolved   Transformer   neural   architecture   search   (NAS)   was   used   as   an   example   of   an   expensive   NLP   \nmodel   [Str19].   Although   it   is   now   surpassed   by   other   models   in   terms   of   training   cost   (Table   4),   we   discuss   it   \nhere   as   a   concrete   example   of   the   complexity   of   estimating   the   cost   of   a   ML   method   retroactively.   \nAs   Table   4   shows,   the   actual   cost   of   Evolved   Transformer   NAS   is   nearly   two   orders   of   magnitude   smaller   \nthan   previously   estimated   [Str19].   Why   the   discrepancy?   The   answer   is   that,   in   addition   to   the   efficiency   of   \nGoogle   datacenters,   there   was   a   confusion   in   estimating   the   energy   cost   of   NAS.   In   Evolved   Transformer   NAS,   \nresearchers   used   a   small    proxy   task    to   search   for   the   best   models   to   save   time   and   money,   and   then   scaled   up   \nthe   found   models   to   full   size.   Small   proxies   may   not   be   obvious,   which   made   it   hard   to   estimate   the   CO 2 e   \ncorrectly   in   retrospect   from   the   NAS   paper   [So19].   Due   to   the   misunderstanding   of   the   usage   of   proxy   tasks   in   \nNAS,    it   was    assumed   the   search   was   done   with   full   size   tasks .   Because   of   this   assumption,   despite   \nconsiderable   effort   on   their   part,   Strubell    et   al. ’s   energy   estimate   for   NAS   ended   up   18.7X   too   high   for   the   \naverage   organization   (see   Appendix   C)   and   88X   off   in   emissions   for   energy-efficient   organizations   like   Google   \n(see   Appendix   D).   This   example   led   us   to   our   first   recommendation—that   more   researchers   measure   energy   \nusage   and   CO 2 e   for   computationally   intensive   projects,   and   report   them   when   practical,   rather   than   counting   \non   others   to   estimate   it   retrospectively.   \n  Another   confusion   in   the   general   public   is   the   misperception   that   NAS   (and   therefore,   the   cost   associated   \nwith   NAS)   is   conducted   once   per   model   training.   In   practice,   however,    NAS   is   generally   not   performed   once   \nper   model   training,   but   once   per    problem     domain+architectural   search   space   combination .   For   example,   the   \nEvolved   Transformer,   found   by   NAS   on   translation,   can   be   used   for   language   modeling   without   a   new   search   \n[So19,   Adi20].   Unfortunately,    results   in   the   earlier   work   by   [Str19]   characterizing   NAS   were   misattributed   to   \nsingle   model   training   costs   in   the   popular   press.     \nAs   an   analogy,   NAS   is   like   optimizing   the   energy   efficiency   and   cost   of   an   LED   light   bulb   with   extensive   \nsimulations   on   a   supercomputer,   training   a   model   is   akin   to   building   LED   light   bulbs,   and   inference   is   \nanalogous   to   all   the   customers   using   LEDs   to   light   their   homes.   The   analogous   confusion   would   be   claiming   \nthat   the   one-time   upfront   supercomputer   simulation   cost   should   be   included   in   the   CO 2 e   cost   of   every   light   bulb   \nmanufactured.   In   this   analogy,   the   onetime   CO 2    expenditure   of   the   supercomputer   simulations   can   be   more   \nthan   paid   back   with   the   improved   energy-efficiency   of   the   mass-produced   light   bulbs,   as   was   the   case   for   the   \nactual   NAS   of   [So19]   (see   next   paragraph).     \nIn   terms   of   cost-benefit   tradeoff ,   NAS   can   also   lead   to   improved   energy   efficiency   in   training   of   downstream   \napplications,   and   the   benefit   can   dramatically   outweigh   the   cost.    Figure   4   shows   that   the   Evolved   Transformer,   \nfound   by   NAS   [So19],   has   37%   fewer   parameters   and   converges   to   the   same   accuracy   with   25%   less   energy   \nexpenditure   (see   Table   1)   than   the   vanilla   Transformer   (Big)   model   on   WMT   English   to   German   translation.   \nThe   use   of   Evolved   Transformer   instead   of   a   regular   Transformer   architecture   saved   48.5   t CO 2 e   during   the   \ntraining   of   the   Meena   DNN   (see   Tables   1   and   4).   The   savings   from   this   single   reuse   in   Meena   are   ~15X   larger   \nthan   the   energy   cost   of   running   the   search   to   discover   it.   The   results   of   the   Evolved   Transformer   neural   \n8\n\narchitecture   search   have   been   open-sourced.   It   can   readily   be   used   by   anyone   training   ML   models   for   NLP   \nproblems,   similar   to   how   a   Transformer-style   model   can   be   used   for   NLP   problems   [Evo19]. 15   \nIt   would   be   beneficial   to   compare   the   cost-savings   ratio   of   the   Evolved   Transformer   NAS   to   previous   work   \ndeveloping   more   efficient   architectures.   Unfortunately,   as   others   have   pointed   out   [Dod19,   Str19],   the   full   cost   \nof   model   development   is   rarely,   if   ever,   reported   in   the   literature,   making   it   impossible   to   compare   this   analysis   \nto   prior   work,   and   preventing   straightforward   comparison   among   different   approaches   more   generally.    \nThis   lack   of   training   development   costs   is   one   example   of   how   adopting   higher   standards   for   measuring   \nand   reporting   ML   model   energy   requirements   would   lead   to   a   better   understanding   of   cost-accuracy   tradeoffs   \nin   ML   models,   potentially   further   reducing   overall   emissions   by   empowering   more   informed   ML   model   \nselection,   as   the   next   subsection   explains.\n\nFigure   4:   Reproduction   of   Figure   4   from   So    et   al.    Dots   on   the   blue   line   represent   various   sizes   of   plain   \nTransformer   NLP   models,   while   dots   on   the   red   line   represent   various   sizes   of   the   open-sourced   \nEvolved   Transformer   architecture   that   was   discovered   by   the   neural   architecture   search   run   in   [So19] .   \nRed   arrows   are   at   131M   and   210M   parameters   and   show   that   an   Evolved   Transformer   can   achieve   \nhigher   accuracy   at   less   cost:   it   runs   1.3X   faster   and   produces   1.3x   less   CO 2 e.     \n4.2   There   are   more   resources   used   for   training   than   the   only   final   training   run   \n[Str19]   and   others   point   out   that   it   often   takes   many   attempts   to   get   everything   set   up   correctly   before   the   \nfinal   training   run,   so   the   final   training   run   does   not   reflect   the   total   cost.   Since   it’s   hard   to   improve   what   you   \ncan’t   measure,   one   issue   is   how   to   account   for   such   costs   accurately.   Fortunately,   an   internal   Google   product   \nis   underway   that   will   record   information   about   the   training   process,   originally   intended   to   keep   track   of   \ninformation   like   data   provenance.   The   developers   now   plan   to   add   energy   consumption   so   that   Googlers   can   \nbetter   understand   the   full   training   lifecycle.    An   example   of   an   open   source   tool   to   record   such   information   is   \nexperiment-impact-tracker    [Hen20].    In   addition,   the   developers   of   ML   Emissions   Calculator   [Lac19]   are   \ncurrently   working   on    CodeCarbon ,   whose   goal   is   to   measure/approximate   carbon   consumption   automatically.     \nAlas,   there   will   be   no   way   to   verify   the   claims   in   papers   of   preliminary   training   development.   A   lesson   of   \ncomputer   benchmarking   is   that   requiring   the   release   of   all   information   so   that   others   could   recreate   your   results   \nwas   an   effective   deterrent   to   fudging   the   numbers.   If   more   computationally   intensive   ML   papers   included   \nenergy   consumption   and   carbon   footprint   of   the   final   training   run   with   sufficient   details   that   others   could   check,   \n15  Reuse   reduces   overall   development   effort   and   energy   usage.   For   example,   implementations   of   EfficientNets,   Efficient-   \nDets   [Tan19],   developed   via   NAS   for   image-classification   and   object-detection,   were   forked   on   GitHub   >4000   times.   \n9\n\n[Image page=9 idx=1 name=X75.png] Size: 661x556, Data: 80905 bytes\n\nthat   would   be   a   great   step   forward.   Perhaps   ML   practitioners   could   study   the   total   lifecycle   to   develop   rules   of   \nthumb   to   estimate   the   overall   carbon   footprint   based   on   its   final   training   cost. 16     \nThe   next   subsection   also   emphasizes   the   value   of   measurement.\n\nFigure   5.   Measured   vs   peak   performance,   measured   system   power   vs   peak   chip   power   (TDP),   and   \nmeasured   vs   peak   performance/Watt   for   V100   GPU   and   TPU   v3   (see   Table   4   and   Appendix   A).   \n4.3   Measurements   are   more   interesting   than   extrapolations   \nAlthough   extrapolations   of   carbon   emissions   are   relatively   easy,   more   attention   should   be   paid   to   actual   \nexperiments   that   have   been   conducted   rather   than   to   hypothetical   case   studies.   As   a   problematic   example,   \n16  Since   large   NLP   models   can   take   a   month   to   train,   developers   cannot   afford   to   do   the   full   training   task   many   times.   Like   \n[So19]   for   NAS,   they   likely   use   a   smaller   task   to   explore   the   space   for   a   limited   training   time.   One   indication   comes   from   \nthe   AutoML   work   in   [Li21].   Their   exploration   computation   cost   was   roughly   equal   to   the   final   training   cost.   \n10\n\n[Image page=10 idx=1 name=X82.png] Size: 1574x838, Data: 60978 bytes\n\n[Image page=10 idx=2 name=X83.png] Size: 1574x778, Data: 78322 bytes\n\n[Image page=10 idx=3 name=X84.png] Size: 1574x976, Data: 87759 bytes\n\nlet’s   hypothesize   what   the   CO 2 e   would   be   for   training   Transformer   (Big)   on   the    CTS-1   Quartz   -   Tundra   Extreme   \nScale   supercomputer    at    Lawrence   Livermore   National   Laboratory,     one   of   the    top   500   supercomputers    (but   one   \nwhose   design   is   not   optimized   for   ML   training).   Its   ~100,000   cores   might   use   ~75   MWh   of   power   and    might   \ngenerate   32   tCO 2 e,   ~10,000   times   larger   than   for   TPU   v2s   at   Google   (Table   1) 17 .     \nThe   measurement   advice   applies   to   processors   as   well   DNNs.   Tables   1   and   2   show   that   the   theoretical   \nperformance   per   Watt   is   higher   than   the   measured   performance   per   Watt   on   average   by   factors   of   1.6X   for   \nTPUs   and   by   3.5X   for   GPUs.   Figure   5   shows   the   information   in   Table   1   graphically.   Using   theoretical   \nperformance   per   Watt,   V100   is   1.5X   better   than   TPU   v3,   but   it's   the   other   way   around   for   measured   \nperformance   per   Watt:   TPU   v3   is   2.0X   better   than   V100   on   average   for   these   large   NLP   DNNs.   \nFigure   6   compares   the   gross   CO 2 e   estimates   from   the   ML   Emissions   [Lac19]   and   Green   Algorithms   \n[Lan20]   calculators   to   the   processors   and   programs   in   this   paper   at   the   time   of   this   writing   (April   2021).   \nCompared   to   the   results   in   Tables   1   and   4,   they   differ   by   factors   of   0.53–1.64   and   0.91–2.42   with   geometric   \nmeans   of   0.92   and   1.48,   respectively 18 .    The   ML   Emissions   and   Green   Algorithms   calculators   do   not   \nestimate   net   CO 2 e,   which   could   be   up   to   10X   lower.    The   figure   once   again   shows   the   increase   in   accuracy   \nof   measurement   over   indirect   calculations.   The   authors   of   the   Emissions   Calculator   agree   that   measurement   is   \npreferred,   with   some   calculator   as   the   best   alternative   if   measurement   is   difficult   to   perform   [Luc21].   \nThe   next   discussion   topic   reminds   us   that   improving   the   algorithm   is   often   more   important   than   improving   \nthe   hardware.\n\nFigure   6.   Ratio   of   ML   Emissions   and   Green   Algorithm   calculators   vs   gross   CO 2 e   in   Tables   1   and   4.   \n4.4   Standard   ML   algorithmic   techniques   can   improve   energy   efficiency   \nThere   are   many   algorithmic   techniques   that   can   improve   the   energy   efficiency   of   machine   learning   models.   \nSome   techniques   can   achieve   the   same   accuracy   with   less   overall   computation.   Others   can   use   a   large,   \nalready-trained   model   as   a   starting   point   and   yield   a   lighter-weight,   more   computationally   efficient   model   with   \nalmost   the   same   accuracy.   These   techniques   all   serve   to   reduce   the   computational   cost   and   therefore   energy   \nand   carbon   emissions   of   models.   Some   of   these   techniques   include:   \n● Distillation    transfers   the   knowledge   from   large   models   into   smaller,   more   computationally   efficient   \nmodels   [Hin15,   San20].   \n● Pruning ,    quantization ,   and    efficient   coding    can   improve   the   energy   efficiency   of   DNNs   3X–7X   [Han15].   \n17  We   use   US   averages   for   kg   CO 2 e/KWh   and   datacenter   PUE   and   assume   it   runs   at   40%   of   the   peak   floating   point   \nperformance   of   Quartz-Tundra   (3.2   PetaFLOPS/sec).   For   reference,   Figure   5   shows   V100   running   at   20%   of   peak.   \n18  We   picked   the   closest   geographic   option   per   calculator   to   the   actual   location   in   each   case.   The   Green   Algorithms   paper   \nlists   Meena   CO 2 e   as   164t   [Lan20],   but   the   calculator   result   as   of   April   2020   was   85t   for   Virgina   using   Google   Cloud.   \n11\n\n[Image page=11 idx=1 name=X87.png] Size: 1324x638, Data: 86334 bytes\n\n● Fine-tuning    and    transfer   learning    both   reuse   already-trained   representations,   rather   than   starting   \ntraining   of   each   NLP   task’s   parameters   from   random   initialization,   for   example   [Dod20].     \n● Sparsely   activated   mixture-of-expert-style   models    can   provide   more   than   10X   reductions   in   \ncomputation   requirements   and   energy   costs   for   both   training   and   inference   while   providing   significantly   \nhigher   accuracy   than   dense   Transformer   or   LSTM-based   models   of   equivalent   computational   cost   per   \ntoken   [Sha17,Lep20,Fed21].   Gshard-600B   is   one   example,   evaluated   in   Section   3.   \nWe   commend   the   development   of   such   techniques.   Some   publication   venues,   such   as   the    EACL    and    NAACL   \n2021   NLP   conferences,   have   begun   specifically   soliciting   research   of   this   nature   by   offering   “Efficient   and   \nGreen”   research   tracks,   alongside   workshops   such   as    SustaiNLP    and    EfficientQA .   We   encourage   other   \nvenues   to   follow   suit,   and   hope   that   many   researchers   will   consider   this   line   of   work.   \nThe   next   topic   discusses   one   of   our   biggest   surprises   of   this   investigation,   the   importance   of   geography.     \n4.5   It   matters   which   datacenter   is   used,   even   within   the   same   organization   \nWe   were   amazed   by   how   much   it   matters    where    and    when    a   DNN   is   trained.   Moreover,   this   option   is   likely   \nthe   easiest   path   for   ML   practitioners   to   reduce   CO 2 e.   For   example,   after   reading   early   drafts   of   this   paper,   \nsome   colleagues   switched   to   a   Google   datacenter   with   a   smaller   carbon   footprint    to   train   a   large   NLP   model.   \nReviewers   of   early   drafts   suggested   that   datacenter   energy   use   is   a   zero-sum   game.   They   thought   that   any   \ntasks   run   in   a   green   datacenter   simply   shift   other   work   to   dirtier   datacenters,   so   there   is   no   net   gain.   It’s   not   \ntrue,   but   that   speculation   reveals   many   seemingly   plausible   but   incorrect   fallacies:   \n● Fallacy:   Datacenters   are   fully   utilized .   Applications   are   deployed   to   handle   worst   case   demand   \ndepending   on   the   time   of   day   and   day   of   the   week,   so   for   much   of   the   time   resources   are   idle   [Arm10].   \n● Fallacy:   Cloud   centers   can’t   grow .   Similar   to   the   founding   of   a   new   university,   cloud   companies   buy   \nmuch   more   land   than   they   need   initially   at   a   site   so   that   they   can   construct   more   buildings   in   the   future   \nwithout   first   traversing   the   lengthy   process   of   acquiring   land   [Bar18].   \n● Fallacy:   Renewable   energy   is   fixed   and   can’t   grow .   There   is   often   an   excess   of   renewable   energy   at   \nsome   times   of   day   (see   Appendix   B).   The   amount   of   solar   and   wind   energy   is   also   a   function   of   the   \ninvestment   as   well   as   weather   conditions.   Google’s   long   term   renewable   energy   procurement   normally   \ninvests   in   the   creation   of   new   renewable   energy   resources.   The   greater   the   use   and   investment   in   \nrenewable   energy,   the   more   money   is   available   to   buy   and   deploy   new   solar   panels   and   wind   turbines,   \nthereby   increasing   the   renewable   energy   supply.   Thus,   it’s    not    the   case   that   Google’s   use   of   renewable   \nenergy   means   other   residents   must   use   dirty   energy.   Appendix   B   introduces   issues   around   carbon   free   \nenergy   use   and   investment.   \n● Fallacy:   Google   NLP   model   training   competes   with   other   tasks   in   the   datacenter .   Google   trains   large   \nmodels   on   ML   supercomputers   that   even   have   their   own   interconnection   network,   so   ML   training   is   \ndistinct   from   CPU-only   tasks   [Jou20].   Tasks   for   CPUs   don’t   interfere   with   TPUs,   and   vice   versa.   \n● Fallacy:   Training   must   run   in   all   datacenters .   While   user   facing   inference   applications   need   global   \ndistribution   in   order   to   provide   low-latency   access   to   users   all   around   the   world   [Jou21],   there   is   no   \nproblem   to   limit   ML   training   computation   to   a   smaller   number   of   (green)   datacenters.   For   example,   \nGoogle   is   currently   deploying   numerous   TPU   v4s,   many   of   which   will   be   located   in   windy   Oklahoma,   \nwhose   net   CO 2 e/KWh   is   even   lower   than   Iowa.   \n● Fallacy:   There   is   no   business   reason   to   reduce   carbon   emissions .   Reducing   climate   change   certainly   \nhas   long-term   economic   benefits   for   everyone.   Google   has   been   carbon   neutral   since   2007   and   has   \nprocured   enough   additional   renewable   energy   to   match   100%   of   its   datacenter   energy   usage   since   \n2017,   so   the   impact   of   the   remaining   carbon   from   training   at   Google   is   zero   even   today.   Other   \nhyperscalers   aim   for   carbon   neutrality   by   2025   or   2030,   so   the   whole   cloud   may   become   carbon   \nneutral.   With   its   new   24/7   local   carbon-free   energy   goal   by   2030,   Google   is   now   focused   on   purchasing   \ncarbon-free   energy   to   match   its   hourly   load   at   the   same   location   as   its   datacenters   with   the   goal   to   \ndecarbonize   its   electricity   supply   (see   Appendix   B).     \nThe   next   question   that   arose   is   whether   such   green   datacenters   are   available   to   only   a   few   ML   practitioners.   \n12\n\n4.6   Many   have   access   to   energy-optimized   datacenters   \nThe   increasing   use   of   cloud   computing   has   decreased   the   energy   intensity 19    of   datacenters   20%   annually   \nsince   2010   [Has20].   Access   to   energy-optimized,   low-cost   cloud   datacenters   is   not   restricted   to   employees   of   a   \nfew   companies;   people   around   the   world   can   rent   computers   in   them   using   services   like   Alibaba   Cloud,   \nAmazon   Web   Services,   Google   Cloud   Platform,   and   Microsoft   Azure. 20    Moreover,   Alibaba,   Amazon,   and   \nGoogle   offer   access   to   their   custom   processors   for   DNNs   through   their   cloud   service.   The   popularity   of   the   \npublic   cloud   is   indicated   by   its   annual   growth   in   business   by   up   to   50%   since   2010   [Sch21].   Many   believe   the   \ncloud’s   efficiencies   in   cost   and   energy   mean   that   it   is   the   ultimate   future   of   all   datacenters   [Arm10,   Sch21].     \nThe   next   topic   reminds   us   that   reducing   cost   and   energy   consumption   remains   important   no   matter   how   \ngreen   the   cloud   becomes.   \n4.7   Reducing   the   cost   of   training   matters   too     \nThough   many   have   access   to   these   relatively   efficient   compute   resources   and    cloud   companies   may   \ndramatically   reduce   their   carbon   footprint   in   the   future,   it’s   still   important   to   reduce   the   economic    cost    of   \ntraining.   Saving   money   obviously   matters   to   everyone,   but   e xpensive   training   of   NLP   models   also   makes   this   \nresearch   style   unattainable   for   many   researchers 21 , 22 .   This   inequity   of   access   to   state-of-the-art   models   is   \nanother   strong   motivator,   alongside   environmental   concerns,   to   incentivize   the   development   of   energy-efficient   \nML   models   that   work   as   well   as   their   computationally   hungrier   counterparts.   \nOne   issue   that   was   difficult   for   us   during   our   investigation   was   to   put   into   perspective   the   4   to   552   tCO 2 e   \nfrom   training   of   these   NLP   models,   which   the   next   subsection   explores.     \n4.8   How   does   training   a   large   NLP   model   compare   to   other   activities?     \nGoogle   Flights   estimate    for   the   emissions   of   a   direct   round   trip   of   a   whole   passenger   jet   between   San   \nFrancisco   and   New   York   is   180   tCO 2 e   (see   Table   2   and   Appendix   A).   T5   training   emissions   are   ~26%,   Meena   \nis   53%,   Gshard-600B   is   ~2%,   Switch   Transformer   is   32%,   and   GPT-3   is   ~305%   of   such   a   round   trip.   \nAnother   comparison   point   is   to    Bitcoin .   Every   purchase   that   transfers   bitcoin   currently   costs   ~700   KWh   or   \n~0.3   tCO 2 e,   equivalent   to   the   CO 2 e   produced   by   ~750,000   credit   card   swipes.   Bitcoin   miners   use   custom   chips   \nthat   operate   continuously   24/7   until   they   fail.   Estimates   of   Bitcoin’s   impact   for   2021   are   ~78–121   \nTeraWatt-hours   and   ~37M–58M   tCO 2 e   [Cri21,   Dig21].   Stated   alternatively,   ~70M   people   have   Bitcoin   wallets   \nyet   Google   consumes   1/10th   of   Bitcoin’s   energy   to   provide   services   for   billions   of   people,   and   all   of   Google’s   \nenergy   use   is   offset.   If   Bitcoin   were   a   country,   it   would   be   in   the   top   30   in   CO 2 e;   larger   than   Argentina,   whose   \npopulation   is   45M.   The   estimated   annual   carbon   footprint   of   Bitcoin   mining   this   year   is   equivalent   to   roughly   \n200,000   to   300,000   whole   passenger   jet   SF↔NY   round   trips.   \n  In   2019    the   world   saw   39M    flights   and    US   airlines   flew   925M   passengers ,   which   helps   explain   why   air   \ntravel   was   responsible   for   940   MtCO 2 ,   or   ~2.5%   of   the   world's   annual   CO 2    in   2018   of   33B   tCO 2 e   [Rit20].   \nFinally,   Google   publishes   its   total   energy   consumption,   and   for   2019   it   was   12.2   TeraWatt-hours   [Goo20].   \nRow   18   of   Table   4   shows   the   percentage   that   each   NLP   model   training   was   of   that   total.   Even   if   we   assume   all   \nfour   of   Google’s   large   NLP   models   in   Table   4   were   trained   in   2019,   the   total   represents   less   than   0.005%.    The   \ntraining   of   those   four   large   NLP   models   is   not   a   significant   fraction   of   Google’s   energy   consumption.     \n19  Improvement   in   energy   intensity   is   expressed   as   energy   use   per   compute   instance.   [Has20]   goes   on   to   say   the   cloud’s   \nincreasing   share   of   datacenters   is   causing   a   “notable   improvement   compared   with   recent   annual   efficiency   gains   in   other   \nmajor   demand   sectors   (e.g.,   aviation   and   industry),   which   are   an   order   of   magnitude   lower.”   \n20  There   are   not   many   cloud   companies.   With   new   technologies,   initially   only   a   few   firms   can   practice   the   technology   and   \nthey   sell   it   to   others,   but   these   companies   compete.   There   are   many   examples.   Chemical   technologies   are   in   the   hands   of   \na   relatively   small   number   of   companies;   only   six   or   seven   institutions   worldwide   can   refine   crude   oil;   just   a   few   firms   can   \nmanufacture   computer   chips   in   the   finest   technology   node   (3–5   nm).   \n21  To   support   the   goal   of   making   ML   more   inclusive,    Google   provides   free   access   to   a   total   of   ~500   PetaFLOPS/second   of   \nTPU   compute   power   to   help   ML   researchers   around   the   world   participate   in   advancing   the   start   of   the   art   of   ML .   \n22  One   possible   unintended   consequence   of   making   training   of   a   model   less   expensive   is   that   more   people   will   train   the   \nmodel   and   increase   energy   use,   but   that   seems   like   a   better   risk   than   to   continue   using   inefficient   models.   \n13\n\nHaving   spent   13   pages   on   the   cost   of   large   NLP   models   and   neural   architecture   search,   we   conclude   our   \ndiscussion   with   three   examples   of   the   potential   benefits   of   NLP   models.     \n4.9    Are   the   benefits   of   NLP   models   worth   the   energy   cost?   \nA   recent   example   of   a   societal   benefit   of   NLP   is   the    COVID-19   Research   Explorer ,   which   helps   scientists   \nand   researchers   efficiently   pore   through   articles   for   answers   or   evidence   to   COVID-19-related   questions.   It   is   \npowered   by    BERT ,   a   Transformer-style   model   trained   for   the   biomedical   domain   [Hal20]. 23    Its   training   \nconsumed   ~2.8   MWh   and   produced   0.13   tCO 2 e,   about   one-tenth   of   a   SF-NY   round   trip   by   one   passenger. 24   \nA   more   widespread   example   is   the   use    of   BERT   in   search .   English   is   the   most   popular   language   on   the   \nweb.   This   use   of   BERT   takes   models   that   learn   from   improvements   in   English   and   applies   them   to   other   \nlanguages.   In   particular,   BERT   significantly   improved   featured   snippets—short   text   summary   at   the   top   of   \nGoogle   research   results—in   languages   like   Hindi,   Korean,   and   Portuguese.\n\nFigure   7:   Reproduction   of   Figure   6   from   [Lep20]   with   annotations.   Translation   quality   comparison   of   \nmultilingual   Mixture   of   Expert   (MoE)   Transformer   models   trained   with   GShard   showing   the   increase   in   \nBLEU   score    versus   a   separate   baseline   Transformer   model   trained   on   each   language   pair   for   100   \nlanguages   to   English.   MoE   models   have   large   model   capacity   but   are   only   partially   activated   for   any   \ngiven   token.   The   source   languages   are   grouped   on   the   x-axis   by   the   resources   available   for   each   \nlanguage   in   billions   of   speakers,   with   languages   like   French   and   Spanish   on   the   left   (>1B   examples)   \nand   languages   like   Sindhi   and   Yoruba   on   the   right   (<1M   examples).   The   BLEU   score   improvements   \nfrom   larger   models   and   multilingual   training   are   high   for   all   languages   but   are   even   higher   for   \nlow-resource   languages—the   graph’s   right-hand   side   is   higher   than   the   left—so   Yoruba   translation   \nquality   benefits   more   than   Spanish   translation   quality.\n\nA   final   example   is   the   GShard   multilingual   translation   model   itself.    Bender   &   Gebru    et   al.    [Ben21]   raise   \nseveral   legitimate   issues   in   the   development   and   use   of   large   language   models.    Creating   such   models   \nrequires   careful   attention   to   issues   of   fairness   and   bias   [Ben21,   Gar19,   Joh20,   Kuc18,   Mer19],   but   they   also   \nhave   the   potential   to   benefit   people   everywhere.    For   example,    our   large   scale   translation   models   (M4)   have   \n23  Despite   targeting   a   narrow   audience   of   scientists,   COVID   explorer   served   1000   queries   per   day   at   launch.   It   drew   \ninterest   from   Pfizer,   Bristol   Myers   Squibb,   AstraZeneca,   Regeneron,   British   Medical   Journal,   European   Food   Safety   \nAuthority,   and   the   National   Institute   of   Health.   Pfizer’s   Director   of   Global   Medical   Epidemiology   used   the   tool   daily;   it   led   to   \nPfizer   epidemiology   research   group   to   adapt   the   underlying   ML   models   for   systematic   reviews   and   literature   search.   \n24  Training   COVID   Explorer   took   6   days   on   64   TPU   v3s   running   in   Oklahoma.   It   used   ~2.8   MWh   and   0.13   net   tCO 2 e.     \n14\n\n[Image page=14 idx=1 name=X107.png] Size: 1186x579, Data: 190312 bytes\n\nalready   been   used   to   translate   billions   of   queries   annually   for   each   mid-to-low   resource   language 25    with   2B   \nspeakers   globally   for   these   languages.   Figure   7,   from   the   GShard   paper   [Lep20],   shows   substantial   \nimprovements   for   translation   of   100   different   languages   to   English.   The   blue   line   on   the   top   in   the   left   \nrepresents   the   600B   parameter   multi-lingual   translation   MoE   model   of   GShard.   The   dashed   black   line   near   the   \nbottom   is   for   a   traditional   dense   DNN   that   is   fully   activated   for   every   token.   The   dense   DNN   requires   ~10X   \nmore   computational   resources   to   train   than   the   600B   sparse   MoE   model,   despite   substantially   lower   translation   \nquality.   Figure   7   shows   the   larger   MoE   model,   the   larger   the   BLEU   score   gains   were   across   all   languages;   the   \nlines   rarely   cross.   The   600B   MoE   model   improves   average   quality   +13.5   BLEU,   7.4   higher   than   the   2.3B   dense   \nmodel.     \nGShard-600B’s   emissions   (Table   4)   are   4.3   tCO 2 e   —3.5   passenger   SF-NY   round   trips—from   consuming   24   \nMWh   to   train   the   model   that   could   have   2B   users;   the   amortized   per-user   CO 2 e   impact   of   model   training   would   \nbe   less   than   the   CO 2 e   impact   of   sending   one   text   message 26 .     \n5.   Conclusion   \nGlobal   climate   change   is   a   threat   to   economies,   human   health,   and   the   environment,   and   the   ML   community   \nneeds   to   do   its   share   to   limit   its   carbon   emissions. 27    We’re   thankful   that   papers   like   [Lac19,   Str19,   Sch20,   \nHen20]   helped   make   the   ML   community   aware   of   this   important   issue.   Improving   the   energy   efficiency   of   \nalgorithms,   datacenters,   hardware,   and   software   has   long   been   a   business   priority   for   Google   and   other   Cloud   \ncompanies.   For   example,   Gshard-600B   operates   much   more   efficiently   than   other   large   NLP   models   and   ML   \naccelerators   are   more   efficient   than   off-the-shelf   hardware.     \nAs   mentioned   in   the   introduction,   we   make   three   suggestions   for   publications   on   compute   intensive   models   \nthat   could   eventually   help   reduce   their   CO 2 e   footprint:   report   energy   consumed   and   CO 2 e   explicitly,   ML   \nconferences   should   reward   improvements   in   efficiency   as   well   as   traditional   metrics,   and   include   the   time   and   \nnumber   of   processors   for   training   to   help   everyone   understand   its   cost.   We   believe   power   will   be   included   in   \nupcoming   MLPerf   benchmarks,   which   is   an   important   step   in   the   right   direction.   \nIf   the   ML   community   working   on   computationally   intensive   models   starts   competing   on   training   quality   and   \ncarbon   footprint   rather   than   on   accuracy   alone,   the   most   efficient   datacenters   and   hardware   might   see   the   \nhighest   ML   demand.   If   paired   with   publication   incentives   to   improve   emission   metrics   in   addition   to   accuracy,   \nwe   can   imagine   a   virtuous   cycle   that   slows   the   growth   of   the   carbon   footprint   of   ML   by   accelerating   innovations   \nin   the   efficiency   and   cost   of   algorithms,   systems,   hardware,   datacenters,   and   carbon   free   energy.     \nAcknowledgements   \nWe   wish   to   express   our   thanks   to   colleagues   at   Google   and   elsewhere   who   helped   shape   and   improve   this   \npaper.   Emma   Strubell   made   several   suggestions   of   ideas   and   organization   of   the   paper,   including   suggesting   \nadding   data   about   the   five   large   models.   We   thank   Christopher   Berner,   Ilya   Sutskever,   OpenAI,   and   Microsoft   \nfor   sharing   information   about   GPT-3.   Dmitry   Lepikhin   and   Zongwei   Zhou   did   a   great   deal   of   work   to   measure   \nthe   performance   and   power   of   GPUs   and   TPUs   in   Google   datacenters.   Hallie   Cramer,   Anna   Escuer,   Elke   \nMichlmayr,   Kelli   Wright,   and   Nick   Zakrasek   helped   with   the   sections   on   energy   and   CO 2 e   emissions   at   Google.   \nTim   Kraska   suggested   a   revised   organization   of   this   paper.   We   thank   Daniel   Adiwardana,   Gabriel   Bender,   \nAndrei   Broder,   Charina   Chou,   Jesse   Dodge,   Oren   Etzioni,   Orhan   Firat,   Ananya   Ganesh,   Robbie   Gonzalez,   \nDavid   Grangier,   Marsden   Hanna,   Urs   Hölzle,   Sheng   Li,   Sasha   Luccioni,   Preston   McAfee,   Andrew   McCallum,   \nEsteban   Real,   Stven   Ross,   Brennan   Saeta,   Roy   Schwartz,   Victor   Schmidt,   Ian   Schneider,   Aarush   Selvan,   \nNoah   A.   Smith,   Zak   Stone,   Kate   Weber,   and   Cliff   Young   for   their   help   and   feedback   on   the   manuscript.     \n25  In   our   setup   for   Figure   7,   low   resource   languages   have   less   than   1M   training   examples,   mid   resource   languages   have   \nless   than   10M   training   examples,   and   high   resource   languages   have   more   than   1B   training   examples.   \n26  An    SMS   message   is   0.014   g   of   CO 2 .   That   is   larger   than   24   MWh   /   2B,   which   yields   about   0.005   g   of   CO 2 .   \n27  We   did   not   address   the   carbon   footprint   of   ML   in   phones   and   other   edge   devices.   It   would   be   an   excellent   topic   for   \nanother   paper.   \n15\n\nReferences   \n16   \n[Adi20]   Adiwardana,   D. ,    Luong,   M.,    R.   So,   D.,   Hall,   J.,   Fiedel,   N.,   Thoppilan,   R.,   Yang,   Z.,   Kulshreshtha,   A.,   Nemade,   \nG.,   Lu,   Y.,   and   Le.   Q.    Towards   a   Human-like   Open-Domain   Chatbot .    arXiv   preprint   arXiv:2001.09977 .   \n[Arm10]  Armbrust,   M.,   Fox,   A.,   Griffith,   R.,   Joseph,   A.D.,   Katz,   R.,   Konwinski,   A.,   Lee,   G.,   Patterson,   D.,   Rabkin,   A.,   \nStoica,   I.   and   Zaharia,   M.,   2010.   A   view   of   cloud   computing.    Communications   of   the   ACM,    53(4),   pp.50-58.   \n[Bar19]   Barr,   J.   December   3,   2019.   Amazon   EC2   Update,   \naws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips   \n-for-high-performance-cost-effective-inferencing/ .   \n[Bro20]   Brown,   T.,   Mann,   B.,   Ryder,   N.,   Subbiah,   M.,   Kaplan,   J.,   Dhariwal,   P.,   Neelakantan,   A.,   Shyam   ,   P.,   Sastry,    G.,   \nAskell,   A.,   Agarwal,   S.,   Herbert-Voss,   A.,   Krueger,   G.,   Henighan,   T.,   Child,   R.,   Ramesh,   A.,   Ziegler,   D.,   Wu,   J.,   \nWinter,   C.,   Hesse,   C.,   Chen,   M.,   Sigler,   E.,   Litwin,   M.,   Gray,   S.,   Chess,   B.,   Clark,   J.,   Berner,   C.,   McCandlish,   S.,   \nRadford,   A.,   Sutskever,   I.,   Amodei,    D.   July   22,   2020.   Language   models   are   few-shot   learners.   NeurIPS   2020.   \narXiv   preprint   arXiv:2005.14165 .   \n[Ben21]  Bender,   E.,   Gebru,   T.,   McMillan-Major,   A.   Shmitchell,   S.   On   the   Dangers   of   Stochastic   Parrots:   Can   Language   \nModels   Be   Too   Big?   FAccT   2021.    http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf .   \n[Car21]   Carbon   Offset   Research   and   Education,   2021,   Carbon   Offset   Guide,    https://www.offsetguide.org/ .   \n[Cha19]  Chang,   K.W.,   Prabhakaran,   V.   and   Ordonez,   V.,   2019,   November.   Bias   and   fairness   in   natural   language   \nprocessing.   In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   Processing   and   \nthe   9th   International   Joint   Conference   on   Natural   Language   Processing   (EMNLP-IJCNLP):   Tutorial   Abstracts.   \nhttps://arxiv.org/pdf/1908.09635.pdf .   \n[Cri21]   Criddle,   C.,   February   10,   2021.   Bitcoin   consumes   more   electricity   than   Argentina,   \nwww.bbc.com/news/technology-56012952 .   \n[Dig21]   Digiconomist,   2021,   Bitcoin   Energy   Consumption   Index,    https://digiconomist.net/bitcoin-energy-consumption/    .   \n[Dod19]  Dodge,   J.,   Gururangan,   S.,   Card,   D.,   Schwartz,   R.,   and   Smith,   N.,   2019.   Show   Your   Work:   Improved   Reporting   \nof   Experimental   Results.   In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   \nProcessing   and   the   9th   International   Joint   Conference   on   Natural   Language   Processing   \n(EMNLP-IJCNLP). www.aclweb.org/anthology/D19-1224/ .   \n[Dod20]  Dodge,   J.,   Ilharco,   G.,   Schwartz,   R.,   Farhadi,   A.,   Hajishirzi,   H.   and   Smith,   N.,   2020.   Fine-tuning   pretrained   \nlanguage   models:   Weight   initializations,   data   orders,   and   early   stopping.    arXiv   preprint   arXiv:2002.06305 .   \n[Evo19]    Apache-licensed   Evolved   Transformer   open-source   implementation   in   tensorflow/tensor2tensor   GitHub   \nrepository.   \nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py   \n[Fed21]   Fedus,   W.,   Zoph,   B.,   Shazeer,   N.,   January   11,   2021,   Switch   Transformers:   Scaling   to   Trillion   Parameter   Models   \nwith   Simple   and   Efficient   Sparsity    https://arxiv.org/abs/2101.03961 .   \n[Gar19]   Garg,   S.,   Perot,   V.,   Limtiaco,   N.,   Taly,   A.,   Chi,   E.H.   and   Beutel,   A.,   2019,   January.   Counterfactual   fairness   in   text   \nclassification   through   robustness.   In   Proceedings   of   the   2019   AAAI/ACM   Conference   on   AI,   Ethics,   and   Society   \n(pp.   219-226).    https://research.google/pubs/pub47670/    .   \n[Goo16]  Google,   December   2016,   Achieving   Our   100%   Renewable   Energy   Purchasing   Goal   and   Going   Beyond,   \nhttps://static.   \ngoogleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal \n.pdf .   \n[Goo20]  Google,   Environmental   Report   2020,   \nhttps://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf .   \n[Goo21]  Google,   February   2021,   24/7   Carbon-Free   Energy:   Methodologies   and   Metrics,   \nhttps://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf .   \n[Gup20]  Gupta,   U.,   Kim,   Y.G.,   Lee,   S.,   Tse,   J.,   Lee,   H.H.S.,   Wei,   G.Y.,   Brooks,   D.   and   Wu,   C.J.,   2020.   Chasing   Carbon:   \nThe   Elusive   Environmental   Footprint   of   Computing.    arXiv   preprint   arXiv:2011.02839 .   \n[Hal20]   Hall,   K.,   May   4,   2020,   An   NLU-Powered   Tool   to   Explore   COVID-19,   \nhttps://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html .   \n[Han15]  Han,   S.,   Pool,   J.,   Tran,   J.   and   Dally,   W.J.,   2015.   Learning   both   weights   and   connections   for   efficient   neural   \nnetworks.   ICLR   2016.     arXiv   preprint   arXiv:1510.00149 .   \n[Hen20]  Henderson,   P.,   Hu,   J.,   Romoff,   J.,   Brunskill,   E.,   Jurafsky,   D.   and   Pineau,   J.,   2020.   Towards   the   systematic   \nreporting   of   the   energy   and   carbon   footprints   of   machine   learning.   Journal   of   Machine   Learning   Research.   \nhttps://jmlr.org/papers/v21/20-312.html   \n[Her20]   Hernandez,   D.   and   Brown,   T.B.,   2020.   Measuring   the   algorithmic   efficiency   of   neural   networks.   arXiv   preprint   \narXiv:2005.04305.    https://arxiv.org/abs/2005.04305 .   \n[Hin15]   Hinton,   G.,   Vinyals,   O.   and   Dean,   J.,   2015.   Distilling   the   knowledge   in   a   neural   network.    arXiv   preprint   \narXiv:1503.02531 .   \n[Höl20]   Hölzle,   U.,   Feb   27,   2020.   datacenters   are   more   energy   efficient   than   ever.   \nblog.google/outreach-initiatives/sustainability/data-centers-energy-efficient     \n[Joh20]   Johnson,   M.,    April   22,   2020,   A   Scalable   Approach   to   Reducing   Gender   Bias   in   Google   Translate,   \nhttps://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html    .\n\n17   \n[Jou21]   Jouppi,   N.,   Yoon,   D-H,   Jablin,   T.,   Kurian,   G.,   Laudon,   J.,   Li,   S.,   Ma,   P.,   Ma,   X.,   Patil,   N.,Prasad,   S.,   Young,   C.,   \nZhou,   Z.,   and   Patterson,   D.,   May   2021.   Ten   Lessons   From   Three   Generations   Shaped   Google’s   TPUv4i,   to  \nappear,    the   48th   International   Symposium   on   Computer   Architecture.   \n[Kap20]  Kaplan,   J.,   McCandlish,   S.,   Henighan,   T.,   Brown,   T.B.,   Chess,   B.,   Child,   R.,   Gray,   S.,   Radford,   A.,   Wu,   J.   and   \nAmodei,   D.,   2020.   Scaling   laws   for   neural   language   models.   arXiv   preprint   arXiv:2001.08361.   \n[Kär18]   Kärcher   B.   Formation   and   radiative   forcing   of   contrail   cirrus.    Nature   communication s.   2018   May   8;9(1):1-7.   \nhttps://www.nature.com/articles/s41467-018-04068-0 .   \n[Kuc18]   Kuczmarski,   J.   and   Johnson,   M.,   2018.   Gender-aware   natural   language   \ntranslation. www.tdcommons.org/dpubs_series/1577/ .   \n[Lac19]   Lacoste,   A.,   Luccioni,   A.,   Schmidt,   V.   and   Dandres,   T.,   2019.   Quantifying   the   carbon   emissions   of   machine   \nlearning.    arXiv   preprint   arXiv:1910.09700 .   \n[Lan20]   Lannelongue,   L.,   Grealey,   J.   and   Inouye,   M.,   2020.   Green   algorithms:   Quantifying   the   carbon   footprint   of   \ncomputation.    arXiv:   2007.07610 .   \n[Leo19]   Leopold,   G.   March   19,   2019,   AWS   to   Offer   Nvidia’s   T4   GPUs   for   AI   Inferencing,   \nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/    .   \n[Lep20]   Lepikhin,   D.,   Lee,   H.,   Xu,   Y.,   Chen,   D.,   Firat,   O.,   Huang,   Y.,   Krikun,   M.,   Shazeer,   N.   and   Chen,   Z.,   2020.   GShard:  \nScaling   giant   models   with   conditional   computation   and   automatic   sharding.    arXiv   preprint   arXiv:2006.16668 .   \n[Li21]   Li,   S.,   Tan,   M.,   Pang,   R.,   Li,   A.,   Cheng,   L.,   Le,   Q.   and   Jouppi,   N.P.,   2021.   Searching   for   Fast   Model   Families   on   \nDatacenter   Accelerators.    arXiv   preprint   arXiv:2102.05610 .   \n[Liu18]   Liu,   H.,   Simonyan,   K.   and   Yang,   Y.,   2018.   Darts:   Differentiable   architecture   search.    arXiv   preprint   \narXiv:1806.09055 .   \n[Luc21]   Luccioni,   A.,   and   Schmidt,   V..   March   2021,   Private   Communication.   \n[Mas20]  Masanet,   E.,   Shehabi,   A.,   Lei,   N.,   Smith,   S.   and   Koomey,   J.,   2020.   Recalibrating   global   datacenter   energy-use   \nestimates.    Science ,   367(6481),   pp.984-986.   \nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf .   \n[Mas21]  Masanet,   E.,   March   24,   2021,    Data   Center   Energy   Analysis:   Past,   Present,   and   Future ,   lecture   at   UCSB.   \n[Mer19]   Mehrabi,   N.,   Morstatter,   F.,   Saxena,   N.,   Lerman,   K.   and   Galstyan,   A.,   2019.   A   survey   on   bias   and   fairness   in   \nmachine   learning.   arXiv   preprint   arXiv:1908.09635.    https://arxiv.org/pdf/1908.09635.pdf .   \n[Pha18]  Pham,   H.,   Guan,   M.,   Zoph,   B.,   Le,   Q.   and   Dean,   J.,   2018,   July.   Efficient   neural   architecture   search   via   \nparameters   sharing.   In   International   Conference   on   Machine   Learning   (pp.   4095-4104).   PMLR.     arXiv   preprint   \narXiv:1802.03268 .   \n[Rad20]  Radovanovic,   A.   April   22,   2020,   Our   datacenters   now   work   harder   when   the   sun   shines   and   wind   blows,   \nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows   \n[Raf19]   Raffel,   C.,   Shazeer,   N.,   Roberts,   A.,   Lee,   K.,   Narang,   S.,   Matena,   M.,   Zhou,   Y.,   Li,   W.   and   Liu,   P.J.,   2019.   \nExploring   the   limits   of   transfer   learning   with   a   unified   text-to-text   transformer.    arXiv   preprint   arXiv:1910.10683 .   \n[Rit20]   Ritchie,   H.,   October   22,   2020,   Climate   change   and   flying:   what   share   of   global   CO2   emissions   come   from   \naviation?    https://ourworldindata.org/co2-emissions-from-aviation    .   \n[Ryo14]  Ryor,   J.N.   and   Tawney,   L.E.T.H.A.,   2014.   Utility-Scale   Renewable   Energy:   Understanding   Cost   Parity.   Paris:   \nWorld   Resources   Institute.   \nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf .   \n[San20]  Sanh,   V.,   Debut,   L.,   Chaumond,   J.   and   Wolf,   T.,   2019.   DistilBERT,   a   distilled   version   of   BERT:   smaller,   faster,   \ncheaper   and   lighter.    arXiv   preprint   arXiv:1910.01108 .   \n[Sch20]   Schwartz,   R.,   Dodge,   J.,   Smith,   N.A.   and   Etzioni,   O.,   2020.   Green   AI.    Communications   of   the   ACM ,   63(12),   \npp.54-63.    https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext .   \n[Sch21]   Schleier-Smith,   J.,    Sreekanti,   V.,   Khandelwal,   A.,   Carreira,   J.,   Yadwadkar,   N.,   Popa,   R.,    Joseph   E.   Gonzalez,J.,  \nIon   Stoica,   I.,   and   David   A.   Patterson,   D.,   2021   What   Serverless   Computing   Is   and   Should   Become:   The   Next   \nPhase   of   Cloud   Computing,    Communications   of   the   ACM,    64(5) .     \n[Sha17]  Shazeer,   N.,   Mirhoseini,   A.,   Maziarz,   K.,   Davis,   A.,   Le,   Q.,   Hinton,   G.   and   Dean,   J.,   2017.   Outrageously   large   \nneural   networks:   The   sparsely-gated   mixture-of-experts   layer.   ICLR   2017.     arXiv   preprint   arXiv:1701.06538 .   \n[So19]   So,   D.,   Le,   Q.   and   Liang,   C.,   2019,   May.   The   Evolved   Transformer.   In   International   Conference   on   Machine   \nLearning   2019   (pp.   5877-5886).   PMLR.     arXiv   preprint   arXiv:1901.11117 .   \n[Str19]   Strubell,   E.,   Ganesh,   A.   and   McCallum,   A.,   2019.   Energy   and   policy   considerations   for   deep   learning   in   NLP.   \nACL   2019.     arXiv   preprint   arXiv:1906.02243 .   \n[Sut21]   Sutskever,   I.   Personal   Communication,   February   4,   2021.   \n[Tan19]   Tan,   M.   and   Le,   Q.,   2019,   May.   EfficientNet:   Rethinking   model   scaling   for   convolutional   neural   networks.   In   \nInternational   Conference   on   Machine   Learning   (pp.   6105-6114).   PMLR.    arXiv   preprint   arXiv:1905.11946 .   \n[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.   electricity   generation?    https://www.eia.gov/tools/faqs/faq.php?id=74&t=11 .   \n[Vas17]   Vaswani,   A.,   Shazeer,   N.,   Parmar,   N.,   Uszkoreit,   J.,   Jones,   L.,   Gomez,   A.N.,   Kaiser,   L.   and   Polosukhin,   I.,   2017.   \nAttention   is   all   you   need.   NeurIPS   2017.     arXiv   preprint   arXiv:1706.03762 .   \n[Wan20]  Wang,   Y.,   Yao,   Q.,   Kwok,   J.T.   and   Ni,   L.M.,   2020.   Generalizing   from   a   few   examples:   A   survey   on   few-shot   \nlearning.    ACM   Computing   Surveys ,   53(3),   pp.1-34.\n\nAppendix   A.   Details   of   CO 2    Estimates   for   Four   Large   NLP   Models   in   Tables   1   and   4   \nWe   describe   below   how   we   derived   the   values   in   Tables   1   and   4.     \n● Datacenter   Gross   CO 2 e/KWh   (Table   1,   row   4;   Table   4,   row   7):    The   US   Average   is   from   [USE21].   For   \nGoogle,   we   used   the   CO 2 e   per   KWh   in   the   datacenter   based   at   the   time   that   the   DNNs   ran.     ( Here   is   a   \nlink   for   annual   CFE%   for   Google   Cloud .)   For   Microsoft,   we   use   the   2020   US   national   average.     \n● Datacenter   Net   CO 2 e/KWh   (Table   1,   row   5;   Table   4,   row   8):    No   change   from   above   except   for   Google,  \nwhere   we   used   the   net   CO 2 e   per   KWh   in   the   datacenter   based   on   the   24/7   carbon-free   energy   \nmethodology   to   estimate   net   carbon   emissions   at   the   time 28    that   the   DNNs   ran   (see   Section   2.4   and   \nAppendix   B).    \n● PUE   (Table   1,   row   6;   Table   4,   row   9) :   We   use   the   Google   datacenter   PUE   where   the   DNNs   ran   \n(published   at    https://www.google.com/about/datacenters/efficiency/ ).    OpenAI   told   us   that   the   PUE   for   \nthe   datacenter   where   GPT-3   ran   was   1.10   [Sut21].   \n● Measured   Average   Power   (Table   1,   row   9;   Table   4,   row   12) :   At   Google   we   measured   actual   power   \nusage   rather   than   use   Thermal   Design   Power   (TDP),   as   TDP   is   a   worst   case   for   the   chip.   System   \npower   measurement   includes   the   memory,   fans,   CPU   host,   network   interface   and   so   on,   similar   to   the   \nmethodology   of   [Str19].   OpenAI   measured   V100s   as   running   GPT-3   at   330W.   GPUs   can   run   on   \naverage   closer   to   its   TDP   due   to   GPU's   having   Turbo   Mode   and   Dynamic   Voltage   Frequency   Scaling,   \nnot   found   in   TPU   v2/v3.     \n● Measured   Performance   (Table   1,   row   10;   Table   4,   row   13):    Profiling   data   was   obtained   via   Google's   \ninternal   performance   analysis   tool,   Xprof.   Measured   FLOPs/s   are   calculated   as   the   number   of   \ncomputed   operations   divided   by   execution   time.   \n● Number   of   Chips   (Table   1,   row   11;   Table   4,   row   14) :   We   know   the   number   of   processors   for   the   Google   \nmodels.    NVIDIA’s   press   release   about   GPT-3   suggests   OpenAI   used   10,000   V100   GPUs   for   GPT-3 .   \n● Training   time   (Table   1,   row   12;   Table   4,   row   15) :   We   have   the   exact   training   time   for   Google   DNNs.   \nOpenAI   published   the   total   number   of   floating   point   operations   to   train   their   model:   3.14E+23   [Bro20].   \nOpenAI   told   us   the   V100   runs   GPT-3   at   24.6   TeraFLOPS/sec   [Sut21].   It   takes   ~14.8   days   for   10,000   \nGPUs   at   24.6   TeraFLOPS/sec   to   compute   3.14E+23   FLOPS.   For   the   CO 2 e   calculation,   it   doesn’t   \nactually   matter   whether   it   takes   2   weeks   on   10,000   GPUs   or   20   weeks   on   1,000   GPUs,   but   we   need   \none   number   for   Table   4,   so   we   used   NVIDIA’s   suggestion   of   10,000   GPUs.   \n● Total   Computation   (Table   1,   row   13;   Table   4,   row   16):    We   calculate   from   measured   performance,   \nnumber   of   chips,   and   days   to   train   (except   for   GPT-3,   as   OpenAI   published   the   total   FLOPS).   \n● %   of   Google   2019   Energy   Consumption.   (Table   4,   row   17):    For   all   models   (even   those   not   actually   run   \nin   Google   datacenters   or   not   run   in   2019),   we   calculate   the   percentage   of   Google’s   total   energy   \nconsumption   of   12.2   Terawatt-hours   in   2019   [Goo20].     \n● Ratio   of   round   trips   (Table   4,   row   22) .   To   give   perspective   on   the   CO 2 e   cost   of   training   a   model   is   \ncompared   to   other   activities,   we   show   the   CO 2 e   of   passenger   jets.    Google   Flights    calculated   the   \naverage   CO 2    emission   for   all   the   direct   flights   between   San   Francisco   (SFO)   and   New   York   (JFK)   in   its   \ndatabase   as   90.2t,   so   the   average   round   trip   is   180.4t.   (This   is   for   the   whole   plane,   not   just   for   one   \npassenger.)   Google   Flights   relies   on   this    European   Environmental   Agency   guidebook    for   these   \ncalculations   and   includes   the   minimum   bounds   for   RF   and   NOx   factor   from   Figure   6b   in   [Kär18].   \n● %   Carbon   Free   Energy   (Table   1,   row   17;   Table   4,   row   24) .   Collected   for   when   the   models   were   run.\n\n28   All   the   2020   datacenter   measurements   are   provisional,   awaiting   final   validation   in   May   2021   \n18\n\nAppendix   B.   Carbon   Offset   and   24/7   Carbon   Free   Energy   \nWhile   energy   consumption   is   relatively   straightforward,   policies   to   reduce   carbon   footprint   are   not.   One   reason   \nis   that   they   have   as   much   to   do   about   economics   and   accounting   as   they   do   about   physics.   This   short   \nappendix   tries   to   clarify   the   distinction   between   conventional   carbon   offsets,   Google’s   goal   for   2030   of   24/7   \nCarbon   Free   Energy   (CFE)   for   its   global   datacenters   and   campuses,   and   what   it   is   doing   in   2021   to   set   the   \ngroundwork   for   2030.   Readers   interested   in   greater   depth   should   take   a   look   at   [Ryo14,   Goog16,   Goo21].   \nConventional   carbon   offsets   try   to   create   economic   incentives   to   create   projects   that   avoid   or   remove   \nCO 2 e.   When   pursuing   the   mitigation   of   carbon   emissions   from   electricity   production   and   consumption,   a   \ncompany   can   match   their   MWh   of   consumption   with   MWh   of   clean   energy   through   certificates   called    REC s  \n( Renewable   Energy   Certificates ).   The   rules   for   accounting   and   compensation,   are   defined   as   part   of   the    GHG  \nProtocol ,   under   Scope   2   for   electricity.   Under   the   current   Scope   2   Guidance,   1MWh   of   energy   used   in   July   in,   \nsay,   Georgia   that   produces   carbon   dioxide   can   be   compensated   by   purchasing   1MWh   of   CFE   in   Montana   in   \nNovember.   Typically,   the   period   of   accounting   is   a   calendar   year.   Google   achieved   carbon   neutrality   using   \nconventional   carbon   offsets   starting   in   2007. 29     \nAs   part   of   the    GHG   Protocol ,   the    World   Resource   Institute    defines   terms   and   economic   mechanisms   to   \nensure   consistency   of   claims   about   carbon.   They   defined   the   following   [Car21,   Ryo14]   (also   see   Figure   8):   \n● Additionality :   CO 2 e   reductions   are    additional    if   they   would   not   have   occurred   in   the   absence   of   a   market   \nfor   offset   credits.   Additionality   is   essential   for   the   quality   of   carbon   offset   credits—if   their   associated   \nCO 2 e   reductions   are   not   additional,   then   purchasing   offset   credits   in   lieu   of   reducing   your   own   \nemissions   will   make   climate   change   worse.   \n● The   Grid :   The   transmission   and   distribution   system   that   connects   generators   and   end-users.   \n● Levelized   Cost   Of   Energy   (LCOE) :   The   projected   total   system   and   operating   costs   divided   by   total   KWh   \nproduced   over   the   lifetime   of   the   project   or   contract.   \n● Power   Purchase   Agreement   (PPA) :   A   fixed-price   contractual   agreement   to   purchase   a   power   plant’s   \nenergy,   typically   calculated   using   LCOE.   \n● Renewable   Energy   Certificate   (REC ) 30 :   A   market-based   instrument   that   represents   the   property   rights   \nto   the   environmental,   social,   and   other   non-power   attributes   of   renewable   electricity   generation.   The   \ngoal   is   a   certificate   that   ensures   the   energy   purchased   is   genuinely   renewable   and   not   double   counted.   \nGoogle’s   target   for   2030   is   to   go   beyond   the   traditional   Scope   2   rules   to   restrict   both   the   location   and   the   \naccounting   period.     \n● Instead   of   anywhere   in   a   continent,   the   CFE   purchase   should   be   on   the   same   geographically   local   grid.     \n● Instead   of   the   accounting   period   being   one   year,   the   accounting   should   be   within   the   hour.   \nTo   achieve   100%   24/7   local   CFE,   grids   would   need   to   offer   both   real   time   accounting   of   the   CFE   fraction   of   the   \nstandard   grid   and   the   generating   companies   must   offer   more   flexible   options   to   allow   consumers   to   pick   CFE   \nany   time   of   the   day,   not   just   when   the   wind   blows   or   when   the   sun   shines.   Ideally,   grid   operators   and   \ngenerating   companies   will   deliver   on   that   vision,   and   the   standards   will   evolve   to   certify   and   quantify   the   24/7   \nCFE   approach.   But   we   are   not   there   yet.   \nFigure   8   helps   explain   what   Google   is   doing   today.   Google   signs   long-term   contracts   as   PPAs   with   \nrenewable   energy   generating   companies   to   try   to   cover   Google’s   electricity   consumption. 31    One   benefit   of   \nlong-term   contracts   is   that   they   guarantee   a   reliable   income   stream   for   many   years   and   therefore   make   such   \nprojects   more   easily   financeable.   To   hit   its   24/7   target,   Google   will   continue   to   purchase   clean   energy   from   \nvarious   sources   such   as   energy   storage   and   energy   generation   to   ensure   it   has   a   clean   energy   supply   at   all   24   \nhours   of   the   day,   7   days   a   week.   \n29  In   2017,   Google   became   the   first   major   company   to   match   100%   of   its   annual   electricity   use   with   renewable   \nenergy—purchasing   as   much   clean   energy   as   it   consumed   —which   it   has   done   for   three   consecutive   years.   \n30  RECs   are   more   properly   called    Energy   Attribute   Certificates .   Europe   calls   them    Guarantees   of   Origin    ( GOs ),   not   RECs.   \n31  Google’s   more   than   50   long-term   contracts   to   purchase   renewable   energy   resulted   in   more   than   $7   billion   in   new   capital   \ninvestment   in   renewable   energy   projects   worldwide   as   of   September   2019   [Goo20].   \n19\n\nThe   percentage   of   CFE   for   a   datacenter   is   reported   ex-post,   after   load,   production,   and   grid   mix   data   are   \nsettled   and   made   available   to   Google.   With   the   current   24/7   CFE   framework,   when   Google   cannot   get   100%   \nCFE   from   the   grid   plus   its   clean   energy   contracts   in   a   given   hour,   the   shortfall   counts   against   the   goal.   When   \nthe   grid   and   renewable   energy   contracts   overshoot   in   a   given   hour,   Google   doesn’t   get   any   extra   credit   for   it,   \nas   the   accounting   period   is   reset   every   hour. 32    Since   Google   can   estimate   how   much   CFE   is   expected   in   a  \nspecific   region   based   on   the   grid   and   its   multi-year   clean   energy   contract,   it   incentivizes   programs   to   run   in   this   \nregion. 33   \nTables   1   and   4   show   this   distinction   as    gross   CO 2 e    (energy   from   the   grid)   and   the    net   CO 2 e    (after   applying   \nthe   24/7   local   renewable   energy   purchase   from   the   long-term   contracts).   Since   you   can’t   label   electrons,   there   \nis   no   guarantee   that   Google   is   using   exactly   the   same   clean   energy   that   it   paid   for,   but   in   our   view   the   overall   \neffect   is   the   same.     \nAlas,   Google’s   large   models   in   Table   4   were   run   in   the   Georgia   datacenter,   where   in   the   past   there   was   no   \nor   little   difference   between   gross   and   net   CO 2 e.   Regions   that   have   generator   companies   that   can   supply   clean   \nenergy   24/7   and   offer   marketplaces   that   allow   companies   to   acquire   clean   energy   at   any   time   of   day   will   be   \nmore   compelling   to   expand   future   growth   of   compute   from   a   carbon   impact   perspective.   A   great   example   is   \nOklahoma,   which   allowed   Google   to   average   95.6%   net   CFE   for   2020.    This   is   a   case   of   where   the   grass   \nactually   is   greener   in   Oklahoma   than   in   Georgia.   As   mentioned   above,   in   2021   many   new   TPU   v4   accelerators   \nwill   be   deployed   in   windy   Oklahoma.     \nFigure   8.   This   figure   explains   how   fixed-floating   swaps   work   for   Renewable   Energy   Certificates   (RECs).   \n(Reproduced   from   [Goo16].)   Instead   of   accounting   over   a   full   year   at   a   mix   of   locations   as   in   step   4,   \n24/7   CFE   does   the   accounting   separately   for   every   hour   in   the   year   in   the   same   single   location.     \n32  Excess   CFE   from   Google   projects   is   used   to   support   other   grid   load   as   well   as   incentivizing   additional   renewable   \ndevelopment   by   demonstrating   demand   and   driving   down   prices.   \n33  Google   even   deployed   a   system   in   2020   that    shifts   the   timing   of   non-urgent   compute   tasks   (like   ML   training)   to   when   \ncarbon-free   power   sources   are   most   plentiful    [Rad20].   Its   next   iteration   will   even   move   a   task   to   a   new   datacenter.   \n20\n\n[Image page=20 idx=1 name=X192.png] Size: 1338x895, Data: 162085 bytes\n\nAppendix   C.   Details   of   a    CO 2 e   Estimate   for   NAS   in   an   Average   Datacenter   \n[Str19]   estimates   the   CO 2 e   for   the   neural   architecture   search   (NAS)   to   find   the   more-efficient   Evolved   \nTransformer   architecture   done   by   [So19]   at   Google   as   626,155   pounds   (284   tCO 2 e).    The   estimate   in   [Str19]   \nwas   done   for   the   hypothetical   scenario   of   running   the   computation   on   P100   GPUs   in   the   average   U.S.   \ndatacenter   with   the   average   U.S.   grid   energy   mix.   The   authors   of   this   note   represent   a   superset   of   the   authors   \nof   [So19],   and   we   agree   that   the   information   needed   for   an   accurate   estimate   was   scattered   in   several   \nsubsections   in   the   So    et   al .   paper,   which   makes   it   difficult   to   determine   the   actual   CO 2 e.   This   experience   is   one   \nreason   we   suggest   that   ML   conferences   encourage   future   NLP   papers   that   are   computationally   expensive   to   \ninclude   a   calculation   of   energy   consumed   and   CO 2 e   to   make   sure   all   the   details   are   included,   as   it’s   difficult   to   \ndetermine   them   retrospectively,   as   we   shall   see.   \nNAS   costs   in   [Str19]   are   derived   from   the   NAS   process   described   in   section   5.2   of   [So19]:   \n“The   search   ran   for   15K   child   models,   requiring   a   total   of   979M   train   steps.   Over   13K   models   did   not   \nmake   it   past   the   first   hurdle,   drastically   reducing   the   resources   required   to   view   the   240   thousandth   \ntrain   step   for   top   models,   which   would   have   cost   3.6B   training   steps   for   the   same   number   of   models   \nwithout   hurdles.   After   the   search   concluded,   we   then   selected   the   top   20   models   and   trained   them   for   \nthe   full   300K   steps,   each   on   a   single   TPU   V.2   chip.”   \nThe   projection   of   the   So    et   al .   NAS   cost   by   Strubell    et   al .   overestimates   the   actual   Evolved   Transformer   \nsearch   cost.   Strubell    et   al.    assumed   each   evaluation   in   the   search   is   conducted   using   a   large   configuration:   \nTransformer   (Big)   with   batch   size   32,768.   However,   So    et   al.    actually   used   a   small   proxy   configuration   (Section   \n3.3   of   [So19])   to   reduce   compute   cost   (and   CO 2 e).   This   proxy   version   used   Transformer   (Base)   rather   than   \nTransformer   (Big),   reducing   the   cost/step   by   2.3x.   It   also   reduced   the   training   batch   size   from   32,768   to   4,096   \nwhile   keeping   the   number   of   training   steps   unchanged,   reducing   the   cost/step   by   a   further   8x.     \nAs   a   result,   the   calculations   below   suggest   that   CO 2 e   from   the   misunderstanding   about   the   use   of   the   \nsmaller   proxy   task   were   overestimated   by   a   factor   of   ~18.7:   \nAssume   the   Carbon   Emission   Estimation   Method   in   [Str19]:   \nCO 2 e   =   num_chips   x   num_train_steps   x   hours/train_steps   x   emission/chip_per_hour   \nnum_train_steps   =   979,000,000    #   From   [So19]   \nemission_per_chip_per_hour   ~=   0.2855296   pounds   CO 2 e   #   From    [Str19]    Table   3 34 .   \nEstimation   of   Compute   Cost   in   [Str19]:   \n8   P100s   for   batch   size   32,768   (packed   version)   from   [Vas17]   ( 4096   per   GPU ):   \nnum_chips   =   8     \nThe   Training   speed   of   Transformer   Big   on   P100   from   [Vas17]:   \nhours_per_train_steps     =   84   hours   /   300,000   =   0.00028   (Section   5.2   in   [Vas17])   \nCO 2 e   =   8   *   979,000,000   *   0.00028   *   0.2855296   =    626,155   lbs   (284   t)   \nEstimation   of   Compute   Cost   if   using   GPUs   of   the   Actual   Setting   Adopted   in   [So19]:   \n1   P100   for   batch   size   32,768   /   8=4096   (Section   4.1   second   paragraph   in   [So19]).   \nnum_chips     =   1   (Section   4.3   in   [So19],   note   that   the   actual   search   used   one   TPU   v2   chip   to   fit   the   same   \nbatch   size   as   one   P100)   \nTraining   speed   of   Transformer    Base    on   P100   from   [Vas17]:   \nhours_per_train_steps   =   12   hours   /   100,000   =   0.00012   (Section   5.2   in   [Vas17])   \nCO 2 e   =   1   *   979,000,000   *   0.00012   *   0.2855296   =    33,544   lbs   (15.2   t)   \nAppendix   D   shows   a   ~5X   further   reduction   in   CO 2 e   by   adjusting   for   the   hardware   and   datacenter   where   the   \nNAS   occurred   rather   than   for   P100s   in   a   hypothetical   US   average   datacenter.\n\n34  In   this   calculation,   emission_per_chip_per_hour   =   average   power   per   chip   (in   Watts)   *   PUE   *   lbs   CO 2 e   per   Watt.   \n21\n\nAppendix   D.   Details   of   a   CO 2 e   Estimate   for   Google’s   Actual   NAS   \nTo   calculate   the   emissions   of   the   actual   NAS   in   [So19]   at   Google,   where   the   search   was   actually   performed,   \nwe   must   adjust   by   three   more   factors   beyond   the   assumptions   in   Appendix   C:   \n1. We   use   Google   Georgia   datacenter’s   PUE   from   the   period   in   which   the   search   computation   was   run   \n(1.10   in   Table   4)   instead   of   the   US   average   in   2018   (1.58).     \n2. Strubell    et   al.    used   the   US   average   CO 2    per   kilowatt   hour   (KWh)   as   calculated   by   the   U.S.   \nEnvironmental   Protection   Agency   (EPA)   of   0.423   kg   per   KWh   in   2018.   For   Google,   we   use   the   Georgia   \ndatacenter’s   average   CO 2 e/KWh   for   the   month   when   NAS   was   performed   (0.431    CO 2 e/KWh    in   Table   4).   \n3. So    et   al.    used   Google   TPU   v2   accelerators,   not   NVIDIA   P100   GPUs   as   modeled   in   [Str19].   TPU   v2s   \nare   much   faster,   so   the   search   process   takes   32,633   TPU   v2   hours   instead   of   117,780   P100   hours.   We   \nmeasured   the   power   when   running   the   [So19]   NAS   computation   on   TPU   v2,   including   the   memory,   \nfans,   network   interfaces,   and   the   CPU   host.   The   average   power   was   208   Watts.   [Str19]   estimated   the   \npower   per   P100   as   189   Watts 35 .   The   performance/Watt   for   NAS   of   TPU   v2   improved     \n(   117,780   /   32,633   )   *   (   189   /   208   )   or   3.3X.     \nOur   estimate   of   the   actual   NAS   search   that   So    et   al.    ran   at   Google   after   adjusting   for   the   correct   datacenter   \nPUE,   CO 2 e/KWh,   and   hardware   is   (6.8   *   24   *   200   *    208   *   1.10   /   1000)   *   0.431   /   1000   =   3.2   tCO 2 e   (7096   lbs) . 36   \nThis   actual   emissions   value   is   88X   smaller   than   the   incorrect   estimate   of   the   carbon   emissions   of   this   \nsearch   found   in   Strubell    et   al.     If   we   reran   the   NAS   search   today   on   TPU   v2s   in   Google’s   Iowa   datacenter   \nwith   24/7   local,   real   time   net   CO 2 e   reduction   instead   of   Google’s   Georgia   datacenter,   it   would   drop   from   3.2   \ntCO 2 e   to   0.6   tCO 2 e   (476X   smaller).   If   we   reran   using   newer   TPUs,   tCO 2 e   would   shrink   further.   \nWhen,   where,   how,   and   on   which   hardware   training   occurs   matters   in   addition   to   what   DNN   is   trained,   \nwhich   is   why   it’s   best   to   include   energy   consumed   and   CO 2 e   in   a   publication   rather   than   relying   on   others   to   \nestimate   it   correctly   afterwards.\n\n35  Strubell    et   al .   used   a   mix   of   tools   to   estimate   power   for   GPU,   host   CPU,   and   host   memory   at   189   Watts,   which   they   \nused   to   estimate   NAS.   Our   measurements   for   P100   are   much   higher   in   Table   4   for   Transformer   (Big)   296   Watts.   We   \nincluded   everything   in   the   rack   like   we   do   for   TPUs,   including   TPU   memory,   top   of   rack   switch,   fans,   power   supplies,   and   \nso   on.   The   two   systems   are   running   different   implementations   of   the   same   problem   and   the   CPU   hosts   are   different.   One   \nissue   might   be   that   NVIDIA’s   power   measurement   tool   used   in   [Str18]   samples   power   once   a   minute,   so   there   may   be   \nsampling   issues.     \n36  To   put   3.2   net   tCO 2 e   into   perspective,Table   1   and   Appendix   A   use   Google   Flights   to   calculate   the    CO 2 e    for   the   average   \ndirect   round   trip   flights   between   SFO   and   JFK   as   180.4t.   The   Boeing   767   that   United   Airlines   flies   on   that   route   has   175   \nseats.   Google   Flights   uses   the   historical   average   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   \ntrip.   Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.   \n22", "metadata": {"url": "https://arxiv.org/pdf/2104.10350", "type": "paper", "year": "2021"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Carbon   Emissions   and   Large   Neural   Network   Training     \nDavid   Patterson 1 , 2 ,   Joseph   Gonzalez 2 ,   Quoc   Le 1 ,   Chen   Liang 1 ,   Lluis-Miquel   Munguia 1 ,     \nDaniel   Rothchild 2 ,   David   So 1 ,   Maud   Texier 1 ,   and   Jeff   Dean 1   \n{davidpatterson,   qvl,   crazydonkey,   llmunguia,   davidso,   maudt,   jeff}@google.com,     \n{pattrsn,   jegonzal,   drothchild}@berkeley.edu", "sentences": [{"text": "Carbon   Emissions   and   Large   Neural   Network   Training     \nDavid   Patterson 1 , 2 ,   Joseph   Gonzalez 2 ,   Quoc   Le 1 ,   Chen   Liang 1 ,   Lluis-Miquel   Munguia 1 ,     \nDaniel   Rothchild 2 ,   David   So 1 ,   Maud   Texier 1 ,   and   Jeff   Dean 1   \n{davidpatterson,   qvl,   crazydonkey,   llmunguia,   davidso,   maudt,   jeff}@google.com,     \n{pattrsn,   jegonzal,   drothchild}@berkeley.edu", "metadata": {}}], "metadata": {"page": 1}}, {"text": "Abstract:    The   computation   demand   for   machine   learning   (ML)    has   grown   rapidly    recently,   which   comes   with   a   \nnumber   of   costs.   Estimating   the   energy   cost   helps   measure   its   environmental   impact   and   finding   greener   \nstrategies,   yet   it   is    challenging   without   detailed   information .   \nWe   calculate   the   energy   use   and   carbon   footprint   of   several   recent   large   models— T5 ,    Meena ,    GShard ,   \nSwitch   Transformer ,   and    GPT-3 —and   refine   earlier   estimates   for   the   neural   architecture   search   that   found   \nEvolved   Transformer .   \nWe   highlight   the   following   opportunities   to   improve   energy   efficiency   and    CO 2    equivalent   emissions    ( CO 2 e ):   \n● Large   but   sparsely   activated   DNNs   can   consume   <1/10th   the   energy   of   large,   dense   DNNs   without   \nsacrificing   accuracy   despite   using   as   many   or   even   more   parameters.   \n● Geographic   location   matters   for   ML   workload   scheduling   since   the   fraction   of   carbon-free   energy   and   \nresulting   CO 2 e   vary   ~5X-10X,   even   within   the   same   country   and   the   same   organization.   We   are   now   \noptimizing   where   and   when   large   models   are   trained.   \n● Specific   datacenter   infrastructure   matters,   as   Cloud   datacenters   can   be   ~1.4-2X   more   energy   efficient   \nthan   typical   datacenters,   and   the   ML-oriented   accelerators   inside   them   can   be   ~2-5X   more   effective   \nthan   off-the-shelf   systems.   \nRemarkably,   the   choice   of   DNN,   datacenter,   and   processor   can   reduce   the   carbon   footprint   up   to   ~100-1000X.     \nThese   large   factors   also   make   retroactive   estimates   of   energy   cost   difficult.   To   avoid   miscalculations,   we   \nbelieve   ML   papers   requiring   large   computational   resources   should   make   energy   consumption   and   CO 2 e   \nexplicit   when   practical.   We   are   working   to   be   more   transparent   about   energy   use   and   CO 2 e   in   our   future   \nresearch.   To   help   reduce   the   carbon   footprint   of   ML,   we   believe   energy   usage   and   CO 2 e   should   be   a   key   \nmetric   in   evaluating   models,   and   we   are   collaborating   with   MLPerf   developers   to   include   energy   usage   during   \ntraining   and   inference   in   this   industry   standard   benchmark.   \n1.Introduction   \nAs   ML   models   increase   in   scale,   a   general   trend   is   that   they   become   more   accurate   and   more   capable.   \nHowever,   larger   models   translate   to   greater   computing   demands   and,   by   extension,   greater   energy   demands.   \nWe   focus   on   natural   language   processing   (NLP)   because   it   is   important   in   Google   products   and   because   of   the   \nrecent   development   of   many   large   NLP   models,   e.g.,   T5   [Raf19],   Meena   [Adi20],   GShard   [Lep20],   Switch   \nTransformer   [Fed21],   and   GPT-3   [Bro20].    Recent   studies    attempt   to   evaluate   the   environmental   impact   of   this   \ntrend   in   NLP,   which   is   difficult   [Str19].   Here   we   investigate   and   share   the   estimates   of   the   energy   consumed   \nand   CO 2 e 3    of   these   recent   and   large   NLP   models.   We   also   reduce   by   88X   an   earlier   estimate   of   the   CO 2 e   for   \nthe   neural   architecture   search   for   Evolved   Transformer   [So19,   Str19]   by   characterizing   the   actual   search   \nprocess   on   the   hardware   and   datacenter   on   which   it   was   performed   (see   Appendices   C   and   D).   \n      Our   investigation   into   CO 2 e   revealed   surprises   and   misunderstandings   about   the   full   Deep   Neural   Network  \n(DNN)   lifecycle,   the   datacenters   and   hardware   that   run   them,   the   variations   in   energy   mix,   and   the   difficulty   of   \nassessing   CO 2 e   accurately.   Note   that   we   are   evaluating   the   CO 2 e   of    operating    computers   and   datacenters,   but   \nnot   fabricating   and   recycling   them   (see   [Gup20]   for   the   latter   topic).   \nTo   make   it   easier   for   the   ML   community   to   understand   the   real   impact   of   training   and   how   to   reduce   it,   we   \nendorse   prior   calls   for   new   publication   norms   for   computationally   intensive   ML   models:   \n1  Google   \n2  University   of   California,   Berkeley   \n3  “CO 2 e”   means   CO 2     equivalent   emissions ,   accounting   for   carbon   dioxide   and   all   the   other   greenhouse   gases   as   well:   \nmethane,   nitrous   oxide,   ...   (calculated   from   Equation   A-1   in    40   Code   of   Federal   Regulations   98 ).   “CO 2    emissions”   is   only   \ncarbon   dioxide.    tCO 2 e    stands   for   1000   kg   (metric   ton)   of   CO 2     equivalent   emissions .   \n1", "sentences": [{"text": "Abstract:    The   computation   demand   for   machine   learning   (ML)    has   grown   rapidly    recently,   which   comes   with   a   \nnumber   of   costs.", "metadata": {}}, {"text": "Estimating   the   energy   cost   helps   measure   its   environmental   impact   and   finding   greener   \nstrategies,   yet   it   is    challenging   without   detailed   information .", "metadata": {}}, {"text": "We   calculate   the   energy   use   and   carbon   footprint   of   several   recent   large   models— T5 ,    Meena ,    GShard ,   \nSwitch   Transformer ,   and    GPT-3 —and   refine   earlier   estimates   for   the   neural   architecture   search   that   found   \nEvolved   Transformer .", "metadata": {}}, {"text": "We   highlight   the   following   opportunities   to   improve   energy   efficiency   and    CO 2    equivalent   emissions    ( CO 2 e ):   \n● Large   but   sparsely   activated   DNNs   can   consume   <1/10th   the   energy   of   large,   dense   DNNs   without   \nsacrificing   accuracy   despite   using   as   many   or   even   more   parameters.", "metadata": {}}, {"text": "● Geographic   location   matters   for   ML   workload   scheduling   since   the   fraction   of   carbon-free   energy   and   \nresulting   CO 2 e   vary   ~5X-10X,   even   within   the   same   country   and   the   same   organization.", "metadata": {}}, {"text": "We   are   now   \noptimizing   where   and   when   large   models   are   trained.", "metadata": {}}, {"text": "● Specific   datacenter   infrastructure   matters,   as   Cloud   datacenters   can   be   ~1.4-2X   more   energy   efficient   \nthan   typical   datacenters,   and   the   ML-oriented   accelerators   inside   them   can   be   ~2-5X   more   effective   \nthan   off-the-shelf   systems.", "metadata": {}}, {"text": "Remarkably,   the   choice   of   DNN,   datacenter,   and   processor   can   reduce   the   carbon   footprint   up   to   ~100-1000X.", "metadata": {}}, {"text": "These   large   factors   also   make   retroactive   estimates   of   energy   cost   difficult.", "metadata": {}}, {"text": "To   avoid   miscalculations,   we   \nbelieve   ML   papers   requiring   large   computational   resources   should   make   energy   consumption   and   CO 2 e   \nexplicit   when   practical.", "metadata": {}}, {"text": "We   are   working   to   be   more   transparent   about   energy   use   and   CO 2 e   in   our   future   \nresearch.", "metadata": {}}, {"text": "To   help   reduce   the   carbon   footprint   of   ML,   we   believe   energy   usage   and   CO 2 e   should   be   a   key   \nmetric   in   evaluating   models,   and   we   are   collaborating   with   MLPerf   developers   to   include   energy   usage   during   \ntraining   and   inference   in   this   industry   standard   benchmark.", "metadata": {}}, {"text": "1.Introduction   \nAs   ML   models   increase   in   scale,   a   general   trend   is   that   they   become   more   accurate   and   more   capable.", "metadata": {}}, {"text": "However,   larger   models   translate   to   greater   computing   demands   and,   by   extension,   greater   energy   demands.", "metadata": {}}, {"text": "We   focus   on   natural   language   processing   (NLP)   because   it   is   important   in   Google   products   and   because   of   the   \nrecent   development   of   many   large   NLP   models,   e.g.,   T5   [Raf19],   Meena   [Adi20],   GShard   [Lep20],   Switch   \nTransformer   [Fed21],   and   GPT-3   [Bro20].", "metadata": {}}, {"text": "Recent   studies    attempt   to   evaluate   the   environmental   impact   of   this   \ntrend   in   NLP,   which   is   difficult   [Str19].", "metadata": {}}, {"text": "Here   we   investigate   and   share   the   estimates   of   the   energy   consumed   \nand   CO 2 e 3    of   these   recent   and   large   NLP   models.", "metadata": {}}, {"text": "We   also   reduce   by   88X   an   earlier   estimate   of   the   CO 2 e   for   \nthe   neural   architecture   search   for   Evolved   Transformer   [So19,   Str19]   by   characterizing   the   actual   search   \nprocess   on   the   hardware   and   datacenter   on   which   it   was   performed   (see   Appendices   C   and   D).", "metadata": {}}, {"text": "Our   investigation   into   CO 2 e   revealed   surprises   and   misunderstandings   about   the   full   Deep   Neural   Network  \n(DNN)   lifecycle,   the   datacenters   and   hardware   that   run   them,   the   variations   in   energy   mix,   and   the   difficulty   of   \nassessing   CO 2 e   accurately.", "metadata": {}}, {"text": "Note   that   we   are   evaluating   the   CO 2 e   of    operating    computers   and   datacenters,   but   \nnot   fabricating   and   recycling   them   (see   [Gup20]   for   the   latter   topic).", "metadata": {}}, {"text": "To   make   it   easier   for   the   ML   community   to   understand   the   real   impact   of   training   and   how   to   reduce   it,   we   \nendorse   prior   calls   for   new   publication   norms   for   computationally   intensive   ML   models:   \n1  Google   \n2  University   of   California,   Berkeley   \n3  “CO 2 e”   means   CO 2     equivalent   emissions ,   accounting   for   carbon   dioxide   and   all   the   other   greenhouse   gases   as   well:   \nmethane,   nitrous   oxide,   ...", "metadata": {}}, {"text": "(calculated   from   Equation   A-1   in    40   Code   of   Federal   Regulations   98 ).", "metadata": {}}, {"text": "“CO 2    emissions”   is   only   \ncarbon   dioxide.", "metadata": {}}, {"text": "tCO 2 e    stands   for   1000   kg   (metric   ton)   of   CO 2     equivalent   emissions .", "metadata": {}}, {"text": "1", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "1. We   must   assess   CO 2 e   correctly,   but   it   is   hard   to   quantify   precisely   in   part   because   all   the   required   \ninformation   is   rarely   reported   or   publicly   available   (e.g.,   datacenter,   hardware,   energy   mix)   and   in   part   \nbecause   it   is   hard   to   uncover   important   details   afterwards   (see   Section   4.1).   To   make   the   carbon   costs   \nof   training   transparent,   we   encourage   more   researchers   to   measure   energy   usage   and   CO 2 e—or   to   get   \na   rough   estimate   using   a   tool   like   ML   Emissions   Calculator   [Lac19]   (Section   4.3)—and   publish   the   data.     \n2. We   agree   with   [Str19,Sch20,Hen20]   that   efficiency   should   be   an   evaluation   criterion   for   publishing   ML   \nresearch   on   computationally   intensive   models   besides   accuracy   and   related   measures,   since   we   need   \nto   encourage   advances   across   the   board   as    the   most   sustainable   energy   is   the   energy   you   don’t   use .     \n3. And   even   if   we   could   bring   CO 2 e   to   zero   in   cloud   datacenters,   reducing   training   time   matters,   both   \nbecause   “time   is   money,”   and   because   cheaper   training   lets   more   people   participate.   Hence,   we   also   \nsecond   the   recommendation   of   [Str19]   for   more   researchers   to   publish   the   number   of   accelerators   and   \ntheir   time   to   train   computationally   intensive   models   to   inspire   progress   in   reducing   training   costs.   \nWe   believe   such   new   incentives   could   lead   to   a   virtuous   cycle   where   ML   practitioners   compete   to   increase   \naccuracy   while   lowering   energy   consumption   and   CO 2 e   that   could   bend   the   curve   of   ML   carbon   footprint   \ngrowth   for   computationally   intensive   NLP   models.   \nThe   following   sections   summarize   the   findings   that   led   to   these   recommendations.   They   also   document   our   \nCO 2 e   estimates,   highlight   recent   advances   that   curb   the   CO 2 e   of   ML,   and   estimate   the   CO 2 e   from   training   the   \nfive   recent   large   NLP   models   mentioned   above.   We   end   by   updating   the   results   of   [Str19]   on   the   emissions   of   \nthe   Evolved   Transformer   neural   architecture   search   and   discussing   common   misperceptions.     \nWe   start   with   an   overview   of   the   carbon   footprint   over   the   DNN   lifecycle   and   show   ways   to   improve   a   \nconcrete   example   by   nearly   two   orders   of   magnitude.", "sentences": [{"text": "1.", "metadata": {}}, {"text": "We   must   assess   CO 2 e   correctly,   but   it   is   hard   to   quantify   precisely   in   part   because   all   the   required   \ninformation   is   rarely   reported   or   publicly   available   (e.g.,   datacenter,   hardware,   energy   mix)   and   in   part   \nbecause   it   is   hard   to   uncover   important   details   afterwards   (see   Section   4.1).", "metadata": {}}, {"text": "To   make   the   carbon   costs   \nof   training   transparent,   we   encourage   more   researchers   to   measure   energy   usage   and   CO 2 e—or   to   get   \na   rough   estimate   using   a   tool   like   ML   Emissions   Calculator   [Lac19]   (Section   4.3)—and   publish   the   data.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "We   agree   with   [Str19,Sch20,Hen20]   that   efficiency   should   be   an   evaluation   criterion   for   publishing   ML   \nresearch   on   computationally   intensive   models   besides   accuracy   and   related   measures,   since   we   need   \nto   encourage   advances   across   the   board   as    the   most   sustainable   energy   is   the   energy   you   don’t   use .", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "And   even   if   we   could   bring   CO 2 e   to   zero   in   cloud   datacenters,   reducing   training   time   matters,   both   \nbecause   “time   is   money,”   and   because   cheaper   training   lets   more   people   participate.", "metadata": {}}, {"text": "Hence,   we   also   \nsecond   the   recommendation   of   [Str19]   for   more   researchers   to   publish   the   number   of   accelerators   and   \ntheir   time   to   train   computationally   intensive   models   to   inspire   progress   in   reducing   training   costs.", "metadata": {}}, {"text": "We   believe   such   new   incentives   could   lead   to   a   virtuous   cycle   where   ML   practitioners   compete   to   increase   \naccuracy   while   lowering   energy   consumption   and   CO 2 e   that   could   bend   the   curve   of   ML   carbon   footprint   \ngrowth   for   computationally   intensive   NLP   models.", "metadata": {}}, {"text": "The   following   sections   summarize   the   findings   that   led   to   these   recommendations.", "metadata": {}}, {"text": "They   also   document   our   \nCO 2 e   estimates,   highlight   recent   advances   that   curb   the   CO 2 e   of   ML,   and   estimate   the   CO 2 e   from   training   the   \nfive   recent   large   NLP   models   mentioned   above.", "metadata": {}}, {"text": "We   end   by   updating   the   results   of   [Str19]   on   the   emissions   of   \nthe   Evolved   Transformer   neural   architecture   search   and   discussing   common   misperceptions.", "metadata": {}}, {"text": "We   start   with   an   overview   of   the   carbon   footprint   over   the   DNN   lifecycle   and   show   ways   to   improve   a   \nconcrete   example   by   nearly   two   orders   of   magnitude.", "metadata": {}}], "metadata": {"page": 2}}, {"text": "2.Energy   Consumption   and   Carbon   Footprint   of   an   NLP   Model   \nElectricity   required   to   run   an   ML   model   is   a   function   of   the   algorithm,   the   program   that   implements   it,   the   \nnumber   of   processors   that   run   the   program,   the   speed   and   power   of   those   processors,   a   datecenter’s   \nefficiency   in   delivering   power   and   cooling   the   processors,   and   the   energy   supply   mix   (renewable,   gas,   coal,   \netc.).   A   simplified   formula   for   the   carbon   footprint   of   an   ML   model   that   takes   these   factors   into   account   is:   \n ootprint  electrical energy ueries  ) KWh F =( train  +q ×electrical energyinference ×CO2e \ndatacenter/  \nMost   companies   spend   more   energy   on   serving   a   DNN   model   (performing   inference)   than   on   training   it.   \nFor   example,   NVIDIA   estimated   that   80–90%   of   the   ML   workload   is   inference   processing   [Leo19].   Similarly,   \nAmazon   Web   services   claimed   that    90%    of   the   ML   demand   in   the   cloud   is   for   inference   [Bar19].   Given   its   \nsubstantial   role   in   the   ML   model   lifecycle,   Alibaba,   Amazon,   Google,   and   NVIDIA   designed   ML   accelerators   \nsolely   for   inference.   If   the   total   ML   energy   is   split   10%   on   training   and   90%   on   serving,   then   even   if   a   given   ML   \nmodel   required   double   the   energy   cost   of   training,   it   could   reduce   overall   total   carbon   emissions   if   that   model   \nalso   cut   serving   energy   by   20%.   Because   energy   usage   during   training   is   more   isolated   and   thus   easier   to   \ninvestigate   than   inference,   we   focus   on   it   in   this   paper,   but   keep   in   mind   that   the   carbon   footprint   of   inference   is   \nsignificant.   \nAn   ML   practitioner   is   often   improving   the   quality   of   an   existing   model   rather   than   starting   from   scratch.   We   \nwill   use   as   a   running   example   (found   in   [Str19])   the   CO 2 e   impact   of   going   from   training   a   Transformer   model   \nusing   off-the-shelf   hardware   in   an   average   datacenter   to   training   an   Evolved   Transformer   model   on   Google’s   \ncustom   hardware   for   DNNs   in   Google’s   energy   optimized   datacenters.   The   large   impact   of   each   factor   in   this  \nexample   demonstrates   why   we   suggest   that   the   trainers   of   a   model   be   involved   in   the   calculation   of   its   costs.    \nTable   1   shows   the   CO 2 e   breakdown,   which   we   explain   further   in   the   next   subsections   along   with   the   \nbusiness   rationale   for   these   improvements,   demonstrating   the   cross-cutting   incentives   for   more   efficient   ML.   \nFigure   1   illustrates   the   gains   per   step;   the   overall   improvement   in   CO 2 e   is   57X.   This   large   gain   demonstrates   \nwhy   the   selection   of   the   DNN   model,   processor,   datacenter,   and   geographic   location   are   critical   to   improve   \nCO 2 e.   Table   2   shows   the   units   for   CO 2 e   and   a   running   example   that   puts   these   units   into   perspective.   \nWe   next   go   over   the   four   factors   in   more   detail   that   contribute   to   the   carbon   footprint   of   training.", "sentences": [{"text": "2.Energy   Consumption   and   Carbon   Footprint   of   an   NLP   Model   \nElectricity   required   to   run   an   ML   model   is   a   function   of   the   algorithm,   the   program   that   implements   it,   the   \nnumber   of   processors   that   run   the   program,   the   speed   and   power   of   those   processors,   a   datecenter’s   \nefficiency   in   delivering   power   and   cooling   the   processors,   and   the   energy   supply   mix   (renewable,   gas,   coal,   \netc.).", "metadata": {}}, {"text": "A   simplified   formula   for   the   carbon   footprint   of   an   ML   model   that   takes   these   factors   into   account   is:   \n ootprint  electrical energy ueries  ) KWh F =( train  +q ×electrical energyinference ×CO2e \ndatacenter/  \nMost   companies   spend   more   energy   on   serving   a   DNN   model   (performing   inference)   than   on   training   it.", "metadata": {}}, {"text": "For   example,   NVIDIA   estimated   that   80–90%   of   the   ML   workload   is   inference   processing   [Leo19].", "metadata": {}}, {"text": "Similarly,   \nAmazon   Web   services   claimed   that    90%    of   the   ML   demand   in   the   cloud   is   for   inference   [Bar19].", "metadata": {}}, {"text": "Given   its   \nsubstantial   role   in   the   ML   model   lifecycle,   Alibaba,   Amazon,   Google,   and   NVIDIA   designed   ML   accelerators   \nsolely   for   inference.", "metadata": {}}, {"text": "If   the   total   ML   energy   is   split   10%   on   training   and   90%   on   serving,   then   even   if   a   given   ML   \nmodel   required   double   the   energy   cost   of   training,   it   could   reduce   overall   total   carbon   emissions   if   that   model   \nalso   cut   serving   energy   by   20%.", "metadata": {}}, {"text": "Because   energy   usage   during   training   is   more   isolated   and   thus   easier   to   \ninvestigate   than   inference,   we   focus   on   it   in   this   paper,   but   keep   in   mind   that   the   carbon   footprint   of   inference   is   \nsignificant.", "metadata": {}}, {"text": "An   ML   practitioner   is   often   improving   the   quality   of   an   existing   model   rather   than   starting   from   scratch.", "metadata": {}}, {"text": "We   \nwill   use   as   a   running   example   (found   in   [Str19])   the   CO 2 e   impact   of   going   from   training   a   Transformer   model   \nusing   off-the-shelf   hardware   in   an   average   datacenter   to   training   an   Evolved   Transformer   model   on   Google’s   \ncustom   hardware   for   DNNs   in   Google’s   energy   optimized   datacenters.", "metadata": {}}, {"text": "The   large   impact   of   each   factor   in   this  \nexample   demonstrates   why   we   suggest   that   the   trainers   of   a   model   be   involved   in   the   calculation   of   its   costs.", "metadata": {}}, {"text": "Table   1   shows   the   CO 2 e   breakdown,   which   we   explain   further   in   the   next   subsections   along   with   the   \nbusiness   rationale   for   these   improvements,   demonstrating   the   cross-cutting   incentives   for   more   efficient   ML.", "metadata": {}}, {"text": "Figure   1   illustrates   the   gains   per   step;", "metadata": {}}, {"text": "the   overall   improvement   in   CO 2 e   is   57X.", "metadata": {}}, {"text": "This   large   gain   demonstrates   \nwhy   the   selection   of   the   DNN   model,   processor,   datacenter,   and   geographic   location   are   critical   to   improve   \nCO 2 e.", "metadata": {}}, {"text": "Table   2   shows   the   units   for   CO 2 e   and   a   running   example   that   puts   these   units   into   perspective.", "metadata": {}}, {"text": "We   next   go   over   the   four   factors   in   more   detail   that   contribute   to   the   carbon   footprint   of   training.", "metadata": {}}], "metadata": {"page": 2}}, {"text": "2", "sentences": [{"text": "2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Table   1.   See   Appendix   A   for   more   detail 4 .   Estimates   of   CO 2 e   for   Transformer   and   Evolved   Transformer   \nfor   P100   and   TPU   v2   are   based   on   power   measurements. 5    Evolved   Transformer   (Medium)   reached   the   \nsame   accuracy   as   Transformer   (Big)   in   [So19].   CO 2 e     is   shown   both   before   (“gross”)   and   after   (“net”)   \naccounting   for   24/7   reduction   via   real   time,   local   carbon   free   energy   purchases   (Appendix   B).   To   help   \nput   the   CO 2 e   numbers   in   perspective,   a   single   passenger   round   trip   SF-NY   is   ~1.2t   CO 2 e   (Table   2).", "sentences": [{"text": "Table   1.", "metadata": {}}, {"text": "See   Appendix   A   for   more   detail 4 .", "metadata": {}}, {"text": "Estimates   of   CO 2 e   for   Transformer   and   Evolved   Transformer   \nfor   P100   and   TPU   v2   are   based   on   power   measurements.", "metadata": {}}, {"text": "5    Evolved   Transformer   (Medium)   reached   the   \nsame   accuracy   as   Transformer   (Big)   in   [So19].", "metadata": {}}, {"text": "CO 2 e     is   shown   both   before   (“gross”)   and   after   (“net”)   \naccounting   for   24/7   reduction   via   real   time,   local   carbon   free   energy   purchases   (Appendix   B).", "metadata": {}}, {"text": "To   help   \nput   the   CO 2 e   numbers   in   perspective,   a   single   passenger   round   trip   SF-NY   is   ~1.2t   CO 2 e   (Table   2).", "metadata": {}}], "metadata": {"page": 3}}, {"text": "Figure   1.   Improvement   in   CO 2 e   over   Transformer   (Big)   on   P100   GPUs   in   an   average   US   datacenter   \nversus   Evolved   Transformer   (Medium)   on   TPU   v2s   in   the   Google   Iowa   datacenter.", "sentences": [{"text": "Figure   1.", "metadata": {}}, {"text": "Improvement   in   CO 2 e   over   Transformer   (Big)   on   P100   GPUs   in   an   average   US   datacenter   \nversus   Evolved   Transformer   (Medium)   on   TPU   v2s   in   the   Google   Iowa   datacenter.", "metadata": {}}], "metadata": {"page": 3}}, {"text": "Table   2.   Small   and   large   units   for   energy   and   carbon   footprint   in   this   paper,   plus   airline   travel   CO 2 e   \nused   for   perspective   on   the   relative   size   of   ML   emissions   compared   to   other   activities   (Section   4.8).   \n4  The   peak   TeraFLOPS/second   is   19   for   P100   and   46   for   TPU   v2.   \n5  Training   on   TPU   v3   instead   of   TPU   v2   takes   Transformer   (Big)   0.44   days   (averaging   61   TFLOPS/s)   and   0.37   days   (47   \nTFLOPS/s)   for   Evolved   Transformer   (Medium).   For   TPU   v4,   the   respective   numbers   are   0.25   days   (93   TFLOPS/s)    and   \n0.19   days   (73   TFLOPS/s).   TPU   v3   shrinks   energy   consumed   and   gross   and   net   CO 2 e   from   TPU   v2   by   ~1.4X   for   \nTransformer   and   by   ~1.3X   for   Evolved   Transformer.     \n3   \nModel   Transformer   (Big)   \nEvolved   \nTransformer   \n(Medium)   \nTransformer   \n(Big)   \nEvolved   \nTransformer   \n(Medium)   \nNumber   of   Parameters   (B)   0.21   0.13   0.21   0.13   \nDatacenter   US   Average  Google   Iowa   Council   Bluffs   \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh)   2020   \n(Section   2.4   and   Appendix   D)   0.429   0.478   \nDatacenter   Net   CO 2 e/KWh   (kg/KWh)   2020   (Section   \n2.4   and   Appendix   D)   0.429   0.080   \nDatacenter   PUE   (Latest   quarter   2020)   1.59   1.11   \nProcessor   P100   TPU   v2   \nChip   Thermal   Design   Power   (TDP   in   Watts)   300   280   \nMeasured   System   Average   Power   including   \nmemory,   network   interface,   fans,   host   CPU   (Watts)   296   271   229   227   \nMeasured   Performance   (TFLOPS/s) 5   6.7   4.7   28.8   24.0   \nNumber   of   Chips   8   \nTraining   time   to   accuracy   goal   (days)   3.5   3.2   0.81   0.62   \nTotal   Computation   (floating   point   operations)   1.61E+19   1.03E+19   1.61E+19   1.03E+19   \nEnergy   consumption   (KWh)   316   221   185   40   30   \nGross   CO 2 e   for   Model   Training   (metric   ton)   (Section  \n2.4   and   Appendix   D)   0.1357   0.1055   0.0883   0.0189   0.0143   \nNet   CO 2 e   for   Model   Training   (metric   ton)   (Section   \n2.4   and   Appendix   D)   0.1357   0.0177   0.0148   0.0032   0.0024   \n%   24/7   net   carbon   free   energy   (CY   2019)   N/A   78%   \n  Small   Unit   Large   Unit   \nEnergy   Consumption   Kilowatt   hours   (KWh)   Megawatt   hours   (MWh   =   1000   KWh)   \nCarbon   Footprint   (CO 2 e   or   CO 2 )   Kilograms   (kg)   Metric   ton   (t   =   1000   kg)   \nPerspective   (see   Appendix   A)   Single   passenger   round   \ntrip   SF-NY   (1.2t   CO 2 e)   Passenger   jet   plane   round   trip   SF-NY   (180t   CO 2 e)", "sentences": [{"text": "Table   2.", "metadata": {}}, {"text": "Small   and   large   units   for   energy   and   carbon   footprint   in   this   paper,   plus   airline   travel   CO 2 e   \nused   for   perspective   on   the   relative   size   of   ML   emissions   compared   to   other   activities   (Section   4.8).", "metadata": {}}, {"text": "4  The   peak   TeraFLOPS/second   is   19   for   P100   and   46   for   TPU   v2.", "metadata": {}}, {"text": "5  Training   on   TPU   v3   instead   of   TPU   v2   takes   Transformer   (Big)   0.44   days   (averaging   61   TFLOPS/s)   and   0.37   days   (47   \nTFLOPS/s)   for   Evolved   Transformer   (Medium).", "metadata": {}}, {"text": "For   TPU   v4,   the   respective   numbers   are   0.25   days   (93   TFLOPS/s)    and   \n0.19   days   (73   TFLOPS/s).", "metadata": {}}, {"text": "TPU   v3   shrinks   energy   consumed   and   gross   and   net   CO 2 e   from   TPU   v2   by   ~1.4X   for   \nTransformer   and   by   ~1.3X   for   Evolved   Transformer.", "metadata": {}}, {"text": "3   \nModel   Transformer   (Big)   \nEvolved   \nTransformer   \n(Medium)   \nTransformer   \n(Big)   \nEvolved   \nTransformer   \n(Medium)   \nNumber   of   Parameters   (B)   0.21   0.13   0.21   0.13   \nDatacenter   US   Average  Google   Iowa   Council   Bluffs   \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh)   2020   \n(Section   2.4   and   Appendix   D)   0.429   0.478   \nDatacenter   Net   CO 2 e/KWh   (kg/KWh)   2020   (Section   \n2.4   and   Appendix   D)   0.429   0.080   \nDatacenter   PUE   (Latest   quarter   2020)   1.59   1.11   \nProcessor   P100   TPU   v2   \nChip   Thermal   Design   Power   (TDP   in   Watts)   300   280   \nMeasured   System   Average   Power   including   \nmemory,   network   interface,   fans,   host   CPU   (Watts)   296   271   229   227   \nMeasured   Performance   (TFLOPS/s) 5   6.7   4.7   28.8   24.0   \nNumber   of   Chips   8   \nTraining   time   to   accuracy   goal   (days)   3.5   3.2   0.81   0.62   \nTotal   Computation   (floating   point   operations)   1.61E+19   1.03E+19   1.61E+19   1.03E+19   \nEnergy   consumption   (KWh)   316   221   185   40   30   \nGross   CO 2 e   for   Model   Training   (metric   ton)   (Section  \n2.4   and   Appendix   D)   0.1357   0.1055   0.0883   0.0189   0.0143   \nNet   CO 2 e   for   Model   Training   (metric   ton)   (Section   \n2.4   and   Appendix   D)   0.1357   0.0177   0.0148   0.0032   0.0024   \n%   24/7   net   carbon   free   energy   (CY   2019)   N/A   78%   \n  Small   Unit   Large   Unit   \nEnergy   Consumption   Kilowatt   hours   (KWh)   Megawatt   hours   (MWh   =   1000   KWh)   \nCarbon   Footprint   (CO 2 e   or   CO 2 )   Kilograms   (kg)   Metric   ton   (t   =   1000   kg)   \nPerspective   (see   Appendix   A)   Single   passenger   round   \ntrip   SF-NY   (1.2t   CO 2 e)   Passenger   jet   plane   round   trip   SF-NY   (180t   CO 2 e)", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=X39.png] Size: 1386x522, Data: 45512 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=X39.png] Size: 1386x522, Data: 45512 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "X39.png", "image_width": 1386, "image_height": 522, "attachment_type": "image", "has_image_data": true, "image_data_size": 45512}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "2.1   Algorithm/program   improvement   \nThe   Evolved   Transformer   (Medium)   model   discovered   by   So   et   al.   [So19]   using   neural   architecture   search   \n(see   Section   4.1)   uses   1.6X   fewer   FLOPS   and   1.1X–1.3X   less   time   than   Transformer   (Big)   at   slightly   higher   \naccuracy   (see   Table   1   and   Appendix   A) 6 .   \nBusiness   Rationale .   Training   faster   saves   ML   researchers   time   as   well   as   saves   their   organizations   money   \nand   reduces   CO 2 e.   \n2.2   Processor   improvement   \nGoogle’s   custom   TPU   v2   processor   runs   Transformer   (Big)   4.3X   faster   than   P100   GPUs   and   Evolved   \nTransformer   (Medium)   5.2X   faster. 7    TPU   v2   also   uses   less   power:   1.3X   less   for   Transformer   and   1.2X   less   for   \nEvolved   Transformer.   The   net   gain   in   performance/Watt   is   5.6X   and   6.2X,   respectively.   \nBusiness   Rationale .   The   substantial   increase   in   the   scope   and   scale   of   deep   learning   over   the   past   decade   \nhas   created   the   opportunity   to   build   customized   hardware   that   is   tailored   to   the   kinds   of   computations   involved   \nin   training   and   serving   DNN   models.   Instead   of   using   GPUs   like   many   other   organizations,   over   the   past   seven   \nyears   Google   has   designed,   built,   and   deployed   four   generations   of   custom   Tensor   Processing   Unit   (TPU)   \nhardware   for   DNNs   to   accelerate   model   training   and   serving   [Jou21].   To   get   a   better   return   on   their   investment,   \ncloud   companies   actually   aim   for   improved    cost-performance,    as   opposed   to   simply   performance.   Cost   here   \nmeans    Total   Cost   of   Ownership    ( TCO ),   which   includes   the   annual   operating   costs   such   as   electricity   consumed   \nand   amortization   of   capital   expenditures   for   the   computer,   cooling,   power   distribution,   and   the   building.   Jouppi   \net   al .   show   that   power   consumption   is   nearly   perfectly   linearly   correlated   with   TCO 8    [Jou21],   so   \nperformance/TCO   gains   also   help   performance/Watt,   saving   money   and   reducing   CO 2 e.     \n2.3   Datacenter   improvement   \nA   useful   quantitative   metric   of   datacenter   efficiency   is   the   energy   overhead   above   and   beyond   what   directly   \npowers   the   computing   equipment   inside   the   datacenters.   If   the   overhead   were   50%,   the    Power   Usage   \nEffectiveness    ( PUE )   would   be   1.50.   The   US   national   datacenter   average   in   2018   was   1.58,   which   is   the   value   \n[Str19]     used ;    In   2020,   it   was   1.59 .   Google    publishes   its   datacenter   PUE   online   every   quarter .   The   PUE   for   the   \nIowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,   a   factor   of   1.4X   better.   Cloud   datacenters   are   \nroughly   2X   as   energy   efficient   as   a   typical   enterprise   datacenter   due   to   other   factors   like   server   utilization   (see  \n[Höl20]),   but   we’ll   limit   the   quantitative   improvement   in   this   paper   to   the   easy-to-measure   PUE.   \nMore   broadly,   since   cloud   datacenters   are   much   more   energy   efficient,   the   long-feared   explosion   of   \ndatacenter   energy   usage   has   not   materialized.   A   recent   paper   in    Science    [Mas20]   found   that   global   datacenter   \nenergy   consumption   increased   by   only   6%   compared   with   2010,   despite   computing   capacity   increasing   by   \n550%   over   the   same   time   period   [Mas21].   \nBusiness   Rationale .   Cloud   companies   strive   for   energy   efficient   datacenters   since   it   saves   money   and   \nlowers   emissions.   Perhaps   we   should   add   “energy   is   money”   to   Ben   Franklin’s   “time   is   money”   advice?     \n2.4   Energy   mix   improvement   \n  The   gross   carbon   intensity   of   energy   according   to   the   U.S.   average   mix   is    0.429    kg   of   CO 2 e/KWh   [USE21].   \nAfter   matching   Google’s   clean   energy   purchase   per   its   24/7   carbon-free   energy   framework   (see   Appendix   B),   \nthe   net   CO 2 e   drops   to   0.080   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer,   which   is   5.4X   better.   \nBusiness   Rationale .   Transmitting   electricity   long   distances   is   more   expensive   and   less   efficient   than   \nsending   information   as   photons   over   optical   fibers   [Arm10].   Cloud   computing   allows   companies   like   Google   to   \nhave   a   global   portfolio   of   datacenters,   many   of   which   are   placed   where   the   grid   is   cleaner   (e.g.,   Finland)   or   \nwhere   companies   can   purchase   clean   energy   directly   (e.g.,   Iowa).   In   2020   Google   announced   a   new   objective   \nin   its   energy   strategy:   by   2030,   it   aims   to   run   all   Google   datacenters   and   offices   on   carbon-free   energy   24/7.   \nFor   our   24/7   carbon-free   energy   accounting    (see   Appendix   B),   we   deduct   from   the   hourly   consumption   all   \n6  Their   neural   architecture   search   also   found   another   version   that   had   the   same   performance   but   better   accuracy.   \n7  [Str19]   used   P100s,   which   are   contemporary   GPUs   to   TPU   v2s.   \n8  The   correlation   coefficient   R   between   TCO   and   TDP   is   0.99   out   of   1.00   across   four   generations   of   TPUs.   \n4", "sentences": [{"text": "2.1   Algorithm/program   improvement   \nThe   Evolved   Transformer   (Medium)   model   discovered   by   So   et   al.", "metadata": {}}, {"text": "[So19]   using   neural   architecture   search   \n(see   Section   4.1)   uses   1.6X   fewer   FLOPS   and   1.1X–1.3X   less   time   than   Transformer   (Big)   at   slightly   higher   \naccuracy   (see   Table   1   and   Appendix   A) 6 .", "metadata": {}}, {"text": "Business   Rationale .", "metadata": {}}, {"text": "Training   faster   saves   ML   researchers   time   as   well   as   saves   their   organizations   money   \nand   reduces   CO 2 e.", "metadata": {}}, {"text": "2.2   Processor   improvement   \nGoogle’s   custom   TPU   v2   processor   runs   Transformer   (Big)   4.3X   faster   than   P100   GPUs   and   Evolved   \nTransformer   (Medium)   5.2X   faster.", "metadata": {}}, {"text": "7    TPU   v2   also   uses   less   power:   1.3X   less   for   Transformer   and   1.2X   less   for   \nEvolved   Transformer.", "metadata": {}}, {"text": "The   net   gain   in   performance/Watt   is   5.6X   and   6.2X,   respectively.", "metadata": {}}, {"text": "Business   Rationale .", "metadata": {}}, {"text": "The   substantial   increase   in   the   scope   and   scale   of   deep   learning   over   the   past   decade   \nhas   created   the   opportunity   to   build   customized   hardware   that   is   tailored   to   the   kinds   of   computations   involved   \nin   training   and   serving   DNN   models.", "metadata": {}}, {"text": "Instead   of   using   GPUs   like   many   other   organizations,   over   the   past   seven   \nyears   Google   has   designed,   built,   and   deployed   four   generations   of   custom   Tensor   Processing   Unit   (TPU)   \nhardware   for   DNNs   to   accelerate   model   training   and   serving   [Jou21].", "metadata": {}}, {"text": "To   get   a   better   return   on   their   investment,   \ncloud   companies   actually   aim   for   improved    cost-performance,    as   opposed   to   simply   performance.", "metadata": {}}, {"text": "Cost   here   \nmeans    Total   Cost   of   Ownership    ( TCO ),   which   includes   the   annual   operating   costs   such   as   electricity   consumed   \nand   amortization   of   capital   expenditures   for   the   computer,   cooling,   power   distribution,   and   the   building.", "metadata": {}}, {"text": "Jouppi   \net   al .", "metadata": {}}, {"text": "show   that   power   consumption   is   nearly   perfectly   linearly   correlated   with   TCO 8    [Jou21],   so   \nperformance/TCO   gains   also   help   performance/Watt,   saving   money   and   reducing   CO 2 e.", "metadata": {}}, {"text": "2.3   Datacenter   improvement   \nA   useful   quantitative   metric   of   datacenter   efficiency   is   the   energy   overhead   above   and   beyond   what   directly   \npowers   the   computing   equipment   inside   the   datacenters.", "metadata": {}}, {"text": "If   the   overhead   were   50%,   the    Power   Usage   \nEffectiveness    ( PUE )   would   be   1.50.", "metadata": {}}, {"text": "The   US   national   datacenter   average   in   2018   was   1.58,   which   is   the   value   \n[Str19]     used ;", "metadata": {}}, {"text": "In   2020,   it   was   1.59 .", "metadata": {}}, {"text": "Google    publishes   its   datacenter   PUE   online   every   quarter .", "metadata": {}}, {"text": "The   PUE   for   the   \nIowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,   a   factor   of   1.4X   better.", "metadata": {}}, {"text": "Cloud   datacenters   are   \nroughly   2X   as   energy   efficient   as   a   typical   enterprise   datacenter   due   to   other   factors   like   server   utilization   (see  \n[Höl20]),   but   we’ll   limit   the   quantitative   improvement   in   this   paper   to   the   easy-to-measure   PUE.", "metadata": {}}, {"text": "More   broadly,   since   cloud   datacenters   are   much   more   energy   efficient,   the   long-feared   explosion   of   \ndatacenter   energy   usage   has   not   materialized.", "metadata": {}}, {"text": "A   recent   paper   in    Science    [Mas20]   found   that   global   datacenter   \nenergy   consumption   increased   by   only   6%   compared   with   2010,   despite   computing   capacity   increasing   by   \n550%   over   the   same   time   period   [Mas21].", "metadata": {}}, {"text": "Business   Rationale .", "metadata": {}}, {"text": "Cloud   companies   strive   for   energy   efficient   datacenters   since   it   saves   money   and   \nlowers   emissions.", "metadata": {}}, {"text": "Perhaps   we   should   add   “energy   is   money”   to   Ben   Franklin’s   “time   is   money”   advice?", "metadata": {}}, {"text": "2.4   Energy   mix   improvement   \n  The   gross   carbon   intensity   of   energy   according   to   the   U.S.", "metadata": {}}, {"text": "average   mix   is    0.429    kg   of   CO 2 e/KWh   [USE21].", "metadata": {}}, {"text": "After   matching   Google’s   clean   energy   purchase   per   its   24/7   carbon-free   energy   framework   (see   Appendix   B),   \nthe   net   CO 2 e   drops   to   0.080   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer,   which   is   5.4X   better.", "metadata": {}}, {"text": "Business   Rationale .", "metadata": {}}, {"text": "Transmitting   electricity   long   distances   is   more   expensive   and   less   efficient   than   \nsending   information   as   photons   over   optical   fibers   [Arm10].", "metadata": {}}, {"text": "Cloud   computing   allows   companies   like   Google   to   \nhave   a   global   portfolio   of   datacenters,   many   of   which   are   placed   where   the   grid   is   cleaner   (e.g.,   Finland)   or   \nwhere   companies   can   purchase   clean   energy   directly   (e.g.,   Iowa).", "metadata": {}}, {"text": "In   2020   Google   announced   a   new   objective   \nin   its   energy   strategy:   by   2030,   it   aims   to   run   all   Google   datacenters   and   offices   on   carbon-free   energy   24/7.", "metadata": {}}, {"text": "For   our   24/7   carbon-free   energy   accounting    (see   Appendix   B),   we   deduct   from   the   hourly   consumption   all   \n6  Their   neural   architecture   search   also   found   another   version   that   had   the   same   performance   but   better   accuracy.", "metadata": {}}, {"text": "7  [Str19]   used   P100s,   which   are   contemporary   GPUs   to   TPU   v2s.", "metadata": {}}, {"text": "8  The   correlation   coefficient   R   between   TCO   and   TDP   is   0.99   out   of   1.00   across   four   generations   of   TPUs.", "metadata": {}}, {"text": "4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "clean   energy   purchased   on   that   same   geographically   local   grid   and   the   same   hour,   which   results   in   the   net   \nCO 2 e/KWh   value.   As   Iowa   has   strong   nighttime   winds,   Google’s   wind   portfolio   lowered   Iowa's   datacenter    gross   \naverage   CO 2 e/KWh   in   December   2020   by   6X,   from   the   local   grid’s   0.478   kg   to   a    net    average   of   0.080   kg.     \n2.5   Summary:   Formulas   for   energy   consumption   and   carbon   footprint   of   training   \nReducing   CO 2 e   is   not   only   a   moral   obligation   but   ultimately   sound   business.   To   decrease   the   footprint   of   \ntraining,   an   ML   researcher   should   pick   the   DNN   model,   the   processor,   and   the   datacenter   carefully. 9    Cutting   \nenergy   saves   money   and   CO 2 e   and   improving   the   energy   mix   reduces   CO 2 e.   We   refactor   the   equation   above   \nfor   training   into   energy   consumption   and   its   carbon   footprint   (tCO 2 e     means   metric   tons   of   CO 2 e):   \n ours to train  umber of Processors verage Power per Processor UE  KWh  =H ×N ×A ×P ÷1000   \n CO2e Wh g CO2e per KWh 000 t =K ×k ÷1  \nWe   believe   it   is   straightforward   for   ML   practitioners   to   calculate   energy   consumption.   They   already   know   \nhours   to   train   and   number   of   processors.   Google   and   Facebook   publish   PUE   of   their   datacenters,   so   that   is   \neasy   to   look   up   for   those   clouds.   If   cloud   providers   don’t   share   PUE,   use   the   US   average   PUE   as   in   [Str19].   \nWe   measured   the   power   of   the   processors   during   training,   which   is   ideal,   but   using   the   average   of   the   training   \nof   several   similar   models   is   probably   sufficient   and   much   easier. 10    Table   3   shows   the   average   power   and   \nstandard   deviation   for   the   processors   and   DNNs   that   we   measured   in   this   paper.     \nThe   final   piece   is   the   CO 2 e   of   the   datacenter   at   the   time   the   model   was   run.   Google   calculates   the   average   \nper   month,   which   is   close   enough,   and   it   is   now   available   for   Google   employees   to   look   up.   Without   access   to   \nsuch   a   dashboard,   use   the   ML   Emissions   Calculator   [Lac19]   or   Green   Algorithms   tool   [Lan20]   that   estimate   the   \nCO 2 e   mix   by   region   (see   Figure   6   below) 11 .   While   not   absolutely   necessary,   we   hope   the   ML   community   will   \nlobby   all   cloud   providers   to   reveal   the   actual   energy   mix,   since   it   can   vary   within   a   region.    For   example,    to   let   \ncustomers   pick   the   datacenter   based   on   CO 2 e,    Google   Cloud   recently   released   the   percentage   of   carbon-free   \nenergy   and   gross   CO 2 e   of   its   datacenters    and   committed   to   publishing   updated   figures   going   forward.   \nWe   next   show   the   impact   of   these   three   choices   on   much   larger   NLP   models.   \nTable   3.   Average   system   power   per   processor   and   standard   deviation   for   DNNs   in   this   paper.   We   \nmeasured   the   Google   DNNs   (see   Tables   1   and   4).    OpenAI   measured   GPT-3   in   a   Microsoft   Azure   \ndatacenter   [Sut21].     \n3.Energy   Usage   and   CO 2 e   Emissions    of   Five   Recent   Large   NLP   Models   \nA   natural   question   that   follows   is   what   about   the   training   CO 2 e   of   much   larger   NLP   models?   Table   4   and   \nAppendix   A   show   a   CO 2 e   calculation 11    for   five   of   them:   T5,   Meena,   GShard,   and   Switch   Transformer   from   \nGoogle   plus   GPT-3   from   Open   AI   that   runs   on   Microsoft   Azure   Cloud:  \n● T5    is   a   pre-trained   language   model   that   casts   all   NLP   problems   in   a   unified   text-to-text   format   to   enable   \napplication   of   transfer   learning   techniques   to   reduce   the   cost   of   training   [Raf19].   The   largest   size   has   \n11B   parameters,   and   training   used   86   MWh   and   produced   47   tCO 2 e.     \n● Meena    is   a   multi-turn   open-domain   chatbot   [Adi20].   This   2.6B   parameter   DNN   is   trained   to   minimize   \nperplexity   of   the   next   token.   The   year-old   companion   paper   has   ~150   citations.   Training   Meena   used   \n9  PUE   and   kg   CO 2 e   per   KWh   are   functions   of   the   datacenter   where   the   model   is   run.   \n10  The   ML   Emissions   Calculator   [Lac19]   also   estimates   power   per   processor.   It   now   uses   the   values   in   Table   3   for   TPU   v2   \nand   TPU   v3   [Luc21].   At   the   time   of   this   writing,   the   calculator   shows   CO 2 e   produced   but   not   the   estimated   power   per   \nprocessor,   energy   consumed,   or   CO 2 e/KWh.   \n11   The   Google   models   happen   to   be   run   in   datacenters   where   the   gross   and   net   CO 2 e   were   the   same   or   close.   \n5   \nProcessor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power   \nTPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nTPU   v3   283   10%   T5,   Meena,   Gshard,   Switch   Transformer   \nP100   GPU   271   11%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nV100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]", "sentences": [{"text": "clean   energy   purchased   on   that   same   geographically   local   grid   and   the   same   hour,   which   results   in   the   net   \nCO 2 e/KWh   value.", "metadata": {}}, {"text": "As   Iowa   has   strong   nighttime   winds,   Google’s   wind   portfolio   lowered   Iowa's   datacenter    gross   \naverage   CO 2 e/KWh   in   December   2020   by   6X,   from   the   local   grid’s   0.478   kg   to   a    net    average   of   0.080   kg.", "metadata": {}}, {"text": "2.5   Summary:   Formulas   for   energy   consumption   and   carbon   footprint   of   training   \nReducing   CO 2 e   is   not   only   a   moral   obligation   but   ultimately   sound   business.", "metadata": {}}, {"text": "To   decrease   the   footprint   of   \ntraining,   an   ML   researcher   should   pick   the   DNN   model,   the   processor,   and   the   datacenter   carefully.", "metadata": {}}, {"text": "9    Cutting   \nenergy   saves   money   and   CO 2 e   and   improving   the   energy   mix   reduces   CO 2 e.", "metadata": {}}, {"text": "We   refactor   the   equation   above   \nfor   training   into   energy   consumption   and   its   carbon   footprint   (tCO 2 e     means   metric   tons   of   CO 2 e):   \n ours to train  umber of Processors verage Power per Processor UE  KWh  =H ×N ×A ×P ÷1000   \n CO2e Wh g CO2e per KWh 000 t =K ×k ÷1  \nWe   believe   it   is   straightforward   for   ML   practitioners   to   calculate   energy   consumption.", "metadata": {}}, {"text": "They   already   know   \nhours   to   train   and   number   of   processors.", "metadata": {}}, {"text": "Google   and   Facebook   publish   PUE   of   their   datacenters,   so   that   is   \neasy   to   look   up   for   those   clouds.", "metadata": {}}, {"text": "If   cloud   providers   don’t   share   PUE,   use   the   US   average   PUE   as   in   [Str19].", "metadata": {}}, {"text": "We   measured   the   power   of   the   processors   during   training,   which   is   ideal,   but   using   the   average   of   the   training   \nof   several   similar   models   is   probably   sufficient   and   much   easier.", "metadata": {}}, {"text": "10    Table   3   shows   the   average   power   and   \nstandard   deviation   for   the   processors   and   DNNs   that   we   measured   in   this   paper.", "metadata": {}}, {"text": "The   final   piece   is   the   CO 2 e   of   the   datacenter   at   the   time   the   model   was   run.", "metadata": {}}, {"text": "Google   calculates   the   average   \nper   month,   which   is   close   enough,   and   it   is   now   available   for   Google   employees   to   look   up.", "metadata": {}}, {"text": "Without   access   to   \nsuch   a   dashboard,   use   the   ML   Emissions   Calculator   [Lac19]   or   Green   Algorithms   tool   [Lan20]   that   estimate   the   \nCO 2 e   mix   by   region   (see   Figure   6   below) 11 .", "metadata": {}}, {"text": "While   not   absolutely   necessary,   we   hope   the   ML   community   will   \nlobby   all   cloud   providers   to   reveal   the   actual   energy   mix,   since   it   can   vary   within   a   region.", "metadata": {}}, {"text": "For   example,    to   let   \ncustomers   pick   the   datacenter   based   on   CO 2 e,    Google   Cloud   recently   released   the   percentage   of   carbon-free   \nenergy   and   gross   CO 2 e   of   its   datacenters    and   committed   to   publishing   updated   figures   going   forward.", "metadata": {}}, {"text": "We   next   show   the   impact   of   these   three   choices   on   much   larger   NLP   models.", "metadata": {}}, {"text": "Table   3.", "metadata": {}}, {"text": "Average   system   power   per   processor   and   standard   deviation   for   DNNs   in   this   paper.", "metadata": {}}, {"text": "We   \nmeasured   the   Google   DNNs   (see   Tables   1   and   4).", "metadata": {}}, {"text": "OpenAI   measured   GPT-3   in   a   Microsoft   Azure   \ndatacenter   [Sut21].", "metadata": {}}, {"text": "3.Energy   Usage   and   CO 2 e   Emissions    of   Five   Recent   Large   NLP   Models   \nA   natural   question   that   follows   is   what   about   the   training   CO 2 e   of   much   larger   NLP   models?", "metadata": {}}, {"text": "Table   4   and   \nAppendix   A   show   a   CO 2 e   calculation 11    for   five   of   them:   T5,   Meena,   GShard,   and   Switch   Transformer   from   \nGoogle   plus   GPT-3   from   Open   AI   that   runs   on   Microsoft   Azure   Cloud:  \n● T5    is   a   pre-trained   language   model   that   casts   all   NLP   problems   in   a   unified   text-to-text   format   to   enable   \napplication   of   transfer   learning   techniques   to   reduce   the   cost   of   training   [Raf19].", "metadata": {}}, {"text": "The   largest   size   has   \n11B   parameters,   and   training   used   86   MWh   and   produced   47   tCO 2 e.", "metadata": {}}, {"text": "● Meena    is   a   multi-turn   open-domain   chatbot   [Adi20].", "metadata": {}}, {"text": "This   2.6B   parameter   DNN   is   trained   to   minimize   \nperplexity   of   the   next   token.", "metadata": {}}, {"text": "The   year-old   companion   paper   has   ~150   citations.", "metadata": {}}, {"text": "Training   Meena   used   \n9  PUE   and   kg   CO 2 e   per   KWh   are   functions   of   the   datacenter   where   the   model   is   run.", "metadata": {}}, {"text": "10  The   ML   Emissions   Calculator   [Lac19]   also   estimates   power   per   processor.", "metadata": {}}, {"text": "It   now   uses   the   values   in   Table   3   for   TPU   v2   \nand   TPU   v3   [Luc21].", "metadata": {}}, {"text": "At   the   time   of   this   writing,   the   calculator   shows   CO 2 e   produced   but   not   the   estimated   power   per   \nprocessor,   energy   consumed,   or   CO 2 e/KWh.", "metadata": {}}, {"text": "11   The   Google   models   happen   to   be   run   in   datacenters   where   the   gross   and   net   CO 2 e   were   the   same   or   close.", "metadata": {}}, {"text": "5   \nProcessor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power   \nTPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nTPU   v3   283   10%   T5,   Meena,   Gshard,   Switch   Transformer   \nP100   GPU   271   11%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   \nSearch   [So19]   \nV100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "232   MWh   and   emissions   was   96   tCO 2 e.   As   Evolved   Transformer   saved   48   tCO 2 e   alone   for   the   single   \nuse   case   of   developing   Meena   (see   Table   4),   the   3.2   net   tCO 2 e   cost   for   its   development   returned   15:1.   \n● GShard    is   composed   of   a   set   of   lightweight   annotation   APIs   that   provide   an   elegant   way   to   express   a   \nwide   range   of   parallel   computation   patterns   with   minimal   changes   to   the   existing   model   code   [Lep20].   It   \nenabled   scaling   up   of   a   multilingual   neural   machine   translation   Transformer   model   with   sparsely   gated   \nmixture-of-experts   (MoE)   [Sha17]   using   automatic   sharding.   The   GShard-600B   model   is   a   particular   \nuse   of   that   framework   for   training   a   multi-lingual   translation   model   with   600B   total   parameters.   Sparse   \nmodels   can   have   many   model   parameters   while   requiring   much   less   computation   than   dense   models.   \nTraining   GShard-600B   used   24   MWh   and   produced   4.3   net   tCO 2 e.   \n● Switch   Transformer    simplifies   the   Mixture   of   Expert   (MoE)   routing   algorithm   to   design   intuitive   improved   \nmodels   with   reduced   communication   and   computational   costs   [Fed21].   The   authors   show   large   sparse   \nmodels—1500B   parameters   but   only   0.1%   activated   per   token—can   deliver   up   to   7x   increases   in   \npre-training   speed   with   the   same   computational   resources.   We   estimated   it   used   179   MWh   and   \nproduced   59   net   tCO 2 e.   \nTable   4.   CO 2 e   for   NLP   models   (see   Appendix   A) 12 .   V100’s   TDP   is   closer   to   average   power   due   to    Turbo   \nmode    and    DVFS .   TPUs   don’t   offer   them,   so   their   TDP   is   much   higher   than   their   average   power.     \n12  The   peak   TeraFLOPS/second   is   46   for   TPU   v2,   123   for   TPU   v3,   and   125   for   V100.   \n6   \nModel   \nEvolved   \nTrans-   \nformer   \nNAS   \nT5   Meena   Gshard   \n-600B   \nSwitch   \nTrans-   \nformer   \nGPT-3   \nNumber   of   Parameters   (B)   0.064   per  \nmodel  11  2.6  619  1500  175  \nPercent   of   model   activated   on   every   token   100%  100%  100%  0.25%  0.10%  100%  \nDeveloper   Google   OpenAI   \nDatacenter   of   original   experiment   Google   \nGeorgia   \nGoogle   \nTaiwan   \nGoogle   \nGeorgia   \nGoogle   \nNorth   \nCarolina   \nGoogle   \nGeorgia   Microsoft   \nWhen   model   ran   Dec   2018  Sep   2019  Dec   2019  Apr   2020  Oct   2020  2020  \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.201  0.403  0.429  \nDatacenter   Net   CO2e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.177  0.330  0.429  \nDatacenter   PUE   (when   it   was   run)   1.10  1.12  1.09  1.09  1.10  1.10  \nProcessor   TPU   v2  TPU   v3   V100  \nChip   Thermal   Design   Power   (TDP   in   Watts)   280  450   300  \nMeasured   System   Average   Power   per   Accelerator,   \nincluding   memory,   network   interface,   fans,   host   CPU   (W)  208  310  289  288  245  330  \nMeasured   Performance   (TFLOPS/s) 12   24.8  45.6  42.3  48.0  34.4  24.6  \nNumber   of   Chips   200  512  1024  1024  1024  10,000  \nTraining   time   (days)   6.8  20  30  3.1  27  14.8  \nTotal   Computation   (floating   point   operations)   2.91E+21  4.05E+22  1.12E+23  1.33E+22  8.22E+22  3.14E+23  \nEnergy   Consumption   (MWh)   7.5  85.7  232  24.1  179  1,287  \n%   of   Google   2019   total   energy   consumption   (12.2   TWh  \n=   12,200,000   MWh)   [Goo20]   0.00006%  0.00070%  0.00190%  0.00020%  0.00147%  0.01055%  \nGross   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.8  72.2  552.1  \nNet   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.3  59.1  552.1  \nFraction   of   NAS   Estimate   in   [Str19]   (284   tCO2e)   0.011  0.164  0.340  0.015  0.208  1.944  \nFraction   of   equivalent   jet   plane   CO 2 e   round   trip   San   \nFrancisco   ↔   New   York   (~180   t;   see   Ap.   A)   0.018  0.258  0.533  0.024  0.327  3.054  \ntCO 2 e   savings   by   Meena   using   Evolved   Transformer   --  --  48.5  --  --  --  \n%   24/x7   carbon   free   energy   (when   run)   31%  19%  30%  73%  43%  N/A", "sentences": [{"text": "232   MWh   and   emissions   was   96   tCO 2 e.", "metadata": {}}, {"text": "As   Evolved   Transformer   saved   48   tCO 2 e   alone   for   the   single   \nuse   case   of   developing   Meena   (see   Table   4),   the   3.2   net   tCO 2 e   cost   for   its   development   returned   15:1.", "metadata": {}}, {"text": "● GShard    is   composed   of   a   set   of   lightweight   annotation   APIs   that   provide   an   elegant   way   to   express   a   \nwide   range   of   parallel   computation   patterns   with   minimal   changes   to   the   existing   model   code   [Lep20].", "metadata": {}}, {"text": "It   \nenabled   scaling   up   of   a   multilingual   neural   machine   translation   Transformer   model   with   sparsely   gated   \nmixture-of-experts   (MoE)   [Sha17]   using   automatic   sharding.", "metadata": {}}, {"text": "The   GShard-600B   model   is   a   particular   \nuse   of   that   framework   for   training   a   multi-lingual   translation   model   with   600B   total   parameters.", "metadata": {}}, {"text": "Sparse   \nmodels   can   have   many   model   parameters   while   requiring   much   less   computation   than   dense   models.", "metadata": {}}, {"text": "Training   GShard-600B   used   24   MWh   and   produced   4.3   net   tCO 2 e.", "metadata": {}}, {"text": "● Switch   Transformer    simplifies   the   Mixture   of   Expert   (MoE)   routing   algorithm   to   design   intuitive   improved   \nmodels   with   reduced   communication   and   computational   costs   [Fed21].", "metadata": {}}, {"text": "The   authors   show   large   sparse   \nmodels—1500B   parameters   but   only   0.1%   activated   per   token—can   deliver   up   to   7x   increases   in   \npre-training   speed   with   the   same   computational   resources.", "metadata": {}}, {"text": "We   estimated   it   used   179   MWh   and   \nproduced   59   net   tCO 2 e.", "metadata": {}}, {"text": "Table   4.", "metadata": {}}, {"text": "CO 2 e   for   NLP   models   (see   Appendix   A) 12 .", "metadata": {}}, {"text": "V100’s   TDP   is   closer   to   average   power   due   to    Turbo   \nmode    and    DVFS .", "metadata": {}}, {"text": "TPUs   don’t   offer   them,   so   their   TDP   is   much   higher   than   their   average   power.", "metadata": {}}, {"text": "12  The   peak   TeraFLOPS/second   is   46   for   TPU   v2,   123   for   TPU   v3,   and   125   for   V100.", "metadata": {}}, {"text": "6   \nModel   \nEvolved   \nTrans-   \nformer   \nNAS   \nT5   Meena   Gshard   \n-600B   \nSwitch   \nTrans-   \nformer   \nGPT-3   \nNumber   of   Parameters   (B)   0.064   per  \nmodel  11  2.6  619  1500  175  \nPercent   of   model   activated   on   every   token   100%  100%  100%  0.25%  0.10%  100%  \nDeveloper   Google   OpenAI   \nDatacenter   of   original   experiment   Google   \nGeorgia   \nGoogle   \nTaiwan   \nGoogle   \nGeorgia   \nGoogle   \nNorth   \nCarolina   \nGoogle   \nGeorgia   Microsoft   \nWhen   model   ran   Dec   2018  Sep   2019  Dec   2019  Apr   2020  Oct   2020  2020  \nDatacenter   Gross   CO 2 e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.201  0.403  0.429  \nDatacenter   Net   CO2e/KWh   (kg/KWh   when   it   was   run)   0.431  0.545  0.415  0.177  0.330  0.429  \nDatacenter   PUE   (when   it   was   run)   1.10  1.12  1.09  1.09  1.10  1.10  \nProcessor   TPU   v2  TPU   v3   V100  \nChip   Thermal   Design   Power   (TDP   in   Watts)   280  450   300  \nMeasured   System   Average   Power   per   Accelerator,   \nincluding   memory,   network   interface,   fans,   host   CPU   (W)  208  310  289  288  245  330  \nMeasured   Performance   (TFLOPS/s) 12   24.8  45.6  42.3  48.0  34.4  24.6  \nNumber   of   Chips   200  512  1024  1024  1024  10,000  \nTraining   time   (days)   6.8  20  30  3.1  27  14.8  \nTotal   Computation   (floating   point   operations)   2.91E+21  4.05E+22  1.12E+23  1.33E+22  8.22E+22  3.14E+23  \nEnergy   Consumption   (MWh)   7.5  85.7  232  24.1  179  1,287  \n%   of   Google   2019   total   energy   consumption   (12.2   TWh  \n=   12,200,000   MWh)   [Goo20]   0.00006%  0.00070%  0.00190%  0.00020%  0.00147%  0.01055%  \nGross   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.8  72.2  552.1  \nNet   tCO 2 e   for   Model   Training   3.2  46.7  96.4  4.3  59.1  552.1  \nFraction   of   NAS   Estimate   in   [Str19]   (284   tCO2e)   0.011  0.164  0.340  0.015  0.208  1.944  \nFraction   of   equivalent   jet   plane   CO 2 e   round   trip   San   \nFrancisco   ↔   New   York   (~180   t;", "metadata": {}}, {"text": "see   Ap.", "metadata": {}}, {"text": "A)   0.018  0.258  0.533  0.024  0.327  3.054  \ntCO 2 e   savings   by   Meena   using   Evolved   Transformer   --  --  48.5  --  --  --  \n%   24/x7   carbon   free   energy   (when   run)   31%  19%  30%  73%  43%  N/A", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "● GPT-3    is   an   autoregressive   language   model   with   175B   parameters,   10x   more   than   any   non-sparse   \nlanguage   model   at   the   time   [Bro20].   It   achieves   strong   performance   on   many   NLP   datasets.   A   winner   of   \nthe   best   paper   award   at   NeurIPS   2020,   this   8-month-old   paper   already   has   ~700   citations   and    made   \nmainstream   media   headlines . 13    It   is   now   available   for   commercial   use.   One   potential   energy   benefit   of   a   \nlarge   language   model   like   GPT-3   is   that   they   exhibit    few-shot   generalization ,   which   means   that   they   \ndon’t   need   to   be   retrained   for   every   new   task   like   smaller   models   [Wan20].    Its   estimated   carbon   \nemissions   due   to   training   are   552   tCO 2 e   and   its   energy   consumption   is   1287   MWh. 14   \nTable   4   also   lists   the   neural   architecture   search   for   Evolved   Transformer,   discussed   shortly.", "sentences": [{"text": "● GPT-3    is   an   autoregressive   language   model   with   175B   parameters,   10x   more   than   any   non-sparse   \nlanguage   model   at   the   time   [Bro20].", "metadata": {}}, {"text": "It   achieves   strong   performance   on   many   NLP   datasets.", "metadata": {}}, {"text": "A   winner   of   \nthe   best   paper   award   at   NeurIPS   2020,   this   8-month-old   paper   already   has   ~700   citations   and    made   \nmainstream   media   headlines .", "metadata": {}}, {"text": "13    It   is   now   available   for   commercial   use.", "metadata": {}}, {"text": "One   potential   energy   benefit   of   a   \nlarge   language   model   like   GPT-3   is   that   they   exhibit    few-shot   generalization ,   which   means   that   they   \ndon’t   need   to   be   retrained   for   every   new   task   like   smaller   models   [Wan20].", "metadata": {}}, {"text": "Its   estimated   carbon   \nemissions   due   to   training   are   552   tCO 2 e   and   its   energy   consumption   is   1287   MWh.", "metadata": {}}, {"text": "14   \nTable   4   also   lists   the   neural   architecture   search   for   Evolved   Transformer,   discussed   shortly.", "metadata": {}}], "metadata": {"page": 7}}, {"text": "Figure   2.   Total   FLOPS   versus   number   of   parameters   relative   to   Transformer   (Big)   in   a   log-log   graph   \n(Table   1).   While   all   are   not   doing   the   same   tasks,   a   reason   T5   has   relatively   lower   FLOPS   relative   to   its   \nnumber   of   parameters   is   that   it   trains   until   the   accuracy   is   good   enough   instead   of   to   the   best   possible   \naccuracy.   [Kap20]   notes   that   some   architectures   have   a   much   lower   footprint   than   others   at   equivalent   \naccuracy   and   suggests   that   significant   power   might   be   saved   by   revisiting   accuracy   requirements.", "sentences": [{"text": "Figure   2.", "metadata": {}}, {"text": "Total   FLOPS   versus   number   of   parameters   relative   to   Transformer   (Big)   in   a   log-log   graph   \n(Table   1).", "metadata": {}}, {"text": "While   all   are   not   doing   the   same   tasks,   a   reason   T5   has   relatively   lower   FLOPS   relative   to   its   \nnumber   of   parameters   is   that   it   trains   until   the   accuracy   is   good   enough   instead   of   to   the   best   possible   \naccuracy.", "metadata": {}}, {"text": "[Kap20]   notes   that   some   architectures   have   a   much   lower   footprint   than   others   at   equivalent   \naccuracy   and   suggests   that   significant   power   might   be   saved   by   revisiting   accuracy   requirements.", "metadata": {}}], "metadata": {"page": 7}}, {"text": "Figure   3.   Accelerator   years   of   computation,   energy   consumption,   and   CO 2 e   for   five   large   NLP   DNNs.   \n13  Metz,   C.,   Meet   GPT-3.   It   Has   Learned   to   Code   (and   Blog   and   Argue),   November   24,   2020,    New   York   Times .   \n14  We   measured   all   the   data   for   Google   models.   OpenAI   measured   V100   performance,   V100   power,   total   FLOPS,   and   \nPUE   for   GPT-3.   We   used   the   US   average   CO 2 e/KWh   for   GPT-3   at   Microsoft   Azure   (see   Appendix   A).   \n7", "sentences": [{"text": "Figure   3.", "metadata": {}}, {"text": "Accelerator   years   of   computation,   energy   consumption,   and   CO 2 e   for   five   large   NLP   DNNs.", "metadata": {}}, {"text": "13  Metz,   C.,   Meet   GPT-3.", "metadata": {}}, {"text": "It   Has   Learned   to   Code   (and   Blog   and   Argue),   November   24,   2020,    New   York   Times .", "metadata": {}}, {"text": "14  We   measured   all   the   data   for   Google   models.", "metadata": {}}, {"text": "OpenAI   measured   V100   performance,   V100   power,   total   FLOPS,   and   \nPUE   for   GPT-3.", "metadata": {}}, {"text": "We   used   the   US   average   CO 2 e/KWh   for   GPT-3   at   Microsoft   Azure   (see   Appendix   A).", "metadata": {}}, {"text": "7", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=X66.png] Size: 1200x958, Data: 52353 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=X66.png] Size: 1200x958, Data: 52353 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "X66.png", "image_width": 1200, "image_height": 958, "attachment_type": "image", "has_image_data": true, "image_data_size": 52353}}, {"text": "[Image page=7 idx=2 name=X67.png] Size: 1260x850, Data: 48697 bytes", "sentences": [{"text": "[Image page=7 idx=2 name=X67.png] Size: 1260x850, Data: 48697 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 2, "image_name": "X67.png", "image_width": 1260, "image_height": 850, "attachment_type": "image", "has_image_data": true, "image_data_size": 48697}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "Figures   2   and   3   present   the   same   data   graphically.   Figure   2   plots   the   number   of   parameters   on   the   X   axis   \nand   number   of   total   FLOPS   on   the   Y   axis   relative   to   Transformer   (Big)   [So19]   using   a   log-log   graph.   Sparsely  \nactivated   models   use   many   more   parameters   with   much   lower   total   FLOPS.   Since   performance   is   not   \nnecessarily   linear   in   FLOPS   (see   [Li21]),   Figure   3   shows   computation   in   processor   years   along   with   their   \nenergy   consumption   and   carbon   footprint.   Compared   to   the   dense   GPT-3,   sparsely   activated   Gshard   needs  \n~45X   fewer   processor   years,   uses   ~55X   less   energy,   and   reduces   gross   CO 2 e   ~115X   and   net   CO 2 e   ~130X.     \n4.   Discussion   \nIn   this   section,   we   address   the   additional   factors   relating   to   carbon   emissions   due   to   training   NLP   models.   We   \nstart   by   revisiting   the   estimate   of   neural   architecture   search   in   [Str19]   and   end   with   example   benefits   of   some   \nNLP   models.   \n4.1   Estimating   the   cost   of   neural   architecture   search   (NAS)   \nThe   Evolved   Transformer   neural   architecture   search   (NAS)   was   used   as   an   example   of   an   expensive   NLP   \nmodel   [Str19].   Although   it   is   now   surpassed   by   other   models   in   terms   of   training   cost   (Table   4),   we   discuss   it   \nhere   as   a   concrete   example   of   the   complexity   of   estimating   the   cost   of   a   ML   method   retroactively.   \nAs   Table   4   shows,   the   actual   cost   of   Evolved   Transformer   NAS   is   nearly   two   orders   of   magnitude   smaller   \nthan   previously   estimated   [Str19].   Why   the   discrepancy?   The   answer   is   that,   in   addition   to   the   efficiency   of   \nGoogle   datacenters,   there   was   a   confusion   in   estimating   the   energy   cost   of   NAS.   In   Evolved   Transformer   NAS,   \nresearchers   used   a   small    proxy   task    to   search   for   the   best   models   to   save   time   and   money,   and   then   scaled   up   \nthe   found   models   to   full   size.   Small   proxies   may   not   be   obvious,   which   made   it   hard   to   estimate   the   CO 2 e   \ncorrectly   in   retrospect   from   the   NAS   paper   [So19].   Due   to   the   misunderstanding   of   the   usage   of   proxy   tasks   in   \nNAS,    it   was    assumed   the   search   was   done   with   full   size   tasks .   Because   of   this   assumption,   despite   \nconsiderable   effort   on   their   part,   Strubell    et   al. ’s   energy   estimate   for   NAS   ended   up   18.7X   too   high   for   the   \naverage   organization   (see   Appendix   C)   and   88X   off   in   emissions   for   energy-efficient   organizations   like   Google   \n(see   Appendix   D).   This   example   led   us   to   our   first   recommendation—that   more   researchers   measure   energy   \nusage   and   CO 2 e   for   computationally   intensive   projects,   and   report   them   when   practical,   rather   than   counting   \non   others   to   estimate   it   retrospectively.   \n  Another   confusion   in   the   general   public   is   the   misperception   that   NAS   (and   therefore,   the   cost   associated   \nwith   NAS)   is   conducted   once   per   model   training.   In   practice,   however,    NAS   is   generally   not   performed   once   \nper   model   training,   but   once   per    problem     domain+architectural   search   space   combination .   For   example,   the   \nEvolved   Transformer,   found   by   NAS   on   translation,   can   be   used   for   language   modeling   without   a   new   search   \n[So19,   Adi20].   Unfortunately,    results   in   the   earlier   work   by   [Str19]   characterizing   NAS   were   misattributed   to   \nsingle   model   training   costs   in   the   popular   press.     \nAs   an   analogy,   NAS   is   like   optimizing   the   energy   efficiency   and   cost   of   an   LED   light   bulb   with   extensive   \nsimulations   on   a   supercomputer,   training   a   model   is   akin   to   building   LED   light   bulbs,   and   inference   is   \nanalogous   to   all   the   customers   using   LEDs   to   light   their   homes.   The   analogous   confusion   would   be   claiming   \nthat   the   one-time   upfront   supercomputer   simulation   cost   should   be   included   in   the   CO 2 e   cost   of   every   light   bulb   \nmanufactured.   In   this   analogy,   the   onetime   CO 2    expenditure   of   the   supercomputer   simulations   can   be   more   \nthan   paid   back   with   the   improved   energy-efficiency   of   the   mass-produced   light   bulbs,   as   was   the   case   for   the   \nactual   NAS   of   [So19]   (see   next   paragraph).     \nIn   terms   of   cost-benefit   tradeoff ,   NAS   can   also   lead   to   improved   energy   efficiency   in   training   of   downstream   \napplications,   and   the   benefit   can   dramatically   outweigh   the   cost.    Figure   4   shows   that   the   Evolved   Transformer,   \nfound   by   NAS   [So19],   has   37%   fewer   parameters   and   converges   to   the   same   accuracy   with   25%   less   energy   \nexpenditure   (see   Table   1)   than   the   vanilla   Transformer   (Big)   model   on   WMT   English   to   German   translation.   \nThe   use   of   Evolved   Transformer   instead   of   a   regular   Transformer   architecture   saved   48.5   t CO 2 e   during   the   \ntraining   of   the   Meena   DNN   (see   Tables   1   and   4).   The   savings   from   this   single   reuse   in   Meena   are   ~15X   larger   \nthan   the   energy   cost   of   running   the   search   to   discover   it.   The   results   of   the   Evolved   Transformer   neural   \n8", "sentences": [{"text": "Figures   2   and   3   present   the   same   data   graphically.", "metadata": {}}, {"text": "Figure   2   plots   the   number   of   parameters   on   the   X   axis   \nand   number   of   total   FLOPS   on   the   Y   axis   relative   to   Transformer   (Big)   [So19]   using   a   log-log   graph.", "metadata": {}}, {"text": "Sparsely  \nactivated   models   use   many   more   parameters   with   much   lower   total   FLOPS.", "metadata": {}}, {"text": "Since   performance   is   not   \nnecessarily   linear   in   FLOPS   (see   [Li21]),   Figure   3   shows   computation   in   processor   years   along   with   their   \nenergy   consumption   and   carbon   footprint.", "metadata": {}}, {"text": "Compared   to   the   dense   GPT-3,   sparsely   activated   Gshard   needs  \n~45X   fewer   processor   years,   uses   ~55X   less   energy,   and   reduces   gross   CO 2 e   ~115X   and   net   CO 2 e   ~130X.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "Discussion   \nIn   this   section,   we   address   the   additional   factors   relating   to   carbon   emissions   due   to   training   NLP   models.", "metadata": {}}, {"text": "We   \nstart   by   revisiting   the   estimate   of   neural   architecture   search   in   [Str19]   and   end   with   example   benefits   of   some   \nNLP   models.", "metadata": {}}, {"text": "4.1   Estimating   the   cost   of   neural   architecture   search   (NAS)   \nThe   Evolved   Transformer   neural   architecture   search   (NAS)   was   used   as   an   example   of   an   expensive   NLP   \nmodel   [Str19].", "metadata": {}}, {"text": "Although   it   is   now   surpassed   by   other   models   in   terms   of   training   cost   (Table   4),   we   discuss   it   \nhere   as   a   concrete   example   of   the   complexity   of   estimating   the   cost   of   a   ML   method   retroactively.", "metadata": {}}, {"text": "As   Table   4   shows,   the   actual   cost   of   Evolved   Transformer   NAS   is   nearly   two   orders   of   magnitude   smaller   \nthan   previously   estimated   [Str19].", "metadata": {}}, {"text": "Why   the   discrepancy?", "metadata": {}}, {"text": "The   answer   is   that,   in   addition   to   the   efficiency   of   \nGoogle   datacenters,   there   was   a   confusion   in   estimating   the   energy   cost   of   NAS.", "metadata": {}}, {"text": "In   Evolved   Transformer   NAS,   \nresearchers   used   a   small    proxy   task    to   search   for   the   best   models   to   save   time   and   money,   and   then   scaled   up   \nthe   found   models   to   full   size.", "metadata": {}}, {"text": "Small   proxies   may   not   be   obvious,   which   made   it   hard   to   estimate   the   CO 2 e   \ncorrectly   in   retrospect   from   the   NAS   paper   [So19].", "metadata": {}}, {"text": "Due   to   the   misunderstanding   of   the   usage   of   proxy   tasks   in   \nNAS,    it   was    assumed   the   search   was   done   with   full   size   tasks .", "metadata": {}}, {"text": "Because   of   this   assumption,   despite   \nconsiderable   effort   on   their   part,   Strubell    et   al.", "metadata": {}}, {"text": "’s   energy   estimate   for   NAS   ended   up   18.7X   too   high   for   the   \naverage   organization   (see   Appendix   C)   and   88X   off   in   emissions   for   energy-efficient   organizations   like   Google   \n(see   Appendix   D).", "metadata": {}}, {"text": "This   example   led   us   to   our   first   recommendation—that   more   researchers   measure   energy   \nusage   and   CO 2 e   for   computationally   intensive   projects,   and   report   them   when   practical,   rather   than   counting   \non   others   to   estimate   it   retrospectively.", "metadata": {}}, {"text": "Another   confusion   in   the   general   public   is   the   misperception   that   NAS   (and   therefore,   the   cost   associated   \nwith   NAS)   is   conducted   once   per   model   training.", "metadata": {}}, {"text": "In   practice,   however,    NAS   is   generally   not   performed   once   \nper   model   training,   but   once   per    problem     domain+architectural   search   space   combination .", "metadata": {}}, {"text": "For   example,   the   \nEvolved   Transformer,   found   by   NAS   on   translation,   can   be   used   for   language   modeling   without   a   new   search   \n[So19,   Adi20].", "metadata": {}}, {"text": "Unfortunately,    results   in   the   earlier   work   by   [Str19]   characterizing   NAS   were   misattributed   to   \nsingle   model   training   costs   in   the   popular   press.", "metadata": {}}, {"text": "As   an   analogy,   NAS   is   like   optimizing   the   energy   efficiency   and   cost   of   an   LED   light   bulb   with   extensive   \nsimulations   on   a   supercomputer,   training   a   model   is   akin   to   building   LED   light   bulbs,   and   inference   is   \nanalogous   to   all   the   customers   using   LEDs   to   light   their   homes.", "metadata": {}}, {"text": "The   analogous   confusion   would   be   claiming   \nthat   the   one-time   upfront   supercomputer   simulation   cost   should   be   included   in   the   CO 2 e   cost   of   every   light   bulb   \nmanufactured.", "metadata": {}}, {"text": "In   this   analogy,   the   onetime   CO 2    expenditure   of   the   supercomputer   simulations   can   be   more   \nthan   paid   back   with   the   improved   energy-efficiency   of   the   mass-produced   light   bulbs,   as   was   the   case   for   the   \nactual   NAS   of   [So19]   (see   next   paragraph).", "metadata": {}}, {"text": "In   terms   of   cost-benefit   tradeoff ,   NAS   can   also   lead   to   improved   energy   efficiency   in   training   of   downstream   \napplications,   and   the   benefit   can   dramatically   outweigh   the   cost.", "metadata": {}}, {"text": "Figure   4   shows   that   the   Evolved   Transformer,   \nfound   by   NAS   [So19],   has   37%   fewer   parameters   and   converges   to   the   same   accuracy   with   25%   less   energy   \nexpenditure   (see   Table   1)   than   the   vanilla   Transformer   (Big)   model   on   WMT   English   to   German   translation.", "metadata": {}}, {"text": "The   use   of   Evolved   Transformer   instead   of   a   regular   Transformer   architecture   saved   48.5   t CO 2 e   during   the   \ntraining   of   the   Meena   DNN   (see   Tables   1   and   4).", "metadata": {}}, {"text": "The   savings   from   this   single   reuse   in   Meena   are   ~15X   larger   \nthan   the   energy   cost   of   running   the   search   to   discover   it.", "metadata": {}}, {"text": "The   results   of   the   Evolved   Transformer   neural   \n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "architecture   search   have   been   open-sourced.   It   can   readily   be   used   by   anyone   training   ML   models   for   NLP   \nproblems,   similar   to   how   a   Transformer-style   model   can   be   used   for   NLP   problems   [Evo19]. 15   \nIt   would   be   beneficial   to   compare   the   cost-savings   ratio   of   the   Evolved   Transformer   NAS   to   previous   work   \ndeveloping   more   efficient   architectures.   Unfortunately,   as   others   have   pointed   out   [Dod19,   Str19],   the   full   cost   \nof   model   development   is   rarely,   if   ever,   reported   in   the   literature,   making   it   impossible   to   compare   this   analysis   \nto   prior   work,   and   preventing   straightforward   comparison   among   different   approaches   more   generally.    \nThis   lack   of   training   development   costs   is   one   example   of   how   adopting   higher   standards   for   measuring   \nand   reporting   ML   model   energy   requirements   would   lead   to   a   better   understanding   of   cost-accuracy   tradeoffs   \nin   ML   models,   potentially   further   reducing   overall   emissions   by   empowering   more   informed   ML   model   \nselection,   as   the   next   subsection   explains.", "sentences": [{"text": "architecture   search   have   been   open-sourced.", "metadata": {}}, {"text": "It   can   readily   be   used   by   anyone   training   ML   models   for   NLP   \nproblems,   similar   to   how   a   Transformer-style   model   can   be   used   for   NLP   problems   [Evo19].", "metadata": {}}, {"text": "15   \nIt   would   be   beneficial   to   compare   the   cost-savings   ratio   of   the   Evolved   Transformer   NAS   to   previous   work   \ndeveloping   more   efficient   architectures.", "metadata": {}}, {"text": "Unfortunately,   as   others   have   pointed   out   [Dod19,   Str19],   the   full   cost   \nof   model   development   is   rarely,   if   ever,   reported   in   the   literature,   making   it   impossible   to   compare   this   analysis   \nto   prior   work,   and   preventing   straightforward   comparison   among   different   approaches   more   generally.", "metadata": {}}, {"text": "This   lack   of   training   development   costs   is   one   example   of   how   adopting   higher   standards   for   measuring   \nand   reporting   ML   model   energy   requirements   would   lead   to   a   better   understanding   of   cost-accuracy   tradeoffs   \nin   ML   models,   potentially   further   reducing   overall   emissions   by   empowering   more   informed   ML   model   \nselection,   as   the   next   subsection   explains.", "metadata": {}}], "metadata": {"page": 9}}, {"text": "Figure   4:   Reproduction   of   Figure   4   from   So    et   al.    Dots   on   the   blue   line   represent   various   sizes   of   plain   \nTransformer   NLP   models,   while   dots   on   the   red   line   represent   various   sizes   of   the   open-sourced   \nEvolved   Transformer   architecture   that   was   discovered   by   the   neural   architecture   search   run   in   [So19] .   \nRed   arrows   are   at   131M   and   210M   parameters   and   show   that   an   Evolved   Transformer   can   achieve   \nhigher   accuracy   at   less   cost:   it   runs   1.3X   faster   and   produces   1.3x   less   CO 2 e.     \n4.2   There   are   more   resources   used   for   training   than   the   only   final   training   run   \n[Str19]   and   others   point   out   that   it   often   takes   many   attempts   to   get   everything   set   up   correctly   before   the   \nfinal   training   run,   so   the   final   training   run   does   not   reflect   the   total   cost.   Since   it’s   hard   to   improve   what   you   \ncan’t   measure,   one   issue   is   how   to   account   for   such   costs   accurately.   Fortunately,   an   internal   Google   product   \nis   underway   that   will   record   information   about   the   training   process,   originally   intended   to   keep   track   of   \ninformation   like   data   provenance.   The   developers   now   plan   to   add   energy   consumption   so   that   Googlers   can   \nbetter   understand   the   full   training   lifecycle.    An   example   of   an   open   source   tool   to   record   such   information   is   \nexperiment-impact-tracker    [Hen20].    In   addition,   the   developers   of   ML   Emissions   Calculator   [Lac19]   are   \ncurrently   working   on    CodeCarbon ,   whose   goal   is   to   measure/approximate   carbon   consumption   automatically.     \nAlas,   there   will   be   no   way   to   verify   the   claims   in   papers   of   preliminary   training   development.   A   lesson   of   \ncomputer   benchmarking   is   that   requiring   the   release   of   all   information   so   that   others   could   recreate   your   results   \nwas   an   effective   deterrent   to   fudging   the   numbers.   If   more   computationally   intensive   ML   papers   included   \nenergy   consumption   and   carbon   footprint   of   the   final   training   run   with   sufficient   details   that   others   could   check,   \n15  Reuse   reduces   overall   development   effort   and   energy   usage.   For   example,   implementations   of   EfficientNets,   Efficient-   \nDets   [Tan19],   developed   via   NAS   for   image-classification   and   object-detection,   were   forked   on   GitHub   >4000   times.   \n9", "sentences": [{"text": "Figure   4:   Reproduction   of   Figure   4   from   So    et   al.", "metadata": {}}, {"text": "Dots   on   the   blue   line   represent   various   sizes   of   plain   \nTransformer   NLP   models,   while   dots   on   the   red   line   represent   various   sizes   of   the   open-sourced   \nEvolved   Transformer   architecture   that   was   discovered   by   the   neural   architecture   search   run   in   [So19] .", "metadata": {}}, {"text": "Red   arrows   are   at   131M   and   210M   parameters   and   show   that   an   Evolved   Transformer   can   achieve   \nhigher   accuracy   at   less   cost:   it   runs   1.3X   faster   and   produces   1.3x   less   CO 2 e.", "metadata": {}}, {"text": "4.2   There   are   more   resources   used   for   training   than   the   only   final   training   run   \n[Str19]   and   others   point   out   that   it   often   takes   many   attempts   to   get   everything   set   up   correctly   before   the   \nfinal   training   run,   so   the   final   training   run   does   not   reflect   the   total   cost.", "metadata": {}}, {"text": "Since   it’s   hard   to   improve   what   you   \ncan’t   measure,   one   issue   is   how   to   account   for   such   costs   accurately.", "metadata": {}}, {"text": "Fortunately,   an   internal   Google   product   \nis   underway   that   will   record   information   about   the   training   process,   originally   intended   to   keep   track   of   \ninformation   like   data   provenance.", "metadata": {}}, {"text": "The   developers   now   plan   to   add   energy   consumption   so   that   Googlers   can   \nbetter   understand   the   full   training   lifecycle.", "metadata": {}}, {"text": "An   example   of   an   open   source   tool   to   record   such   information   is   \nexperiment-impact-tracker    [Hen20].", "metadata": {}}, {"text": "In   addition,   the   developers   of   ML   Emissions   Calculator   [Lac19]   are   \ncurrently   working   on    CodeCarbon ,   whose   goal   is   to   measure/approximate   carbon   consumption   automatically.", "metadata": {}}, {"text": "Alas,   there   will   be   no   way   to   verify   the   claims   in   papers   of   preliminary   training   development.", "metadata": {}}, {"text": "A   lesson   of   \ncomputer   benchmarking   is   that   requiring   the   release   of   all   information   so   that   others   could   recreate   your   results   \nwas   an   effective   deterrent   to   fudging   the   numbers.", "metadata": {}}, {"text": "If   more   computationally   intensive   ML   papers   included   \nenergy   consumption   and   carbon   footprint   of   the   final   training   run   with   sufficient   details   that   others   could   check,   \n15  Reuse   reduces   overall   development   effort   and   energy   usage.", "metadata": {}}, {"text": "For   example,   implementations   of   EfficientNets,   Efficient-   \nDets   [Tan19],   developed   via   NAS   for   image-classification   and   object-detection,   were   forked   on   GitHub   >4000   times.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}, {"text": "[Image page=9 idx=1 name=X75.png] Size: 661x556, Data: 80905 bytes", "sentences": [{"text": "[Image page=9 idx=1 name=X75.png] Size: 661x556, Data: 80905 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 1, "image_name": "X75.png", "image_width": 661, "image_height": 556, "attachment_type": "image", "has_image_data": true, "image_data_size": 80905}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "that   would   be   a   great   step   forward.   Perhaps   ML   practitioners   could   study   the   total   lifecycle   to   develop   rules   of   \nthumb   to   estimate   the   overall   carbon   footprint   based   on   its   final   training   cost. 16     \nThe   next   subsection   also   emphasizes   the   value   of   measurement.", "sentences": [{"text": "that   would   be   a   great   step   forward.", "metadata": {}}, {"text": "Perhaps   ML   practitioners   could   study   the   total   lifecycle   to   develop   rules   of   \nthumb   to   estimate   the   overall   carbon   footprint   based   on   its   final   training   cost.", "metadata": {}}, {"text": "16     \nThe   next   subsection   also   emphasizes   the   value   of   measurement.", "metadata": {}}], "metadata": {"page": 10}}, {"text": "Figure   5.   Measured   vs   peak   performance,   measured   system   power   vs   peak   chip   power   (TDP),   and   \nmeasured   vs   peak   performance/Watt   for   V100   GPU   and   TPU   v3   (see   Table   4   and   Appendix   A).   \n4.3   Measurements   are   more   interesting   than   extrapolations   \nAlthough   extrapolations   of   carbon   emissions   are   relatively   easy,   more   attention   should   be   paid   to   actual   \nexperiments   that   have   been   conducted   rather   than   to   hypothetical   case   studies.   As   a   problematic   example,   \n16  Since   large   NLP   models   can   take   a   month   to   train,   developers   cannot   afford   to   do   the   full   training   task   many   times.   Like   \n[So19]   for   NAS,   they   likely   use   a   smaller   task   to   explore   the   space   for   a   limited   training   time.   One   indication   comes   from   \nthe   AutoML   work   in   [Li21].   Their   exploration   computation   cost   was   roughly   equal   to   the   final   training   cost.   \n10", "sentences": [{"text": "Figure   5.", "metadata": {}}, {"text": "Measured   vs   peak   performance,   measured   system   power   vs   peak   chip   power   (TDP),   and   \nmeasured   vs   peak   performance/Watt   for   V100   GPU   and   TPU   v3   (see   Table   4   and   Appendix   A).", "metadata": {}}, {"text": "4.3   Measurements   are   more   interesting   than   extrapolations   \nAlthough   extrapolations   of   carbon   emissions   are   relatively   easy,   more   attention   should   be   paid   to   actual   \nexperiments   that   have   been   conducted   rather   than   to   hypothetical   case   studies.", "metadata": {}}, {"text": "As   a   problematic   example,   \n16  Since   large   NLP   models   can   take   a   month   to   train,   developers   cannot   afford   to   do   the   full   training   task   many   times.", "metadata": {}}, {"text": "Like   \n[So19]   for   NAS,   they   likely   use   a   smaller   task   to   explore   the   space   for   a   limited   training   time.", "metadata": {}}, {"text": "One   indication   comes   from   \nthe   AutoML   work   in   [Li21].", "metadata": {}}, {"text": "Their   exploration   computation   cost   was   roughly   equal   to   the   final   training   cost.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=X82.png] Size: 1574x838, Data: 60978 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=X82.png] Size: 1574x838, Data: 60978 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "X82.png", "image_width": 1574, "image_height": 838, "attachment_type": "image", "has_image_data": true, "image_data_size": 60978}}, {"text": "[Image page=10 idx=2 name=X83.png] Size: 1574x778, Data: 78322 bytes", "sentences": [{"text": "[Image page=10 idx=2 name=X83.png] Size: 1574x778, Data: 78322 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 2, "image_name": "X83.png", "image_width": 1574, "image_height": 778, "attachment_type": "image", "has_image_data": true, "image_data_size": 78322}}, {"text": "[Image page=10 idx=3 name=X84.png] Size: 1574x976, Data: 87759 bytes", "sentences": [{"text": "[Image page=10 idx=3 name=X84.png] Size: 1574x976, Data: 87759 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 3, "image_name": "X84.png", "image_width": 1574, "image_height": 976, "attachment_type": "image", "has_image_data": true, "image_data_size": 87759}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "let’s   hypothesize   what   the   CO 2 e   would   be   for   training   Transformer   (Big)   on   the    CTS-1   Quartz   -   Tundra   Extreme   \nScale   supercomputer    at    Lawrence   Livermore   National   Laboratory,     one   of   the    top   500   supercomputers    (but   one   \nwhose   design   is   not   optimized   for   ML   training).   Its   ~100,000   cores   might   use   ~75   MWh   of   power   and    might   \ngenerate   32   tCO 2 e,   ~10,000   times   larger   than   for   TPU   v2s   at   Google   (Table   1) 17 .     \nThe   measurement   advice   applies   to   processors   as   well   DNNs.   Tables   1   and   2   show   that   the   theoretical   \nperformance   per   Watt   is   higher   than   the   measured   performance   per   Watt   on   average   by   factors   of   1.6X   for   \nTPUs   and   by   3.5X   for   GPUs.   Figure   5   shows   the   information   in   Table   1   graphically.   Using   theoretical   \nperformance   per   Watt,   V100   is   1.5X   better   than   TPU   v3,   but   it's   the   other   way   around   for   measured   \nperformance   per   Watt:   TPU   v3   is   2.0X   better   than   V100   on   average   for   these   large   NLP   DNNs.   \nFigure   6   compares   the   gross   CO 2 e   estimates   from   the   ML   Emissions   [Lac19]   and   Green   Algorithms   \n[Lan20]   calculators   to   the   processors   and   programs   in   this   paper   at   the   time   of   this   writing   (April   2021).   \nCompared   to   the   results   in   Tables   1   and   4,   they   differ   by   factors   of   0.53–1.64   and   0.91–2.42   with   geometric   \nmeans   of   0.92   and   1.48,   respectively 18 .    The   ML   Emissions   and   Green   Algorithms   calculators   do   not   \nestimate   net   CO 2 e,   which   could   be   up   to   10X   lower.    The   figure   once   again   shows   the   increase   in   accuracy   \nof   measurement   over   indirect   calculations.   The   authors   of   the   Emissions   Calculator   agree   that   measurement   is   \npreferred,   with   some   calculator   as   the   best   alternative   if   measurement   is   difficult   to   perform   [Luc21].   \nThe   next   discussion   topic   reminds   us   that   improving   the   algorithm   is   often   more   important   than   improving   \nthe   hardware.", "sentences": [{"text": "let’s   hypothesize   what   the   CO 2 e   would   be   for   training   Transformer   (Big)   on   the    CTS-1   Quartz   -   Tundra   Extreme   \nScale   supercomputer    at    Lawrence   Livermore   National   Laboratory,     one   of   the    top   500   supercomputers    (but   one   \nwhose   design   is   not   optimized   for   ML   training).", "metadata": {}}, {"text": "Its   ~100,000   cores   might   use   ~75   MWh   of   power   and    might   \ngenerate   32   tCO 2 e,   ~10,000   times   larger   than   for   TPU   v2s   at   Google   (Table   1) 17 .", "metadata": {}}, {"text": "The   measurement   advice   applies   to   processors   as   well   DNNs.", "metadata": {}}, {"text": "Tables   1   and   2   show   that   the   theoretical   \nperformance   per   Watt   is   higher   than   the   measured   performance   per   Watt   on   average   by   factors   of   1.6X   for   \nTPUs   and   by   3.5X   for   GPUs.", "metadata": {}}, {"text": "Figure   5   shows   the   information   in   Table   1   graphically.", "metadata": {}}, {"text": "Using   theoretical   \nperformance   per   Watt,   V100   is   1.5X   better   than   TPU   v3,   but   it's   the   other   way   around   for   measured   \nperformance   per   Watt:   TPU   v3   is   2.0X   better   than   V100   on   average   for   these   large   NLP   DNNs.", "metadata": {}}, {"text": "Figure   6   compares   the   gross   CO 2 e   estimates   from   the   ML   Emissions   [Lac19]   and   Green   Algorithms   \n[Lan20]   calculators   to   the   processors   and   programs   in   this   paper   at   the   time   of   this   writing   (April   2021).", "metadata": {}}, {"text": "Compared   to   the   results   in   Tables   1   and   4,   they   differ   by   factors   of   0.53–1.64   and   0.91–2.42   with   geometric   \nmeans   of   0.92   and   1.48,   respectively 18 .", "metadata": {}}, {"text": "The   ML   Emissions   and   Green   Algorithms   calculators   do   not   \nestimate   net   CO 2 e,   which   could   be   up   to   10X   lower.", "metadata": {}}, {"text": "The   figure   once   again   shows   the   increase   in   accuracy   \nof   measurement   over   indirect   calculations.", "metadata": {}}, {"text": "The   authors   of   the   Emissions   Calculator   agree   that   measurement   is   \npreferred,   with   some   calculator   as   the   best   alternative   if   measurement   is   difficult   to   perform   [Luc21].", "metadata": {}}, {"text": "The   next   discussion   topic   reminds   us   that   improving   the   algorithm   is   often   more   important   than   improving   \nthe   hardware.", "metadata": {}}], "metadata": {"page": 11}}, {"text": "Figure   6.   Ratio   of   ML   Emissions   and   Green   Algorithm   calculators   vs   gross   CO 2 e   in   Tables   1   and   4.   \n4.4   Standard   ML   algorithmic   techniques   can   improve   energy   efficiency   \nThere   are   many   algorithmic   techniques   that   can   improve   the   energy   efficiency   of   machine   learning   models.   \nSome   techniques   can   achieve   the   same   accuracy   with   less   overall   computation.   Others   can   use   a   large,   \nalready-trained   model   as   a   starting   point   and   yield   a   lighter-weight,   more   computationally   efficient   model   with   \nalmost   the   same   accuracy.   These   techniques   all   serve   to   reduce   the   computational   cost   and   therefore   energy   \nand   carbon   emissions   of   models.   Some   of   these   techniques   include:   \n● Distillation    transfers   the   knowledge   from   large   models   into   smaller,   more   computationally   efficient   \nmodels   [Hin15,   San20].   \n● Pruning ,    quantization ,   and    efficient   coding    can   improve   the   energy   efficiency   of   DNNs   3X–7X   [Han15].   \n17  We   use   US   averages   for   kg   CO 2 e/KWh   and   datacenter   PUE   and   assume   it   runs   at   40%   of   the   peak   floating   point   \nperformance   of   Quartz-Tundra   (3.2   PetaFLOPS/sec).   For   reference,   Figure   5   shows   V100   running   at   20%   of   peak.   \n18  We   picked   the   closest   geographic   option   per   calculator   to   the   actual   location   in   each   case.   The   Green   Algorithms   paper   \nlists   Meena   CO 2 e   as   164t   [Lan20],   but   the   calculator   result   as   of   April   2020   was   85t   for   Virgina   using   Google   Cloud.   \n11", "sentences": [{"text": "Figure   6.", "metadata": {}}, {"text": "Ratio   of   ML   Emissions   and   Green   Algorithm   calculators   vs   gross   CO 2 e   in   Tables   1   and   4.", "metadata": {}}, {"text": "4.4   Standard   ML   algorithmic   techniques   can   improve   energy   efficiency   \nThere   are   many   algorithmic   techniques   that   can   improve   the   energy   efficiency   of   machine   learning   models.", "metadata": {}}, {"text": "Some   techniques   can   achieve   the   same   accuracy   with   less   overall   computation.", "metadata": {}}, {"text": "Others   can   use   a   large,   \nalready-trained   model   as   a   starting   point   and   yield   a   lighter-weight,   more   computationally   efficient   model   with   \nalmost   the   same   accuracy.", "metadata": {}}, {"text": "These   techniques   all   serve   to   reduce   the   computational   cost   and   therefore   energy   \nand   carbon   emissions   of   models.", "metadata": {}}, {"text": "Some   of   these   techniques   include:   \n● Distillation    transfers   the   knowledge   from   large   models   into   smaller,   more   computationally   efficient   \nmodels   [Hin15,   San20].", "metadata": {}}, {"text": "● Pruning ,    quantization ,   and    efficient   coding    can   improve   the   energy   efficiency   of   DNNs   3X–7X   [Han15].", "metadata": {}}, {"text": "17  We   use   US   averages   for   kg   CO 2 e/KWh   and   datacenter   PUE   and   assume   it   runs   at   40%   of   the   peak   floating   point   \nperformance   of   Quartz-Tundra   (3.2   PetaFLOPS/sec).", "metadata": {}}, {"text": "For   reference,   Figure   5   shows   V100   running   at   20%   of   peak.", "metadata": {}}, {"text": "18  We   picked   the   closest   geographic   option   per   calculator   to   the   actual   location   in   each   case.", "metadata": {}}, {"text": "The   Green   Algorithms   paper   \nlists   Meena   CO 2 e   as   164t   [Lan20],   but   the   calculator   result   as   of   April   2020   was   85t   for   Virgina   using   Google   Cloud.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}, {"text": "[Image page=11 idx=1 name=X87.png] Size: 1324x638, Data: 86334 bytes", "sentences": [{"text": "[Image page=11 idx=1 name=X87.png] Size: 1324x638, Data: 86334 bytes", "metadata": {}}], "metadata": {"page": 11, "image_index": 1, "image_name": "X87.png", "image_width": 1324, "image_height": 638, "attachment_type": "image", "has_image_data": true, "image_data_size": 86334}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "● Fine-tuning    and    transfer   learning    both   reuse   already-trained   representations,   rather   than   starting   \ntraining   of   each   NLP   task’s   parameters   from   random   initialization,   for   example   [Dod20].     \n● Sparsely   activated   mixture-of-expert-style   models    can   provide   more   than   10X   reductions   in   \ncomputation   requirements   and   energy   costs   for   both   training   and   inference   while   providing   significantly   \nhigher   accuracy   than   dense   Transformer   or   LSTM-based   models   of   equivalent   computational   cost   per   \ntoken   [Sha17,Lep20,Fed21].   Gshard-600B   is   one   example,   evaluated   in   Section   3.   \nWe   commend   the   development   of   such   techniques.   Some   publication   venues,   such   as   the    EACL    and    NAACL   \n2021   NLP   conferences,   have   begun   specifically   soliciting   research   of   this   nature   by   offering   “Efficient   and   \nGreen”   research   tracks,   alongside   workshops   such   as    SustaiNLP    and    EfficientQA .   We   encourage   other   \nvenues   to   follow   suit,   and   hope   that   many   researchers   will   consider   this   line   of   work.   \nThe   next   topic   discusses   one   of   our   biggest   surprises   of   this   investigation,   the   importance   of   geography.     \n4.5   It   matters   which   datacenter   is   used,   even   within   the   same   organization   \nWe   were   amazed   by   how   much   it   matters    where    and    when    a   DNN   is   trained.   Moreover,   this   option   is   likely   \nthe   easiest   path   for   ML   practitioners   to   reduce   CO 2 e.   For   example,   after   reading   early   drafts   of   this   paper,   \nsome   colleagues   switched   to   a   Google   datacenter   with   a   smaller   carbon   footprint    to   train   a   large   NLP   model.   \nReviewers   of   early   drafts   suggested   that   datacenter   energy   use   is   a   zero-sum   game.   They   thought   that   any   \ntasks   run   in   a   green   datacenter   simply   shift   other   work   to   dirtier   datacenters,   so   there   is   no   net   gain.   It’s   not   \ntrue,   but   that   speculation   reveals   many   seemingly   plausible   but   incorrect   fallacies:   \n● Fallacy:   Datacenters   are   fully   utilized .   Applications   are   deployed   to   handle   worst   case   demand   \ndepending   on   the   time   of   day   and   day   of   the   week,   so   for   much   of   the   time   resources   are   idle   [Arm10].   \n● Fallacy:   Cloud   centers   can’t   grow .   Similar   to   the   founding   of   a   new   university,   cloud   companies   buy   \nmuch   more   land   than   they   need   initially   at   a   site   so   that   they   can   construct   more   buildings   in   the   future   \nwithout   first   traversing   the   lengthy   process   of   acquiring   land   [Bar18].   \n● Fallacy:   Renewable   energy   is   fixed   and   can’t   grow .   There   is   often   an   excess   of   renewable   energy   at   \nsome   times   of   day   (see   Appendix   B).   The   amount   of   solar   and   wind   energy   is   also   a   function   of   the   \ninvestment   as   well   as   weather   conditions.   Google’s   long   term   renewable   energy   procurement   normally   \ninvests   in   the   creation   of   new   renewable   energy   resources.   The   greater   the   use   and   investment   in   \nrenewable   energy,   the   more   money   is   available   to   buy   and   deploy   new   solar   panels   and   wind   turbines,   \nthereby   increasing   the   renewable   energy   supply.   Thus,   it’s    not    the   case   that   Google’s   use   of   renewable   \nenergy   means   other   residents   must   use   dirty   energy.   Appendix   B   introduces   issues   around   carbon   free   \nenergy   use   and   investment.   \n● Fallacy:   Google   NLP   model   training   competes   with   other   tasks   in   the   datacenter .   Google   trains   large   \nmodels   on   ML   supercomputers   that   even   have   their   own   interconnection   network,   so   ML   training   is   \ndistinct   from   CPU-only   tasks   [Jou20].   Tasks   for   CPUs   don’t   interfere   with   TPUs,   and   vice   versa.   \n● Fallacy:   Training   must   run   in   all   datacenters .   While   user   facing   inference   applications   need   global   \ndistribution   in   order   to   provide   low-latency   access   to   users   all   around   the   world   [Jou21],   there   is   no   \nproblem   to   limit   ML   training   computation   to   a   smaller   number   of   (green)   datacenters.   For   example,   \nGoogle   is   currently   deploying   numerous   TPU   v4s,   many   of   which   will   be   located   in   windy   Oklahoma,   \nwhose   net   CO 2 e/KWh   is   even   lower   than   Iowa.   \n● Fallacy:   There   is   no   business   reason   to   reduce   carbon   emissions .   Reducing   climate   change   certainly   \nhas   long-term   economic   benefits   for   everyone.   Google   has   been   carbon   neutral   since   2007   and   has   \nprocured   enough   additional   renewable   energy   to   match   100%   of   its   datacenter   energy   usage   since   \n2017,   so   the   impact   of   the   remaining   carbon   from   training   at   Google   is   zero   even   today.   Other   \nhyperscalers   aim   for   carbon   neutrality   by   2025   or   2030,   so   the   whole   cloud   may   become   carbon   \nneutral.   With   its   new   24/7   local   carbon-free   energy   goal   by   2030,   Google   is   now   focused   on   purchasing   \ncarbon-free   energy   to   match   its   hourly   load   at   the   same   location   as   its   datacenters   with   the   goal   to   \ndecarbonize   its   electricity   supply   (see   Appendix   B).     \nThe   next   question   that   arose   is   whether   such   green   datacenters   are   available   to   only   a   few   ML   practitioners.   \n12", "sentences": [{"text": "● Fine-tuning    and    transfer   learning    both   reuse   already-trained   representations,   rather   than   starting   \ntraining   of   each   NLP   task’s   parameters   from   random   initialization,   for   example   [Dod20].", "metadata": {}}, {"text": "● Sparsely   activated   mixture-of-expert-style   models    can   provide   more   than   10X   reductions   in   \ncomputation   requirements   and   energy   costs   for   both   training   and   inference   while   providing   significantly   \nhigher   accuracy   than   dense   Transformer   or   LSTM-based   models   of   equivalent   computational   cost   per   \ntoken   [Sha17,Lep20,Fed21].", "metadata": {}}, {"text": "Gshard-600B   is   one   example,   evaluated   in   Section   3.", "metadata": {}}, {"text": "We   commend   the   development   of   such   techniques.", "metadata": {}}, {"text": "Some   publication   venues,   such   as   the    EACL    and    NAACL   \n2021   NLP   conferences,   have   begun   specifically   soliciting   research   of   this   nature   by   offering   “Efficient   and   \nGreen”   research   tracks,   alongside   workshops   such   as    SustaiNLP    and    EfficientQA .", "metadata": {}}, {"text": "We   encourage   other   \nvenues   to   follow   suit,   and   hope   that   many   researchers   will   consider   this   line   of   work.", "metadata": {}}, {"text": "The   next   topic   discusses   one   of   our   biggest   surprises   of   this   investigation,   the   importance   of   geography.", "metadata": {}}, {"text": "4.5   It   matters   which   datacenter   is   used,   even   within   the   same   organization   \nWe   were   amazed   by   how   much   it   matters    where    and    when    a   DNN   is   trained.", "metadata": {}}, {"text": "Moreover,   this   option   is   likely   \nthe   easiest   path   for   ML   practitioners   to   reduce   CO 2 e.", "metadata": {}}, {"text": "For   example,   after   reading   early   drafts   of   this   paper,   \nsome   colleagues   switched   to   a   Google   datacenter   with   a   smaller   carbon   footprint    to   train   a   large   NLP   model.", "metadata": {}}, {"text": "Reviewers   of   early   drafts   suggested   that   datacenter   energy   use   is   a   zero-sum   game.", "metadata": {}}, {"text": "They   thought   that   any   \ntasks   run   in   a   green   datacenter   simply   shift   other   work   to   dirtier   datacenters,   so   there   is   no   net   gain.", "metadata": {}}, {"text": "It’s   not   \ntrue,   but   that   speculation   reveals   many   seemingly   plausible   but   incorrect   fallacies:   \n● Fallacy:   Datacenters   are   fully   utilized .", "metadata": {}}, {"text": "Applications   are   deployed   to   handle   worst   case   demand   \ndepending   on   the   time   of   day   and   day   of   the   week,   so   for   much   of   the   time   resources   are   idle   [Arm10].", "metadata": {}}, {"text": "● Fallacy:   Cloud   centers   can’t   grow .", "metadata": {}}, {"text": "Similar   to   the   founding   of   a   new   university,   cloud   companies   buy   \nmuch   more   land   than   they   need   initially   at   a   site   so   that   they   can   construct   more   buildings   in   the   future   \nwithout   first   traversing   the   lengthy   process   of   acquiring   land   [Bar18].", "metadata": {}}, {"text": "● Fallacy:   Renewable   energy   is   fixed   and   can’t   grow .", "metadata": {}}, {"text": "There   is   often   an   excess   of   renewable   energy   at   \nsome   times   of   day   (see   Appendix   B).", "metadata": {}}, {"text": "The   amount   of   solar   and   wind   energy   is   also   a   function   of   the   \ninvestment   as   well   as   weather   conditions.", "metadata": {}}, {"text": "Google’s   long   term   renewable   energy   procurement   normally   \ninvests   in   the   creation   of   new   renewable   energy   resources.", "metadata": {}}, {"text": "The   greater   the   use   and   investment   in   \nrenewable   energy,   the   more   money   is   available   to   buy   and   deploy   new   solar   panels   and   wind   turbines,   \nthereby   increasing   the   renewable   energy   supply.", "metadata": {}}, {"text": "Thus,   it’s    not    the   case   that   Google’s   use   of   renewable   \nenergy   means   other   residents   must   use   dirty   energy.", "metadata": {}}, {"text": "Appendix   B   introduces   issues   around   carbon   free   \nenergy   use   and   investment.", "metadata": {}}, {"text": "● Fallacy:   Google   NLP   model   training   competes   with   other   tasks   in   the   datacenter .", "metadata": {}}, {"text": "Google   trains   large   \nmodels   on   ML   supercomputers   that   even   have   their   own   interconnection   network,   so   ML   training   is   \ndistinct   from   CPU-only   tasks   [Jou20].", "metadata": {}}, {"text": "Tasks   for   CPUs   don’t   interfere   with   TPUs,   and   vice   versa.", "metadata": {}}, {"text": "● Fallacy:   Training   must   run   in   all   datacenters .", "metadata": {}}, {"text": "While   user   facing   inference   applications   need   global   \ndistribution   in   order   to   provide   low-latency   access   to   users   all   around   the   world   [Jou21],   there   is   no   \nproblem   to   limit   ML   training   computation   to   a   smaller   number   of   (green)   datacenters.", "metadata": {}}, {"text": "For   example,   \nGoogle   is   currently   deploying   numerous   TPU   v4s,   many   of   which   will   be   located   in   windy   Oklahoma,   \nwhose   net   CO 2 e/KWh   is   even   lower   than   Iowa.", "metadata": {}}, {"text": "● Fallacy:   There   is   no   business   reason   to   reduce   carbon   emissions .", "metadata": {}}, {"text": "Reducing   climate   change   certainly   \nhas   long-term   economic   benefits   for   everyone.", "metadata": {}}, {"text": "Google   has   been   carbon   neutral   since   2007   and   has   \nprocured   enough   additional   renewable   energy   to   match   100%   of   its   datacenter   energy   usage   since   \n2017,   so   the   impact   of   the   remaining   carbon   from   training   at   Google   is   zero   even   today.", "metadata": {}}, {"text": "Other   \nhyperscalers   aim   for   carbon   neutrality   by   2025   or   2030,   so   the   whole   cloud   may   become   carbon   \nneutral.", "metadata": {}}, {"text": "With   its   new   24/7   local   carbon-free   energy   goal   by   2030,   Google   is   now   focused   on   purchasing   \ncarbon-free   energy   to   match   its   hourly   load   at   the   same   location   as   its   datacenters   with   the   goal   to   \ndecarbonize   its   electricity   supply   (see   Appendix   B).", "metadata": {}}, {"text": "The   next   question   that   arose   is   whether   such   green   datacenters   are   available   to   only   a   few   ML   practitioners.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "4.6   Many   have   access   to   energy-optimized   datacenters   \nThe   increasing   use   of   cloud   computing   has   decreased   the   energy   intensity 19    of   datacenters   20%   annually   \nsince   2010   [Has20].   Access   to   energy-optimized,   low-cost   cloud   datacenters   is   not   restricted   to   employees   of   a   \nfew   companies;   people   around   the   world   can   rent   computers   in   them   using   services   like   Alibaba   Cloud,   \nAmazon   Web   Services,   Google   Cloud   Platform,   and   Microsoft   Azure. 20    Moreover,   Alibaba,   Amazon,   and   \nGoogle   offer   access   to   their   custom   processors   for   DNNs   through   their   cloud   service.   The   popularity   of   the   \npublic   cloud   is   indicated   by   its   annual   growth   in   business   by   up   to   50%   since   2010   [Sch21].   Many   believe   the   \ncloud’s   efficiencies   in   cost   and   energy   mean   that   it   is   the   ultimate   future   of   all   datacenters   [Arm10,   Sch21].     \nThe   next   topic   reminds   us   that   reducing   cost   and   energy   consumption   remains   important   no   matter   how   \ngreen   the   cloud   becomes.   \n4.7   Reducing   the   cost   of   training   matters   too     \nThough   many   have   access   to   these   relatively   efficient   compute   resources   and    cloud   companies   may   \ndramatically   reduce   their   carbon   footprint   in   the   future,   it’s   still   important   to   reduce   the   economic    cost    of   \ntraining.   Saving   money   obviously   matters   to   everyone,   but   e xpensive   training   of   NLP   models   also   makes   this   \nresearch   style   unattainable   for   many   researchers 21 , 22 .   This   inequity   of   access   to   state-of-the-art   models   is   \nanother   strong   motivator,   alongside   environmental   concerns,   to   incentivize   the   development   of   energy-efficient   \nML   models   that   work   as   well   as   their   computationally   hungrier   counterparts.   \nOne   issue   that   was   difficult   for   us   during   our   investigation   was   to   put   into   perspective   the   4   to   552   tCO 2 e   \nfrom   training   of   these   NLP   models,   which   the   next   subsection   explores.     \n4.8   How   does   training   a   large   NLP   model   compare   to   other   activities?     \nGoogle   Flights   estimate    for   the   emissions   of   a   direct   round   trip   of   a   whole   passenger   jet   between   San   \nFrancisco   and   New   York   is   180   tCO 2 e   (see   Table   2   and   Appendix   A).   T5   training   emissions   are   ~26%,   Meena   \nis   53%,   Gshard-600B   is   ~2%,   Switch   Transformer   is   32%,   and   GPT-3   is   ~305%   of   such   a   round   trip.   \nAnother   comparison   point   is   to    Bitcoin .   Every   purchase   that   transfers   bitcoin   currently   costs   ~700   KWh   or   \n~0.3   tCO 2 e,   equivalent   to   the   CO 2 e   produced   by   ~750,000   credit   card   swipes.   Bitcoin   miners   use   custom   chips   \nthat   operate   continuously   24/7   until   they   fail.   Estimates   of   Bitcoin’s   impact   for   2021   are   ~78–121   \nTeraWatt-hours   and   ~37M–58M   tCO 2 e   [Cri21,   Dig21].   Stated   alternatively,   ~70M   people   have   Bitcoin   wallets   \nyet   Google   consumes   1/10th   of   Bitcoin’s   energy   to   provide   services   for   billions   of   people,   and   all   of   Google’s   \nenergy   use   is   offset.   If   Bitcoin   were   a   country,   it   would   be   in   the   top   30   in   CO 2 e;   larger   than   Argentina,   whose   \npopulation   is   45M.   The   estimated   annual   carbon   footprint   of   Bitcoin   mining   this   year   is   equivalent   to   roughly   \n200,000   to   300,000   whole   passenger   jet   SF↔NY   round   trips.   \n  In   2019    the   world   saw   39M    flights   and    US   airlines   flew   925M   passengers ,   which   helps   explain   why   air   \ntravel   was   responsible   for   940   MtCO 2 ,   or   ~2.5%   of   the   world's   annual   CO 2    in   2018   of   33B   tCO 2 e   [Rit20].   \nFinally,   Google   publishes   its   total   energy   consumption,   and   for   2019   it   was   12.2   TeraWatt-hours   [Goo20].   \nRow   18   of   Table   4   shows   the   percentage   that   each   NLP   model   training   was   of   that   total.   Even   if   we   assume   all   \nfour   of   Google’s   large   NLP   models   in   Table   4   were   trained   in   2019,   the   total   represents   less   than   0.005%.    The   \ntraining   of   those   four   large   NLP   models   is   not   a   significant   fraction   of   Google’s   energy   consumption.     \n19  Improvement   in   energy   intensity   is   expressed   as   energy   use   per   compute   instance.   [Has20]   goes   on   to   say   the   cloud’s   \nincreasing   share   of   datacenters   is   causing   a   “notable   improvement   compared   with   recent   annual   efficiency   gains   in   other   \nmajor   demand   sectors   (e.g.,   aviation   and   industry),   which   are   an   order   of   magnitude   lower.”   \n20  There   are   not   many   cloud   companies.   With   new   technologies,   initially   only   a   few   firms   can   practice   the   technology   and   \nthey   sell   it   to   others,   but   these   companies   compete.   There   are   many   examples.   Chemical   technologies   are   in   the   hands   of   \na   relatively   small   number   of   companies;   only   six   or   seven   institutions   worldwide   can   refine   crude   oil;   just   a   few   firms   can   \nmanufacture   computer   chips   in   the   finest   technology   node   (3–5   nm).   \n21  To   support   the   goal   of   making   ML   more   inclusive,    Google   provides   free   access   to   a   total   of   ~500   PetaFLOPS/second   of   \nTPU   compute   power   to   help   ML   researchers   around   the   world   participate   in   advancing   the   start   of   the   art   of   ML .   \n22  One   possible   unintended   consequence   of   making   training   of   a   model   less   expensive   is   that   more   people   will   train   the   \nmodel   and   increase   energy   use,   but   that   seems   like   a   better   risk   than   to   continue   using   inefficient   models.   \n13", "sentences": [{"text": "4.6   Many   have   access   to   energy-optimized   datacenters   \nThe   increasing   use   of   cloud   computing   has   decreased   the   energy   intensity 19    of   datacenters   20%   annually   \nsince   2010   [Has20].", "metadata": {}}, {"text": "Access   to   energy-optimized,   low-cost   cloud   datacenters   is   not   restricted   to   employees   of   a   \nfew   companies;", "metadata": {}}, {"text": "people   around   the   world   can   rent   computers   in   them   using   services   like   Alibaba   Cloud,   \nAmazon   Web   Services,   Google   Cloud   Platform,   and   Microsoft   Azure.", "metadata": {}}, {"text": "20    Moreover,   Alibaba,   Amazon,   and   \nGoogle   offer   access   to   their   custom   processors   for   DNNs   through   their   cloud   service.", "metadata": {}}, {"text": "The   popularity   of   the   \npublic   cloud   is   indicated   by   its   annual   growth   in   business   by   up   to   50%   since   2010   [Sch21].", "metadata": {}}, {"text": "Many   believe   the   \ncloud’s   efficiencies   in   cost   and   energy   mean   that   it   is   the   ultimate   future   of   all   datacenters   [Arm10,   Sch21].", "metadata": {}}, {"text": "The   next   topic   reminds   us   that   reducing   cost   and   energy   consumption   remains   important   no   matter   how   \ngreen   the   cloud   becomes.", "metadata": {}}, {"text": "4.7   Reducing   the   cost   of   training   matters   too     \nThough   many   have   access   to   these   relatively   efficient   compute   resources   and    cloud   companies   may   \ndramatically   reduce   their   carbon   footprint   in   the   future,   it’s   still   important   to   reduce   the   economic    cost    of   \ntraining.", "metadata": {}}, {"text": "Saving   money   obviously   matters   to   everyone,   but   e xpensive   training   of   NLP   models   also   makes   this   \nresearch   style   unattainable   for   many   researchers 21 , 22 .", "metadata": {}}, {"text": "This   inequity   of   access   to   state-of-the-art   models   is   \nanother   strong   motivator,   alongside   environmental   concerns,   to   incentivize   the   development   of   energy-efficient   \nML   models   that   work   as   well   as   their   computationally   hungrier   counterparts.", "metadata": {}}, {"text": "One   issue   that   was   difficult   for   us   during   our   investigation   was   to   put   into   perspective   the   4   to   552   tCO 2 e   \nfrom   training   of   these   NLP   models,   which   the   next   subsection   explores.", "metadata": {}}, {"text": "4.8   How   does   training   a   large   NLP   model   compare   to   other   activities?", "metadata": {}}, {"text": "Google   Flights   estimate    for   the   emissions   of   a   direct   round   trip   of   a   whole   passenger   jet   between   San   \nFrancisco   and   New   York   is   180   tCO 2 e   (see   Table   2   and   Appendix   A).", "metadata": {}}, {"text": "T5   training   emissions   are   ~26%,   Meena   \nis   53%,   Gshard-600B   is   ~2%,   Switch   Transformer   is   32%,   and   GPT-3   is   ~305%   of   such   a   round   trip.", "metadata": {}}, {"text": "Another   comparison   point   is   to    Bitcoin .", "metadata": {}}, {"text": "Every   purchase   that   transfers   bitcoin   currently   costs   ~700   KWh   or   \n~0.3   tCO 2 e,   equivalent   to   the   CO 2 e   produced   by   ~750,000   credit   card   swipes.", "metadata": {}}, {"text": "Bitcoin   miners   use   custom   chips   \nthat   operate   continuously   24/7   until   they   fail.", "metadata": {}}, {"text": "Estimates   of   Bitcoin’s   impact   for   2021   are   ~78–121   \nTeraWatt-hours   and   ~37M–58M   tCO 2 e   [Cri21,   Dig21].", "metadata": {}}, {"text": "Stated   alternatively,   ~70M   people   have   Bitcoin   wallets   \nyet   Google   consumes   1/10th   of   Bitcoin’s   energy   to   provide   services   for   billions   of   people,   and   all   of   Google’s   \nenergy   use   is   offset.", "metadata": {}}, {"text": "If   Bitcoin   were   a   country,   it   would   be   in   the   top   30   in   CO 2 e;", "metadata": {}}, {"text": "larger   than   Argentina,   whose   \npopulation   is   45M.", "metadata": {}}, {"text": "The   estimated   annual   carbon   footprint   of   Bitcoin   mining   this   year   is   equivalent   to   roughly   \n200,000   to   300,000   whole   passenger   jet   SF↔NY   round   trips.", "metadata": {}}, {"text": "In   2019    the   world   saw   39M    flights   and    US   airlines   flew   925M   passengers ,   which   helps   explain   why   air   \ntravel   was   responsible   for   940   MtCO 2 ,   or   ~2.5%   of   the   world's   annual   CO 2    in   2018   of   33B   tCO 2 e   [Rit20].", "metadata": {}}, {"text": "Finally,   Google   publishes   its   total   energy   consumption,   and   for   2019   it   was   12.2   TeraWatt-hours   [Goo20].", "metadata": {}}, {"text": "Row   18   of   Table   4   shows   the   percentage   that   each   NLP   model   training   was   of   that   total.", "metadata": {}}, {"text": "Even   if   we   assume   all   \nfour   of   Google’s   large   NLP   models   in   Table   4   were   trained   in   2019,   the   total   represents   less   than   0.005%.", "metadata": {}}, {"text": "The   \ntraining   of   those   four   large   NLP   models   is   not   a   significant   fraction   of   Google’s   energy   consumption.", "metadata": {}}, {"text": "19  Improvement   in   energy   intensity   is   expressed   as   energy   use   per   compute   instance.", "metadata": {}}, {"text": "[Has20]   goes   on   to   say   the   cloud’s   \nincreasing   share   of   datacenters   is   causing   a   “notable   improvement   compared   with   recent   annual   efficiency   gains   in   other   \nmajor   demand   sectors   (e.g.,   aviation   and   industry),   which   are   an   order   of   magnitude   lower.”   \n20  There   are   not   many   cloud   companies.", "metadata": {}}, {"text": "With   new   technologies,   initially   only   a   few   firms   can   practice   the   technology   and   \nthey   sell   it   to   others,   but   these   companies   compete.", "metadata": {}}, {"text": "There   are   many   examples.", "metadata": {}}, {"text": "Chemical   technologies   are   in   the   hands   of   \na   relatively   small   number   of   companies;", "metadata": {}}, {"text": "only   six   or   seven   institutions   worldwide   can   refine   crude   oil;", "metadata": {}}, {"text": "just   a   few   firms   can   \nmanufacture   computer   chips   in   the   finest   technology   node   (3–5   nm).", "metadata": {}}, {"text": "21  To   support   the   goal   of   making   ML   more   inclusive,    Google   provides   free   access   to   a   total   of   ~500   PetaFLOPS/second   of   \nTPU   compute   power   to   help   ML   researchers   around   the   world   participate   in   advancing   the   start   of   the   art   of   ML .", "metadata": {}}, {"text": "22  One   possible   unintended   consequence   of   making   training   of   a   model   less   expensive   is   that   more   people   will   train   the   \nmodel   and   increase   energy   use,   but   that   seems   like   a   better   risk   than   to   continue   using   inefficient   models.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "Having   spent   13   pages   on   the   cost   of   large   NLP   models   and   neural   architecture   search,   we   conclude   our   \ndiscussion   with   three   examples   of   the   potential   benefits   of   NLP   models.     \n4.9    Are   the   benefits   of   NLP   models   worth   the   energy   cost?   \nA   recent   example   of   a   societal   benefit   of   NLP   is   the    COVID-19   Research   Explorer ,   which   helps   scientists   \nand   researchers   efficiently   pore   through   articles   for   answers   or   evidence   to   COVID-19-related   questions.   It   is   \npowered   by    BERT ,   a   Transformer-style   model   trained   for   the   biomedical   domain   [Hal20]. 23    Its   training   \nconsumed   ~2.8   MWh   and   produced   0.13   tCO 2 e,   about   one-tenth   of   a   SF-NY   round   trip   by   one   passenger. 24   \nA   more   widespread   example   is   the   use    of   BERT   in   search .   English   is   the   most   popular   language   on   the   \nweb.   This   use   of   BERT   takes   models   that   learn   from   improvements   in   English   and   applies   them   to   other   \nlanguages.   In   particular,   BERT   significantly   improved   featured   snippets—short   text   summary   at   the   top   of   \nGoogle   research   results—in   languages   like   Hindi,   Korean,   and   Portuguese.", "sentences": [{"text": "Having   spent   13   pages   on   the   cost   of   large   NLP   models   and   neural   architecture   search,   we   conclude   our   \ndiscussion   with   three   examples   of   the   potential   benefits   of   NLP   models.", "metadata": {}}, {"text": "4.9    Are   the   benefits   of   NLP   models   worth   the   energy   cost?", "metadata": {}}, {"text": "A   recent   example   of   a   societal   benefit   of   NLP   is   the    COVID-19   Research   Explorer ,   which   helps   scientists   \nand   researchers   efficiently   pore   through   articles   for   answers   or   evidence   to   COVID-19-related   questions.", "metadata": {}}, {"text": "It   is   \npowered   by    BERT ,   a   Transformer-style   model   trained   for   the   biomedical   domain   [Hal20].", "metadata": {}}, {"text": "23    Its   training   \nconsumed   ~2.8   MWh   and   produced   0.13   tCO 2 e,   about   one-tenth   of   a   SF-NY   round   trip   by   one   passenger.", "metadata": {}}, {"text": "24   \nA   more   widespread   example   is   the   use    of   BERT   in   search .", "metadata": {}}, {"text": "English   is   the   most   popular   language   on   the   \nweb.", "metadata": {}}, {"text": "This   use   of   BERT   takes   models   that   learn   from   improvements   in   English   and   applies   them   to   other   \nlanguages.", "metadata": {}}, {"text": "In   particular,   BERT   significantly   improved   featured   snippets—short   text   summary   at   the   top   of   \nGoogle   research   results—in   languages   like   Hindi,   Korean,   and   Portuguese.", "metadata": {}}], "metadata": {"page": 14}}, {"text": "Figure   7:   Reproduction   of   Figure   6   from   [Lep20]   with   annotations.   Translation   quality   comparison   of   \nmultilingual   Mixture   of   Expert   (MoE)   Transformer   models   trained   with   GShard   showing   the   increase   in   \nBLEU   score    versus   a   separate   baseline   Transformer   model   trained   on   each   language   pair   for   100   \nlanguages   to   English.   MoE   models   have   large   model   capacity   but   are   only   partially   activated   for   any   \ngiven   token.   The   source   languages   are   grouped   on   the   x-axis   by   the   resources   available   for   each   \nlanguage   in   billions   of   speakers,   with   languages   like   French   and   Spanish   on   the   left   (>1B   examples)   \nand   languages   like   Sindhi   and   Yoruba   on   the   right   (<1M   examples).   The   BLEU   score   improvements   \nfrom   larger   models   and   multilingual   training   are   high   for   all   languages   but   are   even   higher   for   \nlow-resource   languages—the   graph’s   right-hand   side   is   higher   than   the   left—so   Yoruba   translation   \nquality   benefits   more   than   Spanish   translation   quality.", "sentences": [{"text": "Figure   7:   Reproduction   of   Figure   6   from   [Lep20]   with   annotations.", "metadata": {}}, {"text": "Translation   quality   comparison   of   \nmultilingual   Mixture   of   Expert   (MoE)   Transformer   models   trained   with   GShard   showing   the   increase   in   \nBLEU   score    versus   a   separate   baseline   Transformer   model   trained   on   each   language   pair   for   100   \nlanguages   to   English.", "metadata": {}}, {"text": "MoE   models   have   large   model   capacity   but   are   only   partially   activated   for   any   \ngiven   token.", "metadata": {}}, {"text": "The   source   languages   are   grouped   on   the   x-axis   by   the   resources   available   for   each   \nlanguage   in   billions   of   speakers,   with   languages   like   French   and   Spanish   on   the   left   (>1B   examples)   \nand   languages   like   Sindhi   and   Yoruba   on   the   right   (<1M   examples).", "metadata": {}}, {"text": "The   BLEU   score   improvements   \nfrom   larger   models   and   multilingual   training   are   high   for   all   languages   but   are   even   higher   for   \nlow-resource   languages—the   graph’s   right-hand   side   is   higher   than   the   left—so   Yoruba   translation   \nquality   benefits   more   than   Spanish   translation   quality.", "metadata": {}}], "metadata": {"page": 14}}, {"text": "A   final   example   is   the   GShard   multilingual   translation   model   itself.    Bender   &   Gebru    et   al.    [Ben21]   raise   \nseveral   legitimate   issues   in   the   development   and   use   of   large   language   models.    Creating   such   models   \nrequires   careful   attention   to   issues   of   fairness   and   bias   [Ben21,   Gar19,   Joh20,   Kuc18,   Mer19],   but   they   also   \nhave   the   potential   to   benefit   people   everywhere.    For   example,    our   large   scale   translation   models   (M4)   have   \n23  Despite   targeting   a   narrow   audience   of   scientists,   COVID   explorer   served   1000   queries   per   day   at   launch.   It   drew   \ninterest   from   Pfizer,   Bristol   Myers   Squibb,   AstraZeneca,   Regeneron,   British   Medical   Journal,   European   Food   Safety   \nAuthority,   and   the   National   Institute   of   Health.   Pfizer’s   Director   of   Global   Medical   Epidemiology   used   the   tool   daily;   it   led   to   \nPfizer   epidemiology   research   group   to   adapt   the   underlying   ML   models   for   systematic   reviews   and   literature   search.   \n24  Training   COVID   Explorer   took   6   days   on   64   TPU   v3s   running   in   Oklahoma.   It   used   ~2.8   MWh   and   0.13   net   tCO 2 e.     \n14", "sentences": [{"text": "A   final   example   is   the   GShard   multilingual   translation   model   itself.", "metadata": {}}, {"text": "Bender   &   Gebru    et   al.", "metadata": {}}, {"text": "[Ben21]   raise   \nseveral   legitimate   issues   in   the   development   and   use   of   large   language   models.", "metadata": {}}, {"text": "Creating   such   models   \nrequires   careful   attention   to   issues   of   fairness   and   bias   [Ben21,   Gar19,   Joh20,   Kuc18,   Mer19],   but   they   also   \nhave   the   potential   to   benefit   people   everywhere.", "metadata": {}}, {"text": "For   example,    our   large   scale   translation   models   (M4)   have   \n23  Despite   targeting   a   narrow   audience   of   scientists,   COVID   explorer   served   1000   queries   per   day   at   launch.", "metadata": {}}, {"text": "It   drew   \ninterest   from   Pfizer,   Bristol   Myers   Squibb,   AstraZeneca,   Regeneron,   British   Medical   Journal,   European   Food   Safety   \nAuthority,   and   the   National   Institute   of   Health.", "metadata": {}}, {"text": "Pfizer’s   Director   of   Global   Medical   Epidemiology   used   the   tool   daily;", "metadata": {}}, {"text": "it   led   to   \nPfizer   epidemiology   research   group   to   adapt   the   underlying   ML   models   for   systematic   reviews   and   literature   search.", "metadata": {}}, {"text": "24  Training   COVID   Explorer   took   6   days   on   64   TPU   v3s   running   in   Oklahoma.", "metadata": {}}, {"text": "It   used   ~2.8   MWh   and   0.13   net   tCO 2 e.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}, {"text": "[Image page=14 idx=1 name=X107.png] Size: 1186x579, Data: 190312 bytes", "sentences": [{"text": "[Image page=14 idx=1 name=X107.png] Size: 1186x579, Data: 190312 bytes", "metadata": {}}], "metadata": {"page": 14, "image_index": 1, "image_name": "X107.png", "image_width": 1186, "image_height": 579, "attachment_type": "image", "has_image_data": true, "image_data_size": 190312}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "already   been   used   to   translate   billions   of   queries   annually   for   each   mid-to-low   resource   language 25    with   2B   \nspeakers   globally   for   these   languages.   Figure   7,   from   the   GShard   paper   [Lep20],   shows   substantial   \nimprovements   for   translation   of   100   different   languages   to   English.   The   blue   line   on   the   top   in   the   left   \nrepresents   the   600B   parameter   multi-lingual   translation   MoE   model   of   GShard.   The   dashed   black   line   near   the   \nbottom   is   for   a   traditional   dense   DNN   that   is   fully   activated   for   every   token.   The   dense   DNN   requires   ~10X   \nmore   computational   resources   to   train   than   the   600B   sparse   MoE   model,   despite   substantially   lower   translation   \nquality.   Figure   7   shows   the   larger   MoE   model,   the   larger   the   BLEU   score   gains   were   across   all   languages;   the   \nlines   rarely   cross.   The   600B   MoE   model   improves   average   quality   +13.5   BLEU,   7.4   higher   than   the   2.3B   dense   \nmodel.     \nGShard-600B’s   emissions   (Table   4)   are   4.3   tCO 2 e   —3.5   passenger   SF-NY   round   trips—from   consuming   24   \nMWh   to   train   the   model   that   could   have   2B   users;   the   amortized   per-user   CO 2 e   impact   of   model   training   would   \nbe   less   than   the   CO 2 e   impact   of   sending   one   text   message 26 .     \n5.   Conclusion   \nGlobal   climate   change   is   a   threat   to   economies,   human   health,   and   the   environment,   and   the   ML   community   \nneeds   to   do   its   share   to   limit   its   carbon   emissions. 27    We’re   thankful   that   papers   like   [Lac19,   Str19,   Sch20,   \nHen20]   helped   make   the   ML   community   aware   of   this   important   issue.   Improving   the   energy   efficiency   of   \nalgorithms,   datacenters,   hardware,   and   software   has   long   been   a   business   priority   for   Google   and   other   Cloud   \ncompanies.   For   example,   Gshard-600B   operates   much   more   efficiently   than   other   large   NLP   models   and   ML   \naccelerators   are   more   efficient   than   off-the-shelf   hardware.     \nAs   mentioned   in   the   introduction,   we   make   three   suggestions   for   publications   on   compute   intensive   models   \nthat   could   eventually   help   reduce   their   CO 2 e   footprint:   report   energy   consumed   and   CO 2 e   explicitly,   ML   \nconferences   should   reward   improvements   in   efficiency   as   well   as   traditional   metrics,   and   include   the   time   and   \nnumber   of   processors   for   training   to   help   everyone   understand   its   cost.   We   believe   power   will   be   included   in   \nupcoming   MLPerf   benchmarks,   which   is   an   important   step   in   the   right   direction.   \nIf   the   ML   community   working   on   computationally   intensive   models   starts   competing   on   training   quality   and   \ncarbon   footprint   rather   than   on   accuracy   alone,   the   most   efficient   datacenters   and   hardware   might   see   the   \nhighest   ML   demand.   If   paired   with   publication   incentives   to   improve   emission   metrics   in   addition   to   accuracy,   \nwe   can   imagine   a   virtuous   cycle   that   slows   the   growth   of   the   carbon   footprint   of   ML   by   accelerating   innovations   \nin   the   efficiency   and   cost   of   algorithms,   systems,   hardware,   datacenters,   and   carbon   free   energy.     \nAcknowledgements   \nWe   wish   to   express   our   thanks   to   colleagues   at   Google   and   elsewhere   who   helped   shape   and   improve   this   \npaper.   Emma   Strubell   made   several   suggestions   of   ideas   and   organization   of   the   paper,   including   suggesting   \nadding   data   about   the   five   large   models.   We   thank   Christopher   Berner,   Ilya   Sutskever,   OpenAI,   and   Microsoft   \nfor   sharing   information   about   GPT-3.   Dmitry   Lepikhin   and   Zongwei   Zhou   did   a   great   deal   of   work   to   measure   \nthe   performance   and   power   of   GPUs   and   TPUs   in   Google   datacenters.   Hallie   Cramer,   Anna   Escuer,   Elke   \nMichlmayr,   Kelli   Wright,   and   Nick   Zakrasek   helped   with   the   sections   on   energy   and   CO 2 e   emissions   at   Google.   \nTim   Kraska   suggested   a   revised   organization   of   this   paper.   We   thank   Daniel   Adiwardana,   Gabriel   Bender,   \nAndrei   Broder,   Charina   Chou,   Jesse   Dodge,   Oren   Etzioni,   Orhan   Firat,   Ananya   Ganesh,   Robbie   Gonzalez,   \nDavid   Grangier,   Marsden   Hanna,   Urs   Hölzle,   Sheng   Li,   Sasha   Luccioni,   Preston   McAfee,   Andrew   McCallum,   \nEsteban   Real,   Stven   Ross,   Brennan   Saeta,   Roy   Schwartz,   Victor   Schmidt,   Ian   Schneider,   Aarush   Selvan,   \nNoah   A.   Smith,   Zak   Stone,   Kate   Weber,   and   Cliff   Young   for   their   help   and   feedback   on   the   manuscript.     \n25  In   our   setup   for   Figure   7,   low   resource   languages   have   less   than   1M   training   examples,   mid   resource   languages   have   \nless   than   10M   training   examples,   and   high   resource   languages   have   more   than   1B   training   examples.   \n26  An    SMS   message   is   0.014   g   of   CO 2 .   That   is   larger   than   24   MWh   /   2B,   which   yields   about   0.005   g   of   CO 2 .   \n27  We   did   not   address   the   carbon   footprint   of   ML   in   phones   and   other   edge   devices.   It   would   be   an   excellent   topic   for   \nanother   paper.   \n15", "sentences": [{"text": "already   been   used   to   translate   billions   of   queries   annually   for   each   mid-to-low   resource   language 25    with   2B   \nspeakers   globally   for   these   languages.", "metadata": {}}, {"text": "Figure   7,   from   the   GShard   paper   [Lep20],   shows   substantial   \nimprovements   for   translation   of   100   different   languages   to   English.", "metadata": {}}, {"text": "The   blue   line   on   the   top   in   the   left   \nrepresents   the   600B   parameter   multi-lingual   translation   MoE   model   of   GShard.", "metadata": {}}, {"text": "The   dashed   black   line   near   the   \nbottom   is   for   a   traditional   dense   DNN   that   is   fully   activated   for   every   token.", "metadata": {}}, {"text": "The   dense   DNN   requires   ~10X   \nmore   computational   resources   to   train   than   the   600B   sparse   MoE   model,   despite   substantially   lower   translation   \nquality.", "metadata": {}}, {"text": "Figure   7   shows   the   larger   MoE   model,   the   larger   the   BLEU   score   gains   were   across   all   languages;", "metadata": {}}, {"text": "the   \nlines   rarely   cross.", "metadata": {}}, {"text": "The   600B   MoE   model   improves   average   quality   +13.5   BLEU,   7.4   higher   than   the   2.3B   dense   \nmodel.", "metadata": {}}, {"text": "GShard-600B’s   emissions   (Table   4)   are   4.3   tCO 2 e   —3.5   passenger   SF-NY   round   trips—from   consuming   24   \nMWh   to   train   the   model   that   could   have   2B   users;", "metadata": {}}, {"text": "the   amortized   per-user   CO 2 e   impact   of   model   training   would   \nbe   less   than   the   CO 2 e   impact   of   sending   one   text   message 26 .", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "Conclusion   \nGlobal   climate   change   is   a   threat   to   economies,   human   health,   and   the   environment,   and   the   ML   community   \nneeds   to   do   its   share   to   limit   its   carbon   emissions.", "metadata": {}}, {"text": "27    We’re   thankful   that   papers   like   [Lac19,   Str19,   Sch20,   \nHen20]   helped   make   the   ML   community   aware   of   this   important   issue.", "metadata": {}}, {"text": "Improving   the   energy   efficiency   of   \nalgorithms,   datacenters,   hardware,   and   software   has   long   been   a   business   priority   for   Google   and   other   Cloud   \ncompanies.", "metadata": {}}, {"text": "For   example,   Gshard-600B   operates   much   more   efficiently   than   other   large   NLP   models   and   ML   \naccelerators   are   more   efficient   than   off-the-shelf   hardware.", "metadata": {}}, {"text": "As   mentioned   in   the   introduction,   we   make   three   suggestions   for   publications   on   compute   intensive   models   \nthat   could   eventually   help   reduce   their   CO 2 e   footprint:   report   energy   consumed   and   CO 2 e   explicitly,   ML   \nconferences   should   reward   improvements   in   efficiency   as   well   as   traditional   metrics,   and   include   the   time   and   \nnumber   of   processors   for   training   to   help   everyone   understand   its   cost.", "metadata": {}}, {"text": "We   believe   power   will   be   included   in   \nupcoming   MLPerf   benchmarks,   which   is   an   important   step   in   the   right   direction.", "metadata": {}}, {"text": "If   the   ML   community   working   on   computationally   intensive   models   starts   competing   on   training   quality   and   \ncarbon   footprint   rather   than   on   accuracy   alone,   the   most   efficient   datacenters   and   hardware   might   see   the   \nhighest   ML   demand.", "metadata": {}}, {"text": "If   paired   with   publication   incentives   to   improve   emission   metrics   in   addition   to   accuracy,   \nwe   can   imagine   a   virtuous   cycle   that   slows   the   growth   of   the   carbon   footprint   of   ML   by   accelerating   innovations   \nin   the   efficiency   and   cost   of   algorithms,   systems,   hardware,   datacenters,   and   carbon   free   energy.", "metadata": {}}, {"text": "Acknowledgements   \nWe   wish   to   express   our   thanks   to   colleagues   at   Google   and   elsewhere   who   helped   shape   and   improve   this   \npaper.", "metadata": {}}, {"text": "Emma   Strubell   made   several   suggestions   of   ideas   and   organization   of   the   paper,   including   suggesting   \nadding   data   about   the   five   large   models.", "metadata": {}}, {"text": "We   thank   Christopher   Berner,   Ilya   Sutskever,   OpenAI,   and   Microsoft   \nfor   sharing   information   about   GPT-3.", "metadata": {}}, {"text": "Dmitry   Lepikhin   and   Zongwei   Zhou   did   a   great   deal   of   work   to   measure   \nthe   performance   and   power   of   GPUs   and   TPUs   in   Google   datacenters.", "metadata": {}}, {"text": "Hallie   Cramer,   Anna   Escuer,   Elke   \nMichlmayr,   Kelli   Wright,   and   Nick   Zakrasek   helped   with   the   sections   on   energy   and   CO 2 e   emissions   at   Google.", "metadata": {}}, {"text": "Tim   Kraska   suggested   a   revised   organization   of   this   paper.", "metadata": {}}, {"text": "We   thank   Daniel   Adiwardana,   Gabriel   Bender,   \nAndrei   Broder,   Charina   Chou,   Jesse   Dodge,   Oren   Etzioni,   Orhan   Firat,   Ananya   Ganesh,   Robbie   Gonzalez,   \nDavid   Grangier,   Marsden   Hanna,   Urs   Hölzle,   Sheng   Li,   Sasha   Luccioni,   Preston   McAfee,   Andrew   McCallum,   \nEsteban   Real,   Stven   Ross,   Brennan   Saeta,   Roy   Schwartz,   Victor   Schmidt,   Ian   Schneider,   Aarush   Selvan,   \nNoah   A.", "metadata": {}}, {"text": "Smith,   Zak   Stone,   Kate   Weber,   and   Cliff   Young   for   their   help   and   feedback   on   the   manuscript.", "metadata": {}}, {"text": "25  In   our   setup   for   Figure   7,   low   resource   languages   have   less   than   1M   training   examples,   mid   resource   languages   have   \nless   than   10M   training   examples,   and   high   resource   languages   have   more   than   1B   training   examples.", "metadata": {}}, {"text": "26  An    SMS   message   is   0.014   g   of   CO 2 .", "metadata": {}}, {"text": "That   is   larger   than   24   MWh   /   2B,   which   yields   about   0.005   g   of   CO 2 .", "metadata": {}}, {"text": "27  We   did   not   address   the   carbon   footprint   of   ML   in   phones   and   other   edge   devices.", "metadata": {}}, {"text": "It   would   be   an   excellent   topic   for   \nanother   paper.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "References   \n16   \n[Adi20]   Adiwardana,   D. ,    Luong,   M.,    R.   So,   D.,   Hall,   J.,   Fiedel,   N.,   Thoppilan,   R.,   Yang,   Z.,   Kulshreshtha,   A.,   Nemade,   \nG.,   Lu,   Y.,   and   Le.   Q.    Towards   a   Human-like   Open-Domain   Chatbot .    arXiv   preprint   arXiv:2001.09977 .   \n[Arm10]  Armbrust,   M.,   Fox,   A.,   Griffith,   R.,   Joseph,   A.D.,   Katz,   R.,   Konwinski,   A.,   Lee,   G.,   Patterson,   D.,   Rabkin,   A.,   \nStoica,   I.   and   Zaharia,   M.,   2010.   A   view   of   cloud   computing.    Communications   of   the   ACM,    53(4),   pp.50-58.   \n[Bar19]   Barr,   J.   December   3,   2019.   Amazon   EC2   Update,   \naws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips   \n-for-high-performance-cost-effective-inferencing/ .   \n[Bro20]   Brown,   T.,   Mann,   B.,   Ryder,   N.,   Subbiah,   M.,   Kaplan,   J.,   Dhariwal,   P.,   Neelakantan,   A.,   Shyam   ,   P.,   Sastry,    G.,   \nAskell,   A.,   Agarwal,   S.,   Herbert-Voss,   A.,   Krueger,   G.,   Henighan,   T.,   Child,   R.,   Ramesh,   A.,   Ziegler,   D.,   Wu,   J.,   \nWinter,   C.,   Hesse,   C.,   Chen,   M.,   Sigler,   E.,   Litwin,   M.,   Gray,   S.,   Chess,   B.,   Clark,   J.,   Berner,   C.,   McCandlish,   S.,   \nRadford,   A.,   Sutskever,   I.,   Amodei,    D.   July   22,   2020.   Language   models   are   few-shot   learners.   NeurIPS   2020.   \narXiv   preprint   arXiv:2005.14165 .   \n[Ben21]  Bender,   E.,   Gebru,   T.,   McMillan-Major,   A.   Shmitchell,   S.   On   the   Dangers   of   Stochastic   Parrots:   Can   Language   \nModels   Be   Too   Big?   FAccT   2021.    http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf .   \n[Car21]   Carbon   Offset   Research   and   Education,   2021,   Carbon   Offset   Guide,    https://www.offsetguide.org/ .   \n[Cha19]  Chang,   K.W.,   Prabhakaran,   V.   and   Ordonez,   V.,   2019,   November.   Bias   and   fairness   in   natural   language   \nprocessing.   In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   Processing   and   \nthe   9th   International   Joint   Conference   on   Natural   Language   Processing   (EMNLP-IJCNLP):   Tutorial   Abstracts.   \nhttps://arxiv.org/pdf/1908.09635.pdf .   \n[Cri21]   Criddle,   C.,   February   10,   2021.   Bitcoin   consumes   more   electricity   than   Argentina,   \nwww.bbc.com/news/technology-56012952 .   \n[Dig21]   Digiconomist,   2021,   Bitcoin   Energy   Consumption   Index,    https://digiconomist.net/bitcoin-energy-consumption/    .   \n[Dod19]  Dodge,   J.,   Gururangan,   S.,   Card,   D.,   Schwartz,   R.,   and   Smith,   N.,   2019.   Show   Your   Work:   Improved   Reporting   \nof   Experimental   Results.   In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   \nProcessing   and   the   9th   International   Joint   Conference   on   Natural   Language   Processing   \n(EMNLP-IJCNLP). www.aclweb.org/anthology/D19-1224/ .   \n[Dod20]  Dodge,   J.,   Ilharco,   G.,   Schwartz,   R.,   Farhadi,   A.,   Hajishirzi,   H.   and   Smith,   N.,   2020.   Fine-tuning   pretrained   \nlanguage   models:   Weight   initializations,   data   orders,   and   early   stopping.    arXiv   preprint   arXiv:2002.06305 .   \n[Evo19]    Apache-licensed   Evolved   Transformer   open-source   implementation   in   tensorflow/tensor2tensor   GitHub   \nrepository.   \nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py   \n[Fed21]   Fedus,   W.,   Zoph,   B.,   Shazeer,   N.,   January   11,   2021,   Switch   Transformers:   Scaling   to   Trillion   Parameter   Models   \nwith   Simple   and   Efficient   Sparsity    https://arxiv.org/abs/2101.03961 .   \n[Gar19]   Garg,   S.,   Perot,   V.,   Limtiaco,   N.,   Taly,   A.,   Chi,   E.H.   and   Beutel,   A.,   2019,   January.   Counterfactual   fairness   in   text   \nclassification   through   robustness.   In   Proceedings   of   the   2019   AAAI/ACM   Conference   on   AI,   Ethics,   and   Society   \n(pp.   219-226).    https://research.google/pubs/pub47670/    .   \n[Goo16]  Google,   December   2016,   Achieving   Our   100%   Renewable   Energy   Purchasing   Goal   and   Going   Beyond,   \nhttps://static.   \ngoogleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal \n.pdf .   \n[Goo20]  Google,   Environmental   Report   2020,   \nhttps://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf .   \n[Goo21]  Google,   February   2021,   24/7   Carbon-Free   Energy:   Methodologies   and   Metrics,   \nhttps://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf .   \n[Gup20]  Gupta,   U.,   Kim,   Y.G.,   Lee,   S.,   Tse,   J.,   Lee,   H.H.S.,   Wei,   G.Y.,   Brooks,   D.   and   Wu,   C.J.,   2020.   Chasing   Carbon:   \nThe   Elusive   Environmental   Footprint   of   Computing.    arXiv   preprint   arXiv:2011.02839 .   \n[Hal20]   Hall,   K.,   May   4,   2020,   An   NLU-Powered   Tool   to   Explore   COVID-19,   \nhttps://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html .   \n[Han15]  Han,   S.,   Pool,   J.,   Tran,   J.   and   Dally,   W.J.,   2015.   Learning   both   weights   and   connections   for   efficient   neural   \nnetworks.   ICLR   2016.     arXiv   preprint   arXiv:1510.00149 .   \n[Hen20]  Henderson,   P.,   Hu,   J.,   Romoff,   J.,   Brunskill,   E.,   Jurafsky,   D.   and   Pineau,   J.,   2020.   Towards   the   systematic   \nreporting   of   the   energy   and   carbon   footprints   of   machine   learning.   Journal   of   Machine   Learning   Research.   \nhttps://jmlr.org/papers/v21/20-312.html   \n[Her20]   Hernandez,   D.   and   Brown,   T.B.,   2020.   Measuring   the   algorithmic   efficiency   of   neural   networks.   arXiv   preprint   \narXiv:2005.04305.    https://arxiv.org/abs/2005.04305 .   \n[Hin15]   Hinton,   G.,   Vinyals,   O.   and   Dean,   J.,   2015.   Distilling   the   knowledge   in   a   neural   network.    arXiv   preprint   \narXiv:1503.02531 .   \n[Höl20]   Hölzle,   U.,   Feb   27,   2020.   datacenters   are   more   energy   efficient   than   ever.   \nblog.google/outreach-initiatives/sustainability/data-centers-energy-efficient     \n[Joh20]   Johnson,   M.,    April   22,   2020,   A   Scalable   Approach   to   Reducing   Gender   Bias   in   Google   Translate,   \nhttps://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html    .", "sentences": [{"text": "References   \n16   \n[Adi20]   Adiwardana,   D.", "metadata": {}}, {"text": ",    Luong,   M.,    R.", "metadata": {}}, {"text": "So,   D.,   Hall,   J.,   Fiedel,   N.,   Thoppilan,   R.,   Yang,   Z.,   Kulshreshtha,   A.,   Nemade,   \nG.,   Lu,   Y.,   and   Le.", "metadata": {}}, {"text": "Q.", "metadata": {}}, {"text": "Towards   a   Human-like   Open-Domain   Chatbot .", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2001.09977 .", "metadata": {}}, {"text": "[Arm10]  Armbrust,   M.,   Fox,   A.,   Griffith,   R.,   Joseph,   A.D.,   Katz,   R.,   Konwinski,   A.,   Lee,   G.,   Patterson,   D.,   Rabkin,   A.,   \nStoica,   I.", "metadata": {}}, {"text": "and   Zaharia,   M.,   2010.", "metadata": {}}, {"text": "A   view   of   cloud   computing.", "metadata": {}}, {"text": "Communications   of   the   ACM,    53(4),   pp.50-58.", "metadata": {}}, {"text": "[Bar19]   Barr,   J.", "metadata": {}}, {"text": "December   3,   2019.", "metadata": {}}, {"text": "Amazon   EC2   Update,   \naws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips   \n-for-high-performance-cost-effective-inferencing/ .", "metadata": {}}, {"text": "[Bro20]   Brown,   T.,   Mann,   B.,   Ryder,   N.,   Subbiah,   M.,   Kaplan,   J.,   Dhariwal,   P.,   Neelakantan,   A.,   Shyam   ,   P.,   Sastry,    G.,   \nAskell,   A.,   Agarwal,   S.,   Herbert-Voss,   A.,   Krueger,   G.,   Henighan,   T.,   Child,   R.,   Ramesh,   A.,   Ziegler,   D.,   Wu,   J.,   \nWinter,   C.,   Hesse,   C.,   Chen,   M.,   Sigler,   E.,   Litwin,   M.,   Gray,   S.,   Chess,   B.,   Clark,   J.,   Berner,   C.,   McCandlish,   S.,   \nRadford,   A.,   Sutskever,   I.,   Amodei,    D.", "metadata": {}}, {"text": "July   22,   2020.", "metadata": {}}, {"text": "Language   models   are   few-shot   learners.", "metadata": {}}, {"text": "NeurIPS   2020.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2005.14165 .", "metadata": {}}, {"text": "[Ben21]  Bender,   E.,   Gebru,   T.,   McMillan-Major,   A.", "metadata": {}}, {"text": "Shmitchell,   S.", "metadata": {}}, {"text": "On   the   Dangers   of   Stochastic   Parrots:   Can   Language   \nModels   Be   Too   Big?", "metadata": {}}, {"text": "FAccT   2021.", "metadata": {}}, {"text": "http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf .", "metadata": {}}, {"text": "[Car21]   Carbon   Offset   Research   and   Education,   2021,   Carbon   Offset   Guide,    https://www.offsetguide.org/ .", "metadata": {}}, {"text": "[Cha19]  Chang,   K.W.,   Prabhakaran,   V.", "metadata": {}}, {"text": "and   Ordonez,   V.,   2019,   November.", "metadata": {}}, {"text": "Bias   and   fairness   in   natural   language   \nprocessing.", "metadata": {}}, {"text": "In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   Processing   and   \nthe   9th   International   Joint   Conference   on   Natural   Language   Processing   (EMNLP-IJCNLP):   Tutorial   Abstracts.", "metadata": {}}, {"text": "https://arxiv.org/pdf/1908.09635.pdf .", "metadata": {}}, {"text": "[Cri21]   Criddle,   C.,   February   10,   2021.", "metadata": {}}, {"text": "Bitcoin   consumes   more   electricity   than   Argentina,   \nwww.bbc.com/news/technology-56012952 .", "metadata": {}}, {"text": "[Dig21]   Digiconomist,   2021,   Bitcoin   Energy   Consumption   Index,    https://digiconomist.net/bitcoin-energy-consumption/    .", "metadata": {}}, {"text": "[Dod19]  Dodge,   J.,   Gururangan,   S.,   Card,   D.,   Schwartz,   R.,   and   Smith,   N.,   2019.", "metadata": {}}, {"text": "Show   Your   Work:   Improved   Reporting   \nof   Experimental   Results.", "metadata": {}}, {"text": "In   Proceedings   of   the   2019   Conference   on   Empirical   Methods   in   Natural   Language   \nProcessing   and   the   9th   International   Joint   Conference   on   Natural   Language   Processing   \n(EMNLP-IJCNLP).", "metadata": {}}, {"text": "www.aclweb.org/anthology/D19-1224/ .", "metadata": {}}, {"text": "[Dod20]  Dodge,   J.,   Ilharco,   G.,   Schwartz,   R.,   Farhadi,   A.,   Hajishirzi,   H.", "metadata": {}}, {"text": "and   Smith,   N.,   2020.", "metadata": {}}, {"text": "Fine-tuning   pretrained   \nlanguage   models:   Weight   initializations,   data   orders,   and   early   stopping.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2002.06305 .", "metadata": {}}, {"text": "[Evo19]    Apache-licensed   Evolved   Transformer   open-source   implementation   in   tensorflow/tensor2tensor   GitHub   \nrepository.", "metadata": {}}, {"text": "https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py   \n[Fed21]   Fedus,   W.,   Zoph,   B.,   Shazeer,   N.,   January   11,   2021,   Switch   Transformers:   Scaling   to   Trillion   Parameter   Models   \nwith   Simple   and   Efficient   Sparsity    https://arxiv.org/abs/2101.03961 .", "metadata": {}}, {"text": "[Gar19]   Garg,   S.,   Perot,   V.,   Limtiaco,   N.,   Taly,   A.,   Chi,   E.H.", "metadata": {}}, {"text": "and   Beutel,   A.,   2019,   January.", "metadata": {}}, {"text": "Counterfactual   fairness   in   text   \nclassification   through   robustness.", "metadata": {}}, {"text": "In   Proceedings   of   the   2019   AAAI/ACM   Conference   on   AI,   Ethics,   and   Society   \n(pp.", "metadata": {}}, {"text": "219-226).", "metadata": {}}, {"text": "https://research.google/pubs/pub47670/    .", "metadata": {}}, {"text": "[Goo16]  Google,   December   2016,   Achieving   Our   100%   Renewable   Energy   Purchasing   Goal   and   Going   Beyond,   \nhttps://static.", "metadata": {}}, {"text": "googleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal \n.pdf .", "metadata": {}}, {"text": "[Goo20]  Google,   Environmental   Report   2020,   \nhttps://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf .", "metadata": {}}, {"text": "[Goo21]  Google,   February   2021,   24/7   Carbon-Free   Energy:   Methodologies   and   Metrics,   \nhttps://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf .", "metadata": {}}, {"text": "[Gup20]  Gupta,   U.,   Kim,   Y.G.,   Lee,   S.,   Tse,   J.,   Lee,   H.H.S.,   Wei,   G.Y.,   Brooks,   D.", "metadata": {}}, {"text": "and   Wu,   C.J.,   2020.", "metadata": {}}, {"text": "Chasing   Carbon:   \nThe   Elusive   Environmental   Footprint   of   Computing.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2011.02839 .", "metadata": {}}, {"text": "[Hal20]   Hall,   K.,   May   4,   2020,   An   NLU-Powered   Tool   to   Explore   COVID-19,   \nhttps://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html .", "metadata": {}}, {"text": "[Han15]  Han,   S.,   Pool,   J.,   Tran,   J.", "metadata": {}}, {"text": "and   Dally,   W.J.,   2015.", "metadata": {}}, {"text": "Learning   both   weights   and   connections   for   efficient   neural   \nnetworks.", "metadata": {}}, {"text": "ICLR   2016.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1510.00149 .", "metadata": {}}, {"text": "[Hen20]  Henderson,   P.,   Hu,   J.,   Romoff,   J.,   Brunskill,   E.,   Jurafsky,   D.", "metadata": {}}, {"text": "and   Pineau,   J.,   2020.", "metadata": {}}, {"text": "Towards   the   systematic   \nreporting   of   the   energy   and   carbon   footprints   of   machine   learning.", "metadata": {}}, {"text": "Journal   of   Machine   Learning   Research.", "metadata": {}}, {"text": "https://jmlr.org/papers/v21/20-312.html   \n[Her20]   Hernandez,   D.", "metadata": {}}, {"text": "and   Brown,   T.B.,   2020.", "metadata": {}}, {"text": "Measuring   the   algorithmic   efficiency   of   neural   networks.", "metadata": {}}, {"text": "arXiv   preprint   \narXiv:2005.04305.", "metadata": {}}, {"text": "https://arxiv.org/abs/2005.04305 .", "metadata": {}}, {"text": "[Hin15]   Hinton,   G.,   Vinyals,   O.", "metadata": {}}, {"text": "and   Dean,   J.,   2015.", "metadata": {}}, {"text": "Distilling   the   knowledge   in   a   neural   network.", "metadata": {}}, {"text": "arXiv   preprint   \narXiv:1503.02531 .", "metadata": {}}, {"text": "[Höl20]   Hölzle,   U.,   Feb   27,   2020.", "metadata": {}}, {"text": "datacenters   are   more   energy   efficient   than   ever.", "metadata": {}}, {"text": "blog.google/outreach-initiatives/sustainability/data-centers-energy-efficient     \n[Joh20]   Johnson,   M.,    April   22,   2020,   A   Scalable   Approach   to   Reducing   Gender   Bias   in   Google   Translate,   \nhttps://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html    .", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "17   \n[Jou21]   Jouppi,   N.,   Yoon,   D-H,   Jablin,   T.,   Kurian,   G.,   Laudon,   J.,   Li,   S.,   Ma,   P.,   Ma,   X.,   Patil,   N.,Prasad,   S.,   Young,   C.,   \nZhou,   Z.,   and   Patterson,   D.,   May   2021.   Ten   Lessons   From   Three   Generations   Shaped   Google’s   TPUv4i,   to  \nappear,    the   48th   International   Symposium   on   Computer   Architecture.   \n[Kap20]  Kaplan,   J.,   McCandlish,   S.,   Henighan,   T.,   Brown,   T.B.,   Chess,   B.,   Child,   R.,   Gray,   S.,   Radford,   A.,   Wu,   J.   and   \nAmodei,   D.,   2020.   Scaling   laws   for   neural   language   models.   arXiv   preprint   arXiv:2001.08361.   \n[Kär18]   Kärcher   B.   Formation   and   radiative   forcing   of   contrail   cirrus.    Nature   communication s.   2018   May   8;9(1):1-7.   \nhttps://www.nature.com/articles/s41467-018-04068-0 .   \n[Kuc18]   Kuczmarski,   J.   and   Johnson,   M.,   2018.   Gender-aware   natural   language   \ntranslation. www.tdcommons.org/dpubs_series/1577/ .   \n[Lac19]   Lacoste,   A.,   Luccioni,   A.,   Schmidt,   V.   and   Dandres,   T.,   2019.   Quantifying   the   carbon   emissions   of   machine   \nlearning.    arXiv   preprint   arXiv:1910.09700 .   \n[Lan20]   Lannelongue,   L.,   Grealey,   J.   and   Inouye,   M.,   2020.   Green   algorithms:   Quantifying   the   carbon   footprint   of   \ncomputation.    arXiv:   2007.07610 .   \n[Leo19]   Leopold,   G.   March   19,   2019,   AWS   to   Offer   Nvidia’s   T4   GPUs   for   AI   Inferencing,   \nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/    .   \n[Lep20]   Lepikhin,   D.,   Lee,   H.,   Xu,   Y.,   Chen,   D.,   Firat,   O.,   Huang,   Y.,   Krikun,   M.,   Shazeer,   N.   and   Chen,   Z.,   2020.   GShard:  \nScaling   giant   models   with   conditional   computation   and   automatic   sharding.    arXiv   preprint   arXiv:2006.16668 .   \n[Li21]   Li,   S.,   Tan,   M.,   Pang,   R.,   Li,   A.,   Cheng,   L.,   Le,   Q.   and   Jouppi,   N.P.,   2021.   Searching   for   Fast   Model   Families   on   \nDatacenter   Accelerators.    arXiv   preprint   arXiv:2102.05610 .   \n[Liu18]   Liu,   H.,   Simonyan,   K.   and   Yang,   Y.,   2018.   Darts:   Differentiable   architecture   search.    arXiv   preprint   \narXiv:1806.09055 .   \n[Luc21]   Luccioni,   A.,   and   Schmidt,   V..   March   2021,   Private   Communication.   \n[Mas20]  Masanet,   E.,   Shehabi,   A.,   Lei,   N.,   Smith,   S.   and   Koomey,   J.,   2020.   Recalibrating   global   datacenter   energy-use   \nestimates.    Science ,   367(6481),   pp.984-986.   \nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf .   \n[Mas21]  Masanet,   E.,   March   24,   2021,    Data   Center   Energy   Analysis:   Past,   Present,   and   Future ,   lecture   at   UCSB.   \n[Mer19]   Mehrabi,   N.,   Morstatter,   F.,   Saxena,   N.,   Lerman,   K.   and   Galstyan,   A.,   2019.   A   survey   on   bias   and   fairness   in   \nmachine   learning.   arXiv   preprint   arXiv:1908.09635.    https://arxiv.org/pdf/1908.09635.pdf .   \n[Pha18]  Pham,   H.,   Guan,   M.,   Zoph,   B.,   Le,   Q.   and   Dean,   J.,   2018,   July.   Efficient   neural   architecture   search   via   \nparameters   sharing.   In   International   Conference   on   Machine   Learning   (pp.   4095-4104).   PMLR.     arXiv   preprint   \narXiv:1802.03268 .   \n[Rad20]  Radovanovic,   A.   April   22,   2020,   Our   datacenters   now   work   harder   when   the   sun   shines   and   wind   blows,   \nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows   \n[Raf19]   Raffel,   C.,   Shazeer,   N.,   Roberts,   A.,   Lee,   K.,   Narang,   S.,   Matena,   M.,   Zhou,   Y.,   Li,   W.   and   Liu,   P.J.,   2019.   \nExploring   the   limits   of   transfer   learning   with   a   unified   text-to-text   transformer.    arXiv   preprint   arXiv:1910.10683 .   \n[Rit20]   Ritchie,   H.,   October   22,   2020,   Climate   change   and   flying:   what   share   of   global   CO2   emissions   come   from   \naviation?    https://ourworldindata.org/co2-emissions-from-aviation    .   \n[Ryo14]  Ryor,   J.N.   and   Tawney,   L.E.T.H.A.,   2014.   Utility-Scale   Renewable   Energy:   Understanding   Cost   Parity.   Paris:   \nWorld   Resources   Institute.   \nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf .   \n[San20]  Sanh,   V.,   Debut,   L.,   Chaumond,   J.   and   Wolf,   T.,   2019.   DistilBERT,   a   distilled   version   of   BERT:   smaller,   faster,   \ncheaper   and   lighter.    arXiv   preprint   arXiv:1910.01108 .   \n[Sch20]   Schwartz,   R.,   Dodge,   J.,   Smith,   N.A.   and   Etzioni,   O.,   2020.   Green   AI.    Communications   of   the   ACM ,   63(12),   \npp.54-63.    https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext .   \n[Sch21]   Schleier-Smith,   J.,    Sreekanti,   V.,   Khandelwal,   A.,   Carreira,   J.,   Yadwadkar,   N.,   Popa,   R.,    Joseph   E.   Gonzalez,J.,  \nIon   Stoica,   I.,   and   David   A.   Patterson,   D.,   2021   What   Serverless   Computing   Is   and   Should   Become:   The   Next   \nPhase   of   Cloud   Computing,    Communications   of   the   ACM,    64(5) .     \n[Sha17]  Shazeer,   N.,   Mirhoseini,   A.,   Maziarz,   K.,   Davis,   A.,   Le,   Q.,   Hinton,   G.   and   Dean,   J.,   2017.   Outrageously   large   \nneural   networks:   The   sparsely-gated   mixture-of-experts   layer.   ICLR   2017.     arXiv   preprint   arXiv:1701.06538 .   \n[So19]   So,   D.,   Le,   Q.   and   Liang,   C.,   2019,   May.   The   Evolved   Transformer.   In   International   Conference   on   Machine   \nLearning   2019   (pp.   5877-5886).   PMLR.     arXiv   preprint   arXiv:1901.11117 .   \n[Str19]   Strubell,   E.,   Ganesh,   A.   and   McCallum,   A.,   2019.   Energy   and   policy   considerations   for   deep   learning   in   NLP.   \nACL   2019.     arXiv   preprint   arXiv:1906.02243 .   \n[Sut21]   Sutskever,   I.   Personal   Communication,   February   4,   2021.   \n[Tan19]   Tan,   M.   and   Le,   Q.,   2019,   May.   EfficientNet:   Rethinking   model   scaling   for   convolutional   neural   networks.   In   \nInternational   Conference   on   Machine   Learning   (pp.   6105-6114).   PMLR.    arXiv   preprint   arXiv:1905.11946 .   \n[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.   electricity   generation?    https://www.eia.gov/tools/faqs/faq.php?id=74&t=11 .   \n[Vas17]   Vaswani,   A.,   Shazeer,   N.,   Parmar,   N.,   Uszkoreit,   J.,   Jones,   L.,   Gomez,   A.N.,   Kaiser,   L.   and   Polosukhin,   I.,   2017.   \nAttention   is   all   you   need.   NeurIPS   2017.     arXiv   preprint   arXiv:1706.03762 .   \n[Wan20]  Wang,   Y.,   Yao,   Q.,   Kwok,   J.T.   and   Ni,   L.M.,   2020.   Generalizing   from   a   few   examples:   A   survey   on   few-shot   \nlearning.    ACM   Computing   Surveys ,   53(3),   pp.1-34.", "sentences": [{"text": "17   \n[Jou21]   Jouppi,   N.,   Yoon,   D-H,   Jablin,   T.,   Kurian,   G.,   Laudon,   J.,   Li,   S.,   Ma,   P.,   Ma,   X.,   Patil,   N.,Prasad,   S.,   Young,   C.,   \nZhou,   Z.,   and   Patterson,   D.,   May   2021.", "metadata": {}}, {"text": "Ten   Lessons   From   Three   Generations   Shaped   Google’s   TPUv4i,   to  \nappear,    the   48th   International   Symposium   on   Computer   Architecture.", "metadata": {}}, {"text": "[Kap20]  Kaplan,   J.,   McCandlish,   S.,   Henighan,   T.,   Brown,   T.B.,   Chess,   B.,   Child,   R.,   Gray,   S.,   Radford,   A.,   Wu,   J.", "metadata": {}}, {"text": "and   \nAmodei,   D.,   2020.", "metadata": {}}, {"text": "Scaling   laws   for   neural   language   models.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2001.08361.", "metadata": {}}, {"text": "[Kär18]   Kärcher   B.", "metadata": {}}, {"text": "Formation   and   radiative   forcing   of   contrail   cirrus.", "metadata": {}}, {"text": "Nature   communication s.", "metadata": {}}, {"text": "2018   May   8;9(1):1-7.", "metadata": {}}, {"text": "https://www.nature.com/articles/s41467-018-04068-0 .", "metadata": {}}, {"text": "[Kuc18]   Kuczmarski,   J.", "metadata": {}}, {"text": "and   Johnson,   M.,   2018.", "metadata": {}}, {"text": "Gender-aware   natural   language   \ntranslation.", "metadata": {}}, {"text": "www.tdcommons.org/dpubs_series/1577/ .", "metadata": {}}, {"text": "[Lac19]   Lacoste,   A.,   Luccioni,   A.,   Schmidt,   V.", "metadata": {}}, {"text": "and   Dandres,   T.,   2019.", "metadata": {}}, {"text": "Quantifying   the   carbon   emissions   of   machine   \nlearning.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1910.09700 .", "metadata": {}}, {"text": "[Lan20]   Lannelongue,   L.,   Grealey,   J.", "metadata": {}}, {"text": "and   Inouye,   M.,   2020.", "metadata": {}}, {"text": "Green   algorithms:   Quantifying   the   carbon   footprint   of   \ncomputation.", "metadata": {}}, {"text": "arXiv:   2007.07610 .", "metadata": {}}, {"text": "[Leo19]   Leopold,   G.", "metadata": {}}, {"text": "March   19,   2019,   AWS   to   Offer   Nvidia’s   T4   GPUs   for   AI   Inferencing,   \nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/    .", "metadata": {}}, {"text": "[Lep20]   Lepikhin,   D.,   Lee,   H.,   Xu,   Y.,   Chen,   D.,   Firat,   O.,   Huang,   Y.,   Krikun,   M.,   Shazeer,   N.", "metadata": {}}, {"text": "and   Chen,   Z.,   2020.", "metadata": {}}, {"text": "GShard:  \nScaling   giant   models   with   conditional   computation   and   automatic   sharding.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2006.16668 .", "metadata": {}}, {"text": "[Li21]   Li,   S.,   Tan,   M.,   Pang,   R.,   Li,   A.,   Cheng,   L.,   Le,   Q.", "metadata": {}}, {"text": "and   Jouppi,   N.P.,   2021.", "metadata": {}}, {"text": "Searching   for   Fast   Model   Families   on   \nDatacenter   Accelerators.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:2102.05610 .", "metadata": {}}, {"text": "[Liu18]   Liu,   H.,   Simonyan,   K.", "metadata": {}}, {"text": "and   Yang,   Y.,   2018.", "metadata": {}}, {"text": "Darts:   Differentiable   architecture   search.", "metadata": {}}, {"text": "arXiv   preprint   \narXiv:1806.09055 .", "metadata": {}}, {"text": "[Luc21]   Luccioni,   A.,   and   Schmidt,   V..", "metadata": {}}, {"text": "March   2021,   Private   Communication.", "metadata": {}}, {"text": "[Mas20]  Masanet,   E.,   Shehabi,   A.,   Lei,   N.,   Smith,   S.", "metadata": {}}, {"text": "and   Koomey,   J.,   2020.", "metadata": {}}, {"text": "Recalibrating   global   datacenter   energy-use   \nestimates.", "metadata": {}}, {"text": "Science ,   367(6481),   pp.984-986.", "metadata": {}}, {"text": "https://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf .", "metadata": {}}, {"text": "[Mas21]  Masanet,   E.,   March   24,   2021,    Data   Center   Energy   Analysis:   Past,   Present,   and   Future ,   lecture   at   UCSB.", "metadata": {}}, {"text": "[Mer19]   Mehrabi,   N.,   Morstatter,   F.,   Saxena,   N.,   Lerman,   K.", "metadata": {}}, {"text": "and   Galstyan,   A.,   2019.", "metadata": {}}, {"text": "A   survey   on   bias   and   fairness   in   \nmachine   learning.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1908.09635.", "metadata": {}}, {"text": "https://arxiv.org/pdf/1908.09635.pdf .", "metadata": {}}, {"text": "[Pha18]  Pham,   H.,   Guan,   M.,   Zoph,   B.,   Le,   Q.", "metadata": {}}, {"text": "and   Dean,   J.,   2018,   July.", "metadata": {}}, {"text": "Efficient   neural   architecture   search   via   \nparameters   sharing.", "metadata": {}}, {"text": "In   International   Conference   on   Machine   Learning   (pp.", "metadata": {}}, {"text": "4095-4104).", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "arXiv   preprint   \narXiv:1802.03268 .", "metadata": {}}, {"text": "[Rad20]  Radovanovic,   A.", "metadata": {}}, {"text": "April   22,   2020,   Our   datacenters   now   work   harder   when   the   sun   shines   and   wind   blows,   \nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows   \n[Raf19]   Raffel,   C.,   Shazeer,   N.,   Roberts,   A.,   Lee,   K.,   Narang,   S.,   Matena,   M.,   Zhou,   Y.,   Li,   W.", "metadata": {}}, {"text": "and   Liu,   P.J.,   2019.", "metadata": {}}, {"text": "Exploring   the   limits   of   transfer   learning   with   a   unified   text-to-text   transformer.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1910.10683 .", "metadata": {}}, {"text": "[Rit20]   Ritchie,   H.,   October   22,   2020,   Climate   change   and   flying:   what   share   of   global   CO2   emissions   come   from   \naviation?", "metadata": {}}, {"text": "https://ourworldindata.org/co2-emissions-from-aviation    .", "metadata": {}}, {"text": "[Ryo14]  Ryor,   J.N.", "metadata": {}}, {"text": "and   Tawney,   L.E.T.H.A.,   2014.", "metadata": {}}, {"text": "Utility-Scale   Renewable   Energy:   Understanding   Cost   Parity.", "metadata": {}}, {"text": "Paris:   \nWorld   Resources   Institute.", "metadata": {}}, {"text": "https://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf .", "metadata": {}}, {"text": "[San20]  Sanh,   V.,   Debut,   L.,   Chaumond,   J.", "metadata": {}}, {"text": "and   Wolf,   T.,   2019.", "metadata": {}}, {"text": "DistilBERT,   a   distilled   version   of   BERT:   smaller,   faster,   \ncheaper   and   lighter.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1910.01108 .", "metadata": {}}, {"text": "[Sch20]   Schwartz,   R.,   Dodge,   J.,   Smith,   N.A.", "metadata": {}}, {"text": "and   Etzioni,   O.,   2020.", "metadata": {}}, {"text": "Green   AI.", "metadata": {}}, {"text": "Communications   of   the   ACM ,   63(12),   \npp.54-63.", "metadata": {}}, {"text": "https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext .", "metadata": {}}, {"text": "[Sch21]   Schleier-Smith,   J.,    Sreekanti,   V.,   Khandelwal,   A.,   Carreira,   J.,   Yadwadkar,   N.,   Popa,   R.,    Joseph   E.", "metadata": {}}, {"text": "Gonzalez,J.,  \nIon   Stoica,   I.,   and   David   A.", "metadata": {}}, {"text": "Patterson,   D.,   2021   What   Serverless   Computing   Is   and   Should   Become:   The   Next   \nPhase   of   Cloud   Computing,    Communications   of   the   ACM,    64(5) .", "metadata": {}}, {"text": "[Sha17]  Shazeer,   N.,   Mirhoseini,   A.,   Maziarz,   K.,   Davis,   A.,   Le,   Q.,   Hinton,   G.", "metadata": {}}, {"text": "and   Dean,   J.,   2017.", "metadata": {}}, {"text": "Outrageously   large   \nneural   networks:   The   sparsely-gated   mixture-of-experts   layer.", "metadata": {}}, {"text": "ICLR   2017.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1701.06538 .", "metadata": {}}, {"text": "[So19]   So,   D.,   Le,   Q.", "metadata": {}}, {"text": "and   Liang,   C.,   2019,   May.", "metadata": {}}, {"text": "The   Evolved   Transformer.", "metadata": {}}, {"text": "In   International   Conference   on   Machine   \nLearning   2019   (pp.", "metadata": {}}, {"text": "5877-5886).", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1901.11117 .", "metadata": {}}, {"text": "[Str19]   Strubell,   E.,   Ganesh,   A.", "metadata": {}}, {"text": "and   McCallum,   A.,   2019.", "metadata": {}}, {"text": "Energy   and   policy   considerations   for   deep   learning   in   NLP.", "metadata": {}}, {"text": "ACL   2019.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1906.02243 .", "metadata": {}}, {"text": "[Sut21]   Sutskever,   I.", "metadata": {}}, {"text": "Personal   Communication,   February   4,   2021.", "metadata": {}}, {"text": "[Tan19]   Tan,   M.", "metadata": {}}, {"text": "and   Le,   Q.,   2019,   May.", "metadata": {}}, {"text": "EfficientNet:   Rethinking   model   scaling   for   convolutional   neural   networks.", "metadata": {}}, {"text": "In   \nInternational   Conference   on   Machine   Learning   (pp.", "metadata": {}}, {"text": "6105-6114).", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1905.11946 .", "metadata": {}}, {"text": "[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.", "metadata": {}}, {"text": "electricity   generation?", "metadata": {}}, {"text": "https://www.eia.gov/tools/faqs/faq.php?id=74&t=11 .", "metadata": {}}, {"text": "[Vas17]   Vaswani,   A.,   Shazeer,   N.,   Parmar,   N.,   Uszkoreit,   J.,   Jones,   L.,   Gomez,   A.N.,   Kaiser,   L.", "metadata": {}}, {"text": "and   Polosukhin,   I.,   2017.", "metadata": {}}, {"text": "Attention   is   all   you   need.", "metadata": {}}, {"text": "NeurIPS   2017.", "metadata": {}}, {"text": "arXiv   preprint   arXiv:1706.03762 .", "metadata": {}}, {"text": "[Wan20]  Wang,   Y.,   Yao,   Q.,   Kwok,   J.T.", "metadata": {}}, {"text": "and   Ni,   L.M.,   2020.", "metadata": {}}, {"text": "Generalizing   from   a   few   examples:   A   survey   on   few-shot   \nlearning.", "metadata": {}}, {"text": "ACM   Computing   Surveys ,   53(3),   pp.1-34.", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "Appendix   A.   Details   of   CO 2    Estimates   for   Four   Large   NLP   Models   in   Tables   1   and   4   \nWe   describe   below   how   we   derived   the   values   in   Tables   1   and   4.     \n● Datacenter   Gross   CO 2 e/KWh   (Table   1,   row   4;   Table   4,   row   7):    The   US   Average   is   from   [USE21].   For   \nGoogle,   we   used   the   CO 2 e   per   KWh   in   the   datacenter   based   at   the   time   that   the   DNNs   ran.     ( Here   is   a   \nlink   for   annual   CFE%   for   Google   Cloud .)   For   Microsoft,   we   use   the   2020   US   national   average.     \n● Datacenter   Net   CO 2 e/KWh   (Table   1,   row   5;   Table   4,   row   8):    No   change   from   above   except   for   Google,  \nwhere   we   used   the   net   CO 2 e   per   KWh   in   the   datacenter   based   on   the   24/7   carbon-free   energy   \nmethodology   to   estimate   net   carbon   emissions   at   the   time 28    that   the   DNNs   ran   (see   Section   2.4   and   \nAppendix   B).    \n● PUE   (Table   1,   row   6;   Table   4,   row   9) :   We   use   the   Google   datacenter   PUE   where   the   DNNs   ran   \n(published   at    https://www.google.com/about/datacenters/efficiency/ ).    OpenAI   told   us   that   the   PUE   for   \nthe   datacenter   where   GPT-3   ran   was   1.10   [Sut21].   \n● Measured   Average   Power   (Table   1,   row   9;   Table   4,   row   12) :   At   Google   we   measured   actual   power   \nusage   rather   than   use   Thermal   Design   Power   (TDP),   as   TDP   is   a   worst   case   for   the   chip.   System   \npower   measurement   includes   the   memory,   fans,   CPU   host,   network   interface   and   so   on,   similar   to   the   \nmethodology   of   [Str19].   OpenAI   measured   V100s   as   running   GPT-3   at   330W.   GPUs   can   run   on   \naverage   closer   to   its   TDP   due   to   GPU's   having   Turbo   Mode   and   Dynamic   Voltage   Frequency   Scaling,   \nnot   found   in   TPU   v2/v3.     \n● Measured   Performance   (Table   1,   row   10;   Table   4,   row   13):    Profiling   data   was   obtained   via   Google's   \ninternal   performance   analysis   tool,   Xprof.   Measured   FLOPs/s   are   calculated   as   the   number   of   \ncomputed   operations   divided   by   execution   time.   \n● Number   of   Chips   (Table   1,   row   11;   Table   4,   row   14) :   We   know   the   number   of   processors   for   the   Google   \nmodels.    NVIDIA’s   press   release   about   GPT-3   suggests   OpenAI   used   10,000   V100   GPUs   for   GPT-3 .   \n● Training   time   (Table   1,   row   12;   Table   4,   row   15) :   We   have   the   exact   training   time   for   Google   DNNs.   \nOpenAI   published   the   total   number   of   floating   point   operations   to   train   their   model:   3.14E+23   [Bro20].   \nOpenAI   told   us   the   V100   runs   GPT-3   at   24.6   TeraFLOPS/sec   [Sut21].   It   takes   ~14.8   days   for   10,000   \nGPUs   at   24.6   TeraFLOPS/sec   to   compute   3.14E+23   FLOPS.   For   the   CO 2 e   calculation,   it   doesn’t   \nactually   matter   whether   it   takes   2   weeks   on   10,000   GPUs   or   20   weeks   on   1,000   GPUs,   but   we   need   \none   number   for   Table   4,   so   we   used   NVIDIA’s   suggestion   of   10,000   GPUs.   \n● Total   Computation   (Table   1,   row   13;   Table   4,   row   16):    We   calculate   from   measured   performance,   \nnumber   of   chips,   and   days   to   train   (except   for   GPT-3,   as   OpenAI   published   the   total   FLOPS).   \n● %   of   Google   2019   Energy   Consumption.   (Table   4,   row   17):    For   all   models   (even   those   not   actually   run   \nin   Google   datacenters   or   not   run   in   2019),   we   calculate   the   percentage   of   Google’s   total   energy   \nconsumption   of   12.2   Terawatt-hours   in   2019   [Goo20].     \n● Ratio   of   round   trips   (Table   4,   row   22) .   To   give   perspective   on   the   CO 2 e   cost   of   training   a   model   is   \ncompared   to   other   activities,   we   show   the   CO 2 e   of   passenger   jets.    Google   Flights    calculated   the   \naverage   CO 2    emission   for   all   the   direct   flights   between   San   Francisco   (SFO)   and   New   York   (JFK)   in   its   \ndatabase   as   90.2t,   so   the   average   round   trip   is   180.4t.   (This   is   for   the   whole   plane,   not   just   for   one   \npassenger.)   Google   Flights   relies   on   this    European   Environmental   Agency   guidebook    for   these   \ncalculations   and   includes   the   minimum   bounds   for   RF   and   NOx   factor   from   Figure   6b   in   [Kär18].   \n● %   Carbon   Free   Energy   (Table   1,   row   17;   Table   4,   row   24) .   Collected   for   when   the   models   were   run.", "sentences": [{"text": "Appendix   A.", "metadata": {}}, {"text": "Details   of   CO 2    Estimates   for   Four   Large   NLP   Models   in   Tables   1   and   4   \nWe   describe   below   how   we   derived   the   values   in   Tables   1   and   4.", "metadata": {}}, {"text": "● Datacenter   Gross   CO 2 e/KWh   (Table   1,   row   4;", "metadata": {}}, {"text": "Table   4,   row   7):    The   US   Average   is   from   [USE21].", "metadata": {}}, {"text": "For   \nGoogle,   we   used   the   CO 2 e   per   KWh   in   the   datacenter   based   at   the   time   that   the   DNNs   ran.", "metadata": {}}, {"text": "( Here   is   a   \nlink   for   annual   CFE%   for   Google   Cloud .)   For   Microsoft,   we   use   the   2020   US   national   average.", "metadata": {}}, {"text": "● Datacenter   Net   CO 2 e/KWh   (Table   1,   row   5;", "metadata": {}}, {"text": "Table   4,   row   8):    No   change   from   above   except   for   Google,  \nwhere   we   used   the   net   CO 2 e   per   KWh   in   the   datacenter   based   on   the   24/7   carbon-free   energy   \nmethodology   to   estimate   net   carbon   emissions   at   the   time 28    that   the   DNNs   ran   (see   Section   2.4   and   \nAppendix   B).", "metadata": {}}, {"text": "● PUE   (Table   1,   row   6;", "metadata": {}}, {"text": "Table   4,   row   9) :   We   use   the   Google   datacenter   PUE   where   the   DNNs   ran   \n(published   at    https://www.google.com/about/datacenters/efficiency/ ).", "metadata": {}}, {"text": "OpenAI   told   us   that   the   PUE   for   \nthe   datacenter   where   GPT-3   ran   was   1.10   [Sut21].", "metadata": {}}, {"text": "● Measured   Average   Power   (Table   1,   row   9;", "metadata": {}}, {"text": "Table   4,   row   12) :   At   Google   we   measured   actual   power   \nusage   rather   than   use   Thermal   Design   Power   (TDP),   as   TDP   is   a   worst   case   for   the   chip.", "metadata": {}}, {"text": "System   \npower   measurement   includes   the   memory,   fans,   CPU   host,   network   interface   and   so   on,   similar   to   the   \nmethodology   of   [Str19].", "metadata": {}}, {"text": "OpenAI   measured   V100s   as   running   GPT-3   at   330W.", "metadata": {}}, {"text": "GPUs   can   run   on   \naverage   closer   to   its   TDP   due   to   GPU's   having   Turbo   Mode   and   Dynamic   Voltage   Frequency   Scaling,   \nnot   found   in   TPU   v2/v3.", "metadata": {}}, {"text": "● Measured   Performance   (Table   1,   row   10;", "metadata": {}}, {"text": "Table   4,   row   13):    Profiling   data   was   obtained   via   Google's   \ninternal   performance   analysis   tool,   Xprof.", "metadata": {}}, {"text": "Measured   FLOPs/s   are   calculated   as   the   number   of   \ncomputed   operations   divided   by   execution   time.", "metadata": {}}, {"text": "● Number   of   Chips   (Table   1,   row   11;", "metadata": {}}, {"text": "Table   4,   row   14) :   We   know   the   number   of   processors   for   the   Google   \nmodels.", "metadata": {}}, {"text": "NVIDIA’s   press   release   about   GPT-3   suggests   OpenAI   used   10,000   V100   GPUs   for   GPT-3 .", "metadata": {}}, {"text": "● Training   time   (Table   1,   row   12;", "metadata": {}}, {"text": "Table   4,   row   15) :   We   have   the   exact   training   time   for   Google   DNNs.", "metadata": {}}, {"text": "OpenAI   published   the   total   number   of   floating   point   operations   to   train   their   model:   3.14E+23   [Bro20].", "metadata": {}}, {"text": "OpenAI   told   us   the   V100   runs   GPT-3   at   24.6   TeraFLOPS/sec   [Sut21].", "metadata": {}}, {"text": "It   takes   ~14.8   days   for   10,000   \nGPUs   at   24.6   TeraFLOPS/sec   to   compute   3.14E+23   FLOPS.", "metadata": {}}, {"text": "For   the   CO 2 e   calculation,   it   doesn’t   \nactually   matter   whether   it   takes   2   weeks   on   10,000   GPUs   or   20   weeks   on   1,000   GPUs,   but   we   need   \none   number   for   Table   4,   so   we   used   NVIDIA’s   suggestion   of   10,000   GPUs.", "metadata": {}}, {"text": "● Total   Computation   (Table   1,   row   13;", "metadata": {}}, {"text": "Table   4,   row   16):    We   calculate   from   measured   performance,   \nnumber   of   chips,   and   days   to   train   (except   for   GPT-3,   as   OpenAI   published   the   total   FLOPS).", "metadata": {}}, {"text": "● %   of   Google   2019   Energy   Consumption.", "metadata": {}}, {"text": "(Table   4,   row   17):    For   all   models   (even   those   not   actually   run   \nin   Google   datacenters   or   not   run   in   2019),   we   calculate   the   percentage   of   Google’s   total   energy   \nconsumption   of   12.2   Terawatt-hours   in   2019   [Goo20].", "metadata": {}}, {"text": "● Ratio   of   round   trips   (Table   4,   row   22) .", "metadata": {}}, {"text": "To   give   perspective   on   the   CO 2 e   cost   of   training   a   model   is   \ncompared   to   other   activities,   we   show   the   CO 2 e   of   passenger   jets.", "metadata": {}}, {"text": "Google   Flights    calculated   the   \naverage   CO 2    emission   for   all   the   direct   flights   between   San   Francisco   (SFO)   and   New   York   (JFK)   in   its   \ndatabase   as   90.2t,   so   the   average   round   trip   is   180.4t.", "metadata": {}}, {"text": "(This   is   for   the   whole   plane,   not   just   for   one   \npassenger.)   Google   Flights   relies   on   this    European   Environmental   Agency   guidebook    for   these   \ncalculations   and   includes   the   minimum   bounds   for   RF   and   NOx   factor   from   Figure   6b   in   [Kär18].", "metadata": {}}, {"text": "● %   Carbon   Free   Energy   (Table   1,   row   17;", "metadata": {}}, {"text": "Table   4,   row   24) .", "metadata": {}}, {"text": "Collected   for   when   the   models   were   run.", "metadata": {}}], "metadata": {"page": 18}}, {"text": "28   All   the   2020   datacenter   measurements   are   provisional,   awaiting   final   validation   in   May   2021   \n18", "sentences": [{"text": "28   All   the   2020   datacenter   measurements   are   provisional,   awaiting   final   validation   in   May   2021   \n18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Appendix   B.   Carbon   Offset   and   24/7   Carbon   Free   Energy   \nWhile   energy   consumption   is   relatively   straightforward,   policies   to   reduce   carbon   footprint   are   not.   One   reason   \nis   that   they   have   as   much   to   do   about   economics   and   accounting   as   they   do   about   physics.   This   short   \nappendix   tries   to   clarify   the   distinction   between   conventional   carbon   offsets,   Google’s   goal   for   2030   of   24/7   \nCarbon   Free   Energy   (CFE)   for   its   global   datacenters   and   campuses,   and   what   it   is   doing   in   2021   to   set   the   \ngroundwork   for   2030.   Readers   interested   in   greater   depth   should   take   a   look   at   [Ryo14,   Goog16,   Goo21].   \nConventional   carbon   offsets   try   to   create   economic   incentives   to   create   projects   that   avoid   or   remove   \nCO 2 e.   When   pursuing   the   mitigation   of   carbon   emissions   from   electricity   production   and   consumption,   a   \ncompany   can   match   their   MWh   of   consumption   with   MWh   of   clean   energy   through   certificates   called    REC s  \n( Renewable   Energy   Certificates ).   The   rules   for   accounting   and   compensation,   are   defined   as   part   of   the    GHG  \nProtocol ,   under   Scope   2   for   electricity.   Under   the   current   Scope   2   Guidance,   1MWh   of   energy   used   in   July   in,   \nsay,   Georgia   that   produces   carbon   dioxide   can   be   compensated   by   purchasing   1MWh   of   CFE   in   Montana   in   \nNovember.   Typically,   the   period   of   accounting   is   a   calendar   year.   Google   achieved   carbon   neutrality   using   \nconventional   carbon   offsets   starting   in   2007. 29     \nAs   part   of   the    GHG   Protocol ,   the    World   Resource   Institute    defines   terms   and   economic   mechanisms   to   \nensure   consistency   of   claims   about   carbon.   They   defined   the   following   [Car21,   Ryo14]   (also   see   Figure   8):   \n● Additionality :   CO 2 e   reductions   are    additional    if   they   would   not   have   occurred   in   the   absence   of   a   market   \nfor   offset   credits.   Additionality   is   essential   for   the   quality   of   carbon   offset   credits—if   their   associated   \nCO 2 e   reductions   are   not   additional,   then   purchasing   offset   credits   in   lieu   of   reducing   your   own   \nemissions   will   make   climate   change   worse.   \n● The   Grid :   The   transmission   and   distribution   system   that   connects   generators   and   end-users.   \n● Levelized   Cost   Of   Energy   (LCOE) :   The   projected   total   system   and   operating   costs   divided   by   total   KWh   \nproduced   over   the   lifetime   of   the   project   or   contract.   \n● Power   Purchase   Agreement   (PPA) :   A   fixed-price   contractual   agreement   to   purchase   a   power   plant’s   \nenergy,   typically   calculated   using   LCOE.   \n● Renewable   Energy   Certificate   (REC ) 30 :   A   market-based   instrument   that   represents   the   property   rights   \nto   the   environmental,   social,   and   other   non-power   attributes   of   renewable   electricity   generation.   The   \ngoal   is   a   certificate   that   ensures   the   energy   purchased   is   genuinely   renewable   and   not   double   counted.   \nGoogle’s   target   for   2030   is   to   go   beyond   the   traditional   Scope   2   rules   to   restrict   both   the   location   and   the   \naccounting   period.     \n● Instead   of   anywhere   in   a   continent,   the   CFE   purchase   should   be   on   the   same   geographically   local   grid.     \n● Instead   of   the   accounting   period   being   one   year,   the   accounting   should   be   within   the   hour.   \nTo   achieve   100%   24/7   local   CFE,   grids   would   need   to   offer   both   real   time   accounting   of   the   CFE   fraction   of   the   \nstandard   grid   and   the   generating   companies   must   offer   more   flexible   options   to   allow   consumers   to   pick   CFE   \nany   time   of   the   day,   not   just   when   the   wind   blows   or   when   the   sun   shines.   Ideally,   grid   operators   and   \ngenerating   companies   will   deliver   on   that   vision,   and   the   standards   will   evolve   to   certify   and   quantify   the   24/7   \nCFE   approach.   But   we   are   not   there   yet.   \nFigure   8   helps   explain   what   Google   is   doing   today.   Google   signs   long-term   contracts   as   PPAs   with   \nrenewable   energy   generating   companies   to   try   to   cover   Google’s   electricity   consumption. 31    One   benefit   of   \nlong-term   contracts   is   that   they   guarantee   a   reliable   income   stream   for   many   years   and   therefore   make   such   \nprojects   more   easily   financeable.   To   hit   its   24/7   target,   Google   will   continue   to   purchase   clean   energy   from   \nvarious   sources   such   as   energy   storage   and   energy   generation   to   ensure   it   has   a   clean   energy   supply   at   all   24   \nhours   of   the   day,   7   days   a   week.   \n29  In   2017,   Google   became   the   first   major   company   to   match   100%   of   its   annual   electricity   use   with   renewable   \nenergy—purchasing   as   much   clean   energy   as   it   consumed   —which   it   has   done   for   three   consecutive   years.   \n30  RECs   are   more   properly   called    Energy   Attribute   Certificates .   Europe   calls   them    Guarantees   of   Origin    ( GOs ),   not   RECs.   \n31  Google’s   more   than   50   long-term   contracts   to   purchase   renewable   energy   resulted   in   more   than   $7   billion   in   new   capital   \ninvestment   in   renewable   energy   projects   worldwide   as   of   September   2019   [Goo20].   \n19", "sentences": [{"text": "Appendix   B.", "metadata": {}}, {"text": "Carbon   Offset   and   24/7   Carbon   Free   Energy   \nWhile   energy   consumption   is   relatively   straightforward,   policies   to   reduce   carbon   footprint   are   not.", "metadata": {}}, {"text": "One   reason   \nis   that   they   have   as   much   to   do   about   economics   and   accounting   as   they   do   about   physics.", "metadata": {}}, {"text": "This   short   \nappendix   tries   to   clarify   the   distinction   between   conventional   carbon   offsets,   Google’s   goal   for   2030   of   24/7   \nCarbon   Free   Energy   (CFE)   for   its   global   datacenters   and   campuses,   and   what   it   is   doing   in   2021   to   set   the   \ngroundwork   for   2030.", "metadata": {}}, {"text": "Readers   interested   in   greater   depth   should   take   a   look   at   [Ryo14,   Goog16,   Goo21].", "metadata": {}}, {"text": "Conventional   carbon   offsets   try   to   create   economic   incentives   to   create   projects   that   avoid   or   remove   \nCO 2 e.", "metadata": {}}, {"text": "When   pursuing   the   mitigation   of   carbon   emissions   from   electricity   production   and   consumption,   a   \ncompany   can   match   their   MWh   of   consumption   with   MWh   of   clean   energy   through   certificates   called    REC s  \n( Renewable   Energy   Certificates ).", "metadata": {}}, {"text": "The   rules   for   accounting   and   compensation,   are   defined   as   part   of   the    GHG  \nProtocol ,   under   Scope   2   for   electricity.", "metadata": {}}, {"text": "Under   the   current   Scope   2   Guidance,   1MWh   of   energy   used   in   July   in,   \nsay,   Georgia   that   produces   carbon   dioxide   can   be   compensated   by   purchasing   1MWh   of   CFE   in   Montana   in   \nNovember.", "metadata": {}}, {"text": "Typically,   the   period   of   accounting   is   a   calendar   year.", "metadata": {}}, {"text": "Google   achieved   carbon   neutrality   using   \nconventional   carbon   offsets   starting   in   2007.", "metadata": {}}, {"text": "29     \nAs   part   of   the    GHG   Protocol ,   the    World   Resource   Institute    defines   terms   and   economic   mechanisms   to   \nensure   consistency   of   claims   about   carbon.", "metadata": {}}, {"text": "They   defined   the   following   [Car21,   Ryo14]   (also   see   Figure   8):   \n● Additionality :   CO 2 e   reductions   are    additional    if   they   would   not   have   occurred   in   the   absence   of   a   market   \nfor   offset   credits.", "metadata": {}}, {"text": "Additionality   is   essential   for   the   quality   of   carbon   offset   credits—if   their   associated   \nCO 2 e   reductions   are   not   additional,   then   purchasing   offset   credits   in   lieu   of   reducing   your   own   \nemissions   will   make   climate   change   worse.", "metadata": {}}, {"text": "● The   Grid :   The   transmission   and   distribution   system   that   connects   generators   and   end-users.", "metadata": {}}, {"text": "● Levelized   Cost   Of   Energy   (LCOE) :   The   projected   total   system   and   operating   costs   divided   by   total   KWh   \nproduced   over   the   lifetime   of   the   project   or   contract.", "metadata": {}}, {"text": "● Power   Purchase   Agreement   (PPA) :   A   fixed-price   contractual   agreement   to   purchase   a   power   plant’s   \nenergy,   typically   calculated   using   LCOE.", "metadata": {}}, {"text": "● Renewable   Energy   Certificate   (REC ) 30 :   A   market-based   instrument   that   represents   the   property   rights   \nto   the   environmental,   social,   and   other   non-power   attributes   of   renewable   electricity   generation.", "metadata": {}}, {"text": "The   \ngoal   is   a   certificate   that   ensures   the   energy   purchased   is   genuinely   renewable   and   not   double   counted.", "metadata": {}}, {"text": "Google’s   target   for   2030   is   to   go   beyond   the   traditional   Scope   2   rules   to   restrict   both   the   location   and   the   \naccounting   period.", "metadata": {}}, {"text": "● Instead   of   anywhere   in   a   continent,   the   CFE   purchase   should   be   on   the   same   geographically   local   grid.", "metadata": {}}, {"text": "● Instead   of   the   accounting   period   being   one   year,   the   accounting   should   be   within   the   hour.", "metadata": {}}, {"text": "To   achieve   100%   24/7   local   CFE,   grids   would   need   to   offer   both   real   time   accounting   of   the   CFE   fraction   of   the   \nstandard   grid   and   the   generating   companies   must   offer   more   flexible   options   to   allow   consumers   to   pick   CFE   \nany   time   of   the   day,   not   just   when   the   wind   blows   or   when   the   sun   shines.", "metadata": {}}, {"text": "Ideally,   grid   operators   and   \ngenerating   companies   will   deliver   on   that   vision,   and   the   standards   will   evolve   to   certify   and   quantify   the   24/7   \nCFE   approach.", "metadata": {}}, {"text": "But   we   are   not   there   yet.", "metadata": {}}, {"text": "Figure   8   helps   explain   what   Google   is   doing   today.", "metadata": {}}, {"text": "Google   signs   long-term   contracts   as   PPAs   with   \nrenewable   energy   generating   companies   to   try   to   cover   Google’s   electricity   consumption.", "metadata": {}}, {"text": "31    One   benefit   of   \nlong-term   contracts   is   that   they   guarantee   a   reliable   income   stream   for   many   years   and   therefore   make   such   \nprojects   more   easily   financeable.", "metadata": {}}, {"text": "To   hit   its   24/7   target,   Google   will   continue   to   purchase   clean   energy   from   \nvarious   sources   such   as   energy   storage   and   energy   generation   to   ensure   it   has   a   clean   energy   supply   at   all   24   \nhours   of   the   day,   7   days   a   week.", "metadata": {}}, {"text": "29  In   2017,   Google   became   the   first   major   company   to   match   100%   of   its   annual   electricity   use   with   renewable   \nenergy—purchasing   as   much   clean   energy   as   it   consumed   —which   it   has   done   for   three   consecutive   years.", "metadata": {}}, {"text": "30  RECs   are   more   properly   called    Energy   Attribute   Certificates .", "metadata": {}}, {"text": "Europe   calls   them    Guarantees   of   Origin    ( GOs ),   not   RECs.", "metadata": {}}, {"text": "31  Google’s   more   than   50   long-term   contracts   to   purchase   renewable   energy   resulted   in   more   than   $7   billion   in   new   capital   \ninvestment   in   renewable   energy   projects   worldwide   as   of   September   2019   [Goo20].", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "The   percentage   of   CFE   for   a   datacenter   is   reported   ex-post,   after   load,   production,   and   grid   mix   data   are   \nsettled   and   made   available   to   Google.   With   the   current   24/7   CFE   framework,   when   Google   cannot   get   100%   \nCFE   from   the   grid   plus   its   clean   energy   contracts   in   a   given   hour,   the   shortfall   counts   against   the   goal.   When   \nthe   grid   and   renewable   energy   contracts   overshoot   in   a   given   hour,   Google   doesn’t   get   any   extra   credit   for   it,   \nas   the   accounting   period   is   reset   every   hour. 32    Since   Google   can   estimate   how   much   CFE   is   expected   in   a  \nspecific   region   based   on   the   grid   and   its   multi-year   clean   energy   contract,   it   incentivizes   programs   to   run   in   this   \nregion. 33   \nTables   1   and   4   show   this   distinction   as    gross   CO 2 e    (energy   from   the   grid)   and   the    net   CO 2 e    (after   applying   \nthe   24/7   local   renewable   energy   purchase   from   the   long-term   contracts).   Since   you   can’t   label   electrons,   there   \nis   no   guarantee   that   Google   is   using   exactly   the   same   clean   energy   that   it   paid   for,   but   in   our   view   the   overall   \neffect   is   the   same.     \nAlas,   Google’s   large   models   in   Table   4   were   run   in   the   Georgia   datacenter,   where   in   the   past   there   was   no   \nor   little   difference   between   gross   and   net   CO 2 e.   Regions   that   have   generator   companies   that   can   supply   clean   \nenergy   24/7   and   offer   marketplaces   that   allow   companies   to   acquire   clean   energy   at   any   time   of   day   will   be   \nmore   compelling   to   expand   future   growth   of   compute   from   a   carbon   impact   perspective.   A   great   example   is   \nOklahoma,   which   allowed   Google   to   average   95.6%   net   CFE   for   2020.    This   is   a   case   of   where   the   grass   \nactually   is   greener   in   Oklahoma   than   in   Georgia.   As   mentioned   above,   in   2021   many   new   TPU   v4   accelerators   \nwill   be   deployed   in   windy   Oklahoma.     \nFigure   8.   This   figure   explains   how   fixed-floating   swaps   work   for   Renewable   Energy   Certificates   (RECs).   \n(Reproduced   from   [Goo16].)   Instead   of   accounting   over   a   full   year   at   a   mix   of   locations   as   in   step   4,   \n24/7   CFE   does   the   accounting   separately   for   every   hour   in   the   year   in   the   same   single   location.     \n32  Excess   CFE   from   Google   projects   is   used   to   support   other   grid   load   as   well   as   incentivizing   additional   renewable   \ndevelopment   by   demonstrating   demand   and   driving   down   prices.   \n33  Google   even   deployed   a   system   in   2020   that    shifts   the   timing   of   non-urgent   compute   tasks   (like   ML   training)   to   when   \ncarbon-free   power   sources   are   most   plentiful    [Rad20].   Its   next   iteration   will   even   move   a   task   to   a   new   datacenter.   \n20", "sentences": [{"text": "The   percentage   of   CFE   for   a   datacenter   is   reported   ex-post,   after   load,   production,   and   grid   mix   data   are   \nsettled   and   made   available   to   Google.", "metadata": {}}, {"text": "With   the   current   24/7   CFE   framework,   when   Google   cannot   get   100%   \nCFE   from   the   grid   plus   its   clean   energy   contracts   in   a   given   hour,   the   shortfall   counts   against   the   goal.", "metadata": {}}, {"text": "When   \nthe   grid   and   renewable   energy   contracts   overshoot   in   a   given   hour,   Google   doesn’t   get   any   extra   credit   for   it,   \nas   the   accounting   period   is   reset   every   hour.", "metadata": {}}, {"text": "32    Since   Google   can   estimate   how   much   CFE   is   expected   in   a  \nspecific   region   based   on   the   grid   and   its   multi-year   clean   energy   contract,   it   incentivizes   programs   to   run   in   this   \nregion.", "metadata": {}}, {"text": "33   \nTables   1   and   4   show   this   distinction   as    gross   CO 2 e    (energy   from   the   grid)   and   the    net   CO 2 e    (after   applying   \nthe   24/7   local   renewable   energy   purchase   from   the   long-term   contracts).", "metadata": {}}, {"text": "Since   you   can’t   label   electrons,   there   \nis   no   guarantee   that   Google   is   using   exactly   the   same   clean   energy   that   it   paid   for,   but   in   our   view   the   overall   \neffect   is   the   same.", "metadata": {}}, {"text": "Alas,   Google’s   large   models   in   Table   4   were   run   in   the   Georgia   datacenter,   where   in   the   past   there   was   no   \nor   little   difference   between   gross   and   net   CO 2 e.", "metadata": {}}, {"text": "Regions   that   have   generator   companies   that   can   supply   clean   \nenergy   24/7   and   offer   marketplaces   that   allow   companies   to   acquire   clean   energy   at   any   time   of   day   will   be   \nmore   compelling   to   expand   future   growth   of   compute   from   a   carbon   impact   perspective.", "metadata": {}}, {"text": "A   great   example   is   \nOklahoma,   which   allowed   Google   to   average   95.6%   net   CFE   for   2020.", "metadata": {}}, {"text": "This   is   a   case   of   where   the   grass   \nactually   is   greener   in   Oklahoma   than   in   Georgia.", "metadata": {}}, {"text": "As   mentioned   above,   in   2021   many   new   TPU   v4   accelerators   \nwill   be   deployed   in   windy   Oklahoma.", "metadata": {}}, {"text": "Figure   8.", "metadata": {}}, {"text": "This   figure   explains   how   fixed-floating   swaps   work   for   Renewable   Energy   Certificates   (RECs).", "metadata": {}}, {"text": "(Reproduced   from   [Goo16].)   Instead   of   accounting   over   a   full   year   at   a   mix   of   locations   as   in   step   4,   \n24/7   CFE   does   the   accounting   separately   for   every   hour   in   the   year   in   the   same   single   location.", "metadata": {}}, {"text": "32  Excess   CFE   from   Google   projects   is   used   to   support   other   grid   load   as   well   as   incentivizing   additional   renewable   \ndevelopment   by   demonstrating   demand   and   driving   down   prices.", "metadata": {}}, {"text": "33  Google   even   deployed   a   system   in   2020   that    shifts   the   timing   of   non-urgent   compute   tasks   (like   ML   training)   to   when   \ncarbon-free   power   sources   are   most   plentiful    [Rad20].", "metadata": {}}, {"text": "Its   next   iteration   will   even   move   a   task   to   a   new   datacenter.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}, {"text": "[Image page=20 idx=1 name=X192.png] Size: 1338x895, Data: 162085 bytes", "sentences": [{"text": "[Image page=20 idx=1 name=X192.png] Size: 1338x895, Data: 162085 bytes", "metadata": {}}], "metadata": {"page": 20, "image_index": 1, "image_name": "X192.png", "image_width": 1338, "image_height": 895, "attachment_type": "image", "has_image_data": true, "image_data_size": 162085}}], "metadata": {"page": 20}}, {"title": "Page 21", "paragraphs": [{"text": "Appendix   C.   Details   of   a    CO 2 e   Estimate   for   NAS   in   an   Average   Datacenter   \n[Str19]   estimates   the   CO 2 e   for   the   neural   architecture   search   (NAS)   to   find   the   more-efficient   Evolved   \nTransformer   architecture   done   by   [So19]   at   Google   as   626,155   pounds   (284   tCO 2 e).    The   estimate   in   [Str19]   \nwas   done   for   the   hypothetical   scenario   of   running   the   computation   on   P100   GPUs   in   the   average   U.S.   \ndatacenter   with   the   average   U.S.   grid   energy   mix.   The   authors   of   this   note   represent   a   superset   of   the   authors   \nof   [So19],   and   we   agree   that   the   information   needed   for   an   accurate   estimate   was   scattered   in   several   \nsubsections   in   the   So    et   al .   paper,   which   makes   it   difficult   to   determine   the   actual   CO 2 e.   This   experience   is   one   \nreason   we   suggest   that   ML   conferences   encourage   future   NLP   papers   that   are   computationally   expensive   to   \ninclude   a   calculation   of   energy   consumed   and   CO 2 e   to   make   sure   all   the   details   are   included,   as   it’s   difficult   to   \ndetermine   them   retrospectively,   as   we   shall   see.   \nNAS   costs   in   [Str19]   are   derived   from   the   NAS   process   described   in   section   5.2   of   [So19]:   \n“The   search   ran   for   15K   child   models,   requiring   a   total   of   979M   train   steps.   Over   13K   models   did   not   \nmake   it   past   the   first   hurdle,   drastically   reducing   the   resources   required   to   view   the   240   thousandth   \ntrain   step   for   top   models,   which   would   have   cost   3.6B   training   steps   for   the   same   number   of   models   \nwithout   hurdles.   After   the   search   concluded,   we   then   selected   the   top   20   models   and   trained   them   for   \nthe   full   300K   steps,   each   on   a   single   TPU   V.2   chip.”   \nThe   projection   of   the   So    et   al .   NAS   cost   by   Strubell    et   al .   overestimates   the   actual   Evolved   Transformer   \nsearch   cost.   Strubell    et   al.    assumed   each   evaluation   in   the   search   is   conducted   using   a   large   configuration:   \nTransformer   (Big)   with   batch   size   32,768.   However,   So    et   al.    actually   used   a   small   proxy   configuration   (Section   \n3.3   of   [So19])   to   reduce   compute   cost   (and   CO 2 e).   This   proxy   version   used   Transformer   (Base)   rather   than   \nTransformer   (Big),   reducing   the   cost/step   by   2.3x.   It   also   reduced   the   training   batch   size   from   32,768   to   4,096   \nwhile   keeping   the   number   of   training   steps   unchanged,   reducing   the   cost/step   by   a   further   8x.     \nAs   a   result,   the   calculations   below   suggest   that   CO 2 e   from   the   misunderstanding   about   the   use   of   the   \nsmaller   proxy   task   were   overestimated   by   a   factor   of   ~18.7:   \nAssume   the   Carbon   Emission   Estimation   Method   in   [Str19]:   \nCO 2 e   =   num_chips   x   num_train_steps   x   hours/train_steps   x   emission/chip_per_hour   \nnum_train_steps   =   979,000,000    #   From   [So19]   \nemission_per_chip_per_hour   ~=   0.2855296   pounds   CO 2 e   #   From    [Str19]    Table   3 34 .   \nEstimation   of   Compute   Cost   in   [Str19]:   \n8   P100s   for   batch   size   32,768   (packed   version)   from   [Vas17]   ( 4096   per   GPU ):   \nnum_chips   =   8     \nThe   Training   speed   of   Transformer   Big   on   P100   from   [Vas17]:   \nhours_per_train_steps     =   84   hours   /   300,000   =   0.00028   (Section   5.2   in   [Vas17])   \nCO 2 e   =   8   *   979,000,000   *   0.00028   *   0.2855296   =    626,155   lbs   (284   t)   \nEstimation   of   Compute   Cost   if   using   GPUs   of   the   Actual   Setting   Adopted   in   [So19]:   \n1   P100   for   batch   size   32,768   /   8=4096   (Section   4.1   second   paragraph   in   [So19]).   \nnum_chips     =   1   (Section   4.3   in   [So19],   note   that   the   actual   search   used   one   TPU   v2   chip   to   fit   the   same   \nbatch   size   as   one   P100)   \nTraining   speed   of   Transformer    Base    on   P100   from   [Vas17]:   \nhours_per_train_steps   =   12   hours   /   100,000   =   0.00012   (Section   5.2   in   [Vas17])   \nCO 2 e   =   1   *   979,000,000   *   0.00012   *   0.2855296   =    33,544   lbs   (15.2   t)   \nAppendix   D   shows   a   ~5X   further   reduction   in   CO 2 e   by   adjusting   for   the   hardware   and   datacenter   where   the   \nNAS   occurred   rather   than   for   P100s   in   a   hypothetical   US   average   datacenter.", "sentences": [{"text": "Appendix   C.", "metadata": {}}, {"text": "Details   of   a    CO 2 e   Estimate   for   NAS   in   an   Average   Datacenter   \n[Str19]   estimates   the   CO 2 e   for   the   neural   architecture   search   (NAS)   to   find   the   more-efficient   Evolved   \nTransformer   architecture   done   by   [So19]   at   Google   as   626,155   pounds   (284   tCO 2 e).", "metadata": {}}, {"text": "The   estimate   in   [Str19]   \nwas   done   for   the   hypothetical   scenario   of   running   the   computation   on   P100   GPUs   in   the   average   U.S.", "metadata": {}}, {"text": "datacenter   with   the   average   U.S.", "metadata": {}}, {"text": "grid   energy   mix.", "metadata": {}}, {"text": "The   authors   of   this   note   represent   a   superset   of   the   authors   \nof   [So19],   and   we   agree   that   the   information   needed   for   an   accurate   estimate   was   scattered   in   several   \nsubsections   in   the   So    et   al .", "metadata": {}}, {"text": "paper,   which   makes   it   difficult   to   determine   the   actual   CO 2 e.", "metadata": {}}, {"text": "This   experience   is   one   \nreason   we   suggest   that   ML   conferences   encourage   future   NLP   papers   that   are   computationally   expensive   to   \ninclude   a   calculation   of   energy   consumed   and   CO 2 e   to   make   sure   all   the   details   are   included,   as   it’s   difficult   to   \ndetermine   them   retrospectively,   as   we   shall   see.", "metadata": {}}, {"text": "NAS   costs   in   [Str19]   are   derived   from   the   NAS   process   described   in   section   5.2   of   [So19]:   \n“The   search   ran   for   15K   child   models,   requiring   a   total   of   979M   train   steps.", "metadata": {}}, {"text": "Over   13K   models   did   not   \nmake   it   past   the   first   hurdle,   drastically   reducing   the   resources   required   to   view   the   240   thousandth   \ntrain   step   for   top   models,   which   would   have   cost   3.6B   training   steps   for   the   same   number   of   models   \nwithout   hurdles.", "metadata": {}}, {"text": "After   the   search   concluded,   we   then   selected   the   top   20   models   and   trained   them   for   \nthe   full   300K   steps,   each   on   a   single   TPU   V.2   chip.”   \nThe   projection   of   the   So    et   al .", "metadata": {}}, {"text": "NAS   cost   by   Strubell    et   al .", "metadata": {}}, {"text": "overestimates   the   actual   Evolved   Transformer   \nsearch   cost.", "metadata": {}}, {"text": "Strubell    et   al.", "metadata": {}}, {"text": "assumed   each   evaluation   in   the   search   is   conducted   using   a   large   configuration:   \nTransformer   (Big)   with   batch   size   32,768.", "metadata": {}}, {"text": "However,   So    et   al.", "metadata": {}}, {"text": "actually   used   a   small   proxy   configuration   (Section   \n3.3   of   [So19])   to   reduce   compute   cost   (and   CO 2 e).", "metadata": {}}, {"text": "This   proxy   version   used   Transformer   (Base)   rather   than   \nTransformer   (Big),   reducing   the   cost/step   by   2.3x.", "metadata": {}}, {"text": "It   also   reduced   the   training   batch   size   from   32,768   to   4,096   \nwhile   keeping   the   number   of   training   steps   unchanged,   reducing   the   cost/step   by   a   further   8x.", "metadata": {}}, {"text": "As   a   result,   the   calculations   below   suggest   that   CO 2 e   from   the   misunderstanding   about   the   use   of   the   \nsmaller   proxy   task   were   overestimated   by   a   factor   of   ~18.7:   \nAssume   the   Carbon   Emission   Estimation   Method   in   [Str19]:   \nCO 2 e   =   num_chips   x   num_train_steps   x   hours/train_steps   x   emission/chip_per_hour   \nnum_train_steps   =   979,000,000    #   From   [So19]   \nemission_per_chip_per_hour   ~=   0.2855296   pounds   CO 2 e   #   From    [Str19]    Table   3 34 .", "metadata": {}}, {"text": "Estimation   of   Compute   Cost   in   [Str19]:   \n8   P100s   for   batch   size   32,768   (packed   version)   from   [Vas17]   ( 4096   per   GPU ):   \nnum_chips   =   8     \nThe   Training   speed   of   Transformer   Big   on   P100   from   [Vas17]:   \nhours_per_train_steps     =   84   hours   /   300,000   =   0.00028   (Section   5.2   in   [Vas17])   \nCO 2 e   =   8   *   979,000,000   *   0.00028   *   0.2855296   =    626,155   lbs   (284   t)   \nEstimation   of   Compute   Cost   if   using   GPUs   of   the   Actual   Setting   Adopted   in   [So19]:   \n1   P100   for   batch   size   32,768   /   8=4096   (Section   4.1   second   paragraph   in   [So19]).", "metadata": {}}, {"text": "num_chips     =   1   (Section   4.3   in   [So19],   note   that   the   actual   search   used   one   TPU   v2   chip   to   fit   the   same   \nbatch   size   as   one   P100)   \nTraining   speed   of   Transformer    Base    on   P100   from   [Vas17]:   \nhours_per_train_steps   =   12   hours   /   100,000   =   0.00012   (Section   5.2   in   [Vas17])   \nCO 2 e   =   1   *   979,000,000   *   0.00012   *   0.2855296   =    33,544   lbs   (15.2   t)   \nAppendix   D   shows   a   ~5X   further   reduction   in   CO 2 e   by   adjusting   for   the   hardware   and   datacenter   where   the   \nNAS   occurred   rather   than   for   P100s   in   a   hypothetical   US   average   datacenter.", "metadata": {}}], "metadata": {"page": 21}}, {"text": "34  In   this   calculation,   emission_per_chip_per_hour   =   average   power   per   chip   (in   Watts)   *   PUE   *   lbs   CO 2 e   per   Watt.   \n21", "sentences": [{"text": "34  In   this   calculation,   emission_per_chip_per_hour   =   average   power   per   chip   (in   Watts)   *   PUE   *   lbs   CO 2 e   per   Watt.", "metadata": {}}, {"text": "21", "metadata": {}}], "metadata": {"page": 21}}], "metadata": {"page": 21}}, {"title": "Page 22", "paragraphs": [{"text": "Appendix   D.   Details   of   a   CO 2 e   Estimate   for   Google’s   Actual   NAS   \nTo   calculate   the   emissions   of   the   actual   NAS   in   [So19]   at   Google,   where   the   search   was   actually   performed,   \nwe   must   adjust   by   three   more   factors   beyond   the   assumptions   in   Appendix   C:   \n1. We   use   Google   Georgia   datacenter’s   PUE   from   the   period   in   which   the   search   computation   was   run   \n(1.10   in   Table   4)   instead   of   the   US   average   in   2018   (1.58).     \n2. Strubell    et   al.    used   the   US   average   CO 2    per   kilowatt   hour   (KWh)   as   calculated   by   the   U.S.   \nEnvironmental   Protection   Agency   (EPA)   of   0.423   kg   per   KWh   in   2018.   For   Google,   we   use   the   Georgia   \ndatacenter’s   average   CO 2 e/KWh   for   the   month   when   NAS   was   performed   (0.431    CO 2 e/KWh    in   Table   4).   \n3. So    et   al.    used   Google   TPU   v2   accelerators,   not   NVIDIA   P100   GPUs   as   modeled   in   [Str19].   TPU   v2s   \nare   much   faster,   so   the   search   process   takes   32,633   TPU   v2   hours   instead   of   117,780   P100   hours.   We   \nmeasured   the   power   when   running   the   [So19]   NAS   computation   on   TPU   v2,   including   the   memory,   \nfans,   network   interfaces,   and   the   CPU   host.   The   average   power   was   208   Watts.   [Str19]   estimated   the   \npower   per   P100   as   189   Watts 35 .   The   performance/Watt   for   NAS   of   TPU   v2   improved     \n(   117,780   /   32,633   )   *   (   189   /   208   )   or   3.3X.     \nOur   estimate   of   the   actual   NAS   search   that   So    et   al.    ran   at   Google   after   adjusting   for   the   correct   datacenter   \nPUE,   CO 2 e/KWh,   and   hardware   is   (6.8   *   24   *   200   *    208   *   1.10   /   1000)   *   0.431   /   1000   =   3.2   tCO 2 e   (7096   lbs) . 36   \nThis   actual   emissions   value   is   88X   smaller   than   the   incorrect   estimate   of   the   carbon   emissions   of   this   \nsearch   found   in   Strubell    et   al.     If   we   reran   the   NAS   search   today   on   TPU   v2s   in   Google’s   Iowa   datacenter   \nwith   24/7   local,   real   time   net   CO 2 e   reduction   instead   of   Google’s   Georgia   datacenter,   it   would   drop   from   3.2   \ntCO 2 e   to   0.6   tCO 2 e   (476X   smaller).   If   we   reran   using   newer   TPUs,   tCO 2 e   would   shrink   further.   \nWhen,   where,   how,   and   on   which   hardware   training   occurs   matters   in   addition   to   what   DNN   is   trained,   \nwhich   is   why   it’s   best   to   include   energy   consumed   and   CO 2 e   in   a   publication   rather   than   relying   on   others   to   \nestimate   it   correctly   afterwards.", "sentences": [{"text": "Appendix   D.", "metadata": {}}, {"text": "Details   of   a   CO 2 e   Estimate   for   Google’s   Actual   NAS   \nTo   calculate   the   emissions   of   the   actual   NAS   in   [So19]   at   Google,   where   the   search   was   actually   performed,   \nwe   must   adjust   by   three   more   factors   beyond   the   assumptions   in   Appendix   C:   \n1.", "metadata": {}}, {"text": "We   use   Google   Georgia   datacenter’s   PUE   from   the   period   in   which   the   search   computation   was   run   \n(1.10   in   Table   4)   instead   of   the   US   average   in   2018   (1.58).", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Strubell    et   al.", "metadata": {}}, {"text": "used   the   US   average   CO 2    per   kilowatt   hour   (KWh)   as   calculated   by   the   U.S.", "metadata": {}}, {"text": "Environmental   Protection   Agency   (EPA)   of   0.423   kg   per   KWh   in   2018.", "metadata": {}}, {"text": "For   Google,   we   use   the   Georgia   \ndatacenter’s   average   CO 2 e/KWh   for   the   month   when   NAS   was   performed   (0.431    CO 2 e/KWh    in   Table   4).", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "So    et   al.", "metadata": {}}, {"text": "used   Google   TPU   v2   accelerators,   not   NVIDIA   P100   GPUs   as   modeled   in   [Str19].", "metadata": {}}, {"text": "TPU   v2s   \nare   much   faster,   so   the   search   process   takes   32,633   TPU   v2   hours   instead   of   117,780   P100   hours.", "metadata": {}}, {"text": "We   \nmeasured   the   power   when   running   the   [So19]   NAS   computation   on   TPU   v2,   including   the   memory,   \nfans,   network   interfaces,   and   the   CPU   host.", "metadata": {}}, {"text": "The   average   power   was   208   Watts.", "metadata": {}}, {"text": "[Str19]   estimated   the   \npower   per   P100   as   189   Watts 35 .", "metadata": {}}, {"text": "The   performance/Watt   for   NAS   of   TPU   v2   improved     \n(   117,780   /   32,633   )   *   (   189   /   208   )   or   3.3X.", "metadata": {}}, {"text": "Our   estimate   of   the   actual   NAS   search   that   So    et   al.", "metadata": {}}, {"text": "ran   at   Google   after   adjusting   for   the   correct   datacenter   \nPUE,   CO 2 e/KWh,   and   hardware   is   (6.8   *   24   *   200   *    208   *   1.10   /   1000)   *   0.431   /   1000   =   3.2   tCO 2 e   (7096   lbs) .", "metadata": {}}, {"text": "36   \nThis   actual   emissions   value   is   88X   smaller   than   the   incorrect   estimate   of   the   carbon   emissions   of   this   \nsearch   found   in   Strubell    et   al.", "metadata": {}}, {"text": "If   we   reran   the   NAS   search   today   on   TPU   v2s   in   Google’s   Iowa   datacenter   \nwith   24/7   local,   real   time   net   CO 2 e   reduction   instead   of   Google’s   Georgia   datacenter,   it   would   drop   from   3.2   \ntCO 2 e   to   0.6   tCO 2 e   (476X   smaller).", "metadata": {}}, {"text": "If   we   reran   using   newer   TPUs,   tCO 2 e   would   shrink   further.", "metadata": {}}, {"text": "When,   where,   how,   and   on   which   hardware   training   occurs   matters   in   addition   to   what   DNN   is   trained,   \nwhich   is   why   it’s   best   to   include   energy   consumed   and   CO 2 e   in   a   publication   rather   than   relying   on   others   to   \nestimate   it   correctly   afterwards.", "metadata": {}}], "metadata": {"page": 22}}, {"text": "35  Strubell    et   al .   used   a   mix   of   tools   to   estimate   power   for   GPU,   host   CPU,   and   host   memory   at   189   Watts,   which   they   \nused   to   estimate   NAS.   Our   measurements   for   P100   are   much   higher   in   Table   4   for   Transformer   (Big)   296   Watts.   We   \nincluded   everything   in   the   rack   like   we   do   for   TPUs,   including   TPU   memory,   top   of   rack   switch,   fans,   power   supplies,   and   \nso   on.   The   two   systems   are   running   different   implementations   of   the   same   problem   and   the   CPU   hosts   are   different.   One   \nissue   might   be   that   NVIDIA’s   power   measurement   tool   used   in   [Str18]   samples   power   once   a   minute,   so   there   may   be   \nsampling   issues.     \n36  To   put   3.2   net   tCO 2 e   into   perspective,Table   1   and   Appendix   A   use   Google   Flights   to   calculate   the    CO 2 e    for   the   average   \ndirect   round   trip   flights   between   SFO   and   JFK   as   180.4t.   The   Boeing   767   that   United   Airlines   flies   on   that   route   has   175   \nseats.   Google   Flights   uses   the   historical   average   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   \ntrip.   Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.   \n22", "sentences": [{"text": "35  Strubell    et   al .", "metadata": {}}, {"text": "used   a   mix   of   tools   to   estimate   power   for   GPU,   host   CPU,   and   host   memory   at   189   Watts,   which   they   \nused   to   estimate   NAS.", "metadata": {}}, {"text": "Our   measurements   for   P100   are   much   higher   in   Table   4   for   Transformer   (Big)   296   Watts.", "metadata": {}}, {"text": "We   \nincluded   everything   in   the   rack   like   we   do   for   TPUs,   including   TPU   memory,   top   of   rack   switch,   fans,   power   supplies,   and   \nso   on.", "metadata": {}}, {"text": "The   two   systems   are   running   different   implementations   of   the   same   problem   and   the   CPU   hosts   are   different.", "metadata": {}}, {"text": "One   \nissue   might   be   that   NVIDIA’s   power   measurement   tool   used   in   [Str18]   samples   power   once   a   minute,   so   there   may   be   \nsampling   issues.", "metadata": {}}, {"text": "36  To   put   3.2   net   tCO 2 e   into   perspective,Table   1   and   Appendix   A   use   Google   Flights   to   calculate   the    CO 2 e    for   the   average   \ndirect   round   trip   flights   between   SFO   and   JFK   as   180.4t.", "metadata": {}}, {"text": "The   Boeing   767   that   United   Airlines   flies   on   that   route   has   175   \nseats.", "metadata": {}}, {"text": "Google   Flights   uses   the   historical   average   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   \ntrip.", "metadata": {}}, {"text": "Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.", "metadata": {}}, {"text": "22", "metadata": {}}], "metadata": {"page": 22}}], "metadata": {"page": 22}}]}