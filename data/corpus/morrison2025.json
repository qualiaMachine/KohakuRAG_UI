{"document_id": "morrison2025", "title": "Holistically Evaluating the Environmental Impact of Creating Language Models", "text": "Published as a conference paper at ICLR 2025\nHOLISTICALLY EVALUATING THE ENVIRONMENTAL\nIMPACT OF CREATING LANGUAGE MODELS\nJacob Morrison1 Clara Na2 Jared Fernandez2\nTim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n1Allen Institute for AI 2Carnegie Mellon University\njacobm@allenai.org\nABSTRACT\nAs the performance of artificial intelligence systems has dramatically increased,\nso too has the environmental impact of creating these systems. While many model\ndevelopers release estimates of the power consumption and carbon emissions from\nthe final training runs for their latest models, there is comparatively little trans-\nparency into the impact of model development, hardware manufacturing, and total\nwater usage throughout. In this work, we estimate the real-world environmental\nimpact of developing a series of language models, ranging from 20 million to 13\nbillion active parameters, trained on up to 5.6 trillion tokens each. When account-\ning for hardware manufacturing, model development, and our final training runs,\nwe find that our series of models released 493 metric tons of carbon emissions,\nequivalent to powering about 98 homes in the United States for one year, and\nconsumed 2.769 million liters of water , equivalent to about 24.5 years of water\nusage by a person in the United States, even though our data center is extremely\nwater-efficient. We measure and report the environmental impact of our model\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\nwe find that model development, the impact of which is generally not disclosed\nby most model developers, amounted to ∼50% of that of training. By looking at\ndetailed time series data for power consumption, we also find that power usage\nthroughout training is not consistent, fluctuating between ∼15% and ∼85% of\nour hardware’s maximum power draw, with negative implications for grid-scale\nplanning as demand continues to grow. We close with a discussion on the con-\ntinued difficulty of estimating the environmental impact of AI systems, and key\ntakeaways for model developers and the public at large.\n1 I NTRODUCTION\nIn recent years, the field of artificial intelligence has progressed at an unprecedented pace, driven\nin large part by the development and deployment of large language and multimodal models. How-\never, the development of these models comes with significant environmental costs (Schwartz et al.,\n2020; Strubell et al., 2020; Wu et al., 2022). Training these models requires massive computational\nresources, which, in turn, require large amounts of energy. Powering training both emits carbon\n(by burning fossil fuels) and consumes water (by evaporating or polluting it in power plants, data\ncenters, and hardware manufacturing processes; Li et al. (2023)). There is a growing demand for\nenergy to power AI workloads, with projections estimating that datacenters may consume upwards\nof 11.7% of the total US energy demand by 2030 (Shehabi et al., 2024; Green et al., 2024). These\nenergy needs are substantial such that they affect the decisions of both machine learning developers\nand energy providers – for instance, Microsoft recently signed a deal to purchase the next 20 years\nof energy generated by reopening a nuclear power plant, 1 and meanwhile energy providers are ex-\ntending the life of aging fossil fuel energy plants to keep up with demand. 2 As such, especially as\n1https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/\n2https://www.wsj.com/business/energy-oil/electricity-demand-coal-gas-retirement-charts-dd07029a\n1\narXiv:2503.05804v1  [cs.CY]  3 Mar 2025\n\nPublished as a conference paper at ICLR 2025\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is\nimperative to carefully characterize the true cost of building and deploying state-of-the-art models,\nto inform effective strategies for mitigating potential harms and planning for future demand.\n100 101 102\nCarbon Emissions (tCO eq)\n100\n101\n102\n103\nWater Consumption (kL)OLMo 20M\nOLMo 150M\nOLMo 700M\nOLMo 1B\nOLMo 2 7B\nOLMo 2 13B\nOLMoE 0924\nFigure 1: Environmental impact for a selection of the\nfinal training runs described in Section 4.1, where we\nrank each model by both its total water consumption\nand its CO2 emissions. Our small models (<1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion. We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.\nIn this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024; OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens. To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol’s definitions,3 and Scope 1 and 2\nwater consumption following Li et al. (2023);\nin addition, we calculate “upstream” embod-\nied carbon and water consumption, and provide\n“downstream” estimates from use of our mod-\nels (which are part, but not all, of Scope 3).\nImportantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference. To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.\nOur methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current usage trends.4\nWe conclude that more transparency is needed across the industry in reporting the environmental\nimpact of AI systems. Systems orders of magnitude larger than those in this paper are being built,\nand deployed at a global scale, leading to emissions 10s or 100s of times larger than what we\nreport. This work is a step in the right direction, but responsibility of reporting and reducing the\nenvironmental impact must fall on those training the largest models, as they have the largest impact.\n2 R ELATED WORK\nWhile most publicly available models do not report any climate impact, including CO 2 emissions,\nwater usage, or embodied carbon, a few reports recently have included some estimates. For example,\n3https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf\n4https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.\nhtml\n2\n\nPublished as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3’s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) ∝ E · D · H (1)\n5Llama 1 did not use the data center location’s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3\n\nPublished as a conference paper at ICLR 2025\nwhere the cost of a scientific resultR (e.g. a claim that a particular training setup reachesX accuracy\non benchmark Y ) is proportional to the product of the cost of processing a single example E, the\nsize of the training dataset D, and the number of hyperparameter experiments H. In previous work,\nE · D, the cost of training on the training dataset, is what is most commonly reported, and H, the\ntotal number of experiments, is most often excluded.\nIn our analysis, we calculate the total power consumption during model training, development, and\ninference, and use this to estimate the total carbon emissions and water consumption during each\nstage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al.,\n2024) to calculate CO2 emissions (CO2e) from power consumption:\nCO 2e = P · P U E · CI (2)\nwhere the total carbon emissions is equal to the power usage P, multiplied by the power usage\neffectiveness (PUE)6 of the data center, multiplied by the carbon intensity CI of the local power\ngrid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in\nTexas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was\ntrained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\nOur data center providers informed us that Jupiter’s PUE is between 1.1 and 1.2 depending on\nthe current total utilization (we conservatively assume 1.2 for our calculations), and that Augusta’s\ntrailing twelve-month average was 1.12. Jupiter is powered by Austin Energy, which most recently\nreported a carbon intensity of 0.332 kg CO 2 per kWh.7 Augusta is located in Iowa, and the state of\nIowa has an average carbon intensity of 0.352 kg CO2 per kWh,8 which we use for our calculations.\nWe follow Li et al. (2023) to calculate water consumed onsite and through power generation:\nConsumption = P · P U E · (W U Eonsite + W U Eoffsite) (3)\nwhere WUEonsite is the water usage effectiveness of the data center, dictated by the cooling hardware\nused, and WUEoffsite is the water usage effectiveness of the local power provider, dictated by the\nprecise mixture of sources of power generation, as thermo- and hydro-electric power plants lead to\nevaporated water that is lost and will not re-enter circulation in the local environment.\nAs our data center uses an efficient closed-loop cooling system with no evaporative cooling, we\nassume a WUEonsite of 0 liters per kWh. Following Reig et al. (2020), we assume a WUE offsite of\n1.29 L per kWh for our Jupiter cluster and 3.10 L per kWh for our Augusta cluster.\nBoth calculations rely on total power usage. To calculate power usage during development and\ntraining, we analyze detailed time series data for a single node throughout each run, logging power\ndata at sub-second intervals, and extrapolate to the total number of nodes. As we only measure GPU\npower consumption, our estimates should be viewed as a lower bound on the true amount of power\nconsumed during development and training.\n3.2 E MBODIED IMPACTS\nEmbodied impacts are those arising from the production of physical elements required to support\nLLM development and use, such as hardware manufacturing and data center construction. To cal-\nculate embodied emissions, we follow Luccioni et al. (2023) by amortizing the carbon emissions\nfrom manufacturing over the lifetime of the hardware to get an estimate of the per hour cost, and\nmultiplying by the number of GPU hours used throughout model development and training. We\nextend this to include water consumption as well, by amortizing estimates of water consumption\nduring manufacturing over the lifetime of the hardware.\n6https://www.techtarget.com/searchdatacenter/definition/power-usage-effectiveness-PUE\n7austinenergy.com/-/media/project/websites/austinenergy/commercial/\ncarbonemissionscalculator.pdf\n8www.eia.gov/electricity/state/iowa\n4\n\nPublished as a conference paper at ICLR 2025\n3.3 M ODELS , DATA, AND HARDWARE\nMost of the models we evaluate are standard dense transformers, with an architecture similar to\nLlama (Touvron et al., 2023a;b; Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other\nrecent popular models, ranging in size from 20 million to 13 billion active parameters. Each of the\nsub-billion parameter models was trained on 1.7 trillion tokens, the 1 billion parameter model was\ntrained to 3 trillion tokens, the 7 billion parameter models were trained to 2, 3 and 4 trillion tokens,\nand the 13 billion parameter model to 5.6 trillion tokens. We additionally evaluate a mixture-of-\nexperts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.\nEach model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high\nspeed interconnect between each node, and between 2 and 128 nodes concurrently per training run.\nAll models except the 13B were trained in the same data center. See OLMo et al. (2025) for more\ninformation on our technical infrastructure.\n3.4 S IMULATING INFERENCE\nBecause we do not deploy our models, we do not collect or report data about real usage of our\nmodels. We instead report estimated costs associated with deployment of a subset of our models,\nalong with comparison models, with varying inference configurations. In reality, causal language\nmodels can have a variety of use cases and be deployed on a variety of hardware infrastructure. As\na representative deployment setting, we assume a setting in which users interact with the models\nvia chat; we collect measurements assuming models are served on a single H100 GPU via SGLang\n(Zheng et al., 2024). All three inference configurations used can be mapped to a previously proposed\nrealistic online inference scenario (Reddi et al., 2020; Peng et al., 2023). Specifically, other than\nthe “batching” scenario where all requests are sent instantaneously, the requests follow a Poisson\ndistribution, albeit at different rates that influence different batch sizes. The requests themselves\ncome from the ShareGPT dataset, 9 and each inference scenario involves the same sample of 2400\nprompts (same random seed). Input and output lengths, therefore, are the same in theory for a given\nmodel, but due to differences in tokenization and model context length, there are slight variations in\nmean input/output lengths across models, 225-250 and 190-230 tokens respectively.\nIn our inference experiments, we measure cumulative energy consumption using CodeCarbon\n(Courty et al., 2024) tracking, which was verified against the same time series monitoring used\nthroughout training. Notably, we measure total power and energy consumption associated with only\nthe relevant processes, excluding the overhead associated with, for example, holding the model in\nmemory or listening for requests.\nWe ran our inference simulations on our Jupiter cluster, used to train almost all of our models,\nbut we use only a single H100 GPU at a time. See Appendix A.1 for details about our inference\nmethodology and assumptions.\n4 R ESULTS\n4.1 B UILDING OUR MODELS\nIn this section we aim to report a full accounting of the environmental impact of training our series\nof models, from hardware manufacturing, to development, and the final training runs. We follow the\nmethodology outlined in Section 3.1 and Section 3.2.\nWhen calculating environmental impact, we use information from our data center providers and their\npower providers to measure the efficiency of each cluster. For Jupiter, the cluster used to train all\nmodels but the 13B, we assume a carbon intensity of 0.332 kg CO2 emitted per kWh, a power usage\neffectiveness (PUE) of 1.2, and a total water usage effectiveness (WUE) of 1.29 liters per kWh. For\nAugusta, the cluster used to train the 13B, we assume a carbon intensity of 0.351 kg CO 2 emitted\nper kWh, a PUE of 1.12, and a total WUE of 3.1 liters per kWh.\n9https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/\nShareGPT_V3_unfiltered_cleaned_split.json,anon8231489123/ShareGPT_Vicuna_unfiltered\n5\n\nPublished as a conference paper at ICLR 2025\nTable 1: We developed our models in five groups, based on parameter count and architecture: less than 1\nbillion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active\nand 7 billion total parameters. We found that ∼70% of our developmental environmental impact came from\ndeveloping the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth\nof gasoline, and equal to about 7 and a half years of water used by the average person in the United States.\nGPU\nHours\nTotal\nMWh # Runs\nCarbon\nEmissions\n(tCO2eq)\nEquivalent to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...\n(water usage,\n1 person)\n<1B 29k 19 20 6 1 yr, 4 mo 24 3 mo\n7B 269k 196 375 65 13 yrs, 6 mo 252 2 yrs, 7 mo\n13B 191k 116 156 46 9 yrs, 7 mo 402 3 yrs, 7 mo\nMoE 27k 19 35 6 1 yr, 4 mo 24 3 mo\nTotal 680k 459 813 159 33 yrs, 1 mo 843 7 yrs, 5 mo\nHardware manufacturing NVIDIA does not release the embodied carbon emissions or water\nconsumption about the hardware it produces, so we assume the same embodied carbon emissions\nas Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU. There\nis little public information on how much water is required to produce a single GPU, though chip\nmanufacturing facilities require millions of liters per day. 10 Some estimates11 place TSMC water\nusage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which\nwe use for our analysis.\nWe additionally estimate the environmental impact from mining rare earth metals used during man-\nufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials\nconsumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-\ninch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an\nadditional 2.2 liters consumed and 0.013 kg CO2eq per GPU.\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of\n0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied\nimpacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in\ntotal, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.\nDevelopment Before launching our final training runs for each model, we ran a series of controlled\nexperiments to stabilize and improve our training setup, to explore different parameter initializations\nand mid-training recipes, and to determine our final hyperparameters and data mixtures through\nscaling law experiments (Bhagia et al., 2024). We ran these in five distinct groups: small models\nwith less than 1 billion parameters, 1 billion parameter models, 7 billion parameter models, 13\nbillion parameter models, and our mixture-of-experts model. We report detailed development costs\nfor each group in Table 1.\nUnsurprisingly, we find that the majority of development costs ( ∼70%) were incurred at the 7 and\n13 billion parameter scale, due to both the relative size of the model and our own prioritization, and\nwe see this both in the total environmental impact and the number of individual runs per category.\nUsing our data center’s efficiency factors, we find that our development runs led to 159 tCO 2eq\nemitted and 843 kL of water consumed.\nFinal training runs Finally, we fully trained our series of models, ranging from 20 million to\n13 billion active parameters, with detailed information provided in Table 2. As we saw during\ndevelopment, the majority of the cost incurred came from training our 7B and 13B models, which\nwe trained to 2 to 5 trillion tokens. We also see that the 1B dense model required about as much\nenergy per trillion tokens as the MoE model with 1B active parameters, though the MoE model was\nslightly less efficient, most likely due to the extra compute required for routing tokens. In summary,\nwe find that our training runs led to 312 tCO2eq emitted and 1,921 kL of water consumed.\n10https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/\ntsmc-arizona-water-use-recycling/74059522007/\n11https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/\n12https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6\n\nPublished as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n† denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M† 0.8 0.3 3 weeks 1 3 days\nOLMo 60M† 1.2 0.4 1 month 1.6 5 days\nOLMo 150M† 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M† 5 2 5 months 5.9 19 days\nOLMo 700M† 8 3 7 months 10 33 days\nOLMo 7B† 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models’ impact\nmight be if they were put into production. In contrast to §4.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report “breakeven” points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7\n\nPublished as a conference paper at ICLR 2025\nTable 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from\nShareGPT at varying request rates. Since the models were served on machines from the same cluster that\nour OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2\nrespectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption\nand carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported\nin this table account for the GPU processes associated with active inference, but not CPU or RAM associated\nwith e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.\nAlso of note is the relatively small variability in carbon emissions and water consumption across different model\nsizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated;\ngreater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report\n”break-even” points for Qwen 2.5 because its training costs are not public.\nModel\nRequest\nfreq.\n(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.\n(L)\nSeconds\nper 100 req.\n# Inf. for\nCO2 equiv.\nw/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.\n8 0.036 12.0 0.054 12.64 21.5 bil.\n1 0.160 53.1 0.238 100.58 4.83 bil.\nQwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 —\n8 0.053 17.6 0.079 12.77 —\n1 0.308 102.3 0.459 100.58 —\nLlama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.\n8 0.051 16.9 0.076 12.79 59.5 bil.\n1 0.333 110.6 0.496 100.64 9.12 bil.\nLlama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.\n8 0.060 19.9 0.089 13.09 7.52 bil.\n1 0.401 133.1 0.597 100.73 1.13 bil.\nOLMo 1 1B (3T) ∞ 0.004 1.3 0.006 0.99 18.2 bil.\n8 0.038 12.6 0.057 12.63 1.91 bil.\n1 0.165 54.8 0.246 100.58 441 mil.\nOLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.\n8 0.049 16.3 0.073 12.88 7.68 bil.\n1 0.358 118.9 0.533 100.54 1.05 bil.\nOLMo 2 13B ∞ 0.033 11.0 0.049 6.60 22.1 bil.\n8 0.057 18.9 0.085 13.05 12.8 bil.\n1 0.386 128.2 0.575 100.57 1.89 bil.\nOLMoE 0924 ∞ 0.006 2.0 0.009 1.70 21.7 bil.\n8 0.037 12.3 0.055 12.82 3.51 bil.\n1 0.151 50.1 0.225 100.60 861 mil.\ninferences in each scenario required for inference costs to be equal or greater to training costs. See\nTable 4 in Appendix A.1 for additional results.\nWe find that for most models tested, the number of inferences required to outweigh training costs\nis in the hundreds of millions to tens of billions, except for the most over-trained models. As many\nof these models were created to be efficient in deployment-focused scenarios – such as on edge\ndevices, or in popular online products – it is important to consider inference costs in addition to\ntraining costs. The largest model providers are producing up to hundreds of billions of tokens per\nday,17 highlighting that deployed models can quickly reach this tipping point.\n4.3 P OWER FLUCTUATIONS DURING TRAINING\nOne problem caused by training AI models at large scales is that the power demand starts and stops\nsuddenly (Dubey et al., 2024), which power grids can struggle to handle. When demand sharply\nrises, generation sources that can be quickly started and stopped – generally powered by fossil fuels,\nsuch as coal and natural gas – must be brought online quickly, increasing the marginal carbon inten-\nsity of the grid and potentially negatively impacting other consumers in cases where demand rises\nmore quickly than generation can handle. When demand sharply drops, excess power is discarded–\nby grounding the power or venting steam–until generation sources can spin down. Power grids\ncan generally manage some large variations (for example, when communities experience a sudden\n17https://x.com/sama/status/1756089361609981993\n8\n\nPublished as a conference paper at ICLR 2025\npower outage), but as we add more variability to the system, it becomes more difficult to maintain\nthis delicate balance, and infrastructure is not set up to handle frequent, large fluctuations.\nIn Figure 2, we show a snapshot of our model’s GPU power consumption during pre-training. We\nfind that power consumption is not consistent – instead, power is consistentwhile the model is train-\ning, but drops quickly while saving checkpoints. Though our models are relatively small, and we\nhave since improved checkpointing performance, other model developers have experienced similar\nissues caused by checkpointing and synchronization between nodes (Dubey et al., 2024).\n5 D ISCUSSION\n5.1 M ORE TRANSPARENCY IS (STILL ) N EEDED\nWhile many model developers–including some of the largest for-profit entities operating in this\nspace–make best efforts to report at least part of the cost of building their AI systems (Dubey et al.,\n2024; Gemma Team et al., 2024), more transparency is still needed throughout the development\npipeline. The EU AI Act, 18 and some proposed legislation, such as the Artificial Intelligence Envi-\nronmental Impacts Act19 in the United States, would start the process for defining voluntary environ-\nmental impact reporting standards for model developers, but until such standards are widespread in\nthe community, improved transparency can only come through voluntary efforts by companies and\nresearch organizations. Policy action is needed to ensure there is public visibility into environmental\nimpacts across the entire supply chain, from hardware manufacturing, data center construction, and\nenergy production, all the way through to model deployment and inference.\nEmbodied emissions are still an enigma Though a vital piece of all model development\npipelines, the environmental impact of manufacturing the GPUs used is essentially unknown. In\nprevious work, Wu et al. (2022) and Luccioni et al. (2023) highlighted the fact that researchers\nfocused on AI’s environmental impact are forced to use unreliable estimates of the cost of manufac-\nturing state-of-the-art computational hardware, and the situation is no better now, nearly two years\nlater. Many companies that manufacture other pieces of data center hardware disclose estimates of\nthe lifetime environmental impact,20 and until GPU manufacturers release similar information–on a\nvoluntary or compulsory basis–this will not improve.\nDevelopment costs are substantial, and unreported As reported in Section 4.1, we present de-\ntailed information on the cost of developing our training pipeline, in contrast with previous work.\nWe found that development costs–associated with failed runs, hyperparameter searches, testing ar-\nchitecture changes, and more–are responsible for a substantial portion of the total environmental\nimpact of creating our systems, highlighting a need for more transparency from developers. This\nis especially important in light of AutoML tools, where many models may be automatically trained\nwhile searching for a solution, and scaling law experiments, where smaller models are trained to\npredict the performance of larger models, and then discarded (Li et al., 2024; Bhagia et al., 2024).\nWater costs are real, and under-explored While under-explored in previous work, AI’s growing\nwater consumption is beginning to receive more and more attention 21 (Li et al., 2023), though not\nas much as it may deserve. As shown in Section 4.1, even training a series of comparatively small\nmodels uses a large amount of water, the amount of which is also drastically impacted by both\nthe cooling systems used in data centers as well as the power generation methods used. Without\nmore transparency from developers on when, where, and how they are training their models, it will\ncontinue to be difficult to quantify the scale of the issue, stymieing efforts to address it.\n5.2 S MALL CHOICES DURING TRAINING CAN HAVE LARGE IMPACTS\nWhile many issues relating to transparency require action from corporations and large research\ngroups, choices made during training have a large effect downstream.\n18https://artificialintelligenceact.eu/article/95/\n19https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_\nact_of_2024_-_020124pdf.pdf\n20https://www.hpe.com/psnow/doc/a50005151enw\n21https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/\n9\n\nPublished as a conference paper at ICLR 2025\nFigure 2: Average GPU power for a single node for\nthe first 300 logging steps during OLMo 2 7B train-\ning. The first spike is the beginning of training, and\neach drop happens when a model checkpoint is saved.\nWhen actively training, the average GPU power is over\n600W, over 85% of an H100’s maximum power draw\nof 700W, and during checkpointing, power usage drops\nto just over 100W, or about 15% maximum.\nSmaller models are cheaper to train and use,\nbut at what cost? Until recently, to achieve\nhigh model performance, a large model was\nneeded. Compute-optimal scaling laws for neu-\nral network training (Hoffmann et al., 2022;\nKaplan et al., 2020) imply that it is more ef-\nficient to put more data into a larger model,\nbecause of diminishing returns from “over-\ntraining” a small model. This meant that mod-\nels were expensive to both train and deploy,\nlimiting how widespread they could become,\nand how financially feasible they were to be\nused in a variety of scenarios.\nRecently, however, continuing to train models\non more and more tokens beyond the “compute-\noptimal” limit22 has been extremely successful\nin making “deployment-optimized” models that\ncan be substantially cheaper to perform infer-\nence with. This has led to an explosion in both\ntraining cost for small models, and total infer-\nence compute cost, as API-based models be-\ncome cheaper to use2324 and small models are deployed on-device (Gunter et al., 2024; Abdin et al.,\n2024). This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency in-\ncreases, overall consumption of that resource tends to increase, rather than decrease. In other words,\nas the cost of training models decreases, the downstream impact may continue to grow.\nThis is especially clear in context of our results in Section 4.2, showing that though the raw num-\nber of inferences required to outweigh training is objectively quite large, smaller models are being\ndeployed in many new scenarios that will drastically increase their total usage. Many inference use\ncases are also not able to be batched (e.g. generating text on a phone for immediate use), meaning\nthat deployers cannot schedule these requests to take advantage of cheaper or cleaner energy, and\nmust make use of immediately available power. Given that this trend will most likely only accelerate,\nit is vital that we improve transparency into the total cost of deployment in all scenarios.\nPower fluctuations reveal inefficiencies at best, challenges to power grid control at worst\nWhile it is known that the dramatic spike in power consumption at the beginning of training and the\nsubsequent drop at the end are problematic for power grid operators at large scales, little has been\ndiscussed publicly about how power consumption changes throughout training. We found that our\nmodels, using an optimized code base and publicly available tooling, sees rapid power fluctuations\nthroughout training caused by the commonplace practice of frequently saving model checkpoints.\nThis means that without careful engineering, one training run can cause thousands of rapid power\nfluctuations, which poses an immediate challenge for large-scale LLM training in data centers, which\ntypically source energy directly from power providers. Generated power needs to go somewhere,\nand rapid, large drops in consumption during training breaks common assumptions about data center\nsupply and demand, leading to significant control challenges in power systems. While some frame-\nworks have begun to implement workarounds to manage this issue, 25 more awareness is needed on\nthe part of researchers and engineers as training runs scale to tens of thousands of GPUs 26 or more,\nas even some of the largest model developers encounter difficulties from regularly shifting power\ndemand throughout training due to checkpointing, awaiting collective communications, and other\nunforeseen and potentially catastrophic failures (Dubey et al., 2024). We emphasize that address-\ning this will require more comprehensive solutions such as parallelized checkpointing, improved\ndemand response in data centers running large AI workloads, and new, heterogeneous methods for\ndistributed training spanning software, hardware, and scheduling.\n22e.g. scaling from 1 to 2 to 15T tokens for Llama 1, 2, and 3 (Touvron et al., 2023a;b; Dubey et al., 2024)\n23https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n24https://developers.googleblog.com/en/gemini-15-flash-updates-google-ai-studio-gemini-api/\n25E.g. the new PYTORCH NO POWERPLANT BLOWUP environment variable in PyTorch.\n26https://time.com/7021709/elon-musk-xai-grok-memphis/\n10\n\n[Image page=10 idx=1 name=Im2.png] Size: 1406x1028, Data: 182841 bytes\n\nPublished as a conference paper at ICLR 2025\nREFERENCES\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen\nBach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko,\nJohan Bjorck, S ´ebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dong-\ndong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang\nDai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit\nGarg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao,\nRussell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin\nJin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim,\nLev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden,\nXihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong\nLuo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio C ´esar Teodoro\nMendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-\nBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo\nde Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim,\nMichael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp\nWitte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Ji-\nlong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan,\nChenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your\nphone, 2024. URL https://arxiv.org/abs/2404.14219.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit\nSanghai. GQA: Training generalized multi-query transformer models from multi-head check-\npoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing , pp. 4895–4901, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL\nhttps://aclanthology.org/2023.emnlp-main.298.\nAkshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh\nJha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Han-\nnaneh Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL\nhttps://arxiv.org/abs/2412.04403.\nCallum Browning, Stephen Northey, Nawshad Haque, Warren Bruckard, and Mark Cooksey. Life\nCycle Assessment of Rare Earth Production from Monazite , pp. 83–88. Springer International\nPublishing, Cham, 2016. ISBN 978-3-319-48768-7. doi: 10.1007/978-3-319-48768-7 12. URL\nhttps://doi.org/10.1007/978-3-319-48768-7_12 .\nBenoit Courty, Victor Schmidt, Goyal-Kamal, MarionCoutarel, Luis Blanche, Boris Feld, inimaz,\nJ´er´emy Lecourt, LiamConnell, SabAmine, supatomic, Mathilde L ´eval, Alexis Cruveiller, oumi-\nnasara, Franklin Zhao, Aditya Joshi, Christian Bauer, Amine Saboni, Patrick LLORET, Alexis\nBogroff, Niko Laskaris, Hugues de Lavoreille, Alexandre Phiev, Edoardo Abati, rosekelly6400,\nDouglas Blank, Ziyao Wang, Lucas Ot ´avio, and Armin Catovic. mlco2/codecarbon: v2.7.1,\nSeptember 2024. URL https://doi.org/10.5281/zenodo.13744486.\nJesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma\nStrubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Mea-\nsuring the carbon intensity of ai in cloud instances, 2022. URL https://arxiv.org/abs/\n2206.05229.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L ´eonard\nHussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex\n11\n\nPublished as a conference paper at ICLR 2025\nBotev, Alex Castro-Ros, Ambrose Slone, Am´elie H´eliou, Andrea Tacchetti, Anna Bulanova, An-\ntonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo,\nCl´ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric\nNoland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Hen-\nryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu,\nJustin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko\nYotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo\nLiu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree\nPandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh\nGiang, Cl´ement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin\nGhahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah\nFiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on\ngemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295.\nAlistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. How data centers and the\nenergy sector can sate ai’s hunger for power.McKinsey and Company, 2024.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Au-\nthur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,\nTushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crys-\ntal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh\nShah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi,\nNathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini,\nNoah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15789–15809,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n2024.acl-long.841. URL https://aclanthology.org/2024.acl-long.841.\nTom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong\nYin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Pee-\nbles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans,\nTao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao,\nZaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo\nBencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Do-\nminik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu,\nFred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff\nLai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega,\nKelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole\nHay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi,\nRoman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang\nMa, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar,\nXin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun-\nsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy\nNarayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris\nChaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin,\nIrina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mah-\nyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr\nMaj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma\nRao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar,\nWencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia,\nZhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, 2024. URL\nhttps://arxiv.org/abs/2407.21075.\n12\n\nPublished as a conference paper at ICLR 2025\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aure-\nlia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and\nL. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL\nhttps://api.semanticscholar.org/CorpusID:247778764.\nWilliam Stanley Jevons. The Coal Question; An Inquiry Concerning the Progress of the Nation, and\nthe Probable Exhaustion of Our Coal Mines. London: Macmillan and Co, 1865.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal,\nEtash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Rein-\nhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Al-\nbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,\nJosh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Il-\nharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao\nNguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Se-\nwoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev,\nStephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kol-\nlar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\nDatacomp-lm: In search of the next generation of training sets for language models, 2024. URL\nhttps://arxiv.org/abs/2406.11794.\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making ai less ”thirsty”: Uncover-\ning and addressing the secret water footprint of ai models, 2023. URLhttps://arxiv.org/\nabs/2304.03271.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon foot-\nprint of bloom, a 176b parameter language model. Journal of Machine Learning Research , 24\n(253):1–15, 2023. URL http://jmlr.org/papers/v24/23-0069.html.\nSasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the\ncost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Trans-\nparency, pp. 85–99, 2024.\nSachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chen-\nfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Moham-\nmad Rastegari. OpenELM: An efficient language model family with open training and inference\nframework. In Workshop on Efficient Systems for Foundation Models II @ ICML2024 , 2024.\nURL https://openreview.net/forum?id=XNMbTkxroF.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bha-\ngia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord,\nTaira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha\nDziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William\nMerrill, Lester James V . Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py-\natkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2\nolmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656.\nHao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle\nLo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A.\nSmith, and Hannaneh Hajishirzi. Efficiency pentathlon: A standardized arena for efficiency eval-\nuation, 2023. URL https://arxiv.org/abs/2307.09701.\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-\nJean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh\nChukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott\n13\n\nPublished as a conference paper at ICLR 2025\nGardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar,\nDavid Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius,\nColin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish\nSirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi\nYamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. Mlperf in-\nference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer\nArchitecture (ISCA), pp. 446–459, 2020. doi: 10.1109/ISCA45697.2020.00045.\nPaul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water\nuse embedded in purchased electricity, 2020. URL https://www.wri.org/research/\nguidance-calculating-water-use-embedded-purchased-electricity .\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12):\n54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https://doi.org/\n10.1145/3381831.\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek,\nJonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage\nreport. 2024.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 13693–13696, 2020.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023a. URL https://arxiv.org/abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental impli-\ncations, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813,\n2022.\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\nChristos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang:\nEfficient execution of structured language model programs, 2024. URL https://arxiv.\norg/abs/2312.07104.\n14\n\nPublished as a conference paper at ICLR 2025\nA A PPENDIX\nA.1 A DDITIONAL INFERENCE SIMULATION DETAILS\nWe benchmark models using the ShareGPT dataset, assuming an online inference chat setting. In\npractice, with much longer inference examples, OLMo models may have an “unfair” advantage in\nthat they were generally trained with context lengths shorter than the other models we benchmark.\nHowever, we do not believe that to be a significant factor in our results. In fact, we observe that\nLlama 3.1 8B is actually measured to be faster and less energy intensive than OLMo 7b models,\nlikely due to the use of grouped-query attention (GQA; Ainslie et al. (2023)) in Llama 8b, vs not in\nOLMo models.\nWe report additional inference simulation results on a larger set of models in Table 4,\nA.2 L IMITATIONS\nOur main limitations are discussed throughout the main body of this work – in particular, we make\nvarious assumptions about embodied impacts due to lack of real data, and our inference and deploy-\nment numbers were benchmarked in a controlled, limited setting, as we do not in reality serve our\nmodels in the same sense, and we do not have access to data about most other models’ real usage.\nWe present only a limited set of inference simulations following a number of simplistic assump-\ntions. Specifically, we simulate only settings where a deployed model is ingesting input tokens and\ngenerating output tokens following default parameters defined in SGLang (Zheng et al., 2024) – as\nopposed to, for instance, evaluating only the likelihood of a given text. Additionally, we note that\npractitioners frequently employ different inference-time optimizations such as quantization; perform\ngeneration with different decoding algorithms; and/or deploy to and run inference on edge devices,\nsometimes even without GPUs. We do not account for this variety of scenarios in our experiments.\nWe observe linear trends in training costs relative to parameter count across four orders of magnitude\nand eight model sizes. However, we do not necessarily expect that this trend would hold tightly\nin all training settings across all possible scales – for instance, decentralized training or training\nacross multiple datacenters might be expected to incur significantly greater communication overhead\nthroughout training. Though we have not trained these models ourselves, our hope is that our work\nwill encourage others working in a broad range of settings to provide their own holistic reports of\nenvironmental resource consumption.\n15\n\nPublished as a conference paper at ICLR 2025\nTable 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang\nbenchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served\non machines from the same cluster that our models were trained on, so we use the same WUE and\nPUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO 2e /\nkWh. The measurements reported in this table account for the GPU processes associated with active\ninference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be\nconsidered as lower bounds on usage in similar settings. We do not report “break-even” points for\nQwen models since the training costs are not public.\nRequest\nfreq.\n(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.\n(L)\nSeconds\nper 100 req.\n# Inf. for\nCO2 equiv.\nw/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.\n8 0.036 12.0 0.054 12.64 21.5 bil.\n1 0.16 53.1 0.238 100.58 4.83 bil.\nLlama 2 7B ∞ 0.019 6.3 0.028 3.58 11.9 bil.\n8 0.054 17.9 0.08 12.83 4.18 bil.\n1 0.349 115.9 0.52 100.62 647 mil.\nLlama 3 8B ∞ 0.01 3.3 0.015 1.93 282 bil.\n8 0.052 17.3 0.077 12.78 54.2 bil.\n1 0.337 111.9 0.502 100.63 8.37 bil.\nLlama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.\n8 0.051 16.9 0.076 12.79 59.5 bil.\n1 0.333 110.6 0.496 100.64 9.12 bil.\nLlama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.\n8 0.06 19.9 0.089 13.09 7.52 bil.\n1 0.401 133.1 0.597 100.73 1.13 bil.\nQwen 2.5 1.5B ∞ 0.003 1.0 0.004 0.86 –\n8 0.033 11.0 0.049 12.65 –\n1 0.163 54.1 0.243 100.57 –\nQwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 –\n8 0.053 17.6 0.079 12.77 –\n1 0.308 102.3 0.459 100.58 –\nQwen 2.5 14B ∞ 0.018 6.0 0.027 3.45 –\n8 0.058 19.3 0.086 13.02 –\n1 0.387 128.5 0.577 100.64 –\nQwen 1.5 MoE ∞ 0.01 3.3 0.015 2.64 –\n(2.7BA, 14BT) 8 0.043 14.3 0.064 13.11 –\n1 0.165 54.8 0.246 100.68 –\nOLMo 1 1B ∞ 0.004 1.3 0.006 0.99 18.2 bil.\n8 0.038 12.6 0.057 12.63 1.91 bil.\n1 0.165 54.8 0.246 100.58 441 mil.\nOLMo 0724 7B ∞ 0.017 5.6 0.025 3.33 29.8 bil.\n8 0.052 17.3 0.077 12.77 9.73 bil.\n1 0.339 112.5 0.505 100.59 1.49 bil.\nOLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.\n8 0.049 16.3 0.073 12.88 7.68 bil.\n1 0.358 118.9 0.533 100.54 1.05 bil.\nOLMo 2 13B ∞ 0.033 11.0 0.049 6.6 22.1 bil.\n8 0.057 18.9 0.085 13.05 12.8 bil.\n1 0.386 128.2 0.575 100.57 1.89 bil.\nOLMoE 0924 ∞ 0.006 2.0 0.009 1.7 21.7 bil.\n(1BA, 7BT) 8 0.037 12.3 0.055 12.82 3.51 bil.\n1 0.151 50.1 0.225 100.6 861 mil.\n16", "metadata": {"url": "https://arxiv.org/pdf/2503.05804", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nHOLISTICALLY EVALUATING THE ENVIRONMENTAL\nIMPACT OF CREATING LANGUAGE MODELS\nJacob Morrison1 Clara Na2 Jared Fernandez2\nTim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n1Allen Institute for AI 2Carnegie Mellon University\njacobm@allenai.org\nABSTRACT\nAs the performance of artificial intelligence systems has dramatically increased,\nso too has the environmental impact of creating these systems. While many model\ndevelopers release estimates of the power consumption and carbon emissions from\nthe final training runs for their latest models, there is comparatively little trans-\nparency into the impact of model development, hardware manufacturing, and total\nwater usage throughout. In this work, we estimate the real-world environmental\nimpact of developing a series of language models, ranging from 20 million to 13\nbillion active parameters, trained on up to 5.6 trillion tokens each. When account-\ning for hardware manufacturing, model development, and our final training runs,\nwe find that our series of models released 493 metric tons of carbon emissions,\nequivalent to powering about 98 homes in the United States for one year, and\nconsumed 2.769 million liters of water , equivalent to about 24.5 years of water\nusage by a person in the United States, even though our data center is extremely\nwater-efficient. We measure and report the environmental impact of our model\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\nwe find that model development, the impact of which is generally not disclosed\nby most model developers, amounted to ∼50% of that of training. By looking at\ndetailed time series data for power consumption, we also find that power usage\nthroughout training is not consistent, fluctuating between ∼15% and ∼85% of\nour hardware’s maximum power draw, with negative implications for grid-scale\nplanning as demand continues to grow. We close with a discussion on the con-\ntinued difficulty of estimating the environmental impact of AI systems, and key\ntakeaways for model developers and the public at large.\n1 I NTRODUCTION\nIn recent years, the field of artificial intelligence has progressed at an unprecedented pace, driven\nin large part by the development and deployment of large language and multimodal models. How-\never, the development of these models comes with significant environmental costs (Schwartz et al.,\n2020; Strubell et al., 2020; Wu et al., 2022). Training these models requires massive computational\nresources, which, in turn, require large amounts of energy. Powering training both emits carbon\n(by burning fossil fuels) and consumes water (by evaporating or polluting it in power plants, data\ncenters, and hardware manufacturing processes; Li et al. (2023)). There is a growing demand for\nenergy to power AI workloads, with projections estimating that datacenters may consume upwards\nof 11.7% of the total US energy demand by 2030 (Shehabi et al., 2024; Green et al., 2024). These\nenergy needs are substantial such that they affect the decisions of both machine learning developers\nand energy providers – for instance, Microsoft recently signed a deal to purchase the next 20 years\nof energy generated by reopening a nuclear power plant, 1 and meanwhile energy providers are ex-\ntending the life of aging fossil fuel energy plants to keep up with demand. 2 As such, especially as\n1https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/\n2https://www.wsj.com/business/energy-oil/electricity-demand-coal-gas-retirement-charts-dd07029a\n1\narXiv:2503.05804v1  [cs.CY]  3 Mar 2025", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nHOLISTICALLY EVALUATING THE ENVIRONMENTAL\nIMPACT OF CREATING LANGUAGE MODELS\nJacob Morrison1 Clara Na2 Jared Fernandez2\nTim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n1Allen Institute for AI 2Carnegie Mellon University\njacobm@allenai.org\nABSTRACT\nAs the performance of artificial intelligence systems has dramatically increased,\nso too has the environmental impact of creating these systems.", "metadata": {}}, {"text": "While many model\ndevelopers release estimates of the power consumption and carbon emissions from\nthe final training runs for their latest models, there is comparatively little trans-\nparency into the impact of model development, hardware manufacturing, and total\nwater usage throughout.", "metadata": {}}, {"text": "In this work, we estimate the real-world environmental\nimpact of developing a series of language models, ranging from 20 million to 13\nbillion active parameters, trained on up to 5.6 trillion tokens each.", "metadata": {}}, {"text": "When account-\ning for hardware manufacturing, model development, and our final training runs,\nwe find that our series of models released 493 metric tons of carbon emissions,\nequivalent to powering about 98 homes in the United States for one year, and\nconsumed 2.769 million liters of water , equivalent to about 24.5 years of water\nusage by a person in the United States, even though our data center is extremely\nwater-efficient.", "metadata": {}}, {"text": "We measure and report the environmental impact of our model\ndevelopment;", "metadata": {}}, {"text": "to the best of our knowledge we are the first to do so for LLMs, and\nwe find that model development, the impact of which is generally not disclosed\nby most model developers, amounted to ∼50% of that of training.", "metadata": {}}, {"text": "By looking at\ndetailed time series data for power consumption, we also find that power usage\nthroughout training is not consistent, fluctuating between ∼15% and ∼85% of\nour hardware’s maximum power draw, with negative implications for grid-scale\nplanning as demand continues to grow.", "metadata": {}}, {"text": "We close with a discussion on the con-\ntinued difficulty of estimating the environmental impact of AI systems, and key\ntakeaways for model developers and the public at large.", "metadata": {}}, {"text": "1 I NTRODUCTION\nIn recent years, the field of artificial intelligence has progressed at an unprecedented pace, driven\nin large part by the development and deployment of large language and multimodal models.", "metadata": {}}, {"text": "How-\never, the development of these models comes with significant environmental costs (Schwartz et al.,\n2020;", "metadata": {}}, {"text": "Strubell et al., 2020;", "metadata": {}}, {"text": "Wu et al., 2022).", "metadata": {}}, {"text": "Training these models requires massive computational\nresources, which, in turn, require large amounts of energy.", "metadata": {}}, {"text": "Powering training both emits carbon\n(by burning fossil fuels) and consumes water (by evaporating or polluting it in power plants, data\ncenters, and hardware manufacturing processes;", "metadata": {}}, {"text": "Li et al.", "metadata": {}}, {"text": "(2023)).", "metadata": {}}, {"text": "There is a growing demand for\nenergy to power AI workloads, with projections estimating that datacenters may consume upwards\nof 11.7% of the total US energy demand by 2030 (Shehabi et al., 2024;", "metadata": {}}, {"text": "Green et al., 2024).", "metadata": {}}, {"text": "These\nenergy needs are substantial such that they affect the decisions of both machine learning developers\nand energy providers – for instance, Microsoft recently signed a deal to purchase the next 20 years\nof energy generated by reopening a nuclear power plant, 1 and meanwhile energy providers are ex-\ntending the life of aging fossil fuel energy plants to keep up with demand.", "metadata": {}}, {"text": "2 As such, especially as\n1https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/\n2https://www.wsj.com/business/energy-oil/electricity-demand-coal-gas-retirement-charts-dd07029a\n1\narXiv:2503.05804v1  [cs.CY]  3 Mar 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is\nimperative to carefully characterize the true cost of building and deploying state-of-the-art models,\nto inform effective strategies for mitigating potential harms and planning for future demand.\n100 101 102\nCarbon Emissions (tCO eq)\n100\n101\n102\n103\nWater Consumption (kL)OLMo 20M\nOLMo 150M\nOLMo 700M\nOLMo 1B\nOLMo 2 7B\nOLMo 2 13B\nOLMoE 0924\nFigure 1: Environmental impact for a selection of the\nfinal training runs described in Section 4.1, where we\nrank each model by both its total water consumption\nand its CO2 emissions. Our small models (<1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion. We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.\nIn this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024; OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens. To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol’s definitions,3 and Scope 1 and 2\nwater consumption following Li et al. (2023);\nin addition, we calculate “upstream” embod-\nied carbon and water consumption, and provide\n“downstream” estimates from use of our mod-\nels (which are part, but not all, of Scope 3).\nImportantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference. To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.\nOur methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current usage trends.4\nWe conclude that more transparency is needed across the industry in reporting the environmental\nimpact of AI systems. Systems orders of magnitude larger than those in this paper are being built,\nand deployed at a global scale, leading to emissions 10s or 100s of times larger than what we\nreport. This work is a step in the right direction, but responsibility of reporting and reducing the\nenvironmental impact must fall on those training the largest models, as they have the largest impact.\n2 R ELATED WORK\nWhile most publicly available models do not report any climate impact, including CO 2 emissions,\nwater usage, or embodied carbon, a few reports recently have included some estimates. For example,\n3https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf\n4https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.\nhtml\n2", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is\nimperative to carefully characterize the true cost of building and deploying state-of-the-art models,\nto inform effective strategies for mitigating potential harms and planning for future demand.", "metadata": {}}, {"text": "100 101 102\nCarbon Emissions (tCO eq)\n100\n101\n102\n103\nWater Consumption (kL)OLMo 20M\nOLMo 150M\nOLMo 700M\nOLMo 1B\nOLMo 2 7B\nOLMo 2 13B\nOLMoE 0924\nFigure 1: Environmental impact for a selection of the\nfinal training runs described in Section 4.1, where we\nrank each model by both its total water consumption\nand its CO2 emissions.", "metadata": {}}, {"text": "Our small models (<1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion.", "metadata": {}}, {"text": "We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.", "metadata": {}}, {"text": "In this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024;", "metadata": {}}, {"text": "OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens.", "metadata": {}}, {"text": "To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol’s definitions,3 and Scope 1 and 2\nwater consumption following Li et al.", "metadata": {}}, {"text": "(2023);", "metadata": {}}, {"text": "in addition, we calculate “upstream” embod-\nied carbon and water consumption, and provide\n“downstream” estimates from use of our mod-\nels (which are part, but not all, of Scope 3).", "metadata": {}}, {"text": "Importantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference.", "metadata": {}}, {"text": "To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years.", "metadata": {}}, {"text": "We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.", "metadata": {}}, {"text": "Our methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs.", "metadata": {}}, {"text": "For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training.", "metadata": {}}, {"text": "We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024;", "metadata": {}}, {"text": "Mehta et al., 2024;", "metadata": {}}, {"text": "Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios.", "metadata": {}}, {"text": "We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current usage trends.4\nWe conclude that more transparency is needed across the industry in reporting the environmental\nimpact of AI systems.", "metadata": {}}, {"text": "Systems orders of magnitude larger than those in this paper are being built,\nand deployed at a global scale, leading to emissions 10s or 100s of times larger than what we\nreport.", "metadata": {}}, {"text": "This work is a step in the right direction, but responsibility of reporting and reducing the\nenvironmental impact must fall on those training the largest models, as they have the largest impact.", "metadata": {}}, {"text": "2 R ELATED WORK\nWhile most publicly available models do not report any climate impact, including CO 2 emissions,\nwater usage, or embodied carbon, a few reports recently have included some estimates.", "metadata": {}}, {"text": "For example,\n3https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf\n4https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.", "metadata": {}}, {"text": "html\n2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3’s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) ∝ E · D · H (1)\n5Llama 1 did not use the data center location’s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nLuccioni et al.", "metadata": {}}, {"text": "(2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2).", "metadata": {}}, {"text": "Dodge et al.", "metadata": {}}, {"text": "(2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference.", "metadata": {}}, {"text": "Similarly, developers of the Llama models (Touvron et al., 2023a;b;", "metadata": {}}, {"text": "Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels;", "metadata": {}}, {"text": "they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity.", "metadata": {}}, {"text": "The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts.", "metadata": {}}, {"text": "The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs.", "metadata": {}}, {"text": "Energy use and environmental impacts\nare not typically documented for proprietary models.", "metadata": {}}, {"text": "Comparably little transparency has been provided on the water consumption of AI systems.", "metadata": {}}, {"text": "Li et al.", "metadata": {}}, {"text": "(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3’s training.", "metadata": {}}, {"text": "Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque.", "metadata": {}}, {"text": "In addition, almost all reporting\nof environmental impact is based on training of the final model that is released.", "metadata": {}}, {"text": "Instead of only\nfocusing on training, Luccioni et al.", "metadata": {}}, {"text": "(2024) estimate the impact of inference of deployed AI systems.", "metadata": {}}, {"text": "To the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e.", "metadata": {}}, {"text": "hyperparameter tuning and ablations before the main training run.", "metadata": {}}, {"text": "3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts.", "metadata": {}}, {"text": "Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions.", "metadata": {}}, {"text": "However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.", "metadata": {}}, {"text": "Following the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations.", "metadata": {}}, {"text": "We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training.", "metadata": {}}, {"text": "We describe our methodology for measuring\nand estimating these impacts in more detail below.", "metadata": {}}, {"text": "3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling.", "metadata": {}}, {"text": "We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al.", "metadata": {}}, {"text": "(2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) ∝ E · D · H (1)\n5Llama 1 did not use the data center location’s carbon intensity, instead using US national average carbon\nintensity;", "metadata": {}}, {"text": "Llama 2 did not specify the carbon intensity;", "metadata": {}}, {"text": "Llama 3 used a region-specific carbon intensity.", "metadata": {}}, {"text": "All 3\nassumed 100% GPU power draw throughout training.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nwhere the cost of a scientific resultR (e.g. a claim that a particular training setup reachesX accuracy\non benchmark Y ) is proportional to the product of the cost of processing a single example E, the\nsize of the training dataset D, and the number of hyperparameter experiments H. In previous work,\nE · D, the cost of training on the training dataset, is what is most commonly reported, and H, the\ntotal number of experiments, is most often excluded.\nIn our analysis, we calculate the total power consumption during model training, development, and\ninference, and use this to estimate the total carbon emissions and water consumption during each\nstage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al.,\n2024) to calculate CO2 emissions (CO2e) from power consumption:\nCO 2e = P · P U E · CI (2)\nwhere the total carbon emissions is equal to the power usage P, multiplied by the power usage\neffectiveness (PUE)6 of the data center, multiplied by the carbon intensity CI of the local power\ngrid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in\nTexas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was\ntrained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\nOur data center providers informed us that Jupiter’s PUE is between 1.1 and 1.2 depending on\nthe current total utilization (we conservatively assume 1.2 for our calculations), and that Augusta’s\ntrailing twelve-month average was 1.12. Jupiter is powered by Austin Energy, which most recently\nreported a carbon intensity of 0.332 kg CO 2 per kWh.7 Augusta is located in Iowa, and the state of\nIowa has an average carbon intensity of 0.352 kg CO2 per kWh,8 which we use for our calculations.\nWe follow Li et al. (2023) to calculate water consumed onsite and through power generation:\nConsumption = P · P U E · (W U Eonsite + W U Eoffsite) (3)\nwhere WUEonsite is the water usage effectiveness of the data center, dictated by the cooling hardware\nused, and WUEoffsite is the water usage effectiveness of the local power provider, dictated by the\nprecise mixture of sources of power generation, as thermo- and hydro-electric power plants lead to\nevaporated water that is lost and will not re-enter circulation in the local environment.\nAs our data center uses an efficient closed-loop cooling system with no evaporative cooling, we\nassume a WUEonsite of 0 liters per kWh. Following Reig et al. (2020), we assume a WUE offsite of\n1.29 L per kWh for our Jupiter cluster and 3.10 L per kWh for our Augusta cluster.\nBoth calculations rely on total power usage. To calculate power usage during development and\ntraining, we analyze detailed time series data for a single node throughout each run, logging power\ndata at sub-second intervals, and extrapolate to the total number of nodes. As we only measure GPU\npower consumption, our estimates should be viewed as a lower bound on the true amount of power\nconsumed during development and training.\n3.2 E MBODIED IMPACTS\nEmbodied impacts are those arising from the production of physical elements required to support\nLLM development and use, such as hardware manufacturing and data center construction. To cal-\nculate embodied emissions, we follow Luccioni et al. (2023) by amortizing the carbon emissions\nfrom manufacturing over the lifetime of the hardware to get an estimate of the per hour cost, and\nmultiplying by the number of GPU hours used throughout model development and training. We\nextend this to include water consumption as well, by amortizing estimates of water consumption\nduring manufacturing over the lifetime of the hardware.\n6https://www.techtarget.com/searchdatacenter/definition/power-usage-effectiveness-PUE\n7austinenergy.com/-/media/project/websites/austinenergy/commercial/\ncarbonemissionscalculator.pdf\n8www.eia.gov/electricity/state/iowa\n4", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nwhere the cost of a scientific resultR (e.g.", "metadata": {}}, {"text": "a claim that a particular training setup reachesX accuracy\non benchmark Y ) is proportional to the product of the cost of processing a single example E, the\nsize of the training dataset D, and the number of hyperparameter experiments H.", "metadata": {}}, {"text": "In previous work,\nE · D, the cost of training on the training dataset, is what is most commonly reported, and H, the\ntotal number of experiments, is most often excluded.", "metadata": {}}, {"text": "In our analysis, we calculate the total power consumption during model training, development, and\ninference, and use this to estimate the total carbon emissions and water consumption during each\nstage.", "metadata": {}}, {"text": "We follow previous work (Luccioni et al., 2023;", "metadata": {}}, {"text": "Dubey et al., 2024;", "metadata": {}}, {"text": "Gemma Team et al.,\n2024) to calculate CO2 emissions (CO2e) from power consumption:\nCO 2e = P · P U E · CI (2)\nwhere the total carbon emissions is equal to the power usage P, multiplied by the power usage\neffectiveness (PUE)6 of the data center, multiplied by the carbon intensity CI of the local power\ngrid.", "metadata": {}}, {"text": "We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in\nTexas and Iowa, respectively (see OLMo et al.", "metadata": {}}, {"text": "(2025) for more information).", "metadata": {}}, {"text": "Our 13B model was\ntrained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.", "metadata": {}}, {"text": "Our data center providers informed us that Jupiter’s PUE is between 1.1 and 1.2 depending on\nthe current total utilization (we conservatively assume 1.2 for our calculations), and that Augusta’s\ntrailing twelve-month average was 1.12.", "metadata": {}}, {"text": "Jupiter is powered by Austin Energy, which most recently\nreported a carbon intensity of 0.332 kg CO 2 per kWh.7 Augusta is located in Iowa, and the state of\nIowa has an average carbon intensity of 0.352 kg CO2 per kWh,8 which we use for our calculations.", "metadata": {}}, {"text": "We follow Li et al.", "metadata": {}}, {"text": "(2023) to calculate water consumed onsite and through power generation:\nConsumption = P · P U E · (W U Eonsite + W U Eoffsite) (3)\nwhere WUEonsite is the water usage effectiveness of the data center, dictated by the cooling hardware\nused, and WUEoffsite is the water usage effectiveness of the local power provider, dictated by the\nprecise mixture of sources of power generation, as thermo- and hydro-electric power plants lead to\nevaporated water that is lost and will not re-enter circulation in the local environment.", "metadata": {}}, {"text": "As our data center uses an efficient closed-loop cooling system with no evaporative cooling, we\nassume a WUEonsite of 0 liters per kWh.", "metadata": {}}, {"text": "Following Reig et al.", "metadata": {}}, {"text": "(2020), we assume a WUE offsite of\n1.29 L per kWh for our Jupiter cluster and 3.10 L per kWh for our Augusta cluster.", "metadata": {}}, {"text": "Both calculations rely on total power usage.", "metadata": {}}, {"text": "To calculate power usage during development and\ntraining, we analyze detailed time series data for a single node throughout each run, logging power\ndata at sub-second intervals, and extrapolate to the total number of nodes.", "metadata": {}}, {"text": "As we only measure GPU\npower consumption, our estimates should be viewed as a lower bound on the true amount of power\nconsumed during development and training.", "metadata": {}}, {"text": "3.2 E MBODIED IMPACTS\nEmbodied impacts are those arising from the production of physical elements required to support\nLLM development and use, such as hardware manufacturing and data center construction.", "metadata": {}}, {"text": "To cal-\nculate embodied emissions, we follow Luccioni et al.", "metadata": {}}, {"text": "(2023) by amortizing the carbon emissions\nfrom manufacturing over the lifetime of the hardware to get an estimate of the per hour cost, and\nmultiplying by the number of GPU hours used throughout model development and training.", "metadata": {}}, {"text": "We\nextend this to include water consumption as well, by amortizing estimates of water consumption\nduring manufacturing over the lifetime of the hardware.", "metadata": {}}, {"text": "6https://www.techtarget.com/searchdatacenter/definition/power-usage-effectiveness-PUE\n7austinenergy.com/-/media/project/websites/austinenergy/commercial/\ncarbonemissionscalculator.pdf\n8www.eia.gov/electricity/state/iowa\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\n3.3 M ODELS , DATA, AND HARDWARE\nMost of the models we evaluate are standard dense transformers, with an architecture similar to\nLlama (Touvron et al., 2023a;b; Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other\nrecent popular models, ranging in size from 20 million to 13 billion active parameters. Each of the\nsub-billion parameter models was trained on 1.7 trillion tokens, the 1 billion parameter model was\ntrained to 3 trillion tokens, the 7 billion parameter models were trained to 2, 3 and 4 trillion tokens,\nand the 13 billion parameter model to 5.6 trillion tokens. We additionally evaluate a mixture-of-\nexperts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.\nEach model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high\nspeed interconnect between each node, and between 2 and 128 nodes concurrently per training run.\nAll models except the 13B were trained in the same data center. See OLMo et al. (2025) for more\ninformation on our technical infrastructure.\n3.4 S IMULATING INFERENCE\nBecause we do not deploy our models, we do not collect or report data about real usage of our\nmodels. We instead report estimated costs associated with deployment of a subset of our models,\nalong with comparison models, with varying inference configurations. In reality, causal language\nmodels can have a variety of use cases and be deployed on a variety of hardware infrastructure. As\na representative deployment setting, we assume a setting in which users interact with the models\nvia chat; we collect measurements assuming models are served on a single H100 GPU via SGLang\n(Zheng et al., 2024). All three inference configurations used can be mapped to a previously proposed\nrealistic online inference scenario (Reddi et al., 2020; Peng et al., 2023). Specifically, other than\nthe “batching” scenario where all requests are sent instantaneously, the requests follow a Poisson\ndistribution, albeit at different rates that influence different batch sizes. The requests themselves\ncome from the ShareGPT dataset, 9 and each inference scenario involves the same sample of 2400\nprompts (same random seed). Input and output lengths, therefore, are the same in theory for a given\nmodel, but due to differences in tokenization and model context length, there are slight variations in\nmean input/output lengths across models, 225-250 and 190-230 tokens respectively.\nIn our inference experiments, we measure cumulative energy consumption using CodeCarbon\n(Courty et al., 2024) tracking, which was verified against the same time series monitoring used\nthroughout training. Notably, we measure total power and energy consumption associated with only\nthe relevant processes, excluding the overhead associated with, for example, holding the model in\nmemory or listening for requests.\nWe ran our inference simulations on our Jupiter cluster, used to train almost all of our models,\nbut we use only a single H100 GPU at a time. See Appendix A.1 for details about our inference\nmethodology and assumptions.\n4 R ESULTS\n4.1 B UILDING OUR MODELS\nIn this section we aim to report a full accounting of the environmental impact of training our series\nof models, from hardware manufacturing, to development, and the final training runs. We follow the\nmethodology outlined in Section 3.1 and Section 3.2.\nWhen calculating environmental impact, we use information from our data center providers and their\npower providers to measure the efficiency of each cluster. For Jupiter, the cluster used to train all\nmodels but the 13B, we assume a carbon intensity of 0.332 kg CO2 emitted per kWh, a power usage\neffectiveness (PUE) of 1.2, and a total water usage effectiveness (WUE) of 1.29 liters per kWh. For\nAugusta, the cluster used to train the 13B, we assume a carbon intensity of 0.351 kg CO 2 emitted\nper kWh, a PUE of 1.12, and a total WUE of 3.1 liters per kWh.\n9https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/\nShareGPT_V3_unfiltered_cleaned_split.json,anon8231489123/ShareGPT_Vicuna_unfiltered\n5", "sentences": [{"text": "Published as a conference paper at ICLR 2025\n3.3 M ODELS , DATA, AND HARDWARE\nMost of the models we evaluate are standard dense transformers, with an architecture similar to\nLlama (Touvron et al., 2023a;b;", "metadata": {}}, {"text": "Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other\nrecent popular models, ranging in size from 20 million to 13 billion active parameters.", "metadata": {}}, {"text": "Each of the\nsub-billion parameter models was trained on 1.7 trillion tokens, the 1 billion parameter model was\ntrained to 3 trillion tokens, the 7 billion parameter models were trained to 2, 3 and 4 trillion tokens,\nand the 13 billion parameter model to 5.6 trillion tokens.", "metadata": {}}, {"text": "We additionally evaluate a mixture-of-\nexperts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.", "metadata": {}}, {"text": "Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high\nspeed interconnect between each node, and between 2 and 128 nodes concurrently per training run.", "metadata": {}}, {"text": "All models except the 13B were trained in the same data center.", "metadata": {}}, {"text": "See OLMo et al.", "metadata": {}}, {"text": "(2025) for more\ninformation on our technical infrastructure.", "metadata": {}}, {"text": "3.4 S IMULATING INFERENCE\nBecause we do not deploy our models, we do not collect or report data about real usage of our\nmodels.", "metadata": {}}, {"text": "We instead report estimated costs associated with deployment of a subset of our models,\nalong with comparison models, with varying inference configurations.", "metadata": {}}, {"text": "In reality, causal language\nmodels can have a variety of use cases and be deployed on a variety of hardware infrastructure.", "metadata": {}}, {"text": "As\na representative deployment setting, we assume a setting in which users interact with the models\nvia chat;", "metadata": {}}, {"text": "we collect measurements assuming models are served on a single H100 GPU via SGLang\n(Zheng et al., 2024).", "metadata": {}}, {"text": "All three inference configurations used can be mapped to a previously proposed\nrealistic online inference scenario (Reddi et al., 2020;", "metadata": {}}, {"text": "Peng et al., 2023).", "metadata": {}}, {"text": "Specifically, other than\nthe “batching” scenario where all requests are sent instantaneously, the requests follow a Poisson\ndistribution, albeit at different rates that influence different batch sizes.", "metadata": {}}, {"text": "The requests themselves\ncome from the ShareGPT dataset, 9 and each inference scenario involves the same sample of 2400\nprompts (same random seed).", "metadata": {}}, {"text": "Input and output lengths, therefore, are the same in theory for a given\nmodel, but due to differences in tokenization and model context length, there are slight variations in\nmean input/output lengths across models, 225-250 and 190-230 tokens respectively.", "metadata": {}}, {"text": "In our inference experiments, we measure cumulative energy consumption using CodeCarbon\n(Courty et al., 2024) tracking, which was verified against the same time series monitoring used\nthroughout training.", "metadata": {}}, {"text": "Notably, we measure total power and energy consumption associated with only\nthe relevant processes, excluding the overhead associated with, for example, holding the model in\nmemory or listening for requests.", "metadata": {}}, {"text": "We ran our inference simulations on our Jupiter cluster, used to train almost all of our models,\nbut we use only a single H100 GPU at a time.", "metadata": {}}, {"text": "See Appendix A.1 for details about our inference\nmethodology and assumptions.", "metadata": {}}, {"text": "4 R ESULTS\n4.1 B UILDING OUR MODELS\nIn this section we aim to report a full accounting of the environmental impact of training our series\nof models, from hardware manufacturing, to development, and the final training runs.", "metadata": {}}, {"text": "We follow the\nmethodology outlined in Section 3.1 and Section 3.2.", "metadata": {}}, {"text": "When calculating environmental impact, we use information from our data center providers and their\npower providers to measure the efficiency of each cluster.", "metadata": {}}, {"text": "For Jupiter, the cluster used to train all\nmodels but the 13B, we assume a carbon intensity of 0.332 kg CO2 emitted per kWh, a power usage\neffectiveness (PUE) of 1.2, and a total water usage effectiveness (WUE) of 1.29 liters per kWh.", "metadata": {}}, {"text": "For\nAugusta, the cluster used to train the 13B, we assume a carbon intensity of 0.351 kg CO 2 emitted\nper kWh, a PUE of 1.12, and a total WUE of 3.1 liters per kWh.", "metadata": {}}, {"text": "9https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/\nShareGPT_V3_unfiltered_cleaned_split.json,anon8231489123/ShareGPT_Vicuna_unfiltered\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nTable 1: We developed our models in five groups, based on parameter count and architecture: less than 1\nbillion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active\nand 7 billion total parameters. We found that ∼70% of our developmental environmental impact came from\ndeveloping the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth\nof gasoline, and equal to about 7 and a half years of water used by the average person in the United States.\nGPU\nHours\nTotal\nMWh # Runs\nCarbon\nEmissions\n(tCO2eq)\nEquivalent to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...\n(water usage,\n1 person)\n<1B 29k 19 20 6 1 yr, 4 mo 24 3 mo\n7B 269k 196 375 65 13 yrs, 6 mo 252 2 yrs, 7 mo\n13B 191k 116 156 46 9 yrs, 7 mo 402 3 yrs, 7 mo\nMoE 27k 19 35 6 1 yr, 4 mo 24 3 mo\nTotal 680k 459 813 159 33 yrs, 1 mo 843 7 yrs, 5 mo\nHardware manufacturing NVIDIA does not release the embodied carbon emissions or water\nconsumption about the hardware it produces, so we assume the same embodied carbon emissions\nas Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU. There\nis little public information on how much water is required to produce a single GPU, though chip\nmanufacturing facilities require millions of liters per day. 10 Some estimates11 place TSMC water\nusage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which\nwe use for our analysis.\nWe additionally estimate the environmental impact from mining rare earth metals used during man-\nufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials\nconsumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-\ninch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an\nadditional 2.2 liters consumed and 0.013 kg CO2eq per GPU.\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of\n0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied\nimpacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in\ntotal, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.\nDevelopment Before launching our final training runs for each model, we ran a series of controlled\nexperiments to stabilize and improve our training setup, to explore different parameter initializations\nand mid-training recipes, and to determine our final hyperparameters and data mixtures through\nscaling law experiments (Bhagia et al., 2024). We ran these in five distinct groups: small models\nwith less than 1 billion parameters, 1 billion parameter models, 7 billion parameter models, 13\nbillion parameter models, and our mixture-of-experts model. We report detailed development costs\nfor each group in Table 1.\nUnsurprisingly, we find that the majority of development costs ( ∼70%) were incurred at the 7 and\n13 billion parameter scale, due to both the relative size of the model and our own prioritization, and\nwe see this both in the total environmental impact and the number of individual runs per category.\nUsing our data center’s efficiency factors, we find that our development runs led to 159 tCO 2eq\nemitted and 843 kL of water consumed.\nFinal training runs Finally, we fully trained our series of models, ranging from 20 million to\n13 billion active parameters, with detailed information provided in Table 2. As we saw during\ndevelopment, the majority of the cost incurred came from training our 7B and 13B models, which\nwe trained to 2 to 5 trillion tokens. We also see that the 1B dense model required about as much\nenergy per trillion tokens as the MoE model with 1B active parameters, though the MoE model was\nslightly less efficient, most likely due to the extra compute required for routing tokens. In summary,\nwe find that our training runs led to 312 tCO2eq emitted and 1,921 kL of water consumed.\n10https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/\ntsmc-arizona-water-use-recycling/74059522007/\n11https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/\n12https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nTable 1: We developed our models in five groups, based on parameter count and architecture: less than 1\nbillion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active\nand 7 billion total parameters.", "metadata": {}}, {"text": "We found that ∼70% of our developmental environmental impact came from\ndeveloping the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth\nof gasoline, and equal to about 7 and a half years of water used by the average person in the United States.", "metadata": {}}, {"text": "GPU\nHours\nTotal\nMWh # Runs\nCarbon\nEmissions\n(tCO2eq)\nEquivalent to...", "metadata": {}}, {"text": "(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...", "metadata": {}}, {"text": "(water usage,\n1 person)\n<1B 29k 19 20 6 1 yr, 4 mo 24 3 mo\n7B 269k 196 375 65 13 yrs, 6 mo 252 2 yrs, 7 mo\n13B 191k 116 156 46 9 yrs, 7 mo 402 3 yrs, 7 mo\nMoE 27k 19 35 6 1 yr, 4 mo 24 3 mo\nTotal 680k 459 813 159 33 yrs, 1 mo 843 7 yrs, 5 mo\nHardware manufacturing NVIDIA does not release the embodied carbon emissions or water\nconsumption about the hardware it produces, so we assume the same embodied carbon emissions\nas Luccioni et al.", "metadata": {}}, {"text": "(2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.", "metadata": {}}, {"text": "There\nis little public information on how much water is required to produce a single GPU, though chip\nmanufacturing facilities require millions of liters per day.", "metadata": {}}, {"text": "10 Some estimates11 place TSMC water\nusage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which\nwe use for our analysis.", "metadata": {}}, {"text": "We additionally estimate the environmental impact from mining rare earth metals used during man-\nufacturing, assuming an H100 is 0.1% rare earth metal by mass.", "metadata": {}}, {"text": "Mining 1 kg of rare earth materials\nconsumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-\ninch silicon wafer weighs 125 grams 12 and produces about 63 H100s.", "metadata": {}}, {"text": "13 14 Together, these add an\nadditional 2.2 liters consumed and 0.013 kg CO2eq per GPU.", "metadata": {}}, {"text": "Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of\n0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied\nimpacts is amortized over the assumed lifetime of the GPU.", "metadata": {}}, {"text": "We used 1.65 million GPU hours in\ntotal, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.", "metadata": {}}, {"text": "Development Before launching our final training runs for each model, we ran a series of controlled\nexperiments to stabilize and improve our training setup, to explore different parameter initializations\nand mid-training recipes, and to determine our final hyperparameters and data mixtures through\nscaling law experiments (Bhagia et al., 2024).", "metadata": {}}, {"text": "We ran these in five distinct groups: small models\nwith less than 1 billion parameters, 1 billion parameter models, 7 billion parameter models, 13\nbillion parameter models, and our mixture-of-experts model.", "metadata": {}}, {"text": "We report detailed development costs\nfor each group in Table 1.", "metadata": {}}, {"text": "Unsurprisingly, we find that the majority of development costs ( ∼70%) were incurred at the 7 and\n13 billion parameter scale, due to both the relative size of the model and our own prioritization, and\nwe see this both in the total environmental impact and the number of individual runs per category.", "metadata": {}}, {"text": "Using our data center’s efficiency factors, we find that our development runs led to 159 tCO 2eq\nemitted and 843 kL of water consumed.", "metadata": {}}, {"text": "Final training runs Finally, we fully trained our series of models, ranging from 20 million to\n13 billion active parameters, with detailed information provided in Table 2.", "metadata": {}}, {"text": "As we saw during\ndevelopment, the majority of the cost incurred came from training our 7B and 13B models, which\nwe trained to 2 to 5 trillion tokens.", "metadata": {}}, {"text": "We also see that the 1B dense model required about as much\nenergy per trillion tokens as the MoE model with 1B active parameters, though the MoE model was\nslightly less efficient, most likely due to the extra compute required for routing tokens.", "metadata": {}}, {"text": "In summary,\nwe find that our training runs led to 312 tCO2eq emitted and 1,921 kL of water consumed.", "metadata": {}}, {"text": "10https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/\ntsmc-arizona-water-use-recycling/74059522007/\n11https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/\n12https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n† denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M† 0.8 0.3 3 weeks 1 3 days\nOLMo 60M† 1.2 0.4 1 month 1.6 5 days\nOLMo 150M† 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M† 5 2 5 months 5.9 19 days\nOLMo 700M† 8 3 7 months 10 33 days\nOLMo 7B† 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models’ impact\nmight be if they were put into production. In contrast to §4.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report “breakeven” points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.", "metadata": {}}, {"text": "We\nfind that the environmental impact is quite high, even for our relatively small models.", "metadata": {}}, {"text": "Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S.", "metadata": {}}, {"text": "for about 17 years.", "metadata": {}}, {"text": "* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power.", "metadata": {}}, {"text": "See\nGroeneveld et al.", "metadata": {}}, {"text": "(2024) for more information.", "metadata": {}}, {"text": "† denotes unreleased models that were trained for various internal experiments.", "metadata": {}}, {"text": "Power\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv.", "metadata": {}}, {"text": "to...", "metadata": {}}, {"text": "(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv.", "metadata": {}}, {"text": "to...", "metadata": {}}, {"text": "(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M† 0.8 0.3 3 weeks 1 3 days\nOLMo 60M† 1.2 0.4 1 month 1.6 5 days\nOLMo 150M† 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M† 5 2 5 months 5.9 19 days\nOLMo 700M† 8 3 7 months 10 33 days\nOLMo 7B† 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted.", "metadata": {}}, {"text": "Using\nthe U.S.", "metadata": {}}, {"text": "Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S.", "metadata": {}}, {"text": "forests in\none year.", "metadata": {}}, {"text": "We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models.", "metadata": {}}, {"text": "However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.", "metadata": {}}, {"text": "While the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.", "metadata": {}}, {"text": "4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs;", "metadata": {}}, {"text": "that is, we explore the question of what our models’ impact\nmight be if they were put into production.", "metadata": {}}, {"text": "In contrast to §4.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models.", "metadata": {}}, {"text": "We include\ncomparisons with recent instruction-tuned models as well.", "metadata": {}}, {"text": "In Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests.", "metadata": {}}, {"text": "We additionally report “breakeven” points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nTable 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from\nShareGPT at varying request rates. Since the models were served on machines from the same cluster that\nour OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2\nrespectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption\nand carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported\nin this table account for the GPU processes associated with active inference, but not CPU or RAM associated\nwith e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.\nAlso of note is the relatively small variability in carbon emissions and water consumption across different model\nsizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated;\ngreater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report\n”break-even” points for Qwen 2.5 because its training costs are not public.\nModel\nRequest\nfreq.\n(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.\n(L)\nSeconds\nper 100 req.\n# Inf. for\nCO2 equiv.\nw/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.\n8 0.036 12.0 0.054 12.64 21.5 bil.\n1 0.160 53.1 0.238 100.58 4.83 bil.\nQwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 —\n8 0.053 17.6 0.079 12.77 —\n1 0.308 102.3 0.459 100.58 —\nLlama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.\n8 0.051 16.9 0.076 12.79 59.5 bil.\n1 0.333 110.6 0.496 100.64 9.12 bil.\nLlama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.\n8 0.060 19.9 0.089 13.09 7.52 bil.\n1 0.401 133.1 0.597 100.73 1.13 bil.\nOLMo 1 1B (3T) ∞ 0.004 1.3 0.006 0.99 18.2 bil.\n8 0.038 12.6 0.057 12.63 1.91 bil.\n1 0.165 54.8 0.246 100.58 441 mil.\nOLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.\n8 0.049 16.3 0.073 12.88 7.68 bil.\n1 0.358 118.9 0.533 100.54 1.05 bil.\nOLMo 2 13B ∞ 0.033 11.0 0.049 6.60 22.1 bil.\n8 0.057 18.9 0.085 13.05 12.8 bil.\n1 0.386 128.2 0.575 100.57 1.89 bil.\nOLMoE 0924 ∞ 0.006 2.0 0.009 1.70 21.7 bil.\n8 0.037 12.3 0.055 12.82 3.51 bil.\n1 0.151 50.1 0.225 100.60 861 mil.\ninferences in each scenario required for inference costs to be equal or greater to training costs. See\nTable 4 in Appendix A.1 for additional results.\nWe find that for most models tested, the number of inferences required to outweigh training costs\nis in the hundreds of millions to tens of billions, except for the most over-trained models. As many\nof these models were created to be efficient in deployment-focused scenarios – such as on edge\ndevices, or in popular online products – it is important to consider inference costs in addition to\ntraining costs. The largest model providers are producing up to hundreds of billions of tokens per\nday,17 highlighting that deployed models can quickly reach this tipping point.\n4.3 P OWER FLUCTUATIONS DURING TRAINING\nOne problem caused by training AI models at large scales is that the power demand starts and stops\nsuddenly (Dubey et al., 2024), which power grids can struggle to handle. When demand sharply\nrises, generation sources that can be quickly started and stopped – generally powered by fossil fuels,\nsuch as coal and natural gas – must be brought online quickly, increasing the marginal carbon inten-\nsity of the grid and potentially negatively impacting other consumers in cases where demand rises\nmore quickly than generation can handle. When demand sharply drops, excess power is discarded–\nby grounding the power or venting steam–until generation sources can spin down. Power grids\ncan generally manage some large variations (for example, when communities experience a sudden\n17https://x.com/sama/status/1756089361609981993\n8", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nTable 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from\nShareGPT at varying request rates.", "metadata": {}}, {"text": "Since the models were served on machines from the same cluster that\nour OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2\nrespectively, and carbon intensity of 0.332 kg CO2e / kWh.", "metadata": {}}, {"text": "Note the difference in units for energy consumption\nand carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L.", "metadata": {}}, {"text": "The measurements reported\nin this table account for the GPU processes associated with active inference, but not CPU or RAM associated\nwith e.g.", "metadata": {}}, {"text": "server overhead.", "metadata": {}}, {"text": "Thus, these numbers can be considered as lower bounds on usage in similar settings.", "metadata": {}}, {"text": "Also of note is the relatively small variability in carbon emissions and water consumption across different model\nsizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated;", "metadata": {}}, {"text": "greater peak efficiency does not guarantee efficient deployment if inference is not optimized.", "metadata": {}}, {"text": "We do not report\n”break-even” points for Qwen 2.5 because its training costs are not public.", "metadata": {}}, {"text": "Model\nRequest\nfreq.", "metadata": {}}, {"text": "(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.", "metadata": {}}, {"text": "(L)\nSeconds\nper 100 req.", "metadata": {}}, {"text": "# Inf.", "metadata": {}}, {"text": "for\nCO2 equiv.", "metadata": {}}, {"text": "w/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.", "metadata": {}}, {"text": "8 0.036 12.0 0.054 12.64 21.5 bil.", "metadata": {}}, {"text": "1 0.160 53.1 0.238 100.58 4.83 bil.", "metadata": {}}, {"text": "Qwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 —\n8 0.053 17.6 0.079 12.77 —\n1 0.308 102.3 0.459 100.58 —\nLlama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.", "metadata": {}}, {"text": "8 0.051 16.9 0.076 12.79 59.5 bil.", "metadata": {}}, {"text": "1 0.333 110.6 0.496 100.64 9.12 bil.", "metadata": {}}, {"text": "Llama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.", "metadata": {}}, {"text": "8 0.060 19.9 0.089 13.09 7.52 bil.", "metadata": {}}, {"text": "1 0.401 133.1 0.597 100.73 1.13 bil.", "metadata": {}}, {"text": "OLMo 1 1B (3T) ∞ 0.004 1.3 0.006 0.99 18.2 bil.", "metadata": {}}, {"text": "8 0.038 12.6 0.057 12.63 1.91 bil.", "metadata": {}}, {"text": "1 0.165 54.8 0.246 100.58 441 mil.", "metadata": {}}, {"text": "OLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.", "metadata": {}}, {"text": "8 0.049 16.3 0.073 12.88 7.68 bil.", "metadata": {}}, {"text": "1 0.358 118.9 0.533 100.54 1.05 bil.", "metadata": {}}, {"text": "OLMo 2 13B ∞ 0.033 11.0 0.049 6.60 22.1 bil.", "metadata": {}}, {"text": "8 0.057 18.9 0.085 13.05 12.8 bil.", "metadata": {}}, {"text": "1 0.386 128.2 0.575 100.57 1.89 bil.", "metadata": {}}, {"text": "OLMoE 0924 ∞ 0.006 2.0 0.009 1.70 21.7 bil.", "metadata": {}}, {"text": "8 0.037 12.3 0.055 12.82 3.51 bil.", "metadata": {}}, {"text": "1 0.151 50.1 0.225 100.60 861 mil.", "metadata": {}}, {"text": "inferences in each scenario required for inference costs to be equal or greater to training costs.", "metadata": {}}, {"text": "See\nTable 4 in Appendix A.1 for additional results.", "metadata": {}}, {"text": "We find that for most models tested, the number of inferences required to outweigh training costs\nis in the hundreds of millions to tens of billions, except for the most over-trained models.", "metadata": {}}, {"text": "As many\nof these models were created to be efficient in deployment-focused scenarios – such as on edge\ndevices, or in popular online products – it is important to consider inference costs in addition to\ntraining costs.", "metadata": {}}, {"text": "The largest model providers are producing up to hundreds of billions of tokens per\nday,17 highlighting that deployed models can quickly reach this tipping point.", "metadata": {}}, {"text": "4.3 P OWER FLUCTUATIONS DURING TRAINING\nOne problem caused by training AI models at large scales is that the power demand starts and stops\nsuddenly (Dubey et al., 2024), which power grids can struggle to handle.", "metadata": {}}, {"text": "When demand sharply\nrises, generation sources that can be quickly started and stopped – generally powered by fossil fuels,\nsuch as coal and natural gas – must be brought online quickly, increasing the marginal carbon inten-\nsity of the grid and potentially negatively impacting other consumers in cases where demand rises\nmore quickly than generation can handle.", "metadata": {}}, {"text": "When demand sharply drops, excess power is discarded–\nby grounding the power or venting steam–until generation sources can spin down.", "metadata": {}}, {"text": "Power grids\ncan generally manage some large variations (for example, when communities experience a sudden\n17https://x.com/sama/status/1756089361609981993\n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\npower outage), but as we add more variability to the system, it becomes more difficult to maintain\nthis delicate balance, and infrastructure is not set up to handle frequent, large fluctuations.\nIn Figure 2, we show a snapshot of our model’s GPU power consumption during pre-training. We\nfind that power consumption is not consistent – instead, power is consistentwhile the model is train-\ning, but drops quickly while saving checkpoints. Though our models are relatively small, and we\nhave since improved checkpointing performance, other model developers have experienced similar\nissues caused by checkpointing and synchronization between nodes (Dubey et al., 2024).\n5 D ISCUSSION\n5.1 M ORE TRANSPARENCY IS (STILL ) N EEDED\nWhile many model developers–including some of the largest for-profit entities operating in this\nspace–make best efforts to report at least part of the cost of building their AI systems (Dubey et al.,\n2024; Gemma Team et al., 2024), more transparency is still needed throughout the development\npipeline. The EU AI Act, 18 and some proposed legislation, such as the Artificial Intelligence Envi-\nronmental Impacts Act19 in the United States, would start the process for defining voluntary environ-\nmental impact reporting standards for model developers, but until such standards are widespread in\nthe community, improved transparency can only come through voluntary efforts by companies and\nresearch organizations. Policy action is needed to ensure there is public visibility into environmental\nimpacts across the entire supply chain, from hardware manufacturing, data center construction, and\nenergy production, all the way through to model deployment and inference.\nEmbodied emissions are still an enigma Though a vital piece of all model development\npipelines, the environmental impact of manufacturing the GPUs used is essentially unknown. In\nprevious work, Wu et al. (2022) and Luccioni et al. (2023) highlighted the fact that researchers\nfocused on AI’s environmental impact are forced to use unreliable estimates of the cost of manufac-\nturing state-of-the-art computational hardware, and the situation is no better now, nearly two years\nlater. Many companies that manufacture other pieces of data center hardware disclose estimates of\nthe lifetime environmental impact,20 and until GPU manufacturers release similar information–on a\nvoluntary or compulsory basis–this will not improve.\nDevelopment costs are substantial, and unreported As reported in Section 4.1, we present de-\ntailed information on the cost of developing our training pipeline, in contrast with previous work.\nWe found that development costs–associated with failed runs, hyperparameter searches, testing ar-\nchitecture changes, and more–are responsible for a substantial portion of the total environmental\nimpact of creating our systems, highlighting a need for more transparency from developers. This\nis especially important in light of AutoML tools, where many models may be automatically trained\nwhile searching for a solution, and scaling law experiments, where smaller models are trained to\npredict the performance of larger models, and then discarded (Li et al., 2024; Bhagia et al., 2024).\nWater costs are real, and under-explored While under-explored in previous work, AI’s growing\nwater consumption is beginning to receive more and more attention 21 (Li et al., 2023), though not\nas much as it may deserve. As shown in Section 4.1, even training a series of comparatively small\nmodels uses a large amount of water, the amount of which is also drastically impacted by both\nthe cooling systems used in data centers as well as the power generation methods used. Without\nmore transparency from developers on when, where, and how they are training their models, it will\ncontinue to be difficult to quantify the scale of the issue, stymieing efforts to address it.\n5.2 S MALL CHOICES DURING TRAINING CAN HAVE LARGE IMPACTS\nWhile many issues relating to transparency require action from corporations and large research\ngroups, choices made during training have a large effect downstream.\n18https://artificialintelligenceact.eu/article/95/\n19https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_\nact_of_2024_-_020124pdf.pdf\n20https://www.hpe.com/psnow/doc/a50005151enw\n21https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/\n9", "sentences": [{"text": "Published as a conference paper at ICLR 2025\npower outage), but as we add more variability to the system, it becomes more difficult to maintain\nthis delicate balance, and infrastructure is not set up to handle frequent, large fluctuations.", "metadata": {}}, {"text": "In Figure 2, we show a snapshot of our model’s GPU power consumption during pre-training.", "metadata": {}}, {"text": "We\nfind that power consumption is not consistent – instead, power is consistentwhile the model is train-\ning, but drops quickly while saving checkpoints.", "metadata": {}}, {"text": "Though our models are relatively small, and we\nhave since improved checkpointing performance, other model developers have experienced similar\nissues caused by checkpointing and synchronization between nodes (Dubey et al., 2024).", "metadata": {}}, {"text": "5 D ISCUSSION\n5.1 M ORE TRANSPARENCY IS (STILL ) N EEDED\nWhile many model developers–including some of the largest for-profit entities operating in this\nspace–make best efforts to report at least part of the cost of building their AI systems (Dubey et al.,\n2024;", "metadata": {}}, {"text": "Gemma Team et al., 2024), more transparency is still needed throughout the development\npipeline.", "metadata": {}}, {"text": "The EU AI Act, 18 and some proposed legislation, such as the Artificial Intelligence Envi-\nronmental Impacts Act19 in the United States, would start the process for defining voluntary environ-\nmental impact reporting standards for model developers, but until such standards are widespread in\nthe community, improved transparency can only come through voluntary efforts by companies and\nresearch organizations.", "metadata": {}}, {"text": "Policy action is needed to ensure there is public visibility into environmental\nimpacts across the entire supply chain, from hardware manufacturing, data center construction, and\nenergy production, all the way through to model deployment and inference.", "metadata": {}}, {"text": "Embodied emissions are still an enigma Though a vital piece of all model development\npipelines, the environmental impact of manufacturing the GPUs used is essentially unknown.", "metadata": {}}, {"text": "In\nprevious work, Wu et al.", "metadata": {}}, {"text": "(2022) and Luccioni et al.", "metadata": {}}, {"text": "(2023) highlighted the fact that researchers\nfocused on AI’s environmental impact are forced to use unreliable estimates of the cost of manufac-\nturing state-of-the-art computational hardware, and the situation is no better now, nearly two years\nlater.", "metadata": {}}, {"text": "Many companies that manufacture other pieces of data center hardware disclose estimates of\nthe lifetime environmental impact,20 and until GPU manufacturers release similar information–on a\nvoluntary or compulsory basis–this will not improve.", "metadata": {}}, {"text": "Development costs are substantial, and unreported As reported in Section 4.1, we present de-\ntailed information on the cost of developing our training pipeline, in contrast with previous work.", "metadata": {}}, {"text": "We found that development costs–associated with failed runs, hyperparameter searches, testing ar-\nchitecture changes, and more–are responsible for a substantial portion of the total environmental\nimpact of creating our systems, highlighting a need for more transparency from developers.", "metadata": {}}, {"text": "This\nis especially important in light of AutoML tools, where many models may be automatically trained\nwhile searching for a solution, and scaling law experiments, where smaller models are trained to\npredict the performance of larger models, and then discarded (Li et al., 2024;", "metadata": {}}, {"text": "Bhagia et al., 2024).", "metadata": {}}, {"text": "Water costs are real, and under-explored While under-explored in previous work, AI’s growing\nwater consumption is beginning to receive more and more attention 21 (Li et al., 2023), though not\nas much as it may deserve.", "metadata": {}}, {"text": "As shown in Section 4.1, even training a series of comparatively small\nmodels uses a large amount of water, the amount of which is also drastically impacted by both\nthe cooling systems used in data centers as well as the power generation methods used.", "metadata": {}}, {"text": "Without\nmore transparency from developers on when, where, and how they are training their models, it will\ncontinue to be difficult to quantify the scale of the issue, stymieing efforts to address it.", "metadata": {}}, {"text": "5.2 S MALL CHOICES DURING TRAINING CAN HAVE LARGE IMPACTS\nWhile many issues relating to transparency require action from corporations and large research\ngroups, choices made during training have a large effect downstream.", "metadata": {}}, {"text": "18https://artificialintelligenceact.eu/article/95/\n19https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_\nact_of_2024_-_020124pdf.pdf\n20https://www.hpe.com/psnow/doc/a50005151enw\n21https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/\n9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nFigure 2: Average GPU power for a single node for\nthe first 300 logging steps during OLMo 2 7B train-\ning. The first spike is the beginning of training, and\neach drop happens when a model checkpoint is saved.\nWhen actively training, the average GPU power is over\n600W, over 85% of an H100’s maximum power draw\nof 700W, and during checkpointing, power usage drops\nto just over 100W, or about 15% maximum.\nSmaller models are cheaper to train and use,\nbut at what cost? Until recently, to achieve\nhigh model performance, a large model was\nneeded. Compute-optimal scaling laws for neu-\nral network training (Hoffmann et al., 2022;\nKaplan et al., 2020) imply that it is more ef-\nficient to put more data into a larger model,\nbecause of diminishing returns from “over-\ntraining” a small model. This meant that mod-\nels were expensive to both train and deploy,\nlimiting how widespread they could become,\nand how financially feasible they were to be\nused in a variety of scenarios.\nRecently, however, continuing to train models\non more and more tokens beyond the “compute-\noptimal” limit22 has been extremely successful\nin making “deployment-optimized” models that\ncan be substantially cheaper to perform infer-\nence with. This has led to an explosion in both\ntraining cost for small models, and total infer-\nence compute cost, as API-based models be-\ncome cheaper to use2324 and small models are deployed on-device (Gunter et al., 2024; Abdin et al.,\n2024). This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency in-\ncreases, overall consumption of that resource tends to increase, rather than decrease. In other words,\nas the cost of training models decreases, the downstream impact may continue to grow.\nThis is especially clear in context of our results in Section 4.2, showing that though the raw num-\nber of inferences required to outweigh training is objectively quite large, smaller models are being\ndeployed in many new scenarios that will drastically increase their total usage. Many inference use\ncases are also not able to be batched (e.g. generating text on a phone for immediate use), meaning\nthat deployers cannot schedule these requests to take advantage of cheaper or cleaner energy, and\nmust make use of immediately available power. Given that this trend will most likely only accelerate,\nit is vital that we improve transparency into the total cost of deployment in all scenarios.\nPower fluctuations reveal inefficiencies at best, challenges to power grid control at worst\nWhile it is known that the dramatic spike in power consumption at the beginning of training and the\nsubsequent drop at the end are problematic for power grid operators at large scales, little has been\ndiscussed publicly about how power consumption changes throughout training. We found that our\nmodels, using an optimized code base and publicly available tooling, sees rapid power fluctuations\nthroughout training caused by the commonplace practice of frequently saving model checkpoints.\nThis means that without careful engineering, one training run can cause thousands of rapid power\nfluctuations, which poses an immediate challenge for large-scale LLM training in data centers, which\ntypically source energy directly from power providers. Generated power needs to go somewhere,\nand rapid, large drops in consumption during training breaks common assumptions about data center\nsupply and demand, leading to significant control challenges in power systems. While some frame-\nworks have begun to implement workarounds to manage this issue, 25 more awareness is needed on\nthe part of researchers and engineers as training runs scale to tens of thousands of GPUs 26 or more,\nas even some of the largest model developers encounter difficulties from regularly shifting power\ndemand throughout training due to checkpointing, awaiting collective communications, and other\nunforeseen and potentially catastrophic failures (Dubey et al., 2024). We emphasize that address-\ning this will require more comprehensive solutions such as parallelized checkpointing, improved\ndemand response in data centers running large AI workloads, and new, heterogeneous methods for\ndistributed training spanning software, hardware, and scheduling.\n22e.g. scaling from 1 to 2 to 15T tokens for Llama 1, 2, and 3 (Touvron et al., 2023a;b; Dubey et al., 2024)\n23https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n24https://developers.googleblog.com/en/gemini-15-flash-updates-google-ai-studio-gemini-api/\n25E.g. the new PYTORCH NO POWERPLANT BLOWUP environment variable in PyTorch.\n26https://time.com/7021709/elon-musk-xai-grok-memphis/\n10", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nFigure 2: Average GPU power for a single node for\nthe first 300 logging steps during OLMo 2 7B train-\ning.", "metadata": {}}, {"text": "The first spike is the beginning of training, and\neach drop happens when a model checkpoint is saved.", "metadata": {}}, {"text": "When actively training, the average GPU power is over\n600W, over 85% of an H100’s maximum power draw\nof 700W, and during checkpointing, power usage drops\nto just over 100W, or about 15% maximum.", "metadata": {}}, {"text": "Smaller models are cheaper to train and use,\nbut at what cost?", "metadata": {}}, {"text": "Until recently, to achieve\nhigh model performance, a large model was\nneeded.", "metadata": {}}, {"text": "Compute-optimal scaling laws for neu-\nral network training (Hoffmann et al., 2022;", "metadata": {}}, {"text": "Kaplan et al., 2020) imply that it is more ef-\nficient to put more data into a larger model,\nbecause of diminishing returns from “over-\ntraining” a small model.", "metadata": {}}, {"text": "This meant that mod-\nels were expensive to both train and deploy,\nlimiting how widespread they could become,\nand how financially feasible they were to be\nused in a variety of scenarios.", "metadata": {}}, {"text": "Recently, however, continuing to train models\non more and more tokens beyond the “compute-\noptimal” limit22 has been extremely successful\nin making “deployment-optimized” models that\ncan be substantially cheaper to perform infer-\nence with.", "metadata": {}}, {"text": "This has led to an explosion in both\ntraining cost for small models, and total infer-\nence compute cost, as API-based models be-\ncome cheaper to use2324 and small models are deployed on-device (Gunter et al., 2024;", "metadata": {}}, {"text": "Abdin et al.,\n2024).", "metadata": {}}, {"text": "This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency in-\ncreases, overall consumption of that resource tends to increase, rather than decrease.", "metadata": {}}, {"text": "In other words,\nas the cost of training models decreases, the downstream impact may continue to grow.", "metadata": {}}, {"text": "This is especially clear in context of our results in Section 4.2, showing that though the raw num-\nber of inferences required to outweigh training is objectively quite large, smaller models are being\ndeployed in many new scenarios that will drastically increase their total usage.", "metadata": {}}, {"text": "Many inference use\ncases are also not able to be batched (e.g.", "metadata": {}}, {"text": "generating text on a phone for immediate use), meaning\nthat deployers cannot schedule these requests to take advantage of cheaper or cleaner energy, and\nmust make use of immediately available power.", "metadata": {}}, {"text": "Given that this trend will most likely only accelerate,\nit is vital that we improve transparency into the total cost of deployment in all scenarios.", "metadata": {}}, {"text": "Power fluctuations reveal inefficiencies at best, challenges to power grid control at worst\nWhile it is known that the dramatic spike in power consumption at the beginning of training and the\nsubsequent drop at the end are problematic for power grid operators at large scales, little has been\ndiscussed publicly about how power consumption changes throughout training.", "metadata": {}}, {"text": "We found that our\nmodels, using an optimized code base and publicly available tooling, sees rapid power fluctuations\nthroughout training caused by the commonplace practice of frequently saving model checkpoints.", "metadata": {}}, {"text": "This means that without careful engineering, one training run can cause thousands of rapid power\nfluctuations, which poses an immediate challenge for large-scale LLM training in data centers, which\ntypically source energy directly from power providers.", "metadata": {}}, {"text": "Generated power needs to go somewhere,\nand rapid, large drops in consumption during training breaks common assumptions about data center\nsupply and demand, leading to significant control challenges in power systems.", "metadata": {}}, {"text": "While some frame-\nworks have begun to implement workarounds to manage this issue, 25 more awareness is needed on\nthe part of researchers and engineers as training runs scale to tens of thousands of GPUs 26 or more,\nas even some of the largest model developers encounter difficulties from regularly shifting power\ndemand throughout training due to checkpointing, awaiting collective communications, and other\nunforeseen and potentially catastrophic failures (Dubey et al., 2024).", "metadata": {}}, {"text": "We emphasize that address-\ning this will require more comprehensive solutions such as parallelized checkpointing, improved\ndemand response in data centers running large AI workloads, and new, heterogeneous methods for\ndistributed training spanning software, hardware, and scheduling.", "metadata": {}}, {"text": "22e.g.", "metadata": {}}, {"text": "scaling from 1 to 2 to 15T tokens for Llama 1, 2, and 3 (Touvron et al., 2023a;b;", "metadata": {}}, {"text": "Dubey et al., 2024)\n23https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n24https://developers.googleblog.com/en/gemini-15-flash-updates-google-ai-studio-gemini-api/\n25E.g.", "metadata": {}}, {"text": "the new PYTORCH NO POWERPLANT BLOWUP environment variable in PyTorch.", "metadata": {}}, {"text": "26https://time.com/7021709/elon-musk-xai-grok-memphis/\n10", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=Im2.png] Size: 1406x1028, Data: 182841 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=Im2.png] Size: 1406x1028, Data: 182841 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "Im2.png", "image_width": 1406, "image_height": 1028, "attachment_type": "image", "has_image_data": true, "image_data_size": 182841}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nREFERENCES\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen\nBach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko,\nJohan Bjorck, S ´ebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dong-\ndong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang\nDai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit\nGarg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao,\nRussell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin\nJin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim,\nLev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden,\nXihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong\nLuo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio C ´esar Teodoro\nMendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-\nBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo\nde Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim,\nMichael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp\nWitte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Ji-\nlong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan,\nChenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your\nphone, 2024. URL https://arxiv.org/abs/2404.14219.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit\nSanghai. GQA: Training generalized multi-query transformer models from multi-head check-\npoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing , pp. 4895–4901, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL\nhttps://aclanthology.org/2023.emnlp-main.298.\nAkshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh\nJha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Han-\nnaneh Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL\nhttps://arxiv.org/abs/2412.04403.\nCallum Browning, Stephen Northey, Nawshad Haque, Warren Bruckard, and Mark Cooksey. Life\nCycle Assessment of Rare Earth Production from Monazite , pp. 83–88. Springer International\nPublishing, Cham, 2016. ISBN 978-3-319-48768-7. doi: 10.1007/978-3-319-48768-7 12. URL\nhttps://doi.org/10.1007/978-3-319-48768-7_12 .\nBenoit Courty, Victor Schmidt, Goyal-Kamal, MarionCoutarel, Luis Blanche, Boris Feld, inimaz,\nJ´er´emy Lecourt, LiamConnell, SabAmine, supatomic, Mathilde L ´eval, Alexis Cruveiller, oumi-\nnasara, Franklin Zhao, Aditya Joshi, Christian Bauer, Amine Saboni, Patrick LLORET, Alexis\nBogroff, Niko Laskaris, Hugues de Lavoreille, Alexandre Phiev, Edoardo Abati, rosekelly6400,\nDouglas Blank, Ziyao Wang, Lucas Ot ´avio, and Armin Catovic. mlco2/codecarbon: v2.7.1,\nSeptember 2024. URL https://doi.org/10.5281/zenodo.13744486.\nJesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma\nStrubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Mea-\nsuring the carbon intensity of ai in cloud instances, 2022. URL https://arxiv.org/abs/\n2206.05229.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L ´eonard\nHussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex\n11", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nREFERENCES\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen\nBach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko,\nJohan Bjorck, S ´ebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dong-\ndong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang\nDai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit\nGarg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao,\nRussell J.", "metadata": {}}, {"text": "Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin\nJin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim,\nLev Kurilenko, James R.", "metadata": {}}, {"text": "Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden,\nXihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong\nLuo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio C ´esar Teodoro\nMendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-\nBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo\nde Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim,\nMichael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp\nWitte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Ji-\nlong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan,\nChenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou.", "metadata": {}}, {"text": "Phi-3 technical report: A highly capable language model locally on your\nphone, 2024.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/2404.14219.", "metadata": {}}, {"text": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit\nSanghai.", "metadata": {}}, {"text": "GQA: Training generalized multi-query transformer models from multi-head check-\npoints.", "metadata": {}}, {"text": "In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing , pp.", "metadata": {}}, {"text": "4895–4901, Singapore, December\n2023.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "doi: 10.18653/v1/2023.emnlp-main.298.", "metadata": {}}, {"text": "URL\nhttps://aclanthology.org/2023.emnlp-main.298.", "metadata": {}}, {"text": "Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh\nJha, Luca Soldaini, Noah A.", "metadata": {}}, {"text": "Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Han-\nnaneh Hajishirzi.", "metadata": {}}, {"text": "Establishing task scaling laws via compute-efficient model ladders, 2024.", "metadata": {}}, {"text": "URL\nhttps://arxiv.org/abs/2412.04403.", "metadata": {}}, {"text": "Callum Browning, Stephen Northey, Nawshad Haque, Warren Bruckard, and Mark Cooksey.", "metadata": {}}, {"text": "Life\nCycle Assessment of Rare Earth Production from Monazite , pp.", "metadata": {}}, {"text": "83–88.", "metadata": {}}, {"text": "Springer International\nPublishing, Cham, 2016.", "metadata": {}}, {"text": "ISBN 978-3-319-48768-7.", "metadata": {}}, {"text": "doi: 10.1007/978-3-319-48768-7 12.", "metadata": {}}, {"text": "URL\nhttps://doi.org/10.1007/978-3-319-48768-7_12 .", "metadata": {}}, {"text": "Benoit Courty, Victor Schmidt, Goyal-Kamal, MarionCoutarel, Luis Blanche, Boris Feld, inimaz,\nJ´er´emy Lecourt, LiamConnell, SabAmine, supatomic, Mathilde L ´eval, Alexis Cruveiller, oumi-\nnasara, Franklin Zhao, Aditya Joshi, Christian Bauer, Amine Saboni, Patrick LLORET, Alexis\nBogroff, Niko Laskaris, Hugues de Lavoreille, Alexandre Phiev, Edoardo Abati, rosekelly6400,\nDouglas Blank, Ziyao Wang, Lucas Ot ´avio, and Armin Catovic.", "metadata": {}}, {"text": "mlco2/codecarbon: v2.7.1,\nSeptember 2024.", "metadata": {}}, {"text": "URL https://doi.org/10.5281/zenodo.13744486.", "metadata": {}}, {"text": "Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma\nStrubell, Alexandra Sasha Luccioni, Noah A.", "metadata": {}}, {"text": "Smith, Nicole DeCario, and Will Buchanan.", "metadata": {}}, {"text": "Mea-\nsuring the carbon intensity of ai in cloud instances, 2022.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/\n2206.05229.", "metadata": {}}, {"text": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.", "metadata": {}}, {"text": "The llama 3 herd of models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2407.21783, 2024.", "metadata": {}}, {"text": "Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L ´eonard\nHussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex\n11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nBotev, Alex Castro-Ros, Ambrose Slone, Am´elie H´eliou, Andrea Tacchetti, Anna Bulanova, An-\ntonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo,\nCl´ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric\nNoland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Hen-\nryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu,\nJustin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko\nYotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo\nLiu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree\nPandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh\nGiang, Cl´ement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin\nGhahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah\nFiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on\ngemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295.\nAlistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. How data centers and the\nenergy sector can sate ai’s hunger for power.McKinsey and Company, 2024.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Au-\nthur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,\nTushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crys-\ntal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh\nShah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi,\nNathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini,\nNoah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15789–15809,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n2024.acl-long.841. URL https://aclanthology.org/2024.acl-long.841.\nTom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong\nYin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Pee-\nbles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans,\nTao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao,\nZaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo\nBencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Do-\nminik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu,\nFred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff\nLai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega,\nKelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole\nHay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi,\nRoman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang\nMa, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar,\nXin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun-\nsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy\nNarayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris\nChaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin,\nIrina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mah-\nyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr\nMaj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma\nRao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar,\nWencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia,\nZhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, 2024. URL\nhttps://arxiv.org/abs/2407.21075.\n12", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nBotev, Alex Castro-Ros, Ambrose Slone, Am´elie H´eliou, Andrea Tacchetti, Anna Bulanova, An-\ntonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A.", "metadata": {}}, {"text": "Choquette-Choo,\nCl´ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric\nNoland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Hen-\nryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu,\nJustin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko\nYotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo\nLiu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree\nPandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh\nGiang, Cl´ement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin\nGhahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah\nFiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.", "metadata": {}}, {"text": "Gemma: Open models based on\ngemini research and technology, 2024.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/2403.08295.", "metadata": {}}, {"text": "Alistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva.", "metadata": {}}, {"text": "How data centers and the\nenergy sector can sate ai’s hunger for power.McKinsey and Company, 2024.", "metadata": {}}, {"text": "Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Au-\nthur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,\nTushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crys-\ntal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh\nShah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi,\nNathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini,\nNoah Smith, and Hannaneh Hajishirzi.", "metadata": {}}, {"text": "OLMo: Accelerating the science of language models.", "metadata": {}}, {"text": "In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp.", "metadata": {}}, {"text": "15789–15809,\nBangkok, Thailand, August 2024.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "doi: 10.18653/v1/\n2024.acl-long.841.", "metadata": {}}, {"text": "URL https://aclanthology.org/2024.acl-long.841.", "metadata": {}}, {"text": "Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong\nYin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Pee-\nbles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans,\nTao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao,\nZaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo\nBencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Do-\nminik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu,\nFred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff\nLai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega,\nKelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole\nHay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi,\nRoman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang\nMa, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar,\nXin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun-\nsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy\nNarayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris\nChaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin,\nIrina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mah-\nyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr\nMaj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma\nRao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar,\nWencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia,\nZhile Ren, and Zhongzheng Ren.", "metadata": {}}, {"text": "Apple intelligence foundation language models, 2024.", "metadata": {}}, {"text": "URL\nhttps://arxiv.org/abs/2407.21075.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aure-\nlia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and\nL. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL\nhttps://api.semanticscholar.org/CorpusID:247778764.\nWilliam Stanley Jevons. The Coal Question; An Inquiry Concerning the Progress of the Nation, and\nthe Probable Exhaustion of Our Coal Mines. London: Macmillan and Co, 1865.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal,\nEtash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Rein-\nhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Al-\nbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,\nJosh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Il-\nharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao\nNguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Se-\nwoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev,\nStephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kol-\nlar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\nDatacomp-lm: In search of the next generation of training sets for language models, 2024. URL\nhttps://arxiv.org/abs/2406.11794.\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making ai less ”thirsty”: Uncover-\ning and addressing the secret water footprint of ai models, 2023. URLhttps://arxiv.org/\nabs/2304.03271.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon foot-\nprint of bloom, a 176b parameter language model. Journal of Machine Learning Research , 24\n(253):1–15, 2023. URL http://jmlr.org/papers/v24/23-0069.html.\nSasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the\ncost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Trans-\nparency, pp. 85–99, 2024.\nSachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chen-\nfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Moham-\nmad Rastegari. OpenELM: An efficient language model family with open training and inference\nframework. In Workshop on Efficient Systems for Foundation Models II @ ICML2024 , 2024.\nURL https://openreview.net/forum?id=XNMbTkxroF.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bha-\ngia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord,\nTaira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha\nDziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William\nMerrill, Lester James V . Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py-\natkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2\nolmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656.\nHao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle\nLo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A.\nSmith, and Hannaneh Hajishirzi. Efficiency pentathlon: A standardized arena for efficiency eval-\nuation, 2023. URL https://arxiv.org/abs/2307.09701.\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-\nJean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh\nChukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott\n13", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aure-\nlia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.", "metadata": {}}, {"text": "Rae, Oriol Vinyals, and\nL.", "metadata": {}}, {"text": "Sifre.", "metadata": {}}, {"text": "Training compute-optimal large language models.", "metadata": {}}, {"text": "ArXiv, abs/2203.15556, 2022.", "metadata": {}}, {"text": "URL\nhttps://api.semanticscholar.org/CorpusID:247778764.", "metadata": {}}, {"text": "William Stanley Jevons.", "metadata": {}}, {"text": "The Coal Question;", "metadata": {}}, {"text": "An Inquiry Concerning the Progress of the Nation, and\nthe Probable Exhaustion of Our Coal Mines.", "metadata": {}}, {"text": "London: Macmillan and Co, 1865.", "metadata": {}}, {"text": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.", "metadata": {}}, {"text": "Scaling laws for neural language\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2001.08361, 2020.", "metadata": {}}, {"text": "Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal,\nEtash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Rein-\nhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Al-\nbalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh,\nJosh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Il-\nharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao\nNguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Se-\nwoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev,\nStephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kol-\nlar, Alexandros G.", "metadata": {}}, {"text": "Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.", "metadata": {}}, {"text": "Datacomp-lm: In search of the next generation of training sets for language models, 2024.", "metadata": {}}, {"text": "URL\nhttps://arxiv.org/abs/2406.11794.", "metadata": {}}, {"text": "Pengfei Li, Jianyi Yang, Mohammad A.", "metadata": {}}, {"text": "Islam, and Shaolei Ren.", "metadata": {}}, {"text": "Making ai less ”thirsty”: Uncover-\ning and addressing the secret water footprint of ai models, 2023.", "metadata": {}}, {"text": "URLhttps://arxiv.org/\nabs/2304.03271.", "metadata": {}}, {"text": "Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "Estimating the carbon foot-\nprint of bloom, a 176b parameter language model.", "metadata": {}}, {"text": "Journal of Machine Learning Research , 24\n(253):1–15, 2023.", "metadata": {}}, {"text": "URL http://jmlr.org/papers/v24/23-0069.html.", "metadata": {}}, {"text": "Sasha Luccioni, Yacine Jernite, and Emma Strubell.", "metadata": {}}, {"text": "Power hungry processing: Watts driving the\ncost of ai deployment?", "metadata": {}}, {"text": "In The 2024 ACM Conference on Fairness, Accountability, and Trans-\nparency, pp.", "metadata": {}}, {"text": "85–99, 2024.", "metadata": {}}, {"text": "Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chen-\nfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Moham-\nmad Rastegari.", "metadata": {}}, {"text": "OpenELM: An efficient language model family with open training and inference\nframework.", "metadata": {}}, {"text": "In Workshop on Efficient Systems for Foundation Models II @ ICML2024 , 2024.", "metadata": {}}, {"text": "URL https://openreview.net/forum?id=XNMbTkxroF.", "metadata": {}}, {"text": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bha-\ngia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord,\nTaira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha\nDziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William\nMerrill, Lester James V .", "metadata": {}}, {"text": "Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py-\natkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A.", "metadata": {}}, {"text": "Smith, and Hannaneh Hajishirzi.", "metadata": {}}, {"text": "2\nolmo 2 furious, 2025.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/2501.00656.", "metadata": {}}, {"text": "Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E.", "metadata": {}}, {"text": "Peters, Jared Fernandez, Tom Sherborne, Kyle\nLo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A.", "metadata": {}}, {"text": "Smith, and Hannaneh Hajishirzi.", "metadata": {}}, {"text": "Efficiency pentathlon: A standardized arena for efficiency eval-\nuation, 2023.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/2307.09701.", "metadata": {}}, {"text": "Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-\nJean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh\nChukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J.", "metadata": {}}, {"text": "Scott\n13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nGardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar,\nDavid Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius,\nColin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish\nSirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi\nYamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. Mlperf in-\nference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer\nArchitecture (ISCA), pp. 446–459, 2020. doi: 10.1109/ISCA45697.2020.00045.\nPaul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water\nuse embedded in purchased electricity, 2020. URL https://www.wri.org/research/\nguidance-calculating-water-use-embedded-purchased-electricity .\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12):\n54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https://doi.org/\n10.1145/3381831.\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek,\nJonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage\nreport. 2024.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 13693–13696, 2020.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023a. URL https://arxiv.org/abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental impli-\ncations, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813,\n2022.\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\nChristos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang:\nEfficient execution of structured language model programs, 2024. URL https://arxiv.\norg/abs/2312.07104.\n14", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nGardner, Itay Hubara, Sachin Idgunji, Thomas B.", "metadata": {}}, {"text": "Jablin, Jeff Jiao, Tom St.", "metadata": {}}, {"text": "John, Pankaj Kanwar,\nDavid Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius,\nColin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish\nSirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi\nYamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou.", "metadata": {}}, {"text": "Mlperf in-\nference benchmark.", "metadata": {}}, {"text": "In 2020 ACM/IEEE 47th Annual International Symposium on Computer\nArchitecture (ISCA), pp.", "metadata": {}}, {"text": "446–459, 2020.", "metadata": {}}, {"text": "doi: 10.1109/ISCA45697.2020.00045.", "metadata": {}}, {"text": "Paul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore.", "metadata": {}}, {"text": "Guidance for calculating water\nuse embedded in purchased electricity, 2020.", "metadata": {}}, {"text": "URL https://www.wri.org/research/\nguidance-calculating-water-use-embedded-purchased-electricity .", "metadata": {}}, {"text": "Roy Schwartz, Jesse Dodge, Noah A.", "metadata": {}}, {"text": "Smith, and Oren Etzioni.", "metadata": {}}, {"text": "Green ai.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM, 63(12):\n54–63, November 2020.", "metadata": {}}, {"text": "ISSN 0001-0782.", "metadata": {}}, {"text": "doi: 10.1145/3381831.", "metadata": {}}, {"text": "URL https://doi.org/\n10.1145/3381831.", "metadata": {}}, {"text": "Arman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek,\nJonathan Koomey, Eric Masanet, Dale Sartor, et al.", "metadata": {}}, {"text": "2024 united states data center energy usage\nreport.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "Energy and policy considerations for\nmodern deep learning research.", "metadata": {}}, {"text": "In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp.", "metadata": {}}, {"text": "13693–13696, 2020.", "metadata": {}}, {"text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample.", "metadata": {}}, {"text": "Llama: Open and efficient foundation\nlanguage models, 2023a.", "metadata": {}}, {"text": "URL https://arxiv.org/abs/2302.13971.", "metadata": {}}, {"text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.", "metadata": {}}, {"text": "Llama 2: Open founda-\ntion and fine-tuned chat models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2307.09288, 2023b.", "metadata": {}}, {"text": "Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al.", "metadata": {}}, {"text": "Sustainable ai: Environmental impli-\ncations, challenges and opportunities.", "metadata": {}}, {"text": "Proceedings of Machine Learning and Systems, 4:795–813,\n2022.", "metadata": {}}, {"text": "Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,\nChristos Kozyrakis, Ion Stoica, Joseph E.", "metadata": {}}, {"text": "Gonzalez, Clark Barrett, and Ying Sheng.", "metadata": {}}, {"text": "Sglang:\nEfficient execution of structured language model programs, 2024.", "metadata": {}}, {"text": "URL https://arxiv.", "metadata": {}}, {"text": "org/abs/2312.07104.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nA A PPENDIX\nA.1 A DDITIONAL INFERENCE SIMULATION DETAILS\nWe benchmark models using the ShareGPT dataset, assuming an online inference chat setting. In\npractice, with much longer inference examples, OLMo models may have an “unfair” advantage in\nthat they were generally trained with context lengths shorter than the other models we benchmark.\nHowever, we do not believe that to be a significant factor in our results. In fact, we observe that\nLlama 3.1 8B is actually measured to be faster and less energy intensive than OLMo 7b models,\nlikely due to the use of grouped-query attention (GQA; Ainslie et al. (2023)) in Llama 8b, vs not in\nOLMo models.\nWe report additional inference simulation results on a larger set of models in Table 4,\nA.2 L IMITATIONS\nOur main limitations are discussed throughout the main body of this work – in particular, we make\nvarious assumptions about embodied impacts due to lack of real data, and our inference and deploy-\nment numbers were benchmarked in a controlled, limited setting, as we do not in reality serve our\nmodels in the same sense, and we do not have access to data about most other models’ real usage.\nWe present only a limited set of inference simulations following a number of simplistic assump-\ntions. Specifically, we simulate only settings where a deployed model is ingesting input tokens and\ngenerating output tokens following default parameters defined in SGLang (Zheng et al., 2024) – as\nopposed to, for instance, evaluating only the likelihood of a given text. Additionally, we note that\npractitioners frequently employ different inference-time optimizations such as quantization; perform\ngeneration with different decoding algorithms; and/or deploy to and run inference on edge devices,\nsometimes even without GPUs. We do not account for this variety of scenarios in our experiments.\nWe observe linear trends in training costs relative to parameter count across four orders of magnitude\nand eight model sizes. However, we do not necessarily expect that this trend would hold tightly\nin all training settings across all possible scales – for instance, decentralized training or training\nacross multiple datacenters might be expected to incur significantly greater communication overhead\nthroughout training. Though we have not trained these models ourselves, our hope is that our work\nwill encourage others working in a broad range of settings to provide their own holistic reports of\nenvironmental resource consumption.\n15", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nA A PPENDIX\nA.1 A DDITIONAL INFERENCE SIMULATION DETAILS\nWe benchmark models using the ShareGPT dataset, assuming an online inference chat setting.", "metadata": {}}, {"text": "In\npractice, with much longer inference examples, OLMo models may have an “unfair” advantage in\nthat they were generally trained with context lengths shorter than the other models we benchmark.", "metadata": {}}, {"text": "However, we do not believe that to be a significant factor in our results.", "metadata": {}}, {"text": "In fact, we observe that\nLlama 3.1 8B is actually measured to be faster and less energy intensive than OLMo 7b models,\nlikely due to the use of grouped-query attention (GQA;", "metadata": {}}, {"text": "Ainslie et al.", "metadata": {}}, {"text": "(2023)) in Llama 8b, vs not in\nOLMo models.", "metadata": {}}, {"text": "We report additional inference simulation results on a larger set of models in Table 4,\nA.2 L IMITATIONS\nOur main limitations are discussed throughout the main body of this work – in particular, we make\nvarious assumptions about embodied impacts due to lack of real data, and our inference and deploy-\nment numbers were benchmarked in a controlled, limited setting, as we do not in reality serve our\nmodels in the same sense, and we do not have access to data about most other models’ real usage.", "metadata": {}}, {"text": "We present only a limited set of inference simulations following a number of simplistic assump-\ntions.", "metadata": {}}, {"text": "Specifically, we simulate only settings where a deployed model is ingesting input tokens and\ngenerating output tokens following default parameters defined in SGLang (Zheng et al., 2024) – as\nopposed to, for instance, evaluating only the likelihood of a given text.", "metadata": {}}, {"text": "Additionally, we note that\npractitioners frequently employ different inference-time optimizations such as quantization;", "metadata": {}}, {"text": "perform\ngeneration with different decoding algorithms;", "metadata": {}}, {"text": "and/or deploy to and run inference on edge devices,\nsometimes even without GPUs.", "metadata": {}}, {"text": "We do not account for this variety of scenarios in our experiments.", "metadata": {}}, {"text": "We observe linear trends in training costs relative to parameter count across four orders of magnitude\nand eight model sizes.", "metadata": {}}, {"text": "However, we do not necessarily expect that this trend would hold tightly\nin all training settings across all possible scales – for instance, decentralized training or training\nacross multiple datacenters might be expected to incur significantly greater communication overhead\nthroughout training.", "metadata": {}}, {"text": "Though we have not trained these models ourselves, our hope is that our work\nwill encourage others working in a broad range of settings to provide their own holistic reports of\nenvironmental resource consumption.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "Published as a conference paper at ICLR 2025\nTable 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang\nbenchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served\non machines from the same cluster that our models were trained on, so we use the same WUE and\nPUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO 2e /\nkWh. The measurements reported in this table account for the GPU processes associated with active\ninference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be\nconsidered as lower bounds on usage in similar settings. We do not report “break-even” points for\nQwen models since the training costs are not public.\nRequest\nfreq.\n(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.\n(L)\nSeconds\nper 100 req.\n# Inf. for\nCO2 equiv.\nw/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.\n8 0.036 12.0 0.054 12.64 21.5 bil.\n1 0.16 53.1 0.238 100.58 4.83 bil.\nLlama 2 7B ∞ 0.019 6.3 0.028 3.58 11.9 bil.\n8 0.054 17.9 0.08 12.83 4.18 bil.\n1 0.349 115.9 0.52 100.62 647 mil.\nLlama 3 8B ∞ 0.01 3.3 0.015 1.93 282 bil.\n8 0.052 17.3 0.077 12.78 54.2 bil.\n1 0.337 111.9 0.502 100.63 8.37 bil.\nLlama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.\n8 0.051 16.9 0.076 12.79 59.5 bil.\n1 0.333 110.6 0.496 100.64 9.12 bil.\nLlama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.\n8 0.06 19.9 0.089 13.09 7.52 bil.\n1 0.401 133.1 0.597 100.73 1.13 bil.\nQwen 2.5 1.5B ∞ 0.003 1.0 0.004 0.86 –\n8 0.033 11.0 0.049 12.65 –\n1 0.163 54.1 0.243 100.57 –\nQwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 –\n8 0.053 17.6 0.079 12.77 –\n1 0.308 102.3 0.459 100.58 –\nQwen 2.5 14B ∞ 0.018 6.0 0.027 3.45 –\n8 0.058 19.3 0.086 13.02 –\n1 0.387 128.5 0.577 100.64 –\nQwen 1.5 MoE ∞ 0.01 3.3 0.015 2.64 –\n(2.7BA, 14BT) 8 0.043 14.3 0.064 13.11 –\n1 0.165 54.8 0.246 100.68 –\nOLMo 1 1B ∞ 0.004 1.3 0.006 0.99 18.2 bil.\n8 0.038 12.6 0.057 12.63 1.91 bil.\n1 0.165 54.8 0.246 100.58 441 mil.\nOLMo 0724 7B ∞ 0.017 5.6 0.025 3.33 29.8 bil.\n8 0.052 17.3 0.077 12.77 9.73 bil.\n1 0.339 112.5 0.505 100.59 1.49 bil.\nOLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.\n8 0.049 16.3 0.073 12.88 7.68 bil.\n1 0.358 118.9 0.533 100.54 1.05 bil.\nOLMo 2 13B ∞ 0.033 11.0 0.049 6.6 22.1 bil.\n8 0.057 18.9 0.085 13.05 12.8 bil.\n1 0.386 128.2 0.575 100.57 1.89 bil.\nOLMoE 0924 ∞ 0.006 2.0 0.009 1.7 21.7 bil.\n(1BA, 7BT) 8 0.037 12.3 0.055 12.82 3.51 bil.\n1 0.151 50.1 0.225 100.6 861 mil.\n16", "sentences": [{"text": "Published as a conference paper at ICLR 2025\nTable 4: Full version of Table 3 in §4.2.", "metadata": {}}, {"text": "Measurements and estimates of resource costs from SGLang\nbenchmarking on 2400 prompts from ShareGPT at varying request rates.", "metadata": {}}, {"text": "The models were served\non machines from the same cluster that our models were trained on, so we use the same WUE and\nPUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO 2e /\nkWh.", "metadata": {}}, {"text": "The measurements reported in this table account for the GPU processes associated with active\ninference, but not CPU or RAM associated with e.g.", "metadata": {}}, {"text": "server overhead.", "metadata": {}}, {"text": "Thus, these numbers can be\nconsidered as lower bounds on usage in similar settings.", "metadata": {}}, {"text": "We do not report “break-even” points for\nQwen models since the training costs are not public.", "metadata": {}}, {"text": "Request\nfreq.", "metadata": {}}, {"text": "(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.", "metadata": {}}, {"text": "(L)\nSeconds\nper 100 req.", "metadata": {}}, {"text": "# Inf.", "metadata": {}}, {"text": "for\nCO2 equiv.", "metadata": {}}, {"text": "w/ training\nLlama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.", "metadata": {}}, {"text": "8 0.036 12.0 0.054 12.64 21.5 bil.", "metadata": {}}, {"text": "1 0.16 53.1 0.238 100.58 4.83 bil.", "metadata": {}}, {"text": "Llama 2 7B ∞ 0.019 6.3 0.028 3.58 11.9 bil.", "metadata": {}}, {"text": "8 0.054 17.9 0.08 12.83 4.18 bil.", "metadata": {}}, {"text": "1 0.349 115.9 0.52 100.62 647 mil.", "metadata": {}}, {"text": "Llama 3 8B ∞ 0.01 3.3 0.015 1.93 282 bil.", "metadata": {}}, {"text": "8 0.052 17.3 0.077 12.78 54.2 bil.", "metadata": {}}, {"text": "1 0.337 111.9 0.502 100.63 8.37 bil.", "metadata": {}}, {"text": "Llama 3.1 8B ∞ 0.011 3.7 0.016 2.13 276 bil.", "metadata": {}}, {"text": "8 0.051 16.9 0.076 12.79 59.5 bil.", "metadata": {}}, {"text": "1 0.333 110.6 0.496 100.64 9.12 bil.", "metadata": {}}, {"text": "Llama 2 13B ∞ 0.034 11.3 0.051 6.53 13.3 bil.", "metadata": {}}, {"text": "8 0.06 19.9 0.089 13.09 7.52 bil.", "metadata": {}}, {"text": "1 0.401 133.1 0.597 100.73 1.13 bil.", "metadata": {}}, {"text": "Qwen 2.5 1.5B ∞ 0.003 1.0 0.004 0.86 –\n8 0.033 11.0 0.049 12.65 –\n1 0.163 54.1 0.243 100.57 –\nQwen 2.5 7B ∞ 0.009 3.0 0.013 1.79 –\n8 0.053 17.6 0.079 12.77 –\n1 0.308 102.3 0.459 100.58 –\nQwen 2.5 14B ∞ 0.018 6.0 0.027 3.45 –\n8 0.058 19.3 0.086 13.02 –\n1 0.387 128.5 0.577 100.64 –\nQwen 1.5 MoE ∞ 0.01 3.3 0.015 2.64 –\n(2.7BA, 14BT) 8 0.043 14.3 0.064 13.11 –\n1 0.165 54.8 0.246 100.68 –\nOLMo 1 1B ∞ 0.004 1.3 0.006 0.99 18.2 bil.", "metadata": {}}, {"text": "8 0.038 12.6 0.057 12.63 1.91 bil.", "metadata": {}}, {"text": "1 0.165 54.8 0.246 100.58 441 mil.", "metadata": {}}, {"text": "OLMo 0724 7B ∞ 0.017 5.6 0.025 3.33 29.8 bil.", "metadata": {}}, {"text": "8 0.052 17.3 0.077 12.77 9.73 bil.", "metadata": {}}, {"text": "1 0.339 112.5 0.505 100.59 1.49 bil.", "metadata": {}}, {"text": "OLMo 2 7B ∞ 0.018 6.0 0.027 3.68 20.9 bil.", "metadata": {}}, {"text": "8 0.049 16.3 0.073 12.88 7.68 bil.", "metadata": {}}, {"text": "1 0.358 118.9 0.533 100.54 1.05 bil.", "metadata": {}}, {"text": "OLMo 2 13B ∞ 0.033 11.0 0.049 6.6 22.1 bil.", "metadata": {}}, {"text": "8 0.057 18.9 0.085 13.05 12.8 bil.", "metadata": {}}, {"text": "1 0.386 128.2 0.575 100.57 1.89 bil.", "metadata": {}}, {"text": "OLMoE 0924 ∞ 0.006 2.0 0.009 1.7 21.7 bil.", "metadata": {}}, {"text": "(1BA, 7BT) 8 0.037 12.3 0.055 12.82 3.51 bil.", "metadata": {}}, {"text": "1 0.151 50.1 0.225 100.6 861 mil.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}]}