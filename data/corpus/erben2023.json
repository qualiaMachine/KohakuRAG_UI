{"document_id": "erben2023", "title": "How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study", "text": "How Can We Train Deep Learning Models Across\nClouds and Continents? An Experimental Study\nAlexander Erben\nTechnical University of Munich\nalex.erben@tum.de\nRuben Mayer\nUniversity of Bayreuth\nruben.mayer@uni-bayreuth.de\nHans-Arno Jacobsen\nUniversity of Toronto\njacobsen@eecg.toronto.edu\nABSTRACT\nThis paper aims to answer the question: Can deep learning models\nbe cost-efficiently trained on a global market of spot VMs spanning\ndifferent data centers and cloud providers? To provide guidance, we\nextensively evaluate the cost and throughput implications of train-\ning in different zones, continents, and clouds for representative CV,\nNLP and ASR models. To expand the current training options further,\nwe compare the scalability potential for hybrid-cloud scenarios by\nadding cloud resources to on-premise hardware to improve training\nthroughput. Finally, we show how leveraging spot instance pricing\nenables a new cost-efficient way to train models with multiple cheap\nVMs, trumping both more centralized and powerful hardware and\neven on-demand cloud offerings at competitive prices.\nPVLDB Reference Format:\nAlexander Erben,\nRuben Mayer, and Hans-Arno Jacobsen. . PVLDB, 17(6): 1214 - 1226, 2024.\ndoi:10.14778/3648160.3648165\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/cirquit/hivemind-multi-cloud.\n1 INTRODUCTION\nDeciding whether to invest in on-premise hardware or move to the\ncloud for deep learning (DL) is not easy. Wanting to scale existing\ninfrastructure means paying upfront, as combining cloud and on-\npremise is not an option with popular DL frameworks due to needing\na dedicated high-bandwidth interconnect. To enabled model- and\ndata-parallelism, current state-of-the-art accelerators have band-\nwidths of 900 GB/s for intra-node [19] and 25 Gb/s for inter-node\nsetups [1, 26]. Due to the initial investment of the cloud providers in\nthe accelerators, they naturally want to reap profit by maximizing\nresource utilization. Therefore, it is common to have \"spot\" pric-\ning, which offers the VMs at a strongly reduced rate, typically at\na 40-90% discount (Section 1), but with the drawback that the VM\ncan be terminated at any time if another customer is willing to pay\nthe on-demand price [33]. Unfortunately, popular DL frameworks\nhave not been developed with failure semantics in mind and cannot\nadequately deal with peers that fail [12]. While services like Amazon\nSagemaker [14] and projects like Skypilot [43] offer automatic job\nmigration in case of VM termination, they are limited to single-node\ntraining due to the bandwidth requirements between accelerators.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 17, No. 6 ISSN 2150-8097.\ndoi:10.14778/3648160.3648165\nTable 1: Average us-west cloud pricing in April ’23.\nType\nCloud GC A WS Azure\nT4 Spot 0.180 $/h 0.395 $/h 0.134 $/h\nT4 On-Demand 0.572 $/h 0.802 $/h 0.489 $/h\nTraffic (inter-zone) 0.01 $/GB 0.01 $/GB 0.00 $/GB\nTraffic (inter-region) US 0.01 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) EU 0.02 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) ASIA 0.05 $/GB 0.01 $/GB 0.08 $/GB\nTraffic (inter-region) OCE 0.08 $/GB 0.01 $/GB 0.08 $/GB\nTraffic ANY-OCE 0.15 $/GB 0.02 $/GB 0.08 $/GB\nTraffic (between continents)0.08 $/GB 0.02 $/GB 0.02 $/GB\n2\n 4\n 6\n 8\n 10\nCost in $ per 1M Samples\n0\n500Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nDDP 4xT4 DDP 4xT4\nInstance Type\nSpot\nOn-Demand\nFigure 1: Cost to throughput tradeoff for ConvNextLarge at dif-\nferent instance types. Our training setups (circled) are cheaper\n(8xT4) and faster (8xA10) than centralized offerings (DGX-2).\nBut what if we could use spot pricing for long-running, distributed\njobs and reduce bandwidth requirements to leverage multiple low-\ncost GPUs? This could be possible through a framework for collabo-\nrative DL training, Hivemind [39], which inherently deals with peers\nthat can stop running at any time. While there is research on how\nHivemind can be used for training on spot VMs [17, 37, 38], it does not\ncompare the cost-throughput tradeoff for different cloud offerings or\nperform ablation studies on geographic distribution or model sizes.\nTo motivate this new possibility, we trained the ConvNextLarge\nmodel [29] on the Imagenet1K dataset [15] on different Google Cloud\nhardware (T4’s and DGX-2), and on the very competitively priced\nA10 from LambdaLabs (see Section 6 for the full experimental de-\nscription). Figure 1 shows the training throughput and the costs per\n1 million processed samples for each setup. The single node (1xT4,\n1xA10, DGX-2) experiments show the current state-of-the-art cost-\nthroughput ratio for training on GC and LambdaLabs. The DGX-2\nnode is the fastest, with a throughput of 413 SPS, but it also costs\n$6.30/h ($4.24/1M samples), shown by the horizontal and vertical\nlines. The single-accelerator experiments (1xT4, 1xA10) have a better\ncost-throughput ratio ($0.62/1M samples and $0.9/1M samples), but\nhave a much lower throughput of 80 and 185 SPS, respectively. How-\never, when using our approach of distributing the training between\nmultiple GPUs with Hivemind (circled), we make training possible\nthat is both faster (8xA10, 621 SPS, $2.15/1M samples) and cheaper\narXiv:2306.03163v4  [cs.LG]  2 Jun 2024\n\n(8xT4, 262 SPS, $1.77/1M samples) than using the DGX-2. Every\ncloud provider deals differently with how they price spot instances\nand network traffic (cf. Section 1) and has varying interruption rates\nfor different accelerators [23]. Being able to choose the best option\nwas not possible before, and having the option to combine older,\nmore available GPUs is a net benefit for both consumers and cloud\nproviders alike.\nWe aim to develop guidelines and help practitioners assess under\nwhich conditions they can cost-efficiently speed up their training\ntasks with spot instances. To be able to do this, they need a precise\ndefinition of the model size at which geo-distributed spot training\nbecomes viable, what hardware can be used for it, and what the\nminimum bandwidth and latency are. We close this research gap by\nperforming a comprehensive analysis of multiple DL tasks from CV\nand NLP, breaking down how time is spent in each epoch, and com-\nparing them to non-distributed runs to quantify the advantages and\ndisadvantages of distributed spot training. We determine which mod-\nels scale with additional spot instances and which cannot be scaled\nwithout running into a communication bottleneck or resource ineffi-\nciencies. To quantify total training cost, we assess cost-effectiveness\nand evaluate a hybrid or multi-cloud approach with popular cloud\nproviders through training on up to four continents. For comparison\nof the models’ scalability and to show which of them can be trained\nin a distributed fashion, we introduce thegranularity metric, the ratio\nof calculation to communication time, and show how it can be used\nfor predicting performance with different hardware setups. Finally,\nwe summarize our lessons on how to design geo-distributed spot\ntraining and what to watch out for when evaluating the feasibility\nof such a training regime. Our contributions are:\n(1) We analyze the impact of multi-cloud training with\nspot and on-demand instances from Google Cloud\n(GC), Microsoft Azure, Amazon Web Services (A WS),\nand LambdaLabs on cost-efficiency.While we find perfor-\nmance penalties due to remote versus on-premise compute\nresources, the throughput still scales with increased comput-\ning power. By leveraging multiple spot instances with one\nT4 GPU each, we can be more cost-efficient than a DGX-2\nnode or the very competitively priced A10 offerings from\nLambdaLabs.\n(2) We investigate the suitability of geo-distributed train-\ning for various CV and NLP models and hardware con-\nfigurations on up to four continents.Not surprisingly,\nthe more parallelizable and the larger the task, the better the\nperformance. Moreover, we verify the scalability claims of\nthe related work and define additional constraints, such as\nthe minimum granularity for effective training. This enables,\nfor the first time, distributed training of smaller million-\nparameter models (12M-560M) over <1 Gb/s bandwidth and\n>150ms latency networks.\n(3) We evaluate two different hybrid-cloud experimental\nsetups with consumer- and server-grade on-premise\nhardware and try to improve the throughput with a band-\nwidth of, at worst, 50 Mb/s to the cloud resources. While we\nshow that it is possible to improve throughput even at these\nconstraints, local cloud offerings are better suited for models\nthat show limited suitability for distributed training.\n(4) We summarize our findings of training in a geo-distributed,\nmulti-cloud environment. We propose the granularity\nmetric to compare model suitability for distributed\nspot training and estimate training performance with ad-\nditional spot VMs. This provides guidance on the trade-off\nbetween performance and cost when using geo-distributed\nspot instances. To apply our findings, we perform a case-\nstudy on a state-of-the-art model from the ASR domain and\nachieve speedups on low-end hardware.\n2 DEEP LEARNING ON SPOT INSTANCES\nIn this section, we describe how the Hivemind framework works\nand how it can enable distributed spot training.\n2.1 Hivemind\nHivemind [39] is a PyTorch-based [32] framework developed initially\nto enable collaborative DL training where participants could donate\ntheir heterogeneous hardware to train a single model together in\na data-parallel fashion. Its main difference to other state-of-the-art\ndistributed training frameworks, such as PyTorch DDP [26] and\nDeepSpeed [35], is that it runs in a decentralized fashion and can\nhandle peers that drop out at any stage of the training. It does so with\ntwo features: a distributed hash table [31] (DHT) which spans over\nall participating peers for metadata storage, such as training progress\nand peer health, and a gradient averaging algorithm that is designed\nto reduce the impact of lost gradients. A key difference to other dis-\ntributed training frameworks is the definition of ahivemind epoch,\nwhich is the number of samples that must be aggregated before an\naveraging step is performed. This sample count is called thetarget\nbatch size (TBS), which corresponds to the minibatch size in standard\nDL training. The DHT is used for coordination, and shortly before\nthe TBS is predicted to be reached, the peers start to form the initial\ngroups for averaging. The time allocated for group forming is called\nmatchmaking time and typically runs asynchronously to the training\n(cf. Section 3). The individual peer gradients are accumulated locally\nand sent to the other peers via an adaptive all-reduce algorithm\n(MoshpitSGD [38]). The next hivemind epoch starts after each peer\napplies the accumulated gradients to the local model. The advantage\nof Hivemind for geo-distributed training comes from cumulating\ndifferent techniques, such as Delayed Parameter Updates [36], big-\nbatch training [44] and aggressive communication quantization [16].\nAll of these combined reduce time and frequency of the communica-\ntion rounds, which in turn makes training on heterogeneous devices\nand low-bandwidth networks possible.\n2.2 Distributed Spot Training\nIn this paper, we focus only on models that fit into the memory of\na single GPU, as we are interested in utilizing data parallelism on\ncheaper and more readily available hardware. However, our insights\nare applicable to larger models with techniques such as ZeRO of-\nfloading [36], more aggressive quantization [41] and even model\nparallelism [37]. The current options for data parallelism are either\nusing multiple GPUs on the same node (e.g., a DGX system with\neight GPUs) or having multiple nodes with a GPU each in the same\nhigh-bandwidth network (>25 Gb/s) to minimize communication\ntime. The latter does not work on cheap but interruptable instances,\n\nwhile the former has some use in the form of Amazon Sagemaker but\nis limited to a single node and is typically very pricey (spot pricing for\nDGX-2 is $6.30/h versus 8xT4 at $0.72/h on GC). However, using Hive-\nmind, a new training scenario becomes feasible: Distributed training\nin a decentralized fashion on interruptable VMs with bandwidths of\n<1 Gb/s. Since spot instance prices change hourly depending on the\ntime of day and zone availability [23], and can vary widely between\ncloud providers (cf. Section 1), training between continents and in\nmultiple clouds could potentially be more cost-effective than using\na single, more computationally powerful node at spot prices.\nWith the newly added training setups from Figure 1 (circled), it\nwas not previously possible to choose the best option, and having\nthe option to combine older, more available GPUs is a net benefit for\nboth consumers as well as cloud providers. Our paper shows that\nit is possible to train on multiple clouds across multiple continents\nand provides guidelines on how to accomplish this cost-efficiently.\n3 MODEL SUITABILITY\nSelecting suitable models with a big enough parallel workload is\nessential to ensure successful distributed spot training. To cover\na wide range of established models, we drew from MLCommons’\ncomprehensive DL training benchmark [30]. We used models from\nthe CV and NLP domains and gradually increased their size and\nTBS to increase the parallel compute amount. As discussed in Sec-\ntion 2, the TBS may be exclusively responsible for the success of\ndistributed training and was chosen to cover both medium and large\nbatches (8K, 16K and 32K). These minibatch sizes start to become\nmore common due to the LAMB optimizer [44], which works well\nenough for both smaller (512) and huge batches (64K) and should be\nrepresentative of state-of-the-art workloads. For a represenatative\nexperimental study with a minibatch size of 256 on the automatic\nspeech recognition model (Whisper [34]), please refer to Section 11.\nAll experiments were run with FP16 precision, as the target T4 GPUs\nhave a considerable improvement in FLOPs compared to FP32 (8:1).\nFor CV, we take five models from the extended ResNet family,\nstarting with the smallest one, ResNet18 [ 21] (RN18), ResNet50\n(RN50), ResNet152 (RN152), WideResNet101_2 [46] (WRN101) and\nConvNextLarge [29] (CONV), which is almost 20 times larger than\nRN18. The paramter count is 11.7M, 25.6M, 60.2M, 126.9M, and\n197.8M, respectively. These models were popularized due to their\nability to help with the vanishing gradient problem by using resid-\nual connections between layers. Currently, they are not only used\nfor classification, but can serve as an embedding of images by re-\nmoving the classification head [18, 40]. For the dataset, we use Im-\nagenet1K [15] and train the classification task, which tries to assign\none of 1000 classes to each image.\nFor NLP, we selected three models from the BERT family:\nRoBERTaBase [28] (RBase), -Large (RLrg), and -XLM [13] (RXLM).\nThe parameter count is 124.7M, 355.4M, and 560.1M, respectively.\nWe used the same configuration as the original models and trained\nthem on masked language modeling, a common pre-training task.\nRoBERTa models were a replication study of BERT but with a focus\non better hyperparameter tuning, leading to state-of-the-art results\nand proposed using much higher minibatch sizes than previously\ncommon. The text dataset is March ’22 Wikipedia [20].\nWhen we run our experiments in a multi-cloud environment on\nspot instances, we cannot plug in proprietary cloud storage or wait\nfor the dataset to download, as the instances can be terminated any-\ntime. To simulate a real-world deployment with a non-public dataset,\nwe chose an independent S3 storage provider, Backblaze (B2) [4].\nBackblaze has replicated data centers that can better serve requests\nfrom anywhere worldwide, guaranteeing a reasonable ingress rate\nfrom every continent. Additionally, the cost is very manageable at\n$0.01/GB rate for egress and $0.005/GB/month for storage. A de-\ntailed analysis of the costs incurred for the experiments can be found\nin Section 5. We access the datasets on-demand via shards in thetar\nformat with the WebDataset library [10]. We chose WebDataset due\nto its features like automatic local caching, streaming decompression,\nstreaming preprocessing, and having an easy to work with archive\nformat that allows representing the data in its original format. Finally,\nfor the Hivemind parameterization, we enabled delayed parameter\naveraging (DPU) [36] to enable simultaneous gradient communi-\ncation and computation at the expense of a round of staleness. We\nselected FP16 compression for peer-to-peer communication.\nExperimental design. First, we must verify that our models are\nsuitable for cloud training. For this purpose, we evaluate them on\nthe powerful Ampere GPUs first - if they scale there without facing\na communication bottleneck, they should also scale on the slower\nT4, which is common at GC, AWS, and Azure. We use the LambdaL-\nabs [8] for these experiments, which gives us on-demand A10 GPUs\nfor just $0.60/hour, but currently offer their services only in the US\nWest region. All experiments are performed on the 515.65.01 driver,\nCUDA 11.6, and PyTorch 1.13.1. We profiled a network bandwidth\nof 3.3 Gb/s and a latency of 0.3 ms between the Lambda VMs.\nTo establish a fair baseline, we train all models from?? on a single\nGPU that achieves large minibatch sizes through gradient accumu-\nlation. Processes logs system metrics every second and evaluates\nthe training performance whenever a batch is processed. Finally,\nall multi-GPU experiments are monitored with a training monitor\nthat scrapes the DHT every second to log the peer state and training\nprogress synchronously.\n(1) Hivemind penalty. Using Hivemind as middleware to share\ngradients and keep a fully decentralized architecture running harms\nperformance compared to single-node training. We can compare\nthe effects of Hivemind training by looking at three metrics:base-\nline, the single GPU throughput, hivemind local, normalized GPU\nthroughput without the averaging step, and hivemind global, the\nactual normalized GPU throughput. When comparing the baseline\nand local speed in Figure 2 for a setup with two GPUs, running Hive-\nmind reaches at best 78% (RN152) and at worst 48% (CONV) of the\nbaseline performance. Unsurprisingly, the larger the model size, the\nworse the penalty gets due to the increased size of the accumulated\ngradients (GAC) over each step. However, the baseline also applies\ngradient accumulation to reach the target minibatch size without\nthe performance drop. After isolating the respective function calls,\nthere seems to be a slight inefficiency in how GAC is implemented in\nHivemind versus the native PyTorch call. We are working with the\nmaintainers to fix this issue [7]. On the other hand, the disadvantage\nof synchronization is minimal under the perfect conditions of a good\ninterconnect. The global speed in Figures 2a and 2b only degrades\nat best to 97% (CONV) to at worst to 87% (RBase) compared to the\nlocal throughput, meaning that the communication under these con-\nditions only accounts for a fraction of the total training time. This\ndegradation is inversely correlated to the model size due to larger\n\nRN18 RN50 RN152 WRN101 CONV\n0\n500\n1000\n1500\n2000Samples per Second\n1462\n778\n297 423\n185\n1024\n529\n231 211\n89\n958\n499\n217 201 86\nThroughput Type\nbaseline\nhivemind local\nhivemind global\n(a) CV\nRBase RLrg RXLM\n0\n250\n500\n750\n1000\n1250\n1500Samples per Second\n1310\n661\n463\n804\n359\n247\n700\n313\n215\nThroughput Type\nbaseline\nhivemind local\nhivemind global (b) NLP\nFigure 2: Hivemind penalty on normalized throughputs.\n8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500\n2000Samples per Second\n1466 1462 1462\n768 771 778\n292 298 297\n431 436 423\n188 186 185\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500\n2000Samples per Second\n1204\n1954 1916\n864 964 998\n383 418 435\n363 390 403\n164 169 173\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV (b) CV 2xA10\n8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500Samples per Second\n1316 1307 1310\n659 660 661\n462 461 463\nModel\nRBase\nRLrg\nRXLM\n(c) NLP 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500Samples per Second\n680\n1257\n1401\n451\n551 626\n305 383 430\nModel\nRBase\nRLrg\nRXLM (d) NLP 2xA10\nFigure 3: Throughput comparison between single GPU base-\nlines and the Hivemind runs with two GPUs.\nmodels training quadratically longer per parameter, but the commu-\nnication only increases linearly [37]. While an implementation issue\ncurrently affects performance, and the worst total performance drop\nis at 47% (CONV baseline vs. global), scaling is still possible with a\nratio of roughly 2:1 of GPUs to throughput. We further refine this\nratio in the following section by comparing which models are most\nsuitable to be trained in a distributed environment.\n(2) Less suitable models for distributed spot training. While\ntraining billion-parameter NLP models scale well due to the \"square-\ncube\" law, the minimum model size is not yet fully defined [37]. The\nreason is that many factors play a role in whether a model is suited\nfor geo-distributed training. On the one hand, a small model results\nin small gradients exchanged between peers, so the averaging step is\nfast. On the other hand, a small model will also reach the TBS faster\nthan larger models, which may lead to a low speedup if the calculation\ntime is disproportionally lower than the communication time.\nWe found the granularity metric [ 22], typically used in high-\nperformance computing, practical to attach a comparable value to\neach setup to quantify the ratio of the calculation to communication\ntime. The higher the granularity, the more parallelizable the task,\nas more calculation can be distributed between peers, ensuring a\ngood speedup. It is important to note that this metric depends on\nthe model and the hardware being used. The communication time is\naffected by the parameter count, and the calculation time is affected\nby the layer type of the parameters (including feedforward, convolu-\ntion, and transformer). Therefore, the calculation time can decrease\nwith improved hardware, which we evaluate in Section 6. Another\nparameter that affects the calculation time is the TBS that all peers\nwork to accumulate. There is a practical limit to the TBS where a\nmodel is still trainable, which is currently at 64K with the LAMB\noptimizer [44]. This limits the possibility of improving the speedup\nof small models by increasing the batch size, meaning that at some\npoint, the speed will be limited by the communication time. It is\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRN18 RN50\nRN152 WRN101 CONV\n1.3 7.613.23.4 6.8\n11.53.5\n7.0\n14.0\n2.8\n5.6\n11.5\n5.3\n10.7\n21.6Calculation\nCommunication\n(a) CV\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg RXLM\n0.7 2.3 4.4 1.1 2.2\n4.4\n1.0\n2.1\n4.2\nCalculation\nCommunication (b) NLP\nFigure 4: TBS vs. total training time on 2xA10s. Granularity is\nshown above each bar. Dotted lines separate different models.\nimportant to remember that just increasing the TBS to create more\ncalculation time can have a grave effect on training performance if\nthe optimizer is not adequately selected and configured.\nOur experimental results in Figure 3 show the practical implica-\ntions of this observation. For the 2xGPU experiments in Figures 3b\nand 3d, we can see the effect of a TBS increase which improves the\ntotal throughput. Doubling the TBS equals cutting down the per-\nsample communication cost by two, which leads to the slight increase\nin performance visible in both CV and NLP experiments. However,\nthe smallest models, RN18 and RBase, fluctuate significantly at a TBS\nof 8K due to a minimum matchmaking time of 5 seconds. Whenever\nall peers accumulate the TBS in less than 5 seconds, the asynchronous\nthread that matches the peers in groups to perform the all-reduce\nmay still need to finish. This results in an unstable averaging time,\nwhich limits the scalability of small models with a small TBS.\nTo illustrate how the TBS and model size affect the individual\ntimings, we visualize the total training time split up into the calcu-\nlation and communication time in Figure 4. CV models are generally\ncomputationally more expensive and have a higher granularity than\nNLP models, which have slightly longer averaging rounds due to the\nmuch larger model sizes. When comparing the models at the same\nTBS (e.g., 32K), there is an inconclusive relation between runtime and\nparameter count. Some models increase their runtime with parame-\nter count w.r.t. smaller models (RN50 to RN152, RBase to RLrg), while\nothers decrease their runtime (RN152 to WRN101, RLrg to RXLM).\nThis performance is due to not all layer parameters contributing\nsimilarly to computational complexity. Depending on the specific\narchitecture, even models with more parameters can be faster to\ntrain due to a more efficient architecture, such as the WRN101 [46].\nThe communication time between different TBS sizes stays the\nsame, barring the two matchmaking time exceptions (RN18, RBase),\nas the gradients are accumulated before being sent. For all other\nmodels, doubling the TBS leads to exactly double the amount of\nwork and doubles the granularity. With a TBS of 32K, all models\nhave a granularity of at least 4.2 (RXLM) and at most 21.6 (CONV),\nwhich show strong scaling potential. Therefore, we decided to use\na TBS of 32K for all following experiments to ensure that the setup\nscales before introducing bandwidth and computational limitations.\nSummarizing, whether a model is scalable without network band-\nwidth limitations depends on the minimum time to reach the TBS and\non the granularity. Tuning the TBS is possible to a certain extent but\ndepends on the specific training task and optimizer configuration.\n(3) Per-GPU speedup decreases with low granularity.To eval-\nuate the scalability with additional hardware, we profile all models\non 2,3,4, and 8 GPUs with a TBS of 32K. Figure 5 shows the through-\nput for all models in the different hardware scenarios. Generally, all\nmodels scale well regardless of size, with the best speedup of 4.37x\n(RN152) and the lowest at 2.29x (RXLM) with 8 GPUs. There is a\n\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n0\n1000\n2000\n3000\n4000Samples per Second\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n1000\n2000\n3000Samples per Second\nModel\nRBase\nRLrg\nRXLM (b) NLP\nFigure 5: Throughput comparison from 1 to 8 A10 GPUs.\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\n13.22.84.61.0\n11.56.84.41.5\n14.0\n7.65.0\n1.8\n11.5\n6.34.21.5\n21.6\n11.9\n8.4\n3.2\nRN18 RN50\nRN152 WRN101\nCONV\nCalculation\nCommunication\n(a) CV\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg\nRXLM\n4.4 2.7 1.8 0.9\n4.4\n2.6 1.8 0.7\n4.2\n2.4 1.7\n0.7\nCalculation\nCommunication (b) NLP\nFigure 6: Multi-GPU scalability at 32K TBS. Granularity is\nshown above each bar. Dotted lines separate different models.\nTable 2: Geo-distributed experiments on GC with T4 VMs.\nExp. NameResources Total\nA-{1,2,3,4,6,8}{1, 2, 3, 4, 6, 8}xUS 1,2,3,4,6,8\nB-{2,4,6,8}{1, 2, 3, 4}xUS + {1, 2, 3, 4}xEU 2,4,6,8\nC-{3,6}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA 3,6\nC-{4,8}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA + {1, 2}xAUS4,8\nvisible trend in the per-GPU contribution to the speedup (speedup\n#GPUs ).\nThe more GPUs we add, the lower the contribution, e.g., RN18 goes\nfrom 0.7 to 0.4 with two to eight GPUs, respectively. This decrease is\nlikely to continue due to a granularity of 1.0 at 8 GPUs (Figure 6a), as\ndoubling the GPUs would, at best, increase the throughput by 33% by\nhalving the calculation time. However, the more computationally ex-\npensive the models are, the slower the per-GPU contribution falls off\nand the larger the granularity is (RN152, CONV). This does not hold\ntrue for our NLP models (Figure 6b); while they have increasingly\nmore model parameters, the only difference between the two biggest\nmodels, RLrg and RXLM, is the vocabulary size increase of 50K to\n250K. Due to how embedding layers are lookups, the forward pass is\nnot affected by the increased embedding size, but the backward pass\nis. This results in a smaller increase of the calculation time while\ncommunication increases linearly with the number of parameters.\nAdditionally, we see the drop in throughput when comparing\nthe single GPU and dual GPU experiments for most larger models\n(Figure 5), which stems from observation(1) of the Hivemind penalty.\nWe also observe that with each subsequent doubling of GPUs,\nthe calculation time is halved, while the communication increases\nsub-linearly due to the more efficient group-based all-reduce of\nMoshpitSGD [38]. For example, the averaging step for the RXLM on\n2xA10 takes 5 seconds per GPU (10s total), while the 8xA10 averaging\nstep takes 1.8 seconds per GPU (14.4s total).\nIn summary, all models show a speedup but have a decreasing\nper-GPU contribution due to smaller granularity with more GPUs.\nTherefore, the larger the model and TBS, the greater the scaling\npotential. High granularity is a good indicator of scalability, and\nsince the communication time only increases linearly with addi-\ntional peers (cf. Section 2.1), knowing the initial calculation time is\na good indicator of future throughput. Under the optimal conditions\nof good compute performance and an interconnect with relatively\nhigh bandwidth, scaling was not a problem. But what happens under\nless favorable conditions in geo-distributed settings?\n4 GEO-DISTRIBUTED PERFORMANCE\nAs spot prices for the same hardware differ depending on the re-\ngion, zone, and time of day [23], it might be a good idea to use VMs\nacross different data centers. However, is the connectivity between\nregions and continents good enough to enable distributed deep learn-\ning? To explore this question, we decided to conduct three types of\nexperiments (Table 2):\n(A) Intra-zone Can we scale if the VMs are co-located in the same\nzone (us-central-1)?\n(B) Transatlantic Can we scale when we combine VMs from two\nregions (US and EU), and what happens when the compute\nis unevenly distributed across regions?\n(C) Intercontinental Can we scale if we combine VMs from four\ncontinents (US, EU, ASIA, AUS)?\nExperimental design. Based on the insights from Section 3, we\ndecided to use the largest models (CONV, RXLM) for all further cloud\nexperiments in Sections 4 to 6, with the TBS of 32K as a baseline with\ngood scaling properties. We abbreviate them with their respective\ndomain names (CV, NLP). We used Google Cloud [5] for all exper-\niments in this section, as they were the first to give us access to\nall necessary zones. The default networking solution in GC is the\n\"Premium Tier\", which tries to use a Google-owned network instead\nof the public internet. We measured the throughput and latency\nbetween all zones via iperf and ping and report the average of 5\nconsecutive runs in Table 3. Unsurprisingly, the diagonal shows that\nthe local connectivity between zones runs at almost 7 Gb/s with a\nlatency of 0.7ms, probably due to the hypervisors being in the same\ndata center. While the up- and download were perfectly symmetrical\nin all setups, the throughput dropped to <210 Mb/s for all non-local\nconnections. The US-based data center is located in Iowa and is best\nconnected with at least 120 Mb/s to the remaining regions, namely\nBelgium in the EU (6,911km), Taiwan in ASIA (11,853km), and Syd-\nney in Australia (AUS, 14,555km), presumably due to the physical\ndistance. The lowest bandwidth and highest latency connections are\nbetween the EU region and ASIA and AUS, reaching around 80 Mb/s\nand 270ms. We decided to use the n1-standard-8 template with\neight cores, 30 GB memory, and a T4 GPU, as the smaller image with\n15 GB was insufficient to meet the memory requirements for gradient\napplication on the CPU with the biggest models. The experiment\nnaming in this section is prefixed with the type of location(A), (B)\nor (C) and the number of VMs, e.g., A-4 is the intra-zone experiment\nwith 4 VMs. The full experimental description is specified in Table 2.\nTable 3: Throughput and latency between GC zones.\n(a) Single stream TCP throughput in Gb/s.\nFrom To US EU ASIA AUS\nUS 6.900.21 0.13 0.12EU 0.216.81 0.08 0.07ASIA 0.130.08 6.79 0.16AUS 0.120.07 0.16 6.84\n(b) ICMP latency in ms.\nFrom To US EU ASIA AUS\nUS 0.66103.11157.09176.19EU 103.14 0.65253.10271.98ASIA 157.08253.09 0.72131.45AUS 175.98272.08131.42 0.64\n(A) Intra-zone scalability.Figure 7 shows the result of the intra-\nzone experiments, which we used as a baseline to compare geo-\ndistributed deployments to. As the scalability of the CV and NLP mod-\nels was already shown with much better hardware and slightly worse\nnetwork connectivity (cf. Section 3), the scalability with the T4 GPUs\nis not too surprising. We do not see an improvement in throughput\nfor two GPUs for either model due to the Hivemind penalty discussed\n\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV A-3\nCV A-4\nCV A-6\nCV A-8\nNLP A-2\nNLP A-3\nNLP A-4\nNLP A-6\nNLP A-8\n0\n200\n400\n600Time in Seconds\n39.73\n21.23\n14.48\n8.31\n5.39\n6.86\n3.89 2.73 1.66 1.15\nCalculation\nCommunication (b) Granularity\nFigure 7: (A) Intra-zone performance for CV and NLP.\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV B-2\nCV A-4\nCV B-4\nCV A-6\nCV B-6\nCV A-8\nCV B-8\nNLP A-2\nNLP B-2\nNLP A-4\nNLP B-4\nNLP A-6\nNLP B-6\nNLP A-8\nNLP B-8\n0\n200\n400\n600Time in Seconds\n39.714.4\n14.59.3\n8.3 6.6\n5.4 4.9\n6.9\n2.1\n2.7 1.4\n1.7 1.0 1.1 0.6\nCalculation\nCommunication (b) Granularity\nFigure 8: (B) Transatlantic performance for CV and NLP.\nin Section 3. However, starting with three GPUs, we see an increase\nin throughput with a maximum speedup of up to 3.2x CV and 2.75x\nfor NLP at eight GPUs. CV’s per-GPU speedup (speedup\n#GPUs ) is almost\nlinear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster\n(0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively. The\nreason for this is the NLP granularity of 1.15 with 8 GPUs indicating\nan almost equal part in communication and calculation (Figure 7b)\ndue to the much longer averaging round related to the model size\n(198M vs. 560M parameters). The peak network bandwidth utiliza-\ntion between peers was at most a symmetric 1.1 Gb/s while averaging\nand 33 Mb/s ingress while training due to data loading. This means\nthat the network bandwidth of 7 Gb/s was not a limiting factor.\n(B) Transatlantic scalability.We scale when computing hard-\nware is local. However, what happens when there is cheap capacity in\nanother region? In this case, we study the throughput of experiments\nwith resources in theus-west and eu-central regions (B-2,4,6,8).\nThe B-2 experiment has one VM in the US and one in the EU,\nachieving a virtually identical throughput of 68.4 (US-EU) versus 70.1\n(US) at CV (Figure 8a). Our maximum peak egress rate of 250 Mb/s\ndoes not affect the CV experiments, while the US experiments peaked\nat 1.1 Gb/s. The reduction in bandwidth penalizes NLP harder, where\nwe are 16% slower with 177.3 SPS (US-EU) compared to the intra-zone\nexperiment with 211.4 SPS (US). The resulting increased communica-\ntion can be easily seen in the granularity analysis in Figure 8b (NLP\nA-2,4,6,8 vs. B-2,4,6,8). As only communication time increases in the\nNLP (B) experiments compared to(A), a granularity of≫ 1 indicates\ngood scalability: Adding two more GPUs to the B-6 experiment with\na granularity of 1.03 results in a throughput increase of 15% (B-8)\nrelative to the baseline. Meanwhile, adding two more GPUs to the\nB-2 experiment with a granularity of 2.21 results in a throughput\nincrease of 77% (B-4) relative to the baseline.\nIn the B-4 experiment, we look at what happens when we increase\nthe number of VMs to four, with two in the US and two in the EU.\nNothing surprising happens with CV, as the workload continues to\nbe mostly computation, with a throughput of 135.8 (B-4), only 3%\nslower than the intra-zone experiment with 140.4 SPS (A-4). How-\never, at NLP, things get more interesting as we now have more overall\ncommunication with four peers, but they can average locally first\nand only later transmit across the Atlantic. However, compared to\ntheir A-counterparts, we do not see a difference in relative scalability\nwith either B-4, B-6, or B-8. This means that training across regions\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-3\nCV C-3\nCV A-4\nCV C-4\nCV A-6\nCV C-6\nCV A-8\nCV C-8\nNLP A-3\nNLP C-3\nNLP A-4\nNLP C-4\nNLP A-6\nNLP C-6\nNLP A-8\nNLP C-8\n0\n200\n400\n600Time in Seconds\n21.25.9\n14.5\n4.7\n8.3 4.1\n5.4 3.3 3.9\n0.8\n2.7\n0.7\n1.7\n0.6\n1.1\n0.4\nCalculation\nCommunication (b) Granularity\nFigure 9: (C) Intercontinental performance for CV and NLP.\n(B) is slower, but the contribution per GPU decreases at the same\nrate as in training within a zone(A). The per-GPU speedup with ad-\nditional hardware reduces at the same rate for either setup (between\n0.05 and 0.06). This results in two observations: First, communica-\ntion overhead scales linearly with the number of peers. Second, we\nonly have to pay the penalty for transatlantic training once. How-\never, we cannot expect a significant improvement in communication\nefficiency when we increase the amount of available local resources.\nSummarizing, with an transatlantic setup, CV achieves a virtually\nidentical maximum speedup of 3.2x with 8 GPUs compared to A-1\n(B-8 is 2% slower than A-8), while NLP is more affected by lower\nnetwork bandwidth and only achieves a speedup of 2.15x (B-8 is\n22% slower than A-8). The transatlantic training penalty is applied\nonce; however, it does not affect the relative scaling with additional\ncompute resources.\n(C) Intercontinental scalability.To take geo-distribution to the\nextreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS,\nto see how much worse bandwidth affects the training throughput\n(C-3,4,6,8 in Table 2).\nHow does the intercontinental penalty investigated in(B) affect\ndeployments with a single GPU on each continent? Comparing the\nA-3 and C-3 experiments with three local versus three fully remote\nGPUs, CV is only 5% slower, while NLP suffers a 34% drop in through-\nput (Figure 9a) and does not even reach the baseline single GPU\nperformance (A-1). The peak egress for each region was 318, 258,\nand 237 Mb/s for the US, EU, and ASIA, respectively. Since our band-\nwidth measurements were 210 and 130 Mb/s from the US to the EU\nand ASIA, respectively (Table 3), this suggests that the averaging\nwas done over the US node and not an N-to-N all-reduce (a detailed\nanalysis of how averaging affects bandwidths is discussed in Sec-\ntion 6). Thus, the limiting factor was the US-ASIA connection at\n130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend\ncontinues with the C-4 run, which adds AUS as a continent with one\nadditional VM. As we know from the transatlantic experiments(B)\nthat an additional continent has a detrimental effect on throughput,\nwhich, for the four continents experiment, C-4, results in a 9% slower\nthroughput for CV and 36% slower for NLP compared to the A-4 runs\n(Figure 7a). Again, the US VM is used as an averaging intermediary\nwith a peak egress of 365 Mb/s, while the other continents are be-\ntween 318 and 330 Mb/s. When comparing the two continents (B-4)\nversus four continents (C-4) experiments, one GPU on each continent\n(C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs\non two continents (B-4). This reinforces that local hardware should\nbe preferred whenever possible. However, we are always faster than\nthe baseline (A-1), starting from 4 GPUs in both the transatlantic and\nintercontinental settings. While these experiments were specifically\ndesigned to be a worst-case scenario, what about a more balanced\nGPU distribution with at least two GPUs in each region?\n\nD-1\n D-2\n D-3\nT4 Count\n200\n300Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV D-1\nCV D-2\nCV D-3\nNLP D-1\nNLP D-2\nNLP D-3\n0\n200\n400\n600Time in Seconds\n14.5 14.2 12.7\n2.7 2.5 2.0\nCalculation\nCommunication (b) Granularity\nFigure 10: Multi-cloud performance for CV and NLP.\nWhen comparing the C-6 experiment with two GPUs in three\ncontinents to the local A-6 experiments, the throughput slowdown\nis almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%)\nto A-4. Scaling further to two GPUs in four continents, C-8 is slightly\nslower at NLP (41%) compared to C-4 (36%) to their respective local\nruns (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).\nThe small granularity removes the additional gain of four more GPUs\nsince the task is no longer suitable for distributed training. However,\nas the CV task is still at a granularity of 3.33 on C-8, it reaches a\nspeedup of 3.02x, only 7% slower than the fully local A-8 experiment.\nThe peak egress of 678 Mb/s was also reached on one US VM, while the\nremaining VMs were between 450 and 550 Mb/s. These observations\nshow that adding another continent does not significantly reduce\nthroughput when training on three continents with at least two VMs.\nIn summary, while local compute is the best choice for maximum\nthroughput, for high granularity tasks like CV, even distributing VMs\nover four continents only slows down performance by 7%. However,\nintercontinental training leads to a significant penalty on a task with\nlower granularity, like NLP, resulting in a performance drop of 41%\n(C-8) compared to the fully local experiment (A-8). Finally, each addi-\ntional region introduces a constant penalty that is not amortized by\nadding local hardware, which should be considered when running\ngeo-distributed training setups.\n5 MULTI-CLOUD PERFORMANCE\nUsing multiple cloud providers makes sense if we want to use re-\nsources cost-effectively and have additional reliability. In our sce-\nnario, we are interested in what throughput per $ can be expected\nand if any barriers prevent multi-cloud training. However, one can\nalso consider the data center’s carbon footprint, which can change\ndepending on the season and time of day [6].\nTable 4: Average multi-cloud throughput and latency.\n(a) Single stream TCP throughput in Gb/s.\nFrom\nTo GC A WS Azure\nGC 6.35 1.52 0.45\nA WS 1.81 4.87\nAzure 0.47 7.63\n(b) ICMP Latency in ms.\nFrom\nTo GC A WS Azure\nGC 0.71 15.3 51.22\nA WS 13.85 0.15\nAzure 49.80 1.56\nWe have compiled the current prices for spot and on-demand\ninstances for T4 GPUs with 8 CPU cores and the egress costs for\nthree well-known cloud providers, GC [5], AWS [2], and Azure [9]\n(Section 1). There are two different pricing concepts. On the one\nhand, there are GC and Azure, which offer relatively cheap instances,\nwith 69% and 73% savings over on-demand pricing, respectively,\nand relatively expensive egress charges between continents of up to\n$0.15/GB. On the other hand, there is AWS, where the spot instance is\nonly 51% cheaper than the on-demand instance and more than twice\nas expensive as GC or Azure. However, the egress fees here are much\ncheaper at only $0.02/GB. Because of the additional offerings around\ncompute, such as networking, identity and cost management, and\ntooling, it is not easy to fairly compare cloud providers. Therefore,\nwe will limit ourselves to network and VM costs.\nWith the multi-cloud experiments from this section, we want to\nevaluate the following scenarios: First, partially switching from one\nprovider to another without stopping the training. Second, scaling\nresources in the same region when one of the cloud providers is\nalready at capacity for spot-priced VMs or the current price is too\nhigh [24]. We know from Section 4 that scaling resources in the same\nlocation can significantly improve performance, which may only be\npossible using additional cloud providers.\nExperimental design. To enable a fair comparison between the\ncloud providers, we rented hardware most similar to each other in\nthe same region. We used each provider’s default settings and only\nchanged hardware specs. For GC, it is the same instance as in Sec-\ntion 4. At AWS, it is ag4dn.2xlarge with eight cores and 32 GB in\nthe us-west-2c region. Unfortunately, we had to make two compro-\nmises with Azure. There was only the combination of four cores and\n30 GB RAM (NC4as_T4_v3), and there were no T4 GPU resources\navailable in the us-west, so we had to fall back tous-south-2.\nThe network profiling between all cloud providers in Table 4\nshows that their intra-cloud connectivity is comparably fast with 6.4,\n4.9, and 7.6 Gb/s for GC, AWS, and Azure, respectively. All connec-\ntions are mostly symmetric, with inter-cloud connectivity between\nGC and AWS providing up to 1.8 Gb/s and a ping of 15.3ms, indicating\nthat while they are likely not in the same data center, they are close\nto each other and connected to the same Internet exchange point.\nHowever, connectivity to Azure could be better since it operates in\na different zone, with a bandwidth of 0.5 Gb/s and a ping of 51ms.\nOur experimental setup consists of four GPUs with equal contri-\nbutions from each cloud provider. D-1 is the baseline with four GPUs\nat GC, D-2 with two GPUs each at GC and AWS, and D-3 with two\nGPUs at GC and Azure. We compare moving two VMs to a different\ncloud provider to see the impact on cost and throughput.\n(1) No inter-cloud throughput penalty. Figure 10 shows the\nthroughput and granularity of each multi-cloud experiment. CV and\nNLP runs have essentially identical throughput regardless of the\ncombination of cloud providers. Only the D-3 experiments show a\nvery slight slowdown in communication time, reflected in the lower\ngranularity score (Figure 10b) of 12.72 in CV and 1.99 in NLP com-\npared to the D-1 baseline scores of 14.48 and 2.73, respectively. Actual\nthroughput was between 1-2% slower than the baseline, which is\nnegligible and only related to the slightly worse connection to the\nAzure data center. These results confirm our observation from Sec-\ntion 4 that network connectivity determines scalability, and one can\neasily train in a multi-cloud scenario.\n(2) External egress costs can overshadow VM costs.One draw-\nback to training in multiple regions or zones is that egress traffic can\nincur additional costs depending on the cloud provider. We have sum-\nmarized the cost of egress traffic within a zone (intra-zone), between\nzones in each region (inter-zone), and between continents in Sec-\ntion 1. Notably, any traffic to Oceania (Australia, New Zealand, and\nothers, abbreviated as OCE) generates the highest cost of $0.15/GB\nfor GC. We have broken down the costs for the multi-cloud experi-\nment in Figure 11a on an hourly per-VM basis. With only four peers\nin the D-1/2/3 experiments, we have an N-to-N communication, i.e.,\n\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2$ per Hour\n0.572\n0.180\n0.029\n0.058\n0.144\n0.572\n0.180\n0.191\n0.381\n0.083\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.802\n0.395\n0.029\n0.058\n0.144\n0.802\n0.395\n0.191\n0.381\n0.083\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.489\n0.134\n0.000\n0.115\n0.144\n0.489\n0.134\n0.000\n0.763\n0.083\nCloud = Azure\nModel\nCONV RXLM\n(a) Intra- and inter-zone in the US region (D-2/3).\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0\n1\n2\n3\n4\n5$ per Hour\n0.573\n0.182\n0.068\n1.181\n0.123\n0.573\n0.182\n0.251\n4.329\n0.042\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.914\n0.330\n0.068\n0.205\n0.123\n0.914\n0.330\n0.251\n0.753\n0.042\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.687\n0.177\n0.000\n0.513\n0.123\n0.687\n0.177\n0.000\n1.882\n0.042\nCloud = Azure\nModel\nCONV RXLM\n(b) Intercontinental in the US, EU, ASIA and AUS (C-8).\nFigure 11: Costs breakdown for D-2/3 and C-8 experiments.\neach peer sends its gradients to every other peer. This means that1\n3\nof the egress was internal to the partner VM in the same cloud, and\nthe remaining 2\n3 went to the remaining two peers in the other cloud.\nFirst, loading data from Backblaze costs $0.01/GB from anywhere\nin the world, which gives us a rate of $0.144/h for the CV and $0.083/h\nfor the NLP experiments. Even when CV throughput is less than\nhalf of the NLP model (Figure 10a), images are much larger than\ntext, resulting in a higher data rate. While this is close to the spot in-\nstance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time\ncosts until the entire dataset is downloaded and retrieved from the\ndisk cache, assuming large enough local storage. A more detailed\ncomparison of cloud provider storage offerings is beyond our scope,\nbut current prices range from $0.02/GB to $0.14/GB in various GC\nregions, making our setting (B2) competitive.\nSecond, the external egress costs for the NLP experiments are\nvery high compared to the other costs. They are 2.2x higher than the\nspot instance for GC and 5.7x higher for Azure, as the traffic costs\nin the US zone are $0.01/GB and $0.02/GB, respectively. The Azure\ncost is even higher ($0.763/h) than the on-demand instance price\nof $0.489/h. The CV experiments are much less affected due to the\nsmaller model size, but Azure still manages to almost match its spot\ninstance price of $0.134/h with the external egress cost of $0.115/h.\nFinally, the total compute cost, including egress and data loading\nin this multi-cloud constellation, is the sum of all the cloud providers’\nprices times the number of VMs used. For the CV experiments, GC,\nAWS, and Azure cost $0.762/h, $1.192/h, and $0.363/h, respectively,\nmaking the combination of GC with Azure 42% cheaper than GC with\nAWS. For the NLP experiments, GC, AWS, and Azure cost $0.835/h,\n$1.05/h, and $0.973/h, respectively, and GC combined with Azure is\nbetter than GC with AWS by a smaller margin of 3.9%. However, the\nintercontinental network egress prices for both GC and Azure are\nup to 15 times higher than the inter-zone prices, so what about the\ncost-effectiveness compared to geo-distributed experiments?\n(3) Geo-distributed egress can incur most of the cost. To\nillustrate the cost of intercontinental training, we use our C-8 exper-\niment with two VMs in four continents from Section 4 to plug the\ncost for each cloud provider. The egress costs are calculated slightly\ndifferently than in the D-2 and D-3 experiments because four groups\nof two VMs average locally and then distribute the gradients across\nthe other groups. This results in 8\n20 internal egress calls (two calls\nbetween each group), 6\n20 intercontinental egress calls (two calls be-\ntween three regions), and 6\n20 AUS egress calls (three regions share\ntheir gradients with AUS and vice versa).\nFigure 11b shows the resulting egress traffic cost per VM. The\nhigh cost between continents scales to a multiple of the remaining\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n5\n10\n15Egress MB per Second\nRN18\nRN50 RN152 WRN101 CONV\nGPU Count\n2\n3\n4\n8\n(a) CV\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n10\n20\n30\n40\n50Egress MB per Second\nRBse RLrg RXLM\nGPU Count\n2\n3\n4\n8 (b) NLP\nFigure 12: Baseline egress rate on 2-8 A10 GPUs.\ncost for CV and NLP with GC and Azure. For NLP, the external\negress cost for GC is $4.329/h, more than 90% of the total cost per VM\n($4.804/h). Even with Azure having a more moderate rate of $0.02/GB\nfor intercontinental communication and only $0.08/GB for OCE\ntraffic, it still results in $1.882/h external egress cost ($2.101/h total).\nThis is in contrast to AWS, which has a cap of $0.02/GB to any location,\nresulting in the best total cost of $1.376/h per VM. The relatively high\nAWS instance cost compares favorably to the other cloud providers\nregarding geo-distributed training. Keeping egress traffic in mind\nwhen deciding to scale to other continents is essential, as it can be the\nmost significant part of the total cost. This raises another question:\nIf egress traffic matters so much, how does model size affect it?\n(4) Small models have lower egress rates than larger models.\nModel size affects two parts of the distributed training time. First,\nlarger models tend to have slower averaging rates, but more data\nmovement costs due to their size. However, larger models are also av-\neraged less frequently because they take longer to perform a step. To\nanalyze this, we review the experiments in Section 3, where we evalu-\nate different model sizes and GPUs counts. Figure 12 shows the aver-\nage egress rate over each experiment’s runtime for both CV and NLP\nfrom two to eight A10 GPUs. The trend is clear: the smaller the model,\nthe lower the egress rate for all GPUs (e.g., RN18 vs. RN50). This is\nsurprising, as the \"square-cube\" law [37] states that with a decrease in\nparameters, the calculation time will decrease quadratically while the\ncommunication time decreases linearly. This means that with a suffi-\nciently small model, most of the training will consist of communica-\ntion time, and the egress rate would increase, as it is defined through\nparameter count\ncalculation time . However, we find that even with our smallest model,\nRN18, with 11.7M parameters and eight A10 GPUs, we are still not at\nthe point where the communication time takes up most of the time.\nIn summary, multi-cloud training is generally possible and can\nbe cost-effective when keeping the egress costs and granularity in\nmind. Regardless of the cloud provider, staying in the same region is\npreferred, with the US having the most favorable egress price offers.\nA significant portion of the cost may be hidden in egress costs, ac-\ncounting for more than 90% of the total cost in our NLP experiments\nin GC and Azure. Based on the additional egress costs alone, renting\non-demand hardware may be more advantageous than using spot\ninstances between different regions. CV training is generally more\ncalculation- than communication-heavy, resulting in slightly higher\ndata-loading but fewer egress costs. However, from our experiments,\nthis is a favorable trade-off because data-loading is much cheaper\nthan egress costs.\n6 HYBRID-CLOUD PERFORMANCE\nCan augmenting on-premise hardware with cloud resources be\nworthwhile to speed up DL training? In this section, we examine two\n\nTable 5: Average hybrid-cloud throughput and latency.\n(a) Single stream TCP throughput in Gb/s.\nFrom To EU T4US T4US A10\nRTX8000 0.45 0.06 0.05DGX-2 (8xV100)0.55 0.08 0.07\n(b) ICMP Latency in ms.\nFrom To EU T4US T4US A10\nRTX8000 16.73150.80 159.05\nDGX-2 (8xV100)16.19150.27 158.54\nTable 6: Hybrid- vs. cloud-only throughput for the (E) setting.\nModel\nSetup RTX8000E-A-8 E-B-8 E-C-8 8xT4 8xA10\nCONV 194.8 316.8 283.5 429.3 261.9 620.6\nRXLM 431.8 556.7 330.6 223.7 575.1 1059.9\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(a) CV Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n8.21\n7.28\n5.80\n3.24\n1.86\n1.98\n2.15\n1.77\n1.21\n1.21\n0.72\n0.64\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(c) NLP Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n1.27 1.00\n0.88 0.66\n0.34\n0.34\n0.27\n0.27\n0.20\n0.15\n0.12 0.07\nCalculation\nCommunication (d) NLP Granularity\nFigure 13: Hybrid-cloud experiments for the (E) setting.\nsettings: (E), where a consumer-grade GPU, the RTX8000, is deployed\non-site, and (F), where a server-grade node, the DGX-2 (8xV100), is\ndeployed on-site. We vary the extra resources, between one to eight\nT4 EU ({E,F}-A), T4 US ({E,F}-B) and A10 US ({E,F}-C) GPUs.\nExperimental design. In both settings, we want to investigate\nhow to extend local hardware with cloud resources and when this\nleads to better throughput. The cloud resources, in this case, are the\nsame US/EU GC T4 instances as in Section 4 and the US LambdaLabs\nA10 GPUs from Section 3. We double the number of cloud VMs with\neach increment, starting with one additional GPU (i.e., E-A-1) until\nwe have eight additional cloud VMs (i.e., E-A-8). This allows us to com-\npare the same hardware in the EU and the US, and slightly weaker, lo-\ncal hardware (EU T4) and better, but more distant hardware (US A10).\nBoth the (E) and (F) setups share the network uplink between 450\nand 550 Mb/s to the EU datacenter in Belgium, as they are located\nin the same building in Europe (Table 5). However, as this is not a\nGoogle-owned datacenter, the traffic is partly going over the public\ninternet, which results in a lower bandwidth of 50 and 80 Mb/s to\nthe US-based VMs compared to 210 Mb/s between the US and EU\nGC datacenters (Table 3a).\n(E) Consumer-grade setting. The results follow the same trend\nas in Section 4. The CV task has a higher granularity of 8.21 with\n2 GPUs at E-A-1 than NLP (1.27) (Figures 13b and 13d), and scales\nregardless of the location of the cloud resources (Figure 13a). We\nalmost match the baseline throughput of 195 SPS at 5 GPUs in all set-\ntings for CV (E-A-4, E-B-4, E-C-4). The best throughput was reached\nat E-C-8 with the US A10 GPUs with 429 SPS. For NLP, only the\nE-A-8 experiment beats the baseline with a speedup of 1.29x and 556\nSPS due to the low granularity and the intercontinental base penalty\nfor the US experiments.\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(a) CV Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n3.94 3.17 2.32 2.46\n1.66 1.16\n1.71 1.04\n0.82 0.56\n0.66\n0.57\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n500\n1000\n1500Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(c) NLP Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n0.54 0.39 0.41 0.30\n0.15\n0.07\n0.15\n0.94\n0.10 0.09 0.07\n0.02Calculation\nCommunication (d) NLP Granularity\nFigure 14: Hybrid-cloud experiments for the (F) setting.\nHowever, is combining on-premise and remote cloud resources\nbetter than using the cloud without paying the intercontinental band-\nwidth tax? To analyze this, we compare the(E) experiments with the\n8xA10 experiment from Section 3 and 8xT4 experiment from Sec-\ntion 4 in Section 6. First, the 8xA10 experiments are the fastest for\nboth CV and NLP, which removes the respective hybrid-cloud com-\nbination from contention (E-C-8). Second, the 8xT4 experiments\nfor NLP are faster than any other hybrid-cloud setup, making the\ncloud-only solution favorable. Finally, while we always beat the\nbaseline 8xT4 CV throughput (261.9 SPS), but in the case of E-B-8\n(283.5 SPS), just barely. The throughput of E-A-8 (316.8 SPS) makes\nthe hybrid-cloud setup the most favorable in terms of relative GPU\nscaling (32.5 SPS per GPU), but it does not come close to the best\ncloud-only throughput of 8xA10 with 620.6 SPS.\nSummarizing, the cloud-only experiments are the fastest over-\nall due to their single-GPU throughput and locality. Adding cloud\nresources to on-premise hardware leads to a high communication\ntime, which is not compensated by the additional processing speed\nof the GPUs. Proximity to the on-premise hardware is essential, as\nthe more local cloud resources (E-A-8) consistently resulted in a\nbetter throughput than the same remote cloud resources (E-B-8).\n(F) Server-grade setting. The baseline throughput is signifi-\ncantly higher compared to the RTX8000, with a much more powerful\n8xV100 DGX node to 413 SPS for CV and 1811 SPS for NLP (Fig-\nures 14a and 14c) via PyTorch data parallelism [26]. This increases the\npenalties from Section 3, leading to the only speedup from baseline\nfor CV in experiments F-A-8 (507 SPS) and F-C-8 (510 SPS). This is sur-\nprising, as the older T4 GPUs in the EU perform similarly to the much\nnewer A10 GPUs in the US, showcasing the trade-off between slower,\nlocal compute and faster, remote compute. The granularity of 2.46\nfor F-A-8 shows that there is enough calculation time to distribute,\nwhile the F-C-8 experiments spend≈ 62% of the total training time on\ncommunication with a granularity of 0.57 (Figure 14b). The NLP ex-\nperiments never reach the baseline throughput of the 8xV100 due to\nusing most of the time for communication. The NLP F-B and F-C ex-\nperiments mainly consist of communication (Figure 14d) with a gran-\nularity of up to 0.02, which results in a nonlinear, unstable training\ntime due to the minimum matchmaking time issue(2) from Section 3.\nIn summary, the hybrid-cloud experiments conclude that while\non-premise hardware can be augmented with cloud resources, it\nwill likely be cost-efficient if all resources are on the same continent.\nUsing only cloud resources is more advantageous if the on-premises\nhardware is not co-located.\n\n0.5\n 1.0\n 1.5\n 2.0\n 2.5\n 3.0\nCost in $ per 1M Samples\n0\n1000\n2000Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nInstance Type\nSpot\nOn-Demand\nFigure 15: Cost to throughput tradeoff for RoBERTaXLM at\ndifferent instance types. Our training setups (circled), that are\ndue the low granularity of the NLP model, neither cheaper,\nnor faster than the centralized offering (DGX-2).\n7 FURTHER INSIGHTS\nCommunication time can decrease with more peers. Let us\ncompare the granularity of the experiments for E-B (Figure 13b),\nwhich uses T4 GPUs in the US as an additional cloud resource.Both\nthe computation and communication time decrease with the number\nof GPUs, even increasing the granularity from 1.98 at E-B-2 to 2.15\nat E-B-4. This is surprising since, usually, with more peers, the com-\nmunication time should increase, and the US-EU communication\nbottleneck should slow us down to the same extent as the E-B-1\nexperiment. This reduction is a Hivemind-specific anomaly, as it\nuses a single TCP stream per peer. With TCP, there needs to be an\nacknowledgment (ACK) of each packet by the receiving peer, which\nis impacted by the connection’s latency. In our high latency network\nbetween continents, the round trip time (RTT) of 300-318ms limits\nthe maximum bandwidth a single TCP stream to 50-80 Mb/s. How-\never, a way to improve link utilization is to use multiple streams,\none for each peer, which we encounter in experiments E-(B|C)-2,4,8.\nTo verify the potential gains, we perform a microbenchmark of\nthe multi-stream bandwidth from the RTX8000 to the EU and US\ndata centers.Although there is wide variation, likely due to network\nutilization, with 80 clients, we achieve a maximum bandwidth of\n6 Gb/s within the EU and up to 4 Gb/s to the US. While larger peer\ngroups and, consequently, larger models benefit from multi-peer\ncommunication by default and do not see significant changes in com-\nmunication time, small models in unevenly distributed VMs setups\ncan be disproportionately affected. The same trend can be observed\nin all high latency experiments (i.e., between the EU and the US), e.g.,\nE-B, E-C for CV and NLP (Figures 13b and 13d, and F-B and F-C for\nCV (Figure 14b). In summary, uneven distribution of computational\nresources in high-latency networks (e.g., intercontinental) can re-\nduce communication time with Hivemind due to more parallelism,\nlessening the impact of low bandwidth for a single data stream.\nCost analysis. The DGX-2 (8xV100) node from Section 6 rep-\nresents server-grade hardware that could be used to train models.\nHowever, how does it compare in throughput per $ to all of our dis-\ntributed cloud experiments? The Figure 1 (CV) and Figure 15 (NLP)\nshow the complete cost analysis of the DGX-2, the 8xT4 experiments,\nand the 8xA10 experiments for spot and on-demand pricing. We use\nthe internal egress costs from Figure 11a as a reference for the 8xT4\nsetup. For simplicity, we compare the spot pricing without inter-\nruptions, as we assume that a new VM can be spun up fast enough\nnot to affect the training throughput in the long run. We mark the\ncentralized baseline (DGX-2) cost per 1M samples and the through-\nput in samples per second with a horizontal and vertical line. This\nmeans that we are cheaper to the left to the vertical line, and above\nthe horizontal line, we are faster (and vice versa). We circle the new\nvalue propositions that we enable in both figures. Our hardware\nsetups have additional key characteristics: They are resilient by de-\nfault to interruptions due to running in a decentralized fashion and\nthey enable the combination of more GPUs than cloud providers\noffer in a single node. Currently, common hardware configurations\n(DGX) allow up to eight GPUs connected via NVLink, and with older\nhardware, only up to 4xT4s connected via PCIe at 10 GB/s between\nGPUs (with GC). We were able to combine eight single GPU nodes\nfrom GC and LambdaLabs to create competing performance and\nprice setups without dedicated GPU interconnects.\nA spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-\ndemand) in GC US, which makes it the best value proposition for\nthe low granularity NLP task. It is followed by the 8xA10, which\nare 41% slower and 30% more expensive than the DGX-2 (Figure 15).\nThe 8xT4 experiments are even more expensive, as the internal\negress costs take up more than half of the costs, making them the\nworst value proposition. However, for CV, we manage to provide\ntwo new offerings: First, the 8xA10, which is both 50% faster and 49%\ncheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-\n2, while being 37% slower (Figure 1). The CV model can be scaled\nmore easily due to its initially high granularity, which makes the very\ncompetitive offering of $0.6/h per A10 from LambdaLabs an excellent\nvalue proposition. However, while we only evaluated eight T4 GPUs\nfor our GC-based experiments, with a granularity of 5.19 (CV A-8\nin Figure 7b), there is ample space to scale even further. It is important\nto note that LambdaLabs does not charge for any data egress, but GC\ndoes with $0.01/GB, and the 8xT4 experiment is still cheaper. While\nLambdaLabs is often at capacity, Google Cloud positions itself as a\nhyperscaler with the advantage of rarely being at max occupancy.\nWe also evaluated the performance of the 4xT4 PyTorch DDP [26]\nfor CV with the best available multi-T4 node on GC (4xT4). The NLP\nexperiments ran OOM. Since the DDP 4xT4 runs on a single node, it\ncauses no interconnect costs and is priced at $0.96 per 1M samples at\nspot pricing, while our 8xT4 setup costs $1.77 per 1M samples (84%\nmore expensive). However, the 8xT4 setup has a higher throughput\nof 262 SPS (26% faster) compared to the 4xT4 node (207 SPS). This\nhigher speed is not available at the price point of the 4xT4 node.\nMoreover, the 8xT4 setup has the potential for further scaling, which\nwe discussed in detail in Section 4.\nIn summary, the lower spot prices for older GPUs allow us to train\nmodels more cost-efficiently when task granularity allows it and get\nmore value per $ when training on the 8xT4 or 8xA10 compared to\nan DGX-2 node. Combining multiple nodes with single GPUs with\nlower bandwidths enables scaling that was previously impossible to\nachieve without resorting to much more powerful GPUs. Distributed\nspot instance pricing opens up a new value proposition compared\nto on-demand offerings that can even compete with the competitive\npricing of smaller cloud providers.\nSpot VM Interruption Frequency. While we used low spot\nprices as a cost-saving argument in our experiments, we did not\nelaborate on the most significant drawback - the possibility of being\nterminated by the cloud provider at any time. There is already some\nresearch on how different cloud providers track the interruption fre-\nquency and can be used for varying workloads to achieve a positive\n$-per-throughput effect [24, 42, 43].\n\nInterruption affects three aspects: First, the interruption frequency\nis defined by AWS as the number of VMs terminated in the last 30\ndays, which is between 5 and 20% [3]. This value was not representa-\ntive during our experiments with any cloud provider, as we noticed\nthat it is highly dependent on the time of day of the zone.\nSecond, the time needed to setup a VM until training starts. The\nstartup time of a VM depends on the cloud provider (e.g., a preconfig-\nured image) and the one’s technology stack (e.g., Docker, Kubernetes,\nAnsible). In our experience, VM startup time ranges between sec-\nonds to minutes with manual deployment taking up to 10 minutes.\nAlthough startup time can be improved, model training typically\ntakes multiple hours or days, making it a less impactful optimization.\nThird, the time required for the new peer to synchronize the train-\ning state with other peers. In our experience, this took at worst two\nhivemind epochs due to the averaging starting before synchroniza-\ntion is finished. While it is possible to create a hivemind epoch that\nis short enough to prevent new peers from joining, this only hap-\npens with a low enough granularity where scaling is not beneficial\nanymore as we are mostly communication bound.\nFinally, while the VM setup and synchronization of the training\nstate take time, the interruption frequency significantly affects the fi-\nnal throughput. We faced difficulties acquiring even a single spot VM\nduring our GC experiments during daylight hours. This highlights\nthe need for systems like SkyPilot [43], which utilizes automation to\ndeploy spot instances across various clouds and zones. In our case,\nthe interruption frequency can be used as a penalty on the training\nthroughput, i.e., a 5% interruption frequency over the entire training\ntime means roughly a 5% slower training.\n8 LESSONS LEARNED\nWe find it important to summarize our findings more generically to\nprovide guidance for DL practitioners that want to perform distri-\nbuted spot training. These lessons are based on the Sections 3 to 6.\nSmall model training still scales. We have shown that models\nbetween 12M-560M parameters can be trained in a decentralized,\ndistributed fashion achieving a speedup of up to 4.37x on eight\nAmpere-GPUs. The limiting factor as to when a model is suitable\nfor (geo-)distributed training is the target batch size which all peers\nneed to accumulate until synchronization happens. We found a TBS\nof 32K suitable to not only train in a single zone, but even see a\nspeedup when using VMs in four different continents. As long as the\noptimizer can handle big-batch training and the dataset is big enough\nto accommodate large batches, the remaining issue to find the base\ngranularity of the model to decide how to scale it cost-effectively.\nFinally, we found that small models induce less traffic over larger\nmodels over time, even at a much higher averaging rate, making\nthem better suited for cost-efficient training than large models.\nEgress costs can take up most of the total cost.Egress pricing\nfor the NLP experiments overtook the spot and the on-demand costs\nof T4 GPUs when training on four continents or even in two zones.\nFor example, RoBERTaXLM’s high throughput and parameter count\nrequire more data to be sent between peers during averaging due to\nsmaller granularity. Under the current pricing models, AWS has the\nbest value for geo-distributed training, while GC and Azure are best\nat training in a single zone. The biggest cost-saving potential lies in\ncloud providers that do not charge for egress at all, like LambdaLabs.\nGranularity is important to evaluate scalability.We found\nthat the ratio between calculation and communication time, gran-\nularity, is the most important metric to track when deciding on\ndistributed training suitability. It enables us to compare the scala-\nbility potential between different models on the same hardware due\nto summarizing their model size and throughput ratio. Additionally,\nit gives a value to the cost-efficiency: With a granularity of exactly 1,\nthe potential speedup when doubling the number of VMs is, at best,\n1.33x due to halving the calculation time. However, with a granu-\nlarity of 10, the speedup with double the VMs is, at best, 1.83x due\nto the communication time playing a less significant role. With this,\nwe can estimate training performance with additional resources.\nGeo-distributed multi-cloud training is possible and is cost-\nefficient.Even with the current teething pains of Hivemind, we got a\nspeedup in all of our experimental setups of intra-zone, transatlantic,\nand intercontinental settings as long as the granularity of the task\npermitted it. Using older and cheaper Tesla GPUs at spot pricing\nis not only more cost-efficient than the DGX-2 offering, but even\ntrumps the competitive pricing model of LambdaLabs, all while in-\ncluding egress costs. Our network profiling showed that the current\ntraining limitations are not primarily the bandwidth but rather the\nintercontinental latency and the task’s granularity. If the granularity\nis already low at high bandwidth, it can only worsen when used in\na high latency, low bandwidth network. When considering both,\nestimating the potential cost-savings of investing in a multi-/hybrid-\ncloud scenario is possible.\n9 RELATED WORK\nDecentralized deep learning. Training with unreliable peers has\nbeen studied in a collaborative setting, resulting in the Distributed\nDeep Learning in Open Collaborations (DeDLOC) [17] algorithm,\non which the Hivemind framework [ 39] is based. It can interpo-\nlate between traditional distributed DL algorithms like parameter\nservers [25], decentralized SGD [27], or All-Reduce SGD [1]. We\nused the Hivemind framework for all of our experiments, as it pro-\nvided the base for training on spot instances in high latency, low\nbandwidth networks.\nSWARM [37] applies both previous techniques and adds model\nparallelism to the mix by creating pipelines between nodes and re-\nbalancing them in case of failures. The authors find a crucial insight\nin the \"square-cube\" law, which argues for better training scalability\nwith larger model sizes; as the size increases linearly, so does the com-\nmunication time, while the calculation time increases quadratically.\nWe add to that by analyzing distributed training for smaller model\nsizes that pose different trade-offs. We show that while the square-\ncube law still holds for increasing model sizes, under consideration\nof granularity, we can still train small models.\nDecentralized deep learning on heterogeneous hardware with\nslow interconnects can benefit the training of foundation models. To\nachieve this, model and pipeline parallelism can be used in addition\nto data-parallel training [45]. This is a complementary work to ours,\nsince we target smaller models and weaker hardware.\nDeep learning on spot instances. DeepSpotCloud [23] is a\nsystem that uses the AWS API to automatically migrate a DL task\nwith checkpointing whenever the spot instance is terminated. The\nauthors note that the volatility of GPU instance pricing and inter-\nruptions have a unique pattern compared to non-accelerated VMs,\n\nand solve this by using intercontinental provisioning. We noticed\nthe same trends of high interruption ratios in our experiments. How-\never, we have shown that geo-distributed training is possible until\ngranularity permits it, which poses a possibility for ever-migrating\ntraining between continents without checkpointing.\nAmazon Sagemaker [14] is an AWS service that allows to perform\nML under budget constraints. For training, it supports spot VM migra-\ntion until a cost threshold is reached by checkpointing the progress.\nHowever, it lacks the option of training on multiple spot VMs. It can\ndo either spot instance training on DGX-like nodes or combine mul-\ntiple on-demand nodes with PyTorch DDP (or similar), but not both.\nThis eliminates the potential of accelerating the training process\nwith more GPUs that do not fit a single spot-provisioned hypervisor.\nThe analysis by Yang et al. [42] investigates maximizing a tar-\nget accuracy from a spot pricing versus time perspective. Linear\nprogramming was used to decide how to provision the VMs with\ndifferent cost-utility trade-offs. While this shows the potential of uti-\nlizing multiple clouds and continents for non-distributed tasks, we\nevaluated the distributed spot training problem from the throughput,\ncost, and model size perspective on different hardware setups. By in-\ncluding our insights, their technique for scheduling on spot instances\ncould be adapted to optimize the total throughput of all peers.\nSkypilot [43] is a broker system where users can submit their\nhardware requirements, and it tries to provision the necessary re-\nsources on any supported cloud. It features a preemption analysis\nthat counts the number of interruptions in a zone and can decide to\nmigrate whenever they cross a certain threshold. We have shown\nthat multi-, hybrid-cloud, and geo-distributed training is possible,\nand by combining our insights, it would open up auto-migrated,\ndecentralized DL training for the best spot prices in the world.\n10 CONCLUSION\nThis paper analyzes multi- and hybrid-cloud training in a decentral-\nized fashion on spot instances. We define the lower bounds of model\nsizes that can be scaled cost-efficiently using the granularity metric to\nestimate their suitability for distributed training in low-bandwidth,\nhigh-latency situations. We show that training on multiple cloud\nproviders and four continents still scales with additional compute\nresources. Alternatively to the current use of spot instances in DL,\nwe show the potential of using spot instances in a distributed, de-\ncentralized way by being more cost-efficient with eight T4 instances\nover a DGX-2 from the same cloud provider while paying additional\negress costs. Finally, we provide an intuition about where costs in\nsuch a training scenario come from and how different model sizes\nfrom CV and NLP affect throughput and costs. Our work empowers\npractitioners to utilize spot-priced instances for distributed deep\nlearning with relatively small models. Our insights show some po-\ntential that can further improve distributed training performance,\nsuch as optimizers with higher minibatch sizes and improvements\nregarding the communication time with, e.g., better compression.\n11 APPENDIX: ASR CASE STUDY\nWe perform a case study on Automatic Speech Recognition (ASR)\nto showcase spot training on weaker GPUs. Whisper [34] is a state-\nof-the-art ASR model trained on 680,000 hours of labeled data to\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nT4 Count\n0\n10\n20\n30Samples per Second\nTBS\n256\n512\n1024\n(a) Throughput\n2\n 4\n 8\n 2\n 4\n 8\n 2\n 4\n 8\nT4 Count\n0\n100\n200\n300Time in Seconds\nTBS=256\nTBS=512 TBS=1024\n1.86 0.42 0.57\n3.80\n1.36 0.40\n7.47\n2.91\n1.17\nCalculation\nCommunication (b) Granuarity\nFigure 16: WhisperSmall performance with varying TBS.\n0\n 10\n 20\n 30\n 40\n 50\nCost in $ per 1M Samples\n0\n20\n40Samples per Second\nDDP 4xT4 DDP 4xT4\nDDP 2xT4 DDP 2xT4\nA100 A100\n8xT4 8xT4\n4xT4 4xT4\n1xT4\n1xT4\nInstance Type\nSpot\nOn-Demand\nFigure 17: Cost to throughput tradeoff for WhisperSmall at\nTBS=1024 with different instance types. Our training setups\n(circled) provide mixed result of being slightly faster and more\nexpensive than comparable, centralized DDP offering.\ntranscribe audio. It features different sizes, from 37.8M to 1.5B pa-\nrameters, and was trained with a minibatch size of 256. We use the\nCommonvoice [11] dataset, preprocessed to Log-Mel spectrograms.\nIn our distributed experiments, we start with a TBS of 256 and in-\ncrease to 512 and 1024 to combat potential granularity issues. Due to\nmemory constraints, only three model sizes (Tiny, Base, Small) were\ntrainable on the T4 GPU. Unfortunately, the original TBS of 256 was\nnot large enough to train the relatively small models due to their\nsmall granularity (0.04, 0.14 and 0.57 at 8xT4, respectively) with no\nperformance benefits. The only model showing scaling potential is\nWhisperSmall, with a granularity of 1.8 with 2xT4. However, when\nscaling the target batch size to 512 and 1024, we see some benefit\nover the single GPU runs for the WhisperSmall model (Figure 16).\nBy effectively increasing the amount of computation by the factors\nof 2 and 4, we can generate a speedup of1.27× and 2.2× with 8xT4’s\nfor the TBS 512 and 1024, respectively. When compared to other\nhardware setups, our A100 80GB GPU and the best multi-T4 GPU\non GC (4xT4) with Pytorch DDP (Figure 17) have almost double the\nthroughput at 46 SPS and are slightly slower at 24 SPS, respectively,\ncompared to our 8xT4 setup which runs at 28 SPS. This outcome is\nnot surprising due to the generational leap in architecture for the\nA100 and the slower interconnect with our 8xT4 experiments com-\npared to a single 4xT4 node (see Section 3 for a detailed throughput\nanalysis). The proposed cost-throughput ratio is mixed: the A100 is\nat $12.19/1M samples, the DDP 4xT4 is at $8.41/1M, and our 8xT4 is\nat $14.53/1M. Our proposed setup is slightly more expensive than the\nA100, and it will not scale beyond eight T4 GPUs due a granularity\nat 1.17, leaving the A100 as the fastest and the DDP 4xT4 setup as the\ncheaper but slower alternative. Despite these results, our proposed\nsetup has several benefits, including resilience for spot interruptions,\ninterruption-free migration to the lowest cloud prices, and the pos-\nsibility to scale the GPU count up as long as granularity permits it.\nACKNOWLEDGMENTS\nThis work is funded in part by the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) - 392214008.\n\nREFERENCES\n[1] [n.d.]. Horovod: fast and easy distributed deep learning in TensorFlow,\nauthor=Sergeev, Alexander and Del Balso, Mike, journal=arXiv preprint\narXiv:1802.05799, year=2018. ([n. d.]).\n[2] 2023. Amazon A WS. Accessed: 19 May 2023, aws.amazon.com.\n[3] 2023. Amazon AWS Spot Pricing. https://aws.amazon.com/blogs/compute/new-\namazon-ec2-spot-pricing/. Accessed: 2023-09-27.\n[4] 2023. Backblaze. https://backblaze.com/. Accessed: 2023-10-05.\n[5] 2023. Google Cloud. Accessed: 19 May 2023, cloud.google.com.\n[6] 2023. Google Cloud Region Picker. https://cloud.withgoogle.com/region-picker/.\nAccessed: 2023-10-05.\n[7] 2023. Hivemind GAC Issue. https://github.com/learning-at-home/hivemind/\nissues/566. Accessed: 2023-10-05.\n[8] 2023. LambdaLabs. Accessed: 19 May 2023, lambdalabs.com.\n[9] 2023. Microsoft Azure. Accessed: 19 May 2023, portal.azure.com.\n[10] Alex Aizman, Gavin Maltby, and Thomas Breuel. 2019. High Performance I/O\nFor Large Scale Deep Learning. In2019 IEEE International Conference on Big Data\n(Big Data). 5965–5967. https://doi.org/10.1109/BigData47090.2019.9005703\n[11] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,\nJosh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor\nWeber. 2019. Common voice: A massively-multilingual speech corpus. arXiv\npreprint arXiv:1912.06670 (2019).\n[12] Alexander Borzunov, Max Ryabinin, Tim Dettmers, Quentin Lhoest, Lucile\nSaulnier, Michael Diskin, and Yacine Jernite. 2022. Training Transformers\nTogether. InNeurIPS 2021 Competitions and Demonstrations Track. PMLR, 335–342.\n[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning\nat Scale. arXiv:1911.02116 [cs.CL]\n[14] Piali Das, Nikita Ivkin, Tanya Bansal, Laurence Rouesnel, Philip Gautier, Zohar\nKarnin, Leo Dirac, Lakshmi Ramakrishnan, Andre Perunicic, Iaroslav Shcherbatyi,\nWilton Wu, Aida Zolic, Huibin Shen, Amr Ahmed, Fela Winkelmolen, Miroslav\nMiladinovic, Cedric Archembeau, Alex Tang, Bhaskar Dutt, Patricia Grao, and\nKumar Venkateswar. 2020. Amazon SageMaker Autopilot: A White Box AutoML\nSolution at Scale. In Proceedings of the Fourth International Workshop on Data\nManagement for End-to-End Machine Learning (Portland, OR, USA) (DEEM’20).\nAssociation for Computing Machinery, New York, NY, USA, Article 2, 7 pages.\nhttps://doi.org/10.1145/3399579.3399870\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In2009 IEEE conference on computer\nvision and pattern recognition. Ieee, 248–255.\n[16] Tim Dettmers. 2016. 8-Bit Approximations for Parallelism in Deep Learning.\narXiv:1511.04561 [cs.NE]\n[17] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton\nSinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov,\nAlbert Villanova del Moral, et al. 2021. Distributed Deep Learning In Open Collab-\norations. Advances in Neural Information Processing Systems34 (2021), 7879–7897.\n[18] O Elharrouss, Y Akbari, N Almaadeed, and S Al-Maadeed. [n.d.]. Backbones-\nreview: Feature extraction networks for deep learning and deep reinforcement\nlearning approaches. arXiv 2022. arXiv preprint arXiv:2206.08016 ([n. d.]).\n[19] Anne C Elster and Tor A Haugdahl. 2022. NVIDIA Hopper GPU and Grace CPU\nHighlights. Computing in Science & Engineering 24, 2 (2022), 95–100.\n[20] Wikimedia Foundation. 2023. \"Wikimedia Downloads\" . https:\n//dumps.wikimedia.org\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. InProceedings of the IEEE conference on computer\nvision and pattern recognition. 770–778.\n[22] Kai Hwang. 1992. Advanced Computer Architecture: Paral-\nlelism,Scalability,Programmability (1st ed.). McGraw-Hill Higher Education.\n[23] Kyungyong Lee and Myungjun Son. 2017. DeepSpotCloud: Leverag-\ning Cross-Region GPU Spot Instances for Deep Learning. In 2017 IEEE\n10th International Conference on Cloud Computing (CLOUD) . 98–105.\nhttps://doi.org/10.1109/CLOUD.2017.21\n[24] Sungjae Lee, Jaeil Hwang, and Kyungyong Lee. 2022. SpotLake: Di-\nverse Spot Instance Dataset Archive Service. In 2022 IEEE Interna-\ntional Symposium on Workload Characterization (IISWC) . 242–255.\nhttps://doi.org/10.1109/IISWC55918.2022.00029\n[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja\nJosifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distri-\nbuted machine learning with the parameter server. In11th {USENIX} Symposium\non Operating Systems Design and Implementation ({OSDI} 14). 583–598.\n[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,\nAdam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch\ndistributed: Experiences on accelerating data parallel training. arXiv preprint\narXiv:2006.15704 (2020).\n[27] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.\n2017. Can decentralized algorithms outperform centralized algorithms? a case\nstudy for decentralized parallel stochastic gradient descent.Advances in neural\ninformation processing systems 30 (2017).\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,\nand Saining Xie. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 11976–11986.\n[30] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor\nBittorf, et al. 2020. MLPerf Training Benchmark. Proceedings of Machine Learning\nand Systems 2 (2020), 336–349.\n[31] Petar Maymounkov and David Mazieres. 2002. Kademlia: A peer-to-peer\ninformation system based on the xor metric. In Peer-to-Peer Systems: First\nInternationalWorkshop, IPTPS 2002 Cambridge, MA, USA, March 7–8, 2002 Revised\nPapers. Springer, 53–65.\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library.Advances\nin neural information processing systems 32 (2019).\n[33] Gustavo Portella, Genaina N Rodrigues, Eduardo Nakano, and Alba CMA Melo.\n2019. Statistical analysis of Amazon EC2 cloud pricing models.Concurrency and\nComputation: Practice and Experience 31, 18 (2019), e4451.\n[34] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and\nIlya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision.\nIn Proceedings of the 40th International Conference on Machine Learning (Proceed-\nings of Machine Learning Research), Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202.\nPMLR, 28492–28518. https://proceedings.mlr.press/v202/radford23a.html\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.\nDeepspeed: System optimizations enable training deep learning models with\nover 100 billion parameters. InProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. 3505–3506.\n[36] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\nShuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:\nDemocratizing Billion-Scale Model Training. arXiv:2101.06840 [cs.DC]\n[37] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov.\n2023. SWARM Parallelism: Training Large Models Can Be Surprisingly\nCommunication-Efficient. arXiv preprint arXiv:2301.11913 (2023).\n[38] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady\nPekhimenko. 2021. Moshpit SGD: Communication-Efficient Decentralized\nTraining on Heterogeneous Unreliable Devices. InAdvances in Neural Information\nProcessing Systems , Vol. 34. https://proceedings.neurips.cc/paper/2021/file/\n97275a23ca44226c9964043c8462be96-Paper.pdf\n[39] Learning@home team. 2020. Hivemind: a Library for Decentralized Deep\nLearning. https://github.com/learning-at-home/hivemind.\n[40] Chathurika S. Wickramasinghe, Daniel L. Marino, and Milos Manic. 2021. ResNet\nAutoencoders for Unsupervised Feature Learning From High-Dimensional\nData: Deep Models Resistant to Performance Degradation.IEEE Access 9 (2021),\n40511–40520. https://doi.org/10.1109/ACCESS.2021.3064819\n[41] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi,\nand Ludwig Schmidt. 2023. Stable and low-precision training for large-scale\nvision-language models. arXiv preprint arXiv:2304.13013 (2023).\n[42] Sheng Yang, Samir Khuller, Sunav Choudhary, Subrata Mitra, and Kanak Mahadik.\n2022. Scheduling ML Training on Unreliable Spot Instances. InProceedings of the\n14th IEEE/ACM International Conference on Utility and Cloud Computing Compan-\nion (Leicester, United Kingdom)(UCC ’21). Association for Computing Machinery,\nNew York, NY, USA, Article 29, 8 pages. https://doi.org/10.1145/3492323.3495594\n[43] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhard-\nwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal,\nScott Shenker, and Ion Stoica. 2023. SkyPilot: An Intercloud Broker for\nSky Computing. In 20th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 23) . USENIX Association, Boston, MA, 437–455.\nhttps://www.usenix.org/conference/nsdi23/presentation/yang-zongheng\n[44] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh\nBhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learning: Training bert in 76 minutes.\narXiv preprint arXiv:1904.00962 (2019).\n[45] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen,\nPercy S Liang, Christopher Re, and Ce Zhang. 2022. Decentralized training\nof foundation models in heterogeneous environments. Advances in Neural\nInformation Processing Systems 35 (2022), 25464–25477.\n[46] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks.arXiv\npreprint arXiv:1605.07146 (2016).", "metadata": {"url": "https://arxiv.org/pdf/2306.03163", "type": "paper", "year": "2023"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "How Can We Train Deep Learning Models Across\nClouds and Continents? An Experimental Study\nAlexander Erben\nTechnical University of Munich\nalex.erben@tum.de\nRuben Mayer\nUniversity of Bayreuth\nruben.mayer@uni-bayreuth.de\nHans-Arno Jacobsen\nUniversity of Toronto\njacobsen@eecg.toronto.edu\nABSTRACT\nThis paper aims to answer the question: Can deep learning models\nbe cost-efficiently trained on a global market of spot VMs spanning\ndifferent data centers and cloud providers? To provide guidance, we\nextensively evaluate the cost and throughput implications of train-\ning in different zones, continents, and clouds for representative CV,\nNLP and ASR models. To expand the current training options further,\nwe compare the scalability potential for hybrid-cloud scenarios by\nadding cloud resources to on-premise hardware to improve training\nthroughput. Finally, we show how leveraging spot instance pricing\nenables a new cost-efficient way to train models with multiple cheap\nVMs, trumping both more centralized and powerful hardware and\neven on-demand cloud offerings at competitive prices.\nPVLDB Reference Format:\nAlexander Erben,\nRuben Mayer, and Hans-Arno Jacobsen. . PVLDB, 17(6): 1214 - 1226, 2024.\ndoi:10.14778/3648160.3648165\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/cirquit/hivemind-multi-cloud.\n1 INTRODUCTION\nDeciding whether to invest in on-premise hardware or move to the\ncloud for deep learning (DL) is not easy. Wanting to scale existing\ninfrastructure means paying upfront, as combining cloud and on-\npremise is not an option with popular DL frameworks due to needing\na dedicated high-bandwidth interconnect. To enabled model- and\ndata-parallelism, current state-of-the-art accelerators have band-\nwidths of 900 GB/s for intra-node [19] and 25 Gb/s for inter-node\nsetups [1, 26]. Due to the initial investment of the cloud providers in\nthe accelerators, they naturally want to reap profit by maximizing\nresource utilization. Therefore, it is common to have \"spot\" pric-\ning, which offers the VMs at a strongly reduced rate, typically at\na 40-90% discount (Section 1), but with the drawback that the VM\ncan be terminated at any time if another customer is willing to pay\nthe on-demand price [33]. Unfortunately, popular DL frameworks\nhave not been developed with failure semantics in mind and cannot\nadequately deal with peers that fail [12]. While services like Amazon\nSagemaker [14] and projects like Skypilot [43] offer automatic job\nmigration in case of VM termination, they are limited to single-node\ntraining due to the bandwidth requirements between accelerators.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 17, No. 6 ISSN 2150-8097.\ndoi:10.14778/3648160.3648165\nTable 1: Average us-west cloud pricing in April ’23.\nType\nCloud GC A WS Azure\nT4 Spot 0.180 $/h 0.395 $/h 0.134 $/h\nT4 On-Demand 0.572 $/h 0.802 $/h 0.489 $/h\nTraffic (inter-zone) 0.01 $/GB 0.01 $/GB 0.00 $/GB\nTraffic (inter-region) US 0.01 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) EU 0.02 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) ASIA 0.05 $/GB 0.01 $/GB 0.08 $/GB\nTraffic (inter-region) OCE 0.08 $/GB 0.01 $/GB 0.08 $/GB\nTraffic ANY-OCE 0.15 $/GB 0.02 $/GB 0.08 $/GB\nTraffic (between continents)0.08 $/GB 0.02 $/GB 0.02 $/GB\n2\n 4\n 6\n 8\n 10\nCost in $ per 1M Samples\n0\n500Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nDDP 4xT4 DDP 4xT4\nInstance Type\nSpot\nOn-Demand\nFigure 1: Cost to throughput tradeoff for ConvNextLarge at dif-\nferent instance types. Our training setups (circled) are cheaper\n(8xT4) and faster (8xA10) than centralized offerings (DGX-2).\nBut what if we could use spot pricing for long-running, distributed\njobs and reduce bandwidth requirements to leverage multiple low-\ncost GPUs? This could be possible through a framework for collabo-\nrative DL training, Hivemind [39], which inherently deals with peers\nthat can stop running at any time. While there is research on how\nHivemind can be used for training on spot VMs [17, 37, 38], it does not\ncompare the cost-throughput tradeoff for different cloud offerings or\nperform ablation studies on geographic distribution or model sizes.\nTo motivate this new possibility, we trained the ConvNextLarge\nmodel [29] on the Imagenet1K dataset [15] on different Google Cloud\nhardware (T4’s and DGX-2), and on the very competitively priced\nA10 from LambdaLabs (see Section 6 for the full experimental de-\nscription). Figure 1 shows the training throughput and the costs per\n1 million processed samples for each setup. The single node (1xT4,\n1xA10, DGX-2) experiments show the current state-of-the-art cost-\nthroughput ratio for training on GC and LambdaLabs. The DGX-2\nnode is the fastest, with a throughput of 413 SPS, but it also costs\n$6.30/h ($4.24/1M samples), shown by the horizontal and vertical\nlines. The single-accelerator experiments (1xT4, 1xA10) have a better\ncost-throughput ratio ($0.62/1M samples and $0.9/1M samples), but\nhave a much lower throughput of 80 and 185 SPS, respectively. How-\never, when using our approach of distributing the training between\nmultiple GPUs with Hivemind (circled), we make training possible\nthat is both faster (8xA10, 621 SPS, $2.15/1M samples) and cheaper\narXiv:2306.03163v4  [cs.LG]  2 Jun 2024", "sentences": [{"text": "How Can We Train Deep Learning Models Across\nClouds and Continents?", "metadata": {}}, {"text": "An Experimental Study\nAlexander Erben\nTechnical University of Munich\nalex.erben@tum.de\nRuben Mayer\nUniversity of Bayreuth\nruben.mayer@uni-bayreuth.de\nHans-Arno Jacobsen\nUniversity of Toronto\njacobsen@eecg.toronto.edu\nABSTRACT\nThis paper aims to answer the question: Can deep learning models\nbe cost-efficiently trained on a global market of spot VMs spanning\ndifferent data centers and cloud providers?", "metadata": {}}, {"text": "To provide guidance, we\nextensively evaluate the cost and throughput implications of train-\ning in different zones, continents, and clouds for representative CV,\nNLP and ASR models.", "metadata": {}}, {"text": "To expand the current training options further,\nwe compare the scalability potential for hybrid-cloud scenarios by\nadding cloud resources to on-premise hardware to improve training\nthroughput.", "metadata": {}}, {"text": "Finally, we show how leveraging spot instance pricing\nenables a new cost-efficient way to train models with multiple cheap\nVMs, trumping both more centralized and powerful hardware and\neven on-demand cloud offerings at competitive prices.", "metadata": {}}, {"text": "PVLDB Reference Format:\nAlexander Erben,\nRuben Mayer, and Hans-Arno Jacobsen.", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "PVLDB, 17(6): 1214 - 1226, 2024.", "metadata": {}}, {"text": "doi:10.14778/3648160.3648165\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/cirquit/hivemind-multi-cloud.", "metadata": {}}, {"text": "1 INTRODUCTION\nDeciding whether to invest in on-premise hardware or move to the\ncloud for deep learning (DL) is not easy.", "metadata": {}}, {"text": "Wanting to scale existing\ninfrastructure means paying upfront, as combining cloud and on-\npremise is not an option with popular DL frameworks due to needing\na dedicated high-bandwidth interconnect.", "metadata": {}}, {"text": "To enabled model- and\ndata-parallelism, current state-of-the-art accelerators have band-\nwidths of 900 GB/s for intra-node [19] and 25 Gb/s for inter-node\nsetups [1, 26].", "metadata": {}}, {"text": "Due to the initial investment of the cloud providers in\nthe accelerators, they naturally want to reap profit by maximizing\nresource utilization.", "metadata": {}}, {"text": "Therefore, it is common to have \"spot\" pric-\ning, which offers the VMs at a strongly reduced rate, typically at\na 40-90% discount (Section 1), but with the drawback that the VM\ncan be terminated at any time if another customer is willing to pay\nthe on-demand price [33].", "metadata": {}}, {"text": "Unfortunately, popular DL frameworks\nhave not been developed with failure semantics in mind and cannot\nadequately deal with peers that fail [12].", "metadata": {}}, {"text": "While services like Amazon\nSagemaker [14] and projects like Skypilot [43] offer automatic job\nmigration in case of VM termination, they are limited to single-node\ntraining due to the bandwidth requirements between accelerators.", "metadata": {}}, {"text": "This work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense.", "metadata": {}}, {"text": "Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license.", "metadata": {}}, {"text": "For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org.", "metadata": {}}, {"text": "Copyright is held by the owner/author(s).", "metadata": {}}, {"text": "Publication rights\nlicensed to the VLDB Endowment.", "metadata": {}}, {"text": "Proceedings of the VLDB Endowment, Vol.", "metadata": {}}, {"text": "17, No.", "metadata": {}}, {"text": "6 ISSN 2150-8097.", "metadata": {}}, {"text": "doi:10.14778/3648160.3648165\nTable 1: Average us-west cloud pricing in April ’23.", "metadata": {}}, {"text": "Type\nCloud GC A WS Azure\nT4 Spot 0.180 $/h 0.395 $/h 0.134 $/h\nT4 On-Demand 0.572 $/h 0.802 $/h 0.489 $/h\nTraffic (inter-zone) 0.01 $/GB 0.01 $/GB 0.00 $/GB\nTraffic (inter-region) US 0.01 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) EU 0.02 $/GB 0.01 $/GB 0.02 $/GB\nTraffic (inter-region) ASIA 0.05 $/GB 0.01 $/GB 0.08 $/GB\nTraffic (inter-region) OCE 0.08 $/GB 0.01 $/GB 0.08 $/GB\nTraffic ANY-OCE 0.15 $/GB 0.02 $/GB 0.08 $/GB\nTraffic (between continents)0.08 $/GB 0.02 $/GB 0.02 $/GB\n2\n 4\n 6\n 8\n 10\nCost in $ per 1M Samples\n0\n500Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nDDP 4xT4 DDP 4xT4\nInstance Type\nSpot\nOn-Demand\nFigure 1: Cost to throughput tradeoff for ConvNextLarge at dif-\nferent instance types.", "metadata": {}}, {"text": "Our training setups (circled) are cheaper\n(8xT4) and faster (8xA10) than centralized offerings (DGX-2).", "metadata": {}}, {"text": "But what if we could use spot pricing for long-running, distributed\njobs and reduce bandwidth requirements to leverage multiple low-\ncost GPUs?", "metadata": {}}, {"text": "This could be possible through a framework for collabo-\nrative DL training, Hivemind [39], which inherently deals with peers\nthat can stop running at any time.", "metadata": {}}, {"text": "While there is research on how\nHivemind can be used for training on spot VMs [17, 37, 38], it does not\ncompare the cost-throughput tradeoff for different cloud offerings or\nperform ablation studies on geographic distribution or model sizes.", "metadata": {}}, {"text": "To motivate this new possibility, we trained the ConvNextLarge\nmodel [29] on the Imagenet1K dataset [15] on different Google Cloud\nhardware (T4’s and DGX-2), and on the very competitively priced\nA10 from LambdaLabs (see Section 6 for the full experimental de-\nscription).", "metadata": {}}, {"text": "Figure 1 shows the training throughput and the costs per\n1 million processed samples for each setup.", "metadata": {}}, {"text": "The single node (1xT4,\n1xA10, DGX-2) experiments show the current state-of-the-art cost-\nthroughput ratio for training on GC and LambdaLabs.", "metadata": {}}, {"text": "The DGX-2\nnode is the fastest, with a throughput of 413 SPS, but it also costs\n$6.30/h ($4.24/1M samples), shown by the horizontal and vertical\nlines.", "metadata": {}}, {"text": "The single-accelerator experiments (1xT4, 1xA10) have a better\ncost-throughput ratio ($0.62/1M samples and $0.9/1M samples), but\nhave a much lower throughput of 80 and 185 SPS, respectively.", "metadata": {}}, {"text": "How-\never, when using our approach of distributing the training between\nmultiple GPUs with Hivemind (circled), we make training possible\nthat is both faster (8xA10, 621 SPS, $2.15/1M samples) and cheaper\narXiv:2306.03163v4  [cs.LG]  2 Jun 2024", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "(8xT4, 262 SPS, $1.77/1M samples) than using the DGX-2. Every\ncloud provider deals differently with how they price spot instances\nand network traffic (cf. Section 1) and has varying interruption rates\nfor different accelerators [23]. Being able to choose the best option\nwas not possible before, and having the option to combine older,\nmore available GPUs is a net benefit for both consumers and cloud\nproviders alike.\nWe aim to develop guidelines and help practitioners assess under\nwhich conditions they can cost-efficiently speed up their training\ntasks with spot instances. To be able to do this, they need a precise\ndefinition of the model size at which geo-distributed spot training\nbecomes viable, what hardware can be used for it, and what the\nminimum bandwidth and latency are. We close this research gap by\nperforming a comprehensive analysis of multiple DL tasks from CV\nand NLP, breaking down how time is spent in each epoch, and com-\nparing them to non-distributed runs to quantify the advantages and\ndisadvantages of distributed spot training. We determine which mod-\nels scale with additional spot instances and which cannot be scaled\nwithout running into a communication bottleneck or resource ineffi-\nciencies. To quantify total training cost, we assess cost-effectiveness\nand evaluate a hybrid or multi-cloud approach with popular cloud\nproviders through training on up to four continents. For comparison\nof the models’ scalability and to show which of them can be trained\nin a distributed fashion, we introduce thegranularity metric, the ratio\nof calculation to communication time, and show how it can be used\nfor predicting performance with different hardware setups. Finally,\nwe summarize our lessons on how to design geo-distributed spot\ntraining and what to watch out for when evaluating the feasibility\nof such a training regime. Our contributions are:\n(1) We analyze the impact of multi-cloud training with\nspot and on-demand instances from Google Cloud\n(GC), Microsoft Azure, Amazon Web Services (A WS),\nand LambdaLabs on cost-efficiency.While we find perfor-\nmance penalties due to remote versus on-premise compute\nresources, the throughput still scales with increased comput-\ning power. By leveraging multiple spot instances with one\nT4 GPU each, we can be more cost-efficient than a DGX-2\nnode or the very competitively priced A10 offerings from\nLambdaLabs.\n(2) We investigate the suitability of geo-distributed train-\ning for various CV and NLP models and hardware con-\nfigurations on up to four continents.Not surprisingly,\nthe more parallelizable and the larger the task, the better the\nperformance. Moreover, we verify the scalability claims of\nthe related work and define additional constraints, such as\nthe minimum granularity for effective training. This enables,\nfor the first time, distributed training of smaller million-\nparameter models (12M-560M) over <1 Gb/s bandwidth and\n>150ms latency networks.\n(3) We evaluate two different hybrid-cloud experimental\nsetups with consumer- and server-grade on-premise\nhardware and try to improve the throughput with a band-\nwidth of, at worst, 50 Mb/s to the cloud resources. While we\nshow that it is possible to improve throughput even at these\nconstraints, local cloud offerings are better suited for models\nthat show limited suitability for distributed training.\n(4) We summarize our findings of training in a geo-distributed,\nmulti-cloud environment. We propose the granularity\nmetric to compare model suitability for distributed\nspot training and estimate training performance with ad-\nditional spot VMs. This provides guidance on the trade-off\nbetween performance and cost when using geo-distributed\nspot instances. To apply our findings, we perform a case-\nstudy on a state-of-the-art model from the ASR domain and\nachieve speedups on low-end hardware.\n2 DEEP LEARNING ON SPOT INSTANCES\nIn this section, we describe how the Hivemind framework works\nand how it can enable distributed spot training.\n2.1 Hivemind\nHivemind [39] is a PyTorch-based [32] framework developed initially\nto enable collaborative DL training where participants could donate\ntheir heterogeneous hardware to train a single model together in\na data-parallel fashion. Its main difference to other state-of-the-art\ndistributed training frameworks, such as PyTorch DDP [26] and\nDeepSpeed [35], is that it runs in a decentralized fashion and can\nhandle peers that drop out at any stage of the training. It does so with\ntwo features: a distributed hash table [31] (DHT) which spans over\nall participating peers for metadata storage, such as training progress\nand peer health, and a gradient averaging algorithm that is designed\nto reduce the impact of lost gradients. A key difference to other dis-\ntributed training frameworks is the definition of ahivemind epoch,\nwhich is the number of samples that must be aggregated before an\naveraging step is performed. This sample count is called thetarget\nbatch size (TBS), which corresponds to the minibatch size in standard\nDL training. The DHT is used for coordination, and shortly before\nthe TBS is predicted to be reached, the peers start to form the initial\ngroups for averaging. The time allocated for group forming is called\nmatchmaking time and typically runs asynchronously to the training\n(cf. Section 3). The individual peer gradients are accumulated locally\nand sent to the other peers via an adaptive all-reduce algorithm\n(MoshpitSGD [38]). The next hivemind epoch starts after each peer\napplies the accumulated gradients to the local model. The advantage\nof Hivemind for geo-distributed training comes from cumulating\ndifferent techniques, such as Delayed Parameter Updates [36], big-\nbatch training [44] and aggressive communication quantization [16].\nAll of these combined reduce time and frequency of the communica-\ntion rounds, which in turn makes training on heterogeneous devices\nand low-bandwidth networks possible.\n2.2 Distributed Spot Training\nIn this paper, we focus only on models that fit into the memory of\na single GPU, as we are interested in utilizing data parallelism on\ncheaper and more readily available hardware. However, our insights\nare applicable to larger models with techniques such as ZeRO of-\nfloading [36], more aggressive quantization [41] and even model\nparallelism [37]. The current options for data parallelism are either\nusing multiple GPUs on the same node (e.g., a DGX system with\neight GPUs) or having multiple nodes with a GPU each in the same\nhigh-bandwidth network (>25 Gb/s) to minimize communication\ntime. The latter does not work on cheap but interruptable instances,", "sentences": [{"text": "(8xT4, 262 SPS, $1.77/1M samples) than using the DGX-2.", "metadata": {}}, {"text": "Every\ncloud provider deals differently with how they price spot instances\nand network traffic (cf.", "metadata": {}}, {"text": "Section 1) and has varying interruption rates\nfor different accelerators [23].", "metadata": {}}, {"text": "Being able to choose the best option\nwas not possible before, and having the option to combine older,\nmore available GPUs is a net benefit for both consumers and cloud\nproviders alike.", "metadata": {}}, {"text": "We aim to develop guidelines and help practitioners assess under\nwhich conditions they can cost-efficiently speed up their training\ntasks with spot instances.", "metadata": {}}, {"text": "To be able to do this, they need a precise\ndefinition of the model size at which geo-distributed spot training\nbecomes viable, what hardware can be used for it, and what the\nminimum bandwidth and latency are.", "metadata": {}}, {"text": "We close this research gap by\nperforming a comprehensive analysis of multiple DL tasks from CV\nand NLP, breaking down how time is spent in each epoch, and com-\nparing them to non-distributed runs to quantify the advantages and\ndisadvantages of distributed spot training.", "metadata": {}}, {"text": "We determine which mod-\nels scale with additional spot instances and which cannot be scaled\nwithout running into a communication bottleneck or resource ineffi-\nciencies.", "metadata": {}}, {"text": "To quantify total training cost, we assess cost-effectiveness\nand evaluate a hybrid or multi-cloud approach with popular cloud\nproviders through training on up to four continents.", "metadata": {}}, {"text": "For comparison\nof the models’ scalability and to show which of them can be trained\nin a distributed fashion, we introduce thegranularity metric, the ratio\nof calculation to communication time, and show how it can be used\nfor predicting performance with different hardware setups.", "metadata": {}}, {"text": "Finally,\nwe summarize our lessons on how to design geo-distributed spot\ntraining and what to watch out for when evaluating the feasibility\nof such a training regime.", "metadata": {}}, {"text": "Our contributions are:\n(1) We analyze the impact of multi-cloud training with\nspot and on-demand instances from Google Cloud\n(GC), Microsoft Azure, Amazon Web Services (A WS),\nand LambdaLabs on cost-efficiency.While we find perfor-\nmance penalties due to remote versus on-premise compute\nresources, the throughput still scales with increased comput-\ning power.", "metadata": {}}, {"text": "By leveraging multiple spot instances with one\nT4 GPU each, we can be more cost-efficient than a DGX-2\nnode or the very competitively priced A10 offerings from\nLambdaLabs.", "metadata": {}}, {"text": "(2) We investigate the suitability of geo-distributed train-\ning for various CV and NLP models and hardware con-\nfigurations on up to four continents.Not surprisingly,\nthe more parallelizable and the larger the task, the better the\nperformance.", "metadata": {}}, {"text": "Moreover, we verify the scalability claims of\nthe related work and define additional constraints, such as\nthe minimum granularity for effective training.", "metadata": {}}, {"text": "This enables,\nfor the first time, distributed training of smaller million-\nparameter models (12M-560M) over <1 Gb/s bandwidth and\n>150ms latency networks.", "metadata": {}}, {"text": "(3) We evaluate two different hybrid-cloud experimental\nsetups with consumer- and server-grade on-premise\nhardware and try to improve the throughput with a band-\nwidth of, at worst, 50 Mb/s to the cloud resources.", "metadata": {}}, {"text": "While we\nshow that it is possible to improve throughput even at these\nconstraints, local cloud offerings are better suited for models\nthat show limited suitability for distributed training.", "metadata": {}}, {"text": "(4) We summarize our findings of training in a geo-distributed,\nmulti-cloud environment.", "metadata": {}}, {"text": "We propose the granularity\nmetric to compare model suitability for distributed\nspot training and estimate training performance with ad-\nditional spot VMs.", "metadata": {}}, {"text": "This provides guidance on the trade-off\nbetween performance and cost when using geo-distributed\nspot instances.", "metadata": {}}, {"text": "To apply our findings, we perform a case-\nstudy on a state-of-the-art model from the ASR domain and\nachieve speedups on low-end hardware.", "metadata": {}}, {"text": "2 DEEP LEARNING ON SPOT INSTANCES\nIn this section, we describe how the Hivemind framework works\nand how it can enable distributed spot training.", "metadata": {}}, {"text": "2.1 Hivemind\nHivemind [39] is a PyTorch-based [32] framework developed initially\nto enable collaborative DL training where participants could donate\ntheir heterogeneous hardware to train a single model together in\na data-parallel fashion.", "metadata": {}}, {"text": "Its main difference to other state-of-the-art\ndistributed training frameworks, such as PyTorch DDP [26] and\nDeepSpeed [35], is that it runs in a decentralized fashion and can\nhandle peers that drop out at any stage of the training.", "metadata": {}}, {"text": "It does so with\ntwo features: a distributed hash table [31] (DHT) which spans over\nall participating peers for metadata storage, such as training progress\nand peer health, and a gradient averaging algorithm that is designed\nto reduce the impact of lost gradients.", "metadata": {}}, {"text": "A key difference to other dis-\ntributed training frameworks is the definition of ahivemind epoch,\nwhich is the number of samples that must be aggregated before an\naveraging step is performed.", "metadata": {}}, {"text": "This sample count is called thetarget\nbatch size (TBS), which corresponds to the minibatch size in standard\nDL training.", "metadata": {}}, {"text": "The DHT is used for coordination, and shortly before\nthe TBS is predicted to be reached, the peers start to form the initial\ngroups for averaging.", "metadata": {}}, {"text": "The time allocated for group forming is called\nmatchmaking time and typically runs asynchronously to the training\n(cf.", "metadata": {}}, {"text": "Section 3).", "metadata": {}}, {"text": "The individual peer gradients are accumulated locally\nand sent to the other peers via an adaptive all-reduce algorithm\n(MoshpitSGD [38]).", "metadata": {}}, {"text": "The next hivemind epoch starts after each peer\napplies the accumulated gradients to the local model.", "metadata": {}}, {"text": "The advantage\nof Hivemind for geo-distributed training comes from cumulating\ndifferent techniques, such as Delayed Parameter Updates [36], big-\nbatch training [44] and aggressive communication quantization [16].", "metadata": {}}, {"text": "All of these combined reduce time and frequency of the communica-\ntion rounds, which in turn makes training on heterogeneous devices\nand low-bandwidth networks possible.", "metadata": {}}, {"text": "2.2 Distributed Spot Training\nIn this paper, we focus only on models that fit into the memory of\na single GPU, as we are interested in utilizing data parallelism on\ncheaper and more readily available hardware.", "metadata": {}}, {"text": "However, our insights\nare applicable to larger models with techniques such as ZeRO of-\nfloading [36], more aggressive quantization [41] and even model\nparallelism [37].", "metadata": {}}, {"text": "The current options for data parallelism are either\nusing multiple GPUs on the same node (e.g., a DGX system with\neight GPUs) or having multiple nodes with a GPU each in the same\nhigh-bandwidth network (>25 Gb/s) to minimize communication\ntime.", "metadata": {}}, {"text": "The latter does not work on cheap but interruptable instances,", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "while the former has some use in the form of Amazon Sagemaker but\nis limited to a single node and is typically very pricey (spot pricing for\nDGX-2 is $6.30/h versus 8xT4 at $0.72/h on GC). However, using Hive-\nmind, a new training scenario becomes feasible: Distributed training\nin a decentralized fashion on interruptable VMs with bandwidths of\n<1 Gb/s. Since spot instance prices change hourly depending on the\ntime of day and zone availability [23], and can vary widely between\ncloud providers (cf. Section 1), training between continents and in\nmultiple clouds could potentially be more cost-effective than using\na single, more computationally powerful node at spot prices.\nWith the newly added training setups from Figure 1 (circled), it\nwas not previously possible to choose the best option, and having\nthe option to combine older, more available GPUs is a net benefit for\nboth consumers as well as cloud providers. Our paper shows that\nit is possible to train on multiple clouds across multiple continents\nand provides guidelines on how to accomplish this cost-efficiently.\n3 MODEL SUITABILITY\nSelecting suitable models with a big enough parallel workload is\nessential to ensure successful distributed spot training. To cover\na wide range of established models, we drew from MLCommons’\ncomprehensive DL training benchmark [30]. We used models from\nthe CV and NLP domains and gradually increased their size and\nTBS to increase the parallel compute amount. As discussed in Sec-\ntion 2, the TBS may be exclusively responsible for the success of\ndistributed training and was chosen to cover both medium and large\nbatches (8K, 16K and 32K). These minibatch sizes start to become\nmore common due to the LAMB optimizer [44], which works well\nenough for both smaller (512) and huge batches (64K) and should be\nrepresentative of state-of-the-art workloads. For a represenatative\nexperimental study with a minibatch size of 256 on the automatic\nspeech recognition model (Whisper [34]), please refer to Section 11.\nAll experiments were run with FP16 precision, as the target T4 GPUs\nhave a considerable improvement in FLOPs compared to FP32 (8:1).\nFor CV, we take five models from the extended ResNet family,\nstarting with the smallest one, ResNet18 [ 21] (RN18), ResNet50\n(RN50), ResNet152 (RN152), WideResNet101_2 [46] (WRN101) and\nConvNextLarge [29] (CONV), which is almost 20 times larger than\nRN18. The paramter count is 11.7M, 25.6M, 60.2M, 126.9M, and\n197.8M, respectively. These models were popularized due to their\nability to help with the vanishing gradient problem by using resid-\nual connections between layers. Currently, they are not only used\nfor classification, but can serve as an embedding of images by re-\nmoving the classification head [18, 40]. For the dataset, we use Im-\nagenet1K [15] and train the classification task, which tries to assign\none of 1000 classes to each image.\nFor NLP, we selected three models from the BERT family:\nRoBERTaBase [28] (RBase), -Large (RLrg), and -XLM [13] (RXLM).\nThe parameter count is 124.7M, 355.4M, and 560.1M, respectively.\nWe used the same configuration as the original models and trained\nthem on masked language modeling, a common pre-training task.\nRoBERTa models were a replication study of BERT but with a focus\non better hyperparameter tuning, leading to state-of-the-art results\nand proposed using much higher minibatch sizes than previously\ncommon. The text dataset is March ’22 Wikipedia [20].\nWhen we run our experiments in a multi-cloud environment on\nspot instances, we cannot plug in proprietary cloud storage or wait\nfor the dataset to download, as the instances can be terminated any-\ntime. To simulate a real-world deployment with a non-public dataset,\nwe chose an independent S3 storage provider, Backblaze (B2) [4].\nBackblaze has replicated data centers that can better serve requests\nfrom anywhere worldwide, guaranteeing a reasonable ingress rate\nfrom every continent. Additionally, the cost is very manageable at\n$0.01/GB rate for egress and $0.005/GB/month for storage. A de-\ntailed analysis of the costs incurred for the experiments can be found\nin Section 5. We access the datasets on-demand via shards in thetar\nformat with the WebDataset library [10]. We chose WebDataset due\nto its features like automatic local caching, streaming decompression,\nstreaming preprocessing, and having an easy to work with archive\nformat that allows representing the data in its original format. Finally,\nfor the Hivemind parameterization, we enabled delayed parameter\naveraging (DPU) [36] to enable simultaneous gradient communi-\ncation and computation at the expense of a round of staleness. We\nselected FP16 compression for peer-to-peer communication.\nExperimental design. First, we must verify that our models are\nsuitable for cloud training. For this purpose, we evaluate them on\nthe powerful Ampere GPUs first - if they scale there without facing\na communication bottleneck, they should also scale on the slower\nT4, which is common at GC, AWS, and Azure. We use the LambdaL-\nabs [8] for these experiments, which gives us on-demand A10 GPUs\nfor just $0.60/hour, but currently offer their services only in the US\nWest region. All experiments are performed on the 515.65.01 driver,\nCUDA 11.6, and PyTorch 1.13.1. We profiled a network bandwidth\nof 3.3 Gb/s and a latency of 0.3 ms between the Lambda VMs.\nTo establish a fair baseline, we train all models from?? on a single\nGPU that achieves large minibatch sizes through gradient accumu-\nlation. Processes logs system metrics every second and evaluates\nthe training performance whenever a batch is processed. Finally,\nall multi-GPU experiments are monitored with a training monitor\nthat scrapes the DHT every second to log the peer state and training\nprogress synchronously.\n(1) Hivemind penalty. Using Hivemind as middleware to share\ngradients and keep a fully decentralized architecture running harms\nperformance compared to single-node training. We can compare\nthe effects of Hivemind training by looking at three metrics:base-\nline, the single GPU throughput, hivemind local, normalized GPU\nthroughput without the averaging step, and hivemind global, the\nactual normalized GPU throughput. When comparing the baseline\nand local speed in Figure 2 for a setup with two GPUs, running Hive-\nmind reaches at best 78% (RN152) and at worst 48% (CONV) of the\nbaseline performance. Unsurprisingly, the larger the model size, the\nworse the penalty gets due to the increased size of the accumulated\ngradients (GAC) over each step. However, the baseline also applies\ngradient accumulation to reach the target minibatch size without\nthe performance drop. After isolating the respective function calls,\nthere seems to be a slight inefficiency in how GAC is implemented in\nHivemind versus the native PyTorch call. We are working with the\nmaintainers to fix this issue [7]. On the other hand, the disadvantage\nof synchronization is minimal under the perfect conditions of a good\ninterconnect. The global speed in Figures 2a and 2b only degrades\nat best to 97% (CONV) to at worst to 87% (RBase) compared to the\nlocal throughput, meaning that the communication under these con-\nditions only accounts for a fraction of the total training time. This\ndegradation is inversely correlated to the model size due to larger", "sentences": [{"text": "while the former has some use in the form of Amazon Sagemaker but\nis limited to a single node and is typically very pricey (spot pricing for\nDGX-2 is $6.30/h versus 8xT4 at $0.72/h on GC).", "metadata": {}}, {"text": "However, using Hive-\nmind, a new training scenario becomes feasible: Distributed training\nin a decentralized fashion on interruptable VMs with bandwidths of\n<1 Gb/s.", "metadata": {}}, {"text": "Since spot instance prices change hourly depending on the\ntime of day and zone availability [23], and can vary widely between\ncloud providers (cf.", "metadata": {}}, {"text": "Section 1), training between continents and in\nmultiple clouds could potentially be more cost-effective than using\na single, more computationally powerful node at spot prices.", "metadata": {}}, {"text": "With the newly added training setups from Figure 1 (circled), it\nwas not previously possible to choose the best option, and having\nthe option to combine older, more available GPUs is a net benefit for\nboth consumers as well as cloud providers.", "metadata": {}}, {"text": "Our paper shows that\nit is possible to train on multiple clouds across multiple continents\nand provides guidelines on how to accomplish this cost-efficiently.", "metadata": {}}, {"text": "3 MODEL SUITABILITY\nSelecting suitable models with a big enough parallel workload is\nessential to ensure successful distributed spot training.", "metadata": {}}, {"text": "To cover\na wide range of established models, we drew from MLCommons’\ncomprehensive DL training benchmark [30].", "metadata": {}}, {"text": "We used models from\nthe CV and NLP domains and gradually increased their size and\nTBS to increase the parallel compute amount.", "metadata": {}}, {"text": "As discussed in Sec-\ntion 2, the TBS may be exclusively responsible for the success of\ndistributed training and was chosen to cover both medium and large\nbatches (8K, 16K and 32K).", "metadata": {}}, {"text": "These minibatch sizes start to become\nmore common due to the LAMB optimizer [44], which works well\nenough for both smaller (512) and huge batches (64K) and should be\nrepresentative of state-of-the-art workloads.", "metadata": {}}, {"text": "For a represenatative\nexperimental study with a minibatch size of 256 on the automatic\nspeech recognition model (Whisper [34]), please refer to Section 11.", "metadata": {}}, {"text": "All experiments were run with FP16 precision, as the target T4 GPUs\nhave a considerable improvement in FLOPs compared to FP32 (8:1).", "metadata": {}}, {"text": "For CV, we take five models from the extended ResNet family,\nstarting with the smallest one, ResNet18 [ 21] (RN18), ResNet50\n(RN50), ResNet152 (RN152), WideResNet101_2 [46] (WRN101) and\nConvNextLarge [29] (CONV), which is almost 20 times larger than\nRN18.", "metadata": {}}, {"text": "The paramter count is 11.7M, 25.6M, 60.2M, 126.9M, and\n197.8M, respectively.", "metadata": {}}, {"text": "These models were popularized due to their\nability to help with the vanishing gradient problem by using resid-\nual connections between layers.", "metadata": {}}, {"text": "Currently, they are not only used\nfor classification, but can serve as an embedding of images by re-\nmoving the classification head [18, 40].", "metadata": {}}, {"text": "For the dataset, we use Im-\nagenet1K [15] and train the classification task, which tries to assign\none of 1000 classes to each image.", "metadata": {}}, {"text": "For NLP, we selected three models from the BERT family:\nRoBERTaBase [28] (RBase), -Large (RLrg), and -XLM [13] (RXLM).", "metadata": {}}, {"text": "The parameter count is 124.7M, 355.4M, and 560.1M, respectively.", "metadata": {}}, {"text": "We used the same configuration as the original models and trained\nthem on masked language modeling, a common pre-training task.", "metadata": {}}, {"text": "RoBERTa models were a replication study of BERT but with a focus\non better hyperparameter tuning, leading to state-of-the-art results\nand proposed using much higher minibatch sizes than previously\ncommon.", "metadata": {}}, {"text": "The text dataset is March ’22 Wikipedia [20].", "metadata": {}}, {"text": "When we run our experiments in a multi-cloud environment on\nspot instances, we cannot plug in proprietary cloud storage or wait\nfor the dataset to download, as the instances can be terminated any-\ntime.", "metadata": {}}, {"text": "To simulate a real-world deployment with a non-public dataset,\nwe chose an independent S3 storage provider, Backblaze (B2) [4].", "metadata": {}}, {"text": "Backblaze has replicated data centers that can better serve requests\nfrom anywhere worldwide, guaranteeing a reasonable ingress rate\nfrom every continent.", "metadata": {}}, {"text": "Additionally, the cost is very manageable at\n$0.01/GB rate for egress and $0.005/GB/month for storage.", "metadata": {}}, {"text": "A de-\ntailed analysis of the costs incurred for the experiments can be found\nin Section 5.", "metadata": {}}, {"text": "We access the datasets on-demand via shards in thetar\nformat with the WebDataset library [10].", "metadata": {}}, {"text": "We chose WebDataset due\nto its features like automatic local caching, streaming decompression,\nstreaming preprocessing, and having an easy to work with archive\nformat that allows representing the data in its original format.", "metadata": {}}, {"text": "Finally,\nfor the Hivemind parameterization, we enabled delayed parameter\naveraging (DPU) [36] to enable simultaneous gradient communi-\ncation and computation at the expense of a round of staleness.", "metadata": {}}, {"text": "We\nselected FP16 compression for peer-to-peer communication.", "metadata": {}}, {"text": "Experimental design.", "metadata": {}}, {"text": "First, we must verify that our models are\nsuitable for cloud training.", "metadata": {}}, {"text": "For this purpose, we evaluate them on\nthe powerful Ampere GPUs first - if they scale there without facing\na communication bottleneck, they should also scale on the slower\nT4, which is common at GC, AWS, and Azure.", "metadata": {}}, {"text": "We use the LambdaL-\nabs [8] for these experiments, which gives us on-demand A10 GPUs\nfor just $0.60/hour, but currently offer their services only in the US\nWest region.", "metadata": {}}, {"text": "All experiments are performed on the 515.65.01 driver,\nCUDA 11.6, and PyTorch 1.13.1.", "metadata": {}}, {"text": "We profiled a network bandwidth\nof 3.3 Gb/s and a latency of 0.3 ms between the Lambda VMs.", "metadata": {}}, {"text": "To establish a fair baseline, we train all models from??", "metadata": {}}, {"text": "on a single\nGPU that achieves large minibatch sizes through gradient accumu-\nlation.", "metadata": {}}, {"text": "Processes logs system metrics every second and evaluates\nthe training performance whenever a batch is processed.", "metadata": {}}, {"text": "Finally,\nall multi-GPU experiments are monitored with a training monitor\nthat scrapes the DHT every second to log the peer state and training\nprogress synchronously.", "metadata": {}}, {"text": "(1) Hivemind penalty.", "metadata": {}}, {"text": "Using Hivemind as middleware to share\ngradients and keep a fully decentralized architecture running harms\nperformance compared to single-node training.", "metadata": {}}, {"text": "We can compare\nthe effects of Hivemind training by looking at three metrics:base-\nline, the single GPU throughput, hivemind local, normalized GPU\nthroughput without the averaging step, and hivemind global, the\nactual normalized GPU throughput.", "metadata": {}}, {"text": "When comparing the baseline\nand local speed in Figure 2 for a setup with two GPUs, running Hive-\nmind reaches at best 78% (RN152) and at worst 48% (CONV) of the\nbaseline performance.", "metadata": {}}, {"text": "Unsurprisingly, the larger the model size, the\nworse the penalty gets due to the increased size of the accumulated\ngradients (GAC) over each step.", "metadata": {}}, {"text": "However, the baseline also applies\ngradient accumulation to reach the target minibatch size without\nthe performance drop.", "metadata": {}}, {"text": "After isolating the respective function calls,\nthere seems to be a slight inefficiency in how GAC is implemented in\nHivemind versus the native PyTorch call.", "metadata": {}}, {"text": "We are working with the\nmaintainers to fix this issue [7].", "metadata": {}}, {"text": "On the other hand, the disadvantage\nof synchronization is minimal under the perfect conditions of a good\ninterconnect.", "metadata": {}}, {"text": "The global speed in Figures 2a and 2b only degrades\nat best to 97% (CONV) to at worst to 87% (RBase) compared to the\nlocal throughput, meaning that the communication under these con-\nditions only accounts for a fraction of the total training time.", "metadata": {}}, {"text": "This\ndegradation is inversely correlated to the model size due to larger", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "RN18 RN50 RN152 WRN101 CONV\n0\n500\n1000\n1500\n2000Samples per Second\n1462\n778\n297 423\n185\n1024\n529\n231 211\n89\n958\n499\n217 201 86\nThroughput Type\nbaseline\nhivemind local\nhivemind global\n(a) CV\nRBase RLrg RXLM\n0\n250\n500\n750\n1000\n1250\n1500Samples per Second\n1310\n661\n463\n804\n359\n247\n700\n313\n215\nThroughput Type\nbaseline\nhivemind local\nhivemind global (b) NLP\nFigure 2: Hivemind penalty on normalized throughputs.\n8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500\n2000Samples per Second\n1466 1462 1462\n768 771 778\n292 298 297\n431 436 423\n188 186 185\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500\n2000Samples per Second\n1204\n1954 1916\n864 964 998\n383 418 435\n363 390 403\n164 169 173\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV (b) CV 2xA10\n8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500Samples per Second\n1316 1307 1310\n659 660 661\n462 461 463\nModel\nRBase\nRLrg\nRXLM\n(c) NLP 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500Samples per Second\n680\n1257\n1401\n451\n551 626\n305 383 430\nModel\nRBase\nRLrg\nRXLM (d) NLP 2xA10\nFigure 3: Throughput comparison between single GPU base-\nlines and the Hivemind runs with two GPUs.\nmodels training quadratically longer per parameter, but the commu-\nnication only increases linearly [37]. While an implementation issue\ncurrently affects performance, and the worst total performance drop\nis at 47% (CONV baseline vs. global), scaling is still possible with a\nratio of roughly 2:1 of GPUs to throughput. We further refine this\nratio in the following section by comparing which models are most\nsuitable to be trained in a distributed environment.\n(2) Less suitable models for distributed spot training. While\ntraining billion-parameter NLP models scale well due to the \"square-\ncube\" law, the minimum model size is not yet fully defined [37]. The\nreason is that many factors play a role in whether a model is suited\nfor geo-distributed training. On the one hand, a small model results\nin small gradients exchanged between peers, so the averaging step is\nfast. On the other hand, a small model will also reach the TBS faster\nthan larger models, which may lead to a low speedup if the calculation\ntime is disproportionally lower than the communication time.\nWe found the granularity metric [ 22], typically used in high-\nperformance computing, practical to attach a comparable value to\neach setup to quantify the ratio of the calculation to communication\ntime. The higher the granularity, the more parallelizable the task,\nas more calculation can be distributed between peers, ensuring a\ngood speedup. It is important to note that this metric depends on\nthe model and the hardware being used. The communication time is\naffected by the parameter count, and the calculation time is affected\nby the layer type of the parameters (including feedforward, convolu-\ntion, and transformer). Therefore, the calculation time can decrease\nwith improved hardware, which we evaluate in Section 6. Another\nparameter that affects the calculation time is the TBS that all peers\nwork to accumulate. There is a practical limit to the TBS where a\nmodel is still trainable, which is currently at 64K with the LAMB\noptimizer [44]. This limits the possibility of improving the speedup\nof small models by increasing the batch size, meaning that at some\npoint, the speed will be limited by the communication time. It is\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRN18 RN50\nRN152 WRN101 CONV\n1.3 7.613.23.4 6.8\n11.53.5\n7.0\n14.0\n2.8\n5.6\n11.5\n5.3\n10.7\n21.6Calculation\nCommunication\n(a) CV\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg RXLM\n0.7 2.3 4.4 1.1 2.2\n4.4\n1.0\n2.1\n4.2\nCalculation\nCommunication (b) NLP\nFigure 4: TBS vs. total training time on 2xA10s. Granularity is\nshown above each bar. Dotted lines separate different models.\nimportant to remember that just increasing the TBS to create more\ncalculation time can have a grave effect on training performance if\nthe optimizer is not adequately selected and configured.\nOur experimental results in Figure 3 show the practical implica-\ntions of this observation. For the 2xGPU experiments in Figures 3b\nand 3d, we can see the effect of a TBS increase which improves the\ntotal throughput. Doubling the TBS equals cutting down the per-\nsample communication cost by two, which leads to the slight increase\nin performance visible in both CV and NLP experiments. However,\nthe smallest models, RN18 and RBase, fluctuate significantly at a TBS\nof 8K due to a minimum matchmaking time of 5 seconds. Whenever\nall peers accumulate the TBS in less than 5 seconds, the asynchronous\nthread that matches the peers in groups to perform the all-reduce\nmay still need to finish. This results in an unstable averaging time,\nwhich limits the scalability of small models with a small TBS.\nTo illustrate how the TBS and model size affect the individual\ntimings, we visualize the total training time split up into the calcu-\nlation and communication time in Figure 4. CV models are generally\ncomputationally more expensive and have a higher granularity than\nNLP models, which have slightly longer averaging rounds due to the\nmuch larger model sizes. When comparing the models at the same\nTBS (e.g., 32K), there is an inconclusive relation between runtime and\nparameter count. Some models increase their runtime with parame-\nter count w.r.t. smaller models (RN50 to RN152, RBase to RLrg), while\nothers decrease their runtime (RN152 to WRN101, RLrg to RXLM).\nThis performance is due to not all layer parameters contributing\nsimilarly to computational complexity. Depending on the specific\narchitecture, even models with more parameters can be faster to\ntrain due to a more efficient architecture, such as the WRN101 [46].\nThe communication time between different TBS sizes stays the\nsame, barring the two matchmaking time exceptions (RN18, RBase),\nas the gradients are accumulated before being sent. For all other\nmodels, doubling the TBS leads to exactly double the amount of\nwork and doubles the granularity. With a TBS of 32K, all models\nhave a granularity of at least 4.2 (RXLM) and at most 21.6 (CONV),\nwhich show strong scaling potential. Therefore, we decided to use\na TBS of 32K for all following experiments to ensure that the setup\nscales before introducing bandwidth and computational limitations.\nSummarizing, whether a model is scalable without network band-\nwidth limitations depends on the minimum time to reach the TBS and\non the granularity. Tuning the TBS is possible to a certain extent but\ndepends on the specific training task and optimizer configuration.\n(3) Per-GPU speedup decreases with low granularity.To eval-\nuate the scalability with additional hardware, we profile all models\non 2,3,4, and 8 GPUs with a TBS of 32K. Figure 5 shows the through-\nput for all models in the different hardware scenarios. Generally, all\nmodels scale well regardless of size, with the best speedup of 4.37x\n(RN152) and the lowest at 2.29x (RXLM) with 8 GPUs. There is a", "sentences": [{"text": "RN18 RN50 RN152 WRN101 CONV\n0\n500\n1000\n1500\n2000Samples per Second\n1462\n778\n297 423\n185\n1024\n529\n231 211\n89\n958\n499\n217 201 86\nThroughput Type\nbaseline\nhivemind local\nhivemind global\n(a) CV\nRBase RLrg RXLM\n0\n250\n500\n750\n1000\n1250\n1500Samples per Second\n1310\n661\n463\n804\n359\n247\n700\n313\n215\nThroughput Type\nbaseline\nhivemind local\nhivemind global (b) NLP\nFigure 2: Hivemind penalty on normalized throughputs.", "metadata": {}}, {"text": "8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500\n2000Samples per Second\n1466 1462 1462\n768 771 778\n292 298 297\n431 436 423\n188 186 185\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500\n2000Samples per Second\n1204\n1954 1916\n864 964 998\n383 418 435\n363 390 403\n164 169 173\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV (b) CV 2xA10\n8192 16384 32768\nMinibatch Size\n0\n500\n1000\n1500Samples per Second\n1316 1307 1310\n659 660 661\n462 461 463\nModel\nRBase\nRLrg\nRXLM\n(c) NLP 1xA10\n8192 16384 32768\nTBS\n0\n500\n1000\n1500Samples per Second\n680\n1257\n1401\n451\n551 626\n305 383 430\nModel\nRBase\nRLrg\nRXLM (d) NLP 2xA10\nFigure 3: Throughput comparison between single GPU base-\nlines and the Hivemind runs with two GPUs.", "metadata": {}}, {"text": "models training quadratically longer per parameter, but the commu-\nnication only increases linearly [37].", "metadata": {}}, {"text": "While an implementation issue\ncurrently affects performance, and the worst total performance drop\nis at 47% (CONV baseline vs.", "metadata": {}}, {"text": "global), scaling is still possible with a\nratio of roughly 2:1 of GPUs to throughput.", "metadata": {}}, {"text": "We further refine this\nratio in the following section by comparing which models are most\nsuitable to be trained in a distributed environment.", "metadata": {}}, {"text": "(2) Less suitable models for distributed spot training.", "metadata": {}}, {"text": "While\ntraining billion-parameter NLP models scale well due to the \"square-\ncube\" law, the minimum model size is not yet fully defined [37].", "metadata": {}}, {"text": "The\nreason is that many factors play a role in whether a model is suited\nfor geo-distributed training.", "metadata": {}}, {"text": "On the one hand, a small model results\nin small gradients exchanged between peers, so the averaging step is\nfast.", "metadata": {}}, {"text": "On the other hand, a small model will also reach the TBS faster\nthan larger models, which may lead to a low speedup if the calculation\ntime is disproportionally lower than the communication time.", "metadata": {}}, {"text": "We found the granularity metric [ 22], typically used in high-\nperformance computing, practical to attach a comparable value to\neach setup to quantify the ratio of the calculation to communication\ntime.", "metadata": {}}, {"text": "The higher the granularity, the more parallelizable the task,\nas more calculation can be distributed between peers, ensuring a\ngood speedup.", "metadata": {}}, {"text": "It is important to note that this metric depends on\nthe model and the hardware being used.", "metadata": {}}, {"text": "The communication time is\naffected by the parameter count, and the calculation time is affected\nby the layer type of the parameters (including feedforward, convolu-\ntion, and transformer).", "metadata": {}}, {"text": "Therefore, the calculation time can decrease\nwith improved hardware, which we evaluate in Section 6.", "metadata": {}}, {"text": "Another\nparameter that affects the calculation time is the TBS that all peers\nwork to accumulate.", "metadata": {}}, {"text": "There is a practical limit to the TBS where a\nmodel is still trainable, which is currently at 64K with the LAMB\noptimizer [44].", "metadata": {}}, {"text": "This limits the possibility of improving the speedup\nof small models by increasing the batch size, meaning that at some\npoint, the speed will be limited by the communication time.", "metadata": {}}, {"text": "It is\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRN18 RN50\nRN152 WRN101 CONV\n1.3 7.613.23.4 6.8\n11.53.5\n7.0\n14.0\n2.8\n5.6\n11.5\n5.3\n10.7\n21.6Calculation\nCommunication\n(a) CV\n8192\n16384\n32768\n8192\n16384\n32768\n8192\n16384\n32768\nTBS\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg RXLM\n0.7 2.3 4.4 1.1 2.2\n4.4\n1.0\n2.1\n4.2\nCalculation\nCommunication (b) NLP\nFigure 4: TBS vs.", "metadata": {}}, {"text": "total training time on 2xA10s.", "metadata": {}}, {"text": "Granularity is\nshown above each bar.", "metadata": {}}, {"text": "Dotted lines separate different models.", "metadata": {}}, {"text": "important to remember that just increasing the TBS to create more\ncalculation time can have a grave effect on training performance if\nthe optimizer is not adequately selected and configured.", "metadata": {}}, {"text": "Our experimental results in Figure 3 show the practical implica-\ntions of this observation.", "metadata": {}}, {"text": "For the 2xGPU experiments in Figures 3b\nand 3d, we can see the effect of a TBS increase which improves the\ntotal throughput.", "metadata": {}}, {"text": "Doubling the TBS equals cutting down the per-\nsample communication cost by two, which leads to the slight increase\nin performance visible in both CV and NLP experiments.", "metadata": {}}, {"text": "However,\nthe smallest models, RN18 and RBase, fluctuate significantly at a TBS\nof 8K due to a minimum matchmaking time of 5 seconds.", "metadata": {}}, {"text": "Whenever\nall peers accumulate the TBS in less than 5 seconds, the asynchronous\nthread that matches the peers in groups to perform the all-reduce\nmay still need to finish.", "metadata": {}}, {"text": "This results in an unstable averaging time,\nwhich limits the scalability of small models with a small TBS.", "metadata": {}}, {"text": "To illustrate how the TBS and model size affect the individual\ntimings, we visualize the total training time split up into the calcu-\nlation and communication time in Figure 4.", "metadata": {}}, {"text": "CV models are generally\ncomputationally more expensive and have a higher granularity than\nNLP models, which have slightly longer averaging rounds due to the\nmuch larger model sizes.", "metadata": {}}, {"text": "When comparing the models at the same\nTBS (e.g., 32K), there is an inconclusive relation between runtime and\nparameter count.", "metadata": {}}, {"text": "Some models increase their runtime with parame-\nter count w.r.t.", "metadata": {}}, {"text": "smaller models (RN50 to RN152, RBase to RLrg), while\nothers decrease their runtime (RN152 to WRN101, RLrg to RXLM).", "metadata": {}}, {"text": "This performance is due to not all layer parameters contributing\nsimilarly to computational complexity.", "metadata": {}}, {"text": "Depending on the specific\narchitecture, even models with more parameters can be faster to\ntrain due to a more efficient architecture, such as the WRN101 [46].", "metadata": {}}, {"text": "The communication time between different TBS sizes stays the\nsame, barring the two matchmaking time exceptions (RN18, RBase),\nas the gradients are accumulated before being sent.", "metadata": {}}, {"text": "For all other\nmodels, doubling the TBS leads to exactly double the amount of\nwork and doubles the granularity.", "metadata": {}}, {"text": "With a TBS of 32K, all models\nhave a granularity of at least 4.2 (RXLM) and at most 21.6 (CONV),\nwhich show strong scaling potential.", "metadata": {}}, {"text": "Therefore, we decided to use\na TBS of 32K for all following experiments to ensure that the setup\nscales before introducing bandwidth and computational limitations.", "metadata": {}}, {"text": "Summarizing, whether a model is scalable without network band-\nwidth limitations depends on the minimum time to reach the TBS and\non the granularity.", "metadata": {}}, {"text": "Tuning the TBS is possible to a certain extent but\ndepends on the specific training task and optimizer configuration.", "metadata": {}}, {"text": "(3) Per-GPU speedup decreases with low granularity.To eval-\nuate the scalability with additional hardware, we profile all models\non 2,3,4, and 8 GPUs with a TBS of 32K.", "metadata": {}}, {"text": "Figure 5 shows the through-\nput for all models in the different hardware scenarios.", "metadata": {}}, {"text": "Generally, all\nmodels scale well regardless of size, with the best speedup of 4.37x\n(RN152) and the lowest at 2.29x (RXLM) with 8 GPUs.", "metadata": {}}, {"text": "There is a", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n0\n1000\n2000\n3000\n4000Samples per Second\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n1000\n2000\n3000Samples per Second\nModel\nRBase\nRLrg\nRXLM (b) NLP\nFigure 5: Throughput comparison from 1 to 8 A10 GPUs.\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\n13.22.84.61.0\n11.56.84.41.5\n14.0\n7.65.0\n1.8\n11.5\n6.34.21.5\n21.6\n11.9\n8.4\n3.2\nRN18 RN50\nRN152 WRN101\nCONV\nCalculation\nCommunication\n(a) CV\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg\nRXLM\n4.4 2.7 1.8 0.9\n4.4\n2.6 1.8 0.7\n4.2\n2.4 1.7\n0.7\nCalculation\nCommunication (b) NLP\nFigure 6: Multi-GPU scalability at 32K TBS. Granularity is\nshown above each bar. Dotted lines separate different models.\nTable 2: Geo-distributed experiments on GC with T4 VMs.\nExp. NameResources Total\nA-{1,2,3,4,6,8}{1, 2, 3, 4, 6, 8}xUS 1,2,3,4,6,8\nB-{2,4,6,8}{1, 2, 3, 4}xUS + {1, 2, 3, 4}xEU 2,4,6,8\nC-{3,6}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA 3,6\nC-{4,8}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA + {1, 2}xAUS4,8\nvisible trend in the per-GPU contribution to the speedup (speedup\n#GPUs ).\nThe more GPUs we add, the lower the contribution, e.g., RN18 goes\nfrom 0.7 to 0.4 with two to eight GPUs, respectively. This decrease is\nlikely to continue due to a granularity of 1.0 at 8 GPUs (Figure 6a), as\ndoubling the GPUs would, at best, increase the throughput by 33% by\nhalving the calculation time. However, the more computationally ex-\npensive the models are, the slower the per-GPU contribution falls off\nand the larger the granularity is (RN152, CONV). This does not hold\ntrue for our NLP models (Figure 6b); while they have increasingly\nmore model parameters, the only difference between the two biggest\nmodels, RLrg and RXLM, is the vocabulary size increase of 50K to\n250K. Due to how embedding layers are lookups, the forward pass is\nnot affected by the increased embedding size, but the backward pass\nis. This results in a smaller increase of the calculation time while\ncommunication increases linearly with the number of parameters.\nAdditionally, we see the drop in throughput when comparing\nthe single GPU and dual GPU experiments for most larger models\n(Figure 5), which stems from observation(1) of the Hivemind penalty.\nWe also observe that with each subsequent doubling of GPUs,\nthe calculation time is halved, while the communication increases\nsub-linearly due to the more efficient group-based all-reduce of\nMoshpitSGD [38]. For example, the averaging step for the RXLM on\n2xA10 takes 5 seconds per GPU (10s total), while the 8xA10 averaging\nstep takes 1.8 seconds per GPU (14.4s total).\nIn summary, all models show a speedup but have a decreasing\nper-GPU contribution due to smaller granularity with more GPUs.\nTherefore, the larger the model and TBS, the greater the scaling\npotential. High granularity is a good indicator of scalability, and\nsince the communication time only increases linearly with addi-\ntional peers (cf. Section 2.1), knowing the initial calculation time is\na good indicator of future throughput. Under the optimal conditions\nof good compute performance and an interconnect with relatively\nhigh bandwidth, scaling was not a problem. But what happens under\nless favorable conditions in geo-distributed settings?\n4 GEO-DISTRIBUTED PERFORMANCE\nAs spot prices for the same hardware differ depending on the re-\ngion, zone, and time of day [23], it might be a good idea to use VMs\nacross different data centers. However, is the connectivity between\nregions and continents good enough to enable distributed deep learn-\ning? To explore this question, we decided to conduct three types of\nexperiments (Table 2):\n(A) Intra-zone Can we scale if the VMs are co-located in the same\nzone (us-central-1)?\n(B) Transatlantic Can we scale when we combine VMs from two\nregions (US and EU), and what happens when the compute\nis unevenly distributed across regions?\n(C) Intercontinental Can we scale if we combine VMs from four\ncontinents (US, EU, ASIA, AUS)?\nExperimental design. Based on the insights from Section 3, we\ndecided to use the largest models (CONV, RXLM) for all further cloud\nexperiments in Sections 4 to 6, with the TBS of 32K as a baseline with\ngood scaling properties. We abbreviate them with their respective\ndomain names (CV, NLP). We used Google Cloud [5] for all exper-\niments in this section, as they were the first to give us access to\nall necessary zones. The default networking solution in GC is the\n\"Premium Tier\", which tries to use a Google-owned network instead\nof the public internet. We measured the throughput and latency\nbetween all zones via iperf and ping and report the average of 5\nconsecutive runs in Table 3. Unsurprisingly, the diagonal shows that\nthe local connectivity between zones runs at almost 7 Gb/s with a\nlatency of 0.7ms, probably due to the hypervisors being in the same\ndata center. While the up- and download were perfectly symmetrical\nin all setups, the throughput dropped to <210 Mb/s for all non-local\nconnections. The US-based data center is located in Iowa and is best\nconnected with at least 120 Mb/s to the remaining regions, namely\nBelgium in the EU (6,911km), Taiwan in ASIA (11,853km), and Syd-\nney in Australia (AUS, 14,555km), presumably due to the physical\ndistance. The lowest bandwidth and highest latency connections are\nbetween the EU region and ASIA and AUS, reaching around 80 Mb/s\nand 270ms. We decided to use the n1-standard-8 template with\neight cores, 30 GB memory, and a T4 GPU, as the smaller image with\n15 GB was insufficient to meet the memory requirements for gradient\napplication on the CPU with the biggest models. The experiment\nnaming in this section is prefixed with the type of location(A), (B)\nor (C) and the number of VMs, e.g., A-4 is the intra-zone experiment\nwith 4 VMs. The full experimental description is specified in Table 2.\nTable 3: Throughput and latency between GC zones.\n(a) Single stream TCP throughput in Gb/s.\nFrom To US EU ASIA AUS\nUS 6.900.21 0.13 0.12EU 0.216.81 0.08 0.07ASIA 0.130.08 6.79 0.16AUS 0.120.07 0.16 6.84\n(b) ICMP latency in ms.\nFrom To US EU ASIA AUS\nUS 0.66103.11157.09176.19EU 103.14 0.65253.10271.98ASIA 157.08253.09 0.72131.45AUS 175.98272.08131.42 0.64\n(A) Intra-zone scalability.Figure 7 shows the result of the intra-\nzone experiments, which we used as a baseline to compare geo-\ndistributed deployments to. As the scalability of the CV and NLP mod-\nels was already shown with much better hardware and slightly worse\nnetwork connectivity (cf. Section 3), the scalability with the T4 GPUs\nis not too surprising. We do not see an improvement in throughput\nfor two GPUs for either model due to the Hivemind penalty discussed", "sentences": [{"text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n0\n1000\n2000\n3000\n4000Samples per Second\nModel\nRN18\nRN50\nRN152\nWRN101\nCONV\n(a) CV\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nA10 Count\n1000\n2000\n3000Samples per Second\nModel\nRBase\nRLrg\nRXLM (b) NLP\nFigure 5: Throughput comparison from 1 to 8 A10 GPUs.", "metadata": {}}, {"text": "2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\n13.22.84.61.0\n11.56.84.41.5\n14.0\n7.65.0\n1.8\n11.5\n6.34.21.5\n21.6\n11.9\n8.4\n3.2\nRN18 RN50\nRN152 WRN101\nCONV\nCalculation\nCommunication\n(a) CV\n2\n3\n4\n8\n2\n3\n4\n8\n2\n3\n4\n8\nA10 Count\n0\n100\n200\n300\n400Time in Seconds\nRBse RLrg\nRXLM\n4.4 2.7 1.8 0.9\n4.4\n2.6 1.8 0.7\n4.2\n2.4 1.7\n0.7\nCalculation\nCommunication (b) NLP\nFigure 6: Multi-GPU scalability at 32K TBS.", "metadata": {}}, {"text": "Granularity is\nshown above each bar.", "metadata": {}}, {"text": "Dotted lines separate different models.", "metadata": {}}, {"text": "Table 2: Geo-distributed experiments on GC with T4 VMs.", "metadata": {}}, {"text": "Exp.", "metadata": {}}, {"text": "NameResources Total\nA-{1,2,3,4,6,8}{1, 2, 3, 4, 6, 8}xUS 1,2,3,4,6,8\nB-{2,4,6,8}{1, 2, 3, 4}xUS + {1, 2, 3, 4}xEU 2,4,6,8\nC-{3,6}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA 3,6\nC-{4,8}{1, 2}xUS + {1, 2}xEU + {1, 2}xASIA + {1, 2}xAUS4,8\nvisible trend in the per-GPU contribution to the speedup (speedup\n#GPUs ).", "metadata": {}}, {"text": "The more GPUs we add, the lower the contribution, e.g., RN18 goes\nfrom 0.7 to 0.4 with two to eight GPUs, respectively.", "metadata": {}}, {"text": "This decrease is\nlikely to continue due to a granularity of 1.0 at 8 GPUs (Figure 6a), as\ndoubling the GPUs would, at best, increase the throughput by 33% by\nhalving the calculation time.", "metadata": {}}, {"text": "However, the more computationally ex-\npensive the models are, the slower the per-GPU contribution falls off\nand the larger the granularity is (RN152, CONV).", "metadata": {}}, {"text": "This does not hold\ntrue for our NLP models (Figure 6b);", "metadata": {}}, {"text": "while they have increasingly\nmore model parameters, the only difference between the two biggest\nmodels, RLrg and RXLM, is the vocabulary size increase of 50K to\n250K.", "metadata": {}}, {"text": "Due to how embedding layers are lookups, the forward pass is\nnot affected by the increased embedding size, but the backward pass\nis.", "metadata": {}}, {"text": "This results in a smaller increase of the calculation time while\ncommunication increases linearly with the number of parameters.", "metadata": {}}, {"text": "Additionally, we see the drop in throughput when comparing\nthe single GPU and dual GPU experiments for most larger models\n(Figure 5), which stems from observation(1) of the Hivemind penalty.", "metadata": {}}, {"text": "We also observe that with each subsequent doubling of GPUs,\nthe calculation time is halved, while the communication increases\nsub-linearly due to the more efficient group-based all-reduce of\nMoshpitSGD [38].", "metadata": {}}, {"text": "For example, the averaging step for the RXLM on\n2xA10 takes 5 seconds per GPU (10s total), while the 8xA10 averaging\nstep takes 1.8 seconds per GPU (14.4s total).", "metadata": {}}, {"text": "In summary, all models show a speedup but have a decreasing\nper-GPU contribution due to smaller granularity with more GPUs.", "metadata": {}}, {"text": "Therefore, the larger the model and TBS, the greater the scaling\npotential.", "metadata": {}}, {"text": "High granularity is a good indicator of scalability, and\nsince the communication time only increases linearly with addi-\ntional peers (cf.", "metadata": {}}, {"text": "Section 2.1), knowing the initial calculation time is\na good indicator of future throughput.", "metadata": {}}, {"text": "Under the optimal conditions\nof good compute performance and an interconnect with relatively\nhigh bandwidth, scaling was not a problem.", "metadata": {}}, {"text": "But what happens under\nless favorable conditions in geo-distributed settings?", "metadata": {}}, {"text": "4 GEO-DISTRIBUTED PERFORMANCE\nAs spot prices for the same hardware differ depending on the re-\ngion, zone, and time of day [23], it might be a good idea to use VMs\nacross different data centers.", "metadata": {}}, {"text": "However, is the connectivity between\nregions and continents good enough to enable distributed deep learn-\ning?", "metadata": {}}, {"text": "To explore this question, we decided to conduct three types of\nexperiments (Table 2):\n(A) Intra-zone Can we scale if the VMs are co-located in the same\nzone (us-central-1)?", "metadata": {}}, {"text": "(B) Transatlantic Can we scale when we combine VMs from two\nregions (US and EU), and what happens when the compute\nis unevenly distributed across regions?", "metadata": {}}, {"text": "(C) Intercontinental Can we scale if we combine VMs from four\ncontinents (US, EU, ASIA, AUS)?", "metadata": {}}, {"text": "Experimental design.", "metadata": {}}, {"text": "Based on the insights from Section 3, we\ndecided to use the largest models (CONV, RXLM) for all further cloud\nexperiments in Sections 4 to 6, with the TBS of 32K as a baseline with\ngood scaling properties.", "metadata": {}}, {"text": "We abbreviate them with their respective\ndomain names (CV, NLP).", "metadata": {}}, {"text": "We used Google Cloud [5] for all exper-\niments in this section, as they were the first to give us access to\nall necessary zones.", "metadata": {}}, {"text": "The default networking solution in GC is the\n\"Premium Tier\", which tries to use a Google-owned network instead\nof the public internet.", "metadata": {}}, {"text": "We measured the throughput and latency\nbetween all zones via iperf and ping and report the average of 5\nconsecutive runs in Table 3.", "metadata": {}}, {"text": "Unsurprisingly, the diagonal shows that\nthe local connectivity between zones runs at almost 7 Gb/s with a\nlatency of 0.7ms, probably due to the hypervisors being in the same\ndata center.", "metadata": {}}, {"text": "While the up- and download were perfectly symmetrical\nin all setups, the throughput dropped to <210 Mb/s for all non-local\nconnections.", "metadata": {}}, {"text": "The US-based data center is located in Iowa and is best\nconnected with at least 120 Mb/s to the remaining regions, namely\nBelgium in the EU (6,911km), Taiwan in ASIA (11,853km), and Syd-\nney in Australia (AUS, 14,555km), presumably due to the physical\ndistance.", "metadata": {}}, {"text": "The lowest bandwidth and highest latency connections are\nbetween the EU region and ASIA and AUS, reaching around 80 Mb/s\nand 270ms.", "metadata": {}}, {"text": "We decided to use the n1-standard-8 template with\neight cores, 30 GB memory, and a T4 GPU, as the smaller image with\n15 GB was insufficient to meet the memory requirements for gradient\napplication on the CPU with the biggest models.", "metadata": {}}, {"text": "The experiment\nnaming in this section is prefixed with the type of location(A), (B)\nor (C) and the number of VMs, e.g., A-4 is the intra-zone experiment\nwith 4 VMs.", "metadata": {}}, {"text": "The full experimental description is specified in Table 2.", "metadata": {}}, {"text": "Table 3: Throughput and latency between GC zones.", "metadata": {}}, {"text": "(a) Single stream TCP throughput in Gb/s.", "metadata": {}}, {"text": "From To US EU ASIA AUS\nUS 6.900.21 0.13 0.12EU 0.216.81 0.08 0.07ASIA 0.130.08 6.79 0.16AUS 0.120.07 0.16 6.84\n(b) ICMP latency in ms.", "metadata": {}}, {"text": "From To US EU ASIA AUS\nUS 0.66103.11157.09176.19EU 103.14 0.65253.10271.98ASIA 157.08253.09 0.72131.45AUS 175.98272.08131.42 0.64\n(A) Intra-zone scalability.Figure 7 shows the result of the intra-\nzone experiments, which we used as a baseline to compare geo-\ndistributed deployments to.", "metadata": {}}, {"text": "As the scalability of the CV and NLP mod-\nels was already shown with much better hardware and slightly worse\nnetwork connectivity (cf.", "metadata": {}}, {"text": "Section 3), the scalability with the T4 GPUs\nis not too surprising.", "metadata": {}}, {"text": "We do not see an improvement in throughput\nfor two GPUs for either model due to the Hivemind penalty discussed", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV A-3\nCV A-4\nCV A-6\nCV A-8\nNLP A-2\nNLP A-3\nNLP A-4\nNLP A-6\nNLP A-8\n0\n200\n400\n600Time in Seconds\n39.73\n21.23\n14.48\n8.31\n5.39\n6.86\n3.89 2.73 1.66 1.15\nCalculation\nCommunication (b) Granularity\nFigure 7: (A) Intra-zone performance for CV and NLP.\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV B-2\nCV A-4\nCV B-4\nCV A-6\nCV B-6\nCV A-8\nCV B-8\nNLP A-2\nNLP B-2\nNLP A-4\nNLP B-4\nNLP A-6\nNLP B-6\nNLP A-8\nNLP B-8\n0\n200\n400\n600Time in Seconds\n39.714.4\n14.59.3\n8.3 6.6\n5.4 4.9\n6.9\n2.1\n2.7 1.4\n1.7 1.0 1.1 0.6\nCalculation\nCommunication (b) Granularity\nFigure 8: (B) Transatlantic performance for CV and NLP.\nin Section 3. However, starting with three GPUs, we see an increase\nin throughput with a maximum speedup of up to 3.2x CV and 2.75x\nfor NLP at eight GPUs. CV’s per-GPU speedup (speedup\n#GPUs ) is almost\nlinear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster\n(0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively. The\nreason for this is the NLP granularity of 1.15 with 8 GPUs indicating\nan almost equal part in communication and calculation (Figure 7b)\ndue to the much longer averaging round related to the model size\n(198M vs. 560M parameters). The peak network bandwidth utiliza-\ntion between peers was at most a symmetric 1.1 Gb/s while averaging\nand 33 Mb/s ingress while training due to data loading. This means\nthat the network bandwidth of 7 Gb/s was not a limiting factor.\n(B) Transatlantic scalability.We scale when computing hard-\nware is local. However, what happens when there is cheap capacity in\nanother region? In this case, we study the throughput of experiments\nwith resources in theus-west and eu-central regions (B-2,4,6,8).\nThe B-2 experiment has one VM in the US and one in the EU,\nachieving a virtually identical throughput of 68.4 (US-EU) versus 70.1\n(US) at CV (Figure 8a). Our maximum peak egress rate of 250 Mb/s\ndoes not affect the CV experiments, while the US experiments peaked\nat 1.1 Gb/s. The reduction in bandwidth penalizes NLP harder, where\nwe are 16% slower with 177.3 SPS (US-EU) compared to the intra-zone\nexperiment with 211.4 SPS (US). The resulting increased communica-\ntion can be easily seen in the granularity analysis in Figure 8b (NLP\nA-2,4,6,8 vs. B-2,4,6,8). As only communication time increases in the\nNLP (B) experiments compared to(A), a granularity of≫ 1 indicates\ngood scalability: Adding two more GPUs to the B-6 experiment with\na granularity of 1.03 results in a throughput increase of 15% (B-8)\nrelative to the baseline. Meanwhile, adding two more GPUs to the\nB-2 experiment with a granularity of 2.21 results in a throughput\nincrease of 77% (B-4) relative to the baseline.\nIn the B-4 experiment, we look at what happens when we increase\nthe number of VMs to four, with two in the US and two in the EU.\nNothing surprising happens with CV, as the workload continues to\nbe mostly computation, with a throughput of 135.8 (B-4), only 3%\nslower than the intra-zone experiment with 140.4 SPS (A-4). How-\never, at NLP, things get more interesting as we now have more overall\ncommunication with four peers, but they can average locally first\nand only later transmit across the Atlantic. However, compared to\ntheir A-counterparts, we do not see a difference in relative scalability\nwith either B-4, B-6, or B-8. This means that training across regions\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-3\nCV C-3\nCV A-4\nCV C-4\nCV A-6\nCV C-6\nCV A-8\nCV C-8\nNLP A-3\nNLP C-3\nNLP A-4\nNLP C-4\nNLP A-6\nNLP C-6\nNLP A-8\nNLP C-8\n0\n200\n400\n600Time in Seconds\n21.25.9\n14.5\n4.7\n8.3 4.1\n5.4 3.3 3.9\n0.8\n2.7\n0.7\n1.7\n0.6\n1.1\n0.4\nCalculation\nCommunication (b) Granularity\nFigure 9: (C) Intercontinental performance for CV and NLP.\n(B) is slower, but the contribution per GPU decreases at the same\nrate as in training within a zone(A). The per-GPU speedup with ad-\nditional hardware reduces at the same rate for either setup (between\n0.05 and 0.06). This results in two observations: First, communica-\ntion overhead scales linearly with the number of peers. Second, we\nonly have to pay the penalty for transatlantic training once. How-\never, we cannot expect a significant improvement in communication\nefficiency when we increase the amount of available local resources.\nSummarizing, with an transatlantic setup, CV achieves a virtually\nidentical maximum speedup of 3.2x with 8 GPUs compared to A-1\n(B-8 is 2% slower than A-8), while NLP is more affected by lower\nnetwork bandwidth and only achieves a speedup of 2.15x (B-8 is\n22% slower than A-8). The transatlantic training penalty is applied\nonce; however, it does not affect the relative scaling with additional\ncompute resources.\n(C) Intercontinental scalability.To take geo-distribution to the\nextreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS,\nto see how much worse bandwidth affects the training throughput\n(C-3,4,6,8 in Table 2).\nHow does the intercontinental penalty investigated in(B) affect\ndeployments with a single GPU on each continent? Comparing the\nA-3 and C-3 experiments with three local versus three fully remote\nGPUs, CV is only 5% slower, while NLP suffers a 34% drop in through-\nput (Figure 9a) and does not even reach the baseline single GPU\nperformance (A-1). The peak egress for each region was 318, 258,\nand 237 Mb/s for the US, EU, and ASIA, respectively. Since our band-\nwidth measurements were 210 and 130 Mb/s from the US to the EU\nand ASIA, respectively (Table 3), this suggests that the averaging\nwas done over the US node and not an N-to-N all-reduce (a detailed\nanalysis of how averaging affects bandwidths is discussed in Sec-\ntion 6). Thus, the limiting factor was the US-ASIA connection at\n130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend\ncontinues with the C-4 run, which adds AUS as a continent with one\nadditional VM. As we know from the transatlantic experiments(B)\nthat an additional continent has a detrimental effect on throughput,\nwhich, for the four continents experiment, C-4, results in a 9% slower\nthroughput for CV and 36% slower for NLP compared to the A-4 runs\n(Figure 7a). Again, the US VM is used as an averaging intermediary\nwith a peak egress of 365 Mb/s, while the other continents are be-\ntween 318 and 330 Mb/s. When comparing the two continents (B-4)\nversus four continents (C-4) experiments, one GPU on each continent\n(C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs\non two continents (B-4). This reinforces that local hardware should\nbe preferred whenever possible. However, we are always faster than\nthe baseline (A-1), starting from 4 GPUs in both the transatlantic and\nintercontinental settings. While these experiments were specifically\ndesigned to be a worst-case scenario, what about a more balanced\nGPU distribution with at least two GPUs in each region?", "sentences": [{"text": "0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV A-3\nCV A-4\nCV A-6\nCV A-8\nNLP A-2\nNLP A-3\nNLP A-4\nNLP A-6\nNLP A-8\n0\n200\n400\n600Time in Seconds\n39.73\n21.23\n14.48\n8.31\n5.39\n6.86\n3.89 2.73 1.66 1.15\nCalculation\nCommunication (b) Granularity\nFigure 7: (A) Intra-zone performance for CV and NLP.", "metadata": {}}, {"text": "0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-2\nCV B-2\nCV A-4\nCV B-4\nCV A-6\nCV B-6\nCV A-8\nCV B-8\nNLP A-2\nNLP B-2\nNLP A-4\nNLP B-4\nNLP A-6\nNLP B-6\nNLP A-8\nNLP B-8\n0\n200\n400\n600Time in Seconds\n39.714.4\n14.59.3\n8.3 6.6\n5.4 4.9\n6.9\n2.1\n2.7 1.4\n1.7 1.0 1.1 0.6\nCalculation\nCommunication (b) Granularity\nFigure 8: (B) Transatlantic performance for CV and NLP.", "metadata": {}}, {"text": "in Section 3.", "metadata": {}}, {"text": "However, starting with three GPUs, we see an increase\nin throughput with a maximum speedup of up to 3.2x CV and 2.75x\nfor NLP at eight GPUs.", "metadata": {}}, {"text": "CV’s per-GPU speedup (speedup\n#GPUs ) is almost\nlinear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster\n(0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.", "metadata": {}}, {"text": "The\nreason for this is the NLP granularity of 1.15 with 8 GPUs indicating\nan almost equal part in communication and calculation (Figure 7b)\ndue to the much longer averaging round related to the model size\n(198M vs.", "metadata": {}}, {"text": "560M parameters).", "metadata": {}}, {"text": "The peak network bandwidth utiliza-\ntion between peers was at most a symmetric 1.1 Gb/s while averaging\nand 33 Mb/s ingress while training due to data loading.", "metadata": {}}, {"text": "This means\nthat the network bandwidth of 7 Gb/s was not a limiting factor.", "metadata": {}}, {"text": "(B) Transatlantic scalability.We scale when computing hard-\nware is local.", "metadata": {}}, {"text": "However, what happens when there is cheap capacity in\nanother region?", "metadata": {}}, {"text": "In this case, we study the throughput of experiments\nwith resources in theus-west and eu-central regions (B-2,4,6,8).", "metadata": {}}, {"text": "The B-2 experiment has one VM in the US and one in the EU,\nachieving a virtually identical throughput of 68.4 (US-EU) versus 70.1\n(US) at CV (Figure 8a).", "metadata": {}}, {"text": "Our maximum peak egress rate of 250 Mb/s\ndoes not affect the CV experiments, while the US experiments peaked\nat 1.1 Gb/s.", "metadata": {}}, {"text": "The reduction in bandwidth penalizes NLP harder, where\nwe are 16% slower with 177.3 SPS (US-EU) compared to the intra-zone\nexperiment with 211.4 SPS (US).", "metadata": {}}, {"text": "The resulting increased communica-\ntion can be easily seen in the granularity analysis in Figure 8b (NLP\nA-2,4,6,8 vs.", "metadata": {}}, {"text": "B-2,4,6,8).", "metadata": {}}, {"text": "As only communication time increases in the\nNLP (B) experiments compared to(A), a granularity of≫ 1 indicates\ngood scalability: Adding two more GPUs to the B-6 experiment with\na granularity of 1.03 results in a throughput increase of 15% (B-8)\nrelative to the baseline.", "metadata": {}}, {"text": "Meanwhile, adding two more GPUs to the\nB-2 experiment with a granularity of 2.21 results in a throughput\nincrease of 77% (B-4) relative to the baseline.", "metadata": {}}, {"text": "In the B-4 experiment, we look at what happens when we increase\nthe number of VMs to four, with two in the US and two in the EU.", "metadata": {}}, {"text": "Nothing surprising happens with CV, as the workload continues to\nbe mostly computation, with a throughput of 135.8 (B-4), only 3%\nslower than the intra-zone experiment with 140.4 SPS (A-4).", "metadata": {}}, {"text": "How-\never, at NLP, things get more interesting as we now have more overall\ncommunication with four peers, but they can average locally first\nand only later transmit across the Atlantic.", "metadata": {}}, {"text": "However, compared to\ntheir A-counterparts, we do not see a difference in relative scalability\nwith either B-4, B-6, or B-8.", "metadata": {}}, {"text": "This means that training across regions\n0\n 2\n 4\n 6\n 8\n 10\nT4 Count\n0\n200\n400\n600Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV A-3\nCV C-3\nCV A-4\nCV C-4\nCV A-6\nCV C-6\nCV A-8\nCV C-8\nNLP A-3\nNLP C-3\nNLP A-4\nNLP C-4\nNLP A-6\nNLP C-6\nNLP A-8\nNLP C-8\n0\n200\n400\n600Time in Seconds\n21.25.9\n14.5\n4.7\n8.3 4.1\n5.4 3.3 3.9\n0.8\n2.7\n0.7\n1.7\n0.6\n1.1\n0.4\nCalculation\nCommunication (b) Granularity\nFigure 9: (C) Intercontinental performance for CV and NLP.", "metadata": {}}, {"text": "(B) is slower, but the contribution per GPU decreases at the same\nrate as in training within a zone(A).", "metadata": {}}, {"text": "The per-GPU speedup with ad-\nditional hardware reduces at the same rate for either setup (between\n0.05 and 0.06).", "metadata": {}}, {"text": "This results in two observations: First, communica-\ntion overhead scales linearly with the number of peers.", "metadata": {}}, {"text": "Second, we\nonly have to pay the penalty for transatlantic training once.", "metadata": {}}, {"text": "How-\never, we cannot expect a significant improvement in communication\nefficiency when we increase the amount of available local resources.", "metadata": {}}, {"text": "Summarizing, with an transatlantic setup, CV achieves a virtually\nidentical maximum speedup of 3.2x with 8 GPUs compared to A-1\n(B-8 is 2% slower than A-8), while NLP is more affected by lower\nnetwork bandwidth and only achieves a speedup of 2.15x (B-8 is\n22% slower than A-8).", "metadata": {}}, {"text": "The transatlantic training penalty is applied\nonce;", "metadata": {}}, {"text": "however, it does not affect the relative scaling with additional\ncompute resources.", "metadata": {}}, {"text": "(C) Intercontinental scalability.To take geo-distribution to the\nextreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS,\nto see how much worse bandwidth affects the training throughput\n(C-3,4,6,8 in Table 2).", "metadata": {}}, {"text": "How does the intercontinental penalty investigated in(B) affect\ndeployments with a single GPU on each continent?", "metadata": {}}, {"text": "Comparing the\nA-3 and C-3 experiments with three local versus three fully remote\nGPUs, CV is only 5% slower, while NLP suffers a 34% drop in through-\nput (Figure 9a) and does not even reach the baseline single GPU\nperformance (A-1).", "metadata": {}}, {"text": "The peak egress for each region was 318, 258,\nand 237 Mb/s for the US, EU, and ASIA, respectively.", "metadata": {}}, {"text": "Since our band-\nwidth measurements were 210 and 130 Mb/s from the US to the EU\nand ASIA, respectively (Table 3), this suggests that the averaging\nwas done over the US node and not an N-to-N all-reduce (a detailed\nanalysis of how averaging affects bandwidths is discussed in Sec-\ntion 6).", "metadata": {}}, {"text": "Thus, the limiting factor was the US-ASIA connection at\n130 Mb/s rather than the 80 Mb/s from EU-ASIA.", "metadata": {}}, {"text": "The same trend\ncontinues with the C-4 run, which adds AUS as a continent with one\nadditional VM.", "metadata": {}}, {"text": "As we know from the transatlantic experiments(B)\nthat an additional continent has a detrimental effect on throughput,\nwhich, for the four continents experiment, C-4, results in a 9% slower\nthroughput for CV and 36% slower for NLP compared to the A-4 runs\n(Figure 7a).", "metadata": {}}, {"text": "Again, the US VM is used as an averaging intermediary\nwith a peak egress of 365 Mb/s, while the other continents are be-\ntween 318 and 330 Mb/s.", "metadata": {}}, {"text": "When comparing the two continents (B-4)\nversus four continents (C-4) experiments, one GPU on each continent\n(C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs\non two continents (B-4).", "metadata": {}}, {"text": "This reinforces that local hardware should\nbe preferred whenever possible.", "metadata": {}}, {"text": "However, we are always faster than\nthe baseline (A-1), starting from 4 GPUs in both the transatlantic and\nintercontinental settings.", "metadata": {}}, {"text": "While these experiments were specifically\ndesigned to be a worst-case scenario, what about a more balanced\nGPU distribution with at least two GPUs in each region?", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "D-1\n D-2\n D-3\nT4 Count\n200\n300Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV D-1\nCV D-2\nCV D-3\nNLP D-1\nNLP D-2\nNLP D-3\n0\n200\n400\n600Time in Seconds\n14.5 14.2 12.7\n2.7 2.5 2.0\nCalculation\nCommunication (b) Granularity\nFigure 10: Multi-cloud performance for CV and NLP.\nWhen comparing the C-6 experiment with two GPUs in three\ncontinents to the local A-6 experiments, the throughput slowdown\nis almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%)\nto A-4. Scaling further to two GPUs in four continents, C-8 is slightly\nslower at NLP (41%) compared to C-4 (36%) to their respective local\nruns (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).\nThe small granularity removes the additional gain of four more GPUs\nsince the task is no longer suitable for distributed training. However,\nas the CV task is still at a granularity of 3.33 on C-8, it reaches a\nspeedup of 3.02x, only 7% slower than the fully local A-8 experiment.\nThe peak egress of 678 Mb/s was also reached on one US VM, while the\nremaining VMs were between 450 and 550 Mb/s. These observations\nshow that adding another continent does not significantly reduce\nthroughput when training on three continents with at least two VMs.\nIn summary, while local compute is the best choice for maximum\nthroughput, for high granularity tasks like CV, even distributing VMs\nover four continents only slows down performance by 7%. However,\nintercontinental training leads to a significant penalty on a task with\nlower granularity, like NLP, resulting in a performance drop of 41%\n(C-8) compared to the fully local experiment (A-8). Finally, each addi-\ntional region introduces a constant penalty that is not amortized by\nadding local hardware, which should be considered when running\ngeo-distributed training setups.\n5 MULTI-CLOUD PERFORMANCE\nUsing multiple cloud providers makes sense if we want to use re-\nsources cost-effectively and have additional reliability. In our sce-\nnario, we are interested in what throughput per $ can be expected\nand if any barriers prevent multi-cloud training. However, one can\nalso consider the data center’s carbon footprint, which can change\ndepending on the season and time of day [6].\nTable 4: Average multi-cloud throughput and latency.\n(a) Single stream TCP throughput in Gb/s.\nFrom\nTo GC A WS Azure\nGC 6.35 1.52 0.45\nA WS 1.81 4.87\nAzure 0.47 7.63\n(b) ICMP Latency in ms.\nFrom\nTo GC A WS Azure\nGC 0.71 15.3 51.22\nA WS 13.85 0.15\nAzure 49.80 1.56\nWe have compiled the current prices for spot and on-demand\ninstances for T4 GPUs with 8 CPU cores and the egress costs for\nthree well-known cloud providers, GC [5], AWS [2], and Azure [9]\n(Section 1). There are two different pricing concepts. On the one\nhand, there are GC and Azure, which offer relatively cheap instances,\nwith 69% and 73% savings over on-demand pricing, respectively,\nand relatively expensive egress charges between continents of up to\n$0.15/GB. On the other hand, there is AWS, where the spot instance is\nonly 51% cheaper than the on-demand instance and more than twice\nas expensive as GC or Azure. However, the egress fees here are much\ncheaper at only $0.02/GB. Because of the additional offerings around\ncompute, such as networking, identity and cost management, and\ntooling, it is not easy to fairly compare cloud providers. Therefore,\nwe will limit ourselves to network and VM costs.\nWith the multi-cloud experiments from this section, we want to\nevaluate the following scenarios: First, partially switching from one\nprovider to another without stopping the training. Second, scaling\nresources in the same region when one of the cloud providers is\nalready at capacity for spot-priced VMs or the current price is too\nhigh [24]. We know from Section 4 that scaling resources in the same\nlocation can significantly improve performance, which may only be\npossible using additional cloud providers.\nExperimental design. To enable a fair comparison between the\ncloud providers, we rented hardware most similar to each other in\nthe same region. We used each provider’s default settings and only\nchanged hardware specs. For GC, it is the same instance as in Sec-\ntion 4. At AWS, it is ag4dn.2xlarge with eight cores and 32 GB in\nthe us-west-2c region. Unfortunately, we had to make two compro-\nmises with Azure. There was only the combination of four cores and\n30 GB RAM (NC4as_T4_v3), and there were no T4 GPU resources\navailable in the us-west, so we had to fall back tous-south-2.\nThe network profiling between all cloud providers in Table 4\nshows that their intra-cloud connectivity is comparably fast with 6.4,\n4.9, and 7.6 Gb/s for GC, AWS, and Azure, respectively. All connec-\ntions are mostly symmetric, with inter-cloud connectivity between\nGC and AWS providing up to 1.8 Gb/s and a ping of 15.3ms, indicating\nthat while they are likely not in the same data center, they are close\nto each other and connected to the same Internet exchange point.\nHowever, connectivity to Azure could be better since it operates in\na different zone, with a bandwidth of 0.5 Gb/s and a ping of 51ms.\nOur experimental setup consists of four GPUs with equal contri-\nbutions from each cloud provider. D-1 is the baseline with four GPUs\nat GC, D-2 with two GPUs each at GC and AWS, and D-3 with two\nGPUs at GC and Azure. We compare moving two VMs to a different\ncloud provider to see the impact on cost and throughput.\n(1) No inter-cloud throughput penalty. Figure 10 shows the\nthroughput and granularity of each multi-cloud experiment. CV and\nNLP runs have essentially identical throughput regardless of the\ncombination of cloud providers. Only the D-3 experiments show a\nvery slight slowdown in communication time, reflected in the lower\ngranularity score (Figure 10b) of 12.72 in CV and 1.99 in NLP com-\npared to the D-1 baseline scores of 14.48 and 2.73, respectively. Actual\nthroughput was between 1-2% slower than the baseline, which is\nnegligible and only related to the slightly worse connection to the\nAzure data center. These results confirm our observation from Sec-\ntion 4 that network connectivity determines scalability, and one can\neasily train in a multi-cloud scenario.\n(2) External egress costs can overshadow VM costs.One draw-\nback to training in multiple regions or zones is that egress traffic can\nincur additional costs depending on the cloud provider. We have sum-\nmarized the cost of egress traffic within a zone (intra-zone), between\nzones in each region (inter-zone), and between continents in Sec-\ntion 1. Notably, any traffic to Oceania (Australia, New Zealand, and\nothers, abbreviated as OCE) generates the highest cost of $0.15/GB\nfor GC. We have broken down the costs for the multi-cloud experi-\nment in Figure 11a on an hourly per-VM basis. With only four peers\nin the D-1/2/3 experiments, we have an N-to-N communication, i.e.,", "sentences": [{"text": "D-1\n D-2\n D-3\nT4 Count\n200\n300Samples per Second\nModel\nCONV\nRXLM\n(a) Throughput\nCV D-1\nCV D-2\nCV D-3\nNLP D-1\nNLP D-2\nNLP D-3\n0\n200\n400\n600Time in Seconds\n14.5 14.2 12.7\n2.7 2.5 2.0\nCalculation\nCommunication (b) Granularity\nFigure 10: Multi-cloud performance for CV and NLP.", "metadata": {}}, {"text": "When comparing the C-6 experiment with two GPUs in three\ncontinents to the local A-6 experiments, the throughput slowdown\nis almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%)\nto A-4.", "metadata": {}}, {"text": "Scaling further to two GPUs in four continents, C-8 is slightly\nslower at NLP (41%) compared to C-4 (36%) to their respective local\nruns (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).", "metadata": {}}, {"text": "The small granularity removes the additional gain of four more GPUs\nsince the task is no longer suitable for distributed training.", "metadata": {}}, {"text": "However,\nas the CV task is still at a granularity of 3.33 on C-8, it reaches a\nspeedup of 3.02x, only 7% slower than the fully local A-8 experiment.", "metadata": {}}, {"text": "The peak egress of 678 Mb/s was also reached on one US VM, while the\nremaining VMs were between 450 and 550 Mb/s.", "metadata": {}}, {"text": "These observations\nshow that adding another continent does not significantly reduce\nthroughput when training on three continents with at least two VMs.", "metadata": {}}, {"text": "In summary, while local compute is the best choice for maximum\nthroughput, for high granularity tasks like CV, even distributing VMs\nover four continents only slows down performance by 7%.", "metadata": {}}, {"text": "However,\nintercontinental training leads to a significant penalty on a task with\nlower granularity, like NLP, resulting in a performance drop of 41%\n(C-8) compared to the fully local experiment (A-8).", "metadata": {}}, {"text": "Finally, each addi-\ntional region introduces a constant penalty that is not amortized by\nadding local hardware, which should be considered when running\ngeo-distributed training setups.", "metadata": {}}, {"text": "5 MULTI-CLOUD PERFORMANCE\nUsing multiple cloud providers makes sense if we want to use re-\nsources cost-effectively and have additional reliability.", "metadata": {}}, {"text": "In our sce-\nnario, we are interested in what throughput per $ can be expected\nand if any barriers prevent multi-cloud training.", "metadata": {}}, {"text": "However, one can\nalso consider the data center’s carbon footprint, which can change\ndepending on the season and time of day [6].", "metadata": {}}, {"text": "Table 4: Average multi-cloud throughput and latency.", "metadata": {}}, {"text": "(a) Single stream TCP throughput in Gb/s.", "metadata": {}}, {"text": "From\nTo GC A WS Azure\nGC 6.35 1.52 0.45\nA WS 1.81 4.87\nAzure 0.47 7.63\n(b) ICMP Latency in ms.", "metadata": {}}, {"text": "From\nTo GC A WS Azure\nGC 0.71 15.3 51.22\nA WS 13.85 0.15\nAzure 49.80 1.56\nWe have compiled the current prices for spot and on-demand\ninstances for T4 GPUs with 8 CPU cores and the egress costs for\nthree well-known cloud providers, GC [5], AWS [2], and Azure [9]\n(Section 1).", "metadata": {}}, {"text": "There are two different pricing concepts.", "metadata": {}}, {"text": "On the one\nhand, there are GC and Azure, which offer relatively cheap instances,\nwith 69% and 73% savings over on-demand pricing, respectively,\nand relatively expensive egress charges between continents of up to\n$0.15/GB.", "metadata": {}}, {"text": "On the other hand, there is AWS, where the spot instance is\nonly 51% cheaper than the on-demand instance and more than twice\nas expensive as GC or Azure.", "metadata": {}}, {"text": "However, the egress fees here are much\ncheaper at only $0.02/GB.", "metadata": {}}, {"text": "Because of the additional offerings around\ncompute, such as networking, identity and cost management, and\ntooling, it is not easy to fairly compare cloud providers.", "metadata": {}}, {"text": "Therefore,\nwe will limit ourselves to network and VM costs.", "metadata": {}}, {"text": "With the multi-cloud experiments from this section, we want to\nevaluate the following scenarios: First, partially switching from one\nprovider to another without stopping the training.", "metadata": {}}, {"text": "Second, scaling\nresources in the same region when one of the cloud providers is\nalready at capacity for spot-priced VMs or the current price is too\nhigh [24].", "metadata": {}}, {"text": "We know from Section 4 that scaling resources in the same\nlocation can significantly improve performance, which may only be\npossible using additional cloud providers.", "metadata": {}}, {"text": "Experimental design.", "metadata": {}}, {"text": "To enable a fair comparison between the\ncloud providers, we rented hardware most similar to each other in\nthe same region.", "metadata": {}}, {"text": "We used each provider’s default settings and only\nchanged hardware specs.", "metadata": {}}, {"text": "For GC, it is the same instance as in Sec-\ntion 4.", "metadata": {}}, {"text": "At AWS, it is ag4dn.2xlarge with eight cores and 32 GB in\nthe us-west-2c region.", "metadata": {}}, {"text": "Unfortunately, we had to make two compro-\nmises with Azure.", "metadata": {}}, {"text": "There was only the combination of four cores and\n30 GB RAM (NC4as_T4_v3), and there were no T4 GPU resources\navailable in the us-west, so we had to fall back tous-south-2.", "metadata": {}}, {"text": "The network profiling between all cloud providers in Table 4\nshows that their intra-cloud connectivity is comparably fast with 6.4,\n4.9, and 7.6 Gb/s for GC, AWS, and Azure, respectively.", "metadata": {}}, {"text": "All connec-\ntions are mostly symmetric, with inter-cloud connectivity between\nGC and AWS providing up to 1.8 Gb/s and a ping of 15.3ms, indicating\nthat while they are likely not in the same data center, they are close\nto each other and connected to the same Internet exchange point.", "metadata": {}}, {"text": "However, connectivity to Azure could be better since it operates in\na different zone, with a bandwidth of 0.5 Gb/s and a ping of 51ms.", "metadata": {}}, {"text": "Our experimental setup consists of four GPUs with equal contri-\nbutions from each cloud provider.", "metadata": {}}, {"text": "D-1 is the baseline with four GPUs\nat GC, D-2 with two GPUs each at GC and AWS, and D-3 with two\nGPUs at GC and Azure.", "metadata": {}}, {"text": "We compare moving two VMs to a different\ncloud provider to see the impact on cost and throughput.", "metadata": {}}, {"text": "(1) No inter-cloud throughput penalty.", "metadata": {}}, {"text": "Figure 10 shows the\nthroughput and granularity of each multi-cloud experiment.", "metadata": {}}, {"text": "CV and\nNLP runs have essentially identical throughput regardless of the\ncombination of cloud providers.", "metadata": {}}, {"text": "Only the D-3 experiments show a\nvery slight slowdown in communication time, reflected in the lower\ngranularity score (Figure 10b) of 12.72 in CV and 1.99 in NLP com-\npared to the D-1 baseline scores of 14.48 and 2.73, respectively.", "metadata": {}}, {"text": "Actual\nthroughput was between 1-2% slower than the baseline, which is\nnegligible and only related to the slightly worse connection to the\nAzure data center.", "metadata": {}}, {"text": "These results confirm our observation from Sec-\ntion 4 that network connectivity determines scalability, and one can\neasily train in a multi-cloud scenario.", "metadata": {}}, {"text": "(2) External egress costs can overshadow VM costs.One draw-\nback to training in multiple regions or zones is that egress traffic can\nincur additional costs depending on the cloud provider.", "metadata": {}}, {"text": "We have sum-\nmarized the cost of egress traffic within a zone (intra-zone), between\nzones in each region (inter-zone), and between continents in Sec-\ntion 1.", "metadata": {}}, {"text": "Notably, any traffic to Oceania (Australia, New Zealand, and\nothers, abbreviated as OCE) generates the highest cost of $0.15/GB\nfor GC.", "metadata": {}}, {"text": "We have broken down the costs for the multi-cloud experi-\nment in Figure 11a on an hourly per-VM basis.", "metadata": {}}, {"text": "With only four peers\nin the D-1/2/3 experiments, we have an N-to-N communication, i.e.,", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "VM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2$ per Hour\n0.572\n0.180\n0.029\n0.058\n0.144\n0.572\n0.180\n0.191\n0.381\n0.083\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.802\n0.395\n0.029\n0.058\n0.144\n0.802\n0.395\n0.191\n0.381\n0.083\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.489\n0.134\n0.000\n0.115\n0.144\n0.489\n0.134\n0.000\n0.763\n0.083\nCloud = Azure\nModel\nCONV RXLM\n(a) Intra- and inter-zone in the US region (D-2/3).\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0\n1\n2\n3\n4\n5$ per Hour\n0.573\n0.182\n0.068\n1.181\n0.123\n0.573\n0.182\n0.251\n4.329\n0.042\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.914\n0.330\n0.068\n0.205\n0.123\n0.914\n0.330\n0.251\n0.753\n0.042\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.687\n0.177\n0.000\n0.513\n0.123\n0.687\n0.177\n0.000\n1.882\n0.042\nCloud = Azure\nModel\nCONV RXLM\n(b) Intercontinental in the US, EU, ASIA and AUS (C-8).\nFigure 11: Costs breakdown for D-2/3 and C-8 experiments.\neach peer sends its gradients to every other peer. This means that1\n3\nof the egress was internal to the partner VM in the same cloud, and\nthe remaining 2\n3 went to the remaining two peers in the other cloud.\nFirst, loading data from Backblaze costs $0.01/GB from anywhere\nin the world, which gives us a rate of $0.144/h for the CV and $0.083/h\nfor the NLP experiments. Even when CV throughput is less than\nhalf of the NLP model (Figure 10a), images are much larger than\ntext, resulting in a higher data rate. While this is close to the spot in-\nstance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time\ncosts until the entire dataset is downloaded and retrieved from the\ndisk cache, assuming large enough local storage. A more detailed\ncomparison of cloud provider storage offerings is beyond our scope,\nbut current prices range from $0.02/GB to $0.14/GB in various GC\nregions, making our setting (B2) competitive.\nSecond, the external egress costs for the NLP experiments are\nvery high compared to the other costs. They are 2.2x higher than the\nspot instance for GC and 5.7x higher for Azure, as the traffic costs\nin the US zone are $0.01/GB and $0.02/GB, respectively. The Azure\ncost is even higher ($0.763/h) than the on-demand instance price\nof $0.489/h. The CV experiments are much less affected due to the\nsmaller model size, but Azure still manages to almost match its spot\ninstance price of $0.134/h with the external egress cost of $0.115/h.\nFinally, the total compute cost, including egress and data loading\nin this multi-cloud constellation, is the sum of all the cloud providers’\nprices times the number of VMs used. For the CV experiments, GC,\nAWS, and Azure cost $0.762/h, $1.192/h, and $0.363/h, respectively,\nmaking the combination of GC with Azure 42% cheaper than GC with\nAWS. For the NLP experiments, GC, AWS, and Azure cost $0.835/h,\n$1.05/h, and $0.973/h, respectively, and GC combined with Azure is\nbetter than GC with AWS by a smaller margin of 3.9%. However, the\nintercontinental network egress prices for both GC and Azure are\nup to 15 times higher than the inter-zone prices, so what about the\ncost-effectiveness compared to geo-distributed experiments?\n(3) Geo-distributed egress can incur most of the cost. To\nillustrate the cost of intercontinental training, we use our C-8 exper-\niment with two VMs in four continents from Section 4 to plug the\ncost for each cloud provider. The egress costs are calculated slightly\ndifferently than in the D-2 and D-3 experiments because four groups\nof two VMs average locally and then distribute the gradients across\nthe other groups. This results in 8\n20 internal egress calls (two calls\nbetween each group), 6\n20 intercontinental egress calls (two calls be-\ntween three regions), and 6\n20 AUS egress calls (three regions share\ntheir gradients with AUS and vice versa).\nFigure 11b shows the resulting egress traffic cost per VM. The\nhigh cost between continents scales to a multiple of the remaining\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n5\n10\n15Egress MB per Second\nRN18\nRN50 RN152 WRN101 CONV\nGPU Count\n2\n3\n4\n8\n(a) CV\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n10\n20\n30\n40\n50Egress MB per Second\nRBse RLrg RXLM\nGPU Count\n2\n3\n4\n8 (b) NLP\nFigure 12: Baseline egress rate on 2-8 A10 GPUs.\ncost for CV and NLP with GC and Azure. For NLP, the external\negress cost for GC is $4.329/h, more than 90% of the total cost per VM\n($4.804/h). Even with Azure having a more moderate rate of $0.02/GB\nfor intercontinental communication and only $0.08/GB for OCE\ntraffic, it still results in $1.882/h external egress cost ($2.101/h total).\nThis is in contrast to AWS, which has a cap of $0.02/GB to any location,\nresulting in the best total cost of $1.376/h per VM. The relatively high\nAWS instance cost compares favorably to the other cloud providers\nregarding geo-distributed training. Keeping egress traffic in mind\nwhen deciding to scale to other continents is essential, as it can be the\nmost significant part of the total cost. This raises another question:\nIf egress traffic matters so much, how does model size affect it?\n(4) Small models have lower egress rates than larger models.\nModel size affects two parts of the distributed training time. First,\nlarger models tend to have slower averaging rates, but more data\nmovement costs due to their size. However, larger models are also av-\neraged less frequently because they take longer to perform a step. To\nanalyze this, we review the experiments in Section 3, where we evalu-\nate different model sizes and GPUs counts. Figure 12 shows the aver-\nage egress rate over each experiment’s runtime for both CV and NLP\nfrom two to eight A10 GPUs. The trend is clear: the smaller the model,\nthe lower the egress rate for all GPUs (e.g., RN18 vs. RN50). This is\nsurprising, as the \"square-cube\" law [37] states that with a decrease in\nparameters, the calculation time will decrease quadratically while the\ncommunication time decreases linearly. This means that with a suffi-\nciently small model, most of the training will consist of communica-\ntion time, and the egress rate would increase, as it is defined through\nparameter count\ncalculation time . However, we find that even with our smallest model,\nRN18, with 11.7M parameters and eight A10 GPUs, we are still not at\nthe point where the communication time takes up most of the time.\nIn summary, multi-cloud training is generally possible and can\nbe cost-effective when keeping the egress costs and granularity in\nmind. Regardless of the cloud provider, staying in the same region is\npreferred, with the US having the most favorable egress price offers.\nA significant portion of the cost may be hidden in egress costs, ac-\ncounting for more than 90% of the total cost in our NLP experiments\nin GC and Azure. Based on the additional egress costs alone, renting\non-demand hardware may be more advantageous than using spot\ninstances between different regions. CV training is generally more\ncalculation- than communication-heavy, resulting in slightly higher\ndata-loading but fewer egress costs. However, from our experiments,\nthis is a favorable trade-off because data-loading is much cheaper\nthan egress costs.\n6 HYBRID-CLOUD PERFORMANCE\nCan augmenting on-premise hardware with cloud resources be\nworthwhile to speed up DL training? In this section, we examine two", "sentences": [{"text": "VM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2$ per Hour\n0.572\n0.180\n0.029\n0.058\n0.144\n0.572\n0.180\n0.191\n0.381\n0.083\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.802\n0.395\n0.029\n0.058\n0.144\n0.802\n0.395\n0.191\n0.381\n0.083\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.489\n0.134\n0.000\n0.115\n0.144\n0.489\n0.134\n0.000\n0.763\n0.083\nCloud = Azure\nModel\nCONV RXLM\n(a) Intra- and inter-zone in the US region (D-2/3).", "metadata": {}}, {"text": "VM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0\n1\n2\n3\n4\n5$ per Hour\n0.573\n0.182\n0.068\n1.181\n0.123\n0.573\n0.182\n0.251\n4.329\n0.042\nCloud = GC\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.914\n0.330\n0.068\n0.205\n0.123\n0.914\n0.330\n0.251\n0.753\n0.042\nCloud = AWS\nVM On-Demand\nVM Spot\nEgress InternalEgress ExternalDataloading\n0.687\n0.177\n0.000\n0.513\n0.123\n0.687\n0.177\n0.000\n1.882\n0.042\nCloud = Azure\nModel\nCONV RXLM\n(b) Intercontinental in the US, EU, ASIA and AUS (C-8).", "metadata": {}}, {"text": "Figure 11: Costs breakdown for D-2/3 and C-8 experiments.", "metadata": {}}, {"text": "each peer sends its gradients to every other peer.", "metadata": {}}, {"text": "This means that1\n3\nof the egress was internal to the partner VM in the same cloud, and\nthe remaining 2\n3 went to the remaining two peers in the other cloud.", "metadata": {}}, {"text": "First, loading data from Backblaze costs $0.01/GB from anywhere\nin the world, which gives us a rate of $0.144/h for the CV and $0.083/h\nfor the NLP experiments.", "metadata": {}}, {"text": "Even when CV throughput is less than\nhalf of the NLP model (Figure 10a), images are much larger than\ntext, resulting in a higher data rate.", "metadata": {}}, {"text": "While this is close to the spot in-\nstance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time\ncosts until the entire dataset is downloaded and retrieved from the\ndisk cache, assuming large enough local storage.", "metadata": {}}, {"text": "A more detailed\ncomparison of cloud provider storage offerings is beyond our scope,\nbut current prices range from $0.02/GB to $0.14/GB in various GC\nregions, making our setting (B2) competitive.", "metadata": {}}, {"text": "Second, the external egress costs for the NLP experiments are\nvery high compared to the other costs.", "metadata": {}}, {"text": "They are 2.2x higher than the\nspot instance for GC and 5.7x higher for Azure, as the traffic costs\nin the US zone are $0.01/GB and $0.02/GB, respectively.", "metadata": {}}, {"text": "The Azure\ncost is even higher ($0.763/h) than the on-demand instance price\nof $0.489/h.", "metadata": {}}, {"text": "The CV experiments are much less affected due to the\nsmaller model size, but Azure still manages to almost match its spot\ninstance price of $0.134/h with the external egress cost of $0.115/h.", "metadata": {}}, {"text": "Finally, the total compute cost, including egress and data loading\nin this multi-cloud constellation, is the sum of all the cloud providers’\nprices times the number of VMs used.", "metadata": {}}, {"text": "For the CV experiments, GC,\nAWS, and Azure cost $0.762/h, $1.192/h, and $0.363/h, respectively,\nmaking the combination of GC with Azure 42% cheaper than GC with\nAWS.", "metadata": {}}, {"text": "For the NLP experiments, GC, AWS, and Azure cost $0.835/h,\n$1.05/h, and $0.973/h, respectively, and GC combined with Azure is\nbetter than GC with AWS by a smaller margin of 3.9%.", "metadata": {}}, {"text": "However, the\nintercontinental network egress prices for both GC and Azure are\nup to 15 times higher than the inter-zone prices, so what about the\ncost-effectiveness compared to geo-distributed experiments?", "metadata": {}}, {"text": "(3) Geo-distributed egress can incur most of the cost.", "metadata": {}}, {"text": "To\nillustrate the cost of intercontinental training, we use our C-8 exper-\niment with two VMs in four continents from Section 4 to plug the\ncost for each cloud provider.", "metadata": {}}, {"text": "The egress costs are calculated slightly\ndifferently than in the D-2 and D-3 experiments because four groups\nof two VMs average locally and then distribute the gradients across\nthe other groups.", "metadata": {}}, {"text": "This results in 8\n20 internal egress calls (two calls\nbetween each group), 6\n20 intercontinental egress calls (two calls be-\ntween three regions), and 6\n20 AUS egress calls (three regions share\ntheir gradients with AUS and vice versa).", "metadata": {}}, {"text": "Figure 11b shows the resulting egress traffic cost per VM.", "metadata": {}}, {"text": "The\nhigh cost between continents scales to a multiple of the remaining\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n5\n10\n15Egress MB per Second\nRN18\nRN50 RN152 WRN101 CONV\nGPU Count\n2\n3\n4\n8\n(a) CV\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n2xA10\n3xA10\n4xA10\n8xA10\n0\n10\n20\n30\n40\n50Egress MB per Second\nRBse RLrg RXLM\nGPU Count\n2\n3\n4\n8 (b) NLP\nFigure 12: Baseline egress rate on 2-8 A10 GPUs.", "metadata": {}}, {"text": "cost for CV and NLP with GC and Azure.", "metadata": {}}, {"text": "For NLP, the external\negress cost for GC is $4.329/h, more than 90% of the total cost per VM\n($4.804/h).", "metadata": {}}, {"text": "Even with Azure having a more moderate rate of $0.02/GB\nfor intercontinental communication and only $0.08/GB for OCE\ntraffic, it still results in $1.882/h external egress cost ($2.101/h total).", "metadata": {}}, {"text": "This is in contrast to AWS, which has a cap of $0.02/GB to any location,\nresulting in the best total cost of $1.376/h per VM.", "metadata": {}}, {"text": "The relatively high\nAWS instance cost compares favorably to the other cloud providers\nregarding geo-distributed training.", "metadata": {}}, {"text": "Keeping egress traffic in mind\nwhen deciding to scale to other continents is essential, as it can be the\nmost significant part of the total cost.", "metadata": {}}, {"text": "This raises another question:\nIf egress traffic matters so much, how does model size affect it?", "metadata": {}}, {"text": "(4) Small models have lower egress rates than larger models.", "metadata": {}}, {"text": "Model size affects two parts of the distributed training time.", "metadata": {}}, {"text": "First,\nlarger models tend to have slower averaging rates, but more data\nmovement costs due to their size.", "metadata": {}}, {"text": "However, larger models are also av-\neraged less frequently because they take longer to perform a step.", "metadata": {}}, {"text": "To\nanalyze this, we review the experiments in Section 3, where we evalu-\nate different model sizes and GPUs counts.", "metadata": {}}, {"text": "Figure 12 shows the aver-\nage egress rate over each experiment’s runtime for both CV and NLP\nfrom two to eight A10 GPUs.", "metadata": {}}, {"text": "The trend is clear: the smaller the model,\nthe lower the egress rate for all GPUs (e.g., RN18 vs.", "metadata": {}}, {"text": "RN50).", "metadata": {}}, {"text": "This is\nsurprising, as the \"square-cube\" law [37] states that with a decrease in\nparameters, the calculation time will decrease quadratically while the\ncommunication time decreases linearly.", "metadata": {}}, {"text": "This means that with a suffi-\nciently small model, most of the training will consist of communica-\ntion time, and the egress rate would increase, as it is defined through\nparameter count\ncalculation time .", "metadata": {}}, {"text": "However, we find that even with our smallest model,\nRN18, with 11.7M parameters and eight A10 GPUs, we are still not at\nthe point where the communication time takes up most of the time.", "metadata": {}}, {"text": "In summary, multi-cloud training is generally possible and can\nbe cost-effective when keeping the egress costs and granularity in\nmind.", "metadata": {}}, {"text": "Regardless of the cloud provider, staying in the same region is\npreferred, with the US having the most favorable egress price offers.", "metadata": {}}, {"text": "A significant portion of the cost may be hidden in egress costs, ac-\ncounting for more than 90% of the total cost in our NLP experiments\nin GC and Azure.", "metadata": {}}, {"text": "Based on the additional egress costs alone, renting\non-demand hardware may be more advantageous than using spot\ninstances between different regions.", "metadata": {}}, {"text": "CV training is generally more\ncalculation- than communication-heavy, resulting in slightly higher\ndata-loading but fewer egress costs.", "metadata": {}}, {"text": "However, from our experiments,\nthis is a favorable trade-off because data-loading is much cheaper\nthan egress costs.", "metadata": {}}, {"text": "6 HYBRID-CLOUD PERFORMANCE\nCan augmenting on-premise hardware with cloud resources be\nworthwhile to speed up DL training?", "metadata": {}}, {"text": "In this section, we examine two", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Table 5: Average hybrid-cloud throughput and latency.\n(a) Single stream TCP throughput in Gb/s.\nFrom To EU T4US T4US A10\nRTX8000 0.45 0.06 0.05DGX-2 (8xV100)0.55 0.08 0.07\n(b) ICMP Latency in ms.\nFrom To EU T4US T4US A10\nRTX8000 16.73150.80 159.05\nDGX-2 (8xV100)16.19150.27 158.54\nTable 6: Hybrid- vs. cloud-only throughput for the (E) setting.\nModel\nSetup RTX8000E-A-8 E-B-8 E-C-8 8xT4 8xA10\nCONV 194.8 316.8 283.5 429.3 261.9 620.6\nRXLM 431.8 556.7 330.6 223.7 575.1 1059.9\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(a) CV Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n8.21\n7.28\n5.80\n3.24\n1.86\n1.98\n2.15\n1.77\n1.21\n1.21\n0.72\n0.64\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(c) NLP Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n1.27 1.00\n0.88 0.66\n0.34\n0.34\n0.27\n0.27\n0.20\n0.15\n0.12 0.07\nCalculation\nCommunication (d) NLP Granularity\nFigure 13: Hybrid-cloud experiments for the (E) setting.\nsettings: (E), where a consumer-grade GPU, the RTX8000, is deployed\non-site, and (F), where a server-grade node, the DGX-2 (8xV100), is\ndeployed on-site. We vary the extra resources, between one to eight\nT4 EU ({E,F}-A), T4 US ({E,F}-B) and A10 US ({E,F}-C) GPUs.\nExperimental design. In both settings, we want to investigate\nhow to extend local hardware with cloud resources and when this\nleads to better throughput. The cloud resources, in this case, are the\nsame US/EU GC T4 instances as in Section 4 and the US LambdaLabs\nA10 GPUs from Section 3. We double the number of cloud VMs with\neach increment, starting with one additional GPU (i.e., E-A-1) until\nwe have eight additional cloud VMs (i.e., E-A-8). This allows us to com-\npare the same hardware in the EU and the US, and slightly weaker, lo-\ncal hardware (EU T4) and better, but more distant hardware (US A10).\nBoth the (E) and (F) setups share the network uplink between 450\nand 550 Mb/s to the EU datacenter in Belgium, as they are located\nin the same building in Europe (Table 5). However, as this is not a\nGoogle-owned datacenter, the traffic is partly going over the public\ninternet, which results in a lower bandwidth of 50 and 80 Mb/s to\nthe US-based VMs compared to 210 Mb/s between the US and EU\nGC datacenters (Table 3a).\n(E) Consumer-grade setting. The results follow the same trend\nas in Section 4. The CV task has a higher granularity of 8.21 with\n2 GPUs at E-A-1 than NLP (1.27) (Figures 13b and 13d), and scales\nregardless of the location of the cloud resources (Figure 13a). We\nalmost match the baseline throughput of 195 SPS at 5 GPUs in all set-\ntings for CV (E-A-4, E-B-4, E-C-4). The best throughput was reached\nat E-C-8 with the US A10 GPUs with 429 SPS. For NLP, only the\nE-A-8 experiment beats the baseline with a speedup of 1.29x and 556\nSPS due to the low granularity and the intercontinental base penalty\nfor the US experiments.\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(a) CV Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n3.94 3.17 2.32 2.46\n1.66 1.16\n1.71 1.04\n0.82 0.56\n0.66\n0.57\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n500\n1000\n1500Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(c) NLP Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n0.54 0.39 0.41 0.30\n0.15\n0.07\n0.15\n0.94\n0.10 0.09 0.07\n0.02Calculation\nCommunication (d) NLP Granularity\nFigure 14: Hybrid-cloud experiments for the (F) setting.\nHowever, is combining on-premise and remote cloud resources\nbetter than using the cloud without paying the intercontinental band-\nwidth tax? To analyze this, we compare the(E) experiments with the\n8xA10 experiment from Section 3 and 8xT4 experiment from Sec-\ntion 4 in Section 6. First, the 8xA10 experiments are the fastest for\nboth CV and NLP, which removes the respective hybrid-cloud com-\nbination from contention (E-C-8). Second, the 8xT4 experiments\nfor NLP are faster than any other hybrid-cloud setup, making the\ncloud-only solution favorable. Finally, while we always beat the\nbaseline 8xT4 CV throughput (261.9 SPS), but in the case of E-B-8\n(283.5 SPS), just barely. The throughput of E-A-8 (316.8 SPS) makes\nthe hybrid-cloud setup the most favorable in terms of relative GPU\nscaling (32.5 SPS per GPU), but it does not come close to the best\ncloud-only throughput of 8xA10 with 620.6 SPS.\nSummarizing, the cloud-only experiments are the fastest over-\nall due to their single-GPU throughput and locality. Adding cloud\nresources to on-premise hardware leads to a high communication\ntime, which is not compensated by the additional processing speed\nof the GPUs. Proximity to the on-premise hardware is essential, as\nthe more local cloud resources (E-A-8) consistently resulted in a\nbetter throughput than the same remote cloud resources (E-B-8).\n(F) Server-grade setting. The baseline throughput is signifi-\ncantly higher compared to the RTX8000, with a much more powerful\n8xV100 DGX node to 413 SPS for CV and 1811 SPS for NLP (Fig-\nures 14a and 14c) via PyTorch data parallelism [26]. This increases the\npenalties from Section 3, leading to the only speedup from baseline\nfor CV in experiments F-A-8 (507 SPS) and F-C-8 (510 SPS). This is sur-\nprising, as the older T4 GPUs in the EU perform similarly to the much\nnewer A10 GPUs in the US, showcasing the trade-off between slower,\nlocal compute and faster, remote compute. The granularity of 2.46\nfor F-A-8 shows that there is enough calculation time to distribute,\nwhile the F-C-8 experiments spend≈ 62% of the total training time on\ncommunication with a granularity of 0.57 (Figure 14b). The NLP ex-\nperiments never reach the baseline throughput of the 8xV100 due to\nusing most of the time for communication. The NLP F-B and F-C ex-\nperiments mainly consist of communication (Figure 14d) with a gran-\nularity of up to 0.02, which results in a nonlinear, unstable training\ntime due to the minimum matchmaking time issue(2) from Section 3.\nIn summary, the hybrid-cloud experiments conclude that while\non-premise hardware can be augmented with cloud resources, it\nwill likely be cost-efficient if all resources are on the same continent.\nUsing only cloud resources is more advantageous if the on-premises\nhardware is not co-located.", "sentences": [{"text": "Table 5: Average hybrid-cloud throughput and latency.", "metadata": {}}, {"text": "(a) Single stream TCP throughput in Gb/s.", "metadata": {}}, {"text": "From To EU T4US T4US A10\nRTX8000 0.45 0.06 0.05DGX-2 (8xV100)0.55 0.08 0.07\n(b) ICMP Latency in ms.", "metadata": {}}, {"text": "From To EU T4US T4US A10\nRTX8000 16.73150.80 159.05\nDGX-2 (8xV100)16.19150.27 158.54\nTable 6: Hybrid- vs.", "metadata": {}}, {"text": "cloud-only throughput for the (E) setting.", "metadata": {}}, {"text": "Model\nSetup RTX8000E-A-8 E-B-8 E-C-8 8xT4 8xA10\nCONV 194.8 316.8 283.5 429.3 261.9 620.6\nRXLM 431.8 556.7 330.6 223.7 575.1 1059.9\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(a) CV Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n8.21\n7.28\n5.80\n3.24\n1.86\n1.98\n2.15\n1.77\n1.21\n1.21\n0.72\n0.64\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600Samples per Second\nGPU Type\nRTX8000\nEU T4 (E-A)\nUS T4 (E-B)\nUS A10 (E-C)\n(c) NLP Throughput\nE-A-1\nE-A-2\nE-A-4\nE-A-8\nE-B-1\nE-B-2\nE-B-4\nE-B-8\nE-C-1\nE-C-2\nE-C-4\nE-C-8\n0\n200\n400\n600Time in Seconds\n1.27 1.00\n0.88 0.66\n0.34\n0.34\n0.27\n0.27\n0.20\n0.15\n0.12 0.07\nCalculation\nCommunication (d) NLP Granularity\nFigure 13: Hybrid-cloud experiments for the (E) setting.", "metadata": {}}, {"text": "settings: (E), where a consumer-grade GPU, the RTX8000, is deployed\non-site, and (F), where a server-grade node, the DGX-2 (8xV100), is\ndeployed on-site.", "metadata": {}}, {"text": "We vary the extra resources, between one to eight\nT4 EU ({E,F}-A), T4 US ({E,F}-B) and A10 US ({E,F}-C) GPUs.", "metadata": {}}, {"text": "Experimental design.", "metadata": {}}, {"text": "In both settings, we want to investigate\nhow to extend local hardware with cloud resources and when this\nleads to better throughput.", "metadata": {}}, {"text": "The cloud resources, in this case, are the\nsame US/EU GC T4 instances as in Section 4 and the US LambdaLabs\nA10 GPUs from Section 3.", "metadata": {}}, {"text": "We double the number of cloud VMs with\neach increment, starting with one additional GPU (i.e., E-A-1) until\nwe have eight additional cloud VMs (i.e., E-A-8).", "metadata": {}}, {"text": "This allows us to com-\npare the same hardware in the EU and the US, and slightly weaker, lo-\ncal hardware (EU T4) and better, but more distant hardware (US A10).", "metadata": {}}, {"text": "Both the (E) and (F) setups share the network uplink between 450\nand 550 Mb/s to the EU datacenter in Belgium, as they are located\nin the same building in Europe (Table 5).", "metadata": {}}, {"text": "However, as this is not a\nGoogle-owned datacenter, the traffic is partly going over the public\ninternet, which results in a lower bandwidth of 50 and 80 Mb/s to\nthe US-based VMs compared to 210 Mb/s between the US and EU\nGC datacenters (Table 3a).", "metadata": {}}, {"text": "(E) Consumer-grade setting.", "metadata": {}}, {"text": "The results follow the same trend\nas in Section 4.", "metadata": {}}, {"text": "The CV task has a higher granularity of 8.21 with\n2 GPUs at E-A-1 than NLP (1.27) (Figures 13b and 13d), and scales\nregardless of the location of the cloud resources (Figure 13a).", "metadata": {}}, {"text": "We\nalmost match the baseline throughput of 195 SPS at 5 GPUs in all set-\ntings for CV (E-A-4, E-B-4, E-C-4).", "metadata": {}}, {"text": "The best throughput was reached\nat E-C-8 with the US A10 GPUs with 429 SPS.", "metadata": {}}, {"text": "For NLP, only the\nE-A-8 experiment beats the baseline with a speedup of 1.29x and 556\nSPS due to the low granularity and the intercontinental base penalty\nfor the US experiments.", "metadata": {}}, {"text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n200\n400\n600\n800Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(a) CV Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n3.94 3.17 2.32 2.46\n1.66 1.16\n1.71 1.04\n0.82 0.56\n0.66\n0.57\nCalculation\nCommunication (b) CV Granularity\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\nGPU Count\n0\n500\n1000\n1500Samples per Second\nGPU Type\n8xV100\nEU T4 (F-A)\nUS T4 (F-B)\nUS A10 (F-C)\n(c) NLP Throughput\nF-A-1\nF-A-2\nF-A-4\nF-A-8\nF-B-1\nF-B-2\nF-B-4\nF-B-8\nF-C-1\nF-C-2\nF-C-4\nF-C-8\n0\n200\n400\n600Time in Seconds\n0.54 0.39 0.41 0.30\n0.15\n0.07\n0.15\n0.94\n0.10 0.09 0.07\n0.02Calculation\nCommunication (d) NLP Granularity\nFigure 14: Hybrid-cloud experiments for the (F) setting.", "metadata": {}}, {"text": "However, is combining on-premise and remote cloud resources\nbetter than using the cloud without paying the intercontinental band-\nwidth tax?", "metadata": {}}, {"text": "To analyze this, we compare the(E) experiments with the\n8xA10 experiment from Section 3 and 8xT4 experiment from Sec-\ntion 4 in Section 6.", "metadata": {}}, {"text": "First, the 8xA10 experiments are the fastest for\nboth CV and NLP, which removes the respective hybrid-cloud com-\nbination from contention (E-C-8).", "metadata": {}}, {"text": "Second, the 8xT4 experiments\nfor NLP are faster than any other hybrid-cloud setup, making the\ncloud-only solution favorable.", "metadata": {}}, {"text": "Finally, while we always beat the\nbaseline 8xT4 CV throughput (261.9 SPS), but in the case of E-B-8\n(283.5 SPS), just barely.", "metadata": {}}, {"text": "The throughput of E-A-8 (316.8 SPS) makes\nthe hybrid-cloud setup the most favorable in terms of relative GPU\nscaling (32.5 SPS per GPU), but it does not come close to the best\ncloud-only throughput of 8xA10 with 620.6 SPS.", "metadata": {}}, {"text": "Summarizing, the cloud-only experiments are the fastest over-\nall due to their single-GPU throughput and locality.", "metadata": {}}, {"text": "Adding cloud\nresources to on-premise hardware leads to a high communication\ntime, which is not compensated by the additional processing speed\nof the GPUs.", "metadata": {}}, {"text": "Proximity to the on-premise hardware is essential, as\nthe more local cloud resources (E-A-8) consistently resulted in a\nbetter throughput than the same remote cloud resources (E-B-8).", "metadata": {}}, {"text": "(F) Server-grade setting.", "metadata": {}}, {"text": "The baseline throughput is signifi-\ncantly higher compared to the RTX8000, with a much more powerful\n8xV100 DGX node to 413 SPS for CV and 1811 SPS for NLP (Fig-\nures 14a and 14c) via PyTorch data parallelism [26].", "metadata": {}}, {"text": "This increases the\npenalties from Section 3, leading to the only speedup from baseline\nfor CV in experiments F-A-8 (507 SPS) and F-C-8 (510 SPS).", "metadata": {}}, {"text": "This is sur-\nprising, as the older T4 GPUs in the EU perform similarly to the much\nnewer A10 GPUs in the US, showcasing the trade-off between slower,\nlocal compute and faster, remote compute.", "metadata": {}}, {"text": "The granularity of 2.46\nfor F-A-8 shows that there is enough calculation time to distribute,\nwhile the F-C-8 experiments spend≈ 62% of the total training time on\ncommunication with a granularity of 0.57 (Figure 14b).", "metadata": {}}, {"text": "The NLP ex-\nperiments never reach the baseline throughput of the 8xV100 due to\nusing most of the time for communication.", "metadata": {}}, {"text": "The NLP F-B and F-C ex-\nperiments mainly consist of communication (Figure 14d) with a gran-\nularity of up to 0.02, which results in a nonlinear, unstable training\ntime due to the minimum matchmaking time issue(2) from Section 3.", "metadata": {}}, {"text": "In summary, the hybrid-cloud experiments conclude that while\non-premise hardware can be augmented with cloud resources, it\nwill likely be cost-efficient if all resources are on the same continent.", "metadata": {}}, {"text": "Using only cloud resources is more advantageous if the on-premises\nhardware is not co-located.", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "0.5\n 1.0\n 1.5\n 2.0\n 2.5\n 3.0\nCost in $ per 1M Samples\n0\n1000\n2000Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nInstance Type\nSpot\nOn-Demand\nFigure 15: Cost to throughput tradeoff for RoBERTaXLM at\ndifferent instance types. Our training setups (circled), that are\ndue the low granularity of the NLP model, neither cheaper,\nnor faster than the centralized offering (DGX-2).\n7 FURTHER INSIGHTS\nCommunication time can decrease with more peers. Let us\ncompare the granularity of the experiments for E-B (Figure 13b),\nwhich uses T4 GPUs in the US as an additional cloud resource.Both\nthe computation and communication time decrease with the number\nof GPUs, even increasing the granularity from 1.98 at E-B-2 to 2.15\nat E-B-4. This is surprising since, usually, with more peers, the com-\nmunication time should increase, and the US-EU communication\nbottleneck should slow us down to the same extent as the E-B-1\nexperiment. This reduction is a Hivemind-specific anomaly, as it\nuses a single TCP stream per peer. With TCP, there needs to be an\nacknowledgment (ACK) of each packet by the receiving peer, which\nis impacted by the connection’s latency. In our high latency network\nbetween continents, the round trip time (RTT) of 300-318ms limits\nthe maximum bandwidth a single TCP stream to 50-80 Mb/s. How-\never, a way to improve link utilization is to use multiple streams,\none for each peer, which we encounter in experiments E-(B|C)-2,4,8.\nTo verify the potential gains, we perform a microbenchmark of\nthe multi-stream bandwidth from the RTX8000 to the EU and US\ndata centers.Although there is wide variation, likely due to network\nutilization, with 80 clients, we achieve a maximum bandwidth of\n6 Gb/s within the EU and up to 4 Gb/s to the US. While larger peer\ngroups and, consequently, larger models benefit from multi-peer\ncommunication by default and do not see significant changes in com-\nmunication time, small models in unevenly distributed VMs setups\ncan be disproportionately affected. The same trend can be observed\nin all high latency experiments (i.e., between the EU and the US), e.g.,\nE-B, E-C for CV and NLP (Figures 13b and 13d, and F-B and F-C for\nCV (Figure 14b). In summary, uneven distribution of computational\nresources in high-latency networks (e.g., intercontinental) can re-\nduce communication time with Hivemind due to more parallelism,\nlessening the impact of low bandwidth for a single data stream.\nCost analysis. The DGX-2 (8xV100) node from Section 6 rep-\nresents server-grade hardware that could be used to train models.\nHowever, how does it compare in throughput per $ to all of our dis-\ntributed cloud experiments? The Figure 1 (CV) and Figure 15 (NLP)\nshow the complete cost analysis of the DGX-2, the 8xT4 experiments,\nand the 8xA10 experiments for spot and on-demand pricing. We use\nthe internal egress costs from Figure 11a as a reference for the 8xT4\nsetup. For simplicity, we compare the spot pricing without inter-\nruptions, as we assume that a new VM can be spun up fast enough\nnot to affect the training throughput in the long run. We mark the\ncentralized baseline (DGX-2) cost per 1M samples and the through-\nput in samples per second with a horizontal and vertical line. This\nmeans that we are cheaper to the left to the vertical line, and above\nthe horizontal line, we are faster (and vice versa). We circle the new\nvalue propositions that we enable in both figures. Our hardware\nsetups have additional key characteristics: They are resilient by de-\nfault to interruptions due to running in a decentralized fashion and\nthey enable the combination of more GPUs than cloud providers\noffer in a single node. Currently, common hardware configurations\n(DGX) allow up to eight GPUs connected via NVLink, and with older\nhardware, only up to 4xT4s connected via PCIe at 10 GB/s between\nGPUs (with GC). We were able to combine eight single GPU nodes\nfrom GC and LambdaLabs to create competing performance and\nprice setups without dedicated GPU interconnects.\nA spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-\ndemand) in GC US, which makes it the best value proposition for\nthe low granularity NLP task. It is followed by the 8xA10, which\nare 41% slower and 30% more expensive than the DGX-2 (Figure 15).\nThe 8xT4 experiments are even more expensive, as the internal\negress costs take up more than half of the costs, making them the\nworst value proposition. However, for CV, we manage to provide\ntwo new offerings: First, the 8xA10, which is both 50% faster and 49%\ncheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-\n2, while being 37% slower (Figure 1). The CV model can be scaled\nmore easily due to its initially high granularity, which makes the very\ncompetitive offering of $0.6/h per A10 from LambdaLabs an excellent\nvalue proposition. However, while we only evaluated eight T4 GPUs\nfor our GC-based experiments, with a granularity of 5.19 (CV A-8\nin Figure 7b), there is ample space to scale even further. It is important\nto note that LambdaLabs does not charge for any data egress, but GC\ndoes with $0.01/GB, and the 8xT4 experiment is still cheaper. While\nLambdaLabs is often at capacity, Google Cloud positions itself as a\nhyperscaler with the advantage of rarely being at max occupancy.\nWe also evaluated the performance of the 4xT4 PyTorch DDP [26]\nfor CV with the best available multi-T4 node on GC (4xT4). The NLP\nexperiments ran OOM. Since the DDP 4xT4 runs on a single node, it\ncauses no interconnect costs and is priced at $0.96 per 1M samples at\nspot pricing, while our 8xT4 setup costs $1.77 per 1M samples (84%\nmore expensive). However, the 8xT4 setup has a higher throughput\nof 262 SPS (26% faster) compared to the 4xT4 node (207 SPS). This\nhigher speed is not available at the price point of the 4xT4 node.\nMoreover, the 8xT4 setup has the potential for further scaling, which\nwe discussed in detail in Section 4.\nIn summary, the lower spot prices for older GPUs allow us to train\nmodels more cost-efficiently when task granularity allows it and get\nmore value per $ when training on the 8xT4 or 8xA10 compared to\nan DGX-2 node. Combining multiple nodes with single GPUs with\nlower bandwidths enables scaling that was previously impossible to\nachieve without resorting to much more powerful GPUs. Distributed\nspot instance pricing opens up a new value proposition compared\nto on-demand offerings that can even compete with the competitive\npricing of smaller cloud providers.\nSpot VM Interruption Frequency. While we used low spot\nprices as a cost-saving argument in our experiments, we did not\nelaborate on the most significant drawback - the possibility of being\nterminated by the cloud provider at any time. There is already some\nresearch on how different cloud providers track the interruption fre-\nquency and can be used for varying workloads to achieve a positive\n$-per-throughput effect [24, 42, 43].", "sentences": [{"text": "0.5\n 1.0\n 1.5\n 2.0\n 2.5\n 3.0\nCost in $ per 1M Samples\n0\n1000\n2000Samples per Second\nDGX-2 DGX-2\n8xT4 8xT4\n1xT4 1xT4\n8xA10\n1xA10\nInstance Type\nSpot\nOn-Demand\nFigure 15: Cost to throughput tradeoff for RoBERTaXLM at\ndifferent instance types.", "metadata": {}}, {"text": "Our training setups (circled), that are\ndue the low granularity of the NLP model, neither cheaper,\nnor faster than the centralized offering (DGX-2).", "metadata": {}}, {"text": "7 FURTHER INSIGHTS\nCommunication time can decrease with more peers.", "metadata": {}}, {"text": "Let us\ncompare the granularity of the experiments for E-B (Figure 13b),\nwhich uses T4 GPUs in the US as an additional cloud resource.Both\nthe computation and communication time decrease with the number\nof GPUs, even increasing the granularity from 1.98 at E-B-2 to 2.15\nat E-B-4.", "metadata": {}}, {"text": "This is surprising since, usually, with more peers, the com-\nmunication time should increase, and the US-EU communication\nbottleneck should slow us down to the same extent as the E-B-1\nexperiment.", "metadata": {}}, {"text": "This reduction is a Hivemind-specific anomaly, as it\nuses a single TCP stream per peer.", "metadata": {}}, {"text": "With TCP, there needs to be an\nacknowledgment (ACK) of each packet by the receiving peer, which\nis impacted by the connection’s latency.", "metadata": {}}, {"text": "In our high latency network\nbetween continents, the round trip time (RTT) of 300-318ms limits\nthe maximum bandwidth a single TCP stream to 50-80 Mb/s.", "metadata": {}}, {"text": "How-\never, a way to improve link utilization is to use multiple streams,\none for each peer, which we encounter in experiments E-(B|C)-2,4,8.", "metadata": {}}, {"text": "To verify the potential gains, we perform a microbenchmark of\nthe multi-stream bandwidth from the RTX8000 to the EU and US\ndata centers.Although there is wide variation, likely due to network\nutilization, with 80 clients, we achieve a maximum bandwidth of\n6 Gb/s within the EU and up to 4 Gb/s to the US.", "metadata": {}}, {"text": "While larger peer\ngroups and, consequently, larger models benefit from multi-peer\ncommunication by default and do not see significant changes in com-\nmunication time, small models in unevenly distributed VMs setups\ncan be disproportionately affected.", "metadata": {}}, {"text": "The same trend can be observed\nin all high latency experiments (i.e., between the EU and the US), e.g.,\nE-B, E-C for CV and NLP (Figures 13b and 13d, and F-B and F-C for\nCV (Figure 14b).", "metadata": {}}, {"text": "In summary, uneven distribution of computational\nresources in high-latency networks (e.g., intercontinental) can re-\nduce communication time with Hivemind due to more parallelism,\nlessening the impact of low bandwidth for a single data stream.", "metadata": {}}, {"text": "Cost analysis.", "metadata": {}}, {"text": "The DGX-2 (8xV100) node from Section 6 rep-\nresents server-grade hardware that could be used to train models.", "metadata": {}}, {"text": "However, how does it compare in throughput per $ to all of our dis-\ntributed cloud experiments?", "metadata": {}}, {"text": "The Figure 1 (CV) and Figure 15 (NLP)\nshow the complete cost analysis of the DGX-2, the 8xT4 experiments,\nand the 8xA10 experiments for spot and on-demand pricing.", "metadata": {}}, {"text": "We use\nthe internal egress costs from Figure 11a as a reference for the 8xT4\nsetup.", "metadata": {}}, {"text": "For simplicity, we compare the spot pricing without inter-\nruptions, as we assume that a new VM can be spun up fast enough\nnot to affect the training throughput in the long run.", "metadata": {}}, {"text": "We mark the\ncentralized baseline (DGX-2) cost per 1M samples and the through-\nput in samples per second with a horizontal and vertical line.", "metadata": {}}, {"text": "This\nmeans that we are cheaper to the left to the vertical line, and above\nthe horizontal line, we are faster (and vice versa).", "metadata": {}}, {"text": "We circle the new\nvalue propositions that we enable in both figures.", "metadata": {}}, {"text": "Our hardware\nsetups have additional key characteristics: They are resilient by de-\nfault to interruptions due to running in a decentralized fashion and\nthey enable the combination of more GPUs than cloud providers\noffer in a single node.", "metadata": {}}, {"text": "Currently, common hardware configurations\n(DGX) allow up to eight GPUs connected via NVLink, and with older\nhardware, only up to 4xT4s connected via PCIe at 10 GB/s between\nGPUs (with GC).", "metadata": {}}, {"text": "We were able to combine eight single GPU nodes\nfrom GC and LambdaLabs to create competing performance and\nprice setups without dedicated GPU interconnects.", "metadata": {}}, {"text": "A spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-\ndemand) in GC US, which makes it the best value proposition for\nthe low granularity NLP task.", "metadata": {}}, {"text": "It is followed by the 8xA10, which\nare 41% slower and 30% more expensive than the DGX-2 (Figure 15).", "metadata": {}}, {"text": "The 8xT4 experiments are even more expensive, as the internal\negress costs take up more than half of the costs, making them the\nworst value proposition.", "metadata": {}}, {"text": "However, for CV, we manage to provide\ntwo new offerings: First, the 8xA10, which is both 50% faster and 49%\ncheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-\n2, while being 37% slower (Figure 1).", "metadata": {}}, {"text": "The CV model can be scaled\nmore easily due to its initially high granularity, which makes the very\ncompetitive offering of $0.6/h per A10 from LambdaLabs an excellent\nvalue proposition.", "metadata": {}}, {"text": "However, while we only evaluated eight T4 GPUs\nfor our GC-based experiments, with a granularity of 5.19 (CV A-8\nin Figure 7b), there is ample space to scale even further.", "metadata": {}}, {"text": "It is important\nto note that LambdaLabs does not charge for any data egress, but GC\ndoes with $0.01/GB, and the 8xT4 experiment is still cheaper.", "metadata": {}}, {"text": "While\nLambdaLabs is often at capacity, Google Cloud positions itself as a\nhyperscaler with the advantage of rarely being at max occupancy.", "metadata": {}}, {"text": "We also evaluated the performance of the 4xT4 PyTorch DDP [26]\nfor CV with the best available multi-T4 node on GC (4xT4).", "metadata": {}}, {"text": "The NLP\nexperiments ran OOM.", "metadata": {}}, {"text": "Since the DDP 4xT4 runs on a single node, it\ncauses no interconnect costs and is priced at $0.96 per 1M samples at\nspot pricing, while our 8xT4 setup costs $1.77 per 1M samples (84%\nmore expensive).", "metadata": {}}, {"text": "However, the 8xT4 setup has a higher throughput\nof 262 SPS (26% faster) compared to the 4xT4 node (207 SPS).", "metadata": {}}, {"text": "This\nhigher speed is not available at the price point of the 4xT4 node.", "metadata": {}}, {"text": "Moreover, the 8xT4 setup has the potential for further scaling, which\nwe discussed in detail in Section 4.", "metadata": {}}, {"text": "In summary, the lower spot prices for older GPUs allow us to train\nmodels more cost-efficiently when task granularity allows it and get\nmore value per $ when training on the 8xT4 or 8xA10 compared to\nan DGX-2 node.", "metadata": {}}, {"text": "Combining multiple nodes with single GPUs with\nlower bandwidths enables scaling that was previously impossible to\nachieve without resorting to much more powerful GPUs.", "metadata": {}}, {"text": "Distributed\nspot instance pricing opens up a new value proposition compared\nto on-demand offerings that can even compete with the competitive\npricing of smaller cloud providers.", "metadata": {}}, {"text": "Spot VM Interruption Frequency.", "metadata": {}}, {"text": "While we used low spot\nprices as a cost-saving argument in our experiments, we did not\nelaborate on the most significant drawback - the possibility of being\nterminated by the cloud provider at any time.", "metadata": {}}, {"text": "There is already some\nresearch on how different cloud providers track the interruption fre-\nquency and can be used for varying workloads to achieve a positive\n$-per-throughput effect [24, 42, 43].", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Interruption affects three aspects: First, the interruption frequency\nis defined by AWS as the number of VMs terminated in the last 30\ndays, which is between 5 and 20% [3]. This value was not representa-\ntive during our experiments with any cloud provider, as we noticed\nthat it is highly dependent on the time of day of the zone.\nSecond, the time needed to setup a VM until training starts. The\nstartup time of a VM depends on the cloud provider (e.g., a preconfig-\nured image) and the one’s technology stack (e.g., Docker, Kubernetes,\nAnsible). In our experience, VM startup time ranges between sec-\nonds to minutes with manual deployment taking up to 10 minutes.\nAlthough startup time can be improved, model training typically\ntakes multiple hours or days, making it a less impactful optimization.\nThird, the time required for the new peer to synchronize the train-\ning state with other peers. In our experience, this took at worst two\nhivemind epochs due to the averaging starting before synchroniza-\ntion is finished. While it is possible to create a hivemind epoch that\nis short enough to prevent new peers from joining, this only hap-\npens with a low enough granularity where scaling is not beneficial\nanymore as we are mostly communication bound.\nFinally, while the VM setup and synchronization of the training\nstate take time, the interruption frequency significantly affects the fi-\nnal throughput. We faced difficulties acquiring even a single spot VM\nduring our GC experiments during daylight hours. This highlights\nthe need for systems like SkyPilot [43], which utilizes automation to\ndeploy spot instances across various clouds and zones. In our case,\nthe interruption frequency can be used as a penalty on the training\nthroughput, i.e., a 5% interruption frequency over the entire training\ntime means roughly a 5% slower training.\n8 LESSONS LEARNED\nWe find it important to summarize our findings more generically to\nprovide guidance for DL practitioners that want to perform distri-\nbuted spot training. These lessons are based on the Sections 3 to 6.\nSmall model training still scales. We have shown that models\nbetween 12M-560M parameters can be trained in a decentralized,\ndistributed fashion achieving a speedup of up to 4.37x on eight\nAmpere-GPUs. The limiting factor as to when a model is suitable\nfor (geo-)distributed training is the target batch size which all peers\nneed to accumulate until synchronization happens. We found a TBS\nof 32K suitable to not only train in a single zone, but even see a\nspeedup when using VMs in four different continents. As long as the\noptimizer can handle big-batch training and the dataset is big enough\nto accommodate large batches, the remaining issue to find the base\ngranularity of the model to decide how to scale it cost-effectively.\nFinally, we found that small models induce less traffic over larger\nmodels over time, even at a much higher averaging rate, making\nthem better suited for cost-efficient training than large models.\nEgress costs can take up most of the total cost.Egress pricing\nfor the NLP experiments overtook the spot and the on-demand costs\nof T4 GPUs when training on four continents or even in two zones.\nFor example, RoBERTaXLM’s high throughput and parameter count\nrequire more data to be sent between peers during averaging due to\nsmaller granularity. Under the current pricing models, AWS has the\nbest value for geo-distributed training, while GC and Azure are best\nat training in a single zone. The biggest cost-saving potential lies in\ncloud providers that do not charge for egress at all, like LambdaLabs.\nGranularity is important to evaluate scalability.We found\nthat the ratio between calculation and communication time, gran-\nularity, is the most important metric to track when deciding on\ndistributed training suitability. It enables us to compare the scala-\nbility potential between different models on the same hardware due\nto summarizing their model size and throughput ratio. Additionally,\nit gives a value to the cost-efficiency: With a granularity of exactly 1,\nthe potential speedup when doubling the number of VMs is, at best,\n1.33x due to halving the calculation time. However, with a granu-\nlarity of 10, the speedup with double the VMs is, at best, 1.83x due\nto the communication time playing a less significant role. With this,\nwe can estimate training performance with additional resources.\nGeo-distributed multi-cloud training is possible and is cost-\nefficient.Even with the current teething pains of Hivemind, we got a\nspeedup in all of our experimental setups of intra-zone, transatlantic,\nand intercontinental settings as long as the granularity of the task\npermitted it. Using older and cheaper Tesla GPUs at spot pricing\nis not only more cost-efficient than the DGX-2 offering, but even\ntrumps the competitive pricing model of LambdaLabs, all while in-\ncluding egress costs. Our network profiling showed that the current\ntraining limitations are not primarily the bandwidth but rather the\nintercontinental latency and the task’s granularity. If the granularity\nis already low at high bandwidth, it can only worsen when used in\na high latency, low bandwidth network. When considering both,\nestimating the potential cost-savings of investing in a multi-/hybrid-\ncloud scenario is possible.\n9 RELATED WORK\nDecentralized deep learning. Training with unreliable peers has\nbeen studied in a collaborative setting, resulting in the Distributed\nDeep Learning in Open Collaborations (DeDLOC) [17] algorithm,\non which the Hivemind framework [ 39] is based. It can interpo-\nlate between traditional distributed DL algorithms like parameter\nservers [25], decentralized SGD [27], or All-Reduce SGD [1]. We\nused the Hivemind framework for all of our experiments, as it pro-\nvided the base for training on spot instances in high latency, low\nbandwidth networks.\nSWARM [37] applies both previous techniques and adds model\nparallelism to the mix by creating pipelines between nodes and re-\nbalancing them in case of failures. The authors find a crucial insight\nin the \"square-cube\" law, which argues for better training scalability\nwith larger model sizes; as the size increases linearly, so does the com-\nmunication time, while the calculation time increases quadratically.\nWe add to that by analyzing distributed training for smaller model\nsizes that pose different trade-offs. We show that while the square-\ncube law still holds for increasing model sizes, under consideration\nof granularity, we can still train small models.\nDecentralized deep learning on heterogeneous hardware with\nslow interconnects can benefit the training of foundation models. To\nachieve this, model and pipeline parallelism can be used in addition\nto data-parallel training [45]. This is a complementary work to ours,\nsince we target smaller models and weaker hardware.\nDeep learning on spot instances. DeepSpotCloud [23] is a\nsystem that uses the AWS API to automatically migrate a DL task\nwith checkpointing whenever the spot instance is terminated. The\nauthors note that the volatility of GPU instance pricing and inter-\nruptions have a unique pattern compared to non-accelerated VMs,", "sentences": [{"text": "Interruption affects three aspects: First, the interruption frequency\nis defined by AWS as the number of VMs terminated in the last 30\ndays, which is between 5 and 20% [3].", "metadata": {}}, {"text": "This value was not representa-\ntive during our experiments with any cloud provider, as we noticed\nthat it is highly dependent on the time of day of the zone.", "metadata": {}}, {"text": "Second, the time needed to setup a VM until training starts.", "metadata": {}}, {"text": "The\nstartup time of a VM depends on the cloud provider (e.g., a preconfig-\nured image) and the one’s technology stack (e.g., Docker, Kubernetes,\nAnsible).", "metadata": {}}, {"text": "In our experience, VM startup time ranges between sec-\nonds to minutes with manual deployment taking up to 10 minutes.", "metadata": {}}, {"text": "Although startup time can be improved, model training typically\ntakes multiple hours or days, making it a less impactful optimization.", "metadata": {}}, {"text": "Third, the time required for the new peer to synchronize the train-\ning state with other peers.", "metadata": {}}, {"text": "In our experience, this took at worst two\nhivemind epochs due to the averaging starting before synchroniza-\ntion is finished.", "metadata": {}}, {"text": "While it is possible to create a hivemind epoch that\nis short enough to prevent new peers from joining, this only hap-\npens with a low enough granularity where scaling is not beneficial\nanymore as we are mostly communication bound.", "metadata": {}}, {"text": "Finally, while the VM setup and synchronization of the training\nstate take time, the interruption frequency significantly affects the fi-\nnal throughput.", "metadata": {}}, {"text": "We faced difficulties acquiring even a single spot VM\nduring our GC experiments during daylight hours.", "metadata": {}}, {"text": "This highlights\nthe need for systems like SkyPilot [43], which utilizes automation to\ndeploy spot instances across various clouds and zones.", "metadata": {}}, {"text": "In our case,\nthe interruption frequency can be used as a penalty on the training\nthroughput, i.e., a 5% interruption frequency over the entire training\ntime means roughly a 5% slower training.", "metadata": {}}, {"text": "8 LESSONS LEARNED\nWe find it important to summarize our findings more generically to\nprovide guidance for DL practitioners that want to perform distri-\nbuted spot training.", "metadata": {}}, {"text": "These lessons are based on the Sections 3 to 6.", "metadata": {}}, {"text": "Small model training still scales.", "metadata": {}}, {"text": "We have shown that models\nbetween 12M-560M parameters can be trained in a decentralized,\ndistributed fashion achieving a speedup of up to 4.37x on eight\nAmpere-GPUs.", "metadata": {}}, {"text": "The limiting factor as to when a model is suitable\nfor (geo-)distributed training is the target batch size which all peers\nneed to accumulate until synchronization happens.", "metadata": {}}, {"text": "We found a TBS\nof 32K suitable to not only train in a single zone, but even see a\nspeedup when using VMs in four different continents.", "metadata": {}}, {"text": "As long as the\noptimizer can handle big-batch training and the dataset is big enough\nto accommodate large batches, the remaining issue to find the base\ngranularity of the model to decide how to scale it cost-effectively.", "metadata": {}}, {"text": "Finally, we found that small models induce less traffic over larger\nmodels over time, even at a much higher averaging rate, making\nthem better suited for cost-efficient training than large models.", "metadata": {}}, {"text": "Egress costs can take up most of the total cost.Egress pricing\nfor the NLP experiments overtook the spot and the on-demand costs\nof T4 GPUs when training on four continents or even in two zones.", "metadata": {}}, {"text": "For example, RoBERTaXLM’s high throughput and parameter count\nrequire more data to be sent between peers during averaging due to\nsmaller granularity.", "metadata": {}}, {"text": "Under the current pricing models, AWS has the\nbest value for geo-distributed training, while GC and Azure are best\nat training in a single zone.", "metadata": {}}, {"text": "The biggest cost-saving potential lies in\ncloud providers that do not charge for egress at all, like LambdaLabs.", "metadata": {}}, {"text": "Granularity is important to evaluate scalability.We found\nthat the ratio between calculation and communication time, gran-\nularity, is the most important metric to track when deciding on\ndistributed training suitability.", "metadata": {}}, {"text": "It enables us to compare the scala-\nbility potential between different models on the same hardware due\nto summarizing their model size and throughput ratio.", "metadata": {}}, {"text": "Additionally,\nit gives a value to the cost-efficiency: With a granularity of exactly 1,\nthe potential speedup when doubling the number of VMs is, at best,\n1.33x due to halving the calculation time.", "metadata": {}}, {"text": "However, with a granu-\nlarity of 10, the speedup with double the VMs is, at best, 1.83x due\nto the communication time playing a less significant role.", "metadata": {}}, {"text": "With this,\nwe can estimate training performance with additional resources.", "metadata": {}}, {"text": "Geo-distributed multi-cloud training is possible and is cost-\nefficient.Even with the current teething pains of Hivemind, we got a\nspeedup in all of our experimental setups of intra-zone, transatlantic,\nand intercontinental settings as long as the granularity of the task\npermitted it.", "metadata": {}}, {"text": "Using older and cheaper Tesla GPUs at spot pricing\nis not only more cost-efficient than the DGX-2 offering, but even\ntrumps the competitive pricing model of LambdaLabs, all while in-\ncluding egress costs.", "metadata": {}}, {"text": "Our network profiling showed that the current\ntraining limitations are not primarily the bandwidth but rather the\nintercontinental latency and the task’s granularity.", "metadata": {}}, {"text": "If the granularity\nis already low at high bandwidth, it can only worsen when used in\na high latency, low bandwidth network.", "metadata": {}}, {"text": "When considering both,\nestimating the potential cost-savings of investing in a multi-/hybrid-\ncloud scenario is possible.", "metadata": {}}, {"text": "9 RELATED WORK\nDecentralized deep learning.", "metadata": {}}, {"text": "Training with unreliable peers has\nbeen studied in a collaborative setting, resulting in the Distributed\nDeep Learning in Open Collaborations (DeDLOC) [17] algorithm,\non which the Hivemind framework [ 39] is based.", "metadata": {}}, {"text": "It can interpo-\nlate between traditional distributed DL algorithms like parameter\nservers [25], decentralized SGD [27], or All-Reduce SGD [1].", "metadata": {}}, {"text": "We\nused the Hivemind framework for all of our experiments, as it pro-\nvided the base for training on spot instances in high latency, low\nbandwidth networks.", "metadata": {}}, {"text": "SWARM [37] applies both previous techniques and adds model\nparallelism to the mix by creating pipelines between nodes and re-\nbalancing them in case of failures.", "metadata": {}}, {"text": "The authors find a crucial insight\nin the \"square-cube\" law, which argues for better training scalability\nwith larger model sizes;", "metadata": {}}, {"text": "as the size increases linearly, so does the com-\nmunication time, while the calculation time increases quadratically.", "metadata": {}}, {"text": "We add to that by analyzing distributed training for smaller model\nsizes that pose different trade-offs.", "metadata": {}}, {"text": "We show that while the square-\ncube law still holds for increasing model sizes, under consideration\nof granularity, we can still train small models.", "metadata": {}}, {"text": "Decentralized deep learning on heterogeneous hardware with\nslow interconnects can benefit the training of foundation models.", "metadata": {}}, {"text": "To\nachieve this, model and pipeline parallelism can be used in addition\nto data-parallel training [45].", "metadata": {}}, {"text": "This is a complementary work to ours,\nsince we target smaller models and weaker hardware.", "metadata": {}}, {"text": "Deep learning on spot instances.", "metadata": {}}, {"text": "DeepSpotCloud [23] is a\nsystem that uses the AWS API to automatically migrate a DL task\nwith checkpointing whenever the spot instance is terminated.", "metadata": {}}, {"text": "The\nauthors note that the volatility of GPU instance pricing and inter-\nruptions have a unique pattern compared to non-accelerated VMs,", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "and solve this by using intercontinental provisioning. We noticed\nthe same trends of high interruption ratios in our experiments. How-\never, we have shown that geo-distributed training is possible until\ngranularity permits it, which poses a possibility for ever-migrating\ntraining between continents without checkpointing.\nAmazon Sagemaker [14] is an AWS service that allows to perform\nML under budget constraints. For training, it supports spot VM migra-\ntion until a cost threshold is reached by checkpointing the progress.\nHowever, it lacks the option of training on multiple spot VMs. It can\ndo either spot instance training on DGX-like nodes or combine mul-\ntiple on-demand nodes with PyTorch DDP (or similar), but not both.\nThis eliminates the potential of accelerating the training process\nwith more GPUs that do not fit a single spot-provisioned hypervisor.\nThe analysis by Yang et al. [42] investigates maximizing a tar-\nget accuracy from a spot pricing versus time perspective. Linear\nprogramming was used to decide how to provision the VMs with\ndifferent cost-utility trade-offs. While this shows the potential of uti-\nlizing multiple clouds and continents for non-distributed tasks, we\nevaluated the distributed spot training problem from the throughput,\ncost, and model size perspective on different hardware setups. By in-\ncluding our insights, their technique for scheduling on spot instances\ncould be adapted to optimize the total throughput of all peers.\nSkypilot [43] is a broker system where users can submit their\nhardware requirements, and it tries to provision the necessary re-\nsources on any supported cloud. It features a preemption analysis\nthat counts the number of interruptions in a zone and can decide to\nmigrate whenever they cross a certain threshold. We have shown\nthat multi-, hybrid-cloud, and geo-distributed training is possible,\nand by combining our insights, it would open up auto-migrated,\ndecentralized DL training for the best spot prices in the world.\n10 CONCLUSION\nThis paper analyzes multi- and hybrid-cloud training in a decentral-\nized fashion on spot instances. We define the lower bounds of model\nsizes that can be scaled cost-efficiently using the granularity metric to\nestimate their suitability for distributed training in low-bandwidth,\nhigh-latency situations. We show that training on multiple cloud\nproviders and four continents still scales with additional compute\nresources. Alternatively to the current use of spot instances in DL,\nwe show the potential of using spot instances in a distributed, de-\ncentralized way by being more cost-efficient with eight T4 instances\nover a DGX-2 from the same cloud provider while paying additional\negress costs. Finally, we provide an intuition about where costs in\nsuch a training scenario come from and how different model sizes\nfrom CV and NLP affect throughput and costs. Our work empowers\npractitioners to utilize spot-priced instances for distributed deep\nlearning with relatively small models. Our insights show some po-\ntential that can further improve distributed training performance,\nsuch as optimizers with higher minibatch sizes and improvements\nregarding the communication time with, e.g., better compression.\n11 APPENDIX: ASR CASE STUDY\nWe perform a case study on Automatic Speech Recognition (ASR)\nto showcase spot training on weaker GPUs. Whisper [34] is a state-\nof-the-art ASR model trained on 680,000 hours of labeled data to\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nT4 Count\n0\n10\n20\n30Samples per Second\nTBS\n256\n512\n1024\n(a) Throughput\n2\n 4\n 8\n 2\n 4\n 8\n 2\n 4\n 8\nT4 Count\n0\n100\n200\n300Time in Seconds\nTBS=256\nTBS=512 TBS=1024\n1.86 0.42 0.57\n3.80\n1.36 0.40\n7.47\n2.91\n1.17\nCalculation\nCommunication (b) Granuarity\nFigure 16: WhisperSmall performance with varying TBS.\n0\n 10\n 20\n 30\n 40\n 50\nCost in $ per 1M Samples\n0\n20\n40Samples per Second\nDDP 4xT4 DDP 4xT4\nDDP 2xT4 DDP 2xT4\nA100 A100\n8xT4 8xT4\n4xT4 4xT4\n1xT4\n1xT4\nInstance Type\nSpot\nOn-Demand\nFigure 17: Cost to throughput tradeoff for WhisperSmall at\nTBS=1024 with different instance types. Our training setups\n(circled) provide mixed result of being slightly faster and more\nexpensive than comparable, centralized DDP offering.\ntranscribe audio. It features different sizes, from 37.8M to 1.5B pa-\nrameters, and was trained with a minibatch size of 256. We use the\nCommonvoice [11] dataset, preprocessed to Log-Mel spectrograms.\nIn our distributed experiments, we start with a TBS of 256 and in-\ncrease to 512 and 1024 to combat potential granularity issues. Due to\nmemory constraints, only three model sizes (Tiny, Base, Small) were\ntrainable on the T4 GPU. Unfortunately, the original TBS of 256 was\nnot large enough to train the relatively small models due to their\nsmall granularity (0.04, 0.14 and 0.57 at 8xT4, respectively) with no\nperformance benefits. The only model showing scaling potential is\nWhisperSmall, with a granularity of 1.8 with 2xT4. However, when\nscaling the target batch size to 512 and 1024, we see some benefit\nover the single GPU runs for the WhisperSmall model (Figure 16).\nBy effectively increasing the amount of computation by the factors\nof 2 and 4, we can generate a speedup of1.27× and 2.2× with 8xT4’s\nfor the TBS 512 and 1024, respectively. When compared to other\nhardware setups, our A100 80GB GPU and the best multi-T4 GPU\non GC (4xT4) with Pytorch DDP (Figure 17) have almost double the\nthroughput at 46 SPS and are slightly slower at 24 SPS, respectively,\ncompared to our 8xT4 setup which runs at 28 SPS. This outcome is\nnot surprising due to the generational leap in architecture for the\nA100 and the slower interconnect with our 8xT4 experiments com-\npared to a single 4xT4 node (see Section 3 for a detailed throughput\nanalysis). The proposed cost-throughput ratio is mixed: the A100 is\nat $12.19/1M samples, the DDP 4xT4 is at $8.41/1M, and our 8xT4 is\nat $14.53/1M. Our proposed setup is slightly more expensive than the\nA100, and it will not scale beyond eight T4 GPUs due a granularity\nat 1.17, leaving the A100 as the fastest and the DDP 4xT4 setup as the\ncheaper but slower alternative. Despite these results, our proposed\nsetup has several benefits, including resilience for spot interruptions,\ninterruption-free migration to the lowest cloud prices, and the pos-\nsibility to scale the GPU count up as long as granularity permits it.\nACKNOWLEDGMENTS\nThis work is funded in part by the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) - 392214008.", "sentences": [{"text": "and solve this by using intercontinental provisioning.", "metadata": {}}, {"text": "We noticed\nthe same trends of high interruption ratios in our experiments.", "metadata": {}}, {"text": "How-\never, we have shown that geo-distributed training is possible until\ngranularity permits it, which poses a possibility for ever-migrating\ntraining between continents without checkpointing.", "metadata": {}}, {"text": "Amazon Sagemaker [14] is an AWS service that allows to perform\nML under budget constraints.", "metadata": {}}, {"text": "For training, it supports spot VM migra-\ntion until a cost threshold is reached by checkpointing the progress.", "metadata": {}}, {"text": "However, it lacks the option of training on multiple spot VMs.", "metadata": {}}, {"text": "It can\ndo either spot instance training on DGX-like nodes or combine mul-\ntiple on-demand nodes with PyTorch DDP (or similar), but not both.", "metadata": {}}, {"text": "This eliminates the potential of accelerating the training process\nwith more GPUs that do not fit a single spot-provisioned hypervisor.", "metadata": {}}, {"text": "The analysis by Yang et al.", "metadata": {}}, {"text": "[42] investigates maximizing a tar-\nget accuracy from a spot pricing versus time perspective.", "metadata": {}}, {"text": "Linear\nprogramming was used to decide how to provision the VMs with\ndifferent cost-utility trade-offs.", "metadata": {}}, {"text": "While this shows the potential of uti-\nlizing multiple clouds and continents for non-distributed tasks, we\nevaluated the distributed spot training problem from the throughput,\ncost, and model size perspective on different hardware setups.", "metadata": {}}, {"text": "By in-\ncluding our insights, their technique for scheduling on spot instances\ncould be adapted to optimize the total throughput of all peers.", "metadata": {}}, {"text": "Skypilot [43] is a broker system where users can submit their\nhardware requirements, and it tries to provision the necessary re-\nsources on any supported cloud.", "metadata": {}}, {"text": "It features a preemption analysis\nthat counts the number of interruptions in a zone and can decide to\nmigrate whenever they cross a certain threshold.", "metadata": {}}, {"text": "We have shown\nthat multi-, hybrid-cloud, and geo-distributed training is possible,\nand by combining our insights, it would open up auto-migrated,\ndecentralized DL training for the best spot prices in the world.", "metadata": {}}, {"text": "10 CONCLUSION\nThis paper analyzes multi- and hybrid-cloud training in a decentral-\nized fashion on spot instances.", "metadata": {}}, {"text": "We define the lower bounds of model\nsizes that can be scaled cost-efficiently using the granularity metric to\nestimate their suitability for distributed training in low-bandwidth,\nhigh-latency situations.", "metadata": {}}, {"text": "We show that training on multiple cloud\nproviders and four continents still scales with additional compute\nresources.", "metadata": {}}, {"text": "Alternatively to the current use of spot instances in DL,\nwe show the potential of using spot instances in a distributed, de-\ncentralized way by being more cost-efficient with eight T4 instances\nover a DGX-2 from the same cloud provider while paying additional\negress costs.", "metadata": {}}, {"text": "Finally, we provide an intuition about where costs in\nsuch a training scenario come from and how different model sizes\nfrom CV and NLP affect throughput and costs.", "metadata": {}}, {"text": "Our work empowers\npractitioners to utilize spot-priced instances for distributed deep\nlearning with relatively small models.", "metadata": {}}, {"text": "Our insights show some po-\ntential that can further improve distributed training performance,\nsuch as optimizers with higher minibatch sizes and improvements\nregarding the communication time with, e.g., better compression.", "metadata": {}}, {"text": "11 APPENDIX: ASR CASE STUDY\nWe perform a case study on Automatic Speech Recognition (ASR)\nto showcase spot training on weaker GPUs.", "metadata": {}}, {"text": "Whisper [34] is a state-\nof-the-art ASR model trained on 680,000 hours of labeled data to\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\nT4 Count\n0\n10\n20\n30Samples per Second\nTBS\n256\n512\n1024\n(a) Throughput\n2\n 4\n 8\n 2\n 4\n 8\n 2\n 4\n 8\nT4 Count\n0\n100\n200\n300Time in Seconds\nTBS=256\nTBS=512 TBS=1024\n1.86 0.42 0.57\n3.80\n1.36 0.40\n7.47\n2.91\n1.17\nCalculation\nCommunication (b) Granuarity\nFigure 16: WhisperSmall performance with varying TBS.", "metadata": {}}, {"text": "0\n 10\n 20\n 30\n 40\n 50\nCost in $ per 1M Samples\n0\n20\n40Samples per Second\nDDP 4xT4 DDP 4xT4\nDDP 2xT4 DDP 2xT4\nA100 A100\n8xT4 8xT4\n4xT4 4xT4\n1xT4\n1xT4\nInstance Type\nSpot\nOn-Demand\nFigure 17: Cost to throughput tradeoff for WhisperSmall at\nTBS=1024 with different instance types.", "metadata": {}}, {"text": "Our training setups\n(circled) provide mixed result of being slightly faster and more\nexpensive than comparable, centralized DDP offering.", "metadata": {}}, {"text": "transcribe audio.", "metadata": {}}, {"text": "It features different sizes, from 37.8M to 1.5B pa-\nrameters, and was trained with a minibatch size of 256.", "metadata": {}}, {"text": "We use the\nCommonvoice [11] dataset, preprocessed to Log-Mel spectrograms.", "metadata": {}}, {"text": "In our distributed experiments, we start with a TBS of 256 and in-\ncrease to 512 and 1024 to combat potential granularity issues.", "metadata": {}}, {"text": "Due to\nmemory constraints, only three model sizes (Tiny, Base, Small) were\ntrainable on the T4 GPU.", "metadata": {}}, {"text": "Unfortunately, the original TBS of 256 was\nnot large enough to train the relatively small models due to their\nsmall granularity (0.04, 0.14 and 0.57 at 8xT4, respectively) with no\nperformance benefits.", "metadata": {}}, {"text": "The only model showing scaling potential is\nWhisperSmall, with a granularity of 1.8 with 2xT4.", "metadata": {}}, {"text": "However, when\nscaling the target batch size to 512 and 1024, we see some benefit\nover the single GPU runs for the WhisperSmall model (Figure 16).", "metadata": {}}, {"text": "By effectively increasing the amount of computation by the factors\nof 2 and 4, we can generate a speedup of1.27× and 2.2× with 8xT4’s\nfor the TBS 512 and 1024, respectively.", "metadata": {}}, {"text": "When compared to other\nhardware setups, our A100 80GB GPU and the best multi-T4 GPU\non GC (4xT4) with Pytorch DDP (Figure 17) have almost double the\nthroughput at 46 SPS and are slightly slower at 24 SPS, respectively,\ncompared to our 8xT4 setup which runs at 28 SPS.", "metadata": {}}, {"text": "This outcome is\nnot surprising due to the generational leap in architecture for the\nA100 and the slower interconnect with our 8xT4 experiments com-\npared to a single 4xT4 node (see Section 3 for a detailed throughput\nanalysis).", "metadata": {}}, {"text": "The proposed cost-throughput ratio is mixed: the A100 is\nat $12.19/1M samples, the DDP 4xT4 is at $8.41/1M, and our 8xT4 is\nat $14.53/1M.", "metadata": {}}, {"text": "Our proposed setup is slightly more expensive than the\nA100, and it will not scale beyond eight T4 GPUs due a granularity\nat 1.17, leaving the A100 as the fastest and the DDP 4xT4 setup as the\ncheaper but slower alternative.", "metadata": {}}, {"text": "Despite these results, our proposed\nsetup has several benefits, including resilience for spot interruptions,\ninterruption-free migration to the lowest cloud prices, and the pos-\nsibility to scale the GPU count up as long as granularity permits it.", "metadata": {}}, {"text": "ACKNOWLEDGMENTS\nThis work is funded in part by the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) - 392214008.", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "REFERENCES\n[1] [n.d.]. Horovod: fast and easy distributed deep learning in TensorFlow,\nauthor=Sergeev, Alexander and Del Balso, Mike, journal=arXiv preprint\narXiv:1802.05799, year=2018. ([n. d.]).\n[2] 2023. Amazon A WS. Accessed: 19 May 2023, aws.amazon.com.\n[3] 2023. Amazon AWS Spot Pricing. https://aws.amazon.com/blogs/compute/new-\namazon-ec2-spot-pricing/. Accessed: 2023-09-27.\n[4] 2023. Backblaze. https://backblaze.com/. Accessed: 2023-10-05.\n[5] 2023. Google Cloud. Accessed: 19 May 2023, cloud.google.com.\n[6] 2023. Google Cloud Region Picker. https://cloud.withgoogle.com/region-picker/.\nAccessed: 2023-10-05.\n[7] 2023. Hivemind GAC Issue. https://github.com/learning-at-home/hivemind/\nissues/566. Accessed: 2023-10-05.\n[8] 2023. LambdaLabs. Accessed: 19 May 2023, lambdalabs.com.\n[9] 2023. Microsoft Azure. Accessed: 19 May 2023, portal.azure.com.\n[10] Alex Aizman, Gavin Maltby, and Thomas Breuel. 2019. High Performance I/O\nFor Large Scale Deep Learning. In2019 IEEE International Conference on Big Data\n(Big Data). 5965–5967. https://doi.org/10.1109/BigData47090.2019.9005703\n[11] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,\nJosh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor\nWeber. 2019. Common voice: A massively-multilingual speech corpus. arXiv\npreprint arXiv:1912.06670 (2019).\n[12] Alexander Borzunov, Max Ryabinin, Tim Dettmers, Quentin Lhoest, Lucile\nSaulnier, Michael Diskin, and Yacine Jernite. 2022. Training Transformers\nTogether. InNeurIPS 2021 Competitions and Demonstrations Track. PMLR, 335–342.\n[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning\nat Scale. arXiv:1911.02116 [cs.CL]\n[14] Piali Das, Nikita Ivkin, Tanya Bansal, Laurence Rouesnel, Philip Gautier, Zohar\nKarnin, Leo Dirac, Lakshmi Ramakrishnan, Andre Perunicic, Iaroslav Shcherbatyi,\nWilton Wu, Aida Zolic, Huibin Shen, Amr Ahmed, Fela Winkelmolen, Miroslav\nMiladinovic, Cedric Archembeau, Alex Tang, Bhaskar Dutt, Patricia Grao, and\nKumar Venkateswar. 2020. Amazon SageMaker Autopilot: A White Box AutoML\nSolution at Scale. In Proceedings of the Fourth International Workshop on Data\nManagement for End-to-End Machine Learning (Portland, OR, USA) (DEEM’20).\nAssociation for Computing Machinery, New York, NY, USA, Article 2, 7 pages.\nhttps://doi.org/10.1145/3399579.3399870\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In2009 IEEE conference on computer\nvision and pattern recognition. Ieee, 248–255.\n[16] Tim Dettmers. 2016. 8-Bit Approximations for Parallelism in Deep Learning.\narXiv:1511.04561 [cs.NE]\n[17] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton\nSinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov,\nAlbert Villanova del Moral, et al. 2021. Distributed Deep Learning In Open Collab-\norations. Advances in Neural Information Processing Systems34 (2021), 7879–7897.\n[18] O Elharrouss, Y Akbari, N Almaadeed, and S Al-Maadeed. [n.d.]. Backbones-\nreview: Feature extraction networks for deep learning and deep reinforcement\nlearning approaches. arXiv 2022. arXiv preprint arXiv:2206.08016 ([n. d.]).\n[19] Anne C Elster and Tor A Haugdahl. 2022. NVIDIA Hopper GPU and Grace CPU\nHighlights. Computing in Science & Engineering 24, 2 (2022), 95–100.\n[20] Wikimedia Foundation. 2023. \"Wikimedia Downloads\" . https:\n//dumps.wikimedia.org\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. InProceedings of the IEEE conference on computer\nvision and pattern recognition. 770–778.\n[22] Kai Hwang. 1992. Advanced Computer Architecture: Paral-\nlelism,Scalability,Programmability (1st ed.). McGraw-Hill Higher Education.\n[23] Kyungyong Lee and Myungjun Son. 2017. DeepSpotCloud: Leverag-\ning Cross-Region GPU Spot Instances for Deep Learning. In 2017 IEEE\n10th International Conference on Cloud Computing (CLOUD) . 98–105.\nhttps://doi.org/10.1109/CLOUD.2017.21\n[24] Sungjae Lee, Jaeil Hwang, and Kyungyong Lee. 2022. SpotLake: Di-\nverse Spot Instance Dataset Archive Service. In 2022 IEEE Interna-\ntional Symposium on Workload Characterization (IISWC) . 242–255.\nhttps://doi.org/10.1109/IISWC55918.2022.00029\n[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja\nJosifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distri-\nbuted machine learning with the parameter server. In11th {USENIX} Symposium\non Operating Systems Design and Implementation ({OSDI} 14). 583–598.\n[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,\nAdam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch\ndistributed: Experiences on accelerating data parallel training. arXiv preprint\narXiv:2006.15704 (2020).\n[27] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.\n2017. Can decentralized algorithms outperform centralized algorithms? a case\nstudy for decentralized parallel stochastic gradient descent.Advances in neural\ninformation processing systems 30 (2017).\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,\nand Saining Xie. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 11976–11986.\n[30] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor\nBittorf, et al. 2020. MLPerf Training Benchmark. Proceedings of Machine Learning\nand Systems 2 (2020), 336–349.\n[31] Petar Maymounkov and David Mazieres. 2002. Kademlia: A peer-to-peer\ninformation system based on the xor metric. In Peer-to-Peer Systems: First\nInternationalWorkshop, IPTPS 2002 Cambridge, MA, USA, March 7–8, 2002 Revised\nPapers. Springer, 53–65.\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library.Advances\nin neural information processing systems 32 (2019).\n[33] Gustavo Portella, Genaina N Rodrigues, Eduardo Nakano, and Alba CMA Melo.\n2019. Statistical analysis of Amazon EC2 cloud pricing models.Concurrency and\nComputation: Practice and Experience 31, 18 (2019), e4451.\n[34] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and\nIlya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision.\nIn Proceedings of the 40th International Conference on Machine Learning (Proceed-\nings of Machine Learning Research), Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202.\nPMLR, 28492–28518. https://proceedings.mlr.press/v202/radford23a.html\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.\nDeepspeed: System optimizations enable training deep learning models with\nover 100 billion parameters. InProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. 3505–3506.\n[36] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\nShuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:\nDemocratizing Billion-Scale Model Training. arXiv:2101.06840 [cs.DC]\n[37] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov.\n2023. SWARM Parallelism: Training Large Models Can Be Surprisingly\nCommunication-Efficient. arXiv preprint arXiv:2301.11913 (2023).\n[38] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady\nPekhimenko. 2021. Moshpit SGD: Communication-Efficient Decentralized\nTraining on Heterogeneous Unreliable Devices. InAdvances in Neural Information\nProcessing Systems , Vol. 34. https://proceedings.neurips.cc/paper/2021/file/\n97275a23ca44226c9964043c8462be96-Paper.pdf\n[39] Learning@home team. 2020. Hivemind: a Library for Decentralized Deep\nLearning. https://github.com/learning-at-home/hivemind.\n[40] Chathurika S. Wickramasinghe, Daniel L. Marino, and Milos Manic. 2021. ResNet\nAutoencoders for Unsupervised Feature Learning From High-Dimensional\nData: Deep Models Resistant to Performance Degradation.IEEE Access 9 (2021),\n40511–40520. https://doi.org/10.1109/ACCESS.2021.3064819\n[41] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi,\nand Ludwig Schmidt. 2023. Stable and low-precision training for large-scale\nvision-language models. arXiv preprint arXiv:2304.13013 (2023).\n[42] Sheng Yang, Samir Khuller, Sunav Choudhary, Subrata Mitra, and Kanak Mahadik.\n2022. Scheduling ML Training on Unreliable Spot Instances. InProceedings of the\n14th IEEE/ACM International Conference on Utility and Cloud Computing Compan-\nion (Leicester, United Kingdom)(UCC ’21). Association for Computing Machinery,\nNew York, NY, USA, Article 29, 8 pages. https://doi.org/10.1145/3492323.3495594\n[43] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhard-\nwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal,\nScott Shenker, and Ion Stoica. 2023. SkyPilot: An Intercloud Broker for\nSky Computing. In 20th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 23) . USENIX Association, Boston, MA, 437–455.\nhttps://www.usenix.org/conference/nsdi23/presentation/yang-zongheng\n[44] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh\nBhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learning: Training bert in 76 minutes.\narXiv preprint arXiv:1904.00962 (2019).\n[45] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen,\nPercy S Liang, Christopher Re, and Ce Zhang. 2022. Decentralized training\nof foundation models in heterogeneous environments. Advances in Neural\nInformation Processing Systems 35 (2022), 25464–25477.\n[46] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks.arXiv\npreprint arXiv:1605.07146 (2016).", "sentences": [{"text": "REFERENCES\n[1] [n.d.].", "metadata": {}}, {"text": "Horovod: fast and easy distributed deep learning in TensorFlow,\nauthor=Sergeev, Alexander and Del Balso, Mike, journal=arXiv preprint\narXiv:1802.05799, year=2018.", "metadata": {}}, {"text": "([n.", "metadata": {}}, {"text": "d.]).", "metadata": {}}, {"text": "[2] 2023.", "metadata": {}}, {"text": "Amazon A WS.", "metadata": {}}, {"text": "Accessed: 19 May 2023, aws.amazon.com.", "metadata": {}}, {"text": "[3] 2023.", "metadata": {}}, {"text": "Amazon AWS Spot Pricing.", "metadata": {}}, {"text": "https://aws.amazon.com/blogs/compute/new-\namazon-ec2-spot-pricing/.", "metadata": {}}, {"text": "Accessed: 2023-09-27.", "metadata": {}}, {"text": "[4] 2023.", "metadata": {}}, {"text": "Backblaze.", "metadata": {}}, {"text": "https://backblaze.com/.", "metadata": {}}, {"text": "Accessed: 2023-10-05.", "metadata": {}}, {"text": "[5] 2023.", "metadata": {}}, {"text": "Google Cloud.", "metadata": {}}, {"text": "Accessed: 19 May 2023, cloud.google.com.", "metadata": {}}, {"text": "[6] 2023.", "metadata": {}}, {"text": "Google Cloud Region Picker.", "metadata": {}}, {"text": "https://cloud.withgoogle.com/region-picker/.", "metadata": {}}, {"text": "Accessed: 2023-10-05.", "metadata": {}}, {"text": "[7] 2023.", "metadata": {}}, {"text": "Hivemind GAC Issue.", "metadata": {}}, {"text": "https://github.com/learning-at-home/hivemind/\nissues/566.", "metadata": {}}, {"text": "Accessed: 2023-10-05.", "metadata": {}}, {"text": "[8] 2023.", "metadata": {}}, {"text": "LambdaLabs.", "metadata": {}}, {"text": "Accessed: 19 May 2023, lambdalabs.com.", "metadata": {}}, {"text": "[9] 2023.", "metadata": {}}, {"text": "Microsoft Azure.", "metadata": {}}, {"text": "Accessed: 19 May 2023, portal.azure.com.", "metadata": {}}, {"text": "[10] Alex Aizman, Gavin Maltby, and Thomas Breuel.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "High Performance I/O\nFor Large Scale Deep Learning.", "metadata": {}}, {"text": "In2019 IEEE International Conference on Big Data\n(Big Data).", "metadata": {}}, {"text": "5965–5967.", "metadata": {}}, {"text": "https://doi.org/10.1109/BigData47090.2019.9005703\n[11] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,\nJosh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor\nWeber.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Common voice: A massively-multilingual speech corpus.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:1912.06670 (2019).", "metadata": {}}, {"text": "[12] Alexander Borzunov, Max Ryabinin, Tim Dettmers, Quentin Lhoest, Lucile\nSaulnier, Michael Diskin, and Yacine Jernite.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Training Transformers\nTogether.", "metadata": {}}, {"text": "InNeurIPS 2021 Competitions and Demonstrations Track.", "metadata": {}}, {"text": "PMLR, 335–342.", "metadata": {}}, {"text": "[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Unsupervised Cross-lingual Representation Learning\nat Scale.", "metadata": {}}, {"text": "arXiv:1911.02116 [cs.CL]\n[14] Piali Das, Nikita Ivkin, Tanya Bansal, Laurence Rouesnel, Philip Gautier, Zohar\nKarnin, Leo Dirac, Lakshmi Ramakrishnan, Andre Perunicic, Iaroslav Shcherbatyi,\nWilton Wu, Aida Zolic, Huibin Shen, Amr Ahmed, Fela Winkelmolen, Miroslav\nMiladinovic, Cedric Archembeau, Alex Tang, Bhaskar Dutt, Patricia Grao, and\nKumar Venkateswar.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Amazon SageMaker Autopilot: A White Box AutoML\nSolution at Scale.", "metadata": {}}, {"text": "In Proceedings of the Fourth International Workshop on Data\nManagement for End-to-End Machine Learning (Portland, OR, USA) (DEEM’20).", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, Article 2, 7 pages.", "metadata": {}}, {"text": "https://doi.org/10.1145/3399579.3399870\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.", "metadata": {}}, {"text": "2009.", "metadata": {}}, {"text": "Imagenet:\nA large-scale hierarchical image database.", "metadata": {}}, {"text": "In2009 IEEE conference on computer\nvision and pattern recognition.", "metadata": {}}, {"text": "Ieee, 248–255.", "metadata": {}}, {"text": "[16] Tim Dettmers.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "8-Bit Approximations for Parallelism in Deep Learning.", "metadata": {}}, {"text": "arXiv:1511.04561 [cs.NE]\n[17] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton\nSinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov,\nAlbert Villanova del Moral, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Distributed Deep Learning In Open Collab-\norations.", "metadata": {}}, {"text": "Advances in Neural Information Processing Systems34 (2021), 7879–7897.", "metadata": {}}, {"text": "[18] O Elharrouss, Y Akbari, N Almaadeed, and S Al-Maadeed.", "metadata": {}}, {"text": "[n.d.].", "metadata": {}}, {"text": "Backbones-\nreview: Feature extraction networks for deep learning and deep reinforcement\nlearning approaches.", "metadata": {}}, {"text": "arXiv 2022.", "metadata": {}}, {"text": "arXiv preprint arXiv:2206.08016 ([n.", "metadata": {}}, {"text": "d.]).", "metadata": {}}, {"text": "[19] Anne C Elster and Tor A Haugdahl.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "NVIDIA Hopper GPU and Grace CPU\nHighlights.", "metadata": {}}, {"text": "Computing in Science & Engineering 24, 2 (2022), 95–100.", "metadata": {}}, {"text": "[20] Wikimedia Foundation.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "\"Wikimedia Downloads\" .", "metadata": {}}, {"text": "https:\n//dumps.wikimedia.org\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Deep residual\nlearning for image recognition.", "metadata": {}}, {"text": "InProceedings of the IEEE conference on computer\nvision and pattern recognition.", "metadata": {}}, {"text": "770–778.", "metadata": {}}, {"text": "[22] Kai Hwang.", "metadata": {}}, {"text": "1992.", "metadata": {}}, {"text": "Advanced Computer Architecture: Paral-\nlelism,Scalability,Programmability (1st ed.).", "metadata": {}}, {"text": "McGraw-Hill Higher Education.", "metadata": {}}, {"text": "[23] Kyungyong Lee and Myungjun Son.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "DeepSpotCloud: Leverag-\ning Cross-Region GPU Spot Instances for Deep Learning.", "metadata": {}}, {"text": "In 2017 IEEE\n10th International Conference on Cloud Computing (CLOUD) .", "metadata": {}}, {"text": "98–105.", "metadata": {}}, {"text": "https://doi.org/10.1109/CLOUD.2017.21\n[24] Sungjae Lee, Jaeil Hwang, and Kyungyong Lee.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "SpotLake: Di-\nverse Spot Instance Dataset Archive Service.", "metadata": {}}, {"text": "In 2022 IEEE Interna-\ntional Symposium on Workload Characterization (IISWC) .", "metadata": {}}, {"text": "242–255.", "metadata": {}}, {"text": "https://doi.org/10.1109/IISWC55918.2022.00029\n[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja\nJosifovski, James Long, Eugene J Shekita, and Bor-Yiing Su.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Scaling distri-\nbuted machine learning with the parameter server.", "metadata": {}}, {"text": "In11th {USENIX} Symposium\non Operating Systems Design and Implementation ({OSDI} 14).", "metadata": {}}, {"text": "583–598.", "metadata": {}}, {"text": "[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,\nAdam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Pytorch\ndistributed: Experiences on accelerating data parallel training.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2006.15704 (2020).", "metadata": {}}, {"text": "[27] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Can decentralized algorithms outperform centralized algorithms?", "metadata": {}}, {"text": "a case\nstudy for decentralized parallel stochastic gradient descent.Advances in neural\ninformation processing systems 30 (2017).", "metadata": {}}, {"text": "[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "RoBERTa: A\nRobustly Optimized BERT Pretraining Approach.", "metadata": {}}, {"text": "arXiv:1907.11692 [cs.CL]\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,\nand Saining Xie.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "A convnet for the 2020s.", "metadata": {}}, {"text": "In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition.", "metadata": {}}, {"text": "11976–11986.", "metadata": {}}, {"text": "[30] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor\nBittorf, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "MLPerf Training Benchmark.", "metadata": {}}, {"text": "Proceedings of Machine Learning\nand Systems 2 (2020), 336–349.", "metadata": {}}, {"text": "[31] Petar Maymounkov and David Mazieres.", "metadata": {}}, {"text": "2002.", "metadata": {}}, {"text": "Kademlia: A peer-to-peer\ninformation system based on the xor metric.", "metadata": {}}, {"text": "In Peer-to-Peer Systems: First\nInternationalWorkshop, IPTPS 2002 Cambridge, MA, USA, March 7–8, 2002 Revised\nPapers.", "metadata": {}}, {"text": "Springer, 53–65.", "metadata": {}}, {"text": "[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Pytorch: An imperative style, high-performance deep learning library.Advances\nin neural information processing systems 32 (2019).", "metadata": {}}, {"text": "[33] Gustavo Portella, Genaina N Rodrigues, Eduardo Nakano, and Alba CMA Melo.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Statistical analysis of Amazon EC2 cloud pricing models.Concurrency and\nComputation: Practice and Experience 31, 18 (2019), e4451.", "metadata": {}}, {"text": "[34] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and\nIlya Sutskever.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Robust Speech Recognition via Large-Scale Weak Supervision.", "metadata": {}}, {"text": "In Proceedings of the 40th International Conference on Machine Learning (Proceed-\nings of Machine Learning Research), Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol.", "metadata": {}}, {"text": "202.", "metadata": {}}, {"text": "PMLR, 28492–28518.", "metadata": {}}, {"text": "https://proceedings.mlr.press/v202/radford23a.html\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Deepspeed: System optimizations enable training deep learning models with\nover 100 billion parameters.", "metadata": {}}, {"text": "InProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining.", "metadata": {}}, {"text": "3505–3506.", "metadata": {}}, {"text": "[36] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\nShuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "ZeRO-Offload:\nDemocratizing Billion-Scale Model Training.", "metadata": {}}, {"text": "arXiv:2101.06840 [cs.DC]\n[37] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "SWARM Parallelism: Training Large Models Can Be Surprisingly\nCommunication-Efficient.", "metadata": {}}, {"text": "arXiv preprint arXiv:2301.11913 (2023).", "metadata": {}}, {"text": "[38] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady\nPekhimenko.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Moshpit SGD: Communication-Efficient Decentralized\nTraining on Heterogeneous Unreliable Devices.", "metadata": {}}, {"text": "InAdvances in Neural Information\nProcessing Systems , Vol.", "metadata": {}}, {"text": "34.", "metadata": {}}, {"text": "https://proceedings.neurips.cc/paper/2021/file/\n97275a23ca44226c9964043c8462be96-Paper.pdf\n[39] Learning@home team.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Hivemind: a Library for Decentralized Deep\nLearning.", "metadata": {}}, {"text": "https://github.com/learning-at-home/hivemind.", "metadata": {}}, {"text": "[40] Chathurika S.", "metadata": {}}, {"text": "Wickramasinghe, Daniel L.", "metadata": {}}, {"text": "Marino, and Milos Manic.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "ResNet\nAutoencoders for Unsupervised Feature Learning From High-Dimensional\nData: Deep Models Resistant to Performance Degradation.IEEE Access 9 (2021),\n40511–40520.", "metadata": {}}, {"text": "https://doi.org/10.1109/ACCESS.2021.3064819\n[41] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi,\nand Ludwig Schmidt.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Stable and low-precision training for large-scale\nvision-language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2304.13013 (2023).", "metadata": {}}, {"text": "[42] Sheng Yang, Samir Khuller, Sunav Choudhary, Subrata Mitra, and Kanak Mahadik.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Scheduling ML Training on Unreliable Spot Instances.", "metadata": {}}, {"text": "InProceedings of the\n14th IEEE/ACM International Conference on Utility and Cloud Computing Compan-\nion (Leicester, United Kingdom)(UCC ’21).", "metadata": {}}, {"text": "Association for Computing Machinery,\nNew York, NY, USA, Article 29, 8 pages.", "metadata": {}}, {"text": "https://doi.org/10.1145/3492323.3495594\n[43] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhard-\nwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal,\nScott Shenker, and Ion Stoica.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "SkyPilot: An Intercloud Broker for\nSky Computing.", "metadata": {}}, {"text": "In 20th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 23) .", "metadata": {}}, {"text": "USENIX Association, Boston, MA, 437–455.", "metadata": {}}, {"text": "https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng\n[44] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh\nBhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Large batch optimization for deep learning: Training bert in 76 minutes.", "metadata": {}}, {"text": "arXiv preprint arXiv:1904.00962 (2019).", "metadata": {}}, {"text": "[45] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen,\nPercy S Liang, Christopher Re, and Ce Zhang.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Decentralized training\nof foundation models in heterogeneous environments.", "metadata": {}}, {"text": "Advances in Neural\nInformation Processing Systems 35 (2022), 25464–25477.", "metadata": {}}, {"text": "[46] Sergey Zagoruyko and Nikos Komodakis.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Wide residual networks.arXiv\npreprint arXiv:1605.07146 (2016).", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}]}