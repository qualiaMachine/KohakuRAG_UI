{"document_id": "luccioni2025c", "title": "Misinformation by Omission: The Need for More Environmental Transparency in AI", "text": "arXiv:2506.15572v1  [cs.CY]  18 Jun 2025\nMisinformation by Omission:\nThe Need for More Environmental Transparency in AI\nSasha Luccioni1,*, Boris Gamazaychikov2, Theo Alves da Costa3, and Emma Strubell4\n1Hugging Face, Montreal, Canada\n2Salesforce, Paris, France\n3Ekimetrics, Paris, France\n4Carnegie Mellon University, School of Computer Science, Pittsburgh, USA\n*sasha.luccioni@huggingface.co\nABSTRACT\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational\npower and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has\ndecreased, meaning that the users of these technologies have little to no information about their resource demands and\nsubsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying\nAI’s environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping\npublic understanding of AI’s environmental impacts, tracing their origins and their spread in both the media and scientific\npublications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and\nconclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate\nnegative impacts in the future.\nIntroduction\nAI-powered tools and systems are becoming increasingly ubiquitous, reshaping human behavior with corresponding impacts\nto our socioeconomic systems. While the companies that develop and deploy AI-driven technology strongly emphasize AI’s\npositive impacts (real and speculative)1–3, the negative impacts are typically left unspoken. These are often uncovered by\nresearchers or a concerned public who audit AI systems, driven by the desire to understand the impacts of these systems on\nsociety and the planet. Among the many impacts that have been analyzed in recent years are: algorithmic discrimination and\nbias4, 5, the use of AI in military applications6, the threat of AI to democracy7, 8, and environmental impacts9, 10.\nHoning in on the latter, researchers and activists alike have been sounding the alarm on the increasingly unsustainable\ntrends in energy and natural resource consumption arising from the data centers and devices used to train and deploy AI models\nwhich are growing in size and complexity11–15. These resources range from the rare earth minerals necessary to manufacture\ncomputing hardware, the energy needed to power the very tangible “cloud” computation that underpins AI systems, the water\nneeded for cooling and hardware manufacturing, and the greenhouse gases (GHGs) emitted at every stage of this process.\nDespite these rising costs, there exists minimal and often no data quantifying these impacts. When data does exist, it often\nlacks sufficient accessibility, detail and scope to enable effective decision-making or analysis, impeding impact assessment,\nmitigation, forecasting, and even basic understanding by the public at large.\nAs a result, researchers, investors, companies and policymakers are left to attempt best-effort approximations given limited\ndata availability. In some cases, these estimates can be wildly flawed due to lack of critical prerequisite data, understanding or\nexpertise. In other cases, estimates may be taken out of the careful, qualified contexts in which they were originally presented,\nleading to misinterpretation and in some cases severely inaccurate generalizations. The increasingly high stakes political and\neconomic contexts surrounding climate change and AI severely compound this challenge; mistakes and misinterpretation\ndevolve into misinformation as estimates are repeatedly shared and transformed through subsequent analyses, adopted as\naccurate measures and spread as trending posts through social media and the news, finally arriving on the desks of decision-\nmakers. The resulting misconceptions harm all stakeholders: policymakers and the public are unable to make informed\ndecisions, and AI technology developers suffer from negative perceptions arising from overestimates of their social harms,\nfurther exacerbating their lack of disclosure. In this paper, we aim to elucidate some common myths surrounding AI’s\nenvironmental impacts, explore the pitfalls that led to the emergence of those myths, and propose recommendations for\nremedying this challenge in the future.\n1\n\nRelated Work\nApproaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as\nthese systems have grown more impactful and widely deployed in user-facing applications. Initial studies, such as that of\nStrubell et al., underscored the environmental cost of training Transformer-based language models12. Research done in the\nfollowing years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI\nmodels including GPT-3, T5, Meena, and Switch Transformer, providing new estimates16 and expanding the scope of analysis\nbeyond model training to account for operational and embodied emissions 17, improving methodology for software energy\nmeasurement18, and a lifecycle approach to assessing emissions from model training and deployment 11. Wu et al. further\nadvanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,19. Most\nrecently, Luccioni, Jernite, and Strubell 20 pioneered the AI inference impact methodology, revealing generative architectures as\nparticularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference\nimpacts. These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach\nfor comparing the inference efficiency of AI models22.\nAbove and beyond energy considerations, Li et al. 23 expanded the scope of AI environmental impact measurement by\nestimating the water footprint of GPT-3 based on publicly available information, whereas Han et al. 24 assessed the public\nhealth toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants\nequivalent to more than 10,000 round trips by car between Los Angeles and New York City. In another significant advancement,\nGoogle’s recent TPU lifecycle assessment25 offered the most comprehensive cradle-to-grave environmental analysis of AI\nhardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure,\nsignificantly extending existing environmental impact models. Building on many of these approaches, Morrison et al. 26\nperformed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development,\nand training, enhancing the accuracy of these metrics through the use of granular underlying data.\nThe breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating\nAI’s environmental impacts, and the many different perspectives that exist in this space. Whereas several standardized\napproaches have been proposed to measure different aspects of AI’s requirements in terms of energy and water, as well as\nthe emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology\nand standards that cover all dimensions. In the next section, we examine how this translates into decreased environmental\ntransparency in the AI industry via an empirical analysis of AI models over time.\nEnvironmental Transparency Trends\nWhile there has been progress in developing more robust methodologies for measuring AI’s environmental impacts, the broader\nAI industry has paradoxically been trending in the opposite direction, disclosing less information over time. In order to quantify\nthis trend, we analyze Epoch AI’s Notable AI Models dataset27, which tracks information on “models that were state of the\nart, highly cited, or otherwise historically notable”, with respect to transparency about the environmental impacts of those\nmodels. We examine the level of environmental impact transparency for each model based on key information from the Epoch\nAI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content\n(e.g., paper, model card, announcement). We select the time period starting in 2010 as this is the beginning of the modern\n“deep learning era” (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed,\nincluding all 754 models from 2010 to the first quarter of 2025. Our analysis, shown in Figure 1, reveals substantial variation\nin environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others\nprovide no information at all regarding their approach.\nOverall, we find that models exhibit three transparency categories:\n• Direct Disclosure: Developers explicitly reported energy or GHG emissions. Note that this category includes methodolo-\ngies ranging from estimation (e.g., using hardware TDP, country average carbon intensity) to measurements (i.e., using\ntools like CodeCarbon).\n• Indirect Disclosure: Developers provided training compute data or released their model weights, allowing external\nestimates of training or inference impacts.\n• No Disclosure: Environmental impact data was not publicly released and estimation approaches (as noted in Indirect\nDisclosure) were not possible.\nFrom 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact\nof model training (ranging from 0 to 33% each year); no direct environmental impact data was released during this period.\nThis is expected, given that AI models of that era required significantly less compute and resource usage transparency was not\n2/12\n\nFigure 1. Environmental Impact Transparency of Notable AI Models by Release Year27\nyet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of\ntraining data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and\nopen-weights model releases became more common. This period includes the the work of Strubell et al. 12, Luccioni11 and\nothers. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some\ndegree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially\ncatalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and\neven the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental\ndisclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the\nline between research and commercial deployment became increasingly blurred.\nFigure 2. Environmental Impact Transparency of LLM Usage – OpenRouter28 (May 2025)\nBeyond the long term trend, zooming in to examine recent AI model usage data helps illustrate today’s environmental\nimpact transparency conditions. OpenRouter28, a widely-used API platform for LLMs, publicly shares data on model traffic\nincluding top 20 models by month, and the number of tokens running through every model. May 2025 data (Figure 2) indicates\nthat of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1,\nDeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by\nreleasing their model weights to enable efficiency analysis). In terms of token usage, 84% of LLM usage is through models\nwith no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure. This indicates that the\nmajority of users who interact with LLMs have no information about their environmental impacts, and cannot make informed\ndecisions based on model efficiency or carbon intensity.\nFrom the limited data that is publicly available, we can observe significant disparities in energy use and emissions across\nmodels. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh\n(LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of\nelectricity across training locations). Inference workloads also show wide variation depending on model size, architecture and\n3/12\n\n[Image page=3 idx=1 name=Im1.png] Size: 1408x360, Data: 20466 bytes\n\ntask type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R\nPlus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).\nThese ranges highlight not only the scale of potential impacts, but also the pressing need for more standardized and transparent\nreporting to enable meaningful comparisons.\nInvestigating the Urban Legends of AI’s Environmental Impacts\nMaking sustainably-minded decisions when using AI systems requires having the necessary information about different aspects\nof their development and deployment. While there are empirical studies focusing on AI’s environmental impacts, such as those\ncited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size,\narchitecture, optimizations, hardware, location, setup, system) that they are not representative of. This fuels misinformation,\nundermines scientific research, and can result in decisions that are not grounded in facts29. In the paragraphs below, we address\nsome of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore\ntheir potential for spreading environmental misinformation.\nTraining an AI model emits as much CO2 as five cars in their lifetimes\nAmong the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,12 which estimated\nthe monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP)\nmodels of that era, including the first generation of large language models. This analysis included both the costs to train\nseveral individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer\nneural network architecture30 that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e.\nidentifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model\ndevelopment through both a case study of the energy required for them to develop a model published in the previous year, and\nby estimating the energy required to automate that process using an approach called neural architecture search (NAS) based\non figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. 31\nIn the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions\nintensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons)\nCO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.\nThe research article was written for a specialized audience of AI and NLP researchers, who would have the background\nknowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and\nfeaturing a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked\nup by numerous media outlets (including MIT Technology Review32 and Forbes33). The “five cars” number has since been\nmisinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of\narchitectures, training approaches and electricity sources used for powering AI model training; the original article reports AI\ntraining workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity),\nand AI model training more broadly often requires even less energy and corresponding emissions.\nFurther, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much\nless frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis\nto reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational)\nsignificantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today’s\ngenerative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However,\nwhile the “five cars” estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI\ntraining workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically\nexceed the “five cars” estimate: Google reports that training their open source Gemma family of language models emitted\n1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3\nfamily of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.\nA request to ChatGPT consumes ten times more energy than a Google search\nAnother often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours\n(Wh) of energy, which is \"ten times more than a Google search\". This figure is often quoted in the press36, 37 and in industry\nreports38. Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet’s Chairman John\nHennessy during a 2023 interview with Reuters, in which he said that “having an exchange with AI known as a large language\nmodel likely cost 10 times more than a standard keyword search” 39. This remark was used was the basis of an estimate\npublished in October 2023 of “approximately 3 Wh per LLM interaction” 40, with the Google search number taken from a\n2009 blog post from Google that stated that “Queries vary in degree of difficulty, but for the average query [...] this amounts\nto 0.0003 kWh of energy per search” 41. This number is misleading for several reasons. First, Hennessy has no relation to\n4/12\n\nOpenAI or Microsoft (which provides the compute for OpenAI’s services), so the comment he made was based on secondhand\ninformation. Second, even if Hennessy’s comparison were accurate, basing the search estimate on a figure that is 16 years\nold — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current\nTransformer-based models — is also bound to amplify the inaccuracy of the estimate.\nTo understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11,\n2025, that appear when searching for “ChatGPT energy consumption” on Google News. For each article, we noted whether\nit mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by\nacknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal\nthat 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the\nsources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy\nthan a Google search 42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number\nof American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on\nthe energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural\nnetwork architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when\naddressing this subject47, stating that the true figures remain unknown. It is also noteworthy that among these articles, 9% also\nrelay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.\nFigure 3. Analysis of media articles discussing ChatGPT energy consumption.\nAI can reduce 10% of global emissions\nWhile the numbers around AI’s negative environmental impacts can be misinterpreted and taken out of context, so, too, can the\npotential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale. One\nrecurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021\nBoston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and\ntechnology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions\nof the European Union”48. The same number appears in a more recent BCG report from 2023, which was commissioned by\nGoogle and published ahead of COP2649. The reasoning behind the 5-10% reduction estimate is unclear and the underlying\ncalculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and\nusing AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more\ndetail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project\nnumbers to a global scale.\nApplying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—\nin fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous\ncalculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence\nanalysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T\nL.148050 or WBCSD guidance on avoided emissions51. And yet, these numbers were picked up in research52 and the media,\nused as evidence that the potential of AI to stop climate change is overwhelmingly positive53, 54. While AI undoubtedly has\npotential positive applications in sectors ranging from transportation to agriculture to energy55, these global generalizations can\nbe misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the\nbeliefs that the benefits of AI will outweigh its costs56.\n5/12\n\nThe lack of transparency around AI’s environmental impacts can have far-reaching consequences, ranging from specific\nestimates taken out of context and blown out of proportion, to proxies becoming adopted by press and policymakers in the\nabsence of more reliable figures. In the next section, we discuss a potential solution to this situation by proposing a set of\nmetrics that different stakeholders can measure and report to bring more clarity to the extent of AI’s environmental impacts.\nHow to improve environmental impact disclosures in AI\nOpacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed\nprocurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack\nthe information necessary to develop evidence-based regulations. This opacity also generates cascading effects throughout\nvalue chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments. Furthermore,\nthe absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that\ncould drive efficiency improvements. Perhaps most critically, this lack of transparency undermines accountability mechanisms,\nmaking it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress\ntoward sustainability goals.\nThis section explores how comprehensive environmental transparency can address these challenges through four intercon-\nnected pathways:\n1. Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and\ndeployment;\n2. Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustain-\nability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user\nenterprises;\n3. Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful compar-\nisons; and\n4. Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.\nMeasurement and Disclosure As the starting point of AI development, AI researchers and developers are able to gather\nempirical measurements from the systems they create at different steps of the model lifecycle. When developing models from\nscratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools\nlike Code Carbon57 or no-code tools like Green Algorithms 58. When using or adapting existing models, performance and\nefficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production.\nFor instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different\ntasks, which can also be adapted for specific contexts and datasets. These metrics should be reported in model cards 59 and\nscientific publications with complete methodological transparency, including hardware specifications, geographic locations,\nelectricity sources, measurement uncertainties, and allocation methodologies. This empirical foundation enables downstream\norganizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts\nthrough peer-reviewed publication of methodologies and results. AI providers across the entire value chain, including cloud\ninfrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency\nwith granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related\nenvironmental impacts. Government and public sector organizations should mandate transparency in all AI procurements,\nrequire open data for publicly funded research, and align AI deployments with existing net zero commitments.\nOrganizational Implementation and Processes As AI adoption accelerates, organizations should implement comprehensive\nframeworks to assess, measure, and integrate AI’s environmental impacts into existing sustainability management systems using\nstructured approaches tailored to their specific contexts and risk profiles. The materiality assessment framework should aim to\nestablish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct\ntiers of analysis intensity. Organizations developing AI systems utilizing open-source models on their infrastructure should\nimplement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and\norganization-wide aggregations. Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial\nmodels or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by\nincorporating environmental disclosure requirements into procurement processes and contractual agreements60. Specifically,\norganizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services\nunder consideration. These environmental metrics should be systematically integrated into organizations’ GHG accounting\nframeworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and\nunmodeled factors.\n6/12\n\nStandards, Verification and Assurance Environmental AI disclosures require robust verification frameworks to ensure\naccuracy and prevent greenwashing, necessitating new assurance standards adapted to AI’s rapid evolution, distributed compute,\nand complex value chains. While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway\nacross organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD\n(Organization for Economic Cooperation and Development). These bodies are well-positioned to develop standardized\napproaches for stakeholders ranging from developers to governments. Given AI’s transnational nature, coordination and\nharmonization of these efforts is essential. Without alignment, implementation may diverge across jurisdictions, creating\nfurther confusion in the market. However, as formal standards may take years to materialize, interim ad hoc methods (such as\nthose outlined above) can provide valuable insights and help shape the eventual development of formal methodologies. These\nAI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly\nregarding the GHG Protocol’s treatment of electricity emissions measurement. The current allowance for market-based\naccounting enables companies to significantly under-report their actual AI-related emissions through renewable energy\ncertificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility 56. For AI\nservices consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure\nenvironmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured\nthrough market mechanisms.\nPolicy Frameworks and Reporting Environmental transparency documentation is already commonplace for private orga-\nnizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate\ndisclosure requirements in the US, or local and state-level climate disclosure laws. However, policymakers should incorporate\nadditional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability\nReporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage,\nand a transition plan aligned with the Paris Agreement61, particularly as this aligns with existing provisions in the EU AI Act.\nNon-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria\nto incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices. For\norganizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators,\nsemiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements. These\ncould include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure.\nEnforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets\nand funding on compliance with disclosure standards.\nConclusion\nThe current trend toward reduced transparency around AI’s environmental impact contributes to misinformation and hinders\ninformed decision-making across all levels, from individual researchers and developers to organizations and policymakers. This\ndeclining transparency is particularly troubling given AI’s escalating environmental impacts amid global climate concerns and\nlooming planetary boundaries. While competition is frequently cited to justify opacity, other competitive industries, such as\nfood (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between\ntransparency and competition is achievable. Reversing the trend toward opacity in AI environmental reporting is essential for\ninformed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms\nemerge that may alter these impacts. As members of the AI community committed to addressing the climate crisis, we aim to\nensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this\ngoal.\n7/12\n\nReferences\n1. Luers, A. et al. Will ai accelerate or delay the race to net-zero emissions? Nature 628, 718–720 (2024).\n2. Kshirsagar, M. et al. Becoming good at AI for good. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\nSociety, 664–673 (2021).\n3. DeepMind. Using ai to fight climate change (2009).\n4. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. In Ethics of data and analytics , 254–264 (Auerbach\nPublications, 2022).\n5. Buolamwini, J. & Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In\nConference on fairness, accountability and transparency, 77–91 (PMLR, 2018).\n6. Rivera, J.-P.et al. Escalation risks from language models in military and diplomatic decision-making. In Proceedings of\nthe 2024 ACM Conference on Fairness, Accountability, and Transparency, 836–898 (2024).\n7. Manheim, K. & Kaplan, L. Artificial intelligence: Risks to privacy and democracy. Yale JL & Tech.21, 106 (2019).\n8. Summerfield, C. et al. How will advanced ai systems impact democracy? arXiv preprint arXiv:2409.06729 (2024).\n9. Crawford, K. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence (Yale University Press,\n2021).\n10. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).\n11. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Estimating the carbon footprint of BLOOM, a 176b parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n12. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n13. Luccioni, A. S. & Hernandez-Garcia, A. Counting carbon: A survey of factors influencing the emissions of machine\nlearning. arXiv preprint arXiv:2302.08476 (2023).\n14. Dodge, J. et al. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, 1877–1894 (2022).\n15. Ligozat, A.-L., Lefèvre, J., Bugeau, A. & Combaz, J. Unraveling the hidden environmental impacts of AI solutions for\nenvironment. arXiv preprint arXiv:2110.11822 (2021).\n16. Patterson, D. et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n17. Gupta, U. et al. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA), 854–867 (IEEE, 2021).\n18. Cao, Q., Lal, Y . K., Trivedi, H., Balasubramanian, A. & Balasubramanian, N. IrEne: Interpretable energy prediction for\ntransformers. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), 2145–2157, DOI: 10.18653/v1/2021.acl-long.167 (Association for Computational Linguistics, Online,\n2021).\n19. Wu, C.-J. et al. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint\narXiv:2111.00364 (2021).\n20. Luccioni, S., Jernite, Y . & Strubell, E. Power hungry processing: Watts driving the cost of ai deployment? InThe 2024\nACM Conference on Fairness, Accountability, and Transparency, FAccT ’24, 85–99, DOI: 10.1145/3630106.3658542\n(ACM, 2024).\n21. Luccioni, S. & Gamazaychikov, B. AI Energy Score Leaderboard. https://huggingface.co/spaces/AIEnergyScore/\nLeaderboard (2025). Hugging Face Spaces.\n22. Luccioni, S. et al. Light bulbs have energy ratings—so why can’t ai chatbots? Nature 632, 736–738 (2024).\n23. Li, P., Yang, J., Islam, M. A. & Ren, S. Making ai less\" thirsty\": Uncovering and addressing the secret water footprint of ai\nmodels. arXiv preprint arXiv:2304.03271 (2023).\n24. Han, Y ., Wu, Z., Li, P., Wierman, A. & Ren, S. The unpaid toll: Quantifying the public health impact of ai.arXiv preprint\narXiv:2412.06288 (2024).\n8/12\n\n25. Schneider, I. et al. Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025).\n2502.01671.\n26. Morrison, J. et al. Holistically evaluating the environmental impact of creating language models. arXiv preprint\narXiv:2503.05804 (2025).\n27. Epoch AI. Data on notable ai models (2024). Accessed: 2025-04-06.\n28. OpenRouter. Openrouter leaderboard rankings. https://openrouter.ai/rankings?view=month (2025). Accessed: 2025-06-03.\n29. Lovins, A. B. Artificial intelligence meets natural stupidity: Managing the risks (2025).\n30. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n31. So, D., Le, Q. & Liang, C. The evolved transformer. In Chaudhuri, K. & Salakhutdinov, R. (eds.) Proceedings of the\n36th International Conference on Machine Learning, vol. 97 of Proceedings of Machine Learning Research, 5877–5886\n(PMLR, 2019).\n32. Hao, K. Training a single ai model can emit as much carbon as five cars in their lifetimes. MIT technology Rev. 75, 103\n(2019).\n33. Toews, R. Deep learning’s carbon emissions problem. Forbes (2020).\n34. Gemma Team et al. Gemma 2: Improving open language models at a practical size (2024). 2408.00118.\n35. Meta. Llama 3.1 Model Card. https://huggingface.co/meta-llama/Llama-3.1-8B (2024). Hugging Face Model Card.\n36. Kerr, D. & NPR. AI brings soaring emissions for Google and Microsoft, a major contributor to climate change. NPR, July\n(2024).\n37. Chen, S. How much energy will ai really consume? the good, the bad and the unknown. Nature 639, 22–24 (2025).\n38. Aljbour, J., Wilson, T. & Patel, P. Powering intelligence: Analyzing artificial intelligence and data center energy\nconsumption. EPRI White Pap. no. 3002028905 (2024).\n39. Dastin, J. & Nellis, S. Focus: For tech giants, ai like bing and bard poses billion-dollar search problem. Reuters (2023).\n40. De Vries, A. The growing energy footprint of artificial intelligence. Joule 7, 2191–2194 (2023).\n41. Hölzle, U. Powering a google search (2009).\n42. Inês Trindade Pereira. ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume? Euronews\n(online). Accessed: 2025-06-01.\n43. Hamish van der Ven. AI is bad for the environment, and the problem is bigger than energy consumption. The Conversation\n(online). Accessed: 2025-06-01.\n44. Spencer Kimball. Data centers powering artificial intelligence could use more electricity than entire cities. CNBC (online).\nAccessed: 2025-06-01.\n45. James O’Donnell. DeepSeek might not be such good news for energy after all. MIT Technology Review (online). Accessed:\n2025-06-01.\n46. Berry Zwets. Researchers claim to cut energy consumption AI 95 percent. Techzine (online). Accessed: 2025-06-01.\n47. Adam Clark Estes. Should you feel guilty about using AI? V ox (online). Accessed: 2025-06-01.\n48. Degot, C., Duranton, S., Frédeau, M. & Hutchinson, R. Reduce carbon and costs with the power of ai. Boston Consult.\nGroup 26 (2021).\n49. Dannouni, A. et al. Accelerating climate action with ai. Boston Consult. Group Special Rep. Google (2023).\n50. ITU. Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions\nimpact greenhouse gas emissions of other sectors. Tech. Rep. ITU-T L.1480, ITU (2022). Accessed: 2025-06-01.\n51. WBCSD. Guidance on Avoided Emissions. Tech. Rep., WBCSD (2023). Accessed: 2025-06-01.\n52. Das, K. P. & Chandra, J. A survey on artificial intelligence for reducing the climate footprint in healthcare. Energy Nexus\n9, 100167 (2023).\n53. The Environment. Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).\n54. Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023).\n55. Rolnick, D. et al. Tackling climate change with machine learning. ACM Comput. Surv. (CSUR) 55, 1–96 (2022).\n9/12\n\n56. Ambrose, J. & Hern, A. Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).\n57. Schmidt, V .et al. Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).\n58. Lannelongue, L., Grealey, J. & Inouye, M. Green algorithms: Quantifying the carbon footprint of computation. Adv. Sci.\n2100707 (2021).\n59. Mitchell, M. et al. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and\ntransparency, 220–229 (2019).\n60. Luccioni, S. & Gamazaychikov, B. AI Models Hiding Their Energy Footprint? Here’s What You Can Do (2025).\n61. Leal Filho, W. et al. European sustainability reporting standards: An assessment of requirements and preparedness of eu\ncompanies. J. environmental management 380, 125008 (2025).\n62. Gamazaychikov, B. Unveiling salesforce’s blueprint for sustainable ai: Where responsibility meets innovation (2023).\n63. Touvron, H. et al. Llama: Open and efficient foundation language models (2023). 2302.13971.\n64. Mesnard, T. et al. Gemma: Open models based on gemini research and technology (2024). 2403.08295.\n65. Meta. Llama 4 Model Card. https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct (2025). Hugging\nFace Model Card.\n66. Meta. Llama 3 Model Card. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md (2024). GitHub\nRepository.\nAuthor contributions statement\nB.G. conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis. All authors wrote,\nedited and reviewed the manuscript.\n10/12\n\nAppendix\nTable 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel Organization Energy Consumption (MWh) GHG Emissions (tCO2e)\nOLMo 20M 26 Ai2 0.8 0.3\nCodeGen 350M 62 Salesforce 71 6\nLlama 7B 63 Meta 356 14\nBLOOM 11 Big Science 520 30\nT5 16 Google 85.7 47\nOLMo 2 13B 26 Ai2 157 101\nGemma 2B + 9B 64 Google ? 131\nGPT-3 16 OpenAI 1,287 552\nLlama 4 Scout 65 Meta 3,500 1,354\nLlama 3 70B 66 Meta ? 1,900\nLlama 3.1 405B 35 Meta ? 8,930\nMax/Min Variance: 4,375 29,767\n11/12\n\nTable 2. Range of Inference Energy Use21 (Representative Models Displayed)\nModel Organization GPU Energy for 1k Queries (Wh) Task\nbert-tiny-finetuned-squadv2 mrm8488 0.06 Extractive QA\nGIST-all-MiniLM-L6-v2 avsolatorio 0.11 Sentence Similarity\ndynamic_tinybert Intel 0.21 Extractive QA\ndistilbert-imdb lvwerra 0.22 Text Classification\nquestion_answering_v2 Falconsai 0.23 Extractive QA\nResnet 18 Microsoft 0.30 Image Classification\nyolos-tiny hustvl 1.00 Object Detection\nVision Perceiver Conv Google 2.64 Image Classification\nSFR-Embedding-Mistral Salesforce 5.22 Sentence Similarity\nyolos-base hustvl 7.98 Object Detection\nGemma 7B Google 18.90 Text Generation\nT5 11b Google 27.79 Text Classification\nphi-4 Microsoft 28.74 Text Generation\nT5 11b Google 178.13 Extractive QA\nMitsua Diffusion One Mitsua 186.81 Image Generation\nMixtral 8x7B Mistral 615.39 Text Generation\nStable Diffusion XL Base Stability AI 1,639.85 Image Generation\nLlama 3 70B Meta 1,719.66 Text Generation\nQwen2.5 72B Qwen 1,869.55 Text Generation\nCommand-R Plus Cohere 3,426.38 Text Generation\nMax/Min Variance: 57,106\n12/12", "metadata": {"url": "https://arxiv.org/pdf/2506.15572", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "arXiv:2506.15572v1  [cs.CY]  18 Jun 2025\nMisinformation by Omission:\nThe Need for More Environmental Transparency in AI\nSasha Luccioni1,*, Boris Gamazaychikov2, Theo Alves da Costa3, and Emma Strubell4\n1Hugging Face, Montreal, Canada\n2Salesforce, Paris, France\n3Ekimetrics, Paris, France\n4Carnegie Mellon University, School of Computer Science, Pittsburgh, USA\n*sasha.luccioni@huggingface.co\nABSTRACT\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational\npower and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has\ndecreased, meaning that the users of these technologies have little to no information about their resource demands and\nsubsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying\nAI’s environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping\npublic understanding of AI’s environmental impacts, tracing their origins and their spread in both the media and scientific\npublications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and\nconclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate\nnegative impacts in the future.\nIntroduction\nAI-powered tools and systems are becoming increasingly ubiquitous, reshaping human behavior with corresponding impacts\nto our socioeconomic systems. While the companies that develop and deploy AI-driven technology strongly emphasize AI’s\npositive impacts (real and speculative)1–3, the negative impacts are typically left unspoken. These are often uncovered by\nresearchers or a concerned public who audit AI systems, driven by the desire to understand the impacts of these systems on\nsociety and the planet. Among the many impacts that have been analyzed in recent years are: algorithmic discrimination and\nbias4, 5, the use of AI in military applications6, the threat of AI to democracy7, 8, and environmental impacts9, 10.\nHoning in on the latter, researchers and activists alike have been sounding the alarm on the increasingly unsustainable\ntrends in energy and natural resource consumption arising from the data centers and devices used to train and deploy AI models\nwhich are growing in size and complexity11–15. These resources range from the rare earth minerals necessary to manufacture\ncomputing hardware, the energy needed to power the very tangible “cloud” computation that underpins AI systems, the water\nneeded for cooling and hardware manufacturing, and the greenhouse gases (GHGs) emitted at every stage of this process.\nDespite these rising costs, there exists minimal and often no data quantifying these impacts. When data does exist, it often\nlacks sufficient accessibility, detail and scope to enable effective decision-making or analysis, impeding impact assessment,\nmitigation, forecasting, and even basic understanding by the public at large.\nAs a result, researchers, investors, companies and policymakers are left to attempt best-effort approximations given limited\ndata availability. In some cases, these estimates can be wildly flawed due to lack of critical prerequisite data, understanding or\nexpertise. In other cases, estimates may be taken out of the careful, qualified contexts in which they were originally presented,\nleading to misinterpretation and in some cases severely inaccurate generalizations. The increasingly high stakes political and\neconomic contexts surrounding climate change and AI severely compound this challenge; mistakes and misinterpretation\ndevolve into misinformation as estimates are repeatedly shared and transformed through subsequent analyses, adopted as\naccurate measures and spread as trending posts through social media and the news, finally arriving on the desks of decision-\nmakers. The resulting misconceptions harm all stakeholders: policymakers and the public are unable to make informed\ndecisions, and AI technology developers suffer from negative perceptions arising from overestimates of their social harms,\nfurther exacerbating their lack of disclosure. In this paper, we aim to elucidate some common myths surrounding AI’s\nenvironmental impacts, explore the pitfalls that led to the emergence of those myths, and propose recommendations for\nremedying this challenge in the future.\n1", "sentences": [{"text": "arXiv:2506.15572v1  [cs.CY]  18 Jun 2025\nMisinformation by Omission:\nThe Need for More Environmental Transparency in AI\nSasha Luccioni1,*, Boris Gamazaychikov2, Theo Alves da Costa3, and Emma Strubell4\n1Hugging Face, Montreal, Canada\n2Salesforce, Paris, France\n3Ekimetrics, Paris, France\n4Carnegie Mellon University, School of Computer Science, Pittsburgh, USA\n*sasha.luccioni@huggingface.co\nABSTRACT\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational\npower and natural resources.", "metadata": {}}, {"text": "In parallel to this trend, transparency around the costs and impacts of these models has\ndecreased, meaning that the users of these technologies have little to no information about their resource demands and\nsubsequent impacts on the environment.", "metadata": {}}, {"text": "Despite this dearth of adequate data, escalating demand for figures quantifying\nAI’s environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions.", "metadata": {}}, {"text": "In this article, we explore pervasive myths and misconceptions shaping\npublic understanding of AI’s environmental impacts, tracing their origins and their spread in both the media and scientific\npublications.", "metadata": {}}, {"text": "We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and\nconclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate\nnegative impacts in the future.", "metadata": {}}, {"text": "Introduction\nAI-powered tools and systems are becoming increasingly ubiquitous, reshaping human behavior with corresponding impacts\nto our socioeconomic systems.", "metadata": {}}, {"text": "While the companies that develop and deploy AI-driven technology strongly emphasize AI’s\npositive impacts (real and speculative)1–3, the negative impacts are typically left unspoken.", "metadata": {}}, {"text": "These are often uncovered by\nresearchers or a concerned public who audit AI systems, driven by the desire to understand the impacts of these systems on\nsociety and the planet.", "metadata": {}}, {"text": "Among the many impacts that have been analyzed in recent years are: algorithmic discrimination and\nbias4, 5, the use of AI in military applications6, the threat of AI to democracy7, 8, and environmental impacts9, 10.", "metadata": {}}, {"text": "Honing in on the latter, researchers and activists alike have been sounding the alarm on the increasingly unsustainable\ntrends in energy and natural resource consumption arising from the data centers and devices used to train and deploy AI models\nwhich are growing in size and complexity11–15.", "metadata": {}}, {"text": "These resources range from the rare earth minerals necessary to manufacture\ncomputing hardware, the energy needed to power the very tangible “cloud” computation that underpins AI systems, the water\nneeded for cooling and hardware manufacturing, and the greenhouse gases (GHGs) emitted at every stage of this process.", "metadata": {}}, {"text": "Despite these rising costs, there exists minimal and often no data quantifying these impacts.", "metadata": {}}, {"text": "When data does exist, it often\nlacks sufficient accessibility, detail and scope to enable effective decision-making or analysis, impeding impact assessment,\nmitigation, forecasting, and even basic understanding by the public at large.", "metadata": {}}, {"text": "As a result, researchers, investors, companies and policymakers are left to attempt best-effort approximations given limited\ndata availability.", "metadata": {}}, {"text": "In some cases, these estimates can be wildly flawed due to lack of critical prerequisite data, understanding or\nexpertise.", "metadata": {}}, {"text": "In other cases, estimates may be taken out of the careful, qualified contexts in which they were originally presented,\nleading to misinterpretation and in some cases severely inaccurate generalizations.", "metadata": {}}, {"text": "The increasingly high stakes political and\neconomic contexts surrounding climate change and AI severely compound this challenge;", "metadata": {}}, {"text": "mistakes and misinterpretation\ndevolve into misinformation as estimates are repeatedly shared and transformed through subsequent analyses, adopted as\naccurate measures and spread as trending posts through social media and the news, finally arriving on the desks of decision-\nmakers.", "metadata": {}}, {"text": "The resulting misconceptions harm all stakeholders: policymakers and the public are unable to make informed\ndecisions, and AI technology developers suffer from negative perceptions arising from overestimates of their social harms,\nfurther exacerbating their lack of disclosure.", "metadata": {}}, {"text": "In this paper, we aim to elucidate some common myths surrounding AI’s\nenvironmental impacts, explore the pitfalls that led to the emergence of those myths, and propose recommendations for\nremedying this challenge in the future.", "metadata": {}}, {"text": "1", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Related Work\nApproaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as\nthese systems have grown more impactful and widely deployed in user-facing applications. Initial studies, such as that of\nStrubell et al., underscored the environmental cost of training Transformer-based language models12. Research done in the\nfollowing years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI\nmodels including GPT-3, T5, Meena, and Switch Transformer, providing new estimates16 and expanding the scope of analysis\nbeyond model training to account for operational and embodied emissions 17, improving methodology for software energy\nmeasurement18, and a lifecycle approach to assessing emissions from model training and deployment 11. Wu et al. further\nadvanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,19. Most\nrecently, Luccioni, Jernite, and Strubell 20 pioneered the AI inference impact methodology, revealing generative architectures as\nparticularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference\nimpacts. These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach\nfor comparing the inference efficiency of AI models22.\nAbove and beyond energy considerations, Li et al. 23 expanded the scope of AI environmental impact measurement by\nestimating the water footprint of GPT-3 based on publicly available information, whereas Han et al. 24 assessed the public\nhealth toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants\nequivalent to more than 10,000 round trips by car between Los Angeles and New York City. In another significant advancement,\nGoogle’s recent TPU lifecycle assessment25 offered the most comprehensive cradle-to-grave environmental analysis of AI\nhardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure,\nsignificantly extending existing environmental impact models. Building on many of these approaches, Morrison et al. 26\nperformed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development,\nand training, enhancing the accuracy of these metrics through the use of granular underlying data.\nThe breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating\nAI’s environmental impacts, and the many different perspectives that exist in this space. Whereas several standardized\napproaches have been proposed to measure different aspects of AI’s requirements in terms of energy and water, as well as\nthe emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology\nand standards that cover all dimensions. In the next section, we examine how this translates into decreased environmental\ntransparency in the AI industry via an empirical analysis of AI models over time.\nEnvironmental Transparency Trends\nWhile there has been progress in developing more robust methodologies for measuring AI’s environmental impacts, the broader\nAI industry has paradoxically been trending in the opposite direction, disclosing less information over time. In order to quantify\nthis trend, we analyze Epoch AI’s Notable AI Models dataset27, which tracks information on “models that were state of the\nart, highly cited, or otherwise historically notable”, with respect to transparency about the environmental impacts of those\nmodels. We examine the level of environmental impact transparency for each model based on key information from the Epoch\nAI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content\n(e.g., paper, model card, announcement). We select the time period starting in 2010 as this is the beginning of the modern\n“deep learning era” (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed,\nincluding all 754 models from 2010 to the first quarter of 2025. Our analysis, shown in Figure 1, reveals substantial variation\nin environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others\nprovide no information at all regarding their approach.\nOverall, we find that models exhibit three transparency categories:\n• Direct Disclosure: Developers explicitly reported energy or GHG emissions. Note that this category includes methodolo-\ngies ranging from estimation (e.g., using hardware TDP, country average carbon intensity) to measurements (i.e., using\ntools like CodeCarbon).\n• Indirect Disclosure: Developers provided training compute data or released their model weights, allowing external\nestimates of training or inference impacts.\n• No Disclosure: Environmental impact data was not publicly released and estimation approaches (as noted in Indirect\nDisclosure) were not possible.\nFrom 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact\nof model training (ranging from 0 to 33% each year); no direct environmental impact data was released during this period.\nThis is expected, given that AI models of that era required significantly less compute and resource usage transparency was not\n2/12", "sentences": [{"text": "Related Work\nApproaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as\nthese systems have grown more impactful and widely deployed in user-facing applications.", "metadata": {}}, {"text": "Initial studies, such as that of\nStrubell et al., underscored the environmental cost of training Transformer-based language models12.", "metadata": {}}, {"text": "Research done in the\nfollowing years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI\nmodels including GPT-3, T5, Meena, and Switch Transformer, providing new estimates16 and expanding the scope of analysis\nbeyond model training to account for operational and embodied emissions 17, improving methodology for software energy\nmeasurement18, and a lifecycle approach to assessing emissions from model training and deployment 11.", "metadata": {}}, {"text": "Wu et al.", "metadata": {}}, {"text": "further\nadvanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,19.", "metadata": {}}, {"text": "Most\nrecently, Luccioni, Jernite, and Strubell 20 pioneered the AI inference impact methodology, revealing generative architectures as\nparticularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference\nimpacts.", "metadata": {}}, {"text": "These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach\nfor comparing the inference efficiency of AI models22.", "metadata": {}}, {"text": "Above and beyond energy considerations, Li et al.", "metadata": {}}, {"text": "23 expanded the scope of AI environmental impact measurement by\nestimating the water footprint of GPT-3 based on publicly available information, whereas Han et al.", "metadata": {}}, {"text": "24 assessed the public\nhealth toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants\nequivalent to more than 10,000 round trips by car between Los Angeles and New York City.", "metadata": {}}, {"text": "In another significant advancement,\nGoogle’s recent TPU lifecycle assessment25 offered the most comprehensive cradle-to-grave environmental analysis of AI\nhardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure,\nsignificantly extending existing environmental impact models.", "metadata": {}}, {"text": "Building on many of these approaches, Morrison et al.", "metadata": {}}, {"text": "26\nperformed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development,\nand training, enhancing the accuracy of these metrics through the use of granular underlying data.", "metadata": {}}, {"text": "The breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating\nAI’s environmental impacts, and the many different perspectives that exist in this space.", "metadata": {}}, {"text": "Whereas several standardized\napproaches have been proposed to measure different aspects of AI’s requirements in terms of energy and water, as well as\nthe emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology\nand standards that cover all dimensions.", "metadata": {}}, {"text": "In the next section, we examine how this translates into decreased environmental\ntransparency in the AI industry via an empirical analysis of AI models over time.", "metadata": {}}, {"text": "Environmental Transparency Trends\nWhile there has been progress in developing more robust methodologies for measuring AI’s environmental impacts, the broader\nAI industry has paradoxically been trending in the opposite direction, disclosing less information over time.", "metadata": {}}, {"text": "In order to quantify\nthis trend, we analyze Epoch AI’s Notable AI Models dataset27, which tracks information on “models that were state of the\nart, highly cited, or otherwise historically notable”, with respect to transparency about the environmental impacts of those\nmodels.", "metadata": {}}, {"text": "We examine the level of environmental impact transparency for each model based on key information from the Epoch\nAI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content\n(e.g., paper, model card, announcement).", "metadata": {}}, {"text": "We select the time period starting in 2010 as this is the beginning of the modern\n“deep learning era” (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed,\nincluding all 754 models from 2010 to the first quarter of 2025.", "metadata": {}}, {"text": "Our analysis, shown in Figure 1, reveals substantial variation\nin environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others\nprovide no information at all regarding their approach.", "metadata": {}}, {"text": "Overall, we find that models exhibit three transparency categories:\n• Direct Disclosure: Developers explicitly reported energy or GHG emissions.", "metadata": {}}, {"text": "Note that this category includes methodolo-\ngies ranging from estimation (e.g., using hardware TDP, country average carbon intensity) to measurements (i.e., using\ntools like CodeCarbon).", "metadata": {}}, {"text": "• Indirect Disclosure: Developers provided training compute data or released their model weights, allowing external\nestimates of training or inference impacts.", "metadata": {}}, {"text": "• No Disclosure: Environmental impact data was not publicly released and estimation approaches (as noted in Indirect\nDisclosure) were not possible.", "metadata": {}}, {"text": "From 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact\nof model training (ranging from 0 to 33% each year);", "metadata": {}}, {"text": "no direct environmental impact data was released during this period.", "metadata": {}}, {"text": "This is expected, given that AI models of that era required significantly less compute and resource usage transparency was not\n2/12", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Figure 1. Environmental Impact Transparency of Notable AI Models by Release Year27\nyet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of\ntraining data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and\nopen-weights model releases became more common. This period includes the the work of Strubell et al. 12, Luccioni11 and\nothers. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some\ndegree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially\ncatalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and\neven the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental\ndisclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the\nline between research and commercial deployment became increasingly blurred.\nFigure 2. Environmental Impact Transparency of LLM Usage – OpenRouter28 (May 2025)\nBeyond the long term trend, zooming in to examine recent AI model usage data helps illustrate today’s environmental\nimpact transparency conditions. OpenRouter28, a widely-used API platform for LLMs, publicly shares data on model traffic\nincluding top 20 models by month, and the number of tokens running through every model. May 2025 data (Figure 2) indicates\nthat of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1,\nDeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by\nreleasing their model weights to enable efficiency analysis). In terms of token usage, 84% of LLM usage is through models\nwith no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure. This indicates that the\nmajority of users who interact with LLMs have no information about their environmental impacts, and cannot make informed\ndecisions based on model efficiency or carbon intensity.\nFrom the limited data that is publicly available, we can observe significant disparities in energy use and emissions across\nmodels. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh\n(LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of\nelectricity across training locations). Inference workloads also show wide variation depending on model size, architecture and\n3/12", "sentences": [{"text": "Figure 1.", "metadata": {}}, {"text": "Environmental Impact Transparency of Notable AI Models by Release Year27\nyet common practice, although many articles accompanying papers did provide related information about, e.g.", "metadata": {}}, {"text": "the amount of\ntraining data used or number of epochs trained.", "metadata": {}}, {"text": "From 2019 to 2022, transparency improved as awareness of impacts grew and\nopen-weights model releases became more common.", "metadata": {}}, {"text": "This period includes the the work of Strubell et al.", "metadata": {}}, {"text": "12, Luccioni11 and\nothers.", "metadata": {}}, {"text": "The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some\ndegree of information.", "metadata": {}}, {"text": "However, the introduction of increasingly commercial and proprietary models after 2022, potentially\ncatalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and\neven the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental\ndisclosures.", "metadata": {}}, {"text": "By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the\nline between research and commercial deployment became increasingly blurred.", "metadata": {}}, {"text": "Figure 2.", "metadata": {}}, {"text": "Environmental Impact Transparency of LLM Usage – OpenRouter28 (May 2025)\nBeyond the long term trend, zooming in to examine recent AI model usage data helps illustrate today’s environmental\nimpact transparency conditions.", "metadata": {}}, {"text": "OpenRouter28, a widely-used API platform for LLMs, publicly shares data on model traffic\nincluding top 20 models by month, and the number of tokens running through every model.", "metadata": {}}, {"text": "May 2025 data (Figure 2) indicates\nthat of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1,\nDeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by\nreleasing their model weights to enable efficiency analysis).", "metadata": {}}, {"text": "In terms of token usage, 84% of LLM usage is through models\nwith no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.", "metadata": {}}, {"text": "This indicates that the\nmajority of users who interact with LLMs have no information about their environmental impacts, and cannot make informed\ndecisions based on model efficiency or carbon intensity.", "metadata": {}}, {"text": "From the limited data that is publicly available, we can observe significant disparities in energy use and emissions across\nmodels.", "metadata": {}}, {"text": "In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh\n(LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of\nelectricity across training locations).", "metadata": {}}, {"text": "Inference workloads also show wide variation depending on model size, architecture and\n3/12", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=Im1.png] Size: 1408x360, Data: 20466 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=Im1.png] Size: 1408x360, Data: 20466 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "Im1.png", "image_width": 1408, "image_height": 360, "attachment_type": "image", "has_image_data": true, "image_data_size": 20466}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R\nPlus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).\nThese ranges highlight not only the scale of potential impacts, but also the pressing need for more standardized and transparent\nreporting to enable meaningful comparisons.\nInvestigating the Urban Legends of AI’s Environmental Impacts\nMaking sustainably-minded decisions when using AI systems requires having the necessary information about different aspects\nof their development and deployment. While there are empirical studies focusing on AI’s environmental impacts, such as those\ncited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size,\narchitecture, optimizations, hardware, location, setup, system) that they are not representative of. This fuels misinformation,\nundermines scientific research, and can result in decisions that are not grounded in facts29. In the paragraphs below, we address\nsome of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore\ntheir potential for spreading environmental misinformation.\nTraining an AI model emits as much CO2 as five cars in their lifetimes\nAmong the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,12 which estimated\nthe monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP)\nmodels of that era, including the first generation of large language models. This analysis included both the costs to train\nseveral individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer\nneural network architecture30 that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e.\nidentifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model\ndevelopment through both a case study of the energy required for them to develop a model published in the previous year, and\nby estimating the energy required to automate that process using an approach called neural architecture search (NAS) based\non figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. 31\nIn the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions\nintensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons)\nCO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.\nThe research article was written for a specialized audience of AI and NLP researchers, who would have the background\nknowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and\nfeaturing a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked\nup by numerous media outlets (including MIT Technology Review32 and Forbes33). The “five cars” number has since been\nmisinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of\narchitectures, training approaches and electricity sources used for powering AI model training; the original article reports AI\ntraining workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity),\nand AI model training more broadly often requires even less energy and corresponding emissions.\nFurther, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much\nless frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis\nto reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational)\nsignificantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today’s\ngenerative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However,\nwhile the “five cars” estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI\ntraining workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically\nexceed the “five cars” estimate: Google reports that training their open source Gemma family of language models emitted\n1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3\nfamily of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.\nA request to ChatGPT consumes ten times more energy than a Google search\nAnother often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours\n(Wh) of energy, which is \"ten times more than a Google search\". This figure is often quoted in the press36, 37 and in industry\nreports38. Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet’s Chairman John\nHennessy during a 2023 interview with Reuters, in which he said that “having an exchange with AI known as a large language\nmodel likely cost 10 times more than a standard keyword search” 39. This remark was used was the basis of an estimate\npublished in October 2023 of “approximately 3 Wh per LLM interaction” 40, with the Google search number taken from a\n2009 blog post from Google that stated that “Queries vary in degree of difficulty, but for the average query [...] this amounts\nto 0.0003 kWh of energy per search” 41. This number is misleading for several reasons. First, Hennessy has no relation to\n4/12", "sentences": [{"text": "task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R\nPlus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).", "metadata": {}}, {"text": "These ranges highlight not only the scale of potential impacts, but also the pressing need for more standardized and transparent\nreporting to enable meaningful comparisons.", "metadata": {}}, {"text": "Investigating the Urban Legends of AI’s Environmental Impacts\nMaking sustainably-minded decisions when using AI systems requires having the necessary information about different aspects\nof their development and deployment.", "metadata": {}}, {"text": "While there are empirical studies focusing on AI’s environmental impacts, such as those\ncited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size,\narchitecture, optimizations, hardware, location, setup, system) that they are not representative of.", "metadata": {}}, {"text": "This fuels misinformation,\nundermines scientific research, and can result in decisions that are not grounded in facts29.", "metadata": {}}, {"text": "In the paragraphs below, we address\nsome of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore\ntheir potential for spreading environmental misinformation.", "metadata": {}}, {"text": "Training an AI model emits as much CO2 as five cars in their lifetimes\nAmong the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,12 which estimated\nthe monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP)\nmodels of that era, including the first generation of large language models.", "metadata": {}}, {"text": "This analysis included both the costs to train\nseveral individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer\nneural network architecture30 that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e.", "metadata": {}}, {"text": "identifying the best model architecture with respect to some optimization objective.", "metadata": {}}, {"text": "The authors quantified the costs of model\ndevelopment through both a case study of the energy required for them to develop a model published in the previous year, and\nby estimating the energy required to automate that process using an approach called neural architecture search (NAS) based\non figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.", "metadata": {}}, {"text": "31\nIn the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions\nintensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons)\nCO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.", "metadata": {}}, {"text": "The research article was written for a specialized audience of AI and NLP researchers, who would have the background\nknowledge to understand the appropriate scoping for the estimate.", "metadata": {}}, {"text": "However, an author’s tweet publicizing the paper and\nfeaturing a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked\nup by numerous media outlets (including MIT Technology Review32 and Forbes33).", "metadata": {}}, {"text": "The “five cars” number has since been\nmisinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of\narchitectures, training approaches and electricity sources used for powering AI model training;", "metadata": {}}, {"text": "the original article reports AI\ntraining workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S.", "metadata": {}}, {"text": "average energy carbon emissions intensity),\nand AI model training more broadly often requires even less energy and corresponding emissions.", "metadata": {}}, {"text": "Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much\nless frequently than the average AI model training workload.", "metadata": {}}, {"text": "This is both because the result is intended to be re-used as a basis\nto reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational)\nsignificantly limits who can perform such large-scale training runs.", "metadata": {}}, {"text": "In this way, the NAS training workload is similar to today’s\ngenerative AI pretraining workloads, which are similarly performed less frequently than the average AI training.", "metadata": {}}, {"text": "However,\nwhile the “five cars” estimate from Strubell et al.", "metadata": {}}, {"text": "is not an accurate representation of the emissions arising from every AI\ntraining workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically\nexceed the “five cars” estimate: Google reports that training their open source Gemma family of language models emitted\n1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3\nfamily of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.", "metadata": {}}, {"text": "A request to ChatGPT consumes ten times more energy than a Google search\nAnother often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours\n(Wh) of energy, which is \"ten times more than a Google search\".", "metadata": {}}, {"text": "This figure is often quoted in the press36, 37 and in industry\nreports38.", "metadata": {}}, {"text": "Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet’s Chairman John\nHennessy during a 2023 interview with Reuters, in which he said that “having an exchange with AI known as a large language\nmodel likely cost 10 times more than a standard keyword search” 39.", "metadata": {}}, {"text": "This remark was used was the basis of an estimate\npublished in October 2023 of “approximately 3 Wh per LLM interaction” 40, with the Google search number taken from a\n2009 blog post from Google that stated that “Queries vary in degree of difficulty, but for the average query [...] this amounts\nto 0.0003 kWh of energy per search” 41.", "metadata": {}}, {"text": "This number is misleading for several reasons.", "metadata": {}}, {"text": "First, Hennessy has no relation to\n4/12", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "OpenAI or Microsoft (which provides the compute for OpenAI’s services), so the comment he made was based on secondhand\ninformation. Second, even if Hennessy’s comparison were accurate, basing the search estimate on a figure that is 16 years\nold — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current\nTransformer-based models — is also bound to amplify the inaccuracy of the estimate.\nTo understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11,\n2025, that appear when searching for “ChatGPT energy consumption” on Google News. For each article, we noted whether\nit mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by\nacknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal\nthat 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the\nsources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy\nthan a Google search 42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number\nof American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on\nthe energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural\nnetwork architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when\naddressing this subject47, stating that the true figures remain unknown. It is also noteworthy that among these articles, 9% also\nrelay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.\nFigure 3. Analysis of media articles discussing ChatGPT energy consumption.\nAI can reduce 10% of global emissions\nWhile the numbers around AI’s negative environmental impacts can be misinterpreted and taken out of context, so, too, can the\npotential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale. One\nrecurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021\nBoston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and\ntechnology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions\nof the European Union”48. The same number appears in a more recent BCG report from 2023, which was commissioned by\nGoogle and published ahead of COP2649. The reasoning behind the 5-10% reduction estimate is unclear and the underlying\ncalculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and\nusing AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more\ndetail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project\nnumbers to a global scale.\nApplying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—\nin fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous\ncalculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence\nanalysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T\nL.148050 or WBCSD guidance on avoided emissions51. And yet, these numbers were picked up in research52 and the media,\nused as evidence that the potential of AI to stop climate change is overwhelmingly positive53, 54. While AI undoubtedly has\npotential positive applications in sectors ranging from transportation to agriculture to energy55, these global generalizations can\nbe misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the\nbeliefs that the benefits of AI will outweigh its costs56.\n5/12", "sentences": [{"text": "OpenAI or Microsoft (which provides the compute for OpenAI’s services), so the comment he made was based on secondhand\ninformation.", "metadata": {}}, {"text": "Second, even if Hennessy’s comparison were accurate, basing the search estimate on a figure that is 16 years\nold — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current\nTransformer-based models — is also bound to amplify the inaccuracy of the estimate.", "metadata": {}}, {"text": "To understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11,\n2025, that appear when searching for “ChatGPT energy consumption” on Google News.", "metadata": {}}, {"text": "For each article, we noted whether\nit mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by\nacknowledging uncertainty or suggesting that such statistics should be viewed critically.", "metadata": {}}, {"text": "Our results, shown in Figure 3, reveal\nthat 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the\nsources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy\nthan a Google search 42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number\nof American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on\nthe energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural\nnetwork architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when\naddressing this subject47, stating that the true figures remain unknown.", "metadata": {}}, {"text": "It is also noteworthy that among these articles, 9% also\nrelay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.", "metadata": {}}, {"text": "Figure 3.", "metadata": {}}, {"text": "Analysis of media articles discussing ChatGPT energy consumption.", "metadata": {}}, {"text": "AI can reduce 10% of global emissions\nWhile the numbers around AI’s negative environmental impacts can be misinterpreted and taken out of context, so, too, can the\npotential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale.", "metadata": {}}, {"text": "One\nrecurring number states that AI can help reduce global GHG emissions (up to) 10%.", "metadata": {}}, {"text": "This number can be traced back to a 2021\nBoston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and\ntechnology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions\nof the European Union”48.", "metadata": {}}, {"text": "The same number appears in a more recent BCG report from 2023, which was commissioned by\nGoogle and published ahead of COP2649.", "metadata": {}}, {"text": "The reasoning behind the 5-10% reduction estimate is unclear and the underlying\ncalculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and\nusing AI to optimize and improve existing processes.", "metadata": {}}, {"text": "The second, Google-commissioned BCG study provides slightly more\ndetail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project\nnumbers to a global scale.", "metadata": {}}, {"text": "Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—\nin fact, many of the emissions reductions on a global scale require individual, societal and political shifts.", "metadata": {}}, {"text": "Moreover, rigorous\ncalculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence\nanalysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T\nL.148050 or WBCSD guidance on avoided emissions51.", "metadata": {}}, {"text": "And yet, these numbers were picked up in research52 and the media,\nused as evidence that the potential of AI to stop climate change is overwhelmingly positive53, 54.", "metadata": {}}, {"text": "While AI undoubtedly has\npotential positive applications in sectors ranging from transportation to agriculture to energy55, these global generalizations can\nbe misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the\nbeliefs that the benefits of AI will outweigh its costs56.", "metadata": {}}, {"text": "5/12", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "The lack of transparency around AI’s environmental impacts can have far-reaching consequences, ranging from specific\nestimates taken out of context and blown out of proportion, to proxies becoming adopted by press and policymakers in the\nabsence of more reliable figures. In the next section, we discuss a potential solution to this situation by proposing a set of\nmetrics that different stakeholders can measure and report to bring more clarity to the extent of AI’s environmental impacts.\nHow to improve environmental impact disclosures in AI\nOpacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed\nprocurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack\nthe information necessary to develop evidence-based regulations. This opacity also generates cascading effects throughout\nvalue chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments. Furthermore,\nthe absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that\ncould drive efficiency improvements. Perhaps most critically, this lack of transparency undermines accountability mechanisms,\nmaking it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress\ntoward sustainability goals.\nThis section explores how comprehensive environmental transparency can address these challenges through four intercon-\nnected pathways:\n1. Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and\ndeployment;\n2. Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustain-\nability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user\nenterprises;\n3. Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful compar-\nisons; and\n4. Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.\nMeasurement and Disclosure As the starting point of AI development, AI researchers and developers are able to gather\nempirical measurements from the systems they create at different steps of the model lifecycle. When developing models from\nscratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools\nlike Code Carbon57 or no-code tools like Green Algorithms 58. When using or adapting existing models, performance and\nefficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production.\nFor instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different\ntasks, which can also be adapted for specific contexts and datasets. These metrics should be reported in model cards 59 and\nscientific publications with complete methodological transparency, including hardware specifications, geographic locations,\nelectricity sources, measurement uncertainties, and allocation methodologies. This empirical foundation enables downstream\norganizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts\nthrough peer-reviewed publication of methodologies and results. AI providers across the entire value chain, including cloud\ninfrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency\nwith granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related\nenvironmental impacts. Government and public sector organizations should mandate transparency in all AI procurements,\nrequire open data for publicly funded research, and align AI deployments with existing net zero commitments.\nOrganizational Implementation and Processes As AI adoption accelerates, organizations should implement comprehensive\nframeworks to assess, measure, and integrate AI’s environmental impacts into existing sustainability management systems using\nstructured approaches tailored to their specific contexts and risk profiles. The materiality assessment framework should aim to\nestablish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct\ntiers of analysis intensity. Organizations developing AI systems utilizing open-source models on their infrastructure should\nimplement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and\norganization-wide aggregations. Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial\nmodels or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by\nincorporating environmental disclosure requirements into procurement processes and contractual agreements60. Specifically,\norganizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services\nunder consideration. These environmental metrics should be systematically integrated into organizations’ GHG accounting\nframeworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and\nunmodeled factors.\n6/12", "sentences": [{"text": "The lack of transparency around AI’s environmental impacts can have far-reaching consequences, ranging from specific\nestimates taken out of context and blown out of proportion, to proxies becoming adopted by press and policymakers in the\nabsence of more reliable figures.", "metadata": {}}, {"text": "In the next section, we discuss a potential solution to this situation by proposing a set of\nmetrics that different stakeholders can measure and report to bring more clarity to the extent of AI’s environmental impacts.", "metadata": {}}, {"text": "How to improve environmental impact disclosures in AI\nOpacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed\nprocurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack\nthe information necessary to develop evidence-based regulations.", "metadata": {}}, {"text": "This opacity also generates cascading effects throughout\nvalue chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments.", "metadata": {}}, {"text": "Furthermore,\nthe absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that\ncould drive efficiency improvements.", "metadata": {}}, {"text": "Perhaps most critically, this lack of transparency undermines accountability mechanisms,\nmaking it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress\ntoward sustainability goals.", "metadata": {}}, {"text": "This section explores how comprehensive environmental transparency can address these challenges through four intercon-\nnected pathways:\n1.", "metadata": {}}, {"text": "Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and\ndeployment;", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustain-\nability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user\nenterprises;", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful compar-\nisons;", "metadata": {}}, {"text": "and\n4.", "metadata": {}}, {"text": "Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.", "metadata": {}}, {"text": "Measurement and Disclosure As the starting point of AI development, AI researchers and developers are able to gather\nempirical measurements from the systems they create at different steps of the model lifecycle.", "metadata": {}}, {"text": "When developing models from\nscratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools\nlike Code Carbon57 or no-code tools like Green Algorithms 58.", "metadata": {}}, {"text": "When using or adapting existing models, performance and\nefficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production.", "metadata": {}}, {"text": "For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different\ntasks, which can also be adapted for specific contexts and datasets.", "metadata": {}}, {"text": "These metrics should be reported in model cards 59 and\nscientific publications with complete methodological transparency, including hardware specifications, geographic locations,\nelectricity sources, measurement uncertainties, and allocation methodologies.", "metadata": {}}, {"text": "This empirical foundation enables downstream\norganizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts\nthrough peer-reviewed publication of methodologies and results.", "metadata": {}}, {"text": "AI providers across the entire value chain, including cloud\ninfrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency\nwith granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related\nenvironmental impacts.", "metadata": {}}, {"text": "Government and public sector organizations should mandate transparency in all AI procurements,\nrequire open data for publicly funded research, and align AI deployments with existing net zero commitments.", "metadata": {}}, {"text": "Organizational Implementation and Processes As AI adoption accelerates, organizations should implement comprehensive\nframeworks to assess, measure, and integrate AI’s environmental impacts into existing sustainability management systems using\nstructured approaches tailored to their specific contexts and risk profiles.", "metadata": {}}, {"text": "The materiality assessment framework should aim to\nestablish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct\ntiers of analysis intensity.", "metadata": {}}, {"text": "Organizations developing AI systems utilizing open-source models on their infrastructure should\nimplement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and\norganization-wide aggregations.", "metadata": {}}, {"text": "Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial\nmodels or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by\nincorporating environmental disclosure requirements into procurement processes and contractual agreements60.", "metadata": {}}, {"text": "Specifically,\norganizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services\nunder consideration.", "metadata": {}}, {"text": "These environmental metrics should be systematically integrated into organizations’ GHG accounting\nframeworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and\nunmodeled factors.", "metadata": {}}, {"text": "6/12", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Standards, Verification and Assurance Environmental AI disclosures require robust verification frameworks to ensure\naccuracy and prevent greenwashing, necessitating new assurance standards adapted to AI’s rapid evolution, distributed compute,\nand complex value chains. While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway\nacross organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD\n(Organization for Economic Cooperation and Development). These bodies are well-positioned to develop standardized\napproaches for stakeholders ranging from developers to governments. Given AI’s transnational nature, coordination and\nharmonization of these efforts is essential. Without alignment, implementation may diverge across jurisdictions, creating\nfurther confusion in the market. However, as formal standards may take years to materialize, interim ad hoc methods (such as\nthose outlined above) can provide valuable insights and help shape the eventual development of formal methodologies. These\nAI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly\nregarding the GHG Protocol’s treatment of electricity emissions measurement. The current allowance for market-based\naccounting enables companies to significantly under-report their actual AI-related emissions through renewable energy\ncertificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility 56. For AI\nservices consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure\nenvironmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured\nthrough market mechanisms.\nPolicy Frameworks and Reporting Environmental transparency documentation is already commonplace for private orga-\nnizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate\ndisclosure requirements in the US, or local and state-level climate disclosure laws. However, policymakers should incorporate\nadditional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability\nReporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage,\nand a transition plan aligned with the Paris Agreement61, particularly as this aligns with existing provisions in the EU AI Act.\nNon-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria\nto incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices. For\norganizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators,\nsemiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements. These\ncould include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure.\nEnforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets\nand funding on compliance with disclosure standards.\nConclusion\nThe current trend toward reduced transparency around AI’s environmental impact contributes to misinformation and hinders\ninformed decision-making across all levels, from individual researchers and developers to organizations and policymakers. This\ndeclining transparency is particularly troubling given AI’s escalating environmental impacts amid global climate concerns and\nlooming planetary boundaries. While competition is frequently cited to justify opacity, other competitive industries, such as\nfood (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between\ntransparency and competition is achievable. Reversing the trend toward opacity in AI environmental reporting is essential for\ninformed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms\nemerge that may alter these impacts. As members of the AI community committed to addressing the climate crisis, we aim to\nensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this\ngoal.\n7/12", "sentences": [{"text": "Standards, Verification and Assurance Environmental AI disclosures require robust verification frameworks to ensure\naccuracy and prevent greenwashing, necessitating new assurance standards adapted to AI’s rapid evolution, distributed compute,\nand complex value chains.", "metadata": {}}, {"text": "While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway\nacross organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD\n(Organization for Economic Cooperation and Development).", "metadata": {}}, {"text": "These bodies are well-positioned to develop standardized\napproaches for stakeholders ranging from developers to governments.", "metadata": {}}, {"text": "Given AI’s transnational nature, coordination and\nharmonization of these efforts is essential.", "metadata": {}}, {"text": "Without alignment, implementation may diverge across jurisdictions, creating\nfurther confusion in the market.", "metadata": {}}, {"text": "However, as formal standards may take years to materialize, interim ad hoc methods (such as\nthose outlined above) can provide valuable insights and help shape the eventual development of formal methodologies.", "metadata": {}}, {"text": "These\nAI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly\nregarding the GHG Protocol’s treatment of electricity emissions measurement.", "metadata": {}}, {"text": "The current allowance for market-based\naccounting enables companies to significantly under-report their actual AI-related emissions through renewable energy\ncertificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility 56.", "metadata": {}}, {"text": "For AI\nservices consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure\nenvironmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured\nthrough market mechanisms.", "metadata": {}}, {"text": "Policy Frameworks and Reporting Environmental transparency documentation is already commonplace for private orga-\nnizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate\ndisclosure requirements in the US, or local and state-level climate disclosure laws.", "metadata": {}}, {"text": "However, policymakers should incorporate\nadditional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability\nReporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage,\nand a transition plan aligned with the Paris Agreement61, particularly as this aligns with existing provisions in the EU AI Act.", "metadata": {}}, {"text": "Non-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria\nto incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices.", "metadata": {}}, {"text": "For\norganizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators,\nsemiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements.", "metadata": {}}, {"text": "These\ncould include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure.", "metadata": {}}, {"text": "Enforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets\nand funding on compliance with disclosure standards.", "metadata": {}}, {"text": "Conclusion\nThe current trend toward reduced transparency around AI’s environmental impact contributes to misinformation and hinders\ninformed decision-making across all levels, from individual researchers and developers to organizations and policymakers.", "metadata": {}}, {"text": "This\ndeclining transparency is particularly troubling given AI’s escalating environmental impacts amid global climate concerns and\nlooming planetary boundaries.", "metadata": {}}, {"text": "While competition is frequently cited to justify opacity, other competitive industries, such as\nfood (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between\ntransparency and competition is achievable.", "metadata": {}}, {"text": "Reversing the trend toward opacity in AI environmental reporting is essential for\ninformed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms\nemerge that may alter these impacts.", "metadata": {}}, {"text": "As members of the AI community committed to addressing the climate crisis, we aim to\nensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this\ngoal.", "metadata": {}}, {"text": "7/12", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "References\n1. Luers, A. et al. Will ai accelerate or delay the race to net-zero emissions? Nature 628, 718–720 (2024).\n2. Kshirsagar, M. et al. Becoming good at AI for good. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\nSociety, 664–673 (2021).\n3. DeepMind. Using ai to fight climate change (2009).\n4. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. In Ethics of data and analytics , 254–264 (Auerbach\nPublications, 2022).\n5. Buolamwini, J. & Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In\nConference on fairness, accountability and transparency, 77–91 (PMLR, 2018).\n6. Rivera, J.-P.et al. Escalation risks from language models in military and diplomatic decision-making. In Proceedings of\nthe 2024 ACM Conference on Fairness, Accountability, and Transparency, 836–898 (2024).\n7. Manheim, K. & Kaplan, L. Artificial intelligence: Risks to privacy and democracy. Yale JL & Tech.21, 106 (2019).\n8. Summerfield, C. et al. How will advanced ai systems impact democracy? arXiv preprint arXiv:2409.06729 (2024).\n9. Crawford, K. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence (Yale University Press,\n2021).\n10. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).\n11. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Estimating the carbon footprint of BLOOM, a 176b parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n12. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n13. Luccioni, A. S. & Hernandez-Garcia, A. Counting carbon: A survey of factors influencing the emissions of machine\nlearning. arXiv preprint arXiv:2302.08476 (2023).\n14. Dodge, J. et al. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, 1877–1894 (2022).\n15. Ligozat, A.-L., Lefèvre, J., Bugeau, A. & Combaz, J. Unraveling the hidden environmental impacts of AI solutions for\nenvironment. arXiv preprint arXiv:2110.11822 (2021).\n16. Patterson, D. et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n17. Gupta, U. et al. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA), 854–867 (IEEE, 2021).\n18. Cao, Q., Lal, Y . K., Trivedi, H., Balasubramanian, A. & Balasubramanian, N. IrEne: Interpretable energy prediction for\ntransformers. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), 2145–2157, DOI: 10.18653/v1/2021.acl-long.167 (Association for Computational Linguistics, Online,\n2021).\n19. Wu, C.-J. et al. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint\narXiv:2111.00364 (2021).\n20. Luccioni, S., Jernite, Y . & Strubell, E. Power hungry processing: Watts driving the cost of ai deployment? InThe 2024\nACM Conference on Fairness, Accountability, and Transparency, FAccT ’24, 85–99, DOI: 10.1145/3630106.3658542\n(ACM, 2024).\n21. Luccioni, S. & Gamazaychikov, B. AI Energy Score Leaderboard. https://huggingface.co/spaces/AIEnergyScore/\nLeaderboard (2025). Hugging Face Spaces.\n22. Luccioni, S. et al. Light bulbs have energy ratings—so why can’t ai chatbots? Nature 632, 736–738 (2024).\n23. Li, P., Yang, J., Islam, M. A. & Ren, S. Making ai less\" thirsty\": Uncovering and addressing the secret water footprint of ai\nmodels. arXiv preprint arXiv:2304.03271 (2023).\n24. Han, Y ., Wu, Z., Li, P., Wierman, A. & Ren, S. The unpaid toll: Quantifying the public health impact of ai.arXiv preprint\narXiv:2412.06288 (2024).\n8/12", "sentences": [{"text": "References\n1.", "metadata": {}}, {"text": "Luers, A.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Will ai accelerate or delay the race to net-zero emissions?", "metadata": {}}, {"text": "Nature 628, 718–720 (2024).", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Kshirsagar, M.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Becoming good at AI for good.", "metadata": {}}, {"text": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\nSociety, 664–673 (2021).", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "DeepMind.", "metadata": {}}, {"text": "Using ai to fight climate change (2009).", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "Angwin, J., Larson, J., Mattu, S.", "metadata": {}}, {"text": "& Kirchner, L.", "metadata": {}}, {"text": "Machine bias.", "metadata": {}}, {"text": "In Ethics of data and analytics , 254–264 (Auerbach\nPublications, 2022).", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "Buolamwini, J.", "metadata": {}}, {"text": "& Gebru, T.", "metadata": {}}, {"text": "Gender shades: Intersectional accuracy disparities in commercial gender classification.", "metadata": {}}, {"text": "In\nConference on fairness, accountability and transparency, 77–91 (PMLR, 2018).", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "Rivera, J.-P.et al.", "metadata": {}}, {"text": "Escalation risks from language models in military and diplomatic decision-making.", "metadata": {}}, {"text": "In Proceedings of\nthe 2024 ACM Conference on Fairness, Accountability, and Transparency, 836–898 (2024).", "metadata": {}}, {"text": "7.", "metadata": {}}, {"text": "Manheim, K.", "metadata": {}}, {"text": "& Kaplan, L.", "metadata": {}}, {"text": "Artificial intelligence: Risks to privacy and democracy.", "metadata": {}}, {"text": "Yale JL & Tech.21, 106 (2019).", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "Summerfield, C.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "How will advanced ai systems impact democracy?", "metadata": {}}, {"text": "arXiv preprint arXiv:2409.06729 (2024).", "metadata": {}}, {"text": "9.", "metadata": {}}, {"text": "Crawford, K.", "metadata": {}}, {"text": "The atlas of AI: Power, politics, and the planetary costs of artificial intelligence (Yale University Press,\n2021).", "metadata": {}}, {"text": "10.", "metadata": {}}, {"text": "Bender, E.", "metadata": {}}, {"text": "M., Gebru, T., McMillan-Major, A.", "metadata": {}}, {"text": "& Shmitchell, S.", "metadata": {}}, {"text": "On the dangers of stochastic parrots: Can language models\nbe too big?", "metadata": {}}, {"text": "In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).", "metadata": {}}, {"text": "11.", "metadata": {}}, {"text": "Luccioni, A.", "metadata": {}}, {"text": "S., Viguier, S.", "metadata": {}}, {"text": "& Ligozat, A.-L.", "metadata": {}}, {"text": "Estimating the carbon footprint of BLOOM, a 176b parameter language\nmodel.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.02001 (2022).", "metadata": {}}, {"text": "12.", "metadata": {}}, {"text": "Strubell, E., Ganesh, A.", "metadata": {}}, {"text": "& McCallum, A.", "metadata": {}}, {"text": "Energy and policy considerations for deep learning in NLP.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1906.02243 (2019).", "metadata": {}}, {"text": "13.", "metadata": {}}, {"text": "Luccioni, A.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "& Hernandez-Garcia, A.", "metadata": {}}, {"text": "Counting carbon: A survey of factors influencing the emissions of machine\nlearning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.08476 (2023).", "metadata": {}}, {"text": "14.", "metadata": {}}, {"text": "Dodge, J.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Measuring the carbon intensity of AI in cloud instances.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, 1877–1894 (2022).", "metadata": {}}, {"text": "15.", "metadata": {}}, {"text": "Ligozat, A.-L., Lefèvre, J., Bugeau, A.", "metadata": {}}, {"text": "& Combaz, J.", "metadata": {}}, {"text": "Unraveling the hidden environmental impacts of AI solutions for\nenvironment.", "metadata": {}}, {"text": "arXiv preprint arXiv:2110.11822 (2021).", "metadata": {}}, {"text": "16.", "metadata": {}}, {"text": "Patterson, D.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.10350 (2021).", "metadata": {}}, {"text": "17.", "metadata": {}}, {"text": "Gupta, U.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Chasing Carbon: The Elusive Environmental Footprint of Computing.", "metadata": {}}, {"text": "In 2021 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA), 854–867 (IEEE, 2021).", "metadata": {}}, {"text": "18.", "metadata": {}}, {"text": "Cao, Q., Lal, Y .", "metadata": {}}, {"text": "K., Trivedi, H., Balasubramanian, A.", "metadata": {}}, {"text": "& Balasubramanian, N.", "metadata": {}}, {"text": "IrEne: Interpretable energy prediction for\ntransformers.", "metadata": {}}, {"text": "In Zong, C., Xia, F., Li, W.", "metadata": {}}, {"text": "& Navigli, R.", "metadata": {}}, {"text": "(eds.) Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), 2145–2157, DOI: 10.18653/v1/2021.acl-long.167 (Association for Computational Linguistics, Online,\n2021).", "metadata": {}}, {"text": "19.", "metadata": {}}, {"text": "Wu, C.-J.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Sustainable AI: Environmental Implications, Challenges and Opportunities.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2111.00364 (2021).", "metadata": {}}, {"text": "20.", "metadata": {}}, {"text": "Luccioni, S., Jernite, Y .", "metadata": {}}, {"text": "& Strubell, E.", "metadata": {}}, {"text": "Power hungry processing: Watts driving the cost of ai deployment?", "metadata": {}}, {"text": "InThe 2024\nACM Conference on Fairness, Accountability, and Transparency, FAccT ’24, 85–99, DOI: 10.1145/3630106.3658542\n(ACM, 2024).", "metadata": {}}, {"text": "21.", "metadata": {}}, {"text": "Luccioni, S.", "metadata": {}}, {"text": "& Gamazaychikov, B.", "metadata": {}}, {"text": "AI Energy Score Leaderboard.", "metadata": {}}, {"text": "https://huggingface.co/spaces/AIEnergyScore/\nLeaderboard (2025).", "metadata": {}}, {"text": "Hugging Face Spaces.", "metadata": {}}, {"text": "22.", "metadata": {}}, {"text": "Luccioni, S.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Light bulbs have energy ratings—so why can’t ai chatbots?", "metadata": {}}, {"text": "Nature 632, 736–738 (2024).", "metadata": {}}, {"text": "23.", "metadata": {}}, {"text": "Li, P., Yang, J., Islam, M.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "& Ren, S.", "metadata": {}}, {"text": "Making ai less\" thirsty\": Uncovering and addressing the secret water footprint of ai\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2304.03271 (2023).", "metadata": {}}, {"text": "24.", "metadata": {}}, {"text": "Han, Y ., Wu, Z., Li, P., Wierman, A.", "metadata": {}}, {"text": "& Ren, S.", "metadata": {}}, {"text": "The unpaid toll: Quantifying the public health impact of ai.arXiv preprint\narXiv:2412.06288 (2024).", "metadata": {}}, {"text": "8/12", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "25. Schneider, I. et al. Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025).\n2502.01671.\n26. Morrison, J. et al. Holistically evaluating the environmental impact of creating language models. arXiv preprint\narXiv:2503.05804 (2025).\n27. Epoch AI. Data on notable ai models (2024). Accessed: 2025-04-06.\n28. OpenRouter. Openrouter leaderboard rankings. https://openrouter.ai/rankings?view=month (2025). Accessed: 2025-06-03.\n29. Lovins, A. B. Artificial intelligence meets natural stupidity: Managing the risks (2025).\n30. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n31. So, D., Le, Q. & Liang, C. The evolved transformer. In Chaudhuri, K. & Salakhutdinov, R. (eds.) Proceedings of the\n36th International Conference on Machine Learning, vol. 97 of Proceedings of Machine Learning Research, 5877–5886\n(PMLR, 2019).\n32. Hao, K. Training a single ai model can emit as much carbon as five cars in their lifetimes. MIT technology Rev. 75, 103\n(2019).\n33. Toews, R. Deep learning’s carbon emissions problem. Forbes (2020).\n34. Gemma Team et al. Gemma 2: Improving open language models at a practical size (2024). 2408.00118.\n35. Meta. Llama 3.1 Model Card. https://huggingface.co/meta-llama/Llama-3.1-8B (2024). Hugging Face Model Card.\n36. Kerr, D. & NPR. AI brings soaring emissions for Google and Microsoft, a major contributor to climate change. NPR, July\n(2024).\n37. Chen, S. How much energy will ai really consume? the good, the bad and the unknown. Nature 639, 22–24 (2025).\n38. Aljbour, J., Wilson, T. & Patel, P. Powering intelligence: Analyzing artificial intelligence and data center energy\nconsumption. EPRI White Pap. no. 3002028905 (2024).\n39. Dastin, J. & Nellis, S. Focus: For tech giants, ai like bing and bard poses billion-dollar search problem. Reuters (2023).\n40. De Vries, A. The growing energy footprint of artificial intelligence. Joule 7, 2191–2194 (2023).\n41. Hölzle, U. Powering a google search (2009).\n42. Inês Trindade Pereira. ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume? Euronews\n(online). Accessed: 2025-06-01.\n43. Hamish van der Ven. AI is bad for the environment, and the problem is bigger than energy consumption. The Conversation\n(online). Accessed: 2025-06-01.\n44. Spencer Kimball. Data centers powering artificial intelligence could use more electricity than entire cities. CNBC (online).\nAccessed: 2025-06-01.\n45. James O’Donnell. DeepSeek might not be such good news for energy after all. MIT Technology Review (online). Accessed:\n2025-06-01.\n46. Berry Zwets. Researchers claim to cut energy consumption AI 95 percent. Techzine (online). Accessed: 2025-06-01.\n47. Adam Clark Estes. Should you feel guilty about using AI? V ox (online). Accessed: 2025-06-01.\n48. Degot, C., Duranton, S., Frédeau, M. & Hutchinson, R. Reduce carbon and costs with the power of ai. Boston Consult.\nGroup 26 (2021).\n49. Dannouni, A. et al. Accelerating climate action with ai. Boston Consult. Group Special Rep. Google (2023).\n50. ITU. Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions\nimpact greenhouse gas emissions of other sectors. Tech. Rep. ITU-T L.1480, ITU (2022). Accessed: 2025-06-01.\n51. WBCSD. Guidance on Avoided Emissions. Tech. Rep., WBCSD (2023). Accessed: 2025-06-01.\n52. Das, K. P. & Chandra, J. A survey on artificial intelligence for reducing the climate footprint in healthcare. Energy Nexus\n9, 100167 (2023).\n53. The Environment. Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).\n54. Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023).\n55. Rolnick, D. et al. Tackling climate change with machine learning. ACM Comput. Surv. (CSUR) 55, 1–96 (2022).\n9/12", "sentences": [{"text": "25.", "metadata": {}}, {"text": "Schneider, I.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025).", "metadata": {}}, {"text": "2502.01671.", "metadata": {}}, {"text": "26.", "metadata": {}}, {"text": "Morrison, J.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Holistically evaluating the environmental impact of creating language models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2503.05804 (2025).", "metadata": {}}, {"text": "27.", "metadata": {}}, {"text": "Epoch AI.", "metadata": {}}, {"text": "Data on notable ai models (2024).", "metadata": {}}, {"text": "Accessed: 2025-04-06.", "metadata": {}}, {"text": "28.", "metadata": {}}, {"text": "OpenRouter.", "metadata": {}}, {"text": "Openrouter leaderboard rankings.", "metadata": {}}, {"text": "https://openrouter.ai/rankings?view=month (2025).", "metadata": {}}, {"text": "Accessed: 2025-06-03.", "metadata": {}}, {"text": "29.", "metadata": {}}, {"text": "Lovins, A.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Artificial intelligence meets natural stupidity: Managing the risks (2025).", "metadata": {}}, {"text": "30.", "metadata": {}}, {"text": "Vaswani, A.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Attention is all you need.", "metadata": {}}, {"text": "Adv.", "metadata": {}}, {"text": "neural information processing systems 30 (2017).", "metadata": {}}, {"text": "31.", "metadata": {}}, {"text": "So, D., Le, Q.", "metadata": {}}, {"text": "& Liang, C.", "metadata": {}}, {"text": "The evolved transformer.", "metadata": {}}, {"text": "In Chaudhuri, K.", "metadata": {}}, {"text": "& Salakhutdinov, R.", "metadata": {}}, {"text": "(eds.) Proceedings of the\n36th International Conference on Machine Learning, vol.", "metadata": {}}, {"text": "97 of Proceedings of Machine Learning Research, 5877–5886\n(PMLR, 2019).", "metadata": {}}, {"text": "32.", "metadata": {}}, {"text": "Hao, K.", "metadata": {}}, {"text": "Training a single ai model can emit as much carbon as five cars in their lifetimes.", "metadata": {}}, {"text": "MIT technology Rev.", "metadata": {}}, {"text": "75, 103\n(2019).", "metadata": {}}, {"text": "33.", "metadata": {}}, {"text": "Toews, R.", "metadata": {}}, {"text": "Deep learning’s carbon emissions problem.", "metadata": {}}, {"text": "Forbes (2020).", "metadata": {}}, {"text": "34.", "metadata": {}}, {"text": "Gemma Team et al.", "metadata": {}}, {"text": "Gemma 2: Improving open language models at a practical size (2024).", "metadata": {}}, {"text": "2408.00118.", "metadata": {}}, {"text": "35.", "metadata": {}}, {"text": "Meta.", "metadata": {}}, {"text": "Llama 3.1 Model Card.", "metadata": {}}, {"text": "https://huggingface.co/meta-llama/Llama-3.1-8B (2024).", "metadata": {}}, {"text": "Hugging Face Model Card.", "metadata": {}}, {"text": "36.", "metadata": {}}, {"text": "Kerr, D.", "metadata": {}}, {"text": "& NPR.", "metadata": {}}, {"text": "AI brings soaring emissions for Google and Microsoft, a major contributor to climate change.", "metadata": {}}, {"text": "NPR, July\n(2024).", "metadata": {}}, {"text": "37.", "metadata": {}}, {"text": "Chen, S.", "metadata": {}}, {"text": "How much energy will ai really consume?", "metadata": {}}, {"text": "the good, the bad and the unknown.", "metadata": {}}, {"text": "Nature 639, 22–24 (2025).", "metadata": {}}, {"text": "38.", "metadata": {}}, {"text": "Aljbour, J., Wilson, T.", "metadata": {}}, {"text": "& Patel, P.", "metadata": {}}, {"text": "Powering intelligence: Analyzing artificial intelligence and data center energy\nconsumption.", "metadata": {}}, {"text": "EPRI White Pap.", "metadata": {}}, {"text": "no.", "metadata": {}}, {"text": "3002028905 (2024).", "metadata": {}}, {"text": "39.", "metadata": {}}, {"text": "Dastin, J.", "metadata": {}}, {"text": "& Nellis, S.", "metadata": {}}, {"text": "Focus: For tech giants, ai like bing and bard poses billion-dollar search problem.", "metadata": {}}, {"text": "Reuters (2023).", "metadata": {}}, {"text": "40.", "metadata": {}}, {"text": "De Vries, A.", "metadata": {}}, {"text": "The growing energy footprint of artificial intelligence.", "metadata": {}}, {"text": "Joule 7, 2191–2194 (2023).", "metadata": {}}, {"text": "41.", "metadata": {}}, {"text": "Hölzle, U.", "metadata": {}}, {"text": "Powering a google search (2009).", "metadata": {}}, {"text": "42.", "metadata": {}}, {"text": "Inês Trindade Pereira.", "metadata": {}}, {"text": "ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume?", "metadata": {}}, {"text": "Euronews\n(online).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "43.", "metadata": {}}, {"text": "Hamish van der Ven.", "metadata": {}}, {"text": "AI is bad for the environment, and the problem is bigger than energy consumption.", "metadata": {}}, {"text": "The Conversation\n(online).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "44.", "metadata": {}}, {"text": "Spencer Kimball.", "metadata": {}}, {"text": "Data centers powering artificial intelligence could use more electricity than entire cities.", "metadata": {}}, {"text": "CNBC (online).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "45.", "metadata": {}}, {"text": "James O’Donnell.", "metadata": {}}, {"text": "DeepSeek might not be such good news for energy after all.", "metadata": {}}, {"text": "MIT Technology Review (online).", "metadata": {}}, {"text": "Accessed:\n2025-06-01.", "metadata": {}}, {"text": "46.", "metadata": {}}, {"text": "Berry Zwets.", "metadata": {}}, {"text": "Researchers claim to cut energy consumption AI 95 percent.", "metadata": {}}, {"text": "Techzine (online).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "47.", "metadata": {}}, {"text": "Adam Clark Estes.", "metadata": {}}, {"text": "Should you feel guilty about using AI?", "metadata": {}}, {"text": "V ox (online).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "48.", "metadata": {}}, {"text": "Degot, C., Duranton, S., Frédeau, M.", "metadata": {}}, {"text": "& Hutchinson, R.", "metadata": {}}, {"text": "Reduce carbon and costs with the power of ai.", "metadata": {}}, {"text": "Boston Consult.", "metadata": {}}, {"text": "Group 26 (2021).", "metadata": {}}, {"text": "49.", "metadata": {}}, {"text": "Dannouni, A.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Accelerating climate action with ai.", "metadata": {}}, {"text": "Boston Consult.", "metadata": {}}, {"text": "Group Special Rep.", "metadata": {}}, {"text": "Google (2023).", "metadata": {}}, {"text": "50.", "metadata": {}}, {"text": "ITU.", "metadata": {}}, {"text": "Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions\nimpact greenhouse gas emissions of other sectors.", "metadata": {}}, {"text": "Tech.", "metadata": {}}, {"text": "Rep.", "metadata": {}}, {"text": "ITU-T L.1480, ITU (2022).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "51.", "metadata": {}}, {"text": "WBCSD.", "metadata": {}}, {"text": "Guidance on Avoided Emissions.", "metadata": {}}, {"text": "Tech.", "metadata": {}}, {"text": "Rep., WBCSD (2023).", "metadata": {}}, {"text": "Accessed: 2025-06-01.", "metadata": {}}, {"text": "52.", "metadata": {}}, {"text": "Das, K.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "& Chandra, J.", "metadata": {}}, {"text": "A survey on artificial intelligence for reducing the climate footprint in healthcare.", "metadata": {}}, {"text": "Energy Nexus\n9, 100167 (2023).", "metadata": {}}, {"text": "53.", "metadata": {}}, {"text": "The Environment.", "metadata": {}}, {"text": "Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).", "metadata": {}}, {"text": "54.", "metadata": {}}, {"text": "Kakkad, R.", "metadata": {}}, {"text": "Google says AI could mitigate 5 to 10% of global emissions (2023).", "metadata": {}}, {"text": "55.", "metadata": {}}, {"text": "Rolnick, D.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Tackling climate change with machine learning.", "metadata": {}}, {"text": "ACM Comput.", "metadata": {}}, {"text": "Surv.", "metadata": {}}, {"text": "(CSUR) 55, 1–96 (2022).", "metadata": {}}, {"text": "9/12", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "56. Ambrose, J. & Hern, A. Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).\n57. Schmidt, V .et al. Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).\n58. Lannelongue, L., Grealey, J. & Inouye, M. Green algorithms: Quantifying the carbon footprint of computation. Adv. Sci.\n2100707 (2021).\n59. Mitchell, M. et al. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and\ntransparency, 220–229 (2019).\n60. Luccioni, S. & Gamazaychikov, B. AI Models Hiding Their Energy Footprint? Here’s What You Can Do (2025).\n61. Leal Filho, W. et al. European sustainability reporting standards: An assessment of requirements and preparedness of eu\ncompanies. J. environmental management 380, 125008 (2025).\n62. Gamazaychikov, B. Unveiling salesforce’s blueprint for sustainable ai: Where responsibility meets innovation (2023).\n63. Touvron, H. et al. Llama: Open and efficient foundation language models (2023). 2302.13971.\n64. Mesnard, T. et al. Gemma: Open models based on gemini research and technology (2024). 2403.08295.\n65. Meta. Llama 4 Model Card. https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct (2025). Hugging\nFace Model Card.\n66. Meta. Llama 3 Model Card. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md (2024). GitHub\nRepository.\nAuthor contributions statement\nB.G. conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis. All authors wrote,\nedited and reviewed the manuscript.\n10/12", "sentences": [{"text": "56.", "metadata": {}}, {"text": "Ambrose, J.", "metadata": {}}, {"text": "& Hern, A.", "metadata": {}}, {"text": "Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).", "metadata": {}}, {"text": "57.", "metadata": {}}, {"text": "Schmidt, V .et al.", "metadata": {}}, {"text": "Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).", "metadata": {}}, {"text": "58.", "metadata": {}}, {"text": "Lannelongue, L., Grealey, J.", "metadata": {}}, {"text": "& Inouye, M.", "metadata": {}}, {"text": "Green algorithms: Quantifying the carbon footprint of computation.", "metadata": {}}, {"text": "Adv.", "metadata": {}}, {"text": "Sci.", "metadata": {}}, {"text": "2100707 (2021).", "metadata": {}}, {"text": "59.", "metadata": {}}, {"text": "Mitchell, M.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Model cards for model reporting.", "metadata": {}}, {"text": "In Proceedings of the conference on fairness, accountability, and\ntransparency, 220–229 (2019).", "metadata": {}}, {"text": "60.", "metadata": {}}, {"text": "Luccioni, S.", "metadata": {}}, {"text": "& Gamazaychikov, B.", "metadata": {}}, {"text": "AI Models Hiding Their Energy Footprint?", "metadata": {}}, {"text": "Here’s What You Can Do (2025).", "metadata": {}}, {"text": "61.", "metadata": {}}, {"text": "Leal Filho, W.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "European sustainability reporting standards: An assessment of requirements and preparedness of eu\ncompanies.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "environmental management 380, 125008 (2025).", "metadata": {}}, {"text": "62.", "metadata": {}}, {"text": "Gamazaychikov, B.", "metadata": {}}, {"text": "Unveiling salesforce’s blueprint for sustainable ai: Where responsibility meets innovation (2023).", "metadata": {}}, {"text": "63.", "metadata": {}}, {"text": "Touvron, H.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Llama: Open and efficient foundation language models (2023).", "metadata": {}}, {"text": "2302.13971.", "metadata": {}}, {"text": "64.", "metadata": {}}, {"text": "Mesnard, T.", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "Gemma: Open models based on gemini research and technology (2024).", "metadata": {}}, {"text": "2403.08295.", "metadata": {}}, {"text": "65.", "metadata": {}}, {"text": "Meta.", "metadata": {}}, {"text": "Llama 4 Model Card.", "metadata": {}}, {"text": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct (2025).", "metadata": {}}, {"text": "Hugging\nFace Model Card.", "metadata": {}}, {"text": "66.", "metadata": {}}, {"text": "Meta.", "metadata": {}}, {"text": "Llama 3 Model Card.", "metadata": {}}, {"text": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md (2024).", "metadata": {}}, {"text": "GitHub\nRepository.", "metadata": {}}, {"text": "Author contributions statement\nB.G.", "metadata": {}}, {"text": "conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis.", "metadata": {}}, {"text": "All authors wrote,\nedited and reviewed the manuscript.", "metadata": {}}, {"text": "10/12", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Appendix\nTable 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel Organization Energy Consumption (MWh) GHG Emissions (tCO2e)\nOLMo 20M 26 Ai2 0.8 0.3\nCodeGen 350M 62 Salesforce 71 6\nLlama 7B 63 Meta 356 14\nBLOOM 11 Big Science 520 30\nT5 16 Google 85.7 47\nOLMo 2 13B 26 Ai2 157 101\nGemma 2B + 9B 64 Google ? 131\nGPT-3 16 OpenAI 1,287 552\nLlama 4 Scout 65 Meta 3,500 1,354\nLlama 3 70B 66 Meta ? 1,900\nLlama 3.1 405B 35 Meta ? 8,930\nMax/Min Variance: 4,375 29,767\n11/12", "sentences": [{"text": "Appendix\nTable 1.", "metadata": {}}, {"text": "Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel Organization Energy Consumption (MWh) GHG Emissions (tCO2e)\nOLMo 20M 26 Ai2 0.8 0.3\nCodeGen 350M 62 Salesforce 71 6\nLlama 7B 63 Meta 356 14\nBLOOM 11 Big Science 520 30\nT5 16 Google 85.7 47\nOLMo 2 13B 26 Ai2 157 101\nGemma 2B + 9B 64 Google ?", "metadata": {}}, {"text": "131\nGPT-3 16 OpenAI 1,287 552\nLlama 4 Scout 65 Meta 3,500 1,354\nLlama 3 70B 66 Meta ?", "metadata": {}}, {"text": "1,900\nLlama 3.1 405B 35 Meta ?", "metadata": {}}, {"text": "8,930\nMax/Min Variance: 4,375 29,767\n11/12", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "Table 2. Range of Inference Energy Use21 (Representative Models Displayed)\nModel Organization GPU Energy for 1k Queries (Wh) Task\nbert-tiny-finetuned-squadv2 mrm8488 0.06 Extractive QA\nGIST-all-MiniLM-L6-v2 avsolatorio 0.11 Sentence Similarity\ndynamic_tinybert Intel 0.21 Extractive QA\ndistilbert-imdb lvwerra 0.22 Text Classification\nquestion_answering_v2 Falconsai 0.23 Extractive QA\nResnet 18 Microsoft 0.30 Image Classification\nyolos-tiny hustvl 1.00 Object Detection\nVision Perceiver Conv Google 2.64 Image Classification\nSFR-Embedding-Mistral Salesforce 5.22 Sentence Similarity\nyolos-base hustvl 7.98 Object Detection\nGemma 7B Google 18.90 Text Generation\nT5 11b Google 27.79 Text Classification\nphi-4 Microsoft 28.74 Text Generation\nT5 11b Google 178.13 Extractive QA\nMitsua Diffusion One Mitsua 186.81 Image Generation\nMixtral 8x7B Mistral 615.39 Text Generation\nStable Diffusion XL Base Stability AI 1,639.85 Image Generation\nLlama 3 70B Meta 1,719.66 Text Generation\nQwen2.5 72B Qwen 1,869.55 Text Generation\nCommand-R Plus Cohere 3,426.38 Text Generation\nMax/Min Variance: 57,106\n12/12", "sentences": [{"text": "Table 2.", "metadata": {}}, {"text": "Range of Inference Energy Use21 (Representative Models Displayed)\nModel Organization GPU Energy for 1k Queries (Wh) Task\nbert-tiny-finetuned-squadv2 mrm8488 0.06 Extractive QA\nGIST-all-MiniLM-L6-v2 avsolatorio 0.11 Sentence Similarity\ndynamic_tinybert Intel 0.21 Extractive QA\ndistilbert-imdb lvwerra 0.22 Text Classification\nquestion_answering_v2 Falconsai 0.23 Extractive QA\nResnet 18 Microsoft 0.30 Image Classification\nyolos-tiny hustvl 1.00 Object Detection\nVision Perceiver Conv Google 2.64 Image Classification\nSFR-Embedding-Mistral Salesforce 5.22 Sentence Similarity\nyolos-base hustvl 7.98 Object Detection\nGemma 7B Google 18.90 Text Generation\nT5 11b Google 27.79 Text Classification\nphi-4 Microsoft 28.74 Text Generation\nT5 11b Google 178.13 Extractive QA\nMitsua Diffusion One Mitsua 186.81 Image Generation\nMixtral 8x7B Mistral 615.39 Text Generation\nStable Diffusion XL Base Stability AI 1,639.85 Image Generation\nLlama 3 70B Meta 1,719.66 Text Generation\nQwen2.5 72B Qwen 1,869.55 Text Generation\nCommand-R Plus Cohere 3,426.38 Text Generation\nMax/Min Variance: 57,106\n12/12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}]}