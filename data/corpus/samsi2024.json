{"document_id": "samsi2024", "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference", "text": "From Words to Watts: Benchmarking the Energy\nCosts of Large Language Model Inference\nSiddharth Samsi ∗§, Dan Zhao †, Joseph McDonald ∗, Baolin Li ‡, Adam Michaleas ∗,\nMichael Jones ∗, William Bergeron ∗, Jeremy Kepner ∗, Devesh Tiwari ‡, Vijay Gadepally ∗\n∗ MIT, † NYU, ‡ Northeastern University\nAbstract—Large language models (LLMs) have exploded in\npopularity due to their new generative capabilities that go far\nbeyond prior state-of-the-art. These technologies are increasingly\nbeing leveraged in various domains such as law, finance, and\nmedicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for\ninference. Inference energy costs already receive less attention\nthan the energy costs of training LLMs—despite how often these\nlarge models are called on to conduct inference in reality (e.g.,\nChatGPT). As these state-of-the-art LLMs see increasing usage\nand deployment in various domains, a better understanding\nof their resource utilization is crucial for cost-savings, scaling\nperformance, efficient hardware usage, and optimal inference\nstrategies.\nIn this paper, we describe experiments conducted to study the\ncomputational and energy utilization of inference with LLMs. We\nbenchmark and conduct a preliminary analysis of the inference\nperformance and inference energy costs of different sizes of\nLLaMA—a recent state-of-the-art LLM—developed by Meta AI\non two generations of popular GPUs (NVIDIA V100 & A100)\nand two datasets (Alpaca and GSM8K) to reflect the diverse\nset of tasks/benchmarks for LLMs in research and practice.\nWe present the results of multi-node, multi-GPU inference using\nmodel sharding across up to 32 GPUs. To our knowledge, our\nwork is the one of the first to study LLM inference performance\nfrom the perspective of computational and energy resources at\nthis scale.\nIndex Terms —Large Language Models, Natural Language\nProcessing, Inference, Green AI, LLM, NLP, Deep Learning,\nDistributed Computing, Energy, Sustainability\nI. I NTRODUCTION\nGenerative models (GenAI) are able to produce new content\nfrom synthesizing text, images, and audio from which it’s\ntrained on. While GenAI is not entirely new, the recent\napplication and broad availability of this technology via tools\nsuch as Stable Diffusion [1], OpenAI’s ChatGPT, Google’s\nBard and integration into the Microsoft Bing search engine\nhas captured the imagination of the world and led to a massive\nsurge in interest in deploying these types of models across a\nDISTRIBUTION STATEMENT A. Approved for public release. Distribu-\ntion is unlimited. This material is based upon work supported by the Assistant\nSecretary of Defense for Research and Engineering under Air Force Contract\nNo. FA8702-15-D-0001, and United States Air Force Research Laboratory\nCooperative Agreement Number FA8750-19-2-1000. Any opinions, findings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\nof Defense for Research and Engineering, or the United States Air Force.\nThe U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.\nvariety of domains ranging such as education, government,\nengineering, law, finance and many more.\nThe popularity of these models has also put a spotlight on\nmany societal concerns stemming from their usage. From ethi-\ncal concerns ranging from violations of copyright laws [2], [3]\nto safety concerns arising from the fact that these models are\ncapable of hallucinating or fabricating information, concerns\nabout these models in the educational and medical domain [4],\n[5], their carbon footprint, and many more.\nIn this paper, we focus primarily on understanding the\nsignificant amount of resources—time, computation, and\nenergy—required for using and deploying some of the large\nlanguage models (LLM) like those that underlie ChatGPT,\nBard, etc. Several prior works have estimated the compute\nand energy costs of training language models. Works like [6]\ndiscuss the carbon footprint of language models such as BERT,\nELMo, and precursors to larger models such as GPT-3 and\nGPT-4 that power some of the popular AI chatbots today. Oth-\ners have also looked to larger language models; for instance,\nthe largest NVIDIA Megatron-LM model required 3,072 A100\nGPUs [7]–[9] for its training. While the complete details (time\nand resources used) of compute required for training GPT-\n3/4 are not available, several estimates for training [10], [11]\nand inference are publicly available. As industry attempts to\nshore up competitive moats and restrict information regarding\ntheir underlying LLM technologies, these details can become\nless reliable and available. Compounding this issue, estimates\nfor inference are even less readily available [12] despite their\nsignificant share of energy costs and their likely larger impact\non the environment [13]—especially since model inference\ncalls can occur more frequently than training/fine-tuning for\nreal-world deployments and applications.\nWe present the results of our inference experiments on\nLLaMA [14]: an open sourced pre-trained large language\nmodels by Meta AI. The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation). While\nour emphasis is on characterizing the compute performance\nand energy used for multi-node, multi-GPU inference, we\nalso include results from single node instances using smaller\nvariants of the model as a baseline comparison. We hope our\nwork will help illustrate some of the compute performance\nand energy utilization characteristics of LLM inference. We\nalso hope that our experiments, analysis, and data on real-\narXiv:2310.03003v1  [cs.CL]  4 Oct 2023\n\nworld hardware will spur further analysis, benchmarking,\nand more open dissemination of the systematic performance\ncharacteristics for a wider range of large models—especially\nunder different kinds of hardware, data, and optimization\nstrategies.\nII. O VERVIEW OF LARGE LANGUAGE MODELS\nThe landscape of large language models (LLMs) and large\nfoundation models (LFMs) has seen explosive growth in both\nthe speed of development as well as complexity of ever larger\nmodels. Over the past several years, competition has been\nfierce and the pace un-relenting as AI research groups across\nprivate companies and academic institutions have developed\nnew models whose performance continues to improve on a\nwide suite of natural language benchmarks but still requires\nsignificant amounts of compute and energy. We provide a brief\noverview of LLMs and LFMs below along with details around\nthe specific LLM we use for our analysis.\nFig. 1: Development paths of LLMs: A tree diagram illustrat-\ning the development of language models and foundation mod-\nels from 2017 to early 2023. Pink branches indicate encoder-\ntype language models, green indicates encoder-decoder hybrid\nmodels, and the dark grey indicates decoder-style models. The\nbar-plot on the bottom right tallies the number of open/closed\nsource models developed by different companies/institutions.\nWe study LLaMA (outlined by the red arrow and red circle in\nthe diagram above) as an example of one of the more recent,\nmodern, and state-of-the-art LLMs whose size/complexity\nresemble Google’s Bard and OpenAI’s GPT-4, all three of\nwhich were released around the same time (spring 2023).\nOriginal figure from [15].\nA. Large Language Models & Large F oundation Models\nAs seen in Fig. 1, many different LLMs and foundation\nmodels exist—each with their own respective training setup,\narchitectural modifications, purposes or use-cases, etc. Large\nlanguage models and foundation models are best known for\ntheir sheer size, resource intensity (i.e., the amount of com-\nputational resources required for training/inference), and their\nimpressive capabilities in tasks that include, but may not be\nlimited to, natural language.\nTypically, LLMs refer to language models containing on\nthe order of hundreds of millions to billions of parameters\nthat are trained on extremely large datasets of text. These\nmodels are also typically based on some variant of the original\ntransformer architecture [16] usually leveraging the decoder\nhalf or a hybrid encoder-decoder architecture. Large language\nmodels can be considered a subset of large foundation models;\nwhereas LLMs focus almost exclusively on language data\nfor their inputs and outputs, large foundation models include\nmodels that allow for multiple modalities such as image and\ntext (e.g., GPT-4) or other modalities such as image generation\n(e.g., Stable Diffusion) or video generation (e.g., MidJourney).\nWe refer to [17] for a comprehensive review of the broad\nclasses of GenAI and their capabilities.\nB. LLaMA\nDeveloped by Meta AI and released in February of 2023,\nLLaMA [14] (Large Language Model Meta AI) is a large lan-\nguage model (LLM) that relies on the traditional transformer\narchitecture originally introduced in [16]. Most notably, the\nperformance of LLaMA rivaled or exceeded that of GPT-3 on\nmany NLP benchmarks and remains competitive with other\nstate-of-the-art LLMs [14]. Like other LLMs, LLaMA was\npre-trained on a large collection of data including but not\nlimited to CommonCrawl, Github, Wikipedia, etc. As of spring\n2023, alongside other recently timed releases of state-of-the-art\nLLMs such as Google’s Bard and OpenAI’s GPT-4, LLaMA is\ncompetitive in its state-of-the-art performance across multiple\ntasks, making it an ideal workhorse for realistically studying\nand benchmarking inference.\nLLaMA comes in four sizes characterized by the number of\nparameters: 7 billion (LLaMA 7B), 13 billion (LLaMA 13B),\n33 billion (LLaMA 33B) and 65 (LLaMA 65B). LLaMA’s\nmodel weights, across all of its variants, were publicly released\nunder a non-commercial license, making it one of only a select\nfew modern, state-of-the-art LLMs that have been publicly\navailable.\nTo best understand the realities that lie behind the energy\ncosts and throughput of state-of-the-art LLM inference, we fo-\ncus our analysis on the largest available version of LLaMA—\nnamely, LLaMA 65B. We also conduct analysis comparing\nthe 7B and 13B LLaMA variants to establish the baseline\nperformance of the smaller variants of the LLaMA model.\nThe largest model we focus our analysis on, LLaMA 65B, is\na 65 billion parameter model with an effective model dimen-\nsion of 8,192 and a total of 80 layers and 64 attention heads,\ntrained over 1.4 trillion tokens. By focusing on the largest 65B\nversion, we also hope to study inference at its fullest scale,\ncontrolling for and benchmarking phenomena that we may not\nobserve on LLMs of smaller size or complexity. This way, we\ncan realistically benchmark and study the dynamics, as well\nas the implications, of inference energy costs and through-put\non a scale consistent with state-of-the-art LLMs that we see\nand use today.\n\n[Image page=2 idx=1 name=Im1.jpg] Size: 1464x1160, Data: 465436 bytes\n\nIII. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting τ = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as “Alpaca” in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA’s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of\n\nresponses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA’s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n\n(a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also\n\n[Image page=5 idx=1 name=Im2.png] Size: 1189x340, Data: 30465 bytes\n\n[Image page=5 idx=2 name=Im3.png] Size: 1189x340, Data: 30507 bytes\n\n[Image page=5 idx=3 name=Im4.png] Size: 1200x800, Data: 45163 bytes\n\n[Image page=5 idx=4 name=Im5.png] Size: 1200x800, Data: 44016 bytes\n\n[Image page=5 idx=5 name=Im6.png] Size: 1200x800, Data: 46263 bytes\n\n[Image page=5 idx=6 name=Im7.png] Size: 1200x800, Data: 46853 bytes\n\nFig. 5: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nincreases. However, we see little change in the average energy\nper token between max generation length 512 and 1024. For\ninstance, with length 512, we see that it takes about 3-4 Joules\nfor a output token, which is approximately the same amount\nfor length 512. As with energy per second, max generation\nlength seems to have a negligible effect on energy costs from\n512 to 1024. Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset; there exists a “sweet spot”\nat 16 shards where continuously increasing the batch size can\nactually reduce the energy per token at max generation length\n512. However, this disappears under max generation length\n1024 where increasing the batch size increases the energy per\ntoken. The definitive existence of this sweet spot for datasets\nof differing styles/complexities, or others like it, will require\nmore experimentation and benchmarking to establish.\nD. Energy per Response: LLaMA 65B\nFigures 8 and 9 show energy metrics in terms of responses\nfrom the 65B model. Like before, we see that increasing\nthe number of shards still tends to increase the energy costs\nof inference per response most overall while increasing the\nmaximum generation length from 512 (Figure 8) to 1024\n(Figure 9) does not induce a clear or significant effect in\ninference energy costs. Also like before, while we see slight\nincreases in energy costs per response generated within a\nshard configuration as batch size increases, but not consistently\nor significantly. Again, we see that for GSM8K, at max\nFig. 6: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\nFig. 7: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\n\n[Image page=6 idx=1 name=Im10.png] Size: 1200x800, Data: 46025 bytes\n\n[Image page=6 idx=2 name=Im11.png] Size: 1200x800, Data: 48406 bytes\n\n[Image page=6 idx=3 name=Im12.png] Size: 1200x800, Data: 46446 bytes\n\n[Image page=6 idx=4 name=Im13.png] Size: 1200x800, Data: 48843 bytes\n\n[Image page=6 idx=5 name=Im8.png] Size: 1200x800, Data: 46669 bytes\n\n[Image page=6 idx=6 name=Im9.png] Size: 1200x800, Data: 47268 bytes\n\nFig. 8: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\ngeneration length 512, increasing the batch size while keeping\nthe number of shards fixed at 16 is associated with a decrease\nin energy per response, which is consistent with what we\nobserved in energy per tokens in the same setting.\nE. Effects of GPU Power Capping on LLaMA 65B\nPower consumption in AI is an increasingly important\nconcern. In prior work, we have shown [25] that power capping\nGPUs during training of language models such as BERT [26]\nis an effective way of reducing the energy consumed training\nthese models. While the work in [25] focused on model\ntraining, in this paper, we focus on inference. In order to study\nthe effect of power capping on inference using large language\nmodels, we ran a limited set of experiments using LLaMA\n65B. We ran the 65B model on four 80GB A100 GPUs with\nthe power cap set at 250W, 175W and 150W.\nTable III shows the relative change in total inference time,\nenergy and token rate under power cap conditions. Results\nshown here are calculated relative to a power cap of 250W. For\na 30% reduction in power from 250W to 175W, the inference\ntime increases by an average of 6.7% for a corresponding aver-\nage reduction in total energy by 23.21%. However, a reduction\nin power cap to 150W results in a much more significant\n(19.49%) increase in average inference time. These results\nshow that power capping as an energy savings intervention\ncan be effective when applied appropriately. A static power\nFig. 9: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\ncap for all GPU workloads may not show the same effective-\nness depending on the task and additional experimentation is\nrequired to make broader recommendations.\nOutput Time Energy Token Rate\nlength % change % change % change\n175W 150W 175W 150W 175W 150W\n256 6.23 15.33 -21.82 -32.76 -5.87 -13.15\n512 6.51 21.70 -23.95 -34.66 -6.11 -17.83\n1024 7.40 21.65 -23.87 -34.59 -6.89 -17.80\nTABLE III: Effects of GPU power capping on LLaMA 65B\ninference: This table shows the relative performance of the\nLLaMA 65B model on the GSM8k dataset with a batch size of\n64 and output lengths of 256, 512, 1024 using NVIDIA A100\nGPUs. The GPUs were power capped at 250W, 175W and\n150W. Results shown here are relative to model performance\nat 250W to stay consistent with the settings in the rest of the\nexperiments described here.\nF . GPU Resource Utilization under Distributed Inference\nFinally, we briefly examine the average GPU resource\nutilization by the 65B LLaMA model when running model\nsharded inference. For the sake of simplicity, we only consider\na batch size of 64 and a maximum generated output length\n\n[Image page=7 idx=1 name=Im14.png] Size: 1200x800, Data: 42954 bytes\n\n[Image page=7 idx=2 name=Im15.png] Size: 1200x800, Data: 43522 bytes\n\n[Image page=7 idx=3 name=Im16.png] Size: 1200x800, Data: 43346 bytes\n\n[Image page=7 idx=4 name=Im17.png] Size: 1200x800, Data: 45428 bytes\n\nof 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference—\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today—our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the\n\nMIT SuperCloud team: William Arcand, William Bergeron,\nChansup Byun, Michael Houle, Anna Klein, Peter Michaleas,\nLauren Milechin, Julie Mullen, Albert Reuther, Antonio Rosa,\nand Charles Yee. The authors also wish to acknowledge the\nfollowing individuals for their contributions and support: Bob\nBond, Allan Vanterpool, Tucker Hamilton, Jeff Gottschalk,\nMike Kanaan, Charles Leiserson, Dave Martinez, Steve Rejto,\nMarc Zissman.\nREFERENCES\n[1] Stability-AI, “Stable Diffusion,” https://github.com/Stability-AI/\nStableDiffusion, 2023.\n[2] D. Foster, Generative deep learning . ” O’Reilly Media, Inc.”, 2022.\n[3] Z. Epstein, A. Hertzmann, the Investigators of Human Creativity\net al. , “Art and the science of generative ai,” Science, vol.\n380, no. 6650, pp. 1110–1111, 2023. [Online]. Available: https:\n//www.science.org/doi/abs/10.1126/science.adh4451\n[4] B. Mesk ´o and E. J. Topol, “The imperative for regulatory oversight\nof large language models (or generative ai) in healthcare,” npj Digital\nMedicine, vol. 6, no. 1, p. 120, 2023.\n[5] H. Zohny, J. McMillan, and M. King, “Ethics of generative ai,” Journal\nof Medical Ethics , vol. 49, no. 2, pp. 79–80, 2023. [Online]. Available:\nhttps://jme.bmj.com/content/49/2/79\n[6] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in NLP,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics . Florence,\nItaly: Association for Computational Linguistics, Jul. 2019, pp. 3645–\n3650.\n[7] M. Shoeybi, M. Patwary, R. Puri et al. , “Megatron-lm: Training multi-\nbillion parameter language models using model parallelism,” 2020.\n[8] D. Narayanan, M. Shoeybi, J. Casper et al. , “Efficient large-scale\nlanguage model training on gpu clusters using megatron-lm,” 2021.\n[9] “Nvidia/megatron-lm: Ongoing research training transformer models at\nscale,” https://github.com/NVIDIA/Megatron-LM, 2023.\n[10] J. Sevilla, L. Heim, A. Ho et al. , “Compute trends across three eras of\nmachine learning,” 2022.\n[11] D. Patel, “The ai brick wall – a practical limit for scaling dense\ntransformer models, and how gpt 4 will break past it,” https://www.\nsemianalysis.com/p/the-ai-brick-wall-a-practical-limit, 2023.\n[12] R. Desislavov, F. Mart ´ınez-Plumed, and J. Hern´andez-Orallo, “Trends in\nai inference energy consumption: Beyond the performance-vs-parameter\nlaws of deep learning,” Sustainable Computing: Informatics and Sys-\ntems, vol. 38, p. 100857, 2023.\n[13] D. Zhao, N. C. Frey, J. McDonald et al. , “A green(er) world for\na.i.” in 2022 IEEE International Parallel and Distributed Processing\nSymposium Workshops (IPDPSW) , 2022, pp. 742–750.\n[14] H. Touvron, T. Lavril, G. Izacard et al. , “Llama: Open and efficient\nfoundation language models,” 2023.\n[15] “Different development paths of llms,” https://www.interconnects.ai/p/\nllm-development-paths.\n[16] A. Vaswani, N. Shazeer, N. Parmar et al. , “Attention is all you need,”\n2017.\n[17] R. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not all you\nneed. a state of the art review of large generative ai models,” 2023.\n[18] A. Reuther, J. Kepner, C. Byun et al. , “Interactive supercomputing on\n40,000 cores for machine learning and data analysis,” in2018 IEEE High\nPerformance extreme Computing Conference (HPEC) . IEEE, 2018, pp.\n1–6.\n[19] Facebook Research, online, 2023. [Online]. Available: https://github.\ncom/facebookresearch/llama\n[20] FairScale authors, “Fairscale: A general purpose modular pytorch li-\nbrary for high performance and large scale training,” https://github.com/\nfacebookresearch/fairscale, 2021.\n[21] R. Taori, I. Gulrajani, T. Zhang et al., “Stanford alpaca: An instruction-\nfollowing llama model,” https://github.com/tatsu-lab/stanford alpaca,\n2023.\n[22] K. Cobbe, V . Kosaraju, M. Bavarian et al. , “Training verifiers to solve\nmath word problems,” arXiv preprint arXiv:2110.14168 , 2021.\n[23] NVIDIA. Nvidia-smi. [Online]. Available: http://developer.download.\nnvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n[24] ——. Nvidia data center GPU manager (dcgm). [Online]. Available:\nhttps://developer.nvidia.com/dcgm\n[25] J. McDonald, B. Li, N. Frey et al. , “Great power, great responsibility:\nRecommendations for reducing energy for training language models,”\nin Findings of the Association for Computational Linguistics: NAACL\n2022, 2022, pp. 1962–1970.\n[26] J. Devlin, M.-W. Chang, K. Lee et al. , “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019, pp.\n4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n[27] NVIDIA, “Multi-Process Service,” https://docs.nvidia.com/deploy/mps/,\n2023.\n[28] NVIDIA, “NVIDIA Multi Instance GPU User Guide,” https://docs.\nnvidia.com/datacenter/tesla/mig-user-guide/, 2023.\n[29] B. Li, T. Patel, S. Samsi et al. , “Miso: exploiting multi-instance gpu\ncapability on multi-tenant gpu clusters,” in Proceedings of the 13th\nSymposium on Cloud Computing , 2022, pp. 173–189.", "metadata": {"url": "https://arxiv.org/pdf/2310.03003", "type": "paper", "year": "2023"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "From Words to Watts: Benchmarking the Energy\nCosts of Large Language Model Inference\nSiddharth Samsi ∗§, Dan Zhao †, Joseph McDonald ∗, Baolin Li ‡, Adam Michaleas ∗,\nMichael Jones ∗, William Bergeron ∗, Jeremy Kepner ∗, Devesh Tiwari ‡, Vijay Gadepally ∗\n∗ MIT, † NYU, ‡ Northeastern University\nAbstract—Large language models (LLMs) have exploded in\npopularity due to their new generative capabilities that go far\nbeyond prior state-of-the-art. These technologies are increasingly\nbeing leveraged in various domains such as law, finance, and\nmedicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for\ninference. Inference energy costs already receive less attention\nthan the energy costs of training LLMs—despite how often these\nlarge models are called on to conduct inference in reality (e.g.,\nChatGPT). As these state-of-the-art LLMs see increasing usage\nand deployment in various domains, a better understanding\nof their resource utilization is crucial for cost-savings, scaling\nperformance, efficient hardware usage, and optimal inference\nstrategies.\nIn this paper, we describe experiments conducted to study the\ncomputational and energy utilization of inference with LLMs. We\nbenchmark and conduct a preliminary analysis of the inference\nperformance and inference energy costs of different sizes of\nLLaMA—a recent state-of-the-art LLM—developed by Meta AI\non two generations of popular GPUs (NVIDIA V100 & A100)\nand two datasets (Alpaca and GSM8K) to reflect the diverse\nset of tasks/benchmarks for LLMs in research and practice.\nWe present the results of multi-node, multi-GPU inference using\nmodel sharding across up to 32 GPUs. To our knowledge, our\nwork is the one of the first to study LLM inference performance\nfrom the perspective of computational and energy resources at\nthis scale.\nIndex Terms —Large Language Models, Natural Language\nProcessing, Inference, Green AI, LLM, NLP, Deep Learning,\nDistributed Computing, Energy, Sustainability\nI. I NTRODUCTION\nGenerative models (GenAI) are able to produce new content\nfrom synthesizing text, images, and audio from which it’s\ntrained on. While GenAI is not entirely new, the recent\napplication and broad availability of this technology via tools\nsuch as Stable Diffusion [1], OpenAI’s ChatGPT, Google’s\nBard and integration into the Microsoft Bing search engine\nhas captured the imagination of the world and led to a massive\nsurge in interest in deploying these types of models across a\nDISTRIBUTION STATEMENT A. Approved for public release. Distribu-\ntion is unlimited. This material is based upon work supported by the Assistant\nSecretary of Defense for Research and Engineering under Air Force Contract\nNo. FA8702-15-D-0001, and United States Air Force Research Laboratory\nCooperative Agreement Number FA8750-19-2-1000. Any opinions, findings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\nof Defense for Research and Engineering, or the United States Air Force.\nThe U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.\nvariety of domains ranging such as education, government,\nengineering, law, finance and many more.\nThe popularity of these models has also put a spotlight on\nmany societal concerns stemming from their usage. From ethi-\ncal concerns ranging from violations of copyright laws [2], [3]\nto safety concerns arising from the fact that these models are\ncapable of hallucinating or fabricating information, concerns\nabout these models in the educational and medical domain [4],\n[5], their carbon footprint, and many more.\nIn this paper, we focus primarily on understanding the\nsignificant amount of resources—time, computation, and\nenergy—required for using and deploying some of the large\nlanguage models (LLM) like those that underlie ChatGPT,\nBard, etc. Several prior works have estimated the compute\nand energy costs of training language models. Works like [6]\ndiscuss the carbon footprint of language models such as BERT,\nELMo, and precursors to larger models such as GPT-3 and\nGPT-4 that power some of the popular AI chatbots today. Oth-\ners have also looked to larger language models; for instance,\nthe largest NVIDIA Megatron-LM model required 3,072 A100\nGPUs [7]–[9] for its training. While the complete details (time\nand resources used) of compute required for training GPT-\n3/4 are not available, several estimates for training [10], [11]\nand inference are publicly available. As industry attempts to\nshore up competitive moats and restrict information regarding\ntheir underlying LLM technologies, these details can become\nless reliable and available. Compounding this issue, estimates\nfor inference are even less readily available [12] despite their\nsignificant share of energy costs and their likely larger impact\non the environment [13]—especially since model inference\ncalls can occur more frequently than training/fine-tuning for\nreal-world deployments and applications.\nWe present the results of our inference experiments on\nLLaMA [14]: an open sourced pre-trained large language\nmodels by Meta AI. The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation). While\nour emphasis is on characterizing the compute performance\nand energy used for multi-node, multi-GPU inference, we\nalso include results from single node instances using smaller\nvariants of the model as a baseline comparison. We hope our\nwork will help illustrate some of the compute performance\nand energy utilization characteristics of LLM inference. We\nalso hope that our experiments, analysis, and data on real-\narXiv:2310.03003v1  [cs.CL]  4 Oct 2023", "sentences": [{"text": "From Words to Watts: Benchmarking the Energy\nCosts of Large Language Model Inference\nSiddharth Samsi ∗§, Dan Zhao †, Joseph McDonald ∗, Baolin Li ‡, Adam Michaleas ∗,\nMichael Jones ∗, William Bergeron ∗, Jeremy Kepner ∗, Devesh Tiwari ‡, Vijay Gadepally ∗\n∗ MIT, † NYU, ‡ Northeastern University\nAbstract—Large language models (LLMs) have exploded in\npopularity due to their new generative capabilities that go far\nbeyond prior state-of-the-art.", "metadata": {}}, {"text": "These technologies are increasingly\nbeing leveraged in various domains such as law, finance, and\nmedicine.", "metadata": {}}, {"text": "However, these models carry significant computational\nchallenges, especially the compute and energy costs required for\ninference.", "metadata": {}}, {"text": "Inference energy costs already receive less attention\nthan the energy costs of training LLMs—despite how often these\nlarge models are called on to conduct inference in reality (e.g.,\nChatGPT).", "metadata": {}}, {"text": "As these state-of-the-art LLMs see increasing usage\nand deployment in various domains, a better understanding\nof their resource utilization is crucial for cost-savings, scaling\nperformance, efficient hardware usage, and optimal inference\nstrategies.", "metadata": {}}, {"text": "In this paper, we describe experiments conducted to study the\ncomputational and energy utilization of inference with LLMs.", "metadata": {}}, {"text": "We\nbenchmark and conduct a preliminary analysis of the inference\nperformance and inference energy costs of different sizes of\nLLaMA—a recent state-of-the-art LLM—developed by Meta AI\non two generations of popular GPUs (NVIDIA V100 & A100)\nand two datasets (Alpaca and GSM8K) to reflect the diverse\nset of tasks/benchmarks for LLMs in research and practice.", "metadata": {}}, {"text": "We present the results of multi-node, multi-GPU inference using\nmodel sharding across up to 32 GPUs.", "metadata": {}}, {"text": "To our knowledge, our\nwork is the one of the first to study LLM inference performance\nfrom the perspective of computational and energy resources at\nthis scale.", "metadata": {}}, {"text": "Index Terms —Large Language Models, Natural Language\nProcessing, Inference, Green AI, LLM, NLP, Deep Learning,\nDistributed Computing, Energy, Sustainability\nI.", "metadata": {}}, {"text": "I NTRODUCTION\nGenerative models (GenAI) are able to produce new content\nfrom synthesizing text, images, and audio from which it’s\ntrained on.", "metadata": {}}, {"text": "While GenAI is not entirely new, the recent\napplication and broad availability of this technology via tools\nsuch as Stable Diffusion [1], OpenAI’s ChatGPT, Google’s\nBard and integration into the Microsoft Bing search engine\nhas captured the imagination of the world and led to a massive\nsurge in interest in deploying these types of models across a\nDISTRIBUTION STATEMENT A.", "metadata": {}}, {"text": "Approved for public release.", "metadata": {}}, {"text": "Distribu-\ntion is unlimited.", "metadata": {}}, {"text": "This material is based upon work supported by the Assistant\nSecretary of Defense for Research and Engineering under Air Force Contract\nNo.", "metadata": {}}, {"text": "FA8702-15-D-0001, and United States Air Force Research Laboratory\nCooperative Agreement Number FA8750-19-2-1000.", "metadata": {}}, {"text": "Any opinions, findings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\nof Defense for Research and Engineering, or the United States Air Force.", "metadata": {}}, {"text": "The U.S.", "metadata": {}}, {"text": "Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.", "metadata": {}}, {"text": "variety of domains ranging such as education, government,\nengineering, law, finance and many more.", "metadata": {}}, {"text": "The popularity of these models has also put a spotlight on\nmany societal concerns stemming from their usage.", "metadata": {}}, {"text": "From ethi-\ncal concerns ranging from violations of copyright laws [2], [3]\nto safety concerns arising from the fact that these models are\ncapable of hallucinating or fabricating information, concerns\nabout these models in the educational and medical domain [4],\n[5], their carbon footprint, and many more.", "metadata": {}}, {"text": "In this paper, we focus primarily on understanding the\nsignificant amount of resources—time, computation, and\nenergy—required for using and deploying some of the large\nlanguage models (LLM) like those that underlie ChatGPT,\nBard, etc.", "metadata": {}}, {"text": "Several prior works have estimated the compute\nand energy costs of training language models.", "metadata": {}}, {"text": "Works like [6]\ndiscuss the carbon footprint of language models such as BERT,\nELMo, and precursors to larger models such as GPT-3 and\nGPT-4 that power some of the popular AI chatbots today.", "metadata": {}}, {"text": "Oth-\ners have also looked to larger language models;", "metadata": {}}, {"text": "for instance,\nthe largest NVIDIA Megatron-LM model required 3,072 A100\nGPUs [7]–[9] for its training.", "metadata": {}}, {"text": "While the complete details (time\nand resources used) of compute required for training GPT-\n3/4 are not available, several estimates for training [10], [11]\nand inference are publicly available.", "metadata": {}}, {"text": "As industry attempts to\nshore up competitive moats and restrict information regarding\ntheir underlying LLM technologies, these details can become\nless reliable and available.", "metadata": {}}, {"text": "Compounding this issue, estimates\nfor inference are even less readily available [12] despite their\nsignificant share of energy costs and their likely larger impact\non the environment [13]—especially since model inference\ncalls can occur more frequently than training/fine-tuning for\nreal-world deployments and applications.", "metadata": {}}, {"text": "We present the results of our inference experiments on\nLLaMA [14]: an open sourced pre-trained large language\nmodels by Meta AI.", "metadata": {}}, {"text": "The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation).", "metadata": {}}, {"text": "While\nour emphasis is on characterizing the compute performance\nand energy used for multi-node, multi-GPU inference, we\nalso include results from single node instances using smaller\nvariants of the model as a baseline comparison.", "metadata": {}}, {"text": "We hope our\nwork will help illustrate some of the compute performance\nand energy utilization characteristics of LLM inference.", "metadata": {}}, {"text": "We\nalso hope that our experiments, analysis, and data on real-\narXiv:2310.03003v1  [cs.CL]  4 Oct 2023", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "world hardware will spur further analysis, benchmarking,\nand more open dissemination of the systematic performance\ncharacteristics for a wider range of large models—especially\nunder different kinds of hardware, data, and optimization\nstrategies.\nII. O VERVIEW OF LARGE LANGUAGE MODELS\nThe landscape of large language models (LLMs) and large\nfoundation models (LFMs) has seen explosive growth in both\nthe speed of development as well as complexity of ever larger\nmodels. Over the past several years, competition has been\nfierce and the pace un-relenting as AI research groups across\nprivate companies and academic institutions have developed\nnew models whose performance continues to improve on a\nwide suite of natural language benchmarks but still requires\nsignificant amounts of compute and energy. We provide a brief\noverview of LLMs and LFMs below along with details around\nthe specific LLM we use for our analysis.\nFig. 1: Development paths of LLMs: A tree diagram illustrat-\ning the development of language models and foundation mod-\nels from 2017 to early 2023. Pink branches indicate encoder-\ntype language models, green indicates encoder-decoder hybrid\nmodels, and the dark grey indicates decoder-style models. The\nbar-plot on the bottom right tallies the number of open/closed\nsource models developed by different companies/institutions.\nWe study LLaMA (outlined by the red arrow and red circle in\nthe diagram above) as an example of one of the more recent,\nmodern, and state-of-the-art LLMs whose size/complexity\nresemble Google’s Bard and OpenAI’s GPT-4, all three of\nwhich were released around the same time (spring 2023).\nOriginal figure from [15].\nA. Large Language Models & Large F oundation Models\nAs seen in Fig. 1, many different LLMs and foundation\nmodels exist—each with their own respective training setup,\narchitectural modifications, purposes or use-cases, etc. Large\nlanguage models and foundation models are best known for\ntheir sheer size, resource intensity (i.e., the amount of com-\nputational resources required for training/inference), and their\nimpressive capabilities in tasks that include, but may not be\nlimited to, natural language.\nTypically, LLMs refer to language models containing on\nthe order of hundreds of millions to billions of parameters\nthat are trained on extremely large datasets of text. These\nmodels are also typically based on some variant of the original\ntransformer architecture [16] usually leveraging the decoder\nhalf or a hybrid encoder-decoder architecture. Large language\nmodels can be considered a subset of large foundation models;\nwhereas LLMs focus almost exclusively on language data\nfor their inputs and outputs, large foundation models include\nmodels that allow for multiple modalities such as image and\ntext (e.g., GPT-4) or other modalities such as image generation\n(e.g., Stable Diffusion) or video generation (e.g., MidJourney).\nWe refer to [17] for a comprehensive review of the broad\nclasses of GenAI and their capabilities.\nB. LLaMA\nDeveloped by Meta AI and released in February of 2023,\nLLaMA [14] (Large Language Model Meta AI) is a large lan-\nguage model (LLM) that relies on the traditional transformer\narchitecture originally introduced in [16]. Most notably, the\nperformance of LLaMA rivaled or exceeded that of GPT-3 on\nmany NLP benchmarks and remains competitive with other\nstate-of-the-art LLMs [14]. Like other LLMs, LLaMA was\npre-trained on a large collection of data including but not\nlimited to CommonCrawl, Github, Wikipedia, etc. As of spring\n2023, alongside other recently timed releases of state-of-the-art\nLLMs such as Google’s Bard and OpenAI’s GPT-4, LLaMA is\ncompetitive in its state-of-the-art performance across multiple\ntasks, making it an ideal workhorse for realistically studying\nand benchmarking inference.\nLLaMA comes in four sizes characterized by the number of\nparameters: 7 billion (LLaMA 7B), 13 billion (LLaMA 13B),\n33 billion (LLaMA 33B) and 65 (LLaMA 65B). LLaMA’s\nmodel weights, across all of its variants, were publicly released\nunder a non-commercial license, making it one of only a select\nfew modern, state-of-the-art LLMs that have been publicly\navailable.\nTo best understand the realities that lie behind the energy\ncosts and throughput of state-of-the-art LLM inference, we fo-\ncus our analysis on the largest available version of LLaMA—\nnamely, LLaMA 65B. We also conduct analysis comparing\nthe 7B and 13B LLaMA variants to establish the baseline\nperformance of the smaller variants of the LLaMA model.\nThe largest model we focus our analysis on, LLaMA 65B, is\na 65 billion parameter model with an effective model dimen-\nsion of 8,192 and a total of 80 layers and 64 attention heads,\ntrained over 1.4 trillion tokens. By focusing on the largest 65B\nversion, we also hope to study inference at its fullest scale,\ncontrolling for and benchmarking phenomena that we may not\nobserve on LLMs of smaller size or complexity. This way, we\ncan realistically benchmark and study the dynamics, as well\nas the implications, of inference energy costs and through-put\non a scale consistent with state-of-the-art LLMs that we see\nand use today.", "sentences": [{"text": "world hardware will spur further analysis, benchmarking,\nand more open dissemination of the systematic performance\ncharacteristics for a wider range of large models—especially\nunder different kinds of hardware, data, and optimization\nstrategies.", "metadata": {}}, {"text": "II.", "metadata": {}}, {"text": "O VERVIEW OF LARGE LANGUAGE MODELS\nThe landscape of large language models (LLMs) and large\nfoundation models (LFMs) has seen explosive growth in both\nthe speed of development as well as complexity of ever larger\nmodels.", "metadata": {}}, {"text": "Over the past several years, competition has been\nfierce and the pace un-relenting as AI research groups across\nprivate companies and academic institutions have developed\nnew models whose performance continues to improve on a\nwide suite of natural language benchmarks but still requires\nsignificant amounts of compute and energy.", "metadata": {}}, {"text": "We provide a brief\noverview of LLMs and LFMs below along with details around\nthe specific LLM we use for our analysis.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "1: Development paths of LLMs: A tree diagram illustrat-\ning the development of language models and foundation mod-\nels from 2017 to early 2023.", "metadata": {}}, {"text": "Pink branches indicate encoder-\ntype language models, green indicates encoder-decoder hybrid\nmodels, and the dark grey indicates decoder-style models.", "metadata": {}}, {"text": "The\nbar-plot on the bottom right tallies the number of open/closed\nsource models developed by different companies/institutions.", "metadata": {}}, {"text": "We study LLaMA (outlined by the red arrow and red circle in\nthe diagram above) as an example of one of the more recent,\nmodern, and state-of-the-art LLMs whose size/complexity\nresemble Google’s Bard and OpenAI’s GPT-4, all three of\nwhich were released around the same time (spring 2023).", "metadata": {}}, {"text": "Original figure from [15].", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Large Language Models & Large F oundation Models\nAs seen in Fig.", "metadata": {}}, {"text": "1, many different LLMs and foundation\nmodels exist—each with their own respective training setup,\narchitectural modifications, purposes or use-cases, etc.", "metadata": {}}, {"text": "Large\nlanguage models and foundation models are best known for\ntheir sheer size, resource intensity (i.e., the amount of com-\nputational resources required for training/inference), and their\nimpressive capabilities in tasks that include, but may not be\nlimited to, natural language.", "metadata": {}}, {"text": "Typically, LLMs refer to language models containing on\nthe order of hundreds of millions to billions of parameters\nthat are trained on extremely large datasets of text.", "metadata": {}}, {"text": "These\nmodels are also typically based on some variant of the original\ntransformer architecture [16] usually leveraging the decoder\nhalf or a hybrid encoder-decoder architecture.", "metadata": {}}, {"text": "Large language\nmodels can be considered a subset of large foundation models;", "metadata": {}}, {"text": "whereas LLMs focus almost exclusively on language data\nfor their inputs and outputs, large foundation models include\nmodels that allow for multiple modalities such as image and\ntext (e.g., GPT-4) or other modalities such as image generation\n(e.g., Stable Diffusion) or video generation (e.g., MidJourney).", "metadata": {}}, {"text": "We refer to [17] for a comprehensive review of the broad\nclasses of GenAI and their capabilities.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "LLaMA\nDeveloped by Meta AI and released in February of 2023,\nLLaMA [14] (Large Language Model Meta AI) is a large lan-\nguage model (LLM) that relies on the traditional transformer\narchitecture originally introduced in [16].", "metadata": {}}, {"text": "Most notably, the\nperformance of LLaMA rivaled or exceeded that of GPT-3 on\nmany NLP benchmarks and remains competitive with other\nstate-of-the-art LLMs [14].", "metadata": {}}, {"text": "Like other LLMs, LLaMA was\npre-trained on a large collection of data including but not\nlimited to CommonCrawl, Github, Wikipedia, etc.", "metadata": {}}, {"text": "As of spring\n2023, alongside other recently timed releases of state-of-the-art\nLLMs such as Google’s Bard and OpenAI’s GPT-4, LLaMA is\ncompetitive in its state-of-the-art performance across multiple\ntasks, making it an ideal workhorse for realistically studying\nand benchmarking inference.", "metadata": {}}, {"text": "LLaMA comes in four sizes characterized by the number of\nparameters: 7 billion (LLaMA 7B), 13 billion (LLaMA 13B),\n33 billion (LLaMA 33B) and 65 (LLaMA 65B).", "metadata": {}}, {"text": "LLaMA’s\nmodel weights, across all of its variants, were publicly released\nunder a non-commercial license, making it one of only a select\nfew modern, state-of-the-art LLMs that have been publicly\navailable.", "metadata": {}}, {"text": "To best understand the realities that lie behind the energy\ncosts and throughput of state-of-the-art LLM inference, we fo-\ncus our analysis on the largest available version of LLaMA—\nnamely, LLaMA 65B.", "metadata": {}}, {"text": "We also conduct analysis comparing\nthe 7B and 13B LLaMA variants to establish the baseline\nperformance of the smaller variants of the LLaMA model.", "metadata": {}}, {"text": "The largest model we focus our analysis on, LLaMA 65B, is\na 65 billion parameter model with an effective model dimen-\nsion of 8,192 and a total of 80 layers and 64 attention heads,\ntrained over 1.4 trillion tokens.", "metadata": {}}, {"text": "By focusing on the largest 65B\nversion, we also hope to study inference at its fullest scale,\ncontrolling for and benchmarking phenomena that we may not\nobserve on LLMs of smaller size or complexity.", "metadata": {}}, {"text": "This way, we\ncan realistically benchmark and study the dynamics, as well\nas the implications, of inference energy costs and through-put\non a scale consistent with state-of-the-art LLMs that we see\nand use today.", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.jpg] Size: 1464x1160, Data: 465436 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.jpg] Size: 1464x1160, Data: 465436 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.jpg", "image_width": 1464, "image_height": 1160, "attachment_type": "image", "has_image_data": true, "image_data_size": 465436}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "III. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting τ = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as “Alpaca” in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA’s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of", "sentences": [{"text": "III.", "metadata": {}}, {"text": "E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18].", "metadata": {}}, {"text": "This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node.", "metadata": {}}, {"text": "Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch.", "metadata": {}}, {"text": "The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches.", "metadata": {}}, {"text": "All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect.", "metadata": {}}, {"text": "The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.", "metadata": {}}, {"text": "In addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper.", "metadata": {}}, {"text": "A summary\nof the hardware is shown in Table I.", "metadata": {}}, {"text": "All experiments described\nin this paper were run exclusively on NVIDIA GPUs.", "metadata": {}}, {"text": "TABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments.", "metadata": {}}, {"text": "Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below.", "metadata": {}}, {"text": "All GPUs are from NVIDIA.", "metadata": {}}, {"text": "CPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA.", "metadata": {}}, {"text": "Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19].", "metadata": {}}, {"text": "This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes.", "metadata": {}}, {"text": "For the models, we use a decoder\ntemperature setting τ = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used.", "metadata": {}}, {"text": "In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference.", "metadata": {}}, {"text": "While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Datasets\nWe used two datasets to evaluate inference performance.", "metadata": {}}, {"text": "The first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as “Alpaca” in our paper which is not to be confused with\nthe Alpaca model).", "metadata": {}}, {"text": "This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer.", "metadata": {}}, {"text": "The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems.", "metadata": {}}, {"text": "The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance.", "metadata": {}}, {"text": "While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more.", "metadata": {}}, {"text": "Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc.", "metadata": {}}, {"text": "This, in turn, may impact inference performance in\neither throughput rates or energy costs.", "metadata": {}}, {"text": "For this reason, our\nbenchmarking experiments are conducted on both datasets.", "metadata": {}}, {"text": "For both datasets, we sample 4,096 inputs for our inference\nexperiments.", "metadata": {}}, {"text": "Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs.", "metadata": {}}, {"text": "We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work.", "metadata": {}}, {"text": "Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B.", "metadata": {}}, {"text": "While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work.", "metadata": {}}, {"text": "Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc.", "metadata": {}}, {"text": "which we also leave for future work.", "metadata": {}}, {"text": "Inference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second.", "metadata": {}}, {"text": "When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.", "metadata": {}}, {"text": "The number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces.", "metadata": {}}, {"text": "The number of tokens is calculated using LLaMA’s\nown default tokenizer by counting the number of tokens in the\ntokenized output.", "metadata": {}}, {"text": "Lastly, the number of responses per second\nor the response rate is calculated using the total number of", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA’s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).", "sentences": [{"text": "responses and the total time needed to run inference over the\ninput data.", "metadata": {}}, {"text": "We monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc.", "metadata": {}}, {"text": "during our experiments.", "metadata": {}}, {"text": "The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node.", "metadata": {}}, {"text": "For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used.", "metadata": {}}, {"text": "Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated.", "metadata": {}}, {"text": "Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.", "metadata": {}}, {"text": "Inference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA’s utilities described above.", "metadata": {}}, {"text": "Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds.", "metadata": {}}, {"text": "A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size.", "metadata": {}}, {"text": "Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.", "metadata": {}}, {"text": "IV.", "metadata": {}}, {"text": "R ESULTS\nA.", "metadata": {}}, {"text": "Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B.", "metadata": {}}, {"text": "The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference?", "metadata": {}}, {"text": "This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.", "metadata": {}}, {"text": "Given the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs).", "metadata": {}}, {"text": "Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.", "metadata": {}}, {"text": "With these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2.", "metadata": {}}, {"text": "The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively.", "metadata": {}}, {"text": "For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256.", "metadata": {}}, {"text": "The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.", "metadata": {}}, {"text": "These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.", "metadata": {}}, {"text": "While the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.", "metadata": {}}, {"text": "Model Size V100 32GB A100 80GB\nCount Max.", "metadata": {}}, {"text": "Batch size Count Max.", "metadata": {}}, {"text": "Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).", "metadata": {}}, {"text": "As expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second.", "metadata": {}}, {"text": "Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model.", "metadata": {}}, {"text": "We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100;", "metadata": {}}, {"text": "however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements.", "metadata": {}}, {"text": "We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K.", "metadata": {}}, {"text": "This can also be attributed to\nthe different complexities of inputs from each dataset.", "metadata": {}}, {"text": "2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.", "metadata": {}}, {"text": "For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model.", "metadata": {}}, {"text": "Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.", "metadata": {}}, {"text": "Moreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "(a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also", "sentences": [{"text": "(a) Results from the Alpaca dataset.", "metadata": {}}, {"text": "(b) Results from GSM8K dataset\nFig.", "metadata": {}}, {"text": "2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.", "metadata": {}}, {"text": "Color indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars.", "metadata": {}}, {"text": "Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch size.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds.", "metadata": {}}, {"text": "Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding.", "metadata": {}}, {"text": "Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.", "metadata": {}}, {"text": "Overall, we see an average increase in energy per second\nwith the number of shards.", "metadata": {}}, {"text": "While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.", "metadata": {}}, {"text": "Indeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards).", "metadata": {}}, {"text": "For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent.", "metadata": {}}, {"text": "Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also", "metadata": {}}], "metadata": {"page": 5}}, {"text": "[Image page=5 idx=1 name=Im2.png] Size: 1189x340, Data: 30465 bytes", "sentences": [{"text": "[Image page=5 idx=1 name=Im2.png] Size: 1189x340, Data: 30465 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 1, "image_name": "Im2.png", "image_width": 1189, "image_height": 340, "attachment_type": "image", "has_image_data": true, "image_data_size": 30465}}, {"text": "[Image page=5 idx=2 name=Im3.png] Size: 1189x340, Data: 30507 bytes", "sentences": [{"text": "[Image page=5 idx=2 name=Im3.png] Size: 1189x340, Data: 30507 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 2, "image_name": "Im3.png", "image_width": 1189, "image_height": 340, "attachment_type": "image", "has_image_data": true, "image_data_size": 30507}}, {"text": "[Image page=5 idx=3 name=Im4.png] Size: 1200x800, Data: 45163 bytes", "sentences": [{"text": "[Image page=5 idx=3 name=Im4.png] Size: 1200x800, Data: 45163 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 3, "image_name": "Im4.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 45163}}, {"text": "[Image page=5 idx=4 name=Im5.png] Size: 1200x800, Data: 44016 bytes", "sentences": [{"text": "[Image page=5 idx=4 name=Im5.png] Size: 1200x800, Data: 44016 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 4, "image_name": "Im5.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 44016}}, {"text": "[Image page=5 idx=5 name=Im6.png] Size: 1200x800, Data: 46263 bytes", "sentences": [{"text": "[Image page=5 idx=5 name=Im6.png] Size: 1200x800, Data: 46263 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 5, "image_name": "Im6.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 46263}}, {"text": "[Image page=5 idx=6 name=Im7.png] Size: 1200x800, Data: 46853 bytes", "sentences": [{"text": "[Image page=5 idx=6 name=Im7.png] Size: 1200x800, Data: 46853 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 6, "image_name": "Im7.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 46853}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nincreases. However, we see little change in the average energy\nper token between max generation length 512 and 1024. For\ninstance, with length 512, we see that it takes about 3-4 Joules\nfor a output token, which is approximately the same amount\nfor length 512. As with energy per second, max generation\nlength seems to have a negligible effect on energy costs from\n512 to 1024. Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset; there exists a “sweet spot”\nat 16 shards where continuously increasing the batch size can\nactually reduce the energy per token at max generation length\n512. However, this disappears under max generation length\n1024 where increasing the batch size increases the energy per\ntoken. The definitive existence of this sweet spot for datasets\nof differing styles/complexities, or others like it, will require\nmore experimentation and benchmarking to establish.\nD. Energy per Response: LLaMA 65B\nFigures 8 and 9 show energy metrics in terms of responses\nfrom the 65B model. Like before, we see that increasing\nthe number of shards still tends to increase the energy costs\nof inference per response most overall while increasing the\nmaximum generation length from 512 (Figure 8) to 1024\n(Figure 9) does not induce a clear or significant effect in\ninference energy costs. Also like before, while we see slight\nincreases in energy costs per response generated within a\nshard configuration as batch size increases, but not consistently\nor significantly. Again, we see that for GSM8K, at max\nFig. 6: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\nFig. 7: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "5: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch size.", "metadata": {}}, {"text": "increases.", "metadata": {}}, {"text": "However, we see little change in the average energy\nper token between max generation length 512 and 1024.", "metadata": {}}, {"text": "For\ninstance, with length 512, we see that it takes about 3-4 Joules\nfor a output token, which is approximately the same amount\nfor length 512.", "metadata": {}}, {"text": "As with energy per second, max generation\nlength seems to have a negligible effect on energy costs from\n512 to 1024.", "metadata": {}}, {"text": "Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset;", "metadata": {}}, {"text": "there exists a “sweet spot”\nat 16 shards where continuously increasing the batch size can\nactually reduce the energy per token at max generation length\n512.", "metadata": {}}, {"text": "However, this disappears under max generation length\n1024 where increasing the batch size increases the energy per\ntoken.", "metadata": {}}, {"text": "The definitive existence of this sweet spot for datasets\nof differing styles/complexities, or others like it, will require\nmore experimentation and benchmarking to establish.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Energy per Response: LLaMA 65B\nFigures 8 and 9 show energy metrics in terms of responses\nfrom the 65B model.", "metadata": {}}, {"text": "Like before, we see that increasing\nthe number of shards still tends to increase the energy costs\nof inference per response most overall while increasing the\nmaximum generation length from 512 (Figure 8) to 1024\n(Figure 9) does not induce a clear or significant effect in\ninference energy costs.", "metadata": {}}, {"text": "Also like before, while we see slight\nincreases in energy costs per response generated within a\nshard configuration as batch size increases, but not consistently\nor significantly.", "metadata": {}}, {"text": "Again, we see that for GSM8K, at max\nFig.", "metadata": {}}, {"text": "6: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch\nsize.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "7: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch size.", "metadata": {}}], "metadata": {"page": 6}}, {"text": "[Image page=6 idx=1 name=Im10.png] Size: 1200x800, Data: 46025 bytes", "sentences": [{"text": "[Image page=6 idx=1 name=Im10.png] Size: 1200x800, Data: 46025 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 1, "image_name": "Im10.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 46025}}, {"text": "[Image page=6 idx=2 name=Im11.png] Size: 1200x800, Data: 48406 bytes", "sentences": [{"text": "[Image page=6 idx=2 name=Im11.png] Size: 1200x800, Data: 48406 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 2, "image_name": "Im11.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 48406}}, {"text": "[Image page=6 idx=3 name=Im12.png] Size: 1200x800, Data: 46446 bytes", "sentences": [{"text": "[Image page=6 idx=3 name=Im12.png] Size: 1200x800, Data: 46446 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 3, "image_name": "Im12.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 46446}}, {"text": "[Image page=6 idx=4 name=Im13.png] Size: 1200x800, Data: 48843 bytes", "sentences": [{"text": "[Image page=6 idx=4 name=Im13.png] Size: 1200x800, Data: 48843 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 4, "image_name": "Im13.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 48843}}, {"text": "[Image page=6 idx=5 name=Im8.png] Size: 1200x800, Data: 46669 bytes", "sentences": [{"text": "[Image page=6 idx=5 name=Im8.png] Size: 1200x800, Data: 46669 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 5, "image_name": "Im8.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 46669}}, {"text": "[Image page=6 idx=6 name=Im9.png] Size: 1200x800, Data: 47268 bytes", "sentences": [{"text": "[Image page=6 idx=6 name=Im9.png] Size: 1200x800, Data: 47268 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 6, "image_name": "Im9.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 47268}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Fig. 8: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\ngeneration length 512, increasing the batch size while keeping\nthe number of shards fixed at 16 is associated with a decrease\nin energy per response, which is consistent with what we\nobserved in energy per tokens in the same setting.\nE. Effects of GPU Power Capping on LLaMA 65B\nPower consumption in AI is an increasingly important\nconcern. In prior work, we have shown [25] that power capping\nGPUs during training of language models such as BERT [26]\nis an effective way of reducing the energy consumed training\nthese models. While the work in [25] focused on model\ntraining, in this paper, we focus on inference. In order to study\nthe effect of power capping on inference using large language\nmodels, we ran a limited set of experiments using LLaMA\n65B. We ran the 65B model on four 80GB A100 GPUs with\nthe power cap set at 250W, 175W and 150W.\nTable III shows the relative change in total inference time,\nenergy and token rate under power cap conditions. Results\nshown here are calculated relative to a power cap of 250W. For\na 30% reduction in power from 250W to 175W, the inference\ntime increases by an average of 6.7% for a corresponding aver-\nage reduction in total energy by 23.21%. However, a reduction\nin power cap to 150W results in a much more significant\n(19.49%) increase in average inference time. These results\nshow that power capping as an energy savings intervention\ncan be effective when applied appropriately. A static power\nFig. 9: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\ncap for all GPU workloads may not show the same effective-\nness depending on the task and additional experimentation is\nrequired to make broader recommendations.\nOutput Time Energy Token Rate\nlength % change % change % change\n175W 150W 175W 150W 175W 150W\n256 6.23 15.33 -21.82 -32.76 -5.87 -13.15\n512 6.51 21.70 -23.95 -34.66 -6.11 -17.83\n1024 7.40 21.65 -23.87 -34.59 -6.89 -17.80\nTABLE III: Effects of GPU power capping on LLaMA 65B\ninference: This table shows the relative performance of the\nLLaMA 65B model on the GSM8k dataset with a batch size of\n64 and output lengths of 256, 512, 1024 using NVIDIA A100\nGPUs. The GPUs were power capped at 250W, 175W and\n150W. Results shown here are relative to model performance\nat 250W to stay consistent with the settings in the rest of the\nexperiments described here.\nF . GPU Resource Utilization under Distributed Inference\nFinally, we briefly examine the average GPU resource\nutilization by the 65B LLaMA model when running model\nsharded inference. For the sake of simplicity, we only consider\na batch size of 64 and a maximum generated output length", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "8: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch\nsize.", "metadata": {}}, {"text": "generation length 512, increasing the batch size while keeping\nthe number of shards fixed at 16 is associated with a decrease\nin energy per response, which is consistent with what we\nobserved in energy per tokens in the same setting.", "metadata": {}}, {"text": "E.", "metadata": {}}, {"text": "Effects of GPU Power Capping on LLaMA 65B\nPower consumption in AI is an increasingly important\nconcern.", "metadata": {}}, {"text": "In prior work, we have shown [25] that power capping\nGPUs during training of language models such as BERT [26]\nis an effective way of reducing the energy consumed training\nthese models.", "metadata": {}}, {"text": "While the work in [25] focused on model\ntraining, in this paper, we focus on inference.", "metadata": {}}, {"text": "In order to study\nthe effect of power capping on inference using large language\nmodels, we ran a limited set of experiments using LLaMA\n65B.", "metadata": {}}, {"text": "We ran the 65B model on four 80GB A100 GPUs with\nthe power cap set at 250W, 175W and 150W.", "metadata": {}}, {"text": "Table III shows the relative change in total inference time,\nenergy and token rate under power cap conditions.", "metadata": {}}, {"text": "Results\nshown here are calculated relative to a power cap of 250W.", "metadata": {}}, {"text": "For\na 30% reduction in power from 250W to 175W, the inference\ntime increases by an average of 6.7% for a corresponding aver-\nage reduction in total energy by 23.21%.", "metadata": {}}, {"text": "However, a reduction\nin power cap to 150W results in a much more significant\n(19.49%) increase in average inference time.", "metadata": {}}, {"text": "These results\nshow that power capping as an energy savings intervention\ncan be effective when applied appropriately.", "metadata": {}}, {"text": "A static power\nFig.", "metadata": {}}, {"text": "9: Energy per response estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale.", "metadata": {}}, {"text": "Color indicates batch\nsize.", "metadata": {}}, {"text": "cap for all GPU workloads may not show the same effective-\nness depending on the task and additional experimentation is\nrequired to make broader recommendations.", "metadata": {}}, {"text": "Output Time Energy Token Rate\nlength % change % change % change\n175W 150W 175W 150W 175W 150W\n256 6.23 15.33 -21.82 -32.76 -5.87 -13.15\n512 6.51 21.70 -23.95 -34.66 -6.11 -17.83\n1024 7.40 21.65 -23.87 -34.59 -6.89 -17.80\nTABLE III: Effects of GPU power capping on LLaMA 65B\ninference: This table shows the relative performance of the\nLLaMA 65B model on the GSM8k dataset with a batch size of\n64 and output lengths of 256, 512, 1024 using NVIDIA A100\nGPUs.", "metadata": {}}, {"text": "The GPUs were power capped at 250W, 175W and\n150W.", "metadata": {}}, {"text": "Results shown here are relative to model performance\nat 250W to stay consistent with the settings in the rest of the\nexperiments described here.", "metadata": {}}, {"text": "F .", "metadata": {}}, {"text": "GPU Resource Utilization under Distributed Inference\nFinally, we briefly examine the average GPU resource\nutilization by the 65B LLaMA model when running model\nsharded inference.", "metadata": {}}, {"text": "For the sake of simplicity, we only consider\na batch size of 64 and a maximum generated output length", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=Im14.png] Size: 1200x800, Data: 42954 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=Im14.png] Size: 1200x800, Data: 42954 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "Im14.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 42954}}, {"text": "[Image page=7 idx=2 name=Im15.png] Size: 1200x800, Data: 43522 bytes", "sentences": [{"text": "[Image page=7 idx=2 name=Im15.png] Size: 1200x800, Data: 43522 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 2, "image_name": "Im15.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 43522}}, {"text": "[Image page=7 idx=3 name=Im16.png] Size: 1200x800, Data: 43346 bytes", "sentences": [{"text": "[Image page=7 idx=3 name=Im16.png] Size: 1200x800, Data: 43346 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 3, "image_name": "Im16.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 43346}}, {"text": "[Image page=7 idx=4 name=Im17.png] Size: 1200x800, Data: 45428 bytes", "sentences": [{"text": "[Image page=7 idx=4 name=Im17.png] Size: 1200x800, Data: 45428 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 4, "image_name": "Im17.png", "image_width": 1200, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 45428}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "of 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference—\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today—our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the", "sentences": [{"text": "of 256.", "metadata": {}}, {"text": "For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs.", "metadata": {}}, {"text": "These results are summarized in\nTables IV and V.", "metadata": {}}, {"text": "In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range.", "metadata": {}}, {"text": "For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048.", "metadata": {}}, {"text": "Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used.", "metadata": {}}, {"text": "Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength.", "metadata": {}}, {"text": "This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center.", "metadata": {}}, {"text": "With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29].", "metadata": {}}, {"text": "The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.", "metadata": {}}, {"text": "Model Shards Output Length Max.", "metadata": {}}, {"text": "Memory Util.", "metadata": {}}, {"text": "Avg.", "metadata": {}}, {"text": "SM Util.", "metadata": {}}, {"text": "4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.", "metadata": {}}, {"text": "Model Shards Output Length Max.", "metadata": {}}, {"text": "Memory Util.", "metadata": {}}, {"text": "Avg.", "metadata": {}}, {"text": "SM Util.", "metadata": {}}, {"text": "8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.", "metadata": {}}, {"text": "We limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.", "metadata": {}}, {"text": "V.", "metadata": {}}, {"text": "D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs.", "metadata": {}}, {"text": "We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA.", "metadata": {}}, {"text": "We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations.", "metadata": {}}, {"text": "By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.", "metadata": {}}, {"text": "Given the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements.", "metadata": {}}, {"text": "For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model.", "metadata": {}}, {"text": "In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data;", "metadata": {}}, {"text": "however, this results in only 20%-\n25% of the GPU memory being utilized at any given time.", "metadata": {}}, {"text": "This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs.", "metadata": {}}, {"text": "The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads.", "metadata": {}}, {"text": "Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources.", "metadata": {}}, {"text": "This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference—\nwith the potential of only minimal degradation to computa-\ntional performance.", "metadata": {}}, {"text": "Finally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient.", "metadata": {}}, {"text": "Approaches such as model quan-\ntization, distillation, sparsification, etc.", "metadata": {}}, {"text": "are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training.", "metadata": {}}, {"text": "However, simple interventions like GPU power\ncapping is available to be deployed today—our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.", "metadata": {}}, {"text": "If applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.", "metadata": {}}, {"text": "As part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models.", "metadata": {}}, {"text": "It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.", "metadata": {}}, {"text": "ACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper.", "metadata": {}}, {"text": "The authors acknowledge the", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "MIT SuperCloud team: William Arcand, William Bergeron,\nChansup Byun, Michael Houle, Anna Klein, Peter Michaleas,\nLauren Milechin, Julie Mullen, Albert Reuther, Antonio Rosa,\nand Charles Yee. The authors also wish to acknowledge the\nfollowing individuals for their contributions and support: Bob\nBond, Allan Vanterpool, Tucker Hamilton, Jeff Gottschalk,\nMike Kanaan, Charles Leiserson, Dave Martinez, Steve Rejto,\nMarc Zissman.\nREFERENCES\n[1] Stability-AI, “Stable Diffusion,” https://github.com/Stability-AI/\nStableDiffusion, 2023.\n[2] D. Foster, Generative deep learning . ” O’Reilly Media, Inc.”, 2022.\n[3] Z. Epstein, A. Hertzmann, the Investigators of Human Creativity\net al. , “Art and the science of generative ai,” Science, vol.\n380, no. 6650, pp. 1110–1111, 2023. [Online]. Available: https:\n//www.science.org/doi/abs/10.1126/science.adh4451\n[4] B. Mesk ´o and E. J. Topol, “The imperative for regulatory oversight\nof large language models (or generative ai) in healthcare,” npj Digital\nMedicine, vol. 6, no. 1, p. 120, 2023.\n[5] H. Zohny, J. McMillan, and M. King, “Ethics of generative ai,” Journal\nof Medical Ethics , vol. 49, no. 2, pp. 79–80, 2023. [Online]. Available:\nhttps://jme.bmj.com/content/49/2/79\n[6] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in NLP,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics . Florence,\nItaly: Association for Computational Linguistics, Jul. 2019, pp. 3645–\n3650.\n[7] M. Shoeybi, M. Patwary, R. Puri et al. , “Megatron-lm: Training multi-\nbillion parameter language models using model parallelism,” 2020.\n[8] D. Narayanan, M. Shoeybi, J. Casper et al. , “Efficient large-scale\nlanguage model training on gpu clusters using megatron-lm,” 2021.\n[9] “Nvidia/megatron-lm: Ongoing research training transformer models at\nscale,” https://github.com/NVIDIA/Megatron-LM, 2023.\n[10] J. Sevilla, L. Heim, A. Ho et al. , “Compute trends across three eras of\nmachine learning,” 2022.\n[11] D. Patel, “The ai brick wall – a practical limit for scaling dense\ntransformer models, and how gpt 4 will break past it,” https://www.\nsemianalysis.com/p/the-ai-brick-wall-a-practical-limit, 2023.\n[12] R. Desislavov, F. Mart ´ınez-Plumed, and J. Hern´andez-Orallo, “Trends in\nai inference energy consumption: Beyond the performance-vs-parameter\nlaws of deep learning,” Sustainable Computing: Informatics and Sys-\ntems, vol. 38, p. 100857, 2023.\n[13] D. Zhao, N. C. Frey, J. McDonald et al. , “A green(er) world for\na.i.” in 2022 IEEE International Parallel and Distributed Processing\nSymposium Workshops (IPDPSW) , 2022, pp. 742–750.\n[14] H. Touvron, T. Lavril, G. Izacard et al. , “Llama: Open and efficient\nfoundation language models,” 2023.\n[15] “Different development paths of llms,” https://www.interconnects.ai/p/\nllm-development-paths.\n[16] A. Vaswani, N. Shazeer, N. Parmar et al. , “Attention is all you need,”\n2017.\n[17] R. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not all you\nneed. a state of the art review of large generative ai models,” 2023.\n[18] A. Reuther, J. Kepner, C. Byun et al. , “Interactive supercomputing on\n40,000 cores for machine learning and data analysis,” in2018 IEEE High\nPerformance extreme Computing Conference (HPEC) . IEEE, 2018, pp.\n1–6.\n[19] Facebook Research, online, 2023. [Online]. Available: https://github.\ncom/facebookresearch/llama\n[20] FairScale authors, “Fairscale: A general purpose modular pytorch li-\nbrary for high performance and large scale training,” https://github.com/\nfacebookresearch/fairscale, 2021.\n[21] R. Taori, I. Gulrajani, T. Zhang et al., “Stanford alpaca: An instruction-\nfollowing llama model,” https://github.com/tatsu-lab/stanford alpaca,\n2023.\n[22] K. Cobbe, V . Kosaraju, M. Bavarian et al. , “Training verifiers to solve\nmath word problems,” arXiv preprint arXiv:2110.14168 , 2021.\n[23] NVIDIA. Nvidia-smi. [Online]. Available: http://developer.download.\nnvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n[24] ——. Nvidia data center GPU manager (dcgm). [Online]. Available:\nhttps://developer.nvidia.com/dcgm\n[25] J. McDonald, B. Li, N. Frey et al. , “Great power, great responsibility:\nRecommendations for reducing energy for training language models,”\nin Findings of the Association for Computational Linguistics: NAACL\n2022, 2022, pp. 1962–1970.\n[26] J. Devlin, M.-W. Chang, K. Lee et al. , “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019, pp.\n4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n[27] NVIDIA, “Multi-Process Service,” https://docs.nvidia.com/deploy/mps/,\n2023.\n[28] NVIDIA, “NVIDIA Multi Instance GPU User Guide,” https://docs.\nnvidia.com/datacenter/tesla/mig-user-guide/, 2023.\n[29] B. Li, T. Patel, S. Samsi et al. , “Miso: exploiting multi-instance gpu\ncapability on multi-tenant gpu clusters,” in Proceedings of the 13th\nSymposium on Cloud Computing , 2022, pp. 173–189.", "sentences": [{"text": "MIT SuperCloud team: William Arcand, William Bergeron,\nChansup Byun, Michael Houle, Anna Klein, Peter Michaleas,\nLauren Milechin, Julie Mullen, Albert Reuther, Antonio Rosa,\nand Charles Yee.", "metadata": {}}, {"text": "The authors also wish to acknowledge the\nfollowing individuals for their contributions and support: Bob\nBond, Allan Vanterpool, Tucker Hamilton, Jeff Gottschalk,\nMike Kanaan, Charles Leiserson, Dave Martinez, Steve Rejto,\nMarc Zissman.", "metadata": {}}, {"text": "REFERENCES\n[1] Stability-AI, “Stable Diffusion,” https://github.com/Stability-AI/\nStableDiffusion, 2023.", "metadata": {}}, {"text": "[2] D.", "metadata": {}}, {"text": "Foster, Generative deep learning .", "metadata": {}}, {"text": "” O’Reilly Media, Inc.”, 2022.", "metadata": {}}, {"text": "[3] Z.", "metadata": {}}, {"text": "Epstein, A.", "metadata": {}}, {"text": "Hertzmann, the Investigators of Human Creativity\net al.", "metadata": {}}, {"text": ", “Art and the science of generative ai,” Science, vol.", "metadata": {}}, {"text": "380, no.", "metadata": {}}, {"text": "6650, pp.", "metadata": {}}, {"text": "1110–1111, 2023.", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available: https:\n//www.science.org/doi/abs/10.1126/science.adh4451\n[4] B.", "metadata": {}}, {"text": "Mesk ´o and E.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Topol, “The imperative for regulatory oversight\nof large language models (or generative ai) in healthcare,” npj Digital\nMedicine, vol.", "metadata": {}}, {"text": "6, no.", "metadata": {}}, {"text": "1, p.", "metadata": {}}, {"text": "120, 2023.", "metadata": {}}, {"text": "[5] H.", "metadata": {}}, {"text": "Zohny, J.", "metadata": {}}, {"text": "McMillan, and M.", "metadata": {}}, {"text": "King, “Ethics of generative ai,” Journal\nof Medical Ethics , vol.", "metadata": {}}, {"text": "49, no.", "metadata": {}}, {"text": "2, pp.", "metadata": {}}, {"text": "79–80, 2023.", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available:\nhttps://jme.bmj.com/content/49/2/79\n[6] E.", "metadata": {}}, {"text": "Strubell, A.", "metadata": {}}, {"text": "Ganesh, and A.", "metadata": {}}, {"text": "McCallum, “Energy and policy consid-\nerations for deep learning in NLP,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics .", "metadata": {}}, {"text": "Florence,\nItaly: Association for Computational Linguistics, Jul.", "metadata": {}}, {"text": "2019, pp.", "metadata": {}}, {"text": "3645–\n3650.", "metadata": {}}, {"text": "[7] M.", "metadata": {}}, {"text": "Shoeybi, M.", "metadata": {}}, {"text": "Patwary, R.", "metadata": {}}, {"text": "Puri et al.", "metadata": {}}, {"text": ", “Megatron-lm: Training multi-\nbillion parameter language models using model parallelism,” 2020.", "metadata": {}}, {"text": "[8] D.", "metadata": {}}, {"text": "Narayanan, M.", "metadata": {}}, {"text": "Shoeybi, J.", "metadata": {}}, {"text": "Casper et al.", "metadata": {}}, {"text": ", “Efficient large-scale\nlanguage model training on gpu clusters using megatron-lm,” 2021.", "metadata": {}}, {"text": "[9] “Nvidia/megatron-lm: Ongoing research training transformer models at\nscale,” https://github.com/NVIDIA/Megatron-LM, 2023.", "metadata": {}}, {"text": "[10] J.", "metadata": {}}, {"text": "Sevilla, L.", "metadata": {}}, {"text": "Heim, A.", "metadata": {}}, {"text": "Ho et al.", "metadata": {}}, {"text": ", “Compute trends across three eras of\nmachine learning,” 2022.", "metadata": {}}, {"text": "[11] D.", "metadata": {}}, {"text": "Patel, “The ai brick wall – a practical limit for scaling dense\ntransformer models, and how gpt 4 will break past it,” https://www.", "metadata": {}}, {"text": "semianalysis.com/p/the-ai-brick-wall-a-practical-limit, 2023.", "metadata": {}}, {"text": "[12] R.", "metadata": {}}, {"text": "Desislavov, F.", "metadata": {}}, {"text": "Mart ´ınez-Plumed, and J.", "metadata": {}}, {"text": "Hern´andez-Orallo, “Trends in\nai inference energy consumption: Beyond the performance-vs-parameter\nlaws of deep learning,” Sustainable Computing: Informatics and Sys-\ntems, vol.", "metadata": {}}, {"text": "38, p.", "metadata": {}}, {"text": "100857, 2023.", "metadata": {}}, {"text": "[13] D.", "metadata": {}}, {"text": "Zhao, N.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Frey, J.", "metadata": {}}, {"text": "McDonald et al.", "metadata": {}}, {"text": ", “A green(er) world for\na.i.” in 2022 IEEE International Parallel and Distributed Processing\nSymposium Workshops (IPDPSW) , 2022, pp.", "metadata": {}}, {"text": "742–750.", "metadata": {}}, {"text": "[14] H.", "metadata": {}}, {"text": "Touvron, T.", "metadata": {}}, {"text": "Lavril, G.", "metadata": {}}, {"text": "Izacard et al.", "metadata": {}}, {"text": ", “Llama: Open and efficient\nfoundation language models,” 2023.", "metadata": {}}, {"text": "[15] “Different development paths of llms,” https://www.interconnects.ai/p/\nllm-development-paths.", "metadata": {}}, {"text": "[16] A.", "metadata": {}}, {"text": "Vaswani, N.", "metadata": {}}, {"text": "Shazeer, N.", "metadata": {}}, {"text": "Parmar et al.", "metadata": {}}, {"text": ", “Attention is all you need,”\n2017.", "metadata": {}}, {"text": "[17] R.", "metadata": {}}, {"text": "Gozalo-Brizuela and E.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Garrido-Merchan, “Chatgpt is not all you\nneed.", "metadata": {}}, {"text": "a state of the art review of large generative ai models,” 2023.", "metadata": {}}, {"text": "[18] A.", "metadata": {}}, {"text": "Reuther, J.", "metadata": {}}, {"text": "Kepner, C.", "metadata": {}}, {"text": "Byun et al.", "metadata": {}}, {"text": ", “Interactive supercomputing on\n40,000 cores for machine learning and data analysis,” in2018 IEEE High\nPerformance extreme Computing Conference (HPEC) .", "metadata": {}}, {"text": "IEEE, 2018, pp.", "metadata": {}}, {"text": "1–6.", "metadata": {}}, {"text": "[19] Facebook Research, online, 2023.", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available: https://github.", "metadata": {}}, {"text": "com/facebookresearch/llama\n[20] FairScale authors, “Fairscale: A general purpose modular pytorch li-\nbrary for high performance and large scale training,” https://github.com/\nfacebookresearch/fairscale, 2021.", "metadata": {}}, {"text": "[21] R.", "metadata": {}}, {"text": "Taori, I.", "metadata": {}}, {"text": "Gulrajani, T.", "metadata": {}}, {"text": "Zhang et al., “Stanford alpaca: An instruction-\nfollowing llama model,” https://github.com/tatsu-lab/stanford alpaca,\n2023.", "metadata": {}}, {"text": "[22] K.", "metadata": {}}, {"text": "Cobbe, V .", "metadata": {}}, {"text": "Kosaraju, M.", "metadata": {}}, {"text": "Bavarian et al.", "metadata": {}}, {"text": ", “Training verifiers to solve\nmath word problems,” arXiv preprint arXiv:2110.14168 , 2021.", "metadata": {}}, {"text": "[23] NVIDIA.", "metadata": {}}, {"text": "Nvidia-smi.", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available: http://developer.download.", "metadata": {}}, {"text": "nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n[24] ——.", "metadata": {}}, {"text": "Nvidia data center GPU manager (dcgm).", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available:\nhttps://developer.nvidia.com/dcgm\n[25] J.", "metadata": {}}, {"text": "McDonald, B.", "metadata": {}}, {"text": "Li, N.", "metadata": {}}, {"text": "Frey et al.", "metadata": {}}, {"text": ", “Great power, great responsibility:\nRecommendations for reducing energy for training language models,”\nin Findings of the Association for Computational Linguistics: NAACL\n2022, 2022, pp.", "metadata": {}}, {"text": "1962–1970.", "metadata": {}}, {"text": "[26] J.", "metadata": {}}, {"text": "Devlin, M.-W.", "metadata": {}}, {"text": "Chang, K.", "metadata": {}}, {"text": "Lee et al.", "metadata": {}}, {"text": ", “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers) .", "metadata": {}}, {"text": "Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun.", "metadata": {}}, {"text": "2019, pp.", "metadata": {}}, {"text": "4171–4186.", "metadata": {}}, {"text": "[Online].", "metadata": {}}, {"text": "Available: https://aclanthology.org/N19-1423\n[27] NVIDIA, “Multi-Process Service,” https://docs.nvidia.com/deploy/mps/,\n2023.", "metadata": {}}, {"text": "[28] NVIDIA, “NVIDIA Multi Instance GPU User Guide,” https://docs.", "metadata": {}}, {"text": "nvidia.com/datacenter/tesla/mig-user-guide/, 2023.", "metadata": {}}, {"text": "[29] B.", "metadata": {}}, {"text": "Li, T.", "metadata": {}}, {"text": "Patel, S.", "metadata": {}}, {"text": "Samsi et al.", "metadata": {}}, {"text": ", “Miso: exploiting multi-instance gpu\ncapability on multi-tenant gpu clusters,” in Proceedings of the 13th\nSymposium on Cloud Computing , 2022, pp.", "metadata": {}}, {"text": "173–189.", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}]}