{"document_id": "wu2021a", "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities", "text": "1\nSustainable AI: Environmental Implications,\nChallenges and Opportunities\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott,\nAnastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee,\nBugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood\nFacebook AI\nAbstract—This paper explores the environmental impact of\nthe super-linear growth trends for AI from a holistic perspective,\nspanning Data, Algorithms, and System Hardware. We character-\nize the carbon footprint of AI computing by examining the model\ndevelopment cycle across industry-scale machine learning use\ncases and, at the same time, considering the life cycle of system\nhardware. Taking a step further, we capture the operational and\nmanufacturing carbon footprint of AI computing and present an\nend-to-end analysis for what and how hardware-software design\nand at-scale optimization can help reduce the overall carbon\nfootprint of AI. Based on the industry experience and lessons\nlearned, we share the key challenges and chart out important\ndevelopment directions across the many dimensions of AI. We\nhope the key messages and insights presented in this paper\ncan inspire the community to advance the ﬁeld of AI in an\nenvironmentally-responsible manner.\nI. I NTRODUCTION\nArtiﬁcial Intelligence (AI) is one of the fastest growing\ndomains spanning research and product development and\nsigniﬁcant investment in AI is taking place across nearly every\nindustry, policy, and academic research. This investment in\nAI has also stimulated novel applications in domains such as\nscience, medicine, ﬁnance, and education. Figure 1 analyzes\nthe number of papers published within the scientiﬁc disciplines,\nillustrating the growth trend in recent years 1.\nAI plays an instrumental role to push the boundaries of\nknowledge and sparks novel, more efﬁcient approaches to\nconventional tasks. AI is applied to predict protein structures\nradically better than previous methods. It has the potential to\nrevolutionize biological sciences by providing in-silico methods\nfor tasks only possible in a physical laboratory setting [ 1]. AI\nis demonstrated to achieve human-level conversation tasks,\nsuch as the Blender Bot [ 2], and play games at superhuman\nlevels, such as AlphaZero [ 3]. AI is used to discover new\nelectrocatalysts for efﬁcient and scalable ways to store and\nutilize renewable energy [ 4], predicting renewable energy\navailability in advance to improve energy utilization [ 5],\noperating hyperscale data centers efﬁciently [6], growing plants\nusing less natural resources [ 7], and, at the same time, being\nused to tackle climate changes [ 8], [9]. It is projected that, in\nthe next ﬁve years, the market for AI will increase by 10 × into\nhundreds of billions of dollars [ 10]. All of these investments\n1Based on monthly counts, Figure 1 estimates the cumulative number of\npapers published per category on the arXiv database.\n0\n50\n100\n150\n200\n250\n20112013201520172019\nCumulative arXivArticle CountsThousands\nAI in Different Disciplines Computer ScienceMathMachine LearningPhysics\nSource: arXiv.org\nFig. 1. The growth of ML is exceeding that of many other scientiﬁc disciplines.\nSigniﬁcant research growth in machine learning is observed in recent years as\nillustrated by the increasing cumulative number of papers published in machine\nlearning with respect to other scientiﬁc disciplines based on the monthly count\n(y-axis measures the cumulative number of articles on arXiv).\nin research, development, and deployment have led to a super-\nlinear growth in AI data, models, and infrastructure capacity.\nWith the dramatic growth of AI, it is imperative to understand\nthe environmental implications, challenges, and opportunities\nof this nascent technology. This is because technologies tend to\ncreate a self-accelerating growth cycle, putting new demands\non the environment.\nThis work explores the environmental impact of AI from\na holistic perspective. More speciﬁcally, we present the\nchallenges and opportunities to designing sustainable AI\ncomputing across the key phases of the machine learning (ML)\ndevelopment process — Data, Experimentation, Training, and\nInference — for a variety of AI use cases at Facebook, such\nas vision, language, speech, recommendation and ranking. The\nsolution space spans across our ﬂeet of datacenters and on-\ndevice computing. Given particular use cases, we consider the\nimpact of AI data, algorithms, and system hardware. Finally,\nwe consider emissions across the life cycle of hardware systems,\nfrom manufacturing to operational use.\nAI Data Growth. In the past decade, we have seen an\nexponential increase in AI training data and model capacity.\nFigure 2(b) illustrates that the amount of training data at\nFacebook for two recommendation use cases — one of the\nfastest growing areas of ML usage at Facebook— has increased\nby 2.4× and 1.9× in the last two years, reaching exabyte scale.\nThe increase in data size has led to a 3.2 × increase in data\ningestion bandwidth demand. Given this increase, data storage\nand the ingestion pipeline accounts for a signiﬁcant portion of\narXiv:2111.00364v2  [cs.LG]  9 Jan 2022\n\n05101520Model Size\nTime\n(c) Model Growth TrendDLRM Parameter #s\n00.0050.010.0150.020.0250.030.035\n051015202530354045\n0.1101000\nAbsolute AUC Improvement\nBLEU Score\nModel Size (Billions of Parameters in Log Scale)\n(a) 1000x Model Size ScalingGPT English2FrenchGPT French2EnglishRecSys SearchRecSys Images\nxxx\nx\nx\n2019-21\nx\n0.511.522.533.544.5Data & Ingestion BandwidthTime\n(b) Data Growth TrendDLRM1 DataDLRM2 DataData Ingestion BW\n1\n1.5\n2\n2.5\n3\nYr1-Q1Yr1-Q2Yr1-Q3Yr1-Q4Yr2-Q1Yr2-Q2\nAI System Capacity\n(d) System Growth TrendTrainingInference2.9x\n2.5x\n20x1,000x\nFig. 2. Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years. (a) The 1000× model size\ngrowth has led to higher model accuracy for various ML tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a\nmodel 1, 000× larger in size. (b) At Facebook, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2\ntimes increase in the data ingestion bandwidth demand. (c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same\ntime period [11]. (d) The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.\nthe infrastructure and power capacity compared to ML training\nand end-to-end machine learning life cycles.\nAI Model Growth. The ever-increasing data volume has also\ndriven a super-linear trend in model size growth. Figure 2(a) de-\npicts the 1000× model size increase for GPT3-based language\ntranslation tasks [ 12], [13], whereas for Baidu’s search engine,\nthe model of 1000× larger in size improves accuracy in AUC\nby 0.030. Despite small, the accuracy improvement can lead\nto signiﬁcantly higher-quality search outcomes [ 14]. Similarly,\nFigure 2(c) illustrates that between 2019 and 2021, the size\nof recommendation models at Facebook has increased by\n20× [15], [16], [17], [11]. Despite the large increase in model\nsizes, the memory capacity of GPU-based AI accelerators,\ne.g. 32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100,\n2021), has increased by < 2× every 2 years. The resource\nrequirements for strong AI scaling clearly outpaces that of\nsystem hardware.\nAI Infrastructure Growth. The strong performance scaling\ndemand for ML motivates a variety of scale-out solutions [11],\n[18] by leveraging parallelism at scale with a massive collection\nof training accelerators. Figure 2(d) illustrates that the explosive\ngrowth in AI use cases at Facebook has driven 2.9× increase\nin AI training infrastructure capacity over the 1.5 years. In\naddition, we observe trillions of inference per day across\nFacebook’s data centers—more than doubling in the past 3\nyears. The increase in inference demands has also led to an\n2.5× increase in AI inference infrastructure capacity. Last but\nnot least, the carbon footprint of AI goes beyond its operational\nenergy consumption. The embodied carbon footprint of systems\nis becoming a dominating factor for AI’s overall environmental\nimpact (Section III) [19].\nThe Elephant in the Room. Despite the positive societal\nbeneﬁts [20], the endless pursuit of achieving higher model\nquality has led to the exponential scaling of AI with signiﬁcant\nenergy and environmental footprint implications. Although\nrecent work shows the carbon footprint of training one large\nML model, such as Meena [21], is equivalent to 242,231 miles\ndriven by an average passenger vehicle [ 22], this is only one\naspect; to fully understand the real environmental impact we\nmust consider the AI ecosystem holistically going forward —\nbeyond looking at model training alone and by accounting\nfor both operational and embodied carbon footprint of AI.\nWe must look at the ML pipeline end-to-end: data collection,\nmodel exploration and experimentation, model training, model\noptimization and run-time inference. The frequency of training\nand scale of each stage of the ML development cycle matter.\nFrom the systems perspective, the life cycle of ML software\nand system hardware, including manufacturing and operational\nuse, must also be considered.\nOptimizing across ML pipelines and systems life cycles end-\nto-end is a complex and challenging task. While training large,\nsparsely-activated neural networks improves model scalability,\nachieving higher accuracy at lower operational energy foot-\nprint [21], it can incur higher embodied carbon footprint from\nthe increase in the system resource requirement. Shifting model\ntraining and inference to data centers with carbon-free energy\ncan reduce emissions; however, this approach may not scale to\na broad set of use cases. Infrastructure for carbon-free energy\nis limited by factors such as geography and available materials\n(e.g. rare metals), and takes signiﬁcant economic resources and\ntime to build. In addition, as on-device learning becomes more\nubiquitously adopted to improve data privacy, we can see more\ncomputation being shifted away from data centers to the edge,\nwhere access to renewable energy is limited.\nA Holistic Approach. This paper is the ﬁrst to take a holistic\napproach to characterize the environmental footprint of AI\ncomputing from experimentation and training to inference.\nWe characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nmachine learning use cases at Facebook (Section II). This is\nillustrated by the more than 800 × operational carbon footprint\nreduction achieved through judicious hardware-software co-\ndesign for a Transformer-based universal language model.\nTaking a step further, we present an end-to-end analysis for\nboth operational and embodied carbon footprint for AI training\nand inference (Section III). Based on the industry experience\nand lessons learned, we chart out opportunities and important\ndevelopment directions across the dimensions of AI including —\ndata, algorithm, systems, metrics, standards, and best practices\n(Section IV). We hope the key messages (Section VI) and the\ninsights in this paper can inspire the community to advance\nthe ﬁeld of AI in an environmentally-responsible manner.\n2\n\nExperimentationTrainingInference\nStorage –Network --Compute\nManufacturingTransport Product UseRecycling\nDeep Learning Framework & Library\n(a) Fleet View\n(b) Machine Learning Task View [Section 3.1]\n(c) Infrastructure View\n-2024681012\n20162017201820192020\nMetric Tons CO2e Millions[Use] Operational (renewable)[Use] Operational Scope 1,2[Manufacturing] Value Chain CO2 Scope 3Carbon Removal\n0%20%40%60%80%100%\nDataTraining (Offline)Training (Online/Evaluation)DeploymentRM1\nMachine Learning Model Development and Deployment Phases [Section 2.1]\nSystem Life Cycle [Section 2.2]\nDataData Efficiency[Section 4.1]Resource-Efficient Experimentation, Algorithms, and Model Architectures [Section 4.2]\nEfficient, Environmentally-Sustainable AI System Hardware [Section 4.3]\nFig. 3. Model Development Phases over AI System Hardware Life Cycle: (a) At Facebook, we observe a rough power capacity breakdown of 10:20:70 for\nAI infrastructures devoted to the three key phases — Experimentation, Training, and Inference; (b) Considering the primary stages of the ML pipeline\nend-to-end, the energy footprint of RM1 is roughly 31:29:40 over Data, Experimentation/Training, and Inference; (c) Despite the investment to neutralize\nthe operational footprint with carbon-free energy, the overall data center electricity use continues to grow, demanding over 7.17 million MWh in 2020 [23].\nII. M ODEL DEVELOPMENT PHASES AND AI S YSTEM\nHARDWARE LIFE CYCLE\nFigure 3 depicts the major development phases for ML —\nData Processing, Experimentation, Training, and Inference\n(Section II-A) — over the life cycle of AI system hardware\n(Section II-B). Driven by distinct objectives of AI research\nand advanced product development, infrastructure is designed\nand built speciﬁcally to maximize data storage and ingestion\nefﬁciency for the phase of Data Processing , developer efﬁ-\nciency for the phase of Experimentation, training throughput\nefﬁciency for the phase of Training, and tail-latency bounded\nthroughput efﬁciency for Inference.\nA. Machine Learning Model Development Cycle\nML researchers extract features from data during the Data\nProcessing phase and apply weights to individual features\nbased on feature importance to the model optimization objective.\nDuring Experimentation, the researchers design, implement\nand evaluate the quality of proposed algorithms, model ar-\nchitectures, modeling techniques, and/or training methods for\ndetermining model parameters. This model exploration process\nis computationally-intensive. A large collection of diverse ML\nideas are explored simultaneously at-scale. Thus, during this\nphase, we observe unique system resource requirements from\nthe large pool of training experiments. Within Facebook’s ML\nresearch cluster, 50% (p50) of ML training experiments take up\nto 1.5 GPU days while 99% (p99) of the experiments complete\nwithin 24 GPU days. There are a number of large-scale, trillion\nparameter models which require over 500 GPUs days.\nOnce a ML solution is determined as promising, it moves into\nTraining where the ML solution is evaluated using extensive\nproduction data — data that is more recent, is larger in quantity,\nand contains richer features . The process often requires\nadditional hyper-parameter tuning. Depending on the ML task\nrequirement, the models can be trained/re-trained at different\nfrequencies. For example, models supporting Facebook’sSearch\nservice were trained at an hourly cadence whereas the Language\nTranslation models were trained weekly [24]. A p50 production\nmodel training workﬂow takes 2.96 GPU days while a training\nworkﬂow at p99 can take up to 125 GPU days.\nFinally, for Inference, the best-performing model is de-\nployed, producing trillions of daily predictions to serve billions\nof users worldwide. The total compute cycles for inference\npredictions are expected to exceed the corresponding training\ncycles for the deployed model.\nB. Machine Learning System Life Cycle\nLife Cycle Analysis (LCA) is a common methodology to\nassess the carbon emissions over the product life cycle. There\nare four major phases: manufacturing, transport, product use,\nand recycling2. From the perspective of AI’s carbon footprint\nanalysis, manufacturing and product use are the focus. Thus,\nin this work, we consider the overall carbon footprint of\nAI by including manufacturing — carbon emissions from\nbuilding infrastructures speciﬁcally for AI (i.e., embodied\ncarbon footprint) and product use — carbon emissions from\nthe use of AI (i.e., operational carbon footprint ).\nWhile quantifying the exact breakdown between operational\nand embodied carbon footprint is a complex process, we\nestimate the signiﬁcance of embodied carbon emissions using\nFacebook’s Greenhouse Gas (GHG) emission statistics 3. In this\ncase, more than 50% of Facebook’s emissions owe to its value\nchain — Scope 3 of Facebook’s GHG emission . As a result,\na signiﬁcant embodied carbon cost is paid upfront for every\nsystem component brought into Facebook’s ﬂeet of datacenters,\nwhere AI is the biggest growth driver.\n2Recycling is an important domain, for which the industry is developing\na circular economy model to up-cycle system components — design with\nrecycling in mind.\n3Facebook Sustainability Data: https://sustainability.fb.com/report/2020-sust\nainability-report/.\n3\n\n[Image page=3 idx=1 name=Im1.png] Size: 335x335, Data: 6583 bytes\n\n[Image page=3 idx=2 name=Im10.png] Size: 247x247, Data: 3016 bytes\n\n[Image page=3 idx=3 name=Im11.png] Size: 239x225, Data: 3069 bytes\n\n[Image page=3 idx=4 name=Im12.png] Size: 240x225, Data: 3068 bytes\n\n[Image page=3 idx=5 name=Im13.png] Size: 239x225, Data: 3076 bytes\n\n[Image page=3 idx=6 name=Im14.png] Size: 240x225, Data: 3057 bytes\n\n[Image page=3 idx=7 name=Im15.jpg] Size: 934x444, Data: 90606 bytes\n\n[Image page=3 idx=8 name=Im16.png] Size: 196x195, Data: 3029 bytes\n\n[Image page=3 idx=9 name=Im17.png] Size: 284x284, Data: 3232 bytes\n\n[Image page=3 idx=10 name=Im18.png] Size: 233x233, Data: 2114 bytes\n\n[Image page=3 idx=11 name=Im19.png] Size: 1618x618, Data: 28366 bytes\n\n[Image page=3 idx=12 name=Im2.png] Size: 313x312, Data: 6139 bytes\n\n[Image page=3 idx=13 name=Im20.jpg] Size: 312x296, Data: 22536 bytes\n\n[Image page=3 idx=14 name=Im3.png] Size: 217x217, Data: 9936 bytes\n\n[Image page=3 idx=15 name=Im4.png] Size: 202x203, Data: 11186 bytes\n\n[Image page=3 idx=16 name=Im5.png] Size: 212x212, Data: 5419 bytes\n\n[Image page=3 idx=17 name=Im6.png] Size: 205x205, Data: 14473 bytes\n\n[Image page=3 idx=18 name=Im7.png] Size: 243x243, Data: 5006 bytes\n\n[Image page=3 idx=19 name=Im8.png] Size: 243x243, Data: 4956 bytes\n\n[Image page=3 idx=20 name=Im9.png] Size: 243x243, Data: 4863 bytes\n\n*Training footprint only\n0.00\n0.50\n1.00\nLMRM-1RM-2RM-3RM-4RM-5BERT-NASEvolved TransformerT5MeenaGShard-600BSwitch TransformerGPT3\nFacebookOSS Large-Scale ML Models\nCO2e (kg)Millions\nOperational Carbon Footprint of Large-Scale ML TasksOffline TrainingOnline TrainingInference\nFig. 4. The carbon footprint of the LM model is dominated by Inference\nwhereas, for RM1 – RM5, the carbon footprint of Training versus Inference is\nroughly equal. The average carbon footprint for ML training tasks at Facebook\nis 1.8 times larger than that of Meena used in modern conversational agents\nand 0.3 times of GPT-3’s carbon footprint. Carbon footprint for inference\ntasks is included for models that are used in production. Note: the operational\ncarbon footprint of AI does not correlate with the number of model parameters.\nThe OSS large-scale ML tasks are based on the vanilla model architectures\nfrom [21] and may not be reﬂective of production use cases.\nIII. AI C OMPUTING ’S CARBON FOOTPRINT\nA. Carbon Footprint Analysis for Industry-Scale ML Training\nand Deployment\nFigure 4 illustrates the operational carbon emissions for\nmodel training and inference across the ML tasks. We analyze\nsix representative machine learning models in production\nat Facebook 4. LM refers to Facebook’s Transformer-based\nUniversal Language Model for text translation [ 25]. RM1 –\nRM5 represent ﬁve unique deep learning recommendation and\nranking models for various Facebook products [26], [27].\nWe compare the carbon footprint of Facebook’s production\nML models with seven large-scale, open-source (OSS) models:\nBERT-NAS, T5, Meena, GShard-600B, Switch Transformer,\nand GPT-3. Note, we present the operational carbon footprint\nof the OSS model training from [ 28], [ 21]. The operational\ncarbon footprint results can vary based on the exact AI\nsystems used and the carbon intensity of the energy mixture.\nModels with more parameters do not necessarily result in\nlonger training time nor higher carbon emissions. Training\nthe Switch Transformer model equipped with 1.5 trillion\nparameters [ 29] produces signiﬁcantly less carbon emission\nthan that of GPT-3 (750 billion parameters) [13]. This illustrates\nthe carbon footprint advantage of operationally-efﬁcient model\narchitectures.\n4In total, the six models account for a vast majority of compute resources\nfor the overall inference predictions at Facebook, serving billions of users\nworld wide.\n00.20.40.60.811.21.4\nLMRM-1RM-2RM-3RM-4RM-5\nCO2e (kg)Millions\nOverall Carbon Footprint of Large-Scale ML TasksOperational Carbon Cost (Offset with solar)Operational Carbon Cost (Rest)Projected Embodied Carbon Cost\ncarbon-free energy\nFig. 5. When considering the overall life cycle of ML models and systems in\nthis analysis, manufacturing carbon cost is roughly 50% of the (location-based)\noperational carbon footprint of large-scale ML tasks (Figure 4). Taking into\naccount carbon-free energy, such as solar, the operational energy consumption\ncan be signiﬁcantly reduced, leaving the manufacturing carbon cost as the\ndominating source of AI’s carbon footprint.\nBoth Training and Inference can contribute signiﬁcantly to the\noverall carbon footprint of machine learning tasks at Facebook.\nThe exact breakdown between the two phases varies across\nML use cases.\nThe overall operational carbon footprint is categorized into\nofﬂine training, online training, and inference. Ofﬂine training\nencompasses both experimentation and training models with\nhistorical data. Online training is particularly relevant to\nrecommendation models where parameters are continuously\nupdated based on recent data. The inference footprint represents\nthe emission from serving production trafﬁc. The online training\nand inference emissions are considered over the period of\nofﬂine training. For recommendation use cases, we ﬁnd the\ncarbon footprint is split evenly between training and inference.\nOn the other hand, the carbon footprint of LM is dominated\nby the inference phase, using much higher inference resources\n(65%) as compared to training (35%).\nBoth operational and embodied carbon emissions can con-\ntribute signiﬁcantly to the overall footprint of ML tasks .\nOperational Carbon Footprint: Across the life cycle of\nthe Facebook models shown in Figure 4, the average carbon\nfootprint is 1.8 × higher than that of the open-source Meena\nmodel [ 30] and one-third of GPT-3’s training footprint. To\nquantify the emissions of Facebook’s models we measure\nthe total energy consumed, assume location-based carbon\nintensities for energy mixes, 5 and use a data center Power\nUsage Effectiveness (PUE) of 1.1. In addition to model-level\nand hardware-level optimizations, Facebook’s renewable energy\nprocurement [23] programs mitigates these emissions.\nEmbodied Carbon Footprint: To quantify the embodied\ncarbon footprint of AI hardware, we use LCA (Section II-B).\nWe assume GPU-based AI training systems have similar\n5Renewable energy and sustainability programs of Facebook [23].\n4\n\n0%\n5%\n10%\n15%\n20%\n25%Operational PowerFootprint Improvement Normalized to Overall AI Infrastructure\n2-year time period\nOptimization is an Iterative Process\nModelPlatformInfrastructureHardware\nPerformance-per-Watt[Moore’s Law]\nPerformance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\nResource-Efficient AI Models[Figure 8]\nFig. 6. Optimization is an iterative process — we have achieved an average of\n20% operational energy footprint reduction every 6 months across the machine\nlearning hardware-software stack.\nembodied footprint as the production footprint of Apple’s 28-\ncore CPU with dual AMD Radeon GPUs (2000kg CO 2) [31].\nFor CPU-only systems, we assume half the embodied emissions.\nBased on the characterization of model training and inference at\nFacebook, we assume an average utilization of 30-60% over the\n3- to 5-year lifetime for servers. Figure 5 presents the overall\ncarbon footprint for the large scale ML tasks at Facebook,\nspanning both operational and embodied carbon footprint.\nBased on the assumptions of location-based renewable energy\navailability, the split between the embodied and (location-\nbased) operational carbon footprint is roughly 30% / 70%\nfor the large scale ML tasks. Taking into account carbon-free\nenergy, such as solar, the operational carbon footprint can be\nsigniﬁcantly reduced, leaving the manufacturing carbon cost\nas the dominating source of AI’s carbon footprint.\nB. Carbon Footprint Optimization from Hardware-Software\nCo-Design\nOptimization is an iterative process — we reduce the power\nfootprint across the machine learning hardware-software stack\nby 20% every 6 months. But at the same time, AI infrastructure\ncontinued to scale out. The net effect, with Jevon’s Paradox, is\na 28.5% operational power footprint reduction over two years\n(Figure 8).\nOptimization across AI Model Development and System\nStack over Time: Figure 6 shows the operational power\nfootprint reduction across Facebook’s AI ﬂeet over two years.\nThe improvement come from four areas of optimizations:\nmodel (e.g., designing resource-efﬁcient models), platform\n(e.g., PyTorch’s support for quantization), infrastructure (e.g.,\ndata center optimization and low-precision hardware), and\nhardware (e.g., domain-speciﬁc acceleration). Each bar illus-\ntrates the operational power reduction across Facebook’s AI\nﬂeet over 6-month period from each of the optimization areas.\nThe optimizations in aggregate provide, on average, a 20%\nreduction in operational power consumption every six months.\n810\n121\n12511\n10\n100\n1,000\nCPU BaselineCPU DataManagementGPU FP32GPU FP16FasterTransformer\nOperational Power Footprint Normalized to Optimized Transformer on GPUs \nPlatform-Level Caching\nGPU Accelerators\nNumerical Optimization\nPlatform+ Hardware +Algorithm810 x\nOptimizedTransformer\nFig. 7. For the cross-lingual ML task (LM), the operational energy footprint\ncan be signiﬁcantly reduced by more than 800× using platform-level caching,\nGPUs, low precision data format , and additional algorithmic optimization .\nThe compounded beneﬁts highlight the need for cross-stack\noptimizations.\nOptimizing the Carbon Footprint of LMs: We dive\ninto a speciﬁc machine learning task at Facebook: language\ntranslation using a Transformer-based architecture ( LM). LM\nis designed based on the state-of-the-art cross-lingual un-\nderstanding through self-supervision. Figure 7 analyzes the\npower footprint improvements over a collection of optimization\nsteps for LM: platform-level caching, GPU acceleration, low\nprecision format on accelerator , and model optimization. In\naggregate the optimizations reduce the infrastructure resources\nrequired to serve LM at scale by over 800 ×. We outline the\noptimization beneﬁts from each area below.\n• Platform-Level Caching. Starting with a CPU server\nbaseline, application-level caching improves power efﬁ-\nciency by 6.7 ×. These improvements are a result of pre-\ncomputing and caching frequently accessed embeddings\nfor language translation tasks. Using DRAM and Flash\nstorage devices as caches, these pre-computed embeddings\ncan be shared across applications and use cases.\n• GPU acceleration. In addition to caching, deploying LM\nacross GPU-based specialized AI hardware unlocks an\nadditional 10.1× energy efﬁciency improvement.\n• Algorithmic optimization. Finally, algorithmic optimiza-\ntions provide an additional 12 × energy efﬁciency re-\nduction. Halving precision (e.g., going from 32-bit to\n16-bit operations) provides a 2.4 × energy efﬁciency\nimprovement on GPUs. Another 5× energy efﬁciency gain\ncan be achieved by using custom operators to schedule\nencoding steps within a single kernel of the Transformer\nmodule, such as [32].\nOptimizing the Carbon Footprint of RMs: The LM\nanalysis is used as an example to highlight the optimiza-\ntion opportunities available with judicious cross-stack, hard-\nware/software optimization. In addition to optimizing the\ncarbon footprint for the language translation task, we describe\nadditional optimization techniques tailored for ranking and\n5\n\nPerformance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\n0.80.911.11.21.3\nYr1-H1Yr1-H2Yr2-H1Yr2-H2\nOperational Power Footprint\nBaselineOptimized (Section 3.2)\n28.5% improvement \nFig. 8. The iterative optimization process has led to 28.5% operational energy\nfootprint reduction over the two-year time period (Section III-B). Despite the\nsigniﬁcant operational power footprint reduction, we continue to see the overall\nelectricity demand for AI to increase over time — an example of Jevon’s\nParadox, where efﬁciency improvement stimulates additional novel AI use\ncases.\nrecommendation use cases.\nA major infrastructure challenge faced by deep learning\nRM training and deployment ( RM1 – RM5) is the fast-rising\nmemory capacity and bandwidth demands (Figure 2). There are\ntwo primary sub-nets in a RM: the dense fully-connected (FC)\nnetwork and the sparse embedding-based network. The FC\nnetwork is constructed with multi-layer perceptions (MLPs),\nthus computationally-intensive. The embedding network is used\nto project hundreds of sparse, high-dimensional features to low-\ndimension vectors. It can easily contribute to over 95% of the\ntotal model size. For a number of important recommendation\nand ranking use cases, the embedding operation dominates the\ninference execution time [27], [33].\nTo tackle the signiﬁcant memory capacity and bandwidth\nrequirement, we deploy model quantization for RMs [ 34].\nQuantization offers two primary efﬁciency beneﬁts: the low-\nprecision data representation reduces the amount of compu-\ntation requirement and, at the same time, lowers the overall\nmemory capacity need. By converting 32-bit ﬂoating-point\nnumerical representation to 16-bit, we can reduce the overall\nRM2 model size by 15%. This has led to 20.7% reduction in\nmemory bandwidth consumption. Furthermore, the memory\ncapacity reduction enabled by quantization unblocks novel\nsystems with lower on-chip memory. For example, for RM1,\nquantization has enabled RM deployment on highly power-\nefﬁcient systems with smaller on-chip memory, leading to an\nend-to-end inference latency improvement of 2.5 times.\nC. Machine Learning Infrastructures at Scale\nML Accelerators: GPUs are the de-facto training acceler-\nators at Facebook, contributing to signiﬁcant power capacity\ninvestment in the context of Facebook’s ﬂeet of datacenters.\nHowever, GPUs can be severely under-utilized during both the\nML Experimentation and Training phases (Figure 10) [ 35]. To\namortize the upfront embodied carbon cost of every accelerator\ndeployed into Facebook’s datacenters, maximizing accelerator\nutilization is a must.\nEfﬁciency of Scale: The higher throughput performance\ndensity achieved with ML accelerators reduces the total number\nof processors deployed into datacenter racks. This leads to\nmore effective amortization of shared infrastructure overheads.\nFurthermore, datacenter capacity is not only limited by physical\nspace but also power capacity — higher operational power\nefﬁciency directly reduces the inherited carbon cost from\nmanufacturing of IT infrastructures and datacenter buildings.\nAt-Scale Efﬁciency Optimization for Facebook Data\nCenters: Servers in Facebook data center ﬂeets are customized\nfor internal workloads only — machine learning tasks [ 24]\nor not [ 36], [ 37]. Compared to public cloud providers, this\nputs Facebook at a unique position for at-scale resource man-\nagement design and optimization. First, Facebookcustomizes\nserver SKUs — compute, memcached, storage tiers and ML\naccelerators — to maximize performance and power efﬁciency.\nAchieving a Power Usage Effectiveness (PUE) of about 1.10,\nFacebook’s data centers are about 40% more efﬁcient than\nsmall-scale, typical data centers.\nFurthermore, the large-scale deployment of servers of\ndifferent types provides an opportunity to build performance\nmeasurement and optimization tools to ensure high utilization of\nthe underlying infrastructure. For data center ﬂeets in different\ngeographical regions where the actual server utilization exhibits\na diurnal pattern, Auto-Scaling frees the over-provisioned\ncapacity during off-peak hours, by up to 25% of the web\ntier’s machines [ 38]. By doing so, it provides opportunistic\nserver capacity for others to use, including ofﬂine ML training.\nFurthermore, static power consumption plays a non-trivial role\nin the context of the overall data center electricity footprint.\nThis motivates more effective processor idle state management.\nCarbon-Free Energy: Finally, over the past years, Face-\nbookhas invested in carbon free energy sources to neutralize its\noperational carbon footprint [ 23]. Reaching net zero emissions\nentails matching every unit of energy consumed by data\ncenters with 100% renewable energy purchased by Facebook.\nRemaining emissions are offset with various sustainability\nprograms, further reducing the operational carbon footprint of\nAI computing at Facebook. As Section IV-C will later show,\nmore can be done .\nD. Going Beyond Efﬁciency Optimization\nDespite the opportunities for optimizing energy efﬁciency\nand reducing environmental footprint at scale, there are many\nreasons why we must care about scaling AI in a more\nenvironmentally-sustainable manner. AI growth is multiplicative\nbeyond current industrial use cases. Although domain-speciﬁc\narchitectures improve the operational energy footprint of AI\nmodel training by more than 90% [ 21], these architectures\nrequire more system resources, leading to larger embodied\ncarbon footprints.\nWhile shifting model training and inference to data centers\nwith carbon-free energy sources can reduce emissions, the\nsolution may not scale to all AI use cases. Infrastructure for\ncarbon free energy is limited by rare metals and materials,\nand takes signiﬁcant economic resources and time to build.\nFurthermore, the carbon footprint of federated learning and\noptimization use cases at the edge is estimated to be similar to\nthat of training a Transformer Big model (Figure 11). As on-\ndevice learning becomes more ubiquitously adopted to improve\n6\n\n02468\n20%30%50%75%\nLM Carbon Footprint(Normalized to 75%)\nGPU Utilization\nEmbodiedOperational (rest)Operational (offset w/ solar)\nFig. 9. As accelerator utilization improves over time, both operational and\nembodied carbon footprints of AI improve. Carbon-free energy helps reduce\nthe operational carbon footprint, making embodied carbon cost the dominating\nfactor. To reduce the rising carbon footprint of AI computing at-scale, we must\ncomplement efﬁciency and utilization optimization with novel approaches to\nreduce the remaining embodied carbon footprint of AI systems.\ndata privacy, we expect to see more computation being shifted\naway from data centers to the edge, where access to renewable\nenergy may be limited. The edge-cloud space for AI poses\ninteresting design opportunities (Section IV-C).\nThe growth of AI in all dimensions outpaces the efﬁciency im-\nprovement at-scale. Figure 9 illustrates that, as GPU utilization\nis improved (x-axis) for LM training on GPUs, both embodied\nand operational carbon emissions will reduce. Increasing GPU\nutilization up to 80%, the overall carbon footprint decreases\nby 3×. Powering AI services with renewable energy sources\ncan further reduce the overall carbon footprint by a factor of 2.\nEmbodied carbon cost becomes the dominating source of AI’s\noverall carbon footprint. To curb the rising carbon footprint\nof AI computing at-scale (Figure 8 and Figure 9), we must\nlook beyond efﬁciency optimization and complement efﬁciency\nand utilization optimization with efforts to tackle the remaining\nembodied carbon footprint of AI systems.\nIV. A S USTAINABILITY MINDSET FOR AI\nTo tackle the environmental implications of AI’s exponential\ngrowth (Figure 2), the ﬁrst key step requires ML practitioners\nand researchers to develop and adopt an sustainability mindset.\nThe solution space is wide open—while there are signiﬁcant\nefforts looking at AI system and infrastructure efﬁciency opti-\nmization, the AI data, experimentation, and training algorithm\nefﬁciency space (Sections IV-A and IV-B) beyond system\ndesign and optimization (Section IV-C) is less well explored.\nWe cannot optimize what cannot be measured — telemetry to\ntrack the carbon footprint of AI technologies must be adopted\nby the community (Section V-A). We synthesize a number of\nimportant directions to scale AI in a sustainable manner and to\nminimize the environmental impact of AI for the next decades.\nThe ﬁeld of AI is currently primarily driven by research that\nseeks to maximize model accuracy — progress is often used\nsynonymously with improved prediction quality. This endless\npursuit of higher accuracy over the decade of AI research has\nsigniﬁcant implications in computational resource requirement\nand environmental footprint. To develop AI technologies\nresponsibly, we must achieve competitive model accuracy at a\nﬁxed or even reduced computational and environmental cost .\nDespite the recent calls-to-action [28], [39], [40], [41], [21], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI. We\nconjecture the factors that may have contributed to the current\nstate in Appendix A. To bend the exponential growth curve\nof AI and its environmental footprint, we must build a future\nwhere efﬁciency is an evaluation criterion for publishing ML\nresearch on computationally-intensive models beyond accuracy-\nrelated measures.\nA. Data Utilization Efﬁciency\nData Scaling and Sampling: No data is like more data\n— data scaling is the de-facto approach to increase model\nquality, where the primary factor for accuracy improvement\nis driven by the size and quality of training data, instead of\nalgorithmic optimization. However, data scaling has signiﬁcant\nenvironmental footprint implications. To keep the model\ntraining time manageable, overall system resources must be\nscaled with the increase in the data set size, resulting in larger\nembodied carbon footprint and operational carbon footprint\nfrom the data storage and ingestion pipeline and model training.\nAlternatively, if training system resources are kept ﬁxed, data\nscaling increases training time, resulting in a larger operational\nenergy footprint.\nWhen designed well, however, data scaling, sampling and\nselection strategies can improve the competitive analysis for ML\nalgorithms, reducing the environmental footprint of the process\n(Appendix A). For instance, Sachdeva et al. demonstrated that\nintelligent data sampling with merely 10% of data sub-samples\ncan effectively preserve the relative ranking performance\nof different recommendation algorithms [ 42]. This ranking\nperformance is achieved with an average of 5.8 times execution\ntime speedup, leading to signiﬁcant operating carbon footprint\nreduction.\nData Perishability: Understanding key characteristics of\ndata is fundamental to efﬁcient data utilization for AI applica-\ntions. Not all data is created equal and data collected over time\nloses its predictive value gradually. Understanding the rate at\nwhich data loses its predictive value has strong implications on\nthe resulting carbon footprint. For example, natural language\ndata sets can lose half of their predictive value in the time\nperiod of less than 7 years (the half-life time of data) [ 43]. The\nexact half-life period is a function of context. If we were able\nto predict the half-life time of data, we can devise effective\nsampling strategies to subset data at different rates based on\nits half-life. By doing so, the resource requirement for the data\nstorage and ingestion pipeline can be signiﬁcantly reduced [ 44]\n— lower training time (operational carbon footprint) as well as\nstorage needs (embodied carbon footprint).\nB. Experimentation and Training Efﬁciency\nThe experimentation and training phases are closely coupled\n(Section II). There is a natural trade-off between the investment\nin experimentation and the subsequent training cost (Section III).\nNeural architecture search (NAS) and hyperparameter op-\ntimization (HPO) are techniques that automate the design\nspace exploration. Despite their capability to discover higher-\nperforming neural networks, NAS and HPO can be extremely\n7\n\nresource-intensive, involving training many models, especially\nwhen using simple approaches. Strubell et al. show that grid-\nsearch NAS can incur over 3000× environmental footprint\noverhead [28]. Utilizing much more sample-efﬁcient NAS and\nHPO methods [ 45], [ 46] can translate directly into carbon\nfootprint improvement. In addition to reducing the number of\ntraining experiments, one can also reduce the training time of\neach experiment. By detecting and stopping under-performing\ntraining workﬂows early , unnecessary training cycles can be\neliminated.\nMulti-objective optimization explores the Pareto frontier of\nefﬁcient model quality and system resource trade-offs. If used\nearly in the model exploration process, it enables more informed\ndecisions about which model to train fully and deploy given\ncertain infrastructure capacity. Beyond model accuracy and\ntiming performance [ 47], [48], [49], [50], energy and carbon\nfootprint can be directly incorporated into the cost function as\noptimization objectives to enable discovery of environmentally-\nfriendly models. Furthermore, when training is decoupled from\nNAS, sub-networks tailoring to specialized system hardware\ncan be selected without additional training [51], [52], [53], [54].\nSuch approaches can signiﬁcantly reduce the overall training\ntime, however, at the expense of increased embodied carbon\nfootprint.\nDeveloping resource-efﬁcient model architectures funda-\nmentally reduce the overall system capacity need of ML\ntasks. From the systems perspective, accelerator memory\nis scarce. However, DNNs, such as neural recommendation\nmodels, require signiﬁcantly higher memory capacity and\nbandwidth [ 55], [ 33]. This motivates researchers to develop\nmemory-efﬁcient model architectures. For example, the Tensor-\nTrain compression technique (TT-Rec) achieves more than\n100× memory capacity reduction with negligible training time\nand accuracy trade-off [ 56]. Similarly, the design space trade-\noff between memory capacity requirement, training time, and\nmodel accuracy is also explored in Deep Hash Embedding\n(DHE) [ 57]. While training time increases lead to higher\noperational carbon footprint, in the case of TT-Rec and DHE,\nthe memory-efﬁcient model architectures require signiﬁcantly\nlower memory capacity while better utilizing the computational\ncapability of training accelerators, resulting in lower embodied\ncarbon footprint.\nDeveloping efﬁcient training algorithms is a long-time\nobjective of research in optimization and numerical meth-\nods [58]. Evaluations of optimization methods should account\nfor all experimentation efforts required to tune optimizer\nhyperparameters, not just the method performance after tun-\ning [ 59], [ 60]. In addition, signiﬁcant research has gone\ninto algorithmic approaches to efﬁciently scale training [ 61],\n[62] by reducing communication cost via compression [ 63],\n[64], pipelining [ 65], and sharding [ 66], [67]. The advances\nhave enabled efﬁcient scaling to larger models and larger\ndatasets. We expect efﬁcient training methods to continue\nas an important domain. While this paper has focused on\nsupervised learning relying labeled data, algorithmic efﬁciency\nextends to other learning paradigms including self-supervised\nand semi-supervised learning (Appendix C).\nProbability           \nGPU Utilization           \nFig. 10. A vast majority of model experimentation (over tens of thousands of\ntraining workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization\nand efﬁciency improvements.\nC. Efﬁcient, Environmentally-Sustainable AI Infrastructure and\nSystem Hardware\nTo amortize the embodied carbon footprint, model developers\nand system architects must maximize the utilization of acceler-\nator and system resources when in use and prolong the lifetime\nof AI infrastructures . Existing practices such as the move to\ndomain-speciﬁc architectures at cloud scale [ 68], [ 69], [ 70]\nreduce AI computing’s footprint by consolidating computing\nresources at scale and by operating the shared infrastructures\nmore environmentally-friendly with carbon free energy 6.\nAccelerator Virtualization and Multi-Tenancy Support:\nFigure 10 illustrates the utilization of GPU accelerators in Face-\nbook’s research training infrastructure. A signiﬁcant portion\nof machine learning model experimentation utilizes GPUs at\nonly 30-50%, leaving signiﬁcant room for improvements to\nefﬁciency and overall utilization. Virtualization and workload\nconsolidation technologies can help maximize accelerator\nutilization [ 71]. Google’s TPUs have also recently started\nsupporting virtualization [72]. Multi-tenancy for AI accelerators\nis gaining traction as an effective way to improve resource\nutilization, thereby amortizing the upfront embodied carbon\nfootprint of customized system hardware for AI at the expense\nof potential operational carbon footprint increase [ 73], [ 74],\n[75], [76], [77].\nEnvironmental Sustainability as a Key AI System Design\nPrinciple: Today, servers are designed to optimize performance\nand power efﬁciency. However, system design with a focus\non operational energy efﬁciency optimization does not always\nproduce the most environmentally-sustainable solution [ 78],\n[79], [19]. With the rising embodied carbon cost and the expo-\nnential demand growth of AI, system designers and architects\nmust re-think fundamental system hardware design principles\nto minimize computing’s footprint end-to-end, considering the\nentire hardware and ML model development life cycle. In\naddition to the respective performance, power, and cost proﬁles,\nthe environmental footprint characteristics of processors over\nthe generations of CMOS technologies, DDRx and HBM mem-\nory technologies, SSD/NAND-ﬂash/HDD storage technologies\ncan be orders-of-magnitude different [ 80]. Thus, designing AI\n6We discuss additional important directions for building environmentally-\nsustainable systems in Appendix B, including datacenter infrastructure\ndisaggregation; fault tolerant, resilient AI systems.\n8\n\n[Image page=8 idx=1 name=Im1.jpg] Size: 1794x1046, Data: 195086 bytes\n\nsystems with the least environmental impact requires explicit\nconsideration of environmental footprint characteristics at the\ndesign time.\nThe Implications of General-Purpose Processors,\nGeneral-Purpose Accelerators, Reconﬁgurable Systems,\nand ASICs for AI: There is a wide variety of system\nhardware choices for AI from general-purpose processors\n(CPUs), general-purpose accelerators (GPUs or TPUs), ﬁeld-\nprogrammable gate arrays (FPGAs) [81], to application-speciﬁc\nintegrated circuit (ASIC), such as Eyeriss [ 82]. The exact\nsystem deployment choice can be multifaceted — the cadence\nof ML algorithm and model architecture evolution, the di-\nversity of ML use cases and the respective system resource\nrequirements, and the maturity of the software stack. While ML\naccelerator deployment brings a step-function improvement in\noperational energy efﬁciency , it may not necessarily reduce\nthe carbon footprint of AI computing overall. This is because\nof the upfront embodied carbon footprint associated with the\ndifferent system hardware choices. From the environmental\nsustainability perspective, the optimal point depends on the\ncompounding factor of operational efﬁciency improvement over\ngenerations of ML algorithms/models, deployment lifetime\nand embodied carbon footprint of the system hardware. Thus,\nto design for environmental sustainability, one must strike a\ncareful balance between efﬁciency and ﬂexibility and, at the\nsame time, consider environmental impact as a key design\ndimension for next-generation AI systems.\nCarbon-Efﬁcient Scheduling for AI Computing At-Scale:\nAs the electricity consumption of hyperscale data centers\ncontinues to rise, data center operators have devoted signiﬁcant\ninvestment to neutralize operational carbon footprint. By\noperating large-scale computing infrastructures with carbon\nfree energy, technology companies are taking an important step\nto address the environmental implications of computing. More\ncan be done however .\nAs the renewable energy proportion in the electricity grid\nincreases, ﬂuctuations in energy generation will increase due to\nthe intermittent nature of renewable energy sources (i.e. wind,\nsolar). Elastic carbon-aware workload scheduling techniques\ncan be used in and across datacenters to predict and exploit\nthe intermittent energy generation patterns [ 83]. However such\nscheduling algorithms might require server over-provisioning\nto allow for ﬂexibility of shifting workloads to times when\ncarbon-free energy is available. Furthermore, any additional\nserver capacity comes with manufacturing carbon cost which\nneeds to be incorporated into the design space. Alternatively,\nenergy storage (e.g. batteries, pumped hydro, ﬂywheels, molten\nsalt) can be used to store renewable energy during peak\ngeneration times for use during low generation times. There\nis an interesting design space to achieve 24/7 carbon-free AI\ncomputing.\nOn-Device Learning On-device AI is becoming more\nubiquitously adopted to enable model personalization [ 84],\n[85], [86] while improving data privacy [ 87], [88], [89], [90],\nyet its impact in terms of carbon emission is often overlooked.\nOn-device learning emits non-negligible carbon. Figure 11\nillustrates that the operational carbon footprint for training a\nsmall ML task using federated learning (FL) is comparable to\n00.511.522.533.5\nFL-1\nFL-2\nTPU-Base\nTPU-Green\nP100-Base\nP100-GreenFacebookTransformer-big (non-FL)\nCO2e (kg)Hundreds\nDownloadUploadCompute\"\"\nFig. 11. Federated learning and optimization can result in a non-negligible\namount of carbon emissions, equivalent to the carbon footprint of training\nT ransf ormerBig [21]. FL-1 and FL-2 represent two production FL\napplications. P100-Base represents the carbon footprint of T ransf ormerBig\ntraining on P100 GPU whereas TPU-base is T ransf ormerBig training on\nTPU. P100-Green and TPU-Green consider renewable energy at the cloud\n(Methodology detail in Appendix B).\nthat of training an orders-of-magnitude larger Transformer-\nbased model in a centralized setting. As FL trains local\nmodels on client devices and periodically aggregates the model\nparameters for a global model, without collecting raw user\ndata [87], the FL process can emit non-negligible carbon at the\nedge due to both computation and wireless communication.\nIt is important to reduce AI’s environmental footprint at the\nedge. With the ever-increasing demand for on-device use cases\nover billions of client devices, such as teaching AI to understand\nthe physical environment from the ﬁrst-person perception [ 91]\nor personalizing AI tasks, the carbon footprint for on-device\nAI can add up to a dire amount quickly. Also, renewable\nenergy is far more limited for client devices compared to\ndatacenters. Optimizing the overall energy efﬁciency of FL\nand on-device AI is an important ﬁrst step [ 92], [ 93], [ 94],\n[95], [96]. Reducing embodied carbon cost for edge devices is\nalso important, as manufacturing carbon cost accounts for 74%\nof the total footprint [ 19] of client devices. It is particularly\nchallenging to amortize the embodied carbon footprint because\nclient devices are often under-utilized [97].\nV. C ALL -TO-ACTION\nA. Development of Easy-to-Adopt Telemetry for Assessing AI’s\nEnvironmental Footprint\nWhile the open source community has started building tools\nto enable automatic measurement of AI training’s environmental\nfootprint [39], [40], [98], [99] and the ML research community\nrequiring a broader impact statement for the submitted research\nmanuscript, more can be done in order to incorporate efﬁciency\nand sustainability into the design process. Enabling carbon\naccounting methodologies and telemetry that is easy to adopt\nis an important step to quantify the signiﬁcance of our\nprogress in developing AI technologies in an environmentally-\nresponsible manner. While assessing the novelty and quality\nof ML solutions, it is crucial to consider sustainability metrics\nincluding energy consumption and carbon footprint along with\nmeasures of model quality and system performance.\n9\n\nMetrics for AI Model and System Life Cycles: Standard\ncarbon footprint accounting methods for AI’s overall carbon\nfootprint are at a nascent stage. We need simple, easy-to-\nadopt metrics to make fair and useful comparisons between\nAI innovations. Many different aspects must be accounted\nfor, including the life cycles of both AI models ( Data,\nExperimentation, Training, Deployment) and system hardware\n(Manufacturing and Use) (Section II).\nIn addition to incorporating an efﬁciency measure as part\nof leader boards for various ML tasks, data [ 100], models 7,\ntraining algorithms [ 101], environmental impact must also be\nconsidered and adopted by AI system hardware developers. For\nexample, MLPerf [ 102], [103], [104] is the industry standard\nfor ML system performance comparison. The industry has\nwitnessed signiﬁcantly higher system performance speedup,\noutstripping what is enabled by Moore’s Law [ 105], [ 106].\nMoreover, an algorithm efﬁciency benchmark is under develop-\nment8. The MLPerf benchmark standards can advance the ﬁeld\nof AI in an environmentally-competitive manner by enabling\nthe measurement of energy and/or carbon footprint.\nCarbon Impact Statements and Model Cards: We believe\nit is important for all published research papers to disclose\nthe operational and embodied carbon footprint of proposed\ndesign; we are only at the beginning of this journey 9. Note,\nwhile embodied carbon footprints for AI hardware may not be\nreadily available, describing hardware platforms, the number of\nmachines, total runtime used to produce results presented in a\nresearch manuscript is an important ﬁrst step. In addition, new\nmodels must be associated with a model card that, among other\naspects of data sets and models [ 107], describes the model’s\noverall carbon footprint to train and conduct inference.\nVI. K EY TAKEAWAYS\nThe Growth of AI: Deep learning has witnessed an\nexponential growth in training data, model parameters, and\nsystem resources over the recent years (Figure 2). The amount\nof data for AI has grown by 2.4×, leading to 3.2× increase in\nthe data ingestion bandwidth demand at Facebook. Facebook’s\nrecommendation model sizes have increased by 20× between\n2019 and 2021. The explosive growth in AI use cases has\ndriven 2.9× and 2.5× capacity increases for AI training and\ninference at Facebook over the recent 18 months, respectively.\nThe environmental footprint of AI is staggering (Figure 4,\nFigure 5).\nA Holistic Approach: To ensure an environmentally-\nsustainable growth of AI, we must consider the AI ecosystem\nholistically going forward. We must look at the machine learn-\ning pipelines end-to-end — data collection, model exploration\nand experimentation, model training, optimization and run-\ntime inference (Section II). The frequency of training and\nscale of each stage of the ML pipeline must be considered\nto understand salient bottlenecks to sustainable AI. From the\nsystem’s perspective, the life cycle of model development and\n7Papers with code: https://paperswithcode.com/sota/image-classiﬁcation-on\n-imagenet\n8https://github.com/mlcommons/algorithmic-efﬁciency/\n9https://2021.naacl.org/ethics/faq/#-if-my-paper-reports-on-experiments-t\nhat-involve-lots-of-compute-timepower\nsystem hardware, including manufacturing and operational use,\nmust also be accounted for.\nEfﬁciency Optimization: Optimization across the axes of al-\ngorithms, platforms, infrastructures, hardware can signiﬁcantly\nreduce the operational carbon footprint for the Transformer-\nbased universal translation model by 810×. Along with other\nefﬁciency optimization at-scale, this has translated into 25.8%\noperational energy footprint reduction over the two-year period.\nMore must be done to bend the environmental impact from the\nexponential growth of AI (Figure 8 and Figure 9).\nAn Sustainability Mindset for AI: Optimization beyond\nefﬁciency across the software and hardware stack at scale is\ncrucial to enabling future sustainable AI systems. To develop\nAI technologies responsibly, we must achieve competitive\nmodel accuracy at a ﬁxed or even reduced computational\nand environmental cost. We chart out potentially high-impact\nresearch and development directions across the data, algorithms\nand model, experimentation and system hardware, and telemetry\ndimensions for AI at datacenters and at the edge (Section IV).\nWe must take a deliberate approach when developing\nAI research and technologies, considering the environmental\nimpact of innovations and taking a responsible approach to\ntechnology development [108]. That is, we need AI to be green\nand environmentally-sustainable.\nVII. C ONCLUSION\nThis paper is the ﬁrst effort to explore the environmental\nimpact of the super-linear trends for AI growth from a holistic\nperspective, spanning data, algorithms, and system hardware.\nWe characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nML use cases at Facebook and, at the same time, considering\nthe life cycle of system hardware. Furthermore, we capture\nthe operational and manufacturing carbon footprint of AI\ncomputing and present an end-to-end analysis for what and\nhow hardware-software design and at-scale optimization can\nhelp reduce the overall carbon footprint of AI. We share\nthe key challenges and chart out important directions across\nall dimensions of AI—data, algorithms, systems, metrics,\nstandards, and best experimentation practices. Advancing the\nﬁeld of machine intelligence must not in turn make climate\nchange worse. We must develop AI technologies with a deeper\nunderstanding of the societal and environmental implications.\nACKNOWLEDGEMENT\nWe would like to thank Nikhil Gupta, Lei Tian, Weiyi\nZheng, Manisha Jain, Adnan Aziz, and Adam Lerer for\ntheir feedback on many iterations of this draft, and in-depth\ntechnical discussions around building efﬁcient infrastructure\nand platforms; Adina Williams, Emily Dinan, Mona Diab,\nAshkan Yousefpour for the valuable discussions and insights\non AI and environmental responsibility; Mark Zhou, Niket\nAgarwal, Jongsoo Park, Michael Anderson, Xiaodong Wang;\nYatharth Saraf, Hagay Lupesco, Jigar Desai, Joelle Pineau,\nRam Valliyappan, Rajesh Mosur, Ananth Sankarnarayanan and\nEytan Bakshy for their leadership and vision without which\nthis work would not have been possible.\n10\n\nREFERENCES\n[1] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,\nK. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, A. Bridgland,\nC. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-\nParedes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman,\nE. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer,\nS. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu,\nP. Kohli, and D. Hassabis, “Highly accurate protein structure prediction\nwith alphafold,” Nature, 2021.\n[2] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue\ngeneration,” arXiv:2107.07566, 2021.\n[3] D. Silver, T. Hubert, J. Schrittwieser, and D. Hassabis, “AlphaZero:\nShedding new light on chess, shogi, and Go,” 2018.\n[4] C. L. Zitnick, L. Chanussot, A. Das, S. Goyal, J. Heras-Domingo,\nC. Ho, W. Hu, T. Lavril, A. Palizhati, M. Riviere, M. Shuaibi, A. Sriram,\nK. Tran, B. Wood, J. Yoon, D. Parikh, and Z. Ulissi, “An introduction\nto electrocatalyst design using machine learning for renewable energy\nstorage,” arXiv preprint arXiv:2010.09435 , 2020.\n[5] C. Elkin and S. Witherspoon, “Machine learning can boost the value\nof wind energy,” 2019.\n[6] R. Evans and J. Gao, “DeepMind AI Reduces Google Data Centre\nCooling Bill by 40%,” 2016.\n[7] K. Sheikh, “A Growing Presence on the Farm: Robots,” February 2020.\n[8] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. Lacoste,\nK. Sankaran, A. S. Ross, N. Milojevic-Dupont, N. Jaques, A. Waldman-\nBrown, A. Luccioni, T. Maharaj, E. D. Sherwin, S. K. Mukkavilli, K. P.\nKording, C. Gomes, A. Y . Ng, D. Hassabis, J. C. Platt, F. Creutzig,\nJ. Chayes, and Y . Bengio, “Tackling climate change with machine\nlearning,” arXiv:1906.05433, 2019.\n[9] R. Nishant, M. Kennedy, and J. Corbett, “Artiﬁcial intelligence\nfor sustainability: Challenges, opportunities, and a research agenda,”\nInternational Journal of Information Management , vol. 53, 2020.\n[10] Facts and Factors, “Global artiﬁcial intelligence market,” 2021.\n[11] D. Mudigere, Y . Hao, J. Huang, A. Tulloch, S. Sridharan, X. Liu,\nM. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko,\nA. Basant, Y . Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli,\nC. Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y . Ma, J. Yang, E. Wen,\nH. Li, L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. R.\nKishore, T. Graf, A. Eisenman, K. K. Matam, A. Gangidi, G. J.\nChen, M. Krishnan, A. Nayak, K. Nair, B. Muthiah, M. khorashadi,\nP. Bhattacharya, P. Lapukhov, M. Naumov, L. Qiao, M. Smelyanskiy,\nB. Jia, and V . Rao, “Software-hardware co-design for fast and scalable\ntraining of deep learning recommendation models,” arXiv preprint\narXiv:2104.05158, 2021.\n[12] D. Hernandez and T. B. Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.\n[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.\n[14] P. Nayak, “Understanding searches better than ever before,” 2019.\n[15] X. Yi, Y .-F. Chen, S. Ramesh, V . Rajashekhar, L. Hong, N. Fiedel,\nN. Seshadri, L. Heldt, X. Wu, and E. H. Chi, “Factorized deep retrieval\nand distributed tensorﬂow serving,” in Proceedings of Machine Learning\nand Systems, 2018.\n[16] W. Zhao, D. Xie, R. Jia, Y . Qian, R. Ding, M. Sun, and P. Li, “Distributed\nhierarchical gpu parameter server for massive scale deep learning ads\nsystems,” arXiv preprint arXiv:2003.05622 , 2020.\n[17] M. Lui, Y . Yetim, O. Ozkan, Z. Zhao, S.-Y . Tsai, C.-J. Wu, and M. Hemp-\nstead, “Understanding capacity-driven scale-out neural recommendation\ninference,” in Proceedings of the IEEE International Symposium on\nPerformance Analysis of Systems and Software , 2021.\n[18] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y . He, “Zero-inﬁnity:\nBreaking the gpu memory wall for extreme scale deep learning,” arXiv\npreprint arXiv:2104.07857, 2021.\n[19] U. Gupta, Y . Kim, S. Lee, J. Tse, H. S. Lee, G. Wei, D. Brooks,\nand C. Wu, “Chasing carbon: The elusive environmental footprint of\ncomputing,” in Proceedings of the IEEE International Symposium on\nHigh-Performance Computer Architecture, 2021.\n[20] N. Tomasev, J. Cornebise, F. Hutter, S. Mohamed, A. Picciariello,\nB. Connelly, D. Belgrave, D. Ezer, F. C. van der Haert, F. Mugisha,\nG. Abila, H. Arai, H. Almiraat, J. Proskurnia, K. Snyder, M. Otake-\nMatsuura, M. Othman, T. Glasmachers, W. D. Wever, Y . Teh, M. E.\nKhan, R. D. Winne, T. Schaul, and C. Clopath, “Ai for social good:\nunlocking the opportunity for positive impact,” Nature Communications,\nvol. 11, 2020.\n[21] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild,\nD. So, M. Texier, and J. Dean, “Carbon emissions and large neural\nnetwork training,” arXiv preprint arXiv:2104.10350 , 2021.\n[22] EPA, “United states environmental protection agency greenhouse gas\nequivalencies calculator,” 2021.\n[23] Facebook, “2020 sustainability report,” 2021.\n[24] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov,\nM. Fawzy, B. Jia, Y . Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis,\nM. Smelyanskiy, L. Xiong, and X. Wang, “Applied machine learning\nat facebook: A datacenter infrastructure perspective,” in Proceedings\nof the IEEE International Symposium on High Performance Computer\nArchitecture, 2018.\n[25] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,” arXiv\npreprint arXiv:1911.02116, 2020.\n[26] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman,\nJ. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov,\nA. Mallevich, I. Cherniavskii, Y . Lu, R. Krishnamoorthi, A. Yu,\nV . Kondratenko, S. Pereira, X. Chen, W. Chen, V . Rao, B. Jia,\nL. Xiong, and M. Smelyanskiy, “Deep learning recommendation\nmodel for personalization and recommendation systems,” arXiv preprint\narXiv:1906.00091, 2019.\n[27] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks,\nB. Cottel, K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee,\nA. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang,\n“The architectural implications of facebook’s dnn-based personalized\nrecommendation,” in Proceedings of the IEEE International Symposium\non High Performance Computer Architecture , 2020.\n[28] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” arXiv preprint arXiv:1906.02243 ,\n2019.\n[29] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to\ntrillion parameter models with simple and efﬁcient sparsity,” CoRR,\nvol. abs/2101.03961, 2021.\n[30] D. Adiwardana and T. Luong, “Towards a conversational agent that can\nchat about... anything,” 2020.\n[31] Apple, “Product environmental report Mac Pro,” 2019.\n[32] NVIDIA, “Faster Transformer,” 2021.\n[33] L. Ke, U. Gupta, B. Y . Cho, D. Brooks, V . Chandra, U. Diril,\nA. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher,\nD. Mudigere, M. Naumov, M. Schatz, M. Smelyanskiy, X. Wang,\nB. Reagen, C.-J. Wu, M. Hempstead, and X. Zhang, “Recnmp: Accel-\nerating personalized recommendation with near-memory processing,”\nin Proceedings of the ACM/IEEE Annual International Symposium on\nComputer Architecture, 2020.\n[34] Z. Deng, J. Park, P. T. P. Tang, H. Liu, J. Yang, H. Yuen, J. Huang,\nD. Khudia, X. Wei, E. Wen, D. Choudhary, R. Krishnamoorthi, C.-J. Wu,\nS. Nadathur, C. Kim, M. Naumov, S. Naghshineh, and M. Smelyanskiy,\n“Low-precision hardware architectures meet recommendation model\ninference at scale,” IEEE Micro, vol. 41, no. 5, pp. 93–100, 2021.\n[35] L. Wesolowski, B. Acun, V . Andrei, A. Aziz, G. Dankel, C. Gregg,\nX. Meng, C. Meurillon, D. Sheahan, L. Tian, J. Yang, P. Yu, and\nK. Hazelwood, “Datacenter-scale analysis and optimization of gpu\nmachine learning workloads,” IEEE Micro, vol. 41, no. 5, 2021.\n[36] A. Sriraman, A. Dhanotia, and T. F. Wenisch, “Softsku: Optimizing\nserver architectures for microservice diversity @scale,” in Proceed-\nings of the 46th International Symposium on Computer Architecture ,\nAssociation for Computing Machinery, 2019.\n[37] A. Sriraman and A. Dhanotia, “Accelerometer: Understanding ac-\nceleration opportunities for data center overheads at hyperscale,” in\nProceedings of the International Conference on Architectural Support\nfor Programming Languages and Operating Systems , 2020.\n[38] C. Tang, K. Yu, K. Veeraraghavan, J. Kaldor, S. Michelson, T. Kooburat,\nA. Anbudurai, M. Clark, K. Gogia, L. Cheng, B. Christensen, A. Gartrell,\n11\n\nM. Khutornenko, S. Kulkarni, M. Pawlowski, T. Pelkonen, A. Rodrigues,\nR. Tibrewal, V . Venkatesan, and P. Zhang, “Twine: A uniﬁed cluster\nmanagement system for shared infrastructure,” in Proceedings of the\nUSENIX Symposium on Operating Systems Design and Implementation ,\n2020.\n[39] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres, “Quantifying the\ncarbon emissions of machine learning,” Workshop on Tackling Climate\nChange with Machine Learning at NeurIPS 2019 , 2019.\n[40] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau,\n“Towards the systematic reporting of the energy and carbon footprints\nof machine learning,” CoRR, vol. abs/2002.05651, 2020.\n[41] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On\nthe dangers of stochastic parrots: Can language models be too big?,” in\nProceedings of the ACM Conference on Fairness, Accountability, and\nTransparency, 2021.\n[42] N. Sachdeva, C.-J. Wu, and J. McAuley, “Svp-cf: Selection via proxy\nfor collaborative ﬁltering data,” arXiv preprint arXiv:2107.04984 , 2021.\n[43] E. Valavi, J. Hestness, N. Ardalani, and M. Iansiti, Time and the Value\nof Data. Working papers, Harvard Business School, 2020.\n[44] M. Zhao, N. Agarwal, A. Basant, B. Gedik, S. Pan, M. Ozdal,\nR. Komuravelli, J. Pan, T. Bao, H. Lu, S. Narayanan, J. Langman,\nK. Wilfong, H. Rastogi, C. Wu, C. Kozyrakis, and P. Pol, “Understanding\nand co-designing the data ingestion pipeline for industry-scale recsys\ntraining,” CoRR, vol. abs/2108.09373, 2021.\n[45] R. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu,\nand I. Guyon, “Bayesian optimization is superior to random search for\nmachine learning hyperparameter tuning: Analysis of the black-box\noptimization challenge 2020,” CoRR, vol. abs/2104.10201, 2021.\n[46] P. Ren, Y . Xiao, X. Chang, P.-y. Huang, Z. Li, X. Chen, and X. Wang,\n“A comprehensive survey of neural architecture search: Challenges and\nsolutions,” ACM Comput. Surv., vol. 54, no. 4, 2021.\n[47] Q. Song, D. Cheng, H. Zhou, J. Yang, Y . Tian, and X. Hu, “Towards\nautomated neural interaction discovery for click-through rate prediction,”\nProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , 2020.\n[48] M. R. Joglekar, C. Li, M. Chen, T. Xu, X. Wang, J. K. Adams, P. Khaitan,\nJ. Liu, and Q. V . Le, “Neural input search for large scale recommendation\nmodels,” in Proceedings of the ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , 2020.\n[49] M. Tan and Q. V . Le, “Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks,” arXiv preprint arXiv:1905.11946 , 2020.\n[50] D. Eriksson, P. I. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu,\nS. Zhao, A. Aly, G. Venkatesh, and M. Balandat, “Latency-aware neural\narchitecture search with multi-objective bayesian optimization,” CoRR,\nvol. abs/2106.11890, 2021.\n[51] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train\none network and specialize it for efﬁcient deployment,” arXiv preprint\narXiv:1908.09791, 2020.\n[52] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,\nand D. Marculescu, “Single-path nas: Designing hardware-efﬁcient\nconvnets in less than 4 hours,” arXiv preprint arXiv:1904.02877 , 2019.\n[53] W. Chen, X. Gong, and Z. Wang, “Neural architecture search on\nimagenet in four gpu hours: A theoretically inspired perspective,” arXiv\npreprint arXiv:2102.11535, 2021.\n[54] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture\nsearch without training,” arXiv preprint arXiv:2006.04647 , 2021.\n[55] B. Acun, M. Murphy, X. Wang, J. Nie, C. Wu, and K. Hazelwood,\n“Understanding training efﬁciency of deep learning recommendation\nmodels at scale,” in Proceedings of the IEEE International Symposium\non High-Performance Computer Architecture , 2021.\n[56] C. Yin, B. Acun, X. Liu, and C.-J. Wu, “TT-Rec: Tensor train\ncompression for deep learning recommendation models,” in Proceedings\nof the Conference on Machine Learning and Systems , 2021.\n[57] W.-C. Kang, D. Z. Cheng, T. Yao, X. Yi, T. Chen, L. Hong, and E. H.\nChi, “Learning to embed categorical features without embedding tables\nfor recommendation,” arXiv preprint arXiv:2010.10784 , 2021.\n[58] A. S. Nemirovskij and D. B. Yudin, Problem complexity and method\nefﬁciency in optimization . Wiley-Interscience, 1983.\n[59] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E.\nDahl, “On empirical comparisons of optimizers for deep learning,” arXiv\npreprint arXiv:1910.05446, 2019.\n[60] P. T. Sivaprasad, F. Mai, T. V ogels, M. Jaggi, and F. Fleuret, “Opti-\nmizer benchmarking needs to account for hyperparameter tuning,” in\nProceedings of the International Conference on Machine Learning ,\n2020.\n[61] P. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,\nA. Tulloch, Y . Jia, and K. He, “Accurate, large minibatch sgd: Training\nimagenet in 1 hour,” arXiv preprint arXiv:1706.02677 , 2017.\n[62] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine\ntranslation,” arXiv preprint arXiv:1806.00187 , 2018.\n[63] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. V ojnovic, “Qsgd:\nCommunication-efﬁcient sgd via gradient quantization and encoding,” in\nProceedings of the Advances in Neural Information Processing Systems ,\nvol. 30, 2017.\n[64] T. V ogels, S. P. Karinireddy, and M. Jaggi, “Powersgd: Practical low-\nrank gradient compression for distributed optimization,” in Proceedings\nof the Advances In Neural Information Processing Systems , vol. 32,\n2019.\n[65] Y . Huang, Y . Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,\nJ. Ngiam, Q. V . Le, Y . Wu,et al., “Gpipe: Efﬁcient training of giant\nneural networks using pipeline parallelism,” in Proceedings of the\nAdvances in neural information processing systems , vol. 32, 2019.\n[66] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: Memory\noptimizations toward training trillion parameter models,” in Proceedings\nof the International Conference for High Performance Computing,\nNetworking, Storage and Analysis , 2020.\n[67] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He, “Deepspeed: System\noptimizations enable training deep learning models with over 100\nbillion parameters,” in Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2020.\n[68] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin,\nC. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,\nT. V . Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R.\nHo, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey,\nA. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar,\nS. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke,\nA. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Na-\ngarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick,\nN. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani,\nC. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing,\nM. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V . Vasudevan,\nR. Walter, W. Wang, E. Wilcox, and D. H. Yoon, “In-datacenter\nperformance analysis of a tensor processing unit,” in Proceedings of the\nACM/IEEE International Symposium on Computer Architecture , 2017.\n[69] J. Hamilton, “AWS Inferentia Machine Learning Processor,” 2018.\n[70] Azure, “New Azure HPC and partner offerings at Supercomputing 19,”\n2019.\n[71] NVIDIA, “GPUs for Virtualization,” 2021.\n[72] A. Spiridonov, “New Cloud TPU VMs make training your ML models\non TPUs easier than ever,” 2021.\n[73] M. Gschwind, T. Kaldewey, and D. Tam, “Optimizing the efﬁciency\nof deep learning through accelerator virtualization,” IBM Journal of\nResearch and Development , vol. 61, no. 4-5, 2017.\n[74] S. Ghodrati, B. H. Ahn, J. Kyung Kim, S. Kinzer, B. R. Yatham,\nN. Alla, H. Sharma, M. Alian, E. Ebrahimi, N. S. Kim, C. Young, and\nH. Esmaeilzadeh, “Planaria: Dynamic architecture ﬁssion for spatial\nmulti-tenant acceleration of deep neural networks,” in Proceedings of\nthe IEEE/ACM International Symposium on Microarchitecture , 2020.\n[75] S.-C. Kao and T. Krishna, “Domain-speciﬁc genetic algorithm for multi-\ntenant dnnaccelerator scheduling,” arXiv preprint arXiv:2104.13997 ,\n2021.\n[76] M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and\nF. Yang, “Analysis of large-scale multi-tenant gpu clusters for dnn\ntraining workloads,” in Proceedings of the USENIX Annual Technical\nConference, 2019.\n[77] P. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives\nfor deep learning applications,” arXiv preprint arXiv:1902.04610 , 2019.\n[78] R. Jain and J. Wullert, “Challenges: Environmental design for pervasive\ncomputing systems,” in Proceedings of the International Conference\non Mobile Computing and Networking , 2002.\n[79] J. Chang, J. Meza, P. Ranganathan, C. Bash, and A. Shah, “Green server\ndesign: Beyond operational energy to sustainability,” in Proceedings of\n12\n\nthe International Conference on Power Aware Computing and Systems ,\n2010.\n[80] M. Garcia Bardon, P. Wuytens, L.-A. Ragnarsson, G. Mirabelli, D. Jang,\nG. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais, “DTCO\nincluding sustainability: Power-performance-area-cost-environmental\nscore (PPACE) analysis for logic technologies,” in Proceedings of the\nIEEE International Electron Devices Meeting , 2020.\n[81] A. Putnam, A. M. Caulﬁeld, E. S. Chung, D. Chiou, K. Constantinides,\nJ. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray, M. Hasel-\nman, S. Hauck, S. Heil, A. Hormati, J.-Y . Kim, S. Lanka, J. Larus,\nE. Peterson, S. Pope, A. Smith, J. Thong, P. Y . Xiao, and D. Burger,\n“A reconﬁgurable fabric for accelerating large-scale datacenter services,”\nIEEE Micro, 2015.\n[82] Y .-H. Chen, J. Emer, and V . Sze, “Eyeriss: A spatial architecture\nfor energy-efﬁcient dataﬂow for convolutional neural networks,” in\nProceedings of the ACM/IEEE International Symposium on Computer\nArchitecture, 2016.\n[83] A. Radovanovic, R. Koningstein, I. Schneider, B. Chen, A. Duarte,\nB. Roy, D. Xiao, M. Haridasan, P. Hung, N. Care, et al., “Carbon-aware\ncomputing for datacenters,” arXiv preprint arXiv:2106.11750 , 2021.\n[84] H. Cai, C. Gan, L. Zhu, and S. Han, “Tinytl: Reduce memory, not param-\neters for efﬁcient on-device learning,” arXiv preprint arXiv:2007.11622,\n2020.\n[85] K. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and\nD. Ramage, “Federated evaluation of on-device personalization,” arXiv\npreprint arXiv:1910.10252, 2019.\n[86] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,\nV . Ivanov, C. Kiddon, J. Kone ˇcn`y, S. Mazzocchi, H. B. McMahan,\net al. , “Towards federated learning at scale: System design,” arXiv\npreprint arXiv:1902.01046, 2019.\n[87] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-\nstein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for\nmobile keyboard prediction,” arXiv preprint arXiv:1811.03604 , 2018.\n[88] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage,\nand F. Beaufays, “Applied federated learning: Improving google\nkeyboard query suggestions,” arXiv preprint arXiv:1812.02903 , 2018.\n[89] S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated\nlearning for emoji prediction in a mobile keyboard,” arXiv preprint\narXiv:1906.04329, 2019.\n[90] D. Huba, J. Nguyen, K. Malik, R. Zhu, M. Rabbat, A. Yousefpour,\nC.-J. Wu, H. Zhan, P. Ustinov, H. Srinivas, K. Wang, A. Shoumikhin,\nJ. Min, and M. Malek, “Papaya: Practical, private, and scalable federated\nlearning,” arXiv:2111.04877, 2021.\n[91] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar,\nJ. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan,\nI. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray,\nM. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V . Cartillier, S. Crane,\nT. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu,\nC. Fuegen, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y . Huang,\nW. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y . Li,\nZ. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu,\nW. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram,\nA. Southerland, Y . Sugano, R. Tao, M. V o, Y . Wang, X. Wu, T. Yagi,\nY . Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, B. Ghanem,\nV . K. Ithapu, C. V . Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe,\nA. Oliva, H. S. Park, J. M. Rehg, Y . Sato, J. Shi, M. Z. Shou, A. Torralba,\nL. Torresani, M. Yan, and J. Malik, “Ego4d: Around the world in 3,000\nhours of egocentric video,” arXiv:2110.07058, 2021.\n[92] Y . G. Kim and C.-J. Wu, “Autoﬂ: Enabling heterogeneity-aware\nenergy efﬁcient federated learning,” in Proceedings of the IEEE/ACM\nInternational Symposium on Microarchitecture , 2021.\n[93] Y . Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and\nL. Tang, “Neurosurgeon: Collaborative intelligence between the cloud\nand mobile edge,” in Proceedings of the International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2017.\n[94] Y . G. Kim and C.-J. Wu, “Autoscale: Energy efﬁciency optimization for\nstochastic edge inference using reinforcement learning,” in Proceedings\nof the IEEE/ACM International Symposium on Microarchitecture , 2020.\n[95] T.-J. Yang, Y .-H. Chen, and V . Sze, “Designing energy-efﬁcient convolu-\ntional neural networks using energy-aware pruning,” arXiv:1611.05128,\n2017.\n[96] D. Stamoulis, T.-W. R. Chin, A. K. Prakash, H. Fang, S. Sajja,\nM. Bognar, and D. Marculescu, “Designing adaptive neural networks\nfor energy-constrained image classiﬁcation,” in Proceedings of the\nInternational Conference on Computer-Aided Design , 2018.\n[97] C. Gao, A. Gutierrez, M. Rajan, R. G. Dreslinski, T. Mudge, and C.-\nJ. Wu, “A study of mobile device utilization,” in Proceedings of the\nIEEE International Symposium on Performance Analysis of Systems\nand Software, 2015.\n[98] V . Schmidt, K. Goyal, A. Joshi, B. Feld, L. Conell, N. Laskaris, D. Blank,\nJ. Wilson, S. Friedler, and S. Luccioni, “CodeCarbon: Estimate and\nTrack Carbon Emissions from Machine Learning Computing,” 2021.\n[99] K. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, “Energy usage\nreports: Environmental awareness as part of algorithmic accountability,”\nWorkshop on Tackling Climate Change with Machine Learning at\nNeurIPS 2019, 2019.\n[100] D. Kiela, M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu,\nB. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush,\nS. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and\nA. Williams, “Dynabench: Rethinking benchmarking in NLP,” arXiv\npreprint arXiv:2104.14337, 2021.\n[101] D. Hernandez and T. B. Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.\n[102] P. Mattson, V . J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter,\nP. Micikevicius, D. Patterson, G. Schmuelling, H. Tang, G.-Y . Wei, and\nC.-J. Wu, “Mlperf: An industry standard benchmark suite for machine\nlearning performance,” IEEE Micro, vol. 40, no. 2, pp. 8–16, 2020.\n[103] V . J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, and C.-J.\nWu, “The vision behind mlperf: Understanding ai inference performance,”\nIEEE Micro, vol. 41, no. 3, pp. 10–18, 2021.\n[104] V . J. Reddi, D. Kanter, P. Mattson, J. Duke, T. Nguyen, R. Chukka,\nK. Shiring, K.-S. Tan, M. Charlebois, W. Chou, M. El-Khamy, J. Hong,\nM. Buch, C. Trinh, T. Atta-fosu, F. Cakir, M. Charkhabi, X. Chen,\nJ. Chiang, D. Dexter, W. Heo, G. Schmuelling, M. Shabani, and D. Zika,\n“Mlperf mobile inference benchmark,” arXiv:2012.02328, 2021.\n[105] P. Mattson, C. Cheng, G. Diamos, C. Coleman, P. Micikevicius,\nD. Patterson, H. Tang, G.-Y . Wei, P. Bailis, V . Bittorf, D. Brooks,\nD. Chen, D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang,\nD. Kang, D. Kanter, N. Kumar, J. Liao, D. Narayanan, T. Oguntebi,\nG. Pekhimenko, L. Pentecost, V . Janapa Reddi, T. Robie, T. St John, C.-\nJ. Wu, L. Xu, C. Young, and M. Zaharia, “Mlperf training benchmark,”\nin Proceedings of Machine Learning and Systems , vol. 2, 2020.\n[106] V . J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J.\nWu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka,\nC. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S.\nGardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar,\nD. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius,\nC. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao,\nF. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada,\nB. Yu, G. Yuan, A. Zhong, P. Zhang, and Y . Zhou, “Mlperf inference\nbenchmark,” in Proceedings of the ACM/IEEE Annual International\nSymposium on Computer Architecture , 2020.\n[107] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchin-\nson, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model\nreporting,” Proceedings of the Conference on Fairness, Accountability,\nand Transparency, 2019.\n[108] C.-J. Wu, S. Manne, P. Ranganathan, S. Bird, and S. Greenstein,\n“Socio-technological challenges and opportunities: Paths forward,” arXiv\npreprint arXiv:2108.06738, 2021.\n[109] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” arXiv\npreprint arXiv:1907.10597, 2019.\n[110] K. Maeng, S. Bharuka, I. Gao, M. C. Jeffrey, V . Saraph, B.-Y . Su,\nC. Trippel, J. Yang, M. Rabbat, B. Lucia, and C.-J. Wu, “Cpr:\nUnderstanding and improving failure tolerant training for deep learning\nrecommendation with partial recovery,” inProceedings of the Conference\non Machine Learning and Systems , 2021.\n[111] A. Eisenman, K. K. Matam, S. Ingram, D. Mudigere, R. Krishnamoorthi,\nK. Nair, M. Smelyanskiy, and M. Annavaram, “Check-n-run: A\ncheckpointing system for training deep learning recommendation\nmodels,” arXiv preprint arXiv:2010.08679 , 2021.\n[112] H. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy,\nB. Muthiah, and S. Sankar, “Silent data corruptions at scale,” arXiv\npreprint arXiv:2102.11245, 2021.\n13\n\n[113] P. H. Hochschild, P. Turner, J. C. Mogul, R. Govindaraju, P. Ranganathan,\nD. E. Culler, and A. Vahdat, “Cores that don’t count,” in Proceedings\nof the Workshop on Hot Topics in Operating Systems , 2021.\n[114] X. Qiu, T. Parcollet, J. Fernandez-Marques, P. P. B. de Gusmao, D. J.\nBeutel, T. Topal, A. Mathur, and N. D. Lane, “A ﬁrst look into the\ncarbon footprint of federated learning,” arXiv preprint arXiv:2102.07627,\n2021.\n[115] H. Wang, B. Kim, J. Xie, and Z. Han, “How is energy consumed in\nsmartphone deep learning apps? executing locally vs. remotely,” in\nProceedings of the IEEE Global Communications Conference , 2019.\n[116] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan,\nK. Hazelwood, E. Isaac, Y . Jia, B. Jia, T. Leyvand, H. Lu, Y . Lu, L. Qiao,\nB. Reagen, J. Spisak, F. Sun, A. Tulloch, P. Vajda, X. Wang, Y . Wang,\nB. Wasti, Y . Wu, R. Xian, S. Yoo, and P. Zhang, “Machine learning\nat facebook: Understanding inference at the edge,” in Proceedings of\nthe IEEE International Symposium on High Performance Computer\nArchitecture, 2019.\n[117] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On\nthe opportunities and risks of foundation models,” arXiv preprint\narXiv:2108.07258, 2021.\n[118] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in Proceedings of the\nInternational conference on machine learning , pp. 1597–1607, 2020.\n[119] M. Assran, M. Caron, I. Misra, P. Bojanowski, A. Joulin, N. Ballas,\nand M. Rabbat, “Semi-supervised learning of visual features by non-\nparametrically predicting view assignments with support samples,” arXiv\npreprint arXiv:2104.13963, 2021.\n[120] L. M. Dery, P. Michel, A. Talwalkar, and G. Neubig, “Should we be\npre-training? an argument for end-task aware training as an alternative,”\narXiv preprint arXiv:2109.07437 , 2021.\ny = 0.7892x-0.004R²= 0.9969\ny = 0.7903x-0.002R²= 0.9986\n0.787\n0.789\n0.791\n0.793\n0.795\n0.797\n0.1250.250.5124\nModel Error  (Lower is Better)\nEnergy/Step (J)\nData Scale: 1XData Scale: 2XData Scale: 4XData Scale: 8XData Scale: 16XModel Scale: 2XModel Scale: 4XModel Scale: 8XModel Scale: 16XModel Scale: 32XModel Scale: 64XModel Scale: 128X0   \n0.002\n0.004\n0.006\n0.008\n0.010\nFig. 12. Model quality of recommendation use cases improves as we scale up\nthe amount of data and/or the number of model parameters (e.g., embedding\ncardinality or dimension), leading to higher energy and carbon footprint.\nMaximizing model accuracy for the speciﬁc recommendation use case comes\nwith signiﬁcant energy cost — Roughly 4 × energy saving can be achieved\nwith only 0.004 model quality degradation (green vs. yellow stars).\nAPPENDIX\nDespite the recent calls-to-action [ 28], [39], [40], [41], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI. There\nare several factors that may have contributed to the current\nstate of AI:\n• Lack of incentives: Over 90% of the ML publications\nonly focus on model accuracy improvements at the\nexpense of efﬁciency [ 109]. Challenges 10 incentivize\ninvestment into efﬁcient approaches.\n• Lack of common tools: There is no standard telemetry\nin place to provide accurate, reliable energy and carbon\nfootprint measurement. The measurement methodology\nis complex — factors, such as datacenter infrastructures,\nhardware architectures, energy sources, can perturb the\nﬁnal measure easily.\n• Lack of normalization factors: Algorithmic progress in\nML is often presented in some measure of model accuracy,\ne.g., BLEU, points, ELO, cross-entropy loss, but without\nconsidering resource requirement as a normalization factor,\ne.g., the number of\nCPU/GPU/TPU hours used, the overall energy consump-\ntion and/or carbon footprint required.\n• Platform fragmentation: Implementation details can\nhave a signiﬁcant impact on real-world efﬁciency, but\nbest practices remain elusive and platform fragmentation\nprevents performance and efﬁciency portability across\nmodel development.\nA. Data Utilization Efﬁciency\nFigure 12 depicts energy footprint reduction potential when\ndata and model scaling is performed in tandem. The x-axis\n10Efﬁcient Open-Domain Question Answering (https://efﬁcientqa.github.io/),\nSustaiNLP: Simple and Efﬁcient Natural Language Processing (https://site\ns.google.com/view/sustainlp2020/home), and WMT: Machine Translation\nEfﬁciency Task (http://www.statmt.org/wmt21/efﬁciency-task.html).\n14\n\nrepresents the energy footprint required per training step\nwhereas the y-axis represents model error. The blue solid\nlines capture model size scaling (through embedding hash\nscaling) while the training data set size is kept ﬁxed. Each\nline corresponds to a different data set size, in an increasing\norder from top to bottom. The points within each line represent\ndifferent model (embedding) sizes, in an increasing order from\nleft to right. The red dashed lines capture data scaling while the\nmodel size is kept ﬁxed. Each line corresponds to a different\nembedding hash size, in an increasing order from left to right.\nThe points within each line represent different data sizes, in\nan increasing order from top to bottom. The dashed black\nline captures the performance scaling trend as we scale data\nand model sizes in tandem. This represents the energy-optimal\nscaling approach.\nScaling data sizes or model sizes independently deviates from\nthe energy-optimal trend. We highlight two energy-optimal\nsettings along the Pareto-frontier curve. The yellow star uses\nthe scaling setting of Data scaling 2 × and Model scaling 2 ×\nwhereas the green star adopts the setting of Data scaling 8 ×\nand Model scaling 16 ×. The yellow star consumes roughly 4 ×\nlower energy as compared to the green star with only 0.004\nmodel quality degradation in Normalized Entropy. Overall\nmodel quality performance has a (diminishing) power-law\nrelationship with the corresponding energy consumption and\nthe power of the power law is extremely small (0.002-0.004).\nThis means achieving higher model quality through model-data\nscaling for recommendation use cases incurs signiﬁcant energy\ncost.\nB. Efﬁcient, Environmentally-Sustainable AI Systems\nDisaggregating Machine Learning Pipeline Stages: As\ndepicted in Figure 3, the overall training throughput efﬁciency\nfor large-scale ML models depends on the throughput perfor-\nmance of both data ingestion and pre-processing and model\ntraining. Disaggregating the data ingestion and pre-processing\nstage of the machine learning pipeline from model training\nis the de-facto approach for industry-scale machine learning\nmodel training. This allows training accelerator, network\nand storage I/O bandwidth utilization to scale independently,\nthereby increasing the overall model training throughput by\n56% [44]. Disaggregation with well-designed check-pointing\nsupport [110], [111] improves training fault tolerance as well.\nBy doing so, failure on nodes that are responsible for data\ningestion and pre-processing can be recovered efﬁciently\nwithout requiring re-runs of the entire training experiment.\nFrom a sustainability perspective, disaggregating the data\nstorage and ingestion stage from model training maximizes\ninfrastructure efﬁciency by using less system resources to\nachieve higher training throughput, resulting in lower embodied\ncarbon footprint. By increasing fault tolerance, the operational\ncarbon footprint is reduced at the same time.\nFault-Tolerant AI Systems and Hardware: One way to\namortize the rising embodied carbon cost of AI infrastructures\nis to extend hardware lifetime. However, hardware ages\n— depending on the wear-out characteristics, increasingly\nmore errors can surface over time and result in silent data\ncorruption, leading to erroneous computation, model accuracy\ndegradation, non-deterministic ML execution, or fatal system\nfailure. In a large ﬂeet of processors, silent data corruption\ncan occur frequently enough to have disruptive impact on\nservice productivity [ 112], [ 113]. Decommissioning an AI\nsystem entirely because of hardware faults is expensive from the\nperspective of resource and environmental footprints. System\narchitects can design differential reliability levels for micro\narchitectural components on an AI system depending on the\nML model execution characteristics. Alternatively, algorithmic\nfault tolerance can be built into deep learning programming\nframeworks to provide a code execution path that is cognizant\nof hardware wear-out characteristics.\nOn-Device Learning: Federated learning and optimization\ncan result in a non-negligible amount of carbon emissions\nat the edge, similar to the carbon footprint of training\nT ransf ormerBig [21]. Figure 11 shows that the federated\nlearning and optimization process emits non-negligible carbon\nat the edge due to both computation and wireless communi-\ncation during the process. To estimate the carbon emission,\nwe used a similar methodology to [ 114]. We collected the\n90-day log data for federated learning production use cases\nat Facebook, which recorded the time spent on computation,\ndata downloading, and data uploading per client device. We\nmultiplied the computation time with the estimated device\npower and upload/download time with the estimated router\npower, and omitted other energy. We assumed a device power of\n3W and a router power of 7.5W [115], [114]. Model training on\nclient edge devices is inherently less energy-efﬁcient because\nof the high wireless communication overheads, sub-optimal\ntraining data distribution in individual client devices [114], large\ndegree of system heterogeneity among client edge devices, and\nhighly-fragmented edge device architectures that make system-\nlevel optimization signiﬁcantly more challenging [ 116]. Note,\nthe wireless communication energy cost takes up a signiﬁcant\nportion of the overall energy footprint of federated learning,\nmaking energy footprint optimization on communication im-\nportant.\nC. Efﬁciency and Self-Supervised Learning\nSelf-supervised learning (SSL) have received much attention\nin the research community in recent years. SSL methods train\ndeep neural networks without using explicit supervision in\nthe form of human-annotated labels for each training sample.\nHaving humans annotate data is a time-consuming, expensive,\nand typically noisy process. SSL methods are typically used\nto train foundation models — models that can readily be ﬁne-\ntuned using a small amount of labeled data on a down-stream\ntask [117]. SSL methods have been extremely successful for\npre-training large language models, becoming the de-facto\nstandard, and they have also attracted great interest in computer\nvision.\nWhen comparing supervised and self-supervised methods,\nthere is a glaring trade-off between having labels and the\namount of computational overhead involved in pre-training. For\nexample, Chen et al. report achieving 69.3% top-1 validation\naccuracy with a ResNet-50 model after SSL pre-training for\n15\n\n1000 epochs on the ImageNet dataset and using the linear\nevaluation protocol, freezing the pre-trained feature extractor,\nand ﬁne-tuning a linear classiﬁer on top for 60 epochs using the\nfull ImageNet dataset with all labels [118]. In contrast, the same\nmodel typically achieves at least 76.1% top-1 accuracy after\n90 epochs of fully-supervised training. Thus, in this example,\nusing labels and supervised training is worth a roughly 10 ×\nreduction in training effort, measured in terms of number of\npasses over the dataset.\nRecent work suggests that incorporating even a small amount\nof labeled data can signiﬁcantly bridge this gap. Assran et\nal. describe an approach called Predicting view Assignments\nWith Support samples (PAWS) for semi-supervised pre-training\ninspired by SSL [ 119]. With access to labels for just 10% of\nthe training images in ImageNet, a ResNet-50 achieves 75.5%\ntop-1 accuracy after just 200 epochs of PAWS pre-training.\nRunning on 64 V100 GPUs, this takes roughly 16 hours. Similar\nobservations have recently been made for language model pre-\ntraining as well [120].\nSelf-supervised pre-training potentially has advantages in\nthat a single foundation model can be trained (expensive) but\nthen ﬁne-tuned (inexpensive), amortizing the up front cost\nacross many tasks [ 117]. Substantial additional research is\nneeded to better understand the cost-beneﬁt trade-offs for this\nparadigm.\n16", "metadata": {"url": "https://arxiv.org/pdf/2111.00364", "type": "paper", "year": "2021"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "1\nSustainable AI: Environmental Implications,\nChallenges and Opportunities\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott,\nAnastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee,\nBugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood\nFacebook AI\nAbstract—This paper explores the environmental impact of\nthe super-linear growth trends for AI from a holistic perspective,\nspanning Data, Algorithms, and System Hardware. We character-\nize the carbon footprint of AI computing by examining the model\ndevelopment cycle across industry-scale machine learning use\ncases and, at the same time, considering the life cycle of system\nhardware. Taking a step further, we capture the operational and\nmanufacturing carbon footprint of AI computing and present an\nend-to-end analysis for what and how hardware-software design\nand at-scale optimization can help reduce the overall carbon\nfootprint of AI. Based on the industry experience and lessons\nlearned, we share the key challenges and chart out important\ndevelopment directions across the many dimensions of AI. We\nhope the key messages and insights presented in this paper\ncan inspire the community to advance the ﬁeld of AI in an\nenvironmentally-responsible manner.\nI. I NTRODUCTION\nArtiﬁcial Intelligence (AI) is one of the fastest growing\ndomains spanning research and product development and\nsigniﬁcant investment in AI is taking place across nearly every\nindustry, policy, and academic research. This investment in\nAI has also stimulated novel applications in domains such as\nscience, medicine, ﬁnance, and education. Figure 1 analyzes\nthe number of papers published within the scientiﬁc disciplines,\nillustrating the growth trend in recent years 1.\nAI plays an instrumental role to push the boundaries of\nknowledge and sparks novel, more efﬁcient approaches to\nconventional tasks. AI is applied to predict protein structures\nradically better than previous methods. It has the potential to\nrevolutionize biological sciences by providing in-silico methods\nfor tasks only possible in a physical laboratory setting [ 1]. AI\nis demonstrated to achieve human-level conversation tasks,\nsuch as the Blender Bot [ 2], and play games at superhuman\nlevels, such as AlphaZero [ 3]. AI is used to discover new\nelectrocatalysts for efﬁcient and scalable ways to store and\nutilize renewable energy [ 4], predicting renewable energy\navailability in advance to improve energy utilization [ 5],\noperating hyperscale data centers efﬁciently [6], growing plants\nusing less natural resources [ 7], and, at the same time, being\nused to tackle climate changes [ 8], [9]. It is projected that, in\nthe next ﬁve years, the market for AI will increase by 10 × into\nhundreds of billions of dollars [ 10]. All of these investments\n1Based on monthly counts, Figure 1 estimates the cumulative number of\npapers published per category on the arXiv database.\n0\n50\n100\n150\n200\n250\n20112013201520172019\nCumulative arXivArticle CountsThousands\nAI in Different Disciplines Computer ScienceMathMachine LearningPhysics\nSource: arXiv.org\nFig. 1. The growth of ML is exceeding that of many other scientiﬁc disciplines.\nSigniﬁcant research growth in machine learning is observed in recent years as\nillustrated by the increasing cumulative number of papers published in machine\nlearning with respect to other scientiﬁc disciplines based on the monthly count\n(y-axis measures the cumulative number of articles on arXiv).\nin research, development, and deployment have led to a super-\nlinear growth in AI data, models, and infrastructure capacity.\nWith the dramatic growth of AI, it is imperative to understand\nthe environmental implications, challenges, and opportunities\nof this nascent technology. This is because technologies tend to\ncreate a self-accelerating growth cycle, putting new demands\non the environment.\nThis work explores the environmental impact of AI from\na holistic perspective. More speciﬁcally, we present the\nchallenges and opportunities to designing sustainable AI\ncomputing across the key phases of the machine learning (ML)\ndevelopment process — Data, Experimentation, Training, and\nInference — for a variety of AI use cases at Facebook, such\nas vision, language, speech, recommendation and ranking. The\nsolution space spans across our ﬂeet of datacenters and on-\ndevice computing. Given particular use cases, we consider the\nimpact of AI data, algorithms, and system hardware. Finally,\nwe consider emissions across the life cycle of hardware systems,\nfrom manufacturing to operational use.\nAI Data Growth. In the past decade, we have seen an\nexponential increase in AI training data and model capacity.\nFigure 2(b) illustrates that the amount of training data at\nFacebook for two recommendation use cases — one of the\nfastest growing areas of ML usage at Facebook— has increased\nby 2.4× and 1.9× in the last two years, reaching exabyte scale.\nThe increase in data size has led to a 3.2 × increase in data\ningestion bandwidth demand. Given this increase, data storage\nand the ingestion pipeline accounts for a signiﬁcant portion of\narXiv:2111.00364v2  [cs.LG]  9 Jan 2022", "sentences": [{"text": "1\nSustainable AI: Environmental Implications,\nChallenges and Opportunities\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott,\nAnastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S.", "metadata": {}}, {"text": "Lee,\nBugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood\nFacebook AI\nAbstract—This paper explores the environmental impact of\nthe super-linear growth trends for AI from a holistic perspective,\nspanning Data, Algorithms, and System Hardware.", "metadata": {}}, {"text": "We character-\nize the carbon footprint of AI computing by examining the model\ndevelopment cycle across industry-scale machine learning use\ncases and, at the same time, considering the life cycle of system\nhardware.", "metadata": {}}, {"text": "Taking a step further, we capture the operational and\nmanufacturing carbon footprint of AI computing and present an\nend-to-end analysis for what and how hardware-software design\nand at-scale optimization can help reduce the overall carbon\nfootprint of AI.", "metadata": {}}, {"text": "Based on the industry experience and lessons\nlearned, we share the key challenges and chart out important\ndevelopment directions across the many dimensions of AI.", "metadata": {}}, {"text": "We\nhope the key messages and insights presented in this paper\ncan inspire the community to advance the ﬁeld of AI in an\nenvironmentally-responsible manner.", "metadata": {}}, {"text": "I.", "metadata": {}}, {"text": "I NTRODUCTION\nArtiﬁcial Intelligence (AI) is one of the fastest growing\ndomains spanning research and product development and\nsigniﬁcant investment in AI is taking place across nearly every\nindustry, policy, and academic research.", "metadata": {}}, {"text": "This investment in\nAI has also stimulated novel applications in domains such as\nscience, medicine, ﬁnance, and education.", "metadata": {}}, {"text": "Figure 1 analyzes\nthe number of papers published within the scientiﬁc disciplines,\nillustrating the growth trend in recent years 1.", "metadata": {}}, {"text": "AI plays an instrumental role to push the boundaries of\nknowledge and sparks novel, more efﬁcient approaches to\nconventional tasks.", "metadata": {}}, {"text": "AI is applied to predict protein structures\nradically better than previous methods.", "metadata": {}}, {"text": "It has the potential to\nrevolutionize biological sciences by providing in-silico methods\nfor tasks only possible in a physical laboratory setting [ 1].", "metadata": {}}, {"text": "AI\nis demonstrated to achieve human-level conversation tasks,\nsuch as the Blender Bot [ 2], and play games at superhuman\nlevels, such as AlphaZero [ 3].", "metadata": {}}, {"text": "AI is used to discover new\nelectrocatalysts for efﬁcient and scalable ways to store and\nutilize renewable energy [ 4], predicting renewable energy\navailability in advance to improve energy utilization [ 5],\noperating hyperscale data centers efﬁciently [6], growing plants\nusing less natural resources [ 7], and, at the same time, being\nused to tackle climate changes [ 8], [9].", "metadata": {}}, {"text": "It is projected that, in\nthe next ﬁve years, the market for AI will increase by 10 × into\nhundreds of billions of dollars [ 10].", "metadata": {}}, {"text": "All of these investments\n1Based on monthly counts, Figure 1 estimates the cumulative number of\npapers published per category on the arXiv database.", "metadata": {}}, {"text": "0\n50\n100\n150\n200\n250\n20112013201520172019\nCumulative arXivArticle CountsThousands\nAI in Different Disciplines Computer ScienceMathMachine LearningPhysics\nSource: arXiv.org\nFig.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "The growth of ML is exceeding that of many other scientiﬁc disciplines.", "metadata": {}}, {"text": "Signiﬁcant research growth in machine learning is observed in recent years as\nillustrated by the increasing cumulative number of papers published in machine\nlearning with respect to other scientiﬁc disciplines based on the monthly count\n(y-axis measures the cumulative number of articles on arXiv).", "metadata": {}}, {"text": "in research, development, and deployment have led to a super-\nlinear growth in AI data, models, and infrastructure capacity.", "metadata": {}}, {"text": "With the dramatic growth of AI, it is imperative to understand\nthe environmental implications, challenges, and opportunities\nof this nascent technology.", "metadata": {}}, {"text": "This is because technologies tend to\ncreate a self-accelerating growth cycle, putting new demands\non the environment.", "metadata": {}}, {"text": "This work explores the environmental impact of AI from\na holistic perspective.", "metadata": {}}, {"text": "More speciﬁcally, we present the\nchallenges and opportunities to designing sustainable AI\ncomputing across the key phases of the machine learning (ML)\ndevelopment process — Data, Experimentation, Training, and\nInference — for a variety of AI use cases at Facebook, such\nas vision, language, speech, recommendation and ranking.", "metadata": {}}, {"text": "The\nsolution space spans across our ﬂeet of datacenters and on-\ndevice computing.", "metadata": {}}, {"text": "Given particular use cases, we consider the\nimpact of AI data, algorithms, and system hardware.", "metadata": {}}, {"text": "Finally,\nwe consider emissions across the life cycle of hardware systems,\nfrom manufacturing to operational use.", "metadata": {}}, {"text": "AI Data Growth.", "metadata": {}}, {"text": "In the past decade, we have seen an\nexponential increase in AI training data and model capacity.", "metadata": {}}, {"text": "Figure 2(b) illustrates that the amount of training data at\nFacebook for two recommendation use cases — one of the\nfastest growing areas of ML usage at Facebook— has increased\nby 2.4× and 1.9× in the last two years, reaching exabyte scale.", "metadata": {}}, {"text": "The increase in data size has led to a 3.2 × increase in data\ningestion bandwidth demand.", "metadata": {}}, {"text": "Given this increase, data storage\nand the ingestion pipeline accounts for a signiﬁcant portion of\narXiv:2111.00364v2  [cs.LG]  9 Jan 2022", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "05101520Model Size\nTime\n(c) Model Growth TrendDLRM Parameter #s\n00.0050.010.0150.020.0250.030.035\n051015202530354045\n0.1101000\nAbsolute AUC Improvement\nBLEU Score\nModel Size (Billions of Parameters in Log Scale)\n(a) 1000x Model Size ScalingGPT English2FrenchGPT French2EnglishRecSys SearchRecSys Images\nxxx\nx\nx\n2019-21\nx\n0.511.522.533.544.5Data & Ingestion BandwidthTime\n(b) Data Growth TrendDLRM1 DataDLRM2 DataData Ingestion BW\n1\n1.5\n2\n2.5\n3\nYr1-Q1Yr1-Q2Yr1-Q3Yr1-Q4Yr2-Q1Yr2-Q2\nAI System Capacity\n(d) System Growth TrendTrainingInference2.9x\n2.5x\n20x1,000x\nFig. 2. Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years. (a) The 1000× model size\ngrowth has led to higher model accuracy for various ML tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a\nmodel 1, 000× larger in size. (b) At Facebook, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2\ntimes increase in the data ingestion bandwidth demand. (c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same\ntime period [11]. (d) The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.\nthe infrastructure and power capacity compared to ML training\nand end-to-end machine learning life cycles.\nAI Model Growth. The ever-increasing data volume has also\ndriven a super-linear trend in model size growth. Figure 2(a) de-\npicts the 1000× model size increase for GPT3-based language\ntranslation tasks [ 12], [13], whereas for Baidu’s search engine,\nthe model of 1000× larger in size improves accuracy in AUC\nby 0.030. Despite small, the accuracy improvement can lead\nto signiﬁcantly higher-quality search outcomes [ 14]. Similarly,\nFigure 2(c) illustrates that between 2019 and 2021, the size\nof recommendation models at Facebook has increased by\n20× [15], [16], [17], [11]. Despite the large increase in model\nsizes, the memory capacity of GPU-based AI accelerators,\ne.g. 32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100,\n2021), has increased by < 2× every 2 years. The resource\nrequirements for strong AI scaling clearly outpaces that of\nsystem hardware.\nAI Infrastructure Growth. The strong performance scaling\ndemand for ML motivates a variety of scale-out solutions [11],\n[18] by leveraging parallelism at scale with a massive collection\nof training accelerators. Figure 2(d) illustrates that the explosive\ngrowth in AI use cases at Facebook has driven 2.9× increase\nin AI training infrastructure capacity over the 1.5 years. In\naddition, we observe trillions of inference per day across\nFacebook’s data centers—more than doubling in the past 3\nyears. The increase in inference demands has also led to an\n2.5× increase in AI inference infrastructure capacity. Last but\nnot least, the carbon footprint of AI goes beyond its operational\nenergy consumption. The embodied carbon footprint of systems\nis becoming a dominating factor for AI’s overall environmental\nimpact (Section III) [19].\nThe Elephant in the Room. Despite the positive societal\nbeneﬁts [20], the endless pursuit of achieving higher model\nquality has led to the exponential scaling of AI with signiﬁcant\nenergy and environmental footprint implications. Although\nrecent work shows the carbon footprint of training one large\nML model, such as Meena [21], is equivalent to 242,231 miles\ndriven by an average passenger vehicle [ 22], this is only one\naspect; to fully understand the real environmental impact we\nmust consider the AI ecosystem holistically going forward —\nbeyond looking at model training alone and by accounting\nfor both operational and embodied carbon footprint of AI.\nWe must look at the ML pipeline end-to-end: data collection,\nmodel exploration and experimentation, model training, model\noptimization and run-time inference. The frequency of training\nand scale of each stage of the ML development cycle matter.\nFrom the systems perspective, the life cycle of ML software\nand system hardware, including manufacturing and operational\nuse, must also be considered.\nOptimizing across ML pipelines and systems life cycles end-\nto-end is a complex and challenging task. While training large,\nsparsely-activated neural networks improves model scalability,\nachieving higher accuracy at lower operational energy foot-\nprint [21], it can incur higher embodied carbon footprint from\nthe increase in the system resource requirement. Shifting model\ntraining and inference to data centers with carbon-free energy\ncan reduce emissions; however, this approach may not scale to\na broad set of use cases. Infrastructure for carbon-free energy\nis limited by factors such as geography and available materials\n(e.g. rare metals), and takes signiﬁcant economic resources and\ntime to build. In addition, as on-device learning becomes more\nubiquitously adopted to improve data privacy, we can see more\ncomputation being shifted away from data centers to the edge,\nwhere access to renewable energy is limited.\nA Holistic Approach. This paper is the ﬁrst to take a holistic\napproach to characterize the environmental footprint of AI\ncomputing from experimentation and training to inference.\nWe characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nmachine learning use cases at Facebook (Section II). This is\nillustrated by the more than 800 × operational carbon footprint\nreduction achieved through judicious hardware-software co-\ndesign for a Transformer-based universal language model.\nTaking a step further, we present an end-to-end analysis for\nboth operational and embodied carbon footprint for AI training\nand inference (Section III). Based on the industry experience\nand lessons learned, we chart out opportunities and important\ndevelopment directions across the dimensions of AI including —\ndata, algorithm, systems, metrics, standards, and best practices\n(Section IV). We hope the key messages (Section VI) and the\ninsights in this paper can inspire the community to advance\nthe ﬁeld of AI in an environmentally-responsible manner.\n2", "sentences": [{"text": "05101520Model Size\nTime\n(c) Model Growth TrendDLRM Parameter #s\n00.0050.010.0150.020.0250.030.035\n051015202530354045\n0.1101000\nAbsolute AUC Improvement\nBLEU Score\nModel Size (Billions of Parameters in Log Scale)\n(a) 1000x Model Size ScalingGPT English2FrenchGPT French2EnglishRecSys SearchRecSys Images\nxxx\nx\nx\n2019-21\nx\n0.511.522.533.544.5Data & Ingestion BandwidthTime\n(b) Data Growth TrendDLRM1 DataDLRM2 DataData Ingestion BW\n1\n1.5\n2\n2.5\n3\nYr1-Q1Yr1-Q2Yr1-Q3Yr1-Q4Yr2-Q1Yr2-Q2\nAI System Capacity\n(d) System Growth TrendTrainingInference2.9x\n2.5x\n20x1,000x\nFig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years.", "metadata": {}}, {"text": "(a) The 1000× model size\ngrowth has led to higher model accuracy for various ML tasks.", "metadata": {}}, {"text": "For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a\nmodel 1, 000× larger in size.", "metadata": {}}, {"text": "(b) At Facebook, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2\ntimes increase in the data ingestion bandwidth demand.", "metadata": {}}, {"text": "(c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same\ntime period [11].", "metadata": {}}, {"text": "(d) The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.", "metadata": {}}, {"text": "the infrastructure and power capacity compared to ML training\nand end-to-end machine learning life cycles.", "metadata": {}}, {"text": "AI Model Growth.", "metadata": {}}, {"text": "The ever-increasing data volume has also\ndriven a super-linear trend in model size growth.", "metadata": {}}, {"text": "Figure 2(a) de-\npicts the 1000× model size increase for GPT3-based language\ntranslation tasks [ 12], [13], whereas for Baidu’s search engine,\nthe model of 1000× larger in size improves accuracy in AUC\nby 0.030.", "metadata": {}}, {"text": "Despite small, the accuracy improvement can lead\nto signiﬁcantly higher-quality search outcomes [ 14].", "metadata": {}}, {"text": "Similarly,\nFigure 2(c) illustrates that between 2019 and 2021, the size\nof recommendation models at Facebook has increased by\n20× [15], [16], [17], [11].", "metadata": {}}, {"text": "Despite the large increase in model\nsizes, the memory capacity of GPU-based AI accelerators,\ne.g.", "metadata": {}}, {"text": "32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100,\n2021), has increased by < 2× every 2 years.", "metadata": {}}, {"text": "The resource\nrequirements for strong AI scaling clearly outpaces that of\nsystem hardware.", "metadata": {}}, {"text": "AI Infrastructure Growth.", "metadata": {}}, {"text": "The strong performance scaling\ndemand for ML motivates a variety of scale-out solutions [11],\n[18] by leveraging parallelism at scale with a massive collection\nof training accelerators.", "metadata": {}}, {"text": "Figure 2(d) illustrates that the explosive\ngrowth in AI use cases at Facebook has driven 2.9× increase\nin AI training infrastructure capacity over the 1.5 years.", "metadata": {}}, {"text": "In\naddition, we observe trillions of inference per day across\nFacebook’s data centers—more than doubling in the past 3\nyears.", "metadata": {}}, {"text": "The increase in inference demands has also led to an\n2.5× increase in AI inference infrastructure capacity.", "metadata": {}}, {"text": "Last but\nnot least, the carbon footprint of AI goes beyond its operational\nenergy consumption.", "metadata": {}}, {"text": "The embodied carbon footprint of systems\nis becoming a dominating factor for AI’s overall environmental\nimpact (Section III) [19].", "metadata": {}}, {"text": "The Elephant in the Room.", "metadata": {}}, {"text": "Despite the positive societal\nbeneﬁts [20], the endless pursuit of achieving higher model\nquality has led to the exponential scaling of AI with signiﬁcant\nenergy and environmental footprint implications.", "metadata": {}}, {"text": "Although\nrecent work shows the carbon footprint of training one large\nML model, such as Meena [21], is equivalent to 242,231 miles\ndriven by an average passenger vehicle [ 22], this is only one\naspect;", "metadata": {}}, {"text": "to fully understand the real environmental impact we\nmust consider the AI ecosystem holistically going forward —\nbeyond looking at model training alone and by accounting\nfor both operational and embodied carbon footprint of AI.", "metadata": {}}, {"text": "We must look at the ML pipeline end-to-end: data collection,\nmodel exploration and experimentation, model training, model\noptimization and run-time inference.", "metadata": {}}, {"text": "The frequency of training\nand scale of each stage of the ML development cycle matter.", "metadata": {}}, {"text": "From the systems perspective, the life cycle of ML software\nand system hardware, including manufacturing and operational\nuse, must also be considered.", "metadata": {}}, {"text": "Optimizing across ML pipelines and systems life cycles end-\nto-end is a complex and challenging task.", "metadata": {}}, {"text": "While training large,\nsparsely-activated neural networks improves model scalability,\nachieving higher accuracy at lower operational energy foot-\nprint [21], it can incur higher embodied carbon footprint from\nthe increase in the system resource requirement.", "metadata": {}}, {"text": "Shifting model\ntraining and inference to data centers with carbon-free energy\ncan reduce emissions;", "metadata": {}}, {"text": "however, this approach may not scale to\na broad set of use cases.", "metadata": {}}, {"text": "Infrastructure for carbon-free energy\nis limited by factors such as geography and available materials\n(e.g.", "metadata": {}}, {"text": "rare metals), and takes signiﬁcant economic resources and\ntime to build.", "metadata": {}}, {"text": "In addition, as on-device learning becomes more\nubiquitously adopted to improve data privacy, we can see more\ncomputation being shifted away from data centers to the edge,\nwhere access to renewable energy is limited.", "metadata": {}}, {"text": "A Holistic Approach.", "metadata": {}}, {"text": "This paper is the ﬁrst to take a holistic\napproach to characterize the environmental footprint of AI\ncomputing from experimentation and training to inference.", "metadata": {}}, {"text": "We characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nmachine learning use cases at Facebook (Section II).", "metadata": {}}, {"text": "This is\nillustrated by the more than 800 × operational carbon footprint\nreduction achieved through judicious hardware-software co-\ndesign for a Transformer-based universal language model.", "metadata": {}}, {"text": "Taking a step further, we present an end-to-end analysis for\nboth operational and embodied carbon footprint for AI training\nand inference (Section III).", "metadata": {}}, {"text": "Based on the industry experience\nand lessons learned, we chart out opportunities and important\ndevelopment directions across the dimensions of AI including —\ndata, algorithm, systems, metrics, standards, and best practices\n(Section IV).", "metadata": {}}, {"text": "We hope the key messages (Section VI) and the\ninsights in this paper can inspire the community to advance\nthe ﬁeld of AI in an environmentally-responsible manner.", "metadata": {}}, {"text": "2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "ExperimentationTrainingInference\nStorage –Network --Compute\nManufacturingTransport Product UseRecycling\nDeep Learning Framework & Library\n(a) Fleet View\n(b) Machine Learning Task View [Section 3.1]\n(c) Infrastructure View\n-2024681012\n20162017201820192020\nMetric Tons CO2e Millions[Use] Operational (renewable)[Use] Operational Scope 1,2[Manufacturing] Value Chain CO2 Scope 3Carbon Removal\n0%20%40%60%80%100%\nDataTraining (Offline)Training (Online/Evaluation)DeploymentRM1\nMachine Learning Model Development and Deployment Phases [Section 2.1]\nSystem Life Cycle [Section 2.2]\nDataData Efficiency[Section 4.1]Resource-Efficient Experimentation, Algorithms, and Model Architectures [Section 4.2]\nEfficient, Environmentally-Sustainable AI System Hardware [Section 4.3]\nFig. 3. Model Development Phases over AI System Hardware Life Cycle: (a) At Facebook, we observe a rough power capacity breakdown of 10:20:70 for\nAI infrastructures devoted to the three key phases — Experimentation, Training, and Inference; (b) Considering the primary stages of the ML pipeline\nend-to-end, the energy footprint of RM1 is roughly 31:29:40 over Data, Experimentation/Training, and Inference; (c) Despite the investment to neutralize\nthe operational footprint with carbon-free energy, the overall data center electricity use continues to grow, demanding over 7.17 million MWh in 2020 [23].\nII. M ODEL DEVELOPMENT PHASES AND AI S YSTEM\nHARDWARE LIFE CYCLE\nFigure 3 depicts the major development phases for ML —\nData Processing, Experimentation, Training, and Inference\n(Section II-A) — over the life cycle of AI system hardware\n(Section II-B). Driven by distinct objectives of AI research\nand advanced product development, infrastructure is designed\nand built speciﬁcally to maximize data storage and ingestion\nefﬁciency for the phase of Data Processing , developer efﬁ-\nciency for the phase of Experimentation, training throughput\nefﬁciency for the phase of Training, and tail-latency bounded\nthroughput efﬁciency for Inference.\nA. Machine Learning Model Development Cycle\nML researchers extract features from data during the Data\nProcessing phase and apply weights to individual features\nbased on feature importance to the model optimization objective.\nDuring Experimentation, the researchers design, implement\nand evaluate the quality of proposed algorithms, model ar-\nchitectures, modeling techniques, and/or training methods for\ndetermining model parameters. This model exploration process\nis computationally-intensive. A large collection of diverse ML\nideas are explored simultaneously at-scale. Thus, during this\nphase, we observe unique system resource requirements from\nthe large pool of training experiments. Within Facebook’s ML\nresearch cluster, 50% (p50) of ML training experiments take up\nto 1.5 GPU days while 99% (p99) of the experiments complete\nwithin 24 GPU days. There are a number of large-scale, trillion\nparameter models which require over 500 GPUs days.\nOnce a ML solution is determined as promising, it moves into\nTraining where the ML solution is evaluated using extensive\nproduction data — data that is more recent, is larger in quantity,\nand contains richer features . The process often requires\nadditional hyper-parameter tuning. Depending on the ML task\nrequirement, the models can be trained/re-trained at different\nfrequencies. For example, models supporting Facebook’sSearch\nservice were trained at an hourly cadence whereas the Language\nTranslation models were trained weekly [24]. A p50 production\nmodel training workﬂow takes 2.96 GPU days while a training\nworkﬂow at p99 can take up to 125 GPU days.\nFinally, for Inference, the best-performing model is de-\nployed, producing trillions of daily predictions to serve billions\nof users worldwide. The total compute cycles for inference\npredictions are expected to exceed the corresponding training\ncycles for the deployed model.\nB. Machine Learning System Life Cycle\nLife Cycle Analysis (LCA) is a common methodology to\nassess the carbon emissions over the product life cycle. There\nare four major phases: manufacturing, transport, product use,\nand recycling2. From the perspective of AI’s carbon footprint\nanalysis, manufacturing and product use are the focus. Thus,\nin this work, we consider the overall carbon footprint of\nAI by including manufacturing — carbon emissions from\nbuilding infrastructures speciﬁcally for AI (i.e., embodied\ncarbon footprint) and product use — carbon emissions from\nthe use of AI (i.e., operational carbon footprint ).\nWhile quantifying the exact breakdown between operational\nand embodied carbon footprint is a complex process, we\nestimate the signiﬁcance of embodied carbon emissions using\nFacebook’s Greenhouse Gas (GHG) emission statistics 3. In this\ncase, more than 50% of Facebook’s emissions owe to its value\nchain — Scope 3 of Facebook’s GHG emission . As a result,\na signiﬁcant embodied carbon cost is paid upfront for every\nsystem component brought into Facebook’s ﬂeet of datacenters,\nwhere AI is the biggest growth driver.\n2Recycling is an important domain, for which the industry is developing\na circular economy model to up-cycle system components — design with\nrecycling in mind.\n3Facebook Sustainability Data: https://sustainability.fb.com/report/2020-sust\nainability-report/.\n3", "sentences": [{"text": "ExperimentationTrainingInference\nStorage –Network --Compute\nManufacturingTransport Product UseRecycling\nDeep Learning Framework & Library\n(a) Fleet View\n(b) Machine Learning Task View [Section 3.1]\n(c) Infrastructure View\n-2024681012\n20162017201820192020\nMetric Tons CO2e Millions[Use] Operational (renewable)[Use] Operational Scope 1,2[Manufacturing] Value Chain CO2 Scope 3Carbon Removal\n0%20%40%60%80%100%\nDataTraining (Offline)Training (Online/Evaluation)DeploymentRM1\nMachine Learning Model Development and Deployment Phases [Section 2.1]\nSystem Life Cycle [Section 2.2]\nDataData Efficiency[Section 4.1]Resource-Efficient Experimentation, Algorithms, and Model Architectures [Section 4.2]\nEfficient, Environmentally-Sustainable AI System Hardware [Section 4.3]\nFig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Model Development Phases over AI System Hardware Life Cycle: (a) At Facebook, we observe a rough power capacity breakdown of 10:20:70 for\nAI infrastructures devoted to the three key phases — Experimentation, Training, and Inference;", "metadata": {}}, {"text": "(b) Considering the primary stages of the ML pipeline\nend-to-end, the energy footprint of RM1 is roughly 31:29:40 over Data, Experimentation/Training, and Inference;", "metadata": {}}, {"text": "(c) Despite the investment to neutralize\nthe operational footprint with carbon-free energy, the overall data center electricity use continues to grow, demanding over 7.17 million MWh in 2020 [23].", "metadata": {}}, {"text": "II.", "metadata": {}}, {"text": "M ODEL DEVELOPMENT PHASES AND AI S YSTEM\nHARDWARE LIFE CYCLE\nFigure 3 depicts the major development phases for ML —\nData Processing, Experimentation, Training, and Inference\n(Section II-A) — over the life cycle of AI system hardware\n(Section II-B).", "metadata": {}}, {"text": "Driven by distinct objectives of AI research\nand advanced product development, infrastructure is designed\nand built speciﬁcally to maximize data storage and ingestion\nefﬁciency for the phase of Data Processing , developer efﬁ-\nciency for the phase of Experimentation, training throughput\nefﬁciency for the phase of Training, and tail-latency bounded\nthroughput efﬁciency for Inference.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Machine Learning Model Development Cycle\nML researchers extract features from data during the Data\nProcessing phase and apply weights to individual features\nbased on feature importance to the model optimization objective.", "metadata": {}}, {"text": "During Experimentation, the researchers design, implement\nand evaluate the quality of proposed algorithms, model ar-\nchitectures, modeling techniques, and/or training methods for\ndetermining model parameters.", "metadata": {}}, {"text": "This model exploration process\nis computationally-intensive.", "metadata": {}}, {"text": "A large collection of diverse ML\nideas are explored simultaneously at-scale.", "metadata": {}}, {"text": "Thus, during this\nphase, we observe unique system resource requirements from\nthe large pool of training experiments.", "metadata": {}}, {"text": "Within Facebook’s ML\nresearch cluster, 50% (p50) of ML training experiments take up\nto 1.5 GPU days while 99% (p99) of the experiments complete\nwithin 24 GPU days.", "metadata": {}}, {"text": "There are a number of large-scale, trillion\nparameter models which require over 500 GPUs days.", "metadata": {}}, {"text": "Once a ML solution is determined as promising, it moves into\nTraining where the ML solution is evaluated using extensive\nproduction data — data that is more recent, is larger in quantity,\nand contains richer features .", "metadata": {}}, {"text": "The process often requires\nadditional hyper-parameter tuning.", "metadata": {}}, {"text": "Depending on the ML task\nrequirement, the models can be trained/re-trained at different\nfrequencies.", "metadata": {}}, {"text": "For example, models supporting Facebook’sSearch\nservice were trained at an hourly cadence whereas the Language\nTranslation models were trained weekly [24].", "metadata": {}}, {"text": "A p50 production\nmodel training workﬂow takes 2.96 GPU days while a training\nworkﬂow at p99 can take up to 125 GPU days.", "metadata": {}}, {"text": "Finally, for Inference, the best-performing model is de-\nployed, producing trillions of daily predictions to serve billions\nof users worldwide.", "metadata": {}}, {"text": "The total compute cycles for inference\npredictions are expected to exceed the corresponding training\ncycles for the deployed model.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Machine Learning System Life Cycle\nLife Cycle Analysis (LCA) is a common methodology to\nassess the carbon emissions over the product life cycle.", "metadata": {}}, {"text": "There\nare four major phases: manufacturing, transport, product use,\nand recycling2.", "metadata": {}}, {"text": "From the perspective of AI’s carbon footprint\nanalysis, manufacturing and product use are the focus.", "metadata": {}}, {"text": "Thus,\nin this work, we consider the overall carbon footprint of\nAI by including manufacturing — carbon emissions from\nbuilding infrastructures speciﬁcally for AI (i.e., embodied\ncarbon footprint) and product use — carbon emissions from\nthe use of AI (i.e., operational carbon footprint ).", "metadata": {}}, {"text": "While quantifying the exact breakdown between operational\nand embodied carbon footprint is a complex process, we\nestimate the signiﬁcance of embodied carbon emissions using\nFacebook’s Greenhouse Gas (GHG) emission statistics 3.", "metadata": {}}, {"text": "In this\ncase, more than 50% of Facebook’s emissions owe to its value\nchain — Scope 3 of Facebook’s GHG emission .", "metadata": {}}, {"text": "As a result,\na signiﬁcant embodied carbon cost is paid upfront for every\nsystem component brought into Facebook’s ﬂeet of datacenters,\nwhere AI is the biggest growth driver.", "metadata": {}}, {"text": "2Recycling is an important domain, for which the industry is developing\na circular economy model to up-cycle system components — design with\nrecycling in mind.", "metadata": {}}, {"text": "3Facebook Sustainability Data: https://sustainability.fb.com/report/2020-sust\nainability-report/.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=Im1.png] Size: 335x335, Data: 6583 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=Im1.png] Size: 335x335, Data: 6583 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "Im1.png", "image_width": 335, "image_height": 335, "attachment_type": "image", "has_image_data": true, "image_data_size": 6583}}, {"text": "[Image page=3 idx=2 name=Im10.png] Size: 247x247, Data: 3016 bytes", "sentences": [{"text": "[Image page=3 idx=2 name=Im10.png] Size: 247x247, Data: 3016 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 2, "image_name": "Im10.png", "image_width": 247, "image_height": 247, "attachment_type": "image", "has_image_data": true, "image_data_size": 3016}}, {"text": "[Image page=3 idx=3 name=Im11.png] Size: 239x225, Data: 3069 bytes", "sentences": [{"text": "[Image page=3 idx=3 name=Im11.png] Size: 239x225, Data: 3069 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 3, "image_name": "Im11.png", "image_width": 239, "image_height": 225, "attachment_type": "image", "has_image_data": true, "image_data_size": 3069}}, {"text": "[Image page=3 idx=4 name=Im12.png] Size: 240x225, Data: 3068 bytes", "sentences": [{"text": "[Image page=3 idx=4 name=Im12.png] Size: 240x225, Data: 3068 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 4, "image_name": "Im12.png", "image_width": 240, "image_height": 225, "attachment_type": "image", "has_image_data": true, "image_data_size": 3068}}, {"text": "[Image page=3 idx=5 name=Im13.png] Size: 239x225, Data: 3076 bytes", "sentences": [{"text": "[Image page=3 idx=5 name=Im13.png] Size: 239x225, Data: 3076 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 5, "image_name": "Im13.png", "image_width": 239, "image_height": 225, "attachment_type": "image", "has_image_data": true, "image_data_size": 3076}}, {"text": "[Image page=3 idx=6 name=Im14.png] Size: 240x225, Data: 3057 bytes", "sentences": [{"text": "[Image page=3 idx=6 name=Im14.png] Size: 240x225, Data: 3057 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 6, "image_name": "Im14.png", "image_width": 240, "image_height": 225, "attachment_type": "image", "has_image_data": true, "image_data_size": 3057}}, {"text": "[Image page=3 idx=7 name=Im15.jpg] Size: 934x444, Data: 90606 bytes", "sentences": [{"text": "[Image page=3 idx=7 name=Im15.jpg] Size: 934x444, Data: 90606 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 7, "image_name": "Im15.jpg", "image_width": 934, "image_height": 444, "attachment_type": "image", "has_image_data": true, "image_data_size": 90606}}, {"text": "[Image page=3 idx=8 name=Im16.png] Size: 196x195, Data: 3029 bytes", "sentences": [{"text": "[Image page=3 idx=8 name=Im16.png] Size: 196x195, Data: 3029 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 8, "image_name": "Im16.png", "image_width": 196, "image_height": 195, "attachment_type": "image", "has_image_data": true, "image_data_size": 3029}}, {"text": "[Image page=3 idx=9 name=Im17.png] Size: 284x284, Data: 3232 bytes", "sentences": [{"text": "[Image page=3 idx=9 name=Im17.png] Size: 284x284, Data: 3232 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 9, "image_name": "Im17.png", "image_width": 284, "image_height": 284, "attachment_type": "image", "has_image_data": true, "image_data_size": 3232}}, {"text": "[Image page=3 idx=10 name=Im18.png] Size: 233x233, Data: 2114 bytes", "sentences": [{"text": "[Image page=3 idx=10 name=Im18.png] Size: 233x233, Data: 2114 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 10, "image_name": "Im18.png", "image_width": 233, "image_height": 233, "attachment_type": "image", "has_image_data": true, "image_data_size": 2114}}, {"text": "[Image page=3 idx=11 name=Im19.png] Size: 1618x618, Data: 28366 bytes", "sentences": [{"text": "[Image page=3 idx=11 name=Im19.png] Size: 1618x618, Data: 28366 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 11, "image_name": "Im19.png", "image_width": 1618, "image_height": 618, "attachment_type": "image", "has_image_data": true, "image_data_size": 28366}}, {"text": "[Image page=3 idx=12 name=Im2.png] Size: 313x312, Data: 6139 bytes", "sentences": [{"text": "[Image page=3 idx=12 name=Im2.png] Size: 313x312, Data: 6139 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 12, "image_name": "Im2.png", "image_width": 313, "image_height": 312, "attachment_type": "image", "has_image_data": true, "image_data_size": 6139}}, {"text": "[Image page=3 idx=13 name=Im20.jpg] Size: 312x296, Data: 22536 bytes", "sentences": [{"text": "[Image page=3 idx=13 name=Im20.jpg] Size: 312x296, Data: 22536 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 13, "image_name": "Im20.jpg", "image_width": 312, "image_height": 296, "attachment_type": "image", "has_image_data": true, "image_data_size": 22536}}, {"text": "[Image page=3 idx=14 name=Im3.png] Size: 217x217, Data: 9936 bytes", "sentences": [{"text": "[Image page=3 idx=14 name=Im3.png] Size: 217x217, Data: 9936 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 14, "image_name": "Im3.png", "image_width": 217, "image_height": 217, "attachment_type": "image", "has_image_data": true, "image_data_size": 9936}}, {"text": "[Image page=3 idx=15 name=Im4.png] Size: 202x203, Data: 11186 bytes", "sentences": [{"text": "[Image page=3 idx=15 name=Im4.png] Size: 202x203, Data: 11186 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 15, "image_name": "Im4.png", "image_width": 202, "image_height": 203, "attachment_type": "image", "has_image_data": true, "image_data_size": 11186}}, {"text": "[Image page=3 idx=16 name=Im5.png] Size: 212x212, Data: 5419 bytes", "sentences": [{"text": "[Image page=3 idx=16 name=Im5.png] Size: 212x212, Data: 5419 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 16, "image_name": "Im5.png", "image_width": 212, "image_height": 212, "attachment_type": "image", "has_image_data": true, "image_data_size": 5419}}, {"text": "[Image page=3 idx=17 name=Im6.png] Size: 205x205, Data: 14473 bytes", "sentences": [{"text": "[Image page=3 idx=17 name=Im6.png] Size: 205x205, Data: 14473 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 17, "image_name": "Im6.png", "image_width": 205, "image_height": 205, "attachment_type": "image", "has_image_data": true, "image_data_size": 14473}}, {"text": "[Image page=3 idx=18 name=Im7.png] Size: 243x243, Data: 5006 bytes", "sentences": [{"text": "[Image page=3 idx=18 name=Im7.png] Size: 243x243, Data: 5006 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 18, "image_name": "Im7.png", "image_width": 243, "image_height": 243, "attachment_type": "image", "has_image_data": true, "image_data_size": 5006}}, {"text": "[Image page=3 idx=19 name=Im8.png] Size: 243x243, Data: 4956 bytes", "sentences": [{"text": "[Image page=3 idx=19 name=Im8.png] Size: 243x243, Data: 4956 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 19, "image_name": "Im8.png", "image_width": 243, "image_height": 243, "attachment_type": "image", "has_image_data": true, "image_data_size": 4956}}, {"text": "[Image page=3 idx=20 name=Im9.png] Size: 243x243, Data: 4863 bytes", "sentences": [{"text": "[Image page=3 idx=20 name=Im9.png] Size: 243x243, Data: 4863 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 20, "image_name": "Im9.png", "image_width": 243, "image_height": 243, "attachment_type": "image", "has_image_data": true, "image_data_size": 4863}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "*Training footprint only\n0.00\n0.50\n1.00\nLMRM-1RM-2RM-3RM-4RM-5BERT-NASEvolved TransformerT5MeenaGShard-600BSwitch TransformerGPT3\nFacebookOSS Large-Scale ML Models\nCO2e (kg)Millions\nOperational Carbon Footprint of Large-Scale ML TasksOffline TrainingOnline TrainingInference\nFig. 4. The carbon footprint of the LM model is dominated by Inference\nwhereas, for RM1 – RM5, the carbon footprint of Training versus Inference is\nroughly equal. The average carbon footprint for ML training tasks at Facebook\nis 1.8 times larger than that of Meena used in modern conversational agents\nand 0.3 times of GPT-3’s carbon footprint. Carbon footprint for inference\ntasks is included for models that are used in production. Note: the operational\ncarbon footprint of AI does not correlate with the number of model parameters.\nThe OSS large-scale ML tasks are based on the vanilla model architectures\nfrom [21] and may not be reﬂective of production use cases.\nIII. AI C OMPUTING ’S CARBON FOOTPRINT\nA. Carbon Footprint Analysis for Industry-Scale ML Training\nand Deployment\nFigure 4 illustrates the operational carbon emissions for\nmodel training and inference across the ML tasks. We analyze\nsix representative machine learning models in production\nat Facebook 4. LM refers to Facebook’s Transformer-based\nUniversal Language Model for text translation [ 25]. RM1 –\nRM5 represent ﬁve unique deep learning recommendation and\nranking models for various Facebook products [26], [27].\nWe compare the carbon footprint of Facebook’s production\nML models with seven large-scale, open-source (OSS) models:\nBERT-NAS, T5, Meena, GShard-600B, Switch Transformer,\nand GPT-3. Note, we present the operational carbon footprint\nof the OSS model training from [ 28], [ 21]. The operational\ncarbon footprint results can vary based on the exact AI\nsystems used and the carbon intensity of the energy mixture.\nModels with more parameters do not necessarily result in\nlonger training time nor higher carbon emissions. Training\nthe Switch Transformer model equipped with 1.5 trillion\nparameters [ 29] produces signiﬁcantly less carbon emission\nthan that of GPT-3 (750 billion parameters) [13]. This illustrates\nthe carbon footprint advantage of operationally-efﬁcient model\narchitectures.\n4In total, the six models account for a vast majority of compute resources\nfor the overall inference predictions at Facebook, serving billions of users\nworld wide.\n00.20.40.60.811.21.4\nLMRM-1RM-2RM-3RM-4RM-5\nCO2e (kg)Millions\nOverall Carbon Footprint of Large-Scale ML TasksOperational Carbon Cost (Offset with solar)Operational Carbon Cost (Rest)Projected Embodied Carbon Cost\ncarbon-free energy\nFig. 5. When considering the overall life cycle of ML models and systems in\nthis analysis, manufacturing carbon cost is roughly 50% of the (location-based)\noperational carbon footprint of large-scale ML tasks (Figure 4). Taking into\naccount carbon-free energy, such as solar, the operational energy consumption\ncan be signiﬁcantly reduced, leaving the manufacturing carbon cost as the\ndominating source of AI’s carbon footprint.\nBoth Training and Inference can contribute signiﬁcantly to the\noverall carbon footprint of machine learning tasks at Facebook.\nThe exact breakdown between the two phases varies across\nML use cases.\nThe overall operational carbon footprint is categorized into\nofﬂine training, online training, and inference. Ofﬂine training\nencompasses both experimentation and training models with\nhistorical data. Online training is particularly relevant to\nrecommendation models where parameters are continuously\nupdated based on recent data. The inference footprint represents\nthe emission from serving production trafﬁc. The online training\nand inference emissions are considered over the period of\nofﬂine training. For recommendation use cases, we ﬁnd the\ncarbon footprint is split evenly between training and inference.\nOn the other hand, the carbon footprint of LM is dominated\nby the inference phase, using much higher inference resources\n(65%) as compared to training (35%).\nBoth operational and embodied carbon emissions can con-\ntribute signiﬁcantly to the overall footprint of ML tasks .\nOperational Carbon Footprint: Across the life cycle of\nthe Facebook models shown in Figure 4, the average carbon\nfootprint is 1.8 × higher than that of the open-source Meena\nmodel [ 30] and one-third of GPT-3’s training footprint. To\nquantify the emissions of Facebook’s models we measure\nthe total energy consumed, assume location-based carbon\nintensities for energy mixes, 5 and use a data center Power\nUsage Effectiveness (PUE) of 1.1. In addition to model-level\nand hardware-level optimizations, Facebook’s renewable energy\nprocurement [23] programs mitigates these emissions.\nEmbodied Carbon Footprint: To quantify the embodied\ncarbon footprint of AI hardware, we use LCA (Section II-B).\nWe assume GPU-based AI training systems have similar\n5Renewable energy and sustainability programs of Facebook [23].\n4", "sentences": [{"text": "*Training footprint only\n0.00\n0.50\n1.00\nLMRM-1RM-2RM-3RM-4RM-5BERT-NASEvolved TransformerT5MeenaGShard-600BSwitch TransformerGPT3\nFacebookOSS Large-Scale ML Models\nCO2e (kg)Millions\nOperational Carbon Footprint of Large-Scale ML TasksOffline TrainingOnline TrainingInference\nFig.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "The carbon footprint of the LM model is dominated by Inference\nwhereas, for RM1 – RM5, the carbon footprint of Training versus Inference is\nroughly equal.", "metadata": {}}, {"text": "The average carbon footprint for ML training tasks at Facebook\nis 1.8 times larger than that of Meena used in modern conversational agents\nand 0.3 times of GPT-3’s carbon footprint.", "metadata": {}}, {"text": "Carbon footprint for inference\ntasks is included for models that are used in production.", "metadata": {}}, {"text": "Note: the operational\ncarbon footprint of AI does not correlate with the number of model parameters.", "metadata": {}}, {"text": "The OSS large-scale ML tasks are based on the vanilla model architectures\nfrom [21] and may not be reﬂective of production use cases.", "metadata": {}}, {"text": "III.", "metadata": {}}, {"text": "AI C OMPUTING ’S CARBON FOOTPRINT\nA.", "metadata": {}}, {"text": "Carbon Footprint Analysis for Industry-Scale ML Training\nand Deployment\nFigure 4 illustrates the operational carbon emissions for\nmodel training and inference across the ML tasks.", "metadata": {}}, {"text": "We analyze\nsix representative machine learning models in production\nat Facebook 4.", "metadata": {}}, {"text": "LM refers to Facebook’s Transformer-based\nUniversal Language Model for text translation [ 25].", "metadata": {}}, {"text": "RM1 –\nRM5 represent ﬁve unique deep learning recommendation and\nranking models for various Facebook products [26], [27].", "metadata": {}}, {"text": "We compare the carbon footprint of Facebook’s production\nML models with seven large-scale, open-source (OSS) models:\nBERT-NAS, T5, Meena, GShard-600B, Switch Transformer,\nand GPT-3.", "metadata": {}}, {"text": "Note, we present the operational carbon footprint\nof the OSS model training from [ 28], [ 21].", "metadata": {}}, {"text": "The operational\ncarbon footprint results can vary based on the exact AI\nsystems used and the carbon intensity of the energy mixture.", "metadata": {}}, {"text": "Models with more parameters do not necessarily result in\nlonger training time nor higher carbon emissions.", "metadata": {}}, {"text": "Training\nthe Switch Transformer model equipped with 1.5 trillion\nparameters [ 29] produces signiﬁcantly less carbon emission\nthan that of GPT-3 (750 billion parameters) [13].", "metadata": {}}, {"text": "This illustrates\nthe carbon footprint advantage of operationally-efﬁcient model\narchitectures.", "metadata": {}}, {"text": "4In total, the six models account for a vast majority of compute resources\nfor the overall inference predictions at Facebook, serving billions of users\nworld wide.", "metadata": {}}, {"text": "00.20.40.60.811.21.4\nLMRM-1RM-2RM-3RM-4RM-5\nCO2e (kg)Millions\nOverall Carbon Footprint of Large-Scale ML TasksOperational Carbon Cost (Offset with solar)Operational Carbon Cost (Rest)Projected Embodied Carbon Cost\ncarbon-free energy\nFig.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "When considering the overall life cycle of ML models and systems in\nthis analysis, manufacturing carbon cost is roughly 50% of the (location-based)\noperational carbon footprint of large-scale ML tasks (Figure 4).", "metadata": {}}, {"text": "Taking into\naccount carbon-free energy, such as solar, the operational energy consumption\ncan be signiﬁcantly reduced, leaving the manufacturing carbon cost as the\ndominating source of AI’s carbon footprint.", "metadata": {}}, {"text": "Both Training and Inference can contribute signiﬁcantly to the\noverall carbon footprint of machine learning tasks at Facebook.", "metadata": {}}, {"text": "The exact breakdown between the two phases varies across\nML use cases.", "metadata": {}}, {"text": "The overall operational carbon footprint is categorized into\nofﬂine training, online training, and inference.", "metadata": {}}, {"text": "Ofﬂine training\nencompasses both experimentation and training models with\nhistorical data.", "metadata": {}}, {"text": "Online training is particularly relevant to\nrecommendation models where parameters are continuously\nupdated based on recent data.", "metadata": {}}, {"text": "The inference footprint represents\nthe emission from serving production trafﬁc.", "metadata": {}}, {"text": "The online training\nand inference emissions are considered over the period of\nofﬂine training.", "metadata": {}}, {"text": "For recommendation use cases, we ﬁnd the\ncarbon footprint is split evenly between training and inference.", "metadata": {}}, {"text": "On the other hand, the carbon footprint of LM is dominated\nby the inference phase, using much higher inference resources\n(65%) as compared to training (35%).", "metadata": {}}, {"text": "Both operational and embodied carbon emissions can con-\ntribute signiﬁcantly to the overall footprint of ML tasks .", "metadata": {}}, {"text": "Operational Carbon Footprint: Across the life cycle of\nthe Facebook models shown in Figure 4, the average carbon\nfootprint is 1.8 × higher than that of the open-source Meena\nmodel [ 30] and one-third of GPT-3’s training footprint.", "metadata": {}}, {"text": "To\nquantify the emissions of Facebook’s models we measure\nthe total energy consumed, assume location-based carbon\nintensities for energy mixes, 5 and use a data center Power\nUsage Effectiveness (PUE) of 1.1.", "metadata": {}}, {"text": "In addition to model-level\nand hardware-level optimizations, Facebook’s renewable energy\nprocurement [23] programs mitigates these emissions.", "metadata": {}}, {"text": "Embodied Carbon Footprint: To quantify the embodied\ncarbon footprint of AI hardware, we use LCA (Section II-B).", "metadata": {}}, {"text": "We assume GPU-based AI training systems have similar\n5Renewable energy and sustainability programs of Facebook [23].", "metadata": {}}, {"text": "4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "0%\n5%\n10%\n15%\n20%\n25%Operational PowerFootprint Improvement Normalized to Overall AI Infrastructure\n2-year time period\nOptimization is an Iterative Process\nModelPlatformInfrastructureHardware\nPerformance-per-Watt[Moore’s Law]\nPerformance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\nResource-Efficient AI Models[Figure 8]\nFig. 6. Optimization is an iterative process — we have achieved an average of\n20% operational energy footprint reduction every 6 months across the machine\nlearning hardware-software stack.\nembodied footprint as the production footprint of Apple’s 28-\ncore CPU with dual AMD Radeon GPUs (2000kg CO 2) [31].\nFor CPU-only systems, we assume half the embodied emissions.\nBased on the characterization of model training and inference at\nFacebook, we assume an average utilization of 30-60% over the\n3- to 5-year lifetime for servers. Figure 5 presents the overall\ncarbon footprint for the large scale ML tasks at Facebook,\nspanning both operational and embodied carbon footprint.\nBased on the assumptions of location-based renewable energy\navailability, the split between the embodied and (location-\nbased) operational carbon footprint is roughly 30% / 70%\nfor the large scale ML tasks. Taking into account carbon-free\nenergy, such as solar, the operational carbon footprint can be\nsigniﬁcantly reduced, leaving the manufacturing carbon cost\nas the dominating source of AI’s carbon footprint.\nB. Carbon Footprint Optimization from Hardware-Software\nCo-Design\nOptimization is an iterative process — we reduce the power\nfootprint across the machine learning hardware-software stack\nby 20% every 6 months. But at the same time, AI infrastructure\ncontinued to scale out. The net effect, with Jevon’s Paradox, is\na 28.5% operational power footprint reduction over two years\n(Figure 8).\nOptimization across AI Model Development and System\nStack over Time: Figure 6 shows the operational power\nfootprint reduction across Facebook’s AI ﬂeet over two years.\nThe improvement come from four areas of optimizations:\nmodel (e.g., designing resource-efﬁcient models), platform\n(e.g., PyTorch’s support for quantization), infrastructure (e.g.,\ndata center optimization and low-precision hardware), and\nhardware (e.g., domain-speciﬁc acceleration). Each bar illus-\ntrates the operational power reduction across Facebook’s AI\nﬂeet over 6-month period from each of the optimization areas.\nThe optimizations in aggregate provide, on average, a 20%\nreduction in operational power consumption every six months.\n810\n121\n12511\n10\n100\n1,000\nCPU BaselineCPU DataManagementGPU FP32GPU FP16FasterTransformer\nOperational Power Footprint Normalized to Optimized Transformer on GPUs \nPlatform-Level Caching\nGPU Accelerators\nNumerical Optimization\nPlatform+ Hardware +Algorithm810 x\nOptimizedTransformer\nFig. 7. For the cross-lingual ML task (LM), the operational energy footprint\ncan be signiﬁcantly reduced by more than 800× using platform-level caching,\nGPUs, low precision data format , and additional algorithmic optimization .\nThe compounded beneﬁts highlight the need for cross-stack\noptimizations.\nOptimizing the Carbon Footprint of LMs: We dive\ninto a speciﬁc machine learning task at Facebook: language\ntranslation using a Transformer-based architecture ( LM). LM\nis designed based on the state-of-the-art cross-lingual un-\nderstanding through self-supervision. Figure 7 analyzes the\npower footprint improvements over a collection of optimization\nsteps for LM: platform-level caching, GPU acceleration, low\nprecision format on accelerator , and model optimization. In\naggregate the optimizations reduce the infrastructure resources\nrequired to serve LM at scale by over 800 ×. We outline the\noptimization beneﬁts from each area below.\n• Platform-Level Caching. Starting with a CPU server\nbaseline, application-level caching improves power efﬁ-\nciency by 6.7 ×. These improvements are a result of pre-\ncomputing and caching frequently accessed embeddings\nfor language translation tasks. Using DRAM and Flash\nstorage devices as caches, these pre-computed embeddings\ncan be shared across applications and use cases.\n• GPU acceleration. In addition to caching, deploying LM\nacross GPU-based specialized AI hardware unlocks an\nadditional 10.1× energy efﬁciency improvement.\n• Algorithmic optimization. Finally, algorithmic optimiza-\ntions provide an additional 12 × energy efﬁciency re-\nduction. Halving precision (e.g., going from 32-bit to\n16-bit operations) provides a 2.4 × energy efﬁciency\nimprovement on GPUs. Another 5× energy efﬁciency gain\ncan be achieved by using custom operators to schedule\nencoding steps within a single kernel of the Transformer\nmodule, such as [32].\nOptimizing the Carbon Footprint of RMs: The LM\nanalysis is used as an example to highlight the optimiza-\ntion opportunities available with judicious cross-stack, hard-\nware/software optimization. In addition to optimizing the\ncarbon footprint for the language translation task, we describe\nadditional optimization techniques tailored for ranking and\n5", "sentences": [{"text": "0%\n5%\n10%\n15%\n20%\n25%Operational PowerFootprint Improvement Normalized to Overall AI Infrastructure\n2-year time period\nOptimization is an Iterative Process\nModelPlatformInfrastructureHardware\nPerformance-per-Watt[Moore’s Law]\nPerformance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\nResource-Efficient AI Models[Figure 8]\nFig.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "Optimization is an iterative process — we have achieved an average of\n20% operational energy footprint reduction every 6 months across the machine\nlearning hardware-software stack.", "metadata": {}}, {"text": "embodied footprint as the production footprint of Apple’s 28-\ncore CPU with dual AMD Radeon GPUs (2000kg CO 2) [31].", "metadata": {}}, {"text": "For CPU-only systems, we assume half the embodied emissions.", "metadata": {}}, {"text": "Based on the characterization of model training and inference at\nFacebook, we assume an average utilization of 30-60% over the\n3- to 5-year lifetime for servers.", "metadata": {}}, {"text": "Figure 5 presents the overall\ncarbon footprint for the large scale ML tasks at Facebook,\nspanning both operational and embodied carbon footprint.", "metadata": {}}, {"text": "Based on the assumptions of location-based renewable energy\navailability, the split between the embodied and (location-\nbased) operational carbon footprint is roughly 30% / 70%\nfor the large scale ML tasks.", "metadata": {}}, {"text": "Taking into account carbon-free\nenergy, such as solar, the operational carbon footprint can be\nsigniﬁcantly reduced, leaving the manufacturing carbon cost\nas the dominating source of AI’s carbon footprint.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Carbon Footprint Optimization from Hardware-Software\nCo-Design\nOptimization is an iterative process — we reduce the power\nfootprint across the machine learning hardware-software stack\nby 20% every 6 months.", "metadata": {}}, {"text": "But at the same time, AI infrastructure\ncontinued to scale out.", "metadata": {}}, {"text": "The net effect, with Jevon’s Paradox, is\na 28.5% operational power footprint reduction over two years\n(Figure 8).", "metadata": {}}, {"text": "Optimization across AI Model Development and System\nStack over Time: Figure 6 shows the operational power\nfootprint reduction across Facebook’s AI ﬂeet over two years.", "metadata": {}}, {"text": "The improvement come from four areas of optimizations:\nmodel (e.g., designing resource-efﬁcient models), platform\n(e.g., PyTorch’s support for quantization), infrastructure (e.g.,\ndata center optimization and low-precision hardware), and\nhardware (e.g., domain-speciﬁc acceleration).", "metadata": {}}, {"text": "Each bar illus-\ntrates the operational power reduction across Facebook’s AI\nﬂeet over 6-month period from each of the optimization areas.", "metadata": {}}, {"text": "The optimizations in aggregate provide, on average, a 20%\nreduction in operational power consumption every six months.", "metadata": {}}, {"text": "810\n121\n12511\n10\n100\n1,000\nCPU BaselineCPU DataManagementGPU FP32GPU FP16FasterTransformer\nOperational Power Footprint Normalized to Optimized Transformer on GPUs \nPlatform-Level Caching\nGPU Accelerators\nNumerical Optimization\nPlatform+ Hardware +Algorithm810 x\nOptimizedTransformer\nFig.", "metadata": {}}, {"text": "7.", "metadata": {}}, {"text": "For the cross-lingual ML task (LM), the operational energy footprint\ncan be signiﬁcantly reduced by more than 800× using platform-level caching,\nGPUs, low precision data format , and additional algorithmic optimization .", "metadata": {}}, {"text": "The compounded beneﬁts highlight the need for cross-stack\noptimizations.", "metadata": {}}, {"text": "Optimizing the Carbon Footprint of LMs: We dive\ninto a speciﬁc machine learning task at Facebook: language\ntranslation using a Transformer-based architecture ( LM).", "metadata": {}}, {"text": "LM\nis designed based on the state-of-the-art cross-lingual un-\nderstanding through self-supervision.", "metadata": {}}, {"text": "Figure 7 analyzes the\npower footprint improvements over a collection of optimization\nsteps for LM: platform-level caching, GPU acceleration, low\nprecision format on accelerator , and model optimization.", "metadata": {}}, {"text": "In\naggregate the optimizations reduce the infrastructure resources\nrequired to serve LM at scale by over 800 ×.", "metadata": {}}, {"text": "We outline the\noptimization beneﬁts from each area below.", "metadata": {}}, {"text": "• Platform-Level Caching.", "metadata": {}}, {"text": "Starting with a CPU server\nbaseline, application-level caching improves power efﬁ-\nciency by 6.7 ×.", "metadata": {}}, {"text": "These improvements are a result of pre-\ncomputing and caching frequently accessed embeddings\nfor language translation tasks.", "metadata": {}}, {"text": "Using DRAM and Flash\nstorage devices as caches, these pre-computed embeddings\ncan be shared across applications and use cases.", "metadata": {}}, {"text": "• GPU acceleration.", "metadata": {}}, {"text": "In addition to caching, deploying LM\nacross GPU-based specialized AI hardware unlocks an\nadditional 10.1× energy efﬁciency improvement.", "metadata": {}}, {"text": "• Algorithmic optimization.", "metadata": {}}, {"text": "Finally, algorithmic optimiza-\ntions provide an additional 12 × energy efﬁciency re-\nduction.", "metadata": {}}, {"text": "Halving precision (e.g., going from 32-bit to\n16-bit operations) provides a 2.4 × energy efﬁciency\nimprovement on GPUs.", "metadata": {}}, {"text": "Another 5× energy efﬁciency gain\ncan be achieved by using custom operators to schedule\nencoding steps within a single kernel of the Transformer\nmodule, such as [32].", "metadata": {}}, {"text": "Optimizing the Carbon Footprint of RMs: The LM\nanalysis is used as an example to highlight the optimiza-\ntion opportunities available with judicious cross-stack, hard-\nware/software optimization.", "metadata": {}}, {"text": "In addition to optimizing the\ncarbon footprint for the language translation task, we describe\nadditional optimization techniques tailored for ranking and\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Performance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\n0.80.911.11.21.3\nYr1-H1Yr1-H2Yr2-H1Yr2-H2\nOperational Power Footprint\nBaselineOptimized (Section 3.2)\n28.5% improvement \nFig. 8. The iterative optimization process has led to 28.5% operational energy\nfootprint reduction over the two-year time period (Section III-B). Despite the\nsigniﬁcant operational power footprint reduction, we continue to see the overall\nelectricity demand for AI to increase over time — an example of Jevon’s\nParadox, where efﬁciency improvement stimulates additional novel AI use\ncases.\nrecommendation use cases.\nA major infrastructure challenge faced by deep learning\nRM training and deployment ( RM1 – RM5) is the fast-rising\nmemory capacity and bandwidth demands (Figure 2). There are\ntwo primary sub-nets in a RM: the dense fully-connected (FC)\nnetwork and the sparse embedding-based network. The FC\nnetwork is constructed with multi-layer perceptions (MLPs),\nthus computationally-intensive. The embedding network is used\nto project hundreds of sparse, high-dimensional features to low-\ndimension vectors. It can easily contribute to over 95% of the\ntotal model size. For a number of important recommendation\nand ranking use cases, the embedding operation dominates the\ninference execution time [27], [33].\nTo tackle the signiﬁcant memory capacity and bandwidth\nrequirement, we deploy model quantization for RMs [ 34].\nQuantization offers two primary efﬁciency beneﬁts: the low-\nprecision data representation reduces the amount of compu-\ntation requirement and, at the same time, lowers the overall\nmemory capacity need. By converting 32-bit ﬂoating-point\nnumerical representation to 16-bit, we can reduce the overall\nRM2 model size by 15%. This has led to 20.7% reduction in\nmemory bandwidth consumption. Furthermore, the memory\ncapacity reduction enabled by quantization unblocks novel\nsystems with lower on-chip memory. For example, for RM1,\nquantization has enabled RM deployment on highly power-\nefﬁcient systems with smaller on-chip memory, leading to an\nend-to-end inference latency improvement of 2.5 times.\nC. Machine Learning Infrastructures at Scale\nML Accelerators: GPUs are the de-facto training acceler-\nators at Facebook, contributing to signiﬁcant power capacity\ninvestment in the context of Facebook’s ﬂeet of datacenters.\nHowever, GPUs can be severely under-utilized during both the\nML Experimentation and Training phases (Figure 10) [ 35]. To\namortize the upfront embodied carbon cost of every accelerator\ndeployed into Facebook’s datacenters, maximizing accelerator\nutilization is a must.\nEfﬁciency of Scale: The higher throughput performance\ndensity achieved with ML accelerators reduces the total number\nof processors deployed into datacenter racks. This leads to\nmore effective amortization of shared infrastructure overheads.\nFurthermore, datacenter capacity is not only limited by physical\nspace but also power capacity — higher operational power\nefﬁciency directly reduces the inherited carbon cost from\nmanufacturing of IT infrastructures and datacenter buildings.\nAt-Scale Efﬁciency Optimization for Facebook Data\nCenters: Servers in Facebook data center ﬂeets are customized\nfor internal workloads only — machine learning tasks [ 24]\nor not [ 36], [ 37]. Compared to public cloud providers, this\nputs Facebook at a unique position for at-scale resource man-\nagement design and optimization. First, Facebookcustomizes\nserver SKUs — compute, memcached, storage tiers and ML\naccelerators — to maximize performance and power efﬁciency.\nAchieving a Power Usage Effectiveness (PUE) of about 1.10,\nFacebook’s data centers are about 40% more efﬁcient than\nsmall-scale, typical data centers.\nFurthermore, the large-scale deployment of servers of\ndifferent types provides an opportunity to build performance\nmeasurement and optimization tools to ensure high utilization of\nthe underlying infrastructure. For data center ﬂeets in different\ngeographical regions where the actual server utilization exhibits\na diurnal pattern, Auto-Scaling frees the over-provisioned\ncapacity during off-peak hours, by up to 25% of the web\ntier’s machines [ 38]. By doing so, it provides opportunistic\nserver capacity for others to use, including ofﬂine ML training.\nFurthermore, static power consumption plays a non-trivial role\nin the context of the overall data center electricity footprint.\nThis motivates more effective processor idle state management.\nCarbon-Free Energy: Finally, over the past years, Face-\nbookhas invested in carbon free energy sources to neutralize its\noperational carbon footprint [ 23]. Reaching net zero emissions\nentails matching every unit of energy consumed by data\ncenters with 100% renewable energy purchased by Facebook.\nRemaining emissions are offset with various sustainability\nprograms, further reducing the operational carbon footprint of\nAI computing at Facebook. As Section IV-C will later show,\nmore can be done .\nD. Going Beyond Efﬁciency Optimization\nDespite the opportunities for optimizing energy efﬁciency\nand reducing environmental footprint at scale, there are many\nreasons why we must care about scaling AI in a more\nenvironmentally-sustainable manner. AI growth is multiplicative\nbeyond current industrial use cases. Although domain-speciﬁc\narchitectures improve the operational energy footprint of AI\nmodel training by more than 90% [ 21], these architectures\nrequire more system resources, leading to larger embodied\ncarbon footprints.\nWhile shifting model training and inference to data centers\nwith carbon-free energy sources can reduce emissions, the\nsolution may not scale to all AI use cases. Infrastructure for\ncarbon free energy is limited by rare metals and materials,\nand takes signiﬁcant economic resources and time to build.\nFurthermore, the carbon footprint of federated learning and\noptimization use cases at the edge is estimated to be similar to\nthat of training a Transformer Big model (Figure 11). As on-\ndevice learning becomes more ubiquitously adopted to improve\n6", "sentences": [{"text": "Performance-per-Watt[Domain-Specific Acceleration]\nUtilization[At-Scale Data Center Optimization;Low-Precision Hardware]\n0.80.911.11.21.3\nYr1-H1Yr1-H2Yr2-H1Yr2-H2\nOperational Power Footprint\nBaselineOptimized (Section 3.2)\n28.5% improvement \nFig.", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "The iterative optimization process has led to 28.5% operational energy\nfootprint reduction over the two-year time period (Section III-B).", "metadata": {}}, {"text": "Despite the\nsigniﬁcant operational power footprint reduction, we continue to see the overall\nelectricity demand for AI to increase over time — an example of Jevon’s\nParadox, where efﬁciency improvement stimulates additional novel AI use\ncases.", "metadata": {}}, {"text": "recommendation use cases.", "metadata": {}}, {"text": "A major infrastructure challenge faced by deep learning\nRM training and deployment ( RM1 – RM5) is the fast-rising\nmemory capacity and bandwidth demands (Figure 2).", "metadata": {}}, {"text": "There are\ntwo primary sub-nets in a RM: the dense fully-connected (FC)\nnetwork and the sparse embedding-based network.", "metadata": {}}, {"text": "The FC\nnetwork is constructed with multi-layer perceptions (MLPs),\nthus computationally-intensive.", "metadata": {}}, {"text": "The embedding network is used\nto project hundreds of sparse, high-dimensional features to low-\ndimension vectors.", "metadata": {}}, {"text": "It can easily contribute to over 95% of the\ntotal model size.", "metadata": {}}, {"text": "For a number of important recommendation\nand ranking use cases, the embedding operation dominates the\ninference execution time [27], [33].", "metadata": {}}, {"text": "To tackle the signiﬁcant memory capacity and bandwidth\nrequirement, we deploy model quantization for RMs [ 34].", "metadata": {}}, {"text": "Quantization offers two primary efﬁciency beneﬁts: the low-\nprecision data representation reduces the amount of compu-\ntation requirement and, at the same time, lowers the overall\nmemory capacity need.", "metadata": {}}, {"text": "By converting 32-bit ﬂoating-point\nnumerical representation to 16-bit, we can reduce the overall\nRM2 model size by 15%.", "metadata": {}}, {"text": "This has led to 20.7% reduction in\nmemory bandwidth consumption.", "metadata": {}}, {"text": "Furthermore, the memory\ncapacity reduction enabled by quantization unblocks novel\nsystems with lower on-chip memory.", "metadata": {}}, {"text": "For example, for RM1,\nquantization has enabled RM deployment on highly power-\nefﬁcient systems with smaller on-chip memory, leading to an\nend-to-end inference latency improvement of 2.5 times.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Machine Learning Infrastructures at Scale\nML Accelerators: GPUs are the de-facto training acceler-\nators at Facebook, contributing to signiﬁcant power capacity\ninvestment in the context of Facebook’s ﬂeet of datacenters.", "metadata": {}}, {"text": "However, GPUs can be severely under-utilized during both the\nML Experimentation and Training phases (Figure 10) [ 35].", "metadata": {}}, {"text": "To\namortize the upfront embodied carbon cost of every accelerator\ndeployed into Facebook’s datacenters, maximizing accelerator\nutilization is a must.", "metadata": {}}, {"text": "Efﬁciency of Scale: The higher throughput performance\ndensity achieved with ML accelerators reduces the total number\nof processors deployed into datacenter racks.", "metadata": {}}, {"text": "This leads to\nmore effective amortization of shared infrastructure overheads.", "metadata": {}}, {"text": "Furthermore, datacenter capacity is not only limited by physical\nspace but also power capacity — higher operational power\nefﬁciency directly reduces the inherited carbon cost from\nmanufacturing of IT infrastructures and datacenter buildings.", "metadata": {}}, {"text": "At-Scale Efﬁciency Optimization for Facebook Data\nCenters: Servers in Facebook data center ﬂeets are customized\nfor internal workloads only — machine learning tasks [ 24]\nor not [ 36], [ 37].", "metadata": {}}, {"text": "Compared to public cloud providers, this\nputs Facebook at a unique position for at-scale resource man-\nagement design and optimization.", "metadata": {}}, {"text": "First, Facebookcustomizes\nserver SKUs — compute, memcached, storage tiers and ML\naccelerators — to maximize performance and power efﬁciency.", "metadata": {}}, {"text": "Achieving a Power Usage Effectiveness (PUE) of about 1.10,\nFacebook’s data centers are about 40% more efﬁcient than\nsmall-scale, typical data centers.", "metadata": {}}, {"text": "Furthermore, the large-scale deployment of servers of\ndifferent types provides an opportunity to build performance\nmeasurement and optimization tools to ensure high utilization of\nthe underlying infrastructure.", "metadata": {}}, {"text": "For data center ﬂeets in different\ngeographical regions where the actual server utilization exhibits\na diurnal pattern, Auto-Scaling frees the over-provisioned\ncapacity during off-peak hours, by up to 25% of the web\ntier’s machines [ 38].", "metadata": {}}, {"text": "By doing so, it provides opportunistic\nserver capacity for others to use, including ofﬂine ML training.", "metadata": {}}, {"text": "Furthermore, static power consumption plays a non-trivial role\nin the context of the overall data center electricity footprint.", "metadata": {}}, {"text": "This motivates more effective processor idle state management.", "metadata": {}}, {"text": "Carbon-Free Energy: Finally, over the past years, Face-\nbookhas invested in carbon free energy sources to neutralize its\noperational carbon footprint [ 23].", "metadata": {}}, {"text": "Reaching net zero emissions\nentails matching every unit of energy consumed by data\ncenters with 100% renewable energy purchased by Facebook.", "metadata": {}}, {"text": "Remaining emissions are offset with various sustainability\nprograms, further reducing the operational carbon footprint of\nAI computing at Facebook.", "metadata": {}}, {"text": "As Section IV-C will later show,\nmore can be done .", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Going Beyond Efﬁciency Optimization\nDespite the opportunities for optimizing energy efﬁciency\nand reducing environmental footprint at scale, there are many\nreasons why we must care about scaling AI in a more\nenvironmentally-sustainable manner.", "metadata": {}}, {"text": "AI growth is multiplicative\nbeyond current industrial use cases.", "metadata": {}}, {"text": "Although domain-speciﬁc\narchitectures improve the operational energy footprint of AI\nmodel training by more than 90% [ 21], these architectures\nrequire more system resources, leading to larger embodied\ncarbon footprints.", "metadata": {}}, {"text": "While shifting model training and inference to data centers\nwith carbon-free energy sources can reduce emissions, the\nsolution may not scale to all AI use cases.", "metadata": {}}, {"text": "Infrastructure for\ncarbon free energy is limited by rare metals and materials,\nand takes signiﬁcant economic resources and time to build.", "metadata": {}}, {"text": "Furthermore, the carbon footprint of federated learning and\noptimization use cases at the edge is estimated to be similar to\nthat of training a Transformer Big model (Figure 11).", "metadata": {}}, {"text": "As on-\ndevice learning becomes more ubiquitously adopted to improve\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "02468\n20%30%50%75%\nLM Carbon Footprint(Normalized to 75%)\nGPU Utilization\nEmbodiedOperational (rest)Operational (offset w/ solar)\nFig. 9. As accelerator utilization improves over time, both operational and\nembodied carbon footprints of AI improve. Carbon-free energy helps reduce\nthe operational carbon footprint, making embodied carbon cost the dominating\nfactor. To reduce the rising carbon footprint of AI computing at-scale, we must\ncomplement efﬁciency and utilization optimization with novel approaches to\nreduce the remaining embodied carbon footprint of AI systems.\ndata privacy, we expect to see more computation being shifted\naway from data centers to the edge, where access to renewable\nenergy may be limited. The edge-cloud space for AI poses\ninteresting design opportunities (Section IV-C).\nThe growth of AI in all dimensions outpaces the efﬁciency im-\nprovement at-scale. Figure 9 illustrates that, as GPU utilization\nis improved (x-axis) for LM training on GPUs, both embodied\nand operational carbon emissions will reduce. Increasing GPU\nutilization up to 80%, the overall carbon footprint decreases\nby 3×. Powering AI services with renewable energy sources\ncan further reduce the overall carbon footprint by a factor of 2.\nEmbodied carbon cost becomes the dominating source of AI’s\noverall carbon footprint. To curb the rising carbon footprint\nof AI computing at-scale (Figure 8 and Figure 9), we must\nlook beyond efﬁciency optimization and complement efﬁciency\nand utilization optimization with efforts to tackle the remaining\nembodied carbon footprint of AI systems.\nIV. A S USTAINABILITY MINDSET FOR AI\nTo tackle the environmental implications of AI’s exponential\ngrowth (Figure 2), the ﬁrst key step requires ML practitioners\nand researchers to develop and adopt an sustainability mindset.\nThe solution space is wide open—while there are signiﬁcant\nefforts looking at AI system and infrastructure efﬁciency opti-\nmization, the AI data, experimentation, and training algorithm\nefﬁciency space (Sections IV-A and IV-B) beyond system\ndesign and optimization (Section IV-C) is less well explored.\nWe cannot optimize what cannot be measured — telemetry to\ntrack the carbon footprint of AI technologies must be adopted\nby the community (Section V-A). We synthesize a number of\nimportant directions to scale AI in a sustainable manner and to\nminimize the environmental impact of AI for the next decades.\nThe ﬁeld of AI is currently primarily driven by research that\nseeks to maximize model accuracy — progress is often used\nsynonymously with improved prediction quality. This endless\npursuit of higher accuracy over the decade of AI research has\nsigniﬁcant implications in computational resource requirement\nand environmental footprint. To develop AI technologies\nresponsibly, we must achieve competitive model accuracy at a\nﬁxed or even reduced computational and environmental cost .\nDespite the recent calls-to-action [28], [39], [40], [41], [21], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI. We\nconjecture the factors that may have contributed to the current\nstate in Appendix A. To bend the exponential growth curve\nof AI and its environmental footprint, we must build a future\nwhere efﬁciency is an evaluation criterion for publishing ML\nresearch on computationally-intensive models beyond accuracy-\nrelated measures.\nA. Data Utilization Efﬁciency\nData Scaling and Sampling: No data is like more data\n— data scaling is the de-facto approach to increase model\nquality, where the primary factor for accuracy improvement\nis driven by the size and quality of training data, instead of\nalgorithmic optimization. However, data scaling has signiﬁcant\nenvironmental footprint implications. To keep the model\ntraining time manageable, overall system resources must be\nscaled with the increase in the data set size, resulting in larger\nembodied carbon footprint and operational carbon footprint\nfrom the data storage and ingestion pipeline and model training.\nAlternatively, if training system resources are kept ﬁxed, data\nscaling increases training time, resulting in a larger operational\nenergy footprint.\nWhen designed well, however, data scaling, sampling and\nselection strategies can improve the competitive analysis for ML\nalgorithms, reducing the environmental footprint of the process\n(Appendix A). For instance, Sachdeva et al. demonstrated that\nintelligent data sampling with merely 10% of data sub-samples\ncan effectively preserve the relative ranking performance\nof different recommendation algorithms [ 42]. This ranking\nperformance is achieved with an average of 5.8 times execution\ntime speedup, leading to signiﬁcant operating carbon footprint\nreduction.\nData Perishability: Understanding key characteristics of\ndata is fundamental to efﬁcient data utilization for AI applica-\ntions. Not all data is created equal and data collected over time\nloses its predictive value gradually. Understanding the rate at\nwhich data loses its predictive value has strong implications on\nthe resulting carbon footprint. For example, natural language\ndata sets can lose half of their predictive value in the time\nperiod of less than 7 years (the half-life time of data) [ 43]. The\nexact half-life period is a function of context. If we were able\nto predict the half-life time of data, we can devise effective\nsampling strategies to subset data at different rates based on\nits half-life. By doing so, the resource requirement for the data\nstorage and ingestion pipeline can be signiﬁcantly reduced [ 44]\n— lower training time (operational carbon footprint) as well as\nstorage needs (embodied carbon footprint).\nB. Experimentation and Training Efﬁciency\nThe experimentation and training phases are closely coupled\n(Section II). There is a natural trade-off between the investment\nin experimentation and the subsequent training cost (Section III).\nNeural architecture search (NAS) and hyperparameter op-\ntimization (HPO) are techniques that automate the design\nspace exploration. Despite their capability to discover higher-\nperforming neural networks, NAS and HPO can be extremely\n7", "sentences": [{"text": "02468\n20%30%50%75%\nLM Carbon Footprint(Normalized to 75%)\nGPU Utilization\nEmbodiedOperational (rest)Operational (offset w/ solar)\nFig.", "metadata": {}}, {"text": "9.", "metadata": {}}, {"text": "As accelerator utilization improves over time, both operational and\nembodied carbon footprints of AI improve.", "metadata": {}}, {"text": "Carbon-free energy helps reduce\nthe operational carbon footprint, making embodied carbon cost the dominating\nfactor.", "metadata": {}}, {"text": "To reduce the rising carbon footprint of AI computing at-scale, we must\ncomplement efﬁciency and utilization optimization with novel approaches to\nreduce the remaining embodied carbon footprint of AI systems.", "metadata": {}}, {"text": "data privacy, we expect to see more computation being shifted\naway from data centers to the edge, where access to renewable\nenergy may be limited.", "metadata": {}}, {"text": "The edge-cloud space for AI poses\ninteresting design opportunities (Section IV-C).", "metadata": {}}, {"text": "The growth of AI in all dimensions outpaces the efﬁciency im-\nprovement at-scale.", "metadata": {}}, {"text": "Figure 9 illustrates that, as GPU utilization\nis improved (x-axis) for LM training on GPUs, both embodied\nand operational carbon emissions will reduce.", "metadata": {}}, {"text": "Increasing GPU\nutilization up to 80%, the overall carbon footprint decreases\nby 3×.", "metadata": {}}, {"text": "Powering AI services with renewable energy sources\ncan further reduce the overall carbon footprint by a factor of 2.", "metadata": {}}, {"text": "Embodied carbon cost becomes the dominating source of AI’s\noverall carbon footprint.", "metadata": {}}, {"text": "To curb the rising carbon footprint\nof AI computing at-scale (Figure 8 and Figure 9), we must\nlook beyond efﬁciency optimization and complement efﬁciency\nand utilization optimization with efforts to tackle the remaining\nembodied carbon footprint of AI systems.", "metadata": {}}, {"text": "IV.", "metadata": {}}, {"text": "A S USTAINABILITY MINDSET FOR AI\nTo tackle the environmental implications of AI’s exponential\ngrowth (Figure 2), the ﬁrst key step requires ML practitioners\nand researchers to develop and adopt an sustainability mindset.", "metadata": {}}, {"text": "The solution space is wide open—while there are signiﬁcant\nefforts looking at AI system and infrastructure efﬁciency opti-\nmization, the AI data, experimentation, and training algorithm\nefﬁciency space (Sections IV-A and IV-B) beyond system\ndesign and optimization (Section IV-C) is less well explored.", "metadata": {}}, {"text": "We cannot optimize what cannot be measured — telemetry to\ntrack the carbon footprint of AI technologies must be adopted\nby the community (Section V-A).", "metadata": {}}, {"text": "We synthesize a number of\nimportant directions to scale AI in a sustainable manner and to\nminimize the environmental impact of AI for the next decades.", "metadata": {}}, {"text": "The ﬁeld of AI is currently primarily driven by research that\nseeks to maximize model accuracy — progress is often used\nsynonymously with improved prediction quality.", "metadata": {}}, {"text": "This endless\npursuit of higher accuracy over the decade of AI research has\nsigniﬁcant implications in computational resource requirement\nand environmental footprint.", "metadata": {}}, {"text": "To develop AI technologies\nresponsibly, we must achieve competitive model accuracy at a\nﬁxed or even reduced computational and environmental cost .", "metadata": {}}, {"text": "Despite the recent calls-to-action [28], [39], [40], [41], [21], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI.", "metadata": {}}, {"text": "We\nconjecture the factors that may have contributed to the current\nstate in Appendix A.", "metadata": {}}, {"text": "To bend the exponential growth curve\nof AI and its environmental footprint, we must build a future\nwhere efﬁciency is an evaluation criterion for publishing ML\nresearch on computationally-intensive models beyond accuracy-\nrelated measures.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Data Utilization Efﬁciency\nData Scaling and Sampling: No data is like more data\n— data scaling is the de-facto approach to increase model\nquality, where the primary factor for accuracy improvement\nis driven by the size and quality of training data, instead of\nalgorithmic optimization.", "metadata": {}}, {"text": "However, data scaling has signiﬁcant\nenvironmental footprint implications.", "metadata": {}}, {"text": "To keep the model\ntraining time manageable, overall system resources must be\nscaled with the increase in the data set size, resulting in larger\nembodied carbon footprint and operational carbon footprint\nfrom the data storage and ingestion pipeline and model training.", "metadata": {}}, {"text": "Alternatively, if training system resources are kept ﬁxed, data\nscaling increases training time, resulting in a larger operational\nenergy footprint.", "metadata": {}}, {"text": "When designed well, however, data scaling, sampling and\nselection strategies can improve the competitive analysis for ML\nalgorithms, reducing the environmental footprint of the process\n(Appendix A).", "metadata": {}}, {"text": "For instance, Sachdeva et al.", "metadata": {}}, {"text": "demonstrated that\nintelligent data sampling with merely 10% of data sub-samples\ncan effectively preserve the relative ranking performance\nof different recommendation algorithms [ 42].", "metadata": {}}, {"text": "This ranking\nperformance is achieved with an average of 5.8 times execution\ntime speedup, leading to signiﬁcant operating carbon footprint\nreduction.", "metadata": {}}, {"text": "Data Perishability: Understanding key characteristics of\ndata is fundamental to efﬁcient data utilization for AI applica-\ntions.", "metadata": {}}, {"text": "Not all data is created equal and data collected over time\nloses its predictive value gradually.", "metadata": {}}, {"text": "Understanding the rate at\nwhich data loses its predictive value has strong implications on\nthe resulting carbon footprint.", "metadata": {}}, {"text": "For example, natural language\ndata sets can lose half of their predictive value in the time\nperiod of less than 7 years (the half-life time of data) [ 43].", "metadata": {}}, {"text": "The\nexact half-life period is a function of context.", "metadata": {}}, {"text": "If we were able\nto predict the half-life time of data, we can devise effective\nsampling strategies to subset data at different rates based on\nits half-life.", "metadata": {}}, {"text": "By doing so, the resource requirement for the data\nstorage and ingestion pipeline can be signiﬁcantly reduced [ 44]\n— lower training time (operational carbon footprint) as well as\nstorage needs (embodied carbon footprint).", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Experimentation and Training Efﬁciency\nThe experimentation and training phases are closely coupled\n(Section II).", "metadata": {}}, {"text": "There is a natural trade-off between the investment\nin experimentation and the subsequent training cost (Section III).", "metadata": {}}, {"text": "Neural architecture search (NAS) and hyperparameter op-\ntimization (HPO) are techniques that automate the design\nspace exploration.", "metadata": {}}, {"text": "Despite their capability to discover higher-\nperforming neural networks, NAS and HPO can be extremely\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "resource-intensive, involving training many models, especially\nwhen using simple approaches. Strubell et al. show that grid-\nsearch NAS can incur over 3000× environmental footprint\noverhead [28]. Utilizing much more sample-efﬁcient NAS and\nHPO methods [ 45], [ 46] can translate directly into carbon\nfootprint improvement. In addition to reducing the number of\ntraining experiments, one can also reduce the training time of\neach experiment. By detecting and stopping under-performing\ntraining workﬂows early , unnecessary training cycles can be\neliminated.\nMulti-objective optimization explores the Pareto frontier of\nefﬁcient model quality and system resource trade-offs. If used\nearly in the model exploration process, it enables more informed\ndecisions about which model to train fully and deploy given\ncertain infrastructure capacity. Beyond model accuracy and\ntiming performance [ 47], [48], [49], [50], energy and carbon\nfootprint can be directly incorporated into the cost function as\noptimization objectives to enable discovery of environmentally-\nfriendly models. Furthermore, when training is decoupled from\nNAS, sub-networks tailoring to specialized system hardware\ncan be selected without additional training [51], [52], [53], [54].\nSuch approaches can signiﬁcantly reduce the overall training\ntime, however, at the expense of increased embodied carbon\nfootprint.\nDeveloping resource-efﬁcient model architectures funda-\nmentally reduce the overall system capacity need of ML\ntasks. From the systems perspective, accelerator memory\nis scarce. However, DNNs, such as neural recommendation\nmodels, require signiﬁcantly higher memory capacity and\nbandwidth [ 55], [ 33]. This motivates researchers to develop\nmemory-efﬁcient model architectures. For example, the Tensor-\nTrain compression technique (TT-Rec) achieves more than\n100× memory capacity reduction with negligible training time\nand accuracy trade-off [ 56]. Similarly, the design space trade-\noff between memory capacity requirement, training time, and\nmodel accuracy is also explored in Deep Hash Embedding\n(DHE) [ 57]. While training time increases lead to higher\noperational carbon footprint, in the case of TT-Rec and DHE,\nthe memory-efﬁcient model architectures require signiﬁcantly\nlower memory capacity while better utilizing the computational\ncapability of training accelerators, resulting in lower embodied\ncarbon footprint.\nDeveloping efﬁcient training algorithms is a long-time\nobjective of research in optimization and numerical meth-\nods [58]. Evaluations of optimization methods should account\nfor all experimentation efforts required to tune optimizer\nhyperparameters, not just the method performance after tun-\ning [ 59], [ 60]. In addition, signiﬁcant research has gone\ninto algorithmic approaches to efﬁciently scale training [ 61],\n[62] by reducing communication cost via compression [ 63],\n[64], pipelining [ 65], and sharding [ 66], [67]. The advances\nhave enabled efﬁcient scaling to larger models and larger\ndatasets. We expect efﬁcient training methods to continue\nas an important domain. While this paper has focused on\nsupervised learning relying labeled data, algorithmic efﬁciency\nextends to other learning paradigms including self-supervised\nand semi-supervised learning (Appendix C).\nProbability           \nGPU Utilization           \nFig. 10. A vast majority of model experimentation (over tens of thousands of\ntraining workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization\nand efﬁciency improvements.\nC. Efﬁcient, Environmentally-Sustainable AI Infrastructure and\nSystem Hardware\nTo amortize the embodied carbon footprint, model developers\nand system architects must maximize the utilization of acceler-\nator and system resources when in use and prolong the lifetime\nof AI infrastructures . Existing practices such as the move to\ndomain-speciﬁc architectures at cloud scale [ 68], [ 69], [ 70]\nreduce AI computing’s footprint by consolidating computing\nresources at scale and by operating the shared infrastructures\nmore environmentally-friendly with carbon free energy 6.\nAccelerator Virtualization and Multi-Tenancy Support:\nFigure 10 illustrates the utilization of GPU accelerators in Face-\nbook’s research training infrastructure. A signiﬁcant portion\nof machine learning model experimentation utilizes GPUs at\nonly 30-50%, leaving signiﬁcant room for improvements to\nefﬁciency and overall utilization. Virtualization and workload\nconsolidation technologies can help maximize accelerator\nutilization [ 71]. Google’s TPUs have also recently started\nsupporting virtualization [72]. Multi-tenancy for AI accelerators\nis gaining traction as an effective way to improve resource\nutilization, thereby amortizing the upfront embodied carbon\nfootprint of customized system hardware for AI at the expense\nof potential operational carbon footprint increase [ 73], [ 74],\n[75], [76], [77].\nEnvironmental Sustainability as a Key AI System Design\nPrinciple: Today, servers are designed to optimize performance\nand power efﬁciency. However, system design with a focus\non operational energy efﬁciency optimization does not always\nproduce the most environmentally-sustainable solution [ 78],\n[79], [19]. With the rising embodied carbon cost and the expo-\nnential demand growth of AI, system designers and architects\nmust re-think fundamental system hardware design principles\nto minimize computing’s footprint end-to-end, considering the\nentire hardware and ML model development life cycle. In\naddition to the respective performance, power, and cost proﬁles,\nthe environmental footprint characteristics of processors over\nthe generations of CMOS technologies, DDRx and HBM mem-\nory technologies, SSD/NAND-ﬂash/HDD storage technologies\ncan be orders-of-magnitude different [ 80]. Thus, designing AI\n6We discuss additional important directions for building environmentally-\nsustainable systems in Appendix B, including datacenter infrastructure\ndisaggregation; fault tolerant, resilient AI systems.\n8", "sentences": [{"text": "resource-intensive, involving training many models, especially\nwhen using simple approaches.", "metadata": {}}, {"text": "Strubell et al.", "metadata": {}}, {"text": "show that grid-\nsearch NAS can incur over 3000× environmental footprint\noverhead [28].", "metadata": {}}, {"text": "Utilizing much more sample-efﬁcient NAS and\nHPO methods [ 45], [ 46] can translate directly into carbon\nfootprint improvement.", "metadata": {}}, {"text": "In addition to reducing the number of\ntraining experiments, one can also reduce the training time of\neach experiment.", "metadata": {}}, {"text": "By detecting and stopping under-performing\ntraining workﬂows early , unnecessary training cycles can be\neliminated.", "metadata": {}}, {"text": "Multi-objective optimization explores the Pareto frontier of\nefﬁcient model quality and system resource trade-offs.", "metadata": {}}, {"text": "If used\nearly in the model exploration process, it enables more informed\ndecisions about which model to train fully and deploy given\ncertain infrastructure capacity.", "metadata": {}}, {"text": "Beyond model accuracy and\ntiming performance [ 47], [48], [49], [50], energy and carbon\nfootprint can be directly incorporated into the cost function as\noptimization objectives to enable discovery of environmentally-\nfriendly models.", "metadata": {}}, {"text": "Furthermore, when training is decoupled from\nNAS, sub-networks tailoring to specialized system hardware\ncan be selected without additional training [51], [52], [53], [54].", "metadata": {}}, {"text": "Such approaches can signiﬁcantly reduce the overall training\ntime, however, at the expense of increased embodied carbon\nfootprint.", "metadata": {}}, {"text": "Developing resource-efﬁcient model architectures funda-\nmentally reduce the overall system capacity need of ML\ntasks.", "metadata": {}}, {"text": "From the systems perspective, accelerator memory\nis scarce.", "metadata": {}}, {"text": "However, DNNs, such as neural recommendation\nmodels, require signiﬁcantly higher memory capacity and\nbandwidth [ 55], [ 33].", "metadata": {}}, {"text": "This motivates researchers to develop\nmemory-efﬁcient model architectures.", "metadata": {}}, {"text": "For example, the Tensor-\nTrain compression technique (TT-Rec) achieves more than\n100× memory capacity reduction with negligible training time\nand accuracy trade-off [ 56].", "metadata": {}}, {"text": "Similarly, the design space trade-\noff between memory capacity requirement, training time, and\nmodel accuracy is also explored in Deep Hash Embedding\n(DHE) [ 57].", "metadata": {}}, {"text": "While training time increases lead to higher\noperational carbon footprint, in the case of TT-Rec and DHE,\nthe memory-efﬁcient model architectures require signiﬁcantly\nlower memory capacity while better utilizing the computational\ncapability of training accelerators, resulting in lower embodied\ncarbon footprint.", "metadata": {}}, {"text": "Developing efﬁcient training algorithms is a long-time\nobjective of research in optimization and numerical meth-\nods [58].", "metadata": {}}, {"text": "Evaluations of optimization methods should account\nfor all experimentation efforts required to tune optimizer\nhyperparameters, not just the method performance after tun-\ning [ 59], [ 60].", "metadata": {}}, {"text": "In addition, signiﬁcant research has gone\ninto algorithmic approaches to efﬁciently scale training [ 61],\n[62] by reducing communication cost via compression [ 63],\n[64], pipelining [ 65], and sharding [ 66], [67].", "metadata": {}}, {"text": "The advances\nhave enabled efﬁcient scaling to larger models and larger\ndatasets.", "metadata": {}}, {"text": "We expect efﬁcient training methods to continue\nas an important domain.", "metadata": {}}, {"text": "While this paper has focused on\nsupervised learning relying labeled data, algorithmic efﬁciency\nextends to other learning paradigms including self-supervised\nand semi-supervised learning (Appendix C).", "metadata": {}}, {"text": "Probability           \nGPU Utilization           \nFig.", "metadata": {}}, {"text": "10.", "metadata": {}}, {"text": "A vast majority of model experimentation (over tens of thousands of\ntraining workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization\nand efﬁciency improvements.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Efﬁcient, Environmentally-Sustainable AI Infrastructure and\nSystem Hardware\nTo amortize the embodied carbon footprint, model developers\nand system architects must maximize the utilization of acceler-\nator and system resources when in use and prolong the lifetime\nof AI infrastructures .", "metadata": {}}, {"text": "Existing practices such as the move to\ndomain-speciﬁc architectures at cloud scale [ 68], [ 69], [ 70]\nreduce AI computing’s footprint by consolidating computing\nresources at scale and by operating the shared infrastructures\nmore environmentally-friendly with carbon free energy 6.", "metadata": {}}, {"text": "Accelerator Virtualization and Multi-Tenancy Support:\nFigure 10 illustrates the utilization of GPU accelerators in Face-\nbook’s research training infrastructure.", "metadata": {}}, {"text": "A signiﬁcant portion\nof machine learning model experimentation utilizes GPUs at\nonly 30-50%, leaving signiﬁcant room for improvements to\nefﬁciency and overall utilization.", "metadata": {}}, {"text": "Virtualization and workload\nconsolidation technologies can help maximize accelerator\nutilization [ 71].", "metadata": {}}, {"text": "Google’s TPUs have also recently started\nsupporting virtualization [72].", "metadata": {}}, {"text": "Multi-tenancy for AI accelerators\nis gaining traction as an effective way to improve resource\nutilization, thereby amortizing the upfront embodied carbon\nfootprint of customized system hardware for AI at the expense\nof potential operational carbon footprint increase [ 73], [ 74],\n[75], [76], [77].", "metadata": {}}, {"text": "Environmental Sustainability as a Key AI System Design\nPrinciple: Today, servers are designed to optimize performance\nand power efﬁciency.", "metadata": {}}, {"text": "However, system design with a focus\non operational energy efﬁciency optimization does not always\nproduce the most environmentally-sustainable solution [ 78],\n[79], [19].", "metadata": {}}, {"text": "With the rising embodied carbon cost and the expo-\nnential demand growth of AI, system designers and architects\nmust re-think fundamental system hardware design principles\nto minimize computing’s footprint end-to-end, considering the\nentire hardware and ML model development life cycle.", "metadata": {}}, {"text": "In\naddition to the respective performance, power, and cost proﬁles,\nthe environmental footprint characteristics of processors over\nthe generations of CMOS technologies, DDRx and HBM mem-\nory technologies, SSD/NAND-ﬂash/HDD storage technologies\ncan be orders-of-magnitude different [ 80].", "metadata": {}}, {"text": "Thus, designing AI\n6We discuss additional important directions for building environmentally-\nsustainable systems in Appendix B, including datacenter infrastructure\ndisaggregation;", "metadata": {}}, {"text": "fault tolerant, resilient AI systems.", "metadata": {}}, {"text": "8", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im1.jpg] Size: 1794x1046, Data: 195086 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im1.jpg] Size: 1794x1046, Data: 195086 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im1.jpg", "image_width": 1794, "image_height": 1046, "attachment_type": "image", "has_image_data": true, "image_data_size": 195086}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "systems with the least environmental impact requires explicit\nconsideration of environmental footprint characteristics at the\ndesign time.\nThe Implications of General-Purpose Processors,\nGeneral-Purpose Accelerators, Reconﬁgurable Systems,\nand ASICs for AI: There is a wide variety of system\nhardware choices for AI from general-purpose processors\n(CPUs), general-purpose accelerators (GPUs or TPUs), ﬁeld-\nprogrammable gate arrays (FPGAs) [81], to application-speciﬁc\nintegrated circuit (ASIC), such as Eyeriss [ 82]. The exact\nsystem deployment choice can be multifaceted — the cadence\nof ML algorithm and model architecture evolution, the di-\nversity of ML use cases and the respective system resource\nrequirements, and the maturity of the software stack. While ML\naccelerator deployment brings a step-function improvement in\noperational energy efﬁciency , it may not necessarily reduce\nthe carbon footprint of AI computing overall. This is because\nof the upfront embodied carbon footprint associated with the\ndifferent system hardware choices. From the environmental\nsustainability perspective, the optimal point depends on the\ncompounding factor of operational efﬁciency improvement over\ngenerations of ML algorithms/models, deployment lifetime\nand embodied carbon footprint of the system hardware. Thus,\nto design for environmental sustainability, one must strike a\ncareful balance between efﬁciency and ﬂexibility and, at the\nsame time, consider environmental impact as a key design\ndimension for next-generation AI systems.\nCarbon-Efﬁcient Scheduling for AI Computing At-Scale:\nAs the electricity consumption of hyperscale data centers\ncontinues to rise, data center operators have devoted signiﬁcant\ninvestment to neutralize operational carbon footprint. By\noperating large-scale computing infrastructures with carbon\nfree energy, technology companies are taking an important step\nto address the environmental implications of computing. More\ncan be done however .\nAs the renewable energy proportion in the electricity grid\nincreases, ﬂuctuations in energy generation will increase due to\nthe intermittent nature of renewable energy sources (i.e. wind,\nsolar). Elastic carbon-aware workload scheduling techniques\ncan be used in and across datacenters to predict and exploit\nthe intermittent energy generation patterns [ 83]. However such\nscheduling algorithms might require server over-provisioning\nto allow for ﬂexibility of shifting workloads to times when\ncarbon-free energy is available. Furthermore, any additional\nserver capacity comes with manufacturing carbon cost which\nneeds to be incorporated into the design space. Alternatively,\nenergy storage (e.g. batteries, pumped hydro, ﬂywheels, molten\nsalt) can be used to store renewable energy during peak\ngeneration times for use during low generation times. There\nis an interesting design space to achieve 24/7 carbon-free AI\ncomputing.\nOn-Device Learning On-device AI is becoming more\nubiquitously adopted to enable model personalization [ 84],\n[85], [86] while improving data privacy [ 87], [88], [89], [90],\nyet its impact in terms of carbon emission is often overlooked.\nOn-device learning emits non-negligible carbon. Figure 11\nillustrates that the operational carbon footprint for training a\nsmall ML task using federated learning (FL) is comparable to\n00.511.522.533.5\nFL-1\nFL-2\nTPU-Base\nTPU-Green\nP100-Base\nP100-GreenFacebookTransformer-big (non-FL)\nCO2e (kg)Hundreds\nDownloadUploadCompute\"\"\nFig. 11. Federated learning and optimization can result in a non-negligible\namount of carbon emissions, equivalent to the carbon footprint of training\nT ransf ormerBig [21]. FL-1 and FL-2 represent two production FL\napplications. P100-Base represents the carbon footprint of T ransf ormerBig\ntraining on P100 GPU whereas TPU-base is T ransf ormerBig training on\nTPU. P100-Green and TPU-Green consider renewable energy at the cloud\n(Methodology detail in Appendix B).\nthat of training an orders-of-magnitude larger Transformer-\nbased model in a centralized setting. As FL trains local\nmodels on client devices and periodically aggregates the model\nparameters for a global model, without collecting raw user\ndata [87], the FL process can emit non-negligible carbon at the\nedge due to both computation and wireless communication.\nIt is important to reduce AI’s environmental footprint at the\nedge. With the ever-increasing demand for on-device use cases\nover billions of client devices, such as teaching AI to understand\nthe physical environment from the ﬁrst-person perception [ 91]\nor personalizing AI tasks, the carbon footprint for on-device\nAI can add up to a dire amount quickly. Also, renewable\nenergy is far more limited for client devices compared to\ndatacenters. Optimizing the overall energy efﬁciency of FL\nand on-device AI is an important ﬁrst step [ 92], [ 93], [ 94],\n[95], [96]. Reducing embodied carbon cost for edge devices is\nalso important, as manufacturing carbon cost accounts for 74%\nof the total footprint [ 19] of client devices. It is particularly\nchallenging to amortize the embodied carbon footprint because\nclient devices are often under-utilized [97].\nV. C ALL -TO-ACTION\nA. Development of Easy-to-Adopt Telemetry for Assessing AI’s\nEnvironmental Footprint\nWhile the open source community has started building tools\nto enable automatic measurement of AI training’s environmental\nfootprint [39], [40], [98], [99] and the ML research community\nrequiring a broader impact statement for the submitted research\nmanuscript, more can be done in order to incorporate efﬁciency\nand sustainability into the design process. Enabling carbon\naccounting methodologies and telemetry that is easy to adopt\nis an important step to quantify the signiﬁcance of our\nprogress in developing AI technologies in an environmentally-\nresponsible manner. While assessing the novelty and quality\nof ML solutions, it is crucial to consider sustainability metrics\nincluding energy consumption and carbon footprint along with\nmeasures of model quality and system performance.\n9", "sentences": [{"text": "systems with the least environmental impact requires explicit\nconsideration of environmental footprint characteristics at the\ndesign time.", "metadata": {}}, {"text": "The Implications of General-Purpose Processors,\nGeneral-Purpose Accelerators, Reconﬁgurable Systems,\nand ASICs for AI: There is a wide variety of system\nhardware choices for AI from general-purpose processors\n(CPUs), general-purpose accelerators (GPUs or TPUs), ﬁeld-\nprogrammable gate arrays (FPGAs) [81], to application-speciﬁc\nintegrated circuit (ASIC), such as Eyeriss [ 82].", "metadata": {}}, {"text": "The exact\nsystem deployment choice can be multifaceted — the cadence\nof ML algorithm and model architecture evolution, the di-\nversity of ML use cases and the respective system resource\nrequirements, and the maturity of the software stack.", "metadata": {}}, {"text": "While ML\naccelerator deployment brings a step-function improvement in\noperational energy efﬁciency , it may not necessarily reduce\nthe carbon footprint of AI computing overall.", "metadata": {}}, {"text": "This is because\nof the upfront embodied carbon footprint associated with the\ndifferent system hardware choices.", "metadata": {}}, {"text": "From the environmental\nsustainability perspective, the optimal point depends on the\ncompounding factor of operational efﬁciency improvement over\ngenerations of ML algorithms/models, deployment lifetime\nand embodied carbon footprint of the system hardware.", "metadata": {}}, {"text": "Thus,\nto design for environmental sustainability, one must strike a\ncareful balance between efﬁciency and ﬂexibility and, at the\nsame time, consider environmental impact as a key design\ndimension for next-generation AI systems.", "metadata": {}}, {"text": "Carbon-Efﬁcient Scheduling for AI Computing At-Scale:\nAs the electricity consumption of hyperscale data centers\ncontinues to rise, data center operators have devoted signiﬁcant\ninvestment to neutralize operational carbon footprint.", "metadata": {}}, {"text": "By\noperating large-scale computing infrastructures with carbon\nfree energy, technology companies are taking an important step\nto address the environmental implications of computing.", "metadata": {}}, {"text": "More\ncan be done however .", "metadata": {}}, {"text": "As the renewable energy proportion in the electricity grid\nincreases, ﬂuctuations in energy generation will increase due to\nthe intermittent nature of renewable energy sources (i.e.", "metadata": {}}, {"text": "wind,\nsolar).", "metadata": {}}, {"text": "Elastic carbon-aware workload scheduling techniques\ncan be used in and across datacenters to predict and exploit\nthe intermittent energy generation patterns [ 83].", "metadata": {}}, {"text": "However such\nscheduling algorithms might require server over-provisioning\nto allow for ﬂexibility of shifting workloads to times when\ncarbon-free energy is available.", "metadata": {}}, {"text": "Furthermore, any additional\nserver capacity comes with manufacturing carbon cost which\nneeds to be incorporated into the design space.", "metadata": {}}, {"text": "Alternatively,\nenergy storage (e.g.", "metadata": {}}, {"text": "batteries, pumped hydro, ﬂywheels, molten\nsalt) can be used to store renewable energy during peak\ngeneration times for use during low generation times.", "metadata": {}}, {"text": "There\nis an interesting design space to achieve 24/7 carbon-free AI\ncomputing.", "metadata": {}}, {"text": "On-Device Learning On-device AI is becoming more\nubiquitously adopted to enable model personalization [ 84],\n[85], [86] while improving data privacy [ 87], [88], [89], [90],\nyet its impact in terms of carbon emission is often overlooked.", "metadata": {}}, {"text": "On-device learning emits non-negligible carbon.", "metadata": {}}, {"text": "Figure 11\nillustrates that the operational carbon footprint for training a\nsmall ML task using federated learning (FL) is comparable to\n00.511.522.533.5\nFL-1\nFL-2\nTPU-Base\nTPU-Green\nP100-Base\nP100-GreenFacebookTransformer-big (non-FL)\nCO2e (kg)Hundreds\nDownloadUploadCompute\"\"\nFig.", "metadata": {}}, {"text": "11.", "metadata": {}}, {"text": "Federated learning and optimization can result in a non-negligible\namount of carbon emissions, equivalent to the carbon footprint of training\nT ransf ormerBig [21].", "metadata": {}}, {"text": "FL-1 and FL-2 represent two production FL\napplications.", "metadata": {}}, {"text": "P100-Base represents the carbon footprint of T ransf ormerBig\ntraining on P100 GPU whereas TPU-base is T ransf ormerBig training on\nTPU.", "metadata": {}}, {"text": "P100-Green and TPU-Green consider renewable energy at the cloud\n(Methodology detail in Appendix B).", "metadata": {}}, {"text": "that of training an orders-of-magnitude larger Transformer-\nbased model in a centralized setting.", "metadata": {}}, {"text": "As FL trains local\nmodels on client devices and periodically aggregates the model\nparameters for a global model, without collecting raw user\ndata [87], the FL process can emit non-negligible carbon at the\nedge due to both computation and wireless communication.", "metadata": {}}, {"text": "It is important to reduce AI’s environmental footprint at the\nedge.", "metadata": {}}, {"text": "With the ever-increasing demand for on-device use cases\nover billions of client devices, such as teaching AI to understand\nthe physical environment from the ﬁrst-person perception [ 91]\nor personalizing AI tasks, the carbon footprint for on-device\nAI can add up to a dire amount quickly.", "metadata": {}}, {"text": "Also, renewable\nenergy is far more limited for client devices compared to\ndatacenters.", "metadata": {}}, {"text": "Optimizing the overall energy efﬁciency of FL\nand on-device AI is an important ﬁrst step [ 92], [ 93], [ 94],\n[95], [96].", "metadata": {}}, {"text": "Reducing embodied carbon cost for edge devices is\nalso important, as manufacturing carbon cost accounts for 74%\nof the total footprint [ 19] of client devices.", "metadata": {}}, {"text": "It is particularly\nchallenging to amortize the embodied carbon footprint because\nclient devices are often under-utilized [97].", "metadata": {}}, {"text": "V.", "metadata": {}}, {"text": "C ALL -TO-ACTION\nA.", "metadata": {}}, {"text": "Development of Easy-to-Adopt Telemetry for Assessing AI’s\nEnvironmental Footprint\nWhile the open source community has started building tools\nto enable automatic measurement of AI training’s environmental\nfootprint [39], [40], [98], [99] and the ML research community\nrequiring a broader impact statement for the submitted research\nmanuscript, more can be done in order to incorporate efﬁciency\nand sustainability into the design process.", "metadata": {}}, {"text": "Enabling carbon\naccounting methodologies and telemetry that is easy to adopt\nis an important step to quantify the signiﬁcance of our\nprogress in developing AI technologies in an environmentally-\nresponsible manner.", "metadata": {}}, {"text": "While assessing the novelty and quality\nof ML solutions, it is crucial to consider sustainability metrics\nincluding energy consumption and carbon footprint along with\nmeasures of model quality and system performance.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "Metrics for AI Model and System Life Cycles: Standard\ncarbon footprint accounting methods for AI’s overall carbon\nfootprint are at a nascent stage. We need simple, easy-to-\nadopt metrics to make fair and useful comparisons between\nAI innovations. Many different aspects must be accounted\nfor, including the life cycles of both AI models ( Data,\nExperimentation, Training, Deployment) and system hardware\n(Manufacturing and Use) (Section II).\nIn addition to incorporating an efﬁciency measure as part\nof leader boards for various ML tasks, data [ 100], models 7,\ntraining algorithms [ 101], environmental impact must also be\nconsidered and adopted by AI system hardware developers. For\nexample, MLPerf [ 102], [103], [104] is the industry standard\nfor ML system performance comparison. The industry has\nwitnessed signiﬁcantly higher system performance speedup,\noutstripping what is enabled by Moore’s Law [ 105], [ 106].\nMoreover, an algorithm efﬁciency benchmark is under develop-\nment8. The MLPerf benchmark standards can advance the ﬁeld\nof AI in an environmentally-competitive manner by enabling\nthe measurement of energy and/or carbon footprint.\nCarbon Impact Statements and Model Cards: We believe\nit is important for all published research papers to disclose\nthe operational and embodied carbon footprint of proposed\ndesign; we are only at the beginning of this journey 9. Note,\nwhile embodied carbon footprints for AI hardware may not be\nreadily available, describing hardware platforms, the number of\nmachines, total runtime used to produce results presented in a\nresearch manuscript is an important ﬁrst step. In addition, new\nmodels must be associated with a model card that, among other\naspects of data sets and models [ 107], describes the model’s\noverall carbon footprint to train and conduct inference.\nVI. K EY TAKEAWAYS\nThe Growth of AI: Deep learning has witnessed an\nexponential growth in training data, model parameters, and\nsystem resources over the recent years (Figure 2). The amount\nof data for AI has grown by 2.4×, leading to 3.2× increase in\nthe data ingestion bandwidth demand at Facebook. Facebook’s\nrecommendation model sizes have increased by 20× between\n2019 and 2021. The explosive growth in AI use cases has\ndriven 2.9× and 2.5× capacity increases for AI training and\ninference at Facebook over the recent 18 months, respectively.\nThe environmental footprint of AI is staggering (Figure 4,\nFigure 5).\nA Holistic Approach: To ensure an environmentally-\nsustainable growth of AI, we must consider the AI ecosystem\nholistically going forward. We must look at the machine learn-\ning pipelines end-to-end — data collection, model exploration\nand experimentation, model training, optimization and run-\ntime inference (Section II). The frequency of training and\nscale of each stage of the ML pipeline must be considered\nto understand salient bottlenecks to sustainable AI. From the\nsystem’s perspective, the life cycle of model development and\n7Papers with code: https://paperswithcode.com/sota/image-classiﬁcation-on\n-imagenet\n8https://github.com/mlcommons/algorithmic-efﬁciency/\n9https://2021.naacl.org/ethics/faq/#-if-my-paper-reports-on-experiments-t\nhat-involve-lots-of-compute-timepower\nsystem hardware, including manufacturing and operational use,\nmust also be accounted for.\nEfﬁciency Optimization: Optimization across the axes of al-\ngorithms, platforms, infrastructures, hardware can signiﬁcantly\nreduce the operational carbon footprint for the Transformer-\nbased universal translation model by 810×. Along with other\nefﬁciency optimization at-scale, this has translated into 25.8%\noperational energy footprint reduction over the two-year period.\nMore must be done to bend the environmental impact from the\nexponential growth of AI (Figure 8 and Figure 9).\nAn Sustainability Mindset for AI: Optimization beyond\nefﬁciency across the software and hardware stack at scale is\ncrucial to enabling future sustainable AI systems. To develop\nAI technologies responsibly, we must achieve competitive\nmodel accuracy at a ﬁxed or even reduced computational\nand environmental cost. We chart out potentially high-impact\nresearch and development directions across the data, algorithms\nand model, experimentation and system hardware, and telemetry\ndimensions for AI at datacenters and at the edge (Section IV).\nWe must take a deliberate approach when developing\nAI research and technologies, considering the environmental\nimpact of innovations and taking a responsible approach to\ntechnology development [108]. That is, we need AI to be green\nand environmentally-sustainable.\nVII. C ONCLUSION\nThis paper is the ﬁrst effort to explore the environmental\nimpact of the super-linear trends for AI growth from a holistic\nperspective, spanning data, algorithms, and system hardware.\nWe characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nML use cases at Facebook and, at the same time, considering\nthe life cycle of system hardware. Furthermore, we capture\nthe operational and manufacturing carbon footprint of AI\ncomputing and present an end-to-end analysis for what and\nhow hardware-software design and at-scale optimization can\nhelp reduce the overall carbon footprint of AI. We share\nthe key challenges and chart out important directions across\nall dimensions of AI—data, algorithms, systems, metrics,\nstandards, and best experimentation practices. Advancing the\nﬁeld of machine intelligence must not in turn make climate\nchange worse. We must develop AI technologies with a deeper\nunderstanding of the societal and environmental implications.\nACKNOWLEDGEMENT\nWe would like to thank Nikhil Gupta, Lei Tian, Weiyi\nZheng, Manisha Jain, Adnan Aziz, and Adam Lerer for\ntheir feedback on many iterations of this draft, and in-depth\ntechnical discussions around building efﬁcient infrastructure\nand platforms; Adina Williams, Emily Dinan, Mona Diab,\nAshkan Yousefpour for the valuable discussions and insights\non AI and environmental responsibility; Mark Zhou, Niket\nAgarwal, Jongsoo Park, Michael Anderson, Xiaodong Wang;\nYatharth Saraf, Hagay Lupesco, Jigar Desai, Joelle Pineau,\nRam Valliyappan, Rajesh Mosur, Ananth Sankarnarayanan and\nEytan Bakshy for their leadership and vision without which\nthis work would not have been possible.\n10", "sentences": [{"text": "Metrics for AI Model and System Life Cycles: Standard\ncarbon footprint accounting methods for AI’s overall carbon\nfootprint are at a nascent stage.", "metadata": {}}, {"text": "We need simple, easy-to-\nadopt metrics to make fair and useful comparisons between\nAI innovations.", "metadata": {}}, {"text": "Many different aspects must be accounted\nfor, including the life cycles of both AI models ( Data,\nExperimentation, Training, Deployment) and system hardware\n(Manufacturing and Use) (Section II).", "metadata": {}}, {"text": "In addition to incorporating an efﬁciency measure as part\nof leader boards for various ML tasks, data [ 100], models 7,\ntraining algorithms [ 101], environmental impact must also be\nconsidered and adopted by AI system hardware developers.", "metadata": {}}, {"text": "For\nexample, MLPerf [ 102], [103], [104] is the industry standard\nfor ML system performance comparison.", "metadata": {}}, {"text": "The industry has\nwitnessed signiﬁcantly higher system performance speedup,\noutstripping what is enabled by Moore’s Law [ 105], [ 106].", "metadata": {}}, {"text": "Moreover, an algorithm efﬁciency benchmark is under develop-\nment8.", "metadata": {}}, {"text": "The MLPerf benchmark standards can advance the ﬁeld\nof AI in an environmentally-competitive manner by enabling\nthe measurement of energy and/or carbon footprint.", "metadata": {}}, {"text": "Carbon Impact Statements and Model Cards: We believe\nit is important for all published research papers to disclose\nthe operational and embodied carbon footprint of proposed\ndesign;", "metadata": {}}, {"text": "we are only at the beginning of this journey 9.", "metadata": {}}, {"text": "Note,\nwhile embodied carbon footprints for AI hardware may not be\nreadily available, describing hardware platforms, the number of\nmachines, total runtime used to produce results presented in a\nresearch manuscript is an important ﬁrst step.", "metadata": {}}, {"text": "In addition, new\nmodels must be associated with a model card that, among other\naspects of data sets and models [ 107], describes the model’s\noverall carbon footprint to train and conduct inference.", "metadata": {}}, {"text": "VI.", "metadata": {}}, {"text": "K EY TAKEAWAYS\nThe Growth of AI: Deep learning has witnessed an\nexponential growth in training data, model parameters, and\nsystem resources over the recent years (Figure 2).", "metadata": {}}, {"text": "The amount\nof data for AI has grown by 2.4×, leading to 3.2× increase in\nthe data ingestion bandwidth demand at Facebook.", "metadata": {}}, {"text": "Facebook’s\nrecommendation model sizes have increased by 20× between\n2019 and 2021.", "metadata": {}}, {"text": "The explosive growth in AI use cases has\ndriven 2.9× and 2.5× capacity increases for AI training and\ninference at Facebook over the recent 18 months, respectively.", "metadata": {}}, {"text": "The environmental footprint of AI is staggering (Figure 4,\nFigure 5).", "metadata": {}}, {"text": "A Holistic Approach: To ensure an environmentally-\nsustainable growth of AI, we must consider the AI ecosystem\nholistically going forward.", "metadata": {}}, {"text": "We must look at the machine learn-\ning pipelines end-to-end — data collection, model exploration\nand experimentation, model training, optimization and run-\ntime inference (Section II).", "metadata": {}}, {"text": "The frequency of training and\nscale of each stage of the ML pipeline must be considered\nto understand salient bottlenecks to sustainable AI.", "metadata": {}}, {"text": "From the\nsystem’s perspective, the life cycle of model development and\n7Papers with code: https://paperswithcode.com/sota/image-classiﬁcation-on\n-imagenet\n8https://github.com/mlcommons/algorithmic-efﬁciency/\n9https://2021.naacl.org/ethics/faq/#-if-my-paper-reports-on-experiments-t\nhat-involve-lots-of-compute-timepower\nsystem hardware, including manufacturing and operational use,\nmust also be accounted for.", "metadata": {}}, {"text": "Efﬁciency Optimization: Optimization across the axes of al-\ngorithms, platforms, infrastructures, hardware can signiﬁcantly\nreduce the operational carbon footprint for the Transformer-\nbased universal translation model by 810×.", "metadata": {}}, {"text": "Along with other\nefﬁciency optimization at-scale, this has translated into 25.8%\noperational energy footprint reduction over the two-year period.", "metadata": {}}, {"text": "More must be done to bend the environmental impact from the\nexponential growth of AI (Figure 8 and Figure 9).", "metadata": {}}, {"text": "An Sustainability Mindset for AI: Optimization beyond\nefﬁciency across the software and hardware stack at scale is\ncrucial to enabling future sustainable AI systems.", "metadata": {}}, {"text": "To develop\nAI technologies responsibly, we must achieve competitive\nmodel accuracy at a ﬁxed or even reduced computational\nand environmental cost.", "metadata": {}}, {"text": "We chart out potentially high-impact\nresearch and development directions across the data, algorithms\nand model, experimentation and system hardware, and telemetry\ndimensions for AI at datacenters and at the edge (Section IV).", "metadata": {}}, {"text": "We must take a deliberate approach when developing\nAI research and technologies, considering the environmental\nimpact of innovations and taking a responsible approach to\ntechnology development [108].", "metadata": {}}, {"text": "That is, we need AI to be green\nand environmentally-sustainable.", "metadata": {}}, {"text": "VII.", "metadata": {}}, {"text": "C ONCLUSION\nThis paper is the ﬁrst effort to explore the environmental\nimpact of the super-linear trends for AI growth from a holistic\nperspective, spanning data, algorithms, and system hardware.", "metadata": {}}, {"text": "We characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale\nML use cases at Facebook and, at the same time, considering\nthe life cycle of system hardware.", "metadata": {}}, {"text": "Furthermore, we capture\nthe operational and manufacturing carbon footprint of AI\ncomputing and present an end-to-end analysis for what and\nhow hardware-software design and at-scale optimization can\nhelp reduce the overall carbon footprint of AI.", "metadata": {}}, {"text": "We share\nthe key challenges and chart out important directions across\nall dimensions of AI—data, algorithms, systems, metrics,\nstandards, and best experimentation practices.", "metadata": {}}, {"text": "Advancing the\nﬁeld of machine intelligence must not in turn make climate\nchange worse.", "metadata": {}}, {"text": "We must develop AI technologies with a deeper\nunderstanding of the societal and environmental implications.", "metadata": {}}, {"text": "ACKNOWLEDGEMENT\nWe would like to thank Nikhil Gupta, Lei Tian, Weiyi\nZheng, Manisha Jain, Adnan Aziz, and Adam Lerer for\ntheir feedback on many iterations of this draft, and in-depth\ntechnical discussions around building efﬁcient infrastructure\nand platforms;", "metadata": {}}, {"text": "Adina Williams, Emily Dinan, Mona Diab,\nAshkan Yousefpour for the valuable discussions and insights\non AI and environmental responsibility;", "metadata": {}}, {"text": "Mark Zhou, Niket\nAgarwal, Jongsoo Park, Michael Anderson, Xiaodong Wang;", "metadata": {}}, {"text": "Yatharth Saraf, Hagay Lupesco, Jigar Desai, Joelle Pineau,\nRam Valliyappan, Rajesh Mosur, Ananth Sankarnarayanan and\nEytan Bakshy for their leadership and vision without which\nthis work would not have been possible.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "REFERENCES\n[1] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,\nK. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, A. Bridgland,\nC. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-\nParedes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman,\nE. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer,\nS. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu,\nP. Kohli, and D. Hassabis, “Highly accurate protein structure prediction\nwith alphafold,” Nature, 2021.\n[2] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue\ngeneration,” arXiv:2107.07566, 2021.\n[3] D. Silver, T. Hubert, J. Schrittwieser, and D. Hassabis, “AlphaZero:\nShedding new light on chess, shogi, and Go,” 2018.\n[4] C. L. Zitnick, L. Chanussot, A. Das, S. Goyal, J. Heras-Domingo,\nC. Ho, W. Hu, T. Lavril, A. Palizhati, M. Riviere, M. Shuaibi, A. Sriram,\nK. Tran, B. Wood, J. Yoon, D. Parikh, and Z. Ulissi, “An introduction\nto electrocatalyst design using machine learning for renewable energy\nstorage,” arXiv preprint arXiv:2010.09435 , 2020.\n[5] C. Elkin and S. Witherspoon, “Machine learning can boost the value\nof wind energy,” 2019.\n[6] R. Evans and J. Gao, “DeepMind AI Reduces Google Data Centre\nCooling Bill by 40%,” 2016.\n[7] K. Sheikh, “A Growing Presence on the Farm: Robots,” February 2020.\n[8] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. Lacoste,\nK. Sankaran, A. S. Ross, N. Milojevic-Dupont, N. Jaques, A. Waldman-\nBrown, A. Luccioni, T. Maharaj, E. D. Sherwin, S. K. Mukkavilli, K. P.\nKording, C. Gomes, A. Y . Ng, D. Hassabis, J. C. Platt, F. Creutzig,\nJ. Chayes, and Y . Bengio, “Tackling climate change with machine\nlearning,” arXiv:1906.05433, 2019.\n[9] R. Nishant, M. Kennedy, and J. Corbett, “Artiﬁcial intelligence\nfor sustainability: Challenges, opportunities, and a research agenda,”\nInternational Journal of Information Management , vol. 53, 2020.\n[10] Facts and Factors, “Global artiﬁcial intelligence market,” 2021.\n[11] D. Mudigere, Y . Hao, J. Huang, A. Tulloch, S. Sridharan, X. Liu,\nM. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko,\nA. Basant, Y . Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli,\nC. Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y . Ma, J. Yang, E. Wen,\nH. Li, L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. R.\nKishore, T. Graf, A. Eisenman, K. K. Matam, A. Gangidi, G. J.\nChen, M. Krishnan, A. Nayak, K. Nair, B. Muthiah, M. khorashadi,\nP. Bhattacharya, P. Lapukhov, M. Naumov, L. Qiao, M. Smelyanskiy,\nB. Jia, and V . Rao, “Software-hardware co-design for fast and scalable\ntraining of deep learning recommendation models,” arXiv preprint\narXiv:2104.05158, 2021.\n[12] D. Hernandez and T. B. Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.\n[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.\n[14] P. Nayak, “Understanding searches better than ever before,” 2019.\n[15] X. Yi, Y .-F. Chen, S. Ramesh, V . Rajashekhar, L. Hong, N. Fiedel,\nN. Seshadri, L. Heldt, X. Wu, and E. H. Chi, “Factorized deep retrieval\nand distributed tensorﬂow serving,” in Proceedings of Machine Learning\nand Systems, 2018.\n[16] W. Zhao, D. Xie, R. Jia, Y . Qian, R. Ding, M. Sun, and P. Li, “Distributed\nhierarchical gpu parameter server for massive scale deep learning ads\nsystems,” arXiv preprint arXiv:2003.05622 , 2020.\n[17] M. Lui, Y . Yetim, O. Ozkan, Z. Zhao, S.-Y . Tsai, C.-J. Wu, and M. Hemp-\nstead, “Understanding capacity-driven scale-out neural recommendation\ninference,” in Proceedings of the IEEE International Symposium on\nPerformance Analysis of Systems and Software , 2021.\n[18] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y . He, “Zero-inﬁnity:\nBreaking the gpu memory wall for extreme scale deep learning,” arXiv\npreprint arXiv:2104.07857, 2021.\n[19] U. Gupta, Y . Kim, S. Lee, J. Tse, H. S. Lee, G. Wei, D. Brooks,\nand C. Wu, “Chasing carbon: The elusive environmental footprint of\ncomputing,” in Proceedings of the IEEE International Symposium on\nHigh-Performance Computer Architecture, 2021.\n[20] N. Tomasev, J. Cornebise, F. Hutter, S. Mohamed, A. Picciariello,\nB. Connelly, D. Belgrave, D. Ezer, F. C. van der Haert, F. Mugisha,\nG. Abila, H. Arai, H. Almiraat, J. Proskurnia, K. Snyder, M. Otake-\nMatsuura, M. Othman, T. Glasmachers, W. D. Wever, Y . Teh, M. E.\nKhan, R. D. Winne, T. Schaul, and C. Clopath, “Ai for social good:\nunlocking the opportunity for positive impact,” Nature Communications,\nvol. 11, 2020.\n[21] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild,\nD. So, M. Texier, and J. Dean, “Carbon emissions and large neural\nnetwork training,” arXiv preprint arXiv:2104.10350 , 2021.\n[22] EPA, “United states environmental protection agency greenhouse gas\nequivalencies calculator,” 2021.\n[23] Facebook, “2020 sustainability report,” 2021.\n[24] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov,\nM. Fawzy, B. Jia, Y . Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis,\nM. Smelyanskiy, L. Xiong, and X. Wang, “Applied machine learning\nat facebook: A datacenter infrastructure perspective,” in Proceedings\nof the IEEE International Symposium on High Performance Computer\nArchitecture, 2018.\n[25] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,” arXiv\npreprint arXiv:1911.02116, 2020.\n[26] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman,\nJ. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov,\nA. Mallevich, I. Cherniavskii, Y . Lu, R. Krishnamoorthi, A. Yu,\nV . Kondratenko, S. Pereira, X. Chen, W. Chen, V . Rao, B. Jia,\nL. Xiong, and M. Smelyanskiy, “Deep learning recommendation\nmodel for personalization and recommendation systems,” arXiv preprint\narXiv:1906.00091, 2019.\n[27] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks,\nB. Cottel, K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee,\nA. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang,\n“The architectural implications of facebook’s dnn-based personalized\nrecommendation,” in Proceedings of the IEEE International Symposium\non High Performance Computer Architecture , 2020.\n[28] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” arXiv preprint arXiv:1906.02243 ,\n2019.\n[29] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to\ntrillion parameter models with simple and efﬁcient sparsity,” CoRR,\nvol. abs/2101.03961, 2021.\n[30] D. Adiwardana and T. Luong, “Towards a conversational agent that can\nchat about... anything,” 2020.\n[31] Apple, “Product environmental report Mac Pro,” 2019.\n[32] NVIDIA, “Faster Transformer,” 2021.\n[33] L. Ke, U. Gupta, B. Y . Cho, D. Brooks, V . Chandra, U. Diril,\nA. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher,\nD. Mudigere, M. Naumov, M. Schatz, M. Smelyanskiy, X. Wang,\nB. Reagen, C.-J. Wu, M. Hempstead, and X. Zhang, “Recnmp: Accel-\nerating personalized recommendation with near-memory processing,”\nin Proceedings of the ACM/IEEE Annual International Symposium on\nComputer Architecture, 2020.\n[34] Z. Deng, J. Park, P. T. P. Tang, H. Liu, J. Yang, H. Yuen, J. Huang,\nD. Khudia, X. Wei, E. Wen, D. Choudhary, R. Krishnamoorthi, C.-J. Wu,\nS. Nadathur, C. Kim, M. Naumov, S. Naghshineh, and M. Smelyanskiy,\n“Low-precision hardware architectures meet recommendation model\ninference at scale,” IEEE Micro, vol. 41, no. 5, pp. 93–100, 2021.\n[35] L. Wesolowski, B. Acun, V . Andrei, A. Aziz, G. Dankel, C. Gregg,\nX. Meng, C. Meurillon, D. Sheahan, L. Tian, J. Yang, P. Yu, and\nK. Hazelwood, “Datacenter-scale analysis and optimization of gpu\nmachine learning workloads,” IEEE Micro, vol. 41, no. 5, 2021.\n[36] A. Sriraman, A. Dhanotia, and T. F. Wenisch, “Softsku: Optimizing\nserver architectures for microservice diversity @scale,” in Proceed-\nings of the 46th International Symposium on Computer Architecture ,\nAssociation for Computing Machinery, 2019.\n[37] A. Sriraman and A. Dhanotia, “Accelerometer: Understanding ac-\nceleration opportunities for data center overheads at hyperscale,” in\nProceedings of the International Conference on Architectural Support\nfor Programming Languages and Operating Systems , 2020.\n[38] C. Tang, K. Yu, K. Veeraraghavan, J. Kaldor, S. Michelson, T. Kooburat,\nA. Anbudurai, M. Clark, K. Gogia, L. Cheng, B. Christensen, A. Gartrell,\n11", "sentences": [{"text": "REFERENCES\n[1] J.", "metadata": {}}, {"text": "Jumper, R.", "metadata": {}}, {"text": "Evans, A.", "metadata": {}}, {"text": "Pritzel, T.", "metadata": {}}, {"text": "Green, M.", "metadata": {}}, {"text": "Figurnov, O.", "metadata": {}}, {"text": "Ronneberger,\nK.", "metadata": {}}, {"text": "Tunyasuvunakool, R.", "metadata": {}}, {"text": "Bates, A.", "metadata": {}}, {"text": "ˇZ´ıdek, A.", "metadata": {}}, {"text": "Potapenko, A.", "metadata": {}}, {"text": "Bridgland,\nC.", "metadata": {}}, {"text": "Meyer, S.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Kohl, A.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Ballard, A.", "metadata": {}}, {"text": "Cowie, B.", "metadata": {}}, {"text": "Romera-\nParedes, S.", "metadata": {}}, {"text": "Nikolov, R.", "metadata": {}}, {"text": "Jain, J.", "metadata": {}}, {"text": "Adler, T.", "metadata": {}}, {"text": "Back, S.", "metadata": {}}, {"text": "Petersen, D.", "metadata": {}}, {"text": "Reiman,\nE.", "metadata": {}}, {"text": "Clancy, M.", "metadata": {}}, {"text": "Zielinski, M.", "metadata": {}}, {"text": "Steinegger, M.", "metadata": {}}, {"text": "Pacholska, T.", "metadata": {}}, {"text": "Berghammer,\nS.", "metadata": {}}, {"text": "Bodenstein, D.", "metadata": {}}, {"text": "Silver, O.", "metadata": {}}, {"text": "Vinyals, A.", "metadata": {}}, {"text": "W.", "metadata": {}}, {"text": "Senior, K.", "metadata": {}}, {"text": "Kavukcuoglu,\nP.", "metadata": {}}, {"text": "Kohli, and D.", "metadata": {}}, {"text": "Hassabis, “Highly accurate protein structure prediction\nwith alphafold,” Nature, 2021.", "metadata": {}}, {"text": "[2] M.", "metadata": {}}, {"text": "Komeili, K.", "metadata": {}}, {"text": "Shuster, and J.", "metadata": {}}, {"text": "Weston, “Internet-augmented dialogue\ngeneration,” arXiv:2107.07566, 2021.", "metadata": {}}, {"text": "[3] D.", "metadata": {}}, {"text": "Silver, T.", "metadata": {}}, {"text": "Hubert, J.", "metadata": {}}, {"text": "Schrittwieser, and D.", "metadata": {}}, {"text": "Hassabis, “AlphaZero:\nShedding new light on chess, shogi, and Go,” 2018.", "metadata": {}}, {"text": "[4] C.", "metadata": {}}, {"text": "L.", "metadata": {}}, {"text": "Zitnick, L.", "metadata": {}}, {"text": "Chanussot, A.", "metadata": {}}, {"text": "Das, S.", "metadata": {}}, {"text": "Goyal, J.", "metadata": {}}, {"text": "Heras-Domingo,\nC.", "metadata": {}}, {"text": "Ho, W.", "metadata": {}}, {"text": "Hu, T.", "metadata": {}}, {"text": "Lavril, A.", "metadata": {}}, {"text": "Palizhati, M.", "metadata": {}}, {"text": "Riviere, M.", "metadata": {}}, {"text": "Shuaibi, A.", "metadata": {}}, {"text": "Sriram,\nK.", "metadata": {}}, {"text": "Tran, B.", "metadata": {}}, {"text": "Wood, J.", "metadata": {}}, {"text": "Yoon, D.", "metadata": {}}, {"text": "Parikh, and Z.", "metadata": {}}, {"text": "Ulissi, “An introduction\nto electrocatalyst design using machine learning for renewable energy\nstorage,” arXiv preprint arXiv:2010.09435 , 2020.", "metadata": {}}, {"text": "[5] C.", "metadata": {}}, {"text": "Elkin and S.", "metadata": {}}, {"text": "Witherspoon, “Machine learning can boost the value\nof wind energy,” 2019.", "metadata": {}}, {"text": "[6] R.", "metadata": {}}, {"text": "Evans and J.", "metadata": {}}, {"text": "Gao, “DeepMind AI Reduces Google Data Centre\nCooling Bill by 40%,” 2016.", "metadata": {}}, {"text": "[7] K.", "metadata": {}}, {"text": "Sheikh, “A Growing Presence on the Farm: Robots,” February 2020.", "metadata": {}}, {"text": "[8] D.", "metadata": {}}, {"text": "Rolnick, P.", "metadata": {}}, {"text": "L.", "metadata": {}}, {"text": "Donti, L.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Kaack, K.", "metadata": {}}, {"text": "Kochanski, A.", "metadata": {}}, {"text": "Lacoste,\nK.", "metadata": {}}, {"text": "Sankaran, A.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Ross, N.", "metadata": {}}, {"text": "Milojevic-Dupont, N.", "metadata": {}}, {"text": "Jaques, A.", "metadata": {}}, {"text": "Waldman-\nBrown, A.", "metadata": {}}, {"text": "Luccioni, T.", "metadata": {}}, {"text": "Maharaj, E.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Sherwin, S.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Mukkavilli, K.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Kording, C.", "metadata": {}}, {"text": "Gomes, A.", "metadata": {}}, {"text": "Y .", "metadata": {}}, {"text": "Ng, D.", "metadata": {}}, {"text": "Hassabis, J.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Platt, F.", "metadata": {}}, {"text": "Creutzig,\nJ.", "metadata": {}}, {"text": "Chayes, and Y .", "metadata": {}}, {"text": "Bengio, “Tackling climate change with machine\nlearning,” arXiv:1906.05433, 2019.", "metadata": {}}, {"text": "[9] R.", "metadata": {}}, {"text": "Nishant, M.", "metadata": {}}, {"text": "Kennedy, and J.", "metadata": {}}, {"text": "Corbett, “Artiﬁcial intelligence\nfor sustainability: Challenges, opportunities, and a research agenda,”\nInternational Journal of Information Management , vol.", "metadata": {}}, {"text": "53, 2020.", "metadata": {}}, {"text": "[10] Facts and Factors, “Global artiﬁcial intelligence market,” 2021.", "metadata": {}}, {"text": "[11] D.", "metadata": {}}, {"text": "Mudigere, Y .", "metadata": {}}, {"text": "Hao, J.", "metadata": {}}, {"text": "Huang, A.", "metadata": {}}, {"text": "Tulloch, S.", "metadata": {}}, {"text": "Sridharan, X.", "metadata": {}}, {"text": "Liu,\nM.", "metadata": {}}, {"text": "Ozdal, J.", "metadata": {}}, {"text": "Nie, J.", "metadata": {}}, {"text": "Park, L.", "metadata": {}}, {"text": "Luo, J.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Yang, L.", "metadata": {}}, {"text": "Gao, D.", "metadata": {}}, {"text": "Ivchenko,\nA.", "metadata": {}}, {"text": "Basant, Y .", "metadata": {}}, {"text": "Hu, J.", "metadata": {}}, {"text": "Yang, E.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Ardestani, X.", "metadata": {}}, {"text": "Wang, R.", "metadata": {}}, {"text": "Komuravelli,\nC.", "metadata": {}}, {"text": "Chu, S.", "metadata": {}}, {"text": "Yilmaz, H.", "metadata": {}}, {"text": "Li, J.", "metadata": {}}, {"text": "Qian, Z.", "metadata": {}}, {"text": "Feng, Y .", "metadata": {}}, {"text": "Ma, J.", "metadata": {}}, {"text": "Yang, E.", "metadata": {}}, {"text": "Wen,\nH.", "metadata": {}}, {"text": "Li, L.", "metadata": {}}, {"text": "Yang, C.", "metadata": {}}, {"text": "Sun, W.", "metadata": {}}, {"text": "Zhao, D.", "metadata": {}}, {"text": "Melts, K.", "metadata": {}}, {"text": "Dhulipala, K.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Kishore, T.", "metadata": {}}, {"text": "Graf, A.", "metadata": {}}, {"text": "Eisenman, K.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Matam, A.", "metadata": {}}, {"text": "Gangidi, G.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Chen, M.", "metadata": {}}, {"text": "Krishnan, A.", "metadata": {}}, {"text": "Nayak, K.", "metadata": {}}, {"text": "Nair, B.", "metadata": {}}, {"text": "Muthiah, M.", "metadata": {}}, {"text": "khorashadi,\nP.", "metadata": {}}, {"text": "Bhattacharya, P.", "metadata": {}}, {"text": "Lapukhov, M.", "metadata": {}}, {"text": "Naumov, L.", "metadata": {}}, {"text": "Qiao, M.", "metadata": {}}, {"text": "Smelyanskiy,\nB.", "metadata": {}}, {"text": "Jia, and V .", "metadata": {}}, {"text": "Rao, “Software-hardware co-design for fast and scalable\ntraining of deep learning recommendation models,” arXiv preprint\narXiv:2104.05158, 2021.", "metadata": {}}, {"text": "[12] D.", "metadata": {}}, {"text": "Hernandez and T.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.", "metadata": {}}, {"text": "[13] T.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Brown, B.", "metadata": {}}, {"text": "Mann, N.", "metadata": {}}, {"text": "Ryder, M.", "metadata": {}}, {"text": "Subbiah, J.", "metadata": {}}, {"text": "Kaplan, P.", "metadata": {}}, {"text": "Dhariwal,\nA.", "metadata": {}}, {"text": "Neelakantan, P.", "metadata": {}}, {"text": "Shyam, G.", "metadata": {}}, {"text": "Sastry, A.", "metadata": {}}, {"text": "Askell, S.", "metadata": {}}, {"text": "Agarwal, A.", "metadata": {}}, {"text": "Herbert-\nV oss, G.", "metadata": {}}, {"text": "Krueger, T.", "metadata": {}}, {"text": "Henighan, R.", "metadata": {}}, {"text": "Child, A.", "metadata": {}}, {"text": "Ramesh, D.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Ziegler,\nJ.", "metadata": {}}, {"text": "Wu, C.", "metadata": {}}, {"text": "Winter, C.", "metadata": {}}, {"text": "Hesse, M.", "metadata": {}}, {"text": "Chen, E.", "metadata": {}}, {"text": "Sigler, M.", "metadata": {}}, {"text": "Litwin, S.", "metadata": {}}, {"text": "Gray,\nB.", "metadata": {}}, {"text": "Chess, J.", "metadata": {}}, {"text": "Clark, C.", "metadata": {}}, {"text": "Berner, S.", "metadata": {}}, {"text": "McCandlish, A.", "metadata": {}}, {"text": "Radford, I.", "metadata": {}}, {"text": "Sutskever,\nand D.", "metadata": {}}, {"text": "Amodei, “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.", "metadata": {}}, {"text": "[14] P.", "metadata": {}}, {"text": "Nayak, “Understanding searches better than ever before,” 2019.", "metadata": {}}, {"text": "[15] X.", "metadata": {}}, {"text": "Yi, Y .-F.", "metadata": {}}, {"text": "Chen, S.", "metadata": {}}, {"text": "Ramesh, V .", "metadata": {}}, {"text": "Rajashekhar, L.", "metadata": {}}, {"text": "Hong, N.", "metadata": {}}, {"text": "Fiedel,\nN.", "metadata": {}}, {"text": "Seshadri, L.", "metadata": {}}, {"text": "Heldt, X.", "metadata": {}}, {"text": "Wu, and E.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Chi, “Factorized deep retrieval\nand distributed tensorﬂow serving,” in Proceedings of Machine Learning\nand Systems, 2018.", "metadata": {}}, {"text": "[16] W.", "metadata": {}}, {"text": "Zhao, D.", "metadata": {}}, {"text": "Xie, R.", "metadata": {}}, {"text": "Jia, Y .", "metadata": {}}, {"text": "Qian, R.", "metadata": {}}, {"text": "Ding, M.", "metadata": {}}, {"text": "Sun, and P.", "metadata": {}}, {"text": "Li, “Distributed\nhierarchical gpu parameter server for massive scale deep learning ads\nsystems,” arXiv preprint arXiv:2003.05622 , 2020.", "metadata": {}}, {"text": "[17] M.", "metadata": {}}, {"text": "Lui, Y .", "metadata": {}}, {"text": "Yetim, O.", "metadata": {}}, {"text": "Ozkan, Z.", "metadata": {}}, {"text": "Zhao, S.-Y .", "metadata": {}}, {"text": "Tsai, C.-J.", "metadata": {}}, {"text": "Wu, and M.", "metadata": {}}, {"text": "Hemp-\nstead, “Understanding capacity-driven scale-out neural recommendation\ninference,” in Proceedings of the IEEE International Symposium on\nPerformance Analysis of Systems and Software , 2021.", "metadata": {}}, {"text": "[18] S.", "metadata": {}}, {"text": "Rajbhandari, O.", "metadata": {}}, {"text": "Ruwase, J.", "metadata": {}}, {"text": "Rasley, S.", "metadata": {}}, {"text": "Smith, and Y .", "metadata": {}}, {"text": "He, “Zero-inﬁnity:\nBreaking the gpu memory wall for extreme scale deep learning,” arXiv\npreprint arXiv:2104.07857, 2021.", "metadata": {}}, {"text": "[19] U.", "metadata": {}}, {"text": "Gupta, Y .", "metadata": {}}, {"text": "Kim, S.", "metadata": {}}, {"text": "Lee, J.", "metadata": {}}, {"text": "Tse, H.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Lee, G.", "metadata": {}}, {"text": "Wei, D.", "metadata": {}}, {"text": "Brooks,\nand C.", "metadata": {}}, {"text": "Wu, “Chasing carbon: The elusive environmental footprint of\ncomputing,” in Proceedings of the IEEE International Symposium on\nHigh-Performance Computer Architecture, 2021.", "metadata": {}}, {"text": "[20] N.", "metadata": {}}, {"text": "Tomasev, J.", "metadata": {}}, {"text": "Cornebise, F.", "metadata": {}}, {"text": "Hutter, S.", "metadata": {}}, {"text": "Mohamed, A.", "metadata": {}}, {"text": "Picciariello,\nB.", "metadata": {}}, {"text": "Connelly, D.", "metadata": {}}, {"text": "Belgrave, D.", "metadata": {}}, {"text": "Ezer, F.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "van der Haert, F.", "metadata": {}}, {"text": "Mugisha,\nG.", "metadata": {}}, {"text": "Abila, H.", "metadata": {}}, {"text": "Arai, H.", "metadata": {}}, {"text": "Almiraat, J.", "metadata": {}}, {"text": "Proskurnia, K.", "metadata": {}}, {"text": "Snyder, M.", "metadata": {}}, {"text": "Otake-\nMatsuura, M.", "metadata": {}}, {"text": "Othman, T.", "metadata": {}}, {"text": "Glasmachers, W.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Wever, Y .", "metadata": {}}, {"text": "Teh, M.", "metadata": {}}, {"text": "E.", "metadata": {}}, {"text": "Khan, R.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Winne, T.", "metadata": {}}, {"text": "Schaul, and C.", "metadata": {}}, {"text": "Clopath, “Ai for social good:\nunlocking the opportunity for positive impact,” Nature Communications,\nvol.", "metadata": {}}, {"text": "11, 2020.", "metadata": {}}, {"text": "[21] D.", "metadata": {}}, {"text": "Patterson, J.", "metadata": {}}, {"text": "Gonzalez, Q.", "metadata": {}}, {"text": "Le, C.", "metadata": {}}, {"text": "Liang, L.-M.", "metadata": {}}, {"text": "Munguia, D.", "metadata": {}}, {"text": "Rothchild,\nD.", "metadata": {}}, {"text": "So, M.", "metadata": {}}, {"text": "Texier, and J.", "metadata": {}}, {"text": "Dean, “Carbon emissions and large neural\nnetwork training,” arXiv preprint arXiv:2104.10350 , 2021.", "metadata": {}}, {"text": "[22] EPA, “United states environmental protection agency greenhouse gas\nequivalencies calculator,” 2021.", "metadata": {}}, {"text": "[23] Facebook, “2020 sustainability report,” 2021.", "metadata": {}}, {"text": "[24] K.", "metadata": {}}, {"text": "Hazelwood, S.", "metadata": {}}, {"text": "Bird, D.", "metadata": {}}, {"text": "Brooks, S.", "metadata": {}}, {"text": "Chintala, U.", "metadata": {}}, {"text": "Diril, D.", "metadata": {}}, {"text": "Dzhulgakov,\nM.", "metadata": {}}, {"text": "Fawzy, B.", "metadata": {}}, {"text": "Jia, Y .", "metadata": {}}, {"text": "Jia, A.", "metadata": {}}, {"text": "Kalro, J.", "metadata": {}}, {"text": "Law, K.", "metadata": {}}, {"text": "Lee, J.", "metadata": {}}, {"text": "Lu, P.", "metadata": {}}, {"text": "Noordhuis,\nM.", "metadata": {}}, {"text": "Smelyanskiy, L.", "metadata": {}}, {"text": "Xiong, and X.", "metadata": {}}, {"text": "Wang, “Applied machine learning\nat facebook: A datacenter infrastructure perspective,” in Proceedings\nof the IEEE International Symposium on High Performance Computer\nArchitecture, 2018.", "metadata": {}}, {"text": "[25] A.", "metadata": {}}, {"text": "Conneau, K.", "metadata": {}}, {"text": "Khandelwal, N.", "metadata": {}}, {"text": "Goyal, V .", "metadata": {}}, {"text": "Chaudhary, G.", "metadata": {}}, {"text": "Wenzek,\nF.", "metadata": {}}, {"text": "Guzm ´an, E.", "metadata": {}}, {"text": "Grave, M.", "metadata": {}}, {"text": "Ott, L.", "metadata": {}}, {"text": "Zettlemoyer, and V .", "metadata": {}}, {"text": "Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,” arXiv\npreprint arXiv:1911.02116, 2020.", "metadata": {}}, {"text": "[26] M.", "metadata": {}}, {"text": "Naumov, D.", "metadata": {}}, {"text": "Mudigere, H.-J.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Shi, J.", "metadata": {}}, {"text": "Huang, N.", "metadata": {}}, {"text": "Sundaraman,\nJ.", "metadata": {}}, {"text": "Park, X.", "metadata": {}}, {"text": "Wang, U.", "metadata": {}}, {"text": "Gupta, C.-J.", "metadata": {}}, {"text": "Wu, A.", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "Azzolini, D.", "metadata": {}}, {"text": "Dzhulgakov,\nA.", "metadata": {}}, {"text": "Mallevich, I.", "metadata": {}}, {"text": "Cherniavskii, Y .", "metadata": {}}, {"text": "Lu, R.", "metadata": {}}, {"text": "Krishnamoorthi, A.", "metadata": {}}, {"text": "Yu,\nV .", "metadata": {}}, {"text": "Kondratenko, S.", "metadata": {}}, {"text": "Pereira, X.", "metadata": {}}, {"text": "Chen, W.", "metadata": {}}, {"text": "Chen, V .", "metadata": {}}, {"text": "Rao, B.", "metadata": {}}, {"text": "Jia,\nL.", "metadata": {}}, {"text": "Xiong, and M.", "metadata": {}}, {"text": "Smelyanskiy, “Deep learning recommendation\nmodel for personalization and recommendation systems,” arXiv preprint\narXiv:1906.00091, 2019.", "metadata": {}}, {"text": "[27] U.", "metadata": {}}, {"text": "Gupta, C.-J.", "metadata": {}}, {"text": "Wu, X.", "metadata": {}}, {"text": "Wang, M.", "metadata": {}}, {"text": "Naumov, B.", "metadata": {}}, {"text": "Reagen, D.", "metadata": {}}, {"text": "Brooks,\nB.", "metadata": {}}, {"text": "Cottel, K.", "metadata": {}}, {"text": "Hazelwood, M.", "metadata": {}}, {"text": "Hempstead, B.", "metadata": {}}, {"text": "Jia, H.-H.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Lee,\nA.", "metadata": {}}, {"text": "Malevich, D.", "metadata": {}}, {"text": "Mudigere, M.", "metadata": {}}, {"text": "Smelyanskiy, L.", "metadata": {}}, {"text": "Xiong, and X.", "metadata": {}}, {"text": "Zhang,\n“The architectural implications of facebook’s dnn-based personalized\nrecommendation,” in Proceedings of the IEEE International Symposium\non High Performance Computer Architecture , 2020.", "metadata": {}}, {"text": "[28] E.", "metadata": {}}, {"text": "Strubell, A.", "metadata": {}}, {"text": "Ganesh, and A.", "metadata": {}}, {"text": "McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” arXiv preprint arXiv:1906.02243 ,\n2019.", "metadata": {}}, {"text": "[29] W.", "metadata": {}}, {"text": "Fedus, B.", "metadata": {}}, {"text": "Zoph, and N.", "metadata": {}}, {"text": "Shazeer, “Switch transformers: Scaling to\ntrillion parameter models with simple and efﬁcient sparsity,” CoRR,\nvol.", "metadata": {}}, {"text": "abs/2101.03961, 2021.", "metadata": {}}, {"text": "[30] D.", "metadata": {}}, {"text": "Adiwardana and T.", "metadata": {}}, {"text": "Luong, “Towards a conversational agent that can\nchat about...", "metadata": {}}, {"text": "anything,” 2020.", "metadata": {}}, {"text": "[31] Apple, “Product environmental report Mac Pro,” 2019.", "metadata": {}}, {"text": "[32] NVIDIA, “Faster Transformer,” 2021.", "metadata": {}}, {"text": "[33] L.", "metadata": {}}, {"text": "Ke, U.", "metadata": {}}, {"text": "Gupta, B.", "metadata": {}}, {"text": "Y .", "metadata": {}}, {"text": "Cho, D.", "metadata": {}}, {"text": "Brooks, V .", "metadata": {}}, {"text": "Chandra, U.", "metadata": {}}, {"text": "Diril,\nA.", "metadata": {}}, {"text": "Firoozshahian, K.", "metadata": {}}, {"text": "Hazelwood, B.", "metadata": {}}, {"text": "Jia, H.-H.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Lee, M.", "metadata": {}}, {"text": "Li, B.", "metadata": {}}, {"text": "Maher,\nD.", "metadata": {}}, {"text": "Mudigere, M.", "metadata": {}}, {"text": "Naumov, M.", "metadata": {}}, {"text": "Schatz, M.", "metadata": {}}, {"text": "Smelyanskiy, X.", "metadata": {}}, {"text": "Wang,\nB.", "metadata": {}}, {"text": "Reagen, C.-J.", "metadata": {}}, {"text": "Wu, M.", "metadata": {}}, {"text": "Hempstead, and X.", "metadata": {}}, {"text": "Zhang, “Recnmp: Accel-\nerating personalized recommendation with near-memory processing,”\nin Proceedings of the ACM/IEEE Annual International Symposium on\nComputer Architecture, 2020.", "metadata": {}}, {"text": "[34] Z.", "metadata": {}}, {"text": "Deng, J.", "metadata": {}}, {"text": "Park, P.", "metadata": {}}, {"text": "T.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Tang, H.", "metadata": {}}, {"text": "Liu, J.", "metadata": {}}, {"text": "Yang, H.", "metadata": {}}, {"text": "Yuen, J.", "metadata": {}}, {"text": "Huang,\nD.", "metadata": {}}, {"text": "Khudia, X.", "metadata": {}}, {"text": "Wei, E.", "metadata": {}}, {"text": "Wen, D.", "metadata": {}}, {"text": "Choudhary, R.", "metadata": {}}, {"text": "Krishnamoorthi, C.-J.", "metadata": {}}, {"text": "Wu,\nS.", "metadata": {}}, {"text": "Nadathur, C.", "metadata": {}}, {"text": "Kim, M.", "metadata": {}}, {"text": "Naumov, S.", "metadata": {}}, {"text": "Naghshineh, and M.", "metadata": {}}, {"text": "Smelyanskiy,\n“Low-precision hardware architectures meet recommendation model\ninference at scale,” IEEE Micro, vol.", "metadata": {}}, {"text": "41, no.", "metadata": {}}, {"text": "5, pp.", "metadata": {}}, {"text": "93–100, 2021.", "metadata": {}}, {"text": "[35] L.", "metadata": {}}, {"text": "Wesolowski, B.", "metadata": {}}, {"text": "Acun, V .", "metadata": {}}, {"text": "Andrei, A.", "metadata": {}}, {"text": "Aziz, G.", "metadata": {}}, {"text": "Dankel, C.", "metadata": {}}, {"text": "Gregg,\nX.", "metadata": {}}, {"text": "Meng, C.", "metadata": {}}, {"text": "Meurillon, D.", "metadata": {}}, {"text": "Sheahan, L.", "metadata": {}}, {"text": "Tian, J.", "metadata": {}}, {"text": "Yang, P.", "metadata": {}}, {"text": "Yu, and\nK.", "metadata": {}}, {"text": "Hazelwood, “Datacenter-scale analysis and optimization of gpu\nmachine learning workloads,” IEEE Micro, vol.", "metadata": {}}, {"text": "41, no.", "metadata": {}}, {"text": "5, 2021.", "metadata": {}}, {"text": "[36] A.", "metadata": {}}, {"text": "Sriraman, A.", "metadata": {}}, {"text": "Dhanotia, and T.", "metadata": {}}, {"text": "F.", "metadata": {}}, {"text": "Wenisch, “Softsku: Optimizing\nserver architectures for microservice diversity @scale,” in Proceed-\nings of the 46th International Symposium on Computer Architecture ,\nAssociation for Computing Machinery, 2019.", "metadata": {}}, {"text": "[37] A.", "metadata": {}}, {"text": "Sriraman and A.", "metadata": {}}, {"text": "Dhanotia, “Accelerometer: Understanding ac-\nceleration opportunities for data center overheads at hyperscale,” in\nProceedings of the International Conference on Architectural Support\nfor Programming Languages and Operating Systems , 2020.", "metadata": {}}, {"text": "[38] C.", "metadata": {}}, {"text": "Tang, K.", "metadata": {}}, {"text": "Yu, K.", "metadata": {}}, {"text": "Veeraraghavan, J.", "metadata": {}}, {"text": "Kaldor, S.", "metadata": {}}, {"text": "Michelson, T.", "metadata": {}}, {"text": "Kooburat,\nA.", "metadata": {}}, {"text": "Anbudurai, M.", "metadata": {}}, {"text": "Clark, K.", "metadata": {}}, {"text": "Gogia, L.", "metadata": {}}, {"text": "Cheng, B.", "metadata": {}}, {"text": "Christensen, A.", "metadata": {}}, {"text": "Gartrell,\n11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "M. Khutornenko, S. Kulkarni, M. Pawlowski, T. Pelkonen, A. Rodrigues,\nR. Tibrewal, V . Venkatesan, and P. Zhang, “Twine: A uniﬁed cluster\nmanagement system for shared infrastructure,” in Proceedings of the\nUSENIX Symposium on Operating Systems Design and Implementation ,\n2020.\n[39] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres, “Quantifying the\ncarbon emissions of machine learning,” Workshop on Tackling Climate\nChange with Machine Learning at NeurIPS 2019 , 2019.\n[40] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau,\n“Towards the systematic reporting of the energy and carbon footprints\nof machine learning,” CoRR, vol. abs/2002.05651, 2020.\n[41] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On\nthe dangers of stochastic parrots: Can language models be too big?,” in\nProceedings of the ACM Conference on Fairness, Accountability, and\nTransparency, 2021.\n[42] N. Sachdeva, C.-J. Wu, and J. McAuley, “Svp-cf: Selection via proxy\nfor collaborative ﬁltering data,” arXiv preprint arXiv:2107.04984 , 2021.\n[43] E. Valavi, J. Hestness, N. Ardalani, and M. Iansiti, Time and the Value\nof Data. Working papers, Harvard Business School, 2020.\n[44] M. Zhao, N. Agarwal, A. Basant, B. Gedik, S. Pan, M. Ozdal,\nR. Komuravelli, J. Pan, T. Bao, H. Lu, S. Narayanan, J. Langman,\nK. Wilfong, H. Rastogi, C. Wu, C. Kozyrakis, and P. Pol, “Understanding\nand co-designing the data ingestion pipeline for industry-scale recsys\ntraining,” CoRR, vol. abs/2108.09373, 2021.\n[45] R. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu,\nand I. Guyon, “Bayesian optimization is superior to random search for\nmachine learning hyperparameter tuning: Analysis of the black-box\noptimization challenge 2020,” CoRR, vol. abs/2104.10201, 2021.\n[46] P. Ren, Y . Xiao, X. Chang, P.-y. Huang, Z. Li, X. Chen, and X. Wang,\n“A comprehensive survey of neural architecture search: Challenges and\nsolutions,” ACM Comput. Surv., vol. 54, no. 4, 2021.\n[47] Q. Song, D. Cheng, H. Zhou, J. Yang, Y . Tian, and X. Hu, “Towards\nautomated neural interaction discovery for click-through rate prediction,”\nProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , 2020.\n[48] M. R. Joglekar, C. Li, M. Chen, T. Xu, X. Wang, J. K. Adams, P. Khaitan,\nJ. Liu, and Q. V . Le, “Neural input search for large scale recommendation\nmodels,” in Proceedings of the ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , 2020.\n[49] M. Tan and Q. V . Le, “Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks,” arXiv preprint arXiv:1905.11946 , 2020.\n[50] D. Eriksson, P. I. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu,\nS. Zhao, A. Aly, G. Venkatesh, and M. Balandat, “Latency-aware neural\narchitecture search with multi-objective bayesian optimization,” CoRR,\nvol. abs/2106.11890, 2021.\n[51] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train\none network and specialize it for efﬁcient deployment,” arXiv preprint\narXiv:1908.09791, 2020.\n[52] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,\nand D. Marculescu, “Single-path nas: Designing hardware-efﬁcient\nconvnets in less than 4 hours,” arXiv preprint arXiv:1904.02877 , 2019.\n[53] W. Chen, X. Gong, and Z. Wang, “Neural architecture search on\nimagenet in four gpu hours: A theoretically inspired perspective,” arXiv\npreprint arXiv:2102.11535, 2021.\n[54] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture\nsearch without training,” arXiv preprint arXiv:2006.04647 , 2021.\n[55] B. Acun, M. Murphy, X. Wang, J. Nie, C. Wu, and K. Hazelwood,\n“Understanding training efﬁciency of deep learning recommendation\nmodels at scale,” in Proceedings of the IEEE International Symposium\non High-Performance Computer Architecture , 2021.\n[56] C. Yin, B. Acun, X. Liu, and C.-J. Wu, “TT-Rec: Tensor train\ncompression for deep learning recommendation models,” in Proceedings\nof the Conference on Machine Learning and Systems , 2021.\n[57] W.-C. Kang, D. Z. Cheng, T. Yao, X. Yi, T. Chen, L. Hong, and E. H.\nChi, “Learning to embed categorical features without embedding tables\nfor recommendation,” arXiv preprint arXiv:2010.10784 , 2021.\n[58] A. S. Nemirovskij and D. B. Yudin, Problem complexity and method\nefﬁciency in optimization . Wiley-Interscience, 1983.\n[59] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E.\nDahl, “On empirical comparisons of optimizers for deep learning,” arXiv\npreprint arXiv:1910.05446, 2019.\n[60] P. T. Sivaprasad, F. Mai, T. V ogels, M. Jaggi, and F. Fleuret, “Opti-\nmizer benchmarking needs to account for hyperparameter tuning,” in\nProceedings of the International Conference on Machine Learning ,\n2020.\n[61] P. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,\nA. Tulloch, Y . Jia, and K. He, “Accurate, large minibatch sgd: Training\nimagenet in 1 hour,” arXiv preprint arXiv:1706.02677 , 2017.\n[62] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine\ntranslation,” arXiv preprint arXiv:1806.00187 , 2018.\n[63] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. V ojnovic, “Qsgd:\nCommunication-efﬁcient sgd via gradient quantization and encoding,” in\nProceedings of the Advances in Neural Information Processing Systems ,\nvol. 30, 2017.\n[64] T. V ogels, S. P. Karinireddy, and M. Jaggi, “Powersgd: Practical low-\nrank gradient compression for distributed optimization,” in Proceedings\nof the Advances In Neural Information Processing Systems , vol. 32,\n2019.\n[65] Y . Huang, Y . Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,\nJ. Ngiam, Q. V . Le, Y . Wu,et al., “Gpipe: Efﬁcient training of giant\nneural networks using pipeline parallelism,” in Proceedings of the\nAdvances in neural information processing systems , vol. 32, 2019.\n[66] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: Memory\noptimizations toward training trillion parameter models,” in Proceedings\nof the International Conference for High Performance Computing,\nNetworking, Storage and Analysis , 2020.\n[67] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He, “Deepspeed: System\noptimizations enable training deep learning models with over 100\nbillion parameters,” in Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2020.\n[68] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin,\nC. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,\nT. V . Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R.\nHo, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey,\nA. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar,\nS. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke,\nA. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Na-\ngarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick,\nN. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani,\nC. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing,\nM. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V . Vasudevan,\nR. Walter, W. Wang, E. Wilcox, and D. H. Yoon, “In-datacenter\nperformance analysis of a tensor processing unit,” in Proceedings of the\nACM/IEEE International Symposium on Computer Architecture , 2017.\n[69] J. Hamilton, “AWS Inferentia Machine Learning Processor,” 2018.\n[70] Azure, “New Azure HPC and partner offerings at Supercomputing 19,”\n2019.\n[71] NVIDIA, “GPUs for Virtualization,” 2021.\n[72] A. Spiridonov, “New Cloud TPU VMs make training your ML models\non TPUs easier than ever,” 2021.\n[73] M. Gschwind, T. Kaldewey, and D. Tam, “Optimizing the efﬁciency\nof deep learning through accelerator virtualization,” IBM Journal of\nResearch and Development , vol. 61, no. 4-5, 2017.\n[74] S. Ghodrati, B. H. Ahn, J. Kyung Kim, S. Kinzer, B. R. Yatham,\nN. Alla, H. Sharma, M. Alian, E. Ebrahimi, N. S. Kim, C. Young, and\nH. Esmaeilzadeh, “Planaria: Dynamic architecture ﬁssion for spatial\nmulti-tenant acceleration of deep neural networks,” in Proceedings of\nthe IEEE/ACM International Symposium on Microarchitecture , 2020.\n[75] S.-C. Kao and T. Krishna, “Domain-speciﬁc genetic algorithm for multi-\ntenant dnnaccelerator scheduling,” arXiv preprint arXiv:2104.13997 ,\n2021.\n[76] M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and\nF. Yang, “Analysis of large-scale multi-tenant gpu clusters for dnn\ntraining workloads,” in Proceedings of the USENIX Annual Technical\nConference, 2019.\n[77] P. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives\nfor deep learning applications,” arXiv preprint arXiv:1902.04610 , 2019.\n[78] R. Jain and J. Wullert, “Challenges: Environmental design for pervasive\ncomputing systems,” in Proceedings of the International Conference\non Mobile Computing and Networking , 2002.\n[79] J. Chang, J. Meza, P. Ranganathan, C. Bash, and A. Shah, “Green server\ndesign: Beyond operational energy to sustainability,” in Proceedings of\n12", "sentences": [{"text": "M.", "metadata": {}}, {"text": "Khutornenko, S.", "metadata": {}}, {"text": "Kulkarni, M.", "metadata": {}}, {"text": "Pawlowski, T.", "metadata": {}}, {"text": "Pelkonen, A.", "metadata": {}}, {"text": "Rodrigues,\nR.", "metadata": {}}, {"text": "Tibrewal, V .", "metadata": {}}, {"text": "Venkatesan, and P.", "metadata": {}}, {"text": "Zhang, “Twine: A uniﬁed cluster\nmanagement system for shared infrastructure,” in Proceedings of the\nUSENIX Symposium on Operating Systems Design and Implementation ,\n2020.", "metadata": {}}, {"text": "[39] A.", "metadata": {}}, {"text": "Lacoste, A.", "metadata": {}}, {"text": "Luccioni, V .", "metadata": {}}, {"text": "Schmidt, and T.", "metadata": {}}, {"text": "Dandres, “Quantifying the\ncarbon emissions of machine learning,” Workshop on Tackling Climate\nChange with Machine Learning at NeurIPS 2019 , 2019.", "metadata": {}}, {"text": "[40] P.", "metadata": {}}, {"text": "Henderson, J.", "metadata": {}}, {"text": "Hu, J.", "metadata": {}}, {"text": "Romoff, E.", "metadata": {}}, {"text": "Brunskill, D.", "metadata": {}}, {"text": "Jurafsky, and J.", "metadata": {}}, {"text": "Pineau,\n“Towards the systematic reporting of the energy and carbon footprints\nof machine learning,” CoRR, vol.", "metadata": {}}, {"text": "abs/2002.05651, 2020.", "metadata": {}}, {"text": "[41] E.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Bender, T.", "metadata": {}}, {"text": "Gebru, A.", "metadata": {}}, {"text": "McMillan-Major, and S.", "metadata": {}}, {"text": "Shmitchell, “On\nthe dangers of stochastic parrots: Can language models be too big?,” in\nProceedings of the ACM Conference on Fairness, Accountability, and\nTransparency, 2021.", "metadata": {}}, {"text": "[42] N.", "metadata": {}}, {"text": "Sachdeva, C.-J.", "metadata": {}}, {"text": "Wu, and J.", "metadata": {}}, {"text": "McAuley, “Svp-cf: Selection via proxy\nfor collaborative ﬁltering data,” arXiv preprint arXiv:2107.04984 , 2021.", "metadata": {}}, {"text": "[43] E.", "metadata": {}}, {"text": "Valavi, J.", "metadata": {}}, {"text": "Hestness, N.", "metadata": {}}, {"text": "Ardalani, and M.", "metadata": {}}, {"text": "Iansiti, Time and the Value\nof Data.", "metadata": {}}, {"text": "Working papers, Harvard Business School, 2020.", "metadata": {}}, {"text": "[44] M.", "metadata": {}}, {"text": "Zhao, N.", "metadata": {}}, {"text": "Agarwal, A.", "metadata": {}}, {"text": "Basant, B.", "metadata": {}}, {"text": "Gedik, S.", "metadata": {}}, {"text": "Pan, M.", "metadata": {}}, {"text": "Ozdal,\nR.", "metadata": {}}, {"text": "Komuravelli, J.", "metadata": {}}, {"text": "Pan, T.", "metadata": {}}, {"text": "Bao, H.", "metadata": {}}, {"text": "Lu, S.", "metadata": {}}, {"text": "Narayanan, J.", "metadata": {}}, {"text": "Langman,\nK.", "metadata": {}}, {"text": "Wilfong, H.", "metadata": {}}, {"text": "Rastogi, C.", "metadata": {}}, {"text": "Wu, C.", "metadata": {}}, {"text": "Kozyrakis, and P.", "metadata": {}}, {"text": "Pol, “Understanding\nand co-designing the data ingestion pipeline for industry-scale recsys\ntraining,” CoRR, vol.", "metadata": {}}, {"text": "abs/2108.09373, 2021.", "metadata": {}}, {"text": "[45] R.", "metadata": {}}, {"text": "Turner, D.", "metadata": {}}, {"text": "Eriksson, M.", "metadata": {}}, {"text": "McCourt, J.", "metadata": {}}, {"text": "Kiili, E.", "metadata": {}}, {"text": "Laaksonen, Z.", "metadata": {}}, {"text": "Xu,\nand I.", "metadata": {}}, {"text": "Guyon, “Bayesian optimization is superior to random search for\nmachine learning hyperparameter tuning: Analysis of the black-box\noptimization challenge 2020,” CoRR, vol.", "metadata": {}}, {"text": "abs/2104.10201, 2021.", "metadata": {}}, {"text": "[46] P.", "metadata": {}}, {"text": "Ren, Y .", "metadata": {}}, {"text": "Xiao, X.", "metadata": {}}, {"text": "Chang, P.-y.", "metadata": {}}, {"text": "Huang, Z.", "metadata": {}}, {"text": "Li, X.", "metadata": {}}, {"text": "Chen, and X.", "metadata": {}}, {"text": "Wang,\n“A comprehensive survey of neural architecture search: Challenges and\nsolutions,” ACM Comput.", "metadata": {}}, {"text": "Surv., vol.", "metadata": {}}, {"text": "54, no.", "metadata": {}}, {"text": "4, 2021.", "metadata": {}}, {"text": "[47] Q.", "metadata": {}}, {"text": "Song, D.", "metadata": {}}, {"text": "Cheng, H.", "metadata": {}}, {"text": "Zhou, J.", "metadata": {}}, {"text": "Yang, Y .", "metadata": {}}, {"text": "Tian, and X.", "metadata": {}}, {"text": "Hu, “Towards\nautomated neural interaction discovery for click-through rate prediction,”\nProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , 2020.", "metadata": {}}, {"text": "[48] M.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Joglekar, C.", "metadata": {}}, {"text": "Li, M.", "metadata": {}}, {"text": "Chen, T.", "metadata": {}}, {"text": "Xu, X.", "metadata": {}}, {"text": "Wang, J.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Adams, P.", "metadata": {}}, {"text": "Khaitan,\nJ.", "metadata": {}}, {"text": "Liu, and Q.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "Le, “Neural input search for large scale recommendation\nmodels,” in Proceedings of the ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , 2020.", "metadata": {}}, {"text": "[49] M.", "metadata": {}}, {"text": "Tan and Q.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "Le, “Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks,” arXiv preprint arXiv:1905.11946 , 2020.", "metadata": {}}, {"text": "[50] D.", "metadata": {}}, {"text": "Eriksson, P.", "metadata": {}}, {"text": "I.", "metadata": {}}, {"text": "Chuang, S.", "metadata": {}}, {"text": "Daulton, P.", "metadata": {}}, {"text": "Xia, A.", "metadata": {}}, {"text": "Shrivastava, A.", "metadata": {}}, {"text": "Babu,\nS.", "metadata": {}}, {"text": "Zhao, A.", "metadata": {}}, {"text": "Aly, G.", "metadata": {}}, {"text": "Venkatesh, and M.", "metadata": {}}, {"text": "Balandat, “Latency-aware neural\narchitecture search with multi-objective bayesian optimization,” CoRR,\nvol.", "metadata": {}}, {"text": "abs/2106.11890, 2021.", "metadata": {}}, {"text": "[51] H.", "metadata": {}}, {"text": "Cai, C.", "metadata": {}}, {"text": "Gan, T.", "metadata": {}}, {"text": "Wang, Z.", "metadata": {}}, {"text": "Zhang, and S.", "metadata": {}}, {"text": "Han, “Once-for-all: Train\none network and specialize it for efﬁcient deployment,” arXiv preprint\narXiv:1908.09791, 2020.", "metadata": {}}, {"text": "[52] D.", "metadata": {}}, {"text": "Stamoulis, R.", "metadata": {}}, {"text": "Ding, D.", "metadata": {}}, {"text": "Wang, D.", "metadata": {}}, {"text": "Lymberopoulos, B.", "metadata": {}}, {"text": "Priyantha, J.", "metadata": {}}, {"text": "Liu,\nand D.", "metadata": {}}, {"text": "Marculescu, “Single-path nas: Designing hardware-efﬁcient\nconvnets in less than 4 hours,” arXiv preprint arXiv:1904.02877 , 2019.", "metadata": {}}, {"text": "[53] W.", "metadata": {}}, {"text": "Chen, X.", "metadata": {}}, {"text": "Gong, and Z.", "metadata": {}}, {"text": "Wang, “Neural architecture search on\nimagenet in four gpu hours: A theoretically inspired perspective,” arXiv\npreprint arXiv:2102.11535, 2021.", "metadata": {}}, {"text": "[54] J.", "metadata": {}}, {"text": "Mellor, J.", "metadata": {}}, {"text": "Turner, A.", "metadata": {}}, {"text": "Storkey, and E.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Crowley, “Neural architecture\nsearch without training,” arXiv preprint arXiv:2006.04647 , 2021.", "metadata": {}}, {"text": "[55] B.", "metadata": {}}, {"text": "Acun, M.", "metadata": {}}, {"text": "Murphy, X.", "metadata": {}}, {"text": "Wang, J.", "metadata": {}}, {"text": "Nie, C.", "metadata": {}}, {"text": "Wu, and K.", "metadata": {}}, {"text": "Hazelwood,\n“Understanding training efﬁciency of deep learning recommendation\nmodels at scale,” in Proceedings of the IEEE International Symposium\non High-Performance Computer Architecture , 2021.", "metadata": {}}, {"text": "[56] C.", "metadata": {}}, {"text": "Yin, B.", "metadata": {}}, {"text": "Acun, X.", "metadata": {}}, {"text": "Liu, and C.-J.", "metadata": {}}, {"text": "Wu, “TT-Rec: Tensor train\ncompression for deep learning recommendation models,” in Proceedings\nof the Conference on Machine Learning and Systems , 2021.", "metadata": {}}, {"text": "[57] W.-C.", "metadata": {}}, {"text": "Kang, D.", "metadata": {}}, {"text": "Z.", "metadata": {}}, {"text": "Cheng, T.", "metadata": {}}, {"text": "Yao, X.", "metadata": {}}, {"text": "Yi, T.", "metadata": {}}, {"text": "Chen, L.", "metadata": {}}, {"text": "Hong, and E.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Chi, “Learning to embed categorical features without embedding tables\nfor recommendation,” arXiv preprint arXiv:2010.10784 , 2021.", "metadata": {}}, {"text": "[58] A.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Nemirovskij and D.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Yudin, Problem complexity and method\nefﬁciency in optimization .", "metadata": {}}, {"text": "Wiley-Interscience, 1983.", "metadata": {}}, {"text": "[59] D.", "metadata": {}}, {"text": "Choi, C.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Shallue, Z.", "metadata": {}}, {"text": "Nado, J.", "metadata": {}}, {"text": "Lee, C.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Maddison, and G.", "metadata": {}}, {"text": "E.", "metadata": {}}, {"text": "Dahl, “On empirical comparisons of optimizers for deep learning,” arXiv\npreprint arXiv:1910.05446, 2019.", "metadata": {}}, {"text": "[60] P.", "metadata": {}}, {"text": "T.", "metadata": {}}, {"text": "Sivaprasad, F.", "metadata": {}}, {"text": "Mai, T.", "metadata": {}}, {"text": "V ogels, M.", "metadata": {}}, {"text": "Jaggi, and F.", "metadata": {}}, {"text": "Fleuret, “Opti-\nmizer benchmarking needs to account for hyperparameter tuning,” in\nProceedings of the International Conference on Machine Learning ,\n2020.", "metadata": {}}, {"text": "[61] P.", "metadata": {}}, {"text": "Goyal, P.", "metadata": {}}, {"text": "Doll´ar, R.", "metadata": {}}, {"text": "Girshick, P.", "metadata": {}}, {"text": "Noordhuis, L.", "metadata": {}}, {"text": "Wesolowski, A.", "metadata": {}}, {"text": "Kyrola,\nA.", "metadata": {}}, {"text": "Tulloch, Y .", "metadata": {}}, {"text": "Jia, and K.", "metadata": {}}, {"text": "He, “Accurate, large minibatch sgd: Training\nimagenet in 1 hour,” arXiv preprint arXiv:1706.02677 , 2017.", "metadata": {}}, {"text": "[62] M.", "metadata": {}}, {"text": "Ott, S.", "metadata": {}}, {"text": "Edunov, D.", "metadata": {}}, {"text": "Grangier, and M.", "metadata": {}}, {"text": "Auli, “Scaling neural machine\ntranslation,” arXiv preprint arXiv:1806.00187 , 2018.", "metadata": {}}, {"text": "[63] D.", "metadata": {}}, {"text": "Alistarh, D.", "metadata": {}}, {"text": "Grubic, J.", "metadata": {}}, {"text": "Li, R.", "metadata": {}}, {"text": "Tomioka, and M.", "metadata": {}}, {"text": "V ojnovic, “Qsgd:\nCommunication-efﬁcient sgd via gradient quantization and encoding,” in\nProceedings of the Advances in Neural Information Processing Systems ,\nvol.", "metadata": {}}, {"text": "30, 2017.", "metadata": {}}, {"text": "[64] T.", "metadata": {}}, {"text": "V ogels, S.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Karinireddy, and M.", "metadata": {}}, {"text": "Jaggi, “Powersgd: Practical low-\nrank gradient compression for distributed optimization,” in Proceedings\nof the Advances In Neural Information Processing Systems , vol.", "metadata": {}}, {"text": "32,\n2019.", "metadata": {}}, {"text": "[65] Y .", "metadata": {}}, {"text": "Huang, Y .", "metadata": {}}, {"text": "Cheng, A.", "metadata": {}}, {"text": "Bapna, O.", "metadata": {}}, {"text": "Firat, D.", "metadata": {}}, {"text": "Chen, M.", "metadata": {}}, {"text": "Chen, H.", "metadata": {}}, {"text": "Lee,\nJ.", "metadata": {}}, {"text": "Ngiam, Q.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "Le, Y .", "metadata": {}}, {"text": "Wu,et al., “Gpipe: Efﬁcient training of giant\nneural networks using pipeline parallelism,” in Proceedings of the\nAdvances in neural information processing systems , vol.", "metadata": {}}, {"text": "32, 2019.", "metadata": {}}, {"text": "[66] S.", "metadata": {}}, {"text": "Rajbhandari, J.", "metadata": {}}, {"text": "Rasley, O.", "metadata": {}}, {"text": "Ruwase, and Y .", "metadata": {}}, {"text": "He, “Zero: Memory\noptimizations toward training trillion parameter models,” in Proceedings\nof the International Conference for High Performance Computing,\nNetworking, Storage and Analysis , 2020.", "metadata": {}}, {"text": "[67] J.", "metadata": {}}, {"text": "Rasley, S.", "metadata": {}}, {"text": "Rajbhandari, O.", "metadata": {}}, {"text": "Ruwase, and Y .", "metadata": {}}, {"text": "He, “Deepspeed: System\noptimizations enable training deep learning models with over 100\nbillion parameters,” in Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2020.", "metadata": {}}, {"text": "[68] N.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Jouppi, C.", "metadata": {}}, {"text": "Young, N.", "metadata": {}}, {"text": "Patil, D.", "metadata": {}}, {"text": "Patterson, G.", "metadata": {}}, {"text": "Agrawal, R.", "metadata": {}}, {"text": "Bajwa,\nS.", "metadata": {}}, {"text": "Bates, S.", "metadata": {}}, {"text": "Bhatia, N.", "metadata": {}}, {"text": "Boden, A.", "metadata": {}}, {"text": "Borchers, R.", "metadata": {}}, {"text": "Boyle, P.-l.", "metadata": {}}, {"text": "Cantin,\nC.", "metadata": {}}, {"text": "Chao, C.", "metadata": {}}, {"text": "Clark, J.", "metadata": {}}, {"text": "Coriell, M.", "metadata": {}}, {"text": "Daley, M.", "metadata": {}}, {"text": "Dau, J.", "metadata": {}}, {"text": "Dean, B.", "metadata": {}}, {"text": "Gelb,\nT.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "Ghaemmaghami, R.", "metadata": {}}, {"text": "Gottipati, W.", "metadata": {}}, {"text": "Gulland, R.", "metadata": {}}, {"text": "Hagmann, C.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Ho, D.", "metadata": {}}, {"text": "Hogberg, J.", "metadata": {}}, {"text": "Hu, R.", "metadata": {}}, {"text": "Hundt, D.", "metadata": {}}, {"text": "Hurt, J.", "metadata": {}}, {"text": "Ibarz, A.", "metadata": {}}, {"text": "Jaffey,\nA.", "metadata": {}}, {"text": "Jaworski, A.", "metadata": {}}, {"text": "Kaplan, H.", "metadata": {}}, {"text": "Khaitan, D.", "metadata": {}}, {"text": "Killebrew, A.", "metadata": {}}, {"text": "Koch, N.", "metadata": {}}, {"text": "Kumar,\nS.", "metadata": {}}, {"text": "Lacy, J.", "metadata": {}}, {"text": "Laudon, J.", "metadata": {}}, {"text": "Law, D.", "metadata": {}}, {"text": "Le, C.", "metadata": {}}, {"text": "Leary, Z.", "metadata": {}}, {"text": "Liu, K.", "metadata": {}}, {"text": "Lucke,\nA.", "metadata": {}}, {"text": "Lundin, G.", "metadata": {}}, {"text": "MacKean, A.", "metadata": {}}, {"text": "Maggiore, M.", "metadata": {}}, {"text": "Mahony, K.", "metadata": {}}, {"text": "Miller, R.", "metadata": {}}, {"text": "Na-\ngarajan, R.", "metadata": {}}, {"text": "Narayanaswami, R.", "metadata": {}}, {"text": "Ni, K.", "metadata": {}}, {"text": "Nix, T.", "metadata": {}}, {"text": "Norrie, M.", "metadata": {}}, {"text": "Omernick,\nN.", "metadata": {}}, {"text": "Penukonda, A.", "metadata": {}}, {"text": "Phelps, J.", "metadata": {}}, {"text": "Ross, M.", "metadata": {}}, {"text": "Ross, A.", "metadata": {}}, {"text": "Salek, E.", "metadata": {}}, {"text": "Samadiani,\nC.", "metadata": {}}, {"text": "Severn, G.", "metadata": {}}, {"text": "Sizikov, M.", "metadata": {}}, {"text": "Snelham, J.", "metadata": {}}, {"text": "Souter, D.", "metadata": {}}, {"text": "Steinberg, A.", "metadata": {}}, {"text": "Swing,\nM.", "metadata": {}}, {"text": "Tan, G.", "metadata": {}}, {"text": "Thorson, B.", "metadata": {}}, {"text": "Tian, H.", "metadata": {}}, {"text": "Toma, E.", "metadata": {}}, {"text": "Tuttle, V .", "metadata": {}}, {"text": "Vasudevan,\nR.", "metadata": {}}, {"text": "Walter, W.", "metadata": {}}, {"text": "Wang, E.", "metadata": {}}, {"text": "Wilcox, and D.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Yoon, “In-datacenter\nperformance analysis of a tensor processing unit,” in Proceedings of the\nACM/IEEE International Symposium on Computer Architecture , 2017.", "metadata": {}}, {"text": "[69] J.", "metadata": {}}, {"text": "Hamilton, “AWS Inferentia Machine Learning Processor,” 2018.", "metadata": {}}, {"text": "[70] Azure, “New Azure HPC and partner offerings at Supercomputing 19,”\n2019.", "metadata": {}}, {"text": "[71] NVIDIA, “GPUs for Virtualization,” 2021.", "metadata": {}}, {"text": "[72] A.", "metadata": {}}, {"text": "Spiridonov, “New Cloud TPU VMs make training your ML models\non TPUs easier than ever,” 2021.", "metadata": {}}, {"text": "[73] M.", "metadata": {}}, {"text": "Gschwind, T.", "metadata": {}}, {"text": "Kaldewey, and D.", "metadata": {}}, {"text": "Tam, “Optimizing the efﬁciency\nof deep learning through accelerator virtualization,” IBM Journal of\nResearch and Development , vol.", "metadata": {}}, {"text": "61, no.", "metadata": {}}, {"text": "4-5, 2017.", "metadata": {}}, {"text": "[74] S.", "metadata": {}}, {"text": "Ghodrati, B.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Ahn, J.", "metadata": {}}, {"text": "Kyung Kim, S.", "metadata": {}}, {"text": "Kinzer, B.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Yatham,\nN.", "metadata": {}}, {"text": "Alla, H.", "metadata": {}}, {"text": "Sharma, M.", "metadata": {}}, {"text": "Alian, E.", "metadata": {}}, {"text": "Ebrahimi, N.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Kim, C.", "metadata": {}}, {"text": "Young, and\nH.", "metadata": {}}, {"text": "Esmaeilzadeh, “Planaria: Dynamic architecture ﬁssion for spatial\nmulti-tenant acceleration of deep neural networks,” in Proceedings of\nthe IEEE/ACM International Symposium on Microarchitecture , 2020.", "metadata": {}}, {"text": "[75] S.-C.", "metadata": {}}, {"text": "Kao and T.", "metadata": {}}, {"text": "Krishna, “Domain-speciﬁc genetic algorithm for multi-\ntenant dnnaccelerator scheduling,” arXiv preprint arXiv:2104.13997 ,\n2021.", "metadata": {}}, {"text": "[76] M.", "metadata": {}}, {"text": "Jeon, S.", "metadata": {}}, {"text": "Venkataraman, A.", "metadata": {}}, {"text": "Phanishayee, u.", "metadata": {}}, {"text": "Qian, W.", "metadata": {}}, {"text": "Xiao, and\nF.", "metadata": {}}, {"text": "Yang, “Analysis of large-scale multi-tenant gpu clusters for dnn\ntraining workloads,” in Proceedings of the USENIX Annual Technical\nConference, 2019.", "metadata": {}}, {"text": "[77] P.", "metadata": {}}, {"text": "Yu and M.", "metadata": {}}, {"text": "Chowdhury, “Salus: Fine-grained gpu sharing primitives\nfor deep learning applications,” arXiv preprint arXiv:1902.04610 , 2019.", "metadata": {}}, {"text": "[78] R.", "metadata": {}}, {"text": "Jain and J.", "metadata": {}}, {"text": "Wullert, “Challenges: Environmental design for pervasive\ncomputing systems,” in Proceedings of the International Conference\non Mobile Computing and Networking , 2002.", "metadata": {}}, {"text": "[79] J.", "metadata": {}}, {"text": "Chang, J.", "metadata": {}}, {"text": "Meza, P.", "metadata": {}}, {"text": "Ranganathan, C.", "metadata": {}}, {"text": "Bash, and A.", "metadata": {}}, {"text": "Shah, “Green server\ndesign: Beyond operational energy to sustainability,” in Proceedings of\n12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "the International Conference on Power Aware Computing and Systems ,\n2010.\n[80] M. Garcia Bardon, P. Wuytens, L.-A. Ragnarsson, G. Mirabelli, D. Jang,\nG. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais, “DTCO\nincluding sustainability: Power-performance-area-cost-environmental\nscore (PPACE) analysis for logic technologies,” in Proceedings of the\nIEEE International Electron Devices Meeting , 2020.\n[81] A. Putnam, A. M. Caulﬁeld, E. S. Chung, D. Chiou, K. Constantinides,\nJ. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray, M. Hasel-\nman, S. Hauck, S. Heil, A. Hormati, J.-Y . Kim, S. Lanka, J. Larus,\nE. Peterson, S. Pope, A. Smith, J. Thong, P. Y . Xiao, and D. Burger,\n“A reconﬁgurable fabric for accelerating large-scale datacenter services,”\nIEEE Micro, 2015.\n[82] Y .-H. Chen, J. Emer, and V . Sze, “Eyeriss: A spatial architecture\nfor energy-efﬁcient dataﬂow for convolutional neural networks,” in\nProceedings of the ACM/IEEE International Symposium on Computer\nArchitecture, 2016.\n[83] A. Radovanovic, R. Koningstein, I. Schneider, B. Chen, A. Duarte,\nB. Roy, D. Xiao, M. Haridasan, P. Hung, N. Care, et al., “Carbon-aware\ncomputing for datacenters,” arXiv preprint arXiv:2106.11750 , 2021.\n[84] H. Cai, C. Gan, L. Zhu, and S. Han, “Tinytl: Reduce memory, not param-\neters for efﬁcient on-device learning,” arXiv preprint arXiv:2007.11622,\n2020.\n[85] K. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and\nD. Ramage, “Federated evaluation of on-device personalization,” arXiv\npreprint arXiv:1910.10252, 2019.\n[86] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,\nV . Ivanov, C. Kiddon, J. Kone ˇcn`y, S. Mazzocchi, H. B. McMahan,\net al. , “Towards federated learning at scale: System design,” arXiv\npreprint arXiv:1902.01046, 2019.\n[87] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-\nstein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for\nmobile keyboard prediction,” arXiv preprint arXiv:1811.03604 , 2018.\n[88] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage,\nand F. Beaufays, “Applied federated learning: Improving google\nkeyboard query suggestions,” arXiv preprint arXiv:1812.02903 , 2018.\n[89] S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated\nlearning for emoji prediction in a mobile keyboard,” arXiv preprint\narXiv:1906.04329, 2019.\n[90] D. Huba, J. Nguyen, K. Malik, R. Zhu, M. Rabbat, A. Yousefpour,\nC.-J. Wu, H. Zhan, P. Ustinov, H. Srinivas, K. Wang, A. Shoumikhin,\nJ. Min, and M. Malek, “Papaya: Practical, private, and scalable federated\nlearning,” arXiv:2111.04877, 2021.\n[91] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar,\nJ. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan,\nI. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray,\nM. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V . Cartillier, S. Crane,\nT. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu,\nC. Fuegen, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y . Huang,\nW. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y . Li,\nZ. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu,\nW. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram,\nA. Southerland, Y . Sugano, R. Tao, M. V o, Y . Wang, X. Wu, T. Yagi,\nY . Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, B. Ghanem,\nV . K. Ithapu, C. V . Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe,\nA. Oliva, H. S. Park, J. M. Rehg, Y . Sato, J. Shi, M. Z. Shou, A. Torralba,\nL. Torresani, M. Yan, and J. Malik, “Ego4d: Around the world in 3,000\nhours of egocentric video,” arXiv:2110.07058, 2021.\n[92] Y . G. Kim and C.-J. Wu, “Autoﬂ: Enabling heterogeneity-aware\nenergy efﬁcient federated learning,” in Proceedings of the IEEE/ACM\nInternational Symposium on Microarchitecture , 2021.\n[93] Y . Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and\nL. Tang, “Neurosurgeon: Collaborative intelligence between the cloud\nand mobile edge,” in Proceedings of the International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2017.\n[94] Y . G. Kim and C.-J. Wu, “Autoscale: Energy efﬁciency optimization for\nstochastic edge inference using reinforcement learning,” in Proceedings\nof the IEEE/ACM International Symposium on Microarchitecture , 2020.\n[95] T.-J. Yang, Y .-H. Chen, and V . Sze, “Designing energy-efﬁcient convolu-\ntional neural networks using energy-aware pruning,” arXiv:1611.05128,\n2017.\n[96] D. Stamoulis, T.-W. R. Chin, A. K. Prakash, H. Fang, S. Sajja,\nM. Bognar, and D. Marculescu, “Designing adaptive neural networks\nfor energy-constrained image classiﬁcation,” in Proceedings of the\nInternational Conference on Computer-Aided Design , 2018.\n[97] C. Gao, A. Gutierrez, M. Rajan, R. G. Dreslinski, T. Mudge, and C.-\nJ. Wu, “A study of mobile device utilization,” in Proceedings of the\nIEEE International Symposium on Performance Analysis of Systems\nand Software, 2015.\n[98] V . Schmidt, K. Goyal, A. Joshi, B. Feld, L. Conell, N. Laskaris, D. Blank,\nJ. Wilson, S. Friedler, and S. Luccioni, “CodeCarbon: Estimate and\nTrack Carbon Emissions from Machine Learning Computing,” 2021.\n[99] K. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, “Energy usage\nreports: Environmental awareness as part of algorithmic accountability,”\nWorkshop on Tackling Climate Change with Machine Learning at\nNeurIPS 2019, 2019.\n[100] D. Kiela, M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu,\nB. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush,\nS. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and\nA. Williams, “Dynabench: Rethinking benchmarking in NLP,” arXiv\npreprint arXiv:2104.14337, 2021.\n[101] D. Hernandez and T. B. Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.\n[102] P. Mattson, V . J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter,\nP. Micikevicius, D. Patterson, G. Schmuelling, H. Tang, G.-Y . Wei, and\nC.-J. Wu, “Mlperf: An industry standard benchmark suite for machine\nlearning performance,” IEEE Micro, vol. 40, no. 2, pp. 8–16, 2020.\n[103] V . J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, and C.-J.\nWu, “The vision behind mlperf: Understanding ai inference performance,”\nIEEE Micro, vol. 41, no. 3, pp. 10–18, 2021.\n[104] V . J. Reddi, D. Kanter, P. Mattson, J. Duke, T. Nguyen, R. Chukka,\nK. Shiring, K.-S. Tan, M. Charlebois, W. Chou, M. El-Khamy, J. Hong,\nM. Buch, C. Trinh, T. Atta-fosu, F. Cakir, M. Charkhabi, X. Chen,\nJ. Chiang, D. Dexter, W. Heo, G. Schmuelling, M. Shabani, and D. Zika,\n“Mlperf mobile inference benchmark,” arXiv:2012.02328, 2021.\n[105] P. Mattson, C. Cheng, G. Diamos, C. Coleman, P. Micikevicius,\nD. Patterson, H. Tang, G.-Y . Wei, P. Bailis, V . Bittorf, D. Brooks,\nD. Chen, D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang,\nD. Kang, D. Kanter, N. Kumar, J. Liao, D. Narayanan, T. Oguntebi,\nG. Pekhimenko, L. Pentecost, V . Janapa Reddi, T. Robie, T. St John, C.-\nJ. Wu, L. Xu, C. Young, and M. Zaharia, “Mlperf training benchmark,”\nin Proceedings of Machine Learning and Systems , vol. 2, 2020.\n[106] V . J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J.\nWu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka,\nC. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S.\nGardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar,\nD. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius,\nC. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao,\nF. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada,\nB. Yu, G. Yuan, A. Zhong, P. Zhang, and Y . Zhou, “Mlperf inference\nbenchmark,” in Proceedings of the ACM/IEEE Annual International\nSymposium on Computer Architecture , 2020.\n[107] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchin-\nson, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model\nreporting,” Proceedings of the Conference on Fairness, Accountability,\nand Transparency, 2019.\n[108] C.-J. Wu, S. Manne, P. Ranganathan, S. Bird, and S. Greenstein,\n“Socio-technological challenges and opportunities: Paths forward,” arXiv\npreprint arXiv:2108.06738, 2021.\n[109] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” arXiv\npreprint arXiv:1907.10597, 2019.\n[110] K. Maeng, S. Bharuka, I. Gao, M. C. Jeffrey, V . Saraph, B.-Y . Su,\nC. Trippel, J. Yang, M. Rabbat, B. Lucia, and C.-J. Wu, “Cpr:\nUnderstanding and improving failure tolerant training for deep learning\nrecommendation with partial recovery,” inProceedings of the Conference\non Machine Learning and Systems , 2021.\n[111] A. Eisenman, K. K. Matam, S. Ingram, D. Mudigere, R. Krishnamoorthi,\nK. Nair, M. Smelyanskiy, and M. Annavaram, “Check-n-run: A\ncheckpointing system for training deep learning recommendation\nmodels,” arXiv preprint arXiv:2010.08679 , 2021.\n[112] H. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy,\nB. Muthiah, and S. Sankar, “Silent data corruptions at scale,” arXiv\npreprint arXiv:2102.11245, 2021.\n13", "sentences": [{"text": "the International Conference on Power Aware Computing and Systems ,\n2010.", "metadata": {}}, {"text": "[80] M.", "metadata": {}}, {"text": "Garcia Bardon, P.", "metadata": {}}, {"text": "Wuytens, L.-A.", "metadata": {}}, {"text": "Ragnarsson, G.", "metadata": {}}, {"text": "Mirabelli, D.", "metadata": {}}, {"text": "Jang,\nG.", "metadata": {}}, {"text": "Willems, A.", "metadata": {}}, {"text": "Mallik, A.", "metadata": {}}, {"text": "Spessot, J.", "metadata": {}}, {"text": "Ryckaert, and B.", "metadata": {}}, {"text": "Parvais, “DTCO\nincluding sustainability: Power-performance-area-cost-environmental\nscore (PPACE) analysis for logic technologies,” in Proceedings of the\nIEEE International Electron Devices Meeting , 2020.", "metadata": {}}, {"text": "[81] A.", "metadata": {}}, {"text": "Putnam, A.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Caulﬁeld, E.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Chung, D.", "metadata": {}}, {"text": "Chiou, K.", "metadata": {}}, {"text": "Constantinides,\nJ.", "metadata": {}}, {"text": "Demme, H.", "metadata": {}}, {"text": "Esmaeilzadeh, J.", "metadata": {}}, {"text": "Fowers, G.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Gopal, J.", "metadata": {}}, {"text": "Gray, M.", "metadata": {}}, {"text": "Hasel-\nman, S.", "metadata": {}}, {"text": "Hauck, S.", "metadata": {}}, {"text": "Heil, A.", "metadata": {}}, {"text": "Hormati, J.-Y .", "metadata": {}}, {"text": "Kim, S.", "metadata": {}}, {"text": "Lanka, J.", "metadata": {}}, {"text": "Larus,\nE.", "metadata": {}}, {"text": "Peterson, S.", "metadata": {}}, {"text": "Pope, A.", "metadata": {}}, {"text": "Smith, J.", "metadata": {}}, {"text": "Thong, P.", "metadata": {}}, {"text": "Y .", "metadata": {}}, {"text": "Xiao, and D.", "metadata": {}}, {"text": "Burger,\n“A reconﬁgurable fabric for accelerating large-scale datacenter services,”\nIEEE Micro, 2015.", "metadata": {}}, {"text": "[82] Y .-H.", "metadata": {}}, {"text": "Chen, J.", "metadata": {}}, {"text": "Emer, and V .", "metadata": {}}, {"text": "Sze, “Eyeriss: A spatial architecture\nfor energy-efﬁcient dataﬂow for convolutional neural networks,” in\nProceedings of the ACM/IEEE International Symposium on Computer\nArchitecture, 2016.", "metadata": {}}, {"text": "[83] A.", "metadata": {}}, {"text": "Radovanovic, R.", "metadata": {}}, {"text": "Koningstein, I.", "metadata": {}}, {"text": "Schneider, B.", "metadata": {}}, {"text": "Chen, A.", "metadata": {}}, {"text": "Duarte,\nB.", "metadata": {}}, {"text": "Roy, D.", "metadata": {}}, {"text": "Xiao, M.", "metadata": {}}, {"text": "Haridasan, P.", "metadata": {}}, {"text": "Hung, N.", "metadata": {}}, {"text": "Care, et al., “Carbon-aware\ncomputing for datacenters,” arXiv preprint arXiv:2106.11750 , 2021.", "metadata": {}}, {"text": "[84] H.", "metadata": {}}, {"text": "Cai, C.", "metadata": {}}, {"text": "Gan, L.", "metadata": {}}, {"text": "Zhu, and S.", "metadata": {}}, {"text": "Han, “Tinytl: Reduce memory, not param-\neters for efﬁcient on-device learning,” arXiv preprint arXiv:2007.11622,\n2020.", "metadata": {}}, {"text": "[85] K.", "metadata": {}}, {"text": "Wang, R.", "metadata": {}}, {"text": "Mathews, C.", "metadata": {}}, {"text": "Kiddon, H.", "metadata": {}}, {"text": "Eichner, F.", "metadata": {}}, {"text": "Beaufays, and\nD.", "metadata": {}}, {"text": "Ramage, “Federated evaluation of on-device personalization,” arXiv\npreprint arXiv:1910.10252, 2019.", "metadata": {}}, {"text": "[86] K.", "metadata": {}}, {"text": "Bonawitz, H.", "metadata": {}}, {"text": "Eichner, W.", "metadata": {}}, {"text": "Grieskamp, D.", "metadata": {}}, {"text": "Huba, A.", "metadata": {}}, {"text": "Ingerman,\nV .", "metadata": {}}, {"text": "Ivanov, C.", "metadata": {}}, {"text": "Kiddon, J.", "metadata": {}}, {"text": "Kone ˇcn`y, S.", "metadata": {}}, {"text": "Mazzocchi, H.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "McMahan,\net al.", "metadata": {}}, {"text": ", “Towards federated learning at scale: System design,” arXiv\npreprint arXiv:1902.01046, 2019.", "metadata": {}}, {"text": "[87] A.", "metadata": {}}, {"text": "Hard, K.", "metadata": {}}, {"text": "Rao, R.", "metadata": {}}, {"text": "Mathews, S.", "metadata": {}}, {"text": "Ramaswamy, F.", "metadata": {}}, {"text": "Beaufays, S.", "metadata": {}}, {"text": "Augen-\nstein, H.", "metadata": {}}, {"text": "Eichner, C.", "metadata": {}}, {"text": "Kiddon, and D.", "metadata": {}}, {"text": "Ramage, “Federated learning for\nmobile keyboard prediction,” arXiv preprint arXiv:1811.03604 , 2018.", "metadata": {}}, {"text": "[88] T.", "metadata": {}}, {"text": "Yang, G.", "metadata": {}}, {"text": "Andrew, H.", "metadata": {}}, {"text": "Eichner, H.", "metadata": {}}, {"text": "Sun, W.", "metadata": {}}, {"text": "Li, N.", "metadata": {}}, {"text": "Kong, D.", "metadata": {}}, {"text": "Ramage,\nand F.", "metadata": {}}, {"text": "Beaufays, “Applied federated learning: Improving google\nkeyboard query suggestions,” arXiv preprint arXiv:1812.02903 , 2018.", "metadata": {}}, {"text": "[89] S.", "metadata": {}}, {"text": "Ramaswamy, R.", "metadata": {}}, {"text": "Mathews, K.", "metadata": {}}, {"text": "Rao, and F.", "metadata": {}}, {"text": "Beaufays, “Federated\nlearning for emoji prediction in a mobile keyboard,” arXiv preprint\narXiv:1906.04329, 2019.", "metadata": {}}, {"text": "[90] D.", "metadata": {}}, {"text": "Huba, J.", "metadata": {}}, {"text": "Nguyen, K.", "metadata": {}}, {"text": "Malik, R.", "metadata": {}}, {"text": "Zhu, M.", "metadata": {}}, {"text": "Rabbat, A.", "metadata": {}}, {"text": "Yousefpour,\nC.-J.", "metadata": {}}, {"text": "Wu, H.", "metadata": {}}, {"text": "Zhan, P.", "metadata": {}}, {"text": "Ustinov, H.", "metadata": {}}, {"text": "Srinivas, K.", "metadata": {}}, {"text": "Wang, A.", "metadata": {}}, {"text": "Shoumikhin,\nJ.", "metadata": {}}, {"text": "Min, and M.", "metadata": {}}, {"text": "Malek, “Papaya: Practical, private, and scalable federated\nlearning,” arXiv:2111.04877, 2021.", "metadata": {}}, {"text": "[91] K.", "metadata": {}}, {"text": "Grauman, A.", "metadata": {}}, {"text": "Westbury, E.", "metadata": {}}, {"text": "Byrne, Z.", "metadata": {}}, {"text": "Chavis, A.", "metadata": {}}, {"text": "Furnari, R.", "metadata": {}}, {"text": "Girdhar,\nJ.", "metadata": {}}, {"text": "Hamburger, H.", "metadata": {}}, {"text": "Jiang, M.", "metadata": {}}, {"text": "Liu, X.", "metadata": {}}, {"text": "Liu, M.", "metadata": {}}, {"text": "Martin, T.", "metadata": {}}, {"text": "Nagarajan,\nI.", "metadata": {}}, {"text": "Radosavovic, S.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Ramakrishnan, F.", "metadata": {}}, {"text": "Ryan, J.", "metadata": {}}, {"text": "Sharma, M.", "metadata": {}}, {"text": "Wray,\nM.", "metadata": {}}, {"text": "Xu, E.", "metadata": {}}, {"text": "Z.", "metadata": {}}, {"text": "Xu, C.", "metadata": {}}, {"text": "Zhao, S.", "metadata": {}}, {"text": "Bansal, D.", "metadata": {}}, {"text": "Batra, V .", "metadata": {}}, {"text": "Cartillier, S.", "metadata": {}}, {"text": "Crane,\nT.", "metadata": {}}, {"text": "Do, M.", "metadata": {}}, {"text": "Doulaty, A.", "metadata": {}}, {"text": "Erapalli, C.", "metadata": {}}, {"text": "Feichtenhofer, A.", "metadata": {}}, {"text": "Fragomeni, Q.", "metadata": {}}, {"text": "Fu,\nC.", "metadata": {}}, {"text": "Fuegen, A.", "metadata": {}}, {"text": "Gebreselasie, C.", "metadata": {}}, {"text": "Gonzalez, J.", "metadata": {}}, {"text": "Hillis, X.", "metadata": {}}, {"text": "Huang, Y .", "metadata": {}}, {"text": "Huang,\nW.", "metadata": {}}, {"text": "Jia, W.", "metadata": {}}, {"text": "Khoo, J.", "metadata": {}}, {"text": "Kolar, S.", "metadata": {}}, {"text": "Kottur, A.", "metadata": {}}, {"text": "Kumar, F.", "metadata": {}}, {"text": "Landini, C.", "metadata": {}}, {"text": "Li, Y .", "metadata": {}}, {"text": "Li,\nZ.", "metadata": {}}, {"text": "Li, K.", "metadata": {}}, {"text": "Mangalam, R.", "metadata": {}}, {"text": "Modhugu, J.", "metadata": {}}, {"text": "Munro, T.", "metadata": {}}, {"text": "Murrell, T.", "metadata": {}}, {"text": "Nishiyasu,\nW.", "metadata": {}}, {"text": "Price, P.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Puentes, M.", "metadata": {}}, {"text": "Ramazanova, L.", "metadata": {}}, {"text": "Sari, K.", "metadata": {}}, {"text": "Somasundaram,\nA.", "metadata": {}}, {"text": "Southerland, Y .", "metadata": {}}, {"text": "Sugano, R.", "metadata": {}}, {"text": "Tao, M.", "metadata": {}}, {"text": "V o, Y .", "metadata": {}}, {"text": "Wang, X.", "metadata": {}}, {"text": "Wu, T.", "metadata": {}}, {"text": "Yagi,\nY .", "metadata": {}}, {"text": "Zhu, P.", "metadata": {}}, {"text": "Arbelaez, D.", "metadata": {}}, {"text": "Crandall, D.", "metadata": {}}, {"text": "Damen, G.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Farinella, B.", "metadata": {}}, {"text": "Ghanem,\nV .", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Ithapu, C.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "Jawahar, H.", "metadata": {}}, {"text": "Joo, K.", "metadata": {}}, {"text": "Kitani, H.", "metadata": {}}, {"text": "Li, R.", "metadata": {}}, {"text": "Newcombe,\nA.", "metadata": {}}, {"text": "Oliva, H.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Park, J.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Rehg, Y .", "metadata": {}}, {"text": "Sato, J.", "metadata": {}}, {"text": "Shi, M.", "metadata": {}}, {"text": "Z.", "metadata": {}}, {"text": "Shou, A.", "metadata": {}}, {"text": "Torralba,\nL.", "metadata": {}}, {"text": "Torresani, M.", "metadata": {}}, {"text": "Yan, and J.", "metadata": {}}, {"text": "Malik, “Ego4d: Around the world in 3,000\nhours of egocentric video,” arXiv:2110.07058, 2021.", "metadata": {}}, {"text": "[92] Y .", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "Kim and C.-J.", "metadata": {}}, {"text": "Wu, “Autoﬂ: Enabling heterogeneity-aware\nenergy efﬁcient federated learning,” in Proceedings of the IEEE/ACM\nInternational Symposium on Microarchitecture , 2021.", "metadata": {}}, {"text": "[93] Y .", "metadata": {}}, {"text": "Kang, J.", "metadata": {}}, {"text": "Hauswald, C.", "metadata": {}}, {"text": "Gao, A.", "metadata": {}}, {"text": "Rovinski, T.", "metadata": {}}, {"text": "Mudge, J.", "metadata": {}}, {"text": "Mars, and\nL.", "metadata": {}}, {"text": "Tang, “Neurosurgeon: Collaborative intelligence between the cloud\nand mobile edge,” in Proceedings of the International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2017.", "metadata": {}}, {"text": "[94] Y .", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "Kim and C.-J.", "metadata": {}}, {"text": "Wu, “Autoscale: Energy efﬁciency optimization for\nstochastic edge inference using reinforcement learning,” in Proceedings\nof the IEEE/ACM International Symposium on Microarchitecture , 2020.", "metadata": {}}, {"text": "[95] T.-J.", "metadata": {}}, {"text": "Yang, Y .-H.", "metadata": {}}, {"text": "Chen, and V .", "metadata": {}}, {"text": "Sze, “Designing energy-efﬁcient convolu-\ntional neural networks using energy-aware pruning,” arXiv:1611.05128,\n2017.", "metadata": {}}, {"text": "[96] D.", "metadata": {}}, {"text": "Stamoulis, T.-W.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Chin, A.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Prakash, H.", "metadata": {}}, {"text": "Fang, S.", "metadata": {}}, {"text": "Sajja,\nM.", "metadata": {}}, {"text": "Bognar, and D.", "metadata": {}}, {"text": "Marculescu, “Designing adaptive neural networks\nfor energy-constrained image classiﬁcation,” in Proceedings of the\nInternational Conference on Computer-Aided Design , 2018.", "metadata": {}}, {"text": "[97] C.", "metadata": {}}, {"text": "Gao, A.", "metadata": {}}, {"text": "Gutierrez, M.", "metadata": {}}, {"text": "Rajan, R.", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "Dreslinski, T.", "metadata": {}}, {"text": "Mudge, and C.-\nJ.", "metadata": {}}, {"text": "Wu, “A study of mobile device utilization,” in Proceedings of the\nIEEE International Symposium on Performance Analysis of Systems\nand Software, 2015.", "metadata": {}}, {"text": "[98] V .", "metadata": {}}, {"text": "Schmidt, K.", "metadata": {}}, {"text": "Goyal, A.", "metadata": {}}, {"text": "Joshi, B.", "metadata": {}}, {"text": "Feld, L.", "metadata": {}}, {"text": "Conell, N.", "metadata": {}}, {"text": "Laskaris, D.", "metadata": {}}, {"text": "Blank,\nJ.", "metadata": {}}, {"text": "Wilson, S.", "metadata": {}}, {"text": "Friedler, and S.", "metadata": {}}, {"text": "Luccioni, “CodeCarbon: Estimate and\nTrack Carbon Emissions from Machine Learning Computing,” 2021.", "metadata": {}}, {"text": "[99] K.", "metadata": {}}, {"text": "Lottick, S.", "metadata": {}}, {"text": "Susai, S.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Friedler, and J.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "Wilson, “Energy usage\nreports: Environmental awareness as part of algorithmic accountability,”\nWorkshop on Tackling Climate Change with Machine Learning at\nNeurIPS 2019, 2019.", "metadata": {}}, {"text": "[100] D.", "metadata": {}}, {"text": "Kiela, M.", "metadata": {}}, {"text": "Bartolo, Y .", "metadata": {}}, {"text": "Nie, D.", "metadata": {}}, {"text": "Kaushik, A.", "metadata": {}}, {"text": "Geiger, Z.", "metadata": {}}, {"text": "Wu,\nB.", "metadata": {}}, {"text": "Vidgen, G.", "metadata": {}}, {"text": "Prasad, A.", "metadata": {}}, {"text": "Singh, P.", "metadata": {}}, {"text": "Ringshia, Z.", "metadata": {}}, {"text": "Ma, T.", "metadata": {}}, {"text": "Thrush,\nS.", "metadata": {}}, {"text": "Riedel, Z.", "metadata": {}}, {"text": "Waseem, P.", "metadata": {}}, {"text": "Stenetorp, R.", "metadata": {}}, {"text": "Jia, M.", "metadata": {}}, {"text": "Bansal, C.", "metadata": {}}, {"text": "Potts, and\nA.", "metadata": {}}, {"text": "Williams, “Dynabench: Rethinking benchmarking in NLP,” arXiv\npreprint arXiv:2104.14337, 2021.", "metadata": {}}, {"text": "[101] D.", "metadata": {}}, {"text": "Hernandez and T.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Brown, “Measuring the algorithmic efﬁciency\nof neural networks,” arXiv preprint arXiv:2005.04305 , 2020.", "metadata": {}}, {"text": "[102] P.", "metadata": {}}, {"text": "Mattson, V .", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Reddi, C.", "metadata": {}}, {"text": "Cheng, C.", "metadata": {}}, {"text": "Coleman, G.", "metadata": {}}, {"text": "Diamos, D.", "metadata": {}}, {"text": "Kanter,\nP.", "metadata": {}}, {"text": "Micikevicius, D.", "metadata": {}}, {"text": "Patterson, G.", "metadata": {}}, {"text": "Schmuelling, H.", "metadata": {}}, {"text": "Tang, G.-Y .", "metadata": {}}, {"text": "Wei, and\nC.-J.", "metadata": {}}, {"text": "Wu, “Mlperf: An industry standard benchmark suite for machine\nlearning performance,” IEEE Micro, vol.", "metadata": {}}, {"text": "40, no.", "metadata": {}}, {"text": "2, pp.", "metadata": {}}, {"text": "8–16, 2020.", "metadata": {}}, {"text": "[103] V .", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Reddi, C.", "metadata": {}}, {"text": "Cheng, D.", "metadata": {}}, {"text": "Kanter, P.", "metadata": {}}, {"text": "Mattson, G.", "metadata": {}}, {"text": "Schmuelling, and C.-J.", "metadata": {}}, {"text": "Wu, “The vision behind mlperf: Understanding ai inference performance,”\nIEEE Micro, vol.", "metadata": {}}, {"text": "41, no.", "metadata": {}}, {"text": "3, pp.", "metadata": {}}, {"text": "10–18, 2021.", "metadata": {}}, {"text": "[104] V .", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Reddi, D.", "metadata": {}}, {"text": "Kanter, P.", "metadata": {}}, {"text": "Mattson, J.", "metadata": {}}, {"text": "Duke, T.", "metadata": {}}, {"text": "Nguyen, R.", "metadata": {}}, {"text": "Chukka,\nK.", "metadata": {}}, {"text": "Shiring, K.-S.", "metadata": {}}, {"text": "Tan, M.", "metadata": {}}, {"text": "Charlebois, W.", "metadata": {}}, {"text": "Chou, M.", "metadata": {}}, {"text": "El-Khamy, J.", "metadata": {}}, {"text": "Hong,\nM.", "metadata": {}}, {"text": "Buch, C.", "metadata": {}}, {"text": "Trinh, T.", "metadata": {}}, {"text": "Atta-fosu, F.", "metadata": {}}, {"text": "Cakir, M.", "metadata": {}}, {"text": "Charkhabi, X.", "metadata": {}}, {"text": "Chen,\nJ.", "metadata": {}}, {"text": "Chiang, D.", "metadata": {}}, {"text": "Dexter, W.", "metadata": {}}, {"text": "Heo, G.", "metadata": {}}, {"text": "Schmuelling, M.", "metadata": {}}, {"text": "Shabani, and D.", "metadata": {}}, {"text": "Zika,\n“Mlperf mobile inference benchmark,” arXiv:2012.02328, 2021.", "metadata": {}}, {"text": "[105] P.", "metadata": {}}, {"text": "Mattson, C.", "metadata": {}}, {"text": "Cheng, G.", "metadata": {}}, {"text": "Diamos, C.", "metadata": {}}, {"text": "Coleman, P.", "metadata": {}}, {"text": "Micikevicius,\nD.", "metadata": {}}, {"text": "Patterson, H.", "metadata": {}}, {"text": "Tang, G.-Y .", "metadata": {}}, {"text": "Wei, P.", "metadata": {}}, {"text": "Bailis, V .", "metadata": {}}, {"text": "Bittorf, D.", "metadata": {}}, {"text": "Brooks,\nD.", "metadata": {}}, {"text": "Chen, D.", "metadata": {}}, {"text": "Dutta, U.", "metadata": {}}, {"text": "Gupta, K.", "metadata": {}}, {"text": "Hazelwood, A.", "metadata": {}}, {"text": "Hock, X.", "metadata": {}}, {"text": "Huang,\nD.", "metadata": {}}, {"text": "Kang, D.", "metadata": {}}, {"text": "Kanter, N.", "metadata": {}}, {"text": "Kumar, J.", "metadata": {}}, {"text": "Liao, D.", "metadata": {}}, {"text": "Narayanan, T.", "metadata": {}}, {"text": "Oguntebi,\nG.", "metadata": {}}, {"text": "Pekhimenko, L.", "metadata": {}}, {"text": "Pentecost, V .", "metadata": {}}, {"text": "Janapa Reddi, T.", "metadata": {}}, {"text": "Robie, T.", "metadata": {}}, {"text": "St John, C.-\nJ.", "metadata": {}}, {"text": "Wu, L.", "metadata": {}}, {"text": "Xu, C.", "metadata": {}}, {"text": "Young, and M.", "metadata": {}}, {"text": "Zaharia, “Mlperf training benchmark,”\nin Proceedings of Machine Learning and Systems , vol.", "metadata": {}}, {"text": "2, 2020.", "metadata": {}}, {"text": "[106] V .", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Reddi, C.", "metadata": {}}, {"text": "Cheng, D.", "metadata": {}}, {"text": "Kanter, P.", "metadata": {}}, {"text": "Mattson, G.", "metadata": {}}, {"text": "Schmuelling, C.-J.", "metadata": {}}, {"text": "Wu, B.", "metadata": {}}, {"text": "Anderson, M.", "metadata": {}}, {"text": "Breughe, M.", "metadata": {}}, {"text": "Charlebois, W.", "metadata": {}}, {"text": "Chou, R.", "metadata": {}}, {"text": "Chukka,\nC.", "metadata": {}}, {"text": "Coleman, S.", "metadata": {}}, {"text": "Davis, P.", "metadata": {}}, {"text": "Deng, G.", "metadata": {}}, {"text": "Diamos, J.", "metadata": {}}, {"text": "Duke, D.", "metadata": {}}, {"text": "Fick, J.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Gardner, I.", "metadata": {}}, {"text": "Hubara, S.", "metadata": {}}, {"text": "Idgunji, T.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Jablin, J.", "metadata": {}}, {"text": "Jiao, T.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "John, P.", "metadata": {}}, {"text": "Kanwar,\nD.", "metadata": {}}, {"text": "Lee, J.", "metadata": {}}, {"text": "Liao, A.", "metadata": {}}, {"text": "Lokhmotov, F.", "metadata": {}}, {"text": "Massa, P.", "metadata": {}}, {"text": "Meng, P.", "metadata": {}}, {"text": "Micikevicius,\nC.", "metadata": {}}, {"text": "Osborne, G.", "metadata": {}}, {"text": "Pekhimenko, A.", "metadata": {}}, {"text": "T.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Rajan, D.", "metadata": {}}, {"text": "Sequeira, A.", "metadata": {}}, {"text": "Sirasao,\nF.", "metadata": {}}, {"text": "Sun, H.", "metadata": {}}, {"text": "Tang, M.", "metadata": {}}, {"text": "Thomson, F.", "metadata": {}}, {"text": "Wei, E.", "metadata": {}}, {"text": "Wu, L.", "metadata": {}}, {"text": "Xu, K.", "metadata": {}}, {"text": "Yamada,\nB.", "metadata": {}}, {"text": "Yu, G.", "metadata": {}}, {"text": "Yuan, A.", "metadata": {}}, {"text": "Zhong, P.", "metadata": {}}, {"text": "Zhang, and Y .", "metadata": {}}, {"text": "Zhou, “Mlperf inference\nbenchmark,” in Proceedings of the ACM/IEEE Annual International\nSymposium on Computer Architecture , 2020.", "metadata": {}}, {"text": "[107] M.", "metadata": {}}, {"text": "Mitchell, S.", "metadata": {}}, {"text": "Wu, A.", "metadata": {}}, {"text": "Zaldivar, P.", "metadata": {}}, {"text": "Barnes, L.", "metadata": {}}, {"text": "Vasserman, B.", "metadata": {}}, {"text": "Hutchin-\nson, E.", "metadata": {}}, {"text": "Spitzer, I.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Raji, and T.", "metadata": {}}, {"text": "Gebru, “Model cards for model\nreporting,” Proceedings of the Conference on Fairness, Accountability,\nand Transparency, 2019.", "metadata": {}}, {"text": "[108] C.-J.", "metadata": {}}, {"text": "Wu, S.", "metadata": {}}, {"text": "Manne, P.", "metadata": {}}, {"text": "Ranganathan, S.", "metadata": {}}, {"text": "Bird, and S.", "metadata": {}}, {"text": "Greenstein,\n“Socio-technological challenges and opportunities: Paths forward,” arXiv\npreprint arXiv:2108.06738, 2021.", "metadata": {}}, {"text": "[109] R.", "metadata": {}}, {"text": "Schwartz, J.", "metadata": {}}, {"text": "Dodge, N.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Smith, and O.", "metadata": {}}, {"text": "Etzioni, “Green ai,” arXiv\npreprint arXiv:1907.10597, 2019.", "metadata": {}}, {"text": "[110] K.", "metadata": {}}, {"text": "Maeng, S.", "metadata": {}}, {"text": "Bharuka, I.", "metadata": {}}, {"text": "Gao, M.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Jeffrey, V .", "metadata": {}}, {"text": "Saraph, B.-Y .", "metadata": {}}, {"text": "Su,\nC.", "metadata": {}}, {"text": "Trippel, J.", "metadata": {}}, {"text": "Yang, M.", "metadata": {}}, {"text": "Rabbat, B.", "metadata": {}}, {"text": "Lucia, and C.-J.", "metadata": {}}, {"text": "Wu, “Cpr:\nUnderstanding and improving failure tolerant training for deep learning\nrecommendation with partial recovery,” inProceedings of the Conference\non Machine Learning and Systems , 2021.", "metadata": {}}, {"text": "[111] A.", "metadata": {}}, {"text": "Eisenman, K.", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Matam, S.", "metadata": {}}, {"text": "Ingram, D.", "metadata": {}}, {"text": "Mudigere, R.", "metadata": {}}, {"text": "Krishnamoorthi,\nK.", "metadata": {}}, {"text": "Nair, M.", "metadata": {}}, {"text": "Smelyanskiy, and M.", "metadata": {}}, {"text": "Annavaram, “Check-n-run: A\ncheckpointing system for training deep learning recommendation\nmodels,” arXiv preprint arXiv:2010.08679 , 2021.", "metadata": {}}, {"text": "[112] H.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Dixit, S.", "metadata": {}}, {"text": "Pendharkar, M.", "metadata": {}}, {"text": "Beadon, C.", "metadata": {}}, {"text": "Mason, T.", "metadata": {}}, {"text": "Chakravarthy,\nB.", "metadata": {}}, {"text": "Muthiah, and S.", "metadata": {}}, {"text": "Sankar, “Silent data corruptions at scale,” arXiv\npreprint arXiv:2102.11245, 2021.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "[113] P. H. Hochschild, P. Turner, J. C. Mogul, R. Govindaraju, P. Ranganathan,\nD. E. Culler, and A. Vahdat, “Cores that don’t count,” in Proceedings\nof the Workshop on Hot Topics in Operating Systems , 2021.\n[114] X. Qiu, T. Parcollet, J. Fernandez-Marques, P. P. B. de Gusmao, D. J.\nBeutel, T. Topal, A. Mathur, and N. D. Lane, “A ﬁrst look into the\ncarbon footprint of federated learning,” arXiv preprint arXiv:2102.07627,\n2021.\n[115] H. Wang, B. Kim, J. Xie, and Z. Han, “How is energy consumed in\nsmartphone deep learning apps? executing locally vs. remotely,” in\nProceedings of the IEEE Global Communications Conference , 2019.\n[116] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan,\nK. Hazelwood, E. Isaac, Y . Jia, B. Jia, T. Leyvand, H. Lu, Y . Lu, L. Qiao,\nB. Reagen, J. Spisak, F. Sun, A. Tulloch, P. Vajda, X. Wang, Y . Wang,\nB. Wasti, Y . Wu, R. Xian, S. Yoo, and P. Zhang, “Machine learning\nat facebook: Understanding inference at the edge,” in Proceedings of\nthe IEEE International Symposium on High Performance Computer\nArchitecture, 2019.\n[117] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On\nthe opportunities and risks of foundation models,” arXiv preprint\narXiv:2108.07258, 2021.\n[118] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in Proceedings of the\nInternational conference on machine learning , pp. 1597–1607, 2020.\n[119] M. Assran, M. Caron, I. Misra, P. Bojanowski, A. Joulin, N. Ballas,\nand M. Rabbat, “Semi-supervised learning of visual features by non-\nparametrically predicting view assignments with support samples,” arXiv\npreprint arXiv:2104.13963, 2021.\n[120] L. M. Dery, P. Michel, A. Talwalkar, and G. Neubig, “Should we be\npre-training? an argument for end-task aware training as an alternative,”\narXiv preprint arXiv:2109.07437 , 2021.\ny = 0.7892x-0.004R²= 0.9969\ny = 0.7903x-0.002R²= 0.9986\n0.787\n0.789\n0.791\n0.793\n0.795\n0.797\n0.1250.250.5124\nModel Error  (Lower is Better)\nEnergy/Step (J)\nData Scale: 1XData Scale: 2XData Scale: 4XData Scale: 8XData Scale: 16XModel Scale: 2XModel Scale: 4XModel Scale: 8XModel Scale: 16XModel Scale: 32XModel Scale: 64XModel Scale: 128X0   \n0.002\n0.004\n0.006\n0.008\n0.010\nFig. 12. Model quality of recommendation use cases improves as we scale up\nthe amount of data and/or the number of model parameters (e.g., embedding\ncardinality or dimension), leading to higher energy and carbon footprint.\nMaximizing model accuracy for the speciﬁc recommendation use case comes\nwith signiﬁcant energy cost — Roughly 4 × energy saving can be achieved\nwith only 0.004 model quality degradation (green vs. yellow stars).\nAPPENDIX\nDespite the recent calls-to-action [ 28], [39], [40], [41], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI. There\nare several factors that may have contributed to the current\nstate of AI:\n• Lack of incentives: Over 90% of the ML publications\nonly focus on model accuracy improvements at the\nexpense of efﬁciency [ 109]. Challenges 10 incentivize\ninvestment into efﬁcient approaches.\n• Lack of common tools: There is no standard telemetry\nin place to provide accurate, reliable energy and carbon\nfootprint measurement. The measurement methodology\nis complex — factors, such as datacenter infrastructures,\nhardware architectures, energy sources, can perturb the\nﬁnal measure easily.\n• Lack of normalization factors: Algorithmic progress in\nML is often presented in some measure of model accuracy,\ne.g., BLEU, points, ELO, cross-entropy loss, but without\nconsidering resource requirement as a normalization factor,\ne.g., the number of\nCPU/GPU/TPU hours used, the overall energy consump-\ntion and/or carbon footprint required.\n• Platform fragmentation: Implementation details can\nhave a signiﬁcant impact on real-world efﬁciency, but\nbest practices remain elusive and platform fragmentation\nprevents performance and efﬁciency portability across\nmodel development.\nA. Data Utilization Efﬁciency\nFigure 12 depicts energy footprint reduction potential when\ndata and model scaling is performed in tandem. The x-axis\n10Efﬁcient Open-Domain Question Answering (https://efﬁcientqa.github.io/),\nSustaiNLP: Simple and Efﬁcient Natural Language Processing (https://site\ns.google.com/view/sustainlp2020/home), and WMT: Machine Translation\nEfﬁciency Task (http://www.statmt.org/wmt21/efﬁciency-task.html).\n14", "sentences": [{"text": "[113] P.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Hochschild, P.", "metadata": {}}, {"text": "Turner, J.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Mogul, R.", "metadata": {}}, {"text": "Govindaraju, P.", "metadata": {}}, {"text": "Ranganathan,\nD.", "metadata": {}}, {"text": "E.", "metadata": {}}, {"text": "Culler, and A.", "metadata": {}}, {"text": "Vahdat, “Cores that don’t count,” in Proceedings\nof the Workshop on Hot Topics in Operating Systems , 2021.", "metadata": {}}, {"text": "[114] X.", "metadata": {}}, {"text": "Qiu, T.", "metadata": {}}, {"text": "Parcollet, J.", "metadata": {}}, {"text": "Fernandez-Marques, P.", "metadata": {}}, {"text": "P.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "de Gusmao, D.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Beutel, T.", "metadata": {}}, {"text": "Topal, A.", "metadata": {}}, {"text": "Mathur, and N.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Lane, “A ﬁrst look into the\ncarbon footprint of federated learning,” arXiv preprint arXiv:2102.07627,\n2021.", "metadata": {}}, {"text": "[115] H.", "metadata": {}}, {"text": "Wang, B.", "metadata": {}}, {"text": "Kim, J.", "metadata": {}}, {"text": "Xie, and Z.", "metadata": {}}, {"text": "Han, “How is energy consumed in\nsmartphone deep learning apps?", "metadata": {}}, {"text": "executing locally vs.", "metadata": {}}, {"text": "remotely,” in\nProceedings of the IEEE Global Communications Conference , 2019.", "metadata": {}}, {"text": "[116] C.-J.", "metadata": {}}, {"text": "Wu, D.", "metadata": {}}, {"text": "Brooks, K.", "metadata": {}}, {"text": "Chen, D.", "metadata": {}}, {"text": "Chen, S.", "metadata": {}}, {"text": "Choudhury, M.", "metadata": {}}, {"text": "Dukhan,\nK.", "metadata": {}}, {"text": "Hazelwood, E.", "metadata": {}}, {"text": "Isaac, Y .", "metadata": {}}, {"text": "Jia, B.", "metadata": {}}, {"text": "Jia, T.", "metadata": {}}, {"text": "Leyvand, H.", "metadata": {}}, {"text": "Lu, Y .", "metadata": {}}, {"text": "Lu, L.", "metadata": {}}, {"text": "Qiao,\nB.", "metadata": {}}, {"text": "Reagen, J.", "metadata": {}}, {"text": "Spisak, F.", "metadata": {}}, {"text": "Sun, A.", "metadata": {}}, {"text": "Tulloch, P.", "metadata": {}}, {"text": "Vajda, X.", "metadata": {}}, {"text": "Wang, Y .", "metadata": {}}, {"text": "Wang,\nB.", "metadata": {}}, {"text": "Wasti, Y .", "metadata": {}}, {"text": "Wu, R.", "metadata": {}}, {"text": "Xian, S.", "metadata": {}}, {"text": "Yoo, and P.", "metadata": {}}, {"text": "Zhang, “Machine learning\nat facebook: Understanding inference at the edge,” in Proceedings of\nthe IEEE International Symposium on High Performance Computer\nArchitecture, 2019.", "metadata": {}}, {"text": "[117] R.", "metadata": {}}, {"text": "Bommasani, D.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Hudson, E.", "metadata": {}}, {"text": "Adeli, R.", "metadata": {}}, {"text": "Altman, S.", "metadata": {}}, {"text": "Arora, S.", "metadata": {}}, {"text": "von\nArx, M.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Bernstein, J.", "metadata": {}}, {"text": "Bohg, A.", "metadata": {}}, {"text": "Bosselut, E.", "metadata": {}}, {"text": "Brunskill, et al., “On\nthe opportunities and risks of foundation models,” arXiv preprint\narXiv:2108.07258, 2021.", "metadata": {}}, {"text": "[118] T.", "metadata": {}}, {"text": "Chen, S.", "metadata": {}}, {"text": "Kornblith, M.", "metadata": {}}, {"text": "Norouzi, and G.", "metadata": {}}, {"text": "Hinton, “A simple framework\nfor contrastive learning of visual representations,” in Proceedings of the\nInternational conference on machine learning , pp.", "metadata": {}}, {"text": "1597–1607, 2020.", "metadata": {}}, {"text": "[119] M.", "metadata": {}}, {"text": "Assran, M.", "metadata": {}}, {"text": "Caron, I.", "metadata": {}}, {"text": "Misra, P.", "metadata": {}}, {"text": "Bojanowski, A.", "metadata": {}}, {"text": "Joulin, N.", "metadata": {}}, {"text": "Ballas,\nand M.", "metadata": {}}, {"text": "Rabbat, “Semi-supervised learning of visual features by non-\nparametrically predicting view assignments with support samples,” arXiv\npreprint arXiv:2104.13963, 2021.", "metadata": {}}, {"text": "[120] L.", "metadata": {}}, {"text": "M.", "metadata": {}}, {"text": "Dery, P.", "metadata": {}}, {"text": "Michel, A.", "metadata": {}}, {"text": "Talwalkar, and G.", "metadata": {}}, {"text": "Neubig, “Should we be\npre-training?", "metadata": {}}, {"text": "an argument for end-task aware training as an alternative,”\narXiv preprint arXiv:2109.07437 , 2021.", "metadata": {}}, {"text": "y = 0.7892x-0.004R²= 0.9969\ny = 0.7903x-0.002R²= 0.9986\n0.787\n0.789\n0.791\n0.793\n0.795\n0.797\n0.1250.250.5124\nModel Error  (Lower is Better)\nEnergy/Step (J)\nData Scale: 1XData Scale: 2XData Scale: 4XData Scale: 8XData Scale: 16XModel Scale: 2XModel Scale: 4XModel Scale: 8XModel Scale: 16XModel Scale: 32XModel Scale: 64XModel Scale: 128X0   \n0.002\n0.004\n0.006\n0.008\n0.010\nFig.", "metadata": {}}, {"text": "12.", "metadata": {}}, {"text": "Model quality of recommendation use cases improves as we scale up\nthe amount of data and/or the number of model parameters (e.g., embedding\ncardinality or dimension), leading to higher energy and carbon footprint.", "metadata": {}}, {"text": "Maximizing model accuracy for the speciﬁc recommendation use case comes\nwith signiﬁcant energy cost — Roughly 4 × energy saving can be achieved\nwith only 0.004 model quality degradation (green vs.", "metadata": {}}, {"text": "yellow stars).", "metadata": {}}, {"text": "APPENDIX\nDespite the recent calls-to-action [ 28], [39], [40], [41], the\noverall community remains under-invested in research that aims\nat deeply understanding and minimizing the cost of AI.", "metadata": {}}, {"text": "There\nare several factors that may have contributed to the current\nstate of AI:\n• Lack of incentives: Over 90% of the ML publications\nonly focus on model accuracy improvements at the\nexpense of efﬁciency [ 109].", "metadata": {}}, {"text": "Challenges 10 incentivize\ninvestment into efﬁcient approaches.", "metadata": {}}, {"text": "• Lack of common tools: There is no standard telemetry\nin place to provide accurate, reliable energy and carbon\nfootprint measurement.", "metadata": {}}, {"text": "The measurement methodology\nis complex — factors, such as datacenter infrastructures,\nhardware architectures, energy sources, can perturb the\nﬁnal measure easily.", "metadata": {}}, {"text": "• Lack of normalization factors: Algorithmic progress in\nML is often presented in some measure of model accuracy,\ne.g., BLEU, points, ELO, cross-entropy loss, but without\nconsidering resource requirement as a normalization factor,\ne.g., the number of\nCPU/GPU/TPU hours used, the overall energy consump-\ntion and/or carbon footprint required.", "metadata": {}}, {"text": "• Platform fragmentation: Implementation details can\nhave a signiﬁcant impact on real-world efﬁciency, but\nbest practices remain elusive and platform fragmentation\nprevents performance and efﬁciency portability across\nmodel development.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Data Utilization Efﬁciency\nFigure 12 depicts energy footprint reduction potential when\ndata and model scaling is performed in tandem.", "metadata": {}}, {"text": "The x-axis\n10Efﬁcient Open-Domain Question Answering (https://efﬁcientqa.github.io/),\nSustaiNLP: Simple and Efﬁcient Natural Language Processing (https://site\ns.google.com/view/sustainlp2020/home), and WMT: Machine Translation\nEfﬁciency Task (http://www.statmt.org/wmt21/efﬁciency-task.html).", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "represents the energy footprint required per training step\nwhereas the y-axis represents model error. The blue solid\nlines capture model size scaling (through embedding hash\nscaling) while the training data set size is kept ﬁxed. Each\nline corresponds to a different data set size, in an increasing\norder from top to bottom. The points within each line represent\ndifferent model (embedding) sizes, in an increasing order from\nleft to right. The red dashed lines capture data scaling while the\nmodel size is kept ﬁxed. Each line corresponds to a different\nembedding hash size, in an increasing order from left to right.\nThe points within each line represent different data sizes, in\nan increasing order from top to bottom. The dashed black\nline captures the performance scaling trend as we scale data\nand model sizes in tandem. This represents the energy-optimal\nscaling approach.\nScaling data sizes or model sizes independently deviates from\nthe energy-optimal trend. We highlight two energy-optimal\nsettings along the Pareto-frontier curve. The yellow star uses\nthe scaling setting of Data scaling 2 × and Model scaling 2 ×\nwhereas the green star adopts the setting of Data scaling 8 ×\nand Model scaling 16 ×. The yellow star consumes roughly 4 ×\nlower energy as compared to the green star with only 0.004\nmodel quality degradation in Normalized Entropy. Overall\nmodel quality performance has a (diminishing) power-law\nrelationship with the corresponding energy consumption and\nthe power of the power law is extremely small (0.002-0.004).\nThis means achieving higher model quality through model-data\nscaling for recommendation use cases incurs signiﬁcant energy\ncost.\nB. Efﬁcient, Environmentally-Sustainable AI Systems\nDisaggregating Machine Learning Pipeline Stages: As\ndepicted in Figure 3, the overall training throughput efﬁciency\nfor large-scale ML models depends on the throughput perfor-\nmance of both data ingestion and pre-processing and model\ntraining. Disaggregating the data ingestion and pre-processing\nstage of the machine learning pipeline from model training\nis the de-facto approach for industry-scale machine learning\nmodel training. This allows training accelerator, network\nand storage I/O bandwidth utilization to scale independently,\nthereby increasing the overall model training throughput by\n56% [44]. Disaggregation with well-designed check-pointing\nsupport [110], [111] improves training fault tolerance as well.\nBy doing so, failure on nodes that are responsible for data\ningestion and pre-processing can be recovered efﬁciently\nwithout requiring re-runs of the entire training experiment.\nFrom a sustainability perspective, disaggregating the data\nstorage and ingestion stage from model training maximizes\ninfrastructure efﬁciency by using less system resources to\nachieve higher training throughput, resulting in lower embodied\ncarbon footprint. By increasing fault tolerance, the operational\ncarbon footprint is reduced at the same time.\nFault-Tolerant AI Systems and Hardware: One way to\namortize the rising embodied carbon cost of AI infrastructures\nis to extend hardware lifetime. However, hardware ages\n— depending on the wear-out characteristics, increasingly\nmore errors can surface over time and result in silent data\ncorruption, leading to erroneous computation, model accuracy\ndegradation, non-deterministic ML execution, or fatal system\nfailure. In a large ﬂeet of processors, silent data corruption\ncan occur frequently enough to have disruptive impact on\nservice productivity [ 112], [ 113]. Decommissioning an AI\nsystem entirely because of hardware faults is expensive from the\nperspective of resource and environmental footprints. System\narchitects can design differential reliability levels for micro\narchitectural components on an AI system depending on the\nML model execution characteristics. Alternatively, algorithmic\nfault tolerance can be built into deep learning programming\nframeworks to provide a code execution path that is cognizant\nof hardware wear-out characteristics.\nOn-Device Learning: Federated learning and optimization\ncan result in a non-negligible amount of carbon emissions\nat the edge, similar to the carbon footprint of training\nT ransf ormerBig [21]. Figure 11 shows that the federated\nlearning and optimization process emits non-negligible carbon\nat the edge due to both computation and wireless communi-\ncation during the process. To estimate the carbon emission,\nwe used a similar methodology to [ 114]. We collected the\n90-day log data for federated learning production use cases\nat Facebook, which recorded the time spent on computation,\ndata downloading, and data uploading per client device. We\nmultiplied the computation time with the estimated device\npower and upload/download time with the estimated router\npower, and omitted other energy. We assumed a device power of\n3W and a router power of 7.5W [115], [114]. Model training on\nclient edge devices is inherently less energy-efﬁcient because\nof the high wireless communication overheads, sub-optimal\ntraining data distribution in individual client devices [114], large\ndegree of system heterogeneity among client edge devices, and\nhighly-fragmented edge device architectures that make system-\nlevel optimization signiﬁcantly more challenging [ 116]. Note,\nthe wireless communication energy cost takes up a signiﬁcant\nportion of the overall energy footprint of federated learning,\nmaking energy footprint optimization on communication im-\nportant.\nC. Efﬁciency and Self-Supervised Learning\nSelf-supervised learning (SSL) have received much attention\nin the research community in recent years. SSL methods train\ndeep neural networks without using explicit supervision in\nthe form of human-annotated labels for each training sample.\nHaving humans annotate data is a time-consuming, expensive,\nand typically noisy process. SSL methods are typically used\nto train foundation models — models that can readily be ﬁne-\ntuned using a small amount of labeled data on a down-stream\ntask [117]. SSL methods have been extremely successful for\npre-training large language models, becoming the de-facto\nstandard, and they have also attracted great interest in computer\nvision.\nWhen comparing supervised and self-supervised methods,\nthere is a glaring trade-off between having labels and the\namount of computational overhead involved in pre-training. For\nexample, Chen et al. report achieving 69.3% top-1 validation\naccuracy with a ResNet-50 model after SSL pre-training for\n15", "sentences": [{"text": "represents the energy footprint required per training step\nwhereas the y-axis represents model error.", "metadata": {}}, {"text": "The blue solid\nlines capture model size scaling (through embedding hash\nscaling) while the training data set size is kept ﬁxed.", "metadata": {}}, {"text": "Each\nline corresponds to a different data set size, in an increasing\norder from top to bottom.", "metadata": {}}, {"text": "The points within each line represent\ndifferent model (embedding) sizes, in an increasing order from\nleft to right.", "metadata": {}}, {"text": "The red dashed lines capture data scaling while the\nmodel size is kept ﬁxed.", "metadata": {}}, {"text": "Each line corresponds to a different\nembedding hash size, in an increasing order from left to right.", "metadata": {}}, {"text": "The points within each line represent different data sizes, in\nan increasing order from top to bottom.", "metadata": {}}, {"text": "The dashed black\nline captures the performance scaling trend as we scale data\nand model sizes in tandem.", "metadata": {}}, {"text": "This represents the energy-optimal\nscaling approach.", "metadata": {}}, {"text": "Scaling data sizes or model sizes independently deviates from\nthe energy-optimal trend.", "metadata": {}}, {"text": "We highlight two energy-optimal\nsettings along the Pareto-frontier curve.", "metadata": {}}, {"text": "The yellow star uses\nthe scaling setting of Data scaling 2 × and Model scaling 2 ×\nwhereas the green star adopts the setting of Data scaling 8 ×\nand Model scaling 16 ×.", "metadata": {}}, {"text": "The yellow star consumes roughly 4 ×\nlower energy as compared to the green star with only 0.004\nmodel quality degradation in Normalized Entropy.", "metadata": {}}, {"text": "Overall\nmodel quality performance has a (diminishing) power-law\nrelationship with the corresponding energy consumption and\nthe power of the power law is extremely small (0.002-0.004).", "metadata": {}}, {"text": "This means achieving higher model quality through model-data\nscaling for recommendation use cases incurs signiﬁcant energy\ncost.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Efﬁcient, Environmentally-Sustainable AI Systems\nDisaggregating Machine Learning Pipeline Stages: As\ndepicted in Figure 3, the overall training throughput efﬁciency\nfor large-scale ML models depends on the throughput perfor-\nmance of both data ingestion and pre-processing and model\ntraining.", "metadata": {}}, {"text": "Disaggregating the data ingestion and pre-processing\nstage of the machine learning pipeline from model training\nis the de-facto approach for industry-scale machine learning\nmodel training.", "metadata": {}}, {"text": "This allows training accelerator, network\nand storage I/O bandwidth utilization to scale independently,\nthereby increasing the overall model training throughput by\n56% [44].", "metadata": {}}, {"text": "Disaggregation with well-designed check-pointing\nsupport [110], [111] improves training fault tolerance as well.", "metadata": {}}, {"text": "By doing so, failure on nodes that are responsible for data\ningestion and pre-processing can be recovered efﬁciently\nwithout requiring re-runs of the entire training experiment.", "metadata": {}}, {"text": "From a sustainability perspective, disaggregating the data\nstorage and ingestion stage from model training maximizes\ninfrastructure efﬁciency by using less system resources to\nachieve higher training throughput, resulting in lower embodied\ncarbon footprint.", "metadata": {}}, {"text": "By increasing fault tolerance, the operational\ncarbon footprint is reduced at the same time.", "metadata": {}}, {"text": "Fault-Tolerant AI Systems and Hardware: One way to\namortize the rising embodied carbon cost of AI infrastructures\nis to extend hardware lifetime.", "metadata": {}}, {"text": "However, hardware ages\n— depending on the wear-out characteristics, increasingly\nmore errors can surface over time and result in silent data\ncorruption, leading to erroneous computation, model accuracy\ndegradation, non-deterministic ML execution, or fatal system\nfailure.", "metadata": {}}, {"text": "In a large ﬂeet of processors, silent data corruption\ncan occur frequently enough to have disruptive impact on\nservice productivity [ 112], [ 113].", "metadata": {}}, {"text": "Decommissioning an AI\nsystem entirely because of hardware faults is expensive from the\nperspective of resource and environmental footprints.", "metadata": {}}, {"text": "System\narchitects can design differential reliability levels for micro\narchitectural components on an AI system depending on the\nML model execution characteristics.", "metadata": {}}, {"text": "Alternatively, algorithmic\nfault tolerance can be built into deep learning programming\nframeworks to provide a code execution path that is cognizant\nof hardware wear-out characteristics.", "metadata": {}}, {"text": "On-Device Learning: Federated learning and optimization\ncan result in a non-negligible amount of carbon emissions\nat the edge, similar to the carbon footprint of training\nT ransf ormerBig [21].", "metadata": {}}, {"text": "Figure 11 shows that the federated\nlearning and optimization process emits non-negligible carbon\nat the edge due to both computation and wireless communi-\ncation during the process.", "metadata": {}}, {"text": "To estimate the carbon emission,\nwe used a similar methodology to [ 114].", "metadata": {}}, {"text": "We collected the\n90-day log data for federated learning production use cases\nat Facebook, which recorded the time spent on computation,\ndata downloading, and data uploading per client device.", "metadata": {}}, {"text": "We\nmultiplied the computation time with the estimated device\npower and upload/download time with the estimated router\npower, and omitted other energy.", "metadata": {}}, {"text": "We assumed a device power of\n3W and a router power of 7.5W [115], [114].", "metadata": {}}, {"text": "Model training on\nclient edge devices is inherently less energy-efﬁcient because\nof the high wireless communication overheads, sub-optimal\ntraining data distribution in individual client devices [114], large\ndegree of system heterogeneity among client edge devices, and\nhighly-fragmented edge device architectures that make system-\nlevel optimization signiﬁcantly more challenging [ 116].", "metadata": {}}, {"text": "Note,\nthe wireless communication energy cost takes up a signiﬁcant\nportion of the overall energy footprint of federated learning,\nmaking energy footprint optimization on communication im-\nportant.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Efﬁciency and Self-Supervised Learning\nSelf-supervised learning (SSL) have received much attention\nin the research community in recent years.", "metadata": {}}, {"text": "SSL methods train\ndeep neural networks without using explicit supervision in\nthe form of human-annotated labels for each training sample.", "metadata": {}}, {"text": "Having humans annotate data is a time-consuming, expensive,\nand typically noisy process.", "metadata": {}}, {"text": "SSL methods are typically used\nto train foundation models — models that can readily be ﬁne-\ntuned using a small amount of labeled data on a down-stream\ntask [117].", "metadata": {}}, {"text": "SSL methods have been extremely successful for\npre-training large language models, becoming the de-facto\nstandard, and they have also attracted great interest in computer\nvision.", "metadata": {}}, {"text": "When comparing supervised and self-supervised methods,\nthere is a glaring trade-off between having labels and the\namount of computational overhead involved in pre-training.", "metadata": {}}, {"text": "For\nexample, Chen et al.", "metadata": {}}, {"text": "report achieving 69.3% top-1 validation\naccuracy with a ResNet-50 model after SSL pre-training for\n15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "1000 epochs on the ImageNet dataset and using the linear\nevaluation protocol, freezing the pre-trained feature extractor,\nand ﬁne-tuning a linear classiﬁer on top for 60 epochs using the\nfull ImageNet dataset with all labels [118]. In contrast, the same\nmodel typically achieves at least 76.1% top-1 accuracy after\n90 epochs of fully-supervised training. Thus, in this example,\nusing labels and supervised training is worth a roughly 10 ×\nreduction in training effort, measured in terms of number of\npasses over the dataset.\nRecent work suggests that incorporating even a small amount\nof labeled data can signiﬁcantly bridge this gap. Assran et\nal. describe an approach called Predicting view Assignments\nWith Support samples (PAWS) for semi-supervised pre-training\ninspired by SSL [ 119]. With access to labels for just 10% of\nthe training images in ImageNet, a ResNet-50 achieves 75.5%\ntop-1 accuracy after just 200 epochs of PAWS pre-training.\nRunning on 64 V100 GPUs, this takes roughly 16 hours. Similar\nobservations have recently been made for language model pre-\ntraining as well [120].\nSelf-supervised pre-training potentially has advantages in\nthat a single foundation model can be trained (expensive) but\nthen ﬁne-tuned (inexpensive), amortizing the up front cost\nacross many tasks [ 117]. Substantial additional research is\nneeded to better understand the cost-beneﬁt trade-offs for this\nparadigm.\n16", "sentences": [{"text": "1000 epochs on the ImageNet dataset and using the linear\nevaluation protocol, freezing the pre-trained feature extractor,\nand ﬁne-tuning a linear classiﬁer on top for 60 epochs using the\nfull ImageNet dataset with all labels [118].", "metadata": {}}, {"text": "In contrast, the same\nmodel typically achieves at least 76.1% top-1 accuracy after\n90 epochs of fully-supervised training.", "metadata": {}}, {"text": "Thus, in this example,\nusing labels and supervised training is worth a roughly 10 ×\nreduction in training effort, measured in terms of number of\npasses over the dataset.", "metadata": {}}, {"text": "Recent work suggests that incorporating even a small amount\nof labeled data can signiﬁcantly bridge this gap.", "metadata": {}}, {"text": "Assran et\nal.", "metadata": {}}, {"text": "describe an approach called Predicting view Assignments\nWith Support samples (PAWS) for semi-supervised pre-training\ninspired by SSL [ 119].", "metadata": {}}, {"text": "With access to labels for just 10% of\nthe training images in ImageNet, a ResNet-50 achieves 75.5%\ntop-1 accuracy after just 200 epochs of PAWS pre-training.", "metadata": {}}, {"text": "Running on 64 V100 GPUs, this takes roughly 16 hours.", "metadata": {}}, {"text": "Similar\nobservations have recently been made for language model pre-\ntraining as well [120].", "metadata": {}}, {"text": "Self-supervised pre-training potentially has advantages in\nthat a single foundation model can be trained (expensive) but\nthen ﬁne-tuned (inexpensive), amortizing the up front cost\nacross many tasks [ 117].", "metadata": {}}, {"text": "Substantial additional research is\nneeded to better understand the cost-beneﬁt trade-offs for this\nparadigm.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}]}