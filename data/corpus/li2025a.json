{"document_id": "li2025a", "title": "FLM-101B: An Open LLM and How to Train It with $100K Budget", "text": "FLM-101B: An Open LLM and How to Train It with $100K Budget\nXiang Li1‚Ä†, Yiqun Yao1‚Ä†, Xin Jiang1‚Ä†, Xuezhi Fang1‚Ä†, Xuying Meng2,\nSiqi Fan3, Peng Han3, Jing Li4, Li Du1, Bowen Qin1, Zheng Zhang1,\nAixin Sun5, Yequan Wang1‚àó\n1Beijing Academy of Artificial Intelligence, Beijing, China\n2Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n3University of Electronic Science and Technology of China, Chengdu, China\n4Harbin Institute of Technology, Shenzhen, China\n5School of Computer Science and Engineering, Nanyang Technological University, Singapore\nAbstract\nLarge language models (LLMs) are considered important ap-\nproaches towards foundational machine intelligence, achiev-\ning remarkable success in Natural Language Processing and\nmultimodal tasks, among others. However, the carbon foot-\nprints and financial costs originating from heavy pre-training\ncomputation is a non-negligible issue. Progressive training\nmethods, inspired by the neurogenesis process that grows\nneural structures, have shown potential to accelerate LLM\npre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B pa-\nrameters remain underexplored. In this paper, we show that\nour model, namely FLM-101B, trained with our growth strat-\negy under a budget of $100K, reaches 80% of the baselines‚Äô\nperformances with only 10% of their floating-point operations.\nWe believe that further studies on progressive training will ben-\nefit the community by cutting down the costs and promoting\ngreen AI. The checkpoint of FLM-101B is publicly available.\n1 Introduction\nLarge language models (LLMs) (Radford et al. 2018; Tou-\nvron et al. 2023a; Devlin et al. 2019; Raffel et al. 2020) have\nconsistently demonstrated their efficacy across a spectrum of\napplications, especially in language processing (Wang et al.\n2022b,a; Fan et al. 2022; Liu et al. 2022) and multimodal\ntasks (Zhao et al. 2023; Meng et al. 2023). Despite variations\nin architectural designs, a universal challenge confronting\nall LLMs is the escalating cost associated with their train-\ning. Recent trends indicate a shift towards utilizing larger\namounts of data (e.g., 1.4T tokens for Llama-1 (Touvron et al.\n2023a), 2T tokens for Llama-2 (Touvron et al. 2023b), and\n15T tokens for Llama-3 (Meta 2024)). Meanwhile, the sizes\nof open-sourced models continue to increase (Penedo et al.\n2023; Bi et al. 2024; Mistral 2024). Consequently, a major\nfocus within LLM research is the development of innovative\nmethodologies that effectively mitigate training expenses,\naligning with the broader objectives of Green AI (Schwartz\net al. 2020).\nIn this paper, we present our exploration to train an LLM\nat the 100B-parameter scale using a growth strategy inspired\nby previous research on progressive learning (Gong et al.\n2019; Gu et al. 2021; Yao et al. 2024) and neurogenesis\nNon-Growth vs. three Growth Strategies\n(a) Without growth\ntokens (Trillion)\nparameters (Billion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nThe shaded area in the graph represents the training cost\n(b) Linear growth strategy  cost saving = 50\n(c) Superlinear growth  strategy  cost saving > 50%\n(d) Sublinear growth strategy  cost saving < 50%\nparameters (Billion)\ntokens (Trillion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nSublinear\nSuperlinear\n  Linear\nFigure 1: An overview of different growth strategies. (a):\na baseline with constant number of parameters. (b): a straight-\nforward linear growth strategy, cost-saving being exactly\n50%; (c): a superlinear strategy with > 50% cost saving; (d):\nsublinear strategy saving the cost by less than 50%.\n(Eriksson et al. 1998). ‚ÄúGrowth‚Äù means dynamic expansion of\nthe parameter number count, from small to large, through the\ntraining progresses. Figure 1 illustrates three typical growth\nstrategies: linear, sublinear, and superlinear. As the FLOPs\nof LLMs are approximately proportional to their number\nof parameters (Hoffmann et al. 2022), the area under the\nparameter curve represents the computational cost of training.\nWhile existing studies on scaling laws (Hoffmann et al.\n2022) suggest that training a smaller model with more data\nmay potentially result in higher scores on some tasks un-\nder a fixed FLOPs budget, they mainly consider the scenar-\nios where model sizes are fixed through training. We be-\nlieve that verifying the feasibility of a growth strategy (Gu\net al. 2021; Shen et al. 2022; Chen et al. 2022; Yao et al.\n2024) for extremely large models would be an important\ncompletion to scaling laws. To maximize computational effi-\nciency, we strategically focus on implementing an aggressive\ngrowth strategy (Figure 1 (c)) for sanity check. We adapt the\nMSG (Yao et al. 2024) growth operators to train a model at\n100B+ scale. We fix our budget to be$100K with 192 A800\nGPUs.\nAt the 100B scale, it is impractical to conduct strict head-\narXiv:2309.03852v3  [cs.CL]  14 Jan 2025\n\nto-head comparison with the same model trained with fixed\nsize from scratch. Instead, we compare our grown model,\nnamely FLM-101B, with the existing first-generation 100B+\nlanguage models (Brown et al. 2020; Zeng et al. 2023). Our\nmodel is trained with around 300 billion English and Chi-\nnese tokens, aligning it with these predecessors in terms\nof data scale. We first evaluate on the knowledge-oriented\nbenchmarks (i.e., MMLU (Hendrycks et al. 2021) and C-\nEval (Huang et al. 2023)). Nevertheless, such evaluation may\nnot comprehensively reflect the models‚Äô capability: it is diffi-\ncult to distinguish whether the models recall a piece of knowl-\nedge or possess the capacity for reasoning and/or inference.\nBorrowing some ideas from Intelligence Quotient (IQ) tests\n(i.e., Perceptual Reasoning and Working Memory (Watkins\net al. 1995)), we consolidate another range of evaluation,\nincluding symbolic mapping (Wei et al. 2023), rule under-\nstanding, pattern mining, and anti-interference evaluations.\nWe believe these tasks are less likely to be affected by data\nleakage or memorization, offering a more nuanced insight\ninto the model‚Äôs cognitive abilities beyond mere knowledge\nretrieval.\nTo summarize, the paper has made the following contri-\nbutions. First, to the best of our knowledge, this is the first\nattempt to use a growth strategy to train an LLM with 100B+\nparameters from scratch. The training costs only 100,000\nUS dollars. Second, we demonstrate details for addressing\nthe instability issues via improving training objectives, hyper-\nparameter search, and function-preserving growth. Third, we\nconduct extensive evaluations, including both the commonly\nused knowledge-oriented benchmarks and the new range of\nevaluations inspired by IQ tests. Experimental results show\nthat, despite its low training cost, FLM-101B is competitive\nand robust. Lastly, we will release the model checkpoints,\nas well as some related code and tools, to promote related\nresearch. Related literature is reviewed in Appendix B.\n2 Design Overview of FLM-101B\nIn this section, we provide an outline of FLM-101B, detail-\ning its architecture, pre-training methods, and configuration\nspecifics.\n2.1 Architecture\nBackbone. Among the many existing model architectures,\nwe adopt FreeLM (Li et al. 2023) as the backbone for\nour models, with modifications. FreeLM is based on GPT\n(Radford et al. 2019), a transformer-like architecture with\na decoder-only configuration. Different from GPT, FreeLM\nfeatures two pre-training objectives: the language objective\nand the teacher objective (Section 2.2). We preserve the GPT-\n3-style transformer block designs without incorporating the\nlater modifications from Llama series. We employ the tok-\nenizer derived from GPT-4, characterized by a vocabulary\nsize of 100, 256.\nxPos Integration. To enhance long sequence modeling, we\nintegrate the Extrapolatable Position Embedding (xPos) (Sun\net al. 2023). This innovation draws inspiration from RoPE (Su\net al. 2021), which aims to improve the length extrapolation\nability by introducing an exponential decay into the rotation\nmatrix.\nModel Sizes. Benefiting from our growth strategy, the we\nproduce three models with 16B, 51B, and 101B (i.e., FLM-\n101B) parameters in a single training. The training process is\ncarried out in a progressive manner by growing a 16B model\nto 51B, and then 101B.\n2.2 Pre-Training Setup\nFLM-101B. By design, FLM-101B is an English-Chinese\nbilingual model. It mixes English and Chinese corpora at a\nratio of approximately 53.5% : 46 .5%. Inspired by the find-\ning that instruction data can augment LLMs‚Äô comprehension\ncapabilities (Ouyang et al. 2022), we integrate multi-task\ninstructionally prompted data: OIG (Open Instruction Gen-\neralist) 1 and COIG (Chinese Open Instruction Generalist) 2,\nin the pre-training stage.\neFLM-16B. To analyse the effect of domain-specific knowl-\nedge data (Section 4.2), we apply the FreeLM teacher signals\n(Li et al. 2023) to enhance factual capability. Due to computa-\ntional cost, we incorporate these signals only in the smallest\n16B model. This enhanced model is named eFLM-16B.\nSpecifically, we employ two emojis:\nüòà (U+1F621) and\nüò° \n(U+1F608) 3, from the vocabulary to replace the original\nbinary classification labels (Li et al. 2023). For the teacher\nsignal, we supervise only on the emoji tokens, unifying the\nobjective with language modeling. Moreover, we discard the\noriginal multi-task alternating approach and completely mix\nthe samples from both sides in every batch. This strategy can\nenhance the consistency of data sampling distribution as well\nas improve training stability.\nTable 1: Partial configurations for different growth stages.\nParams Learning Warmup Batch Tokens Time Tokens\n(billion) Rate (samples) (million) (day) (billion)\n16 4e‚àí4 4,608,000 4.72 9.63 245.37\n51 3.4e‚àí4 230,400 4.72 5.37 39.64\n101 2e‚àí4 230,400 4.31 6.54 26.54\n2.3 Growth Strategy\nThe essence of the low cost in scaling up FLM-101B is the\ngrowth strategy. Specifically, we train three models, with\n16B, 51B, and 101B parameters, respectively, in a sequential\nmanner. Each model inherits knowledge from its predecessor.\nThis is contrary to the common practice that the models\nof different sizes are trained independently (Touvron et al.\n2023a,b).\nFunction-preserving Growth. Function preservation means\nthat before and after growth, the models yield consistent out-\nputs given the same arbitrary inputs. This property has proven\n1https://huggingface.co/datasets/laion/OIG\n2https://huggingface.co/datasets/BAAI/COIG\n3https://apps.timwhitlock.info/emoji/tables/unicode\n\n[Image page=2 idx=1 name=Im1.png] Size: 160x160, Data: 33553 bytes\n\n[Image page=2 idx=2 name=Im1.png] Size: 160x160, Data: 21053 bytes\n\nTable 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical\nFLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.\nParams Tensor Pipeline Data Number Batch teraFLOP/s FLOPs\n(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization\n16 2 1 96 192 2304 162 51.90%\n51 4 2 24 192 2304 160 51.30%\n101 4 4 12 192 2160 165 52.88%\nbeneficial for both knowledge inheritance (Chen, Goodfel-\nlow, and Shlens 2016; Chen et al. 2022; Shen et al. 2022)\nand training stability (Yao et al. 2024). The growth oper-\nators used in FLM-101B training originate from Masked\nStructural Growth (MSG) (Yao et al. 2024), with adaptation.\nSpecifically, to adapt these operators to the multi-node 3D\nparallel framework, we implement them by extending the\nmodel structures offline and reloading the checkpoint when\nthe next stage starts.\nSchedules and Cost-Effectiveness. Model growth schedul-\ning is a trade-off between the pros and cons inherent to\nmodels of different sizes (Yao et al. 2024): a smaller model\nis faster in computation, enabling more rapid consumption\nof training data for broader commonsense knowledge; con-\nversely, a larger model is better in the reduction of loss per\nstep, indicating a deeper understanding of the nuanced lin-\nguistic patterns. Based on the speed test results and total\nbudget, we train the 16B model with 245.37B tokens, the\n51B model with 39.64B tokens, and the 101B model with\n26.54B tokens. The billion tokens per day of different sizes\nare listed in Table 1. Under this growth schedule, the total\ntime cost for training FLM-101B is 21.54 days, which is\n72% time-saving (or a 3.56x speedup) compared to training\na 101B model from scratch (76.74 days estimated). This is\nconsistent with our motivations depicted in Figure 1.\n2.4 The Parallelism Setup and Model\nConfigurations\nThe Parallel Strategies. FLM-101B is trained on a clus-\nter of 24 DGX-A800 GPU (8 √ó80G) servers. We employ a\n3D parallel strategy for optimal throughput, including the\nstandard data parallelism (Valiant 1990), tensor model par-\nallelism (Shoeybi et al. 2019), and pipeline model paral-\nlelism (Narayanan et al. 2021). Moreover, by employing\nsequence parallelism (Korthikanti et al. 2022), we slice the\ninputs to the Transformer core‚Äôs LayerNorm and Dropout\nlayers along the sequence length dimension, leading to addi-\ntional savings in GPU computational resources and memory\nutilization. We also utilize the Megetron-LM 4 implemen-\ntation of the distributed optimizer (Rajbhandari et al. 2019)\nto further reduce GPU memory consumption, which evenly\ndistributes the optimizer states across data parallel ranks.\nTable 2 shows the parallelism configurations and train-\ning throughput in each stage of FLM-101B training. In\ndifferent stages, we configure different Tensor Parallel √ó\n4https://github.com/NVIDIA/Megatron-LM\n16B stage\n51B stage\n101B stage\nProcessed Tokens (Billions)\nTraining Loss\nFigure 2: Training loss for FLM-101B models.\nPipeline Parallel sizes to achieve higher efficiency. The single-\nGPU throughput for all three training stages consistently ex-\nceeds 160 teraFLOPs/sec with a utilization rate of at least\n51.3%. For comparison, GLM-130B achieves 135 teraFLOP-\ns/sec (Zeng et al. 2023) with a 42.27% utilization rate. We\ncan also find that FLM-101B has a higher FLOP utilization\nrate than Megatron-LM (Korthikanti et al. 2022) under a\nsimilar model size.\nFLM-101B Configurations. The FLM-101B model is struc-\ntured with a hidden state dimension of 10, 240, a layer num-\nber of 80, a context window of 2,048 tokens, 80 attention\nheads, and a vocabulary size of 100, 256. FLM-101B uses\nthe AdamW optimizer (Loshchilov and Hutter 2017) with\nŒ≤1 = 0.9 and Œ≤2 = 0.95. A cosine learning rate schedule is\nemployed, leading to a final learning rate of 6e ‚àí 6. We use a\nweight decay of 0.1 and gradient clipping of 1.0.\nTable 1 presents part of the hyperparameters used in differ-\nent growth stages. In each growth stage, we approximately\ninherit the previous learning rate and adhere to the same\nschedule. The learning rate at the beginning of each stage is\nreported in the table. In the 16B stage, 4,608k samples are\nused for learning rate warmup, while in later growth stages,\nwe use fewer samples of 230.4k. Note that we do not apply\nbatch size warmup because we address the stability issue in\na different manner, detailed in Section 3.\n3 Training Stability of FLM-101B\nModels beyond 100B parameters (Scao et al. 2022; Zeng\net al. 2023) usually suffer from a bunch of notorious stability\nissues including loss divergence, gradient explosion, and nu-\nmerical overflow/underflow. This not only inflates the cost of\nsearching for feasible hyperparameters like optimal learning\n\n[Image page=3 idx=1 name=I1.png] Size: 1500x600, Data: 50847 bytes\n\n[Image page=3 idx=2 name=I2.png] Size: 4x1189, Data: 3607 bytes\n\n[Image page=3 idx=3 name=I3.png] Size: 4x1189, Data: 3607 bytes\n\n[Image page=3 idx=4 name=I4.png] Size: 4x1189, Data: 3077 bytes\n\n[Image page=3 idx=5 name=I5.png] Size: 4x1189, Data: 3694 bytes\n\nTable 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the\ncorresponding references. The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al. 2021).\nModel GPT-3\n(Brown et al. 2020)\nGopher\n(Rae et al. 2021)\nPaLM\n(Anil et al. 2023)\nGLM-130B\n(Zeng et al. 2023)\nLlama-2\n(Touvron et al. 2023b)FLM-101B\nParams 175B 280B 540B 130B 70B 101B\nGPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5\nChip Power/TDP 330 283 378.5 400 400 400\nEnergy (MkWh) 1171 1066 3179 444 688 40\nnet tCO2e 552 380 271 257 291 26\nrates, but also intensifies ongoing maintenance during train-\ning, such as babysitting, issue resolution, data adjustment,\nand rebooting. Moreover, this makes the budget of the whole\nproject unpredictable. In this section, we introduce details for\nmitigating these issues.\nPredictable Scaling. The Tensor Programs theories (Yang\nand Hu 2021; Littwin and Yang 2023) unveil the universal\nrelations of the training dynamics with the model width tend-\ning to infinite. This results in a parameterized mapping for\ncertain classes of hyperparameters between a small model\nand its larger counterparts, which is termed ¬µP (Yang et al.\n2021). Two important insights are: (i) The wider, the better:\ntheoretically, under ¬µP transfer, a wider model will always\nyield lower loss than its narrower counterparts when exposed\nto identical data (Yang et al. 2021). As a direct corollary, if a\nnarrow model converges, its wider counterparts will always\nconverge. (ii) Loss prediction: the loss value of a large model\nis predictable using the loss of its smaller counterparts (Ope-\nnAI 2023). ¬µScaling (Yao and Wang 2023) open-sources a\nmethodology through which loss prediction can be achieved\nby combining ¬µP (Yang et al. 2021) and (a modified) scaling\nlaw (Kaplan et al. 2020; Henighan et al. 2020; Hoffmann\net al. 2022).\nBased on these findings, our method to solve training sta-\nbility is as follows: we first determine the data distribution\nbefore the FLM-16B training starts. Next, we perform a grid\nsearch on three hyperparameters including the learning rate,\ninitialization standard deviation, and the softmax temperature\nin the output layer. This grid search is performed with a small\nproxy model (less than 100M) with a hidden state dimension\n(‚Äúmodel width‚Äù) of 256 and a head number of 2. All the other\nstructural configurations and training data are identical to\nthose of FLM-16B. A single run of grid search takes 24.6\nhours with data parallelism on 6 nodes, which is equivalent to\n6 hours per run given our 24-node infrastructure. Finally, we\nfind a group of well-performing hyperparameters: learning\nrate = 4e ‚àí 4, standard deviation = 1.6e ‚àí 2, and softmax\ntemperature = 2.0. Transferring these hyperparameters to the\n16B model via ¬µP (Yang et al. 2021) led to a seamless train-\ning experience devoid of instabilities. Combined with MSG\n(Yao et al. 2024), we also witness no post-growth divergence\nin FLM-51B and FLM-101B.\nOur implementations of ¬µP are largely consistent with\nthose in ¬µScaling (Yao and Wang 2023), with modifications\nto handle the rotary embedding. Thus, the value range of\nFLM-16B loss is also predictable with the results from multi-\nple proxy widths at the same steps. Mixed-precision training\nis applied to save run-time memory and reduce time costs.\nSpecifically, we chooseBfloat16 instead of FP16 due to its su-\nperior precision for values approaching zero, making it more\nsuitable for ¬µP. As a result, we do not encounter the FP16\nunderflow issue reported by (Yang et al. 2021). Moreover,\nBfloat16 negates the need for loss scale adjustments, making\nour training procedure more promising and reproducible.\nThe full training loss curve is presented in Figure 2. We\nobserve that the loss curve becomes steeper after each growth.\nIt matches the intuition that a larger model is better in loss\nreduction per step. The whole training procedure is robust\nand predictable: even though the 51B stage is short with only\n40B tokens, the 101B training remains stable. This supports\nthe effectiveness of the growth strategy.\n4 Benchmark Evaluation\nMany existing benchmarks ( e.g., Open LLM5) focus on as-\nsessing the knowledgeability of LLMs. In this section, we\ndiscuss the results of FLM on these benchmarks. We be-\nlieve that knowledge alone might not comprehensively reflect\nLLM‚Äôs capability (see Section 4.2 for more details). Thus, in\naddition to the common benchmark evaluation, we borrow\nthe concept of IQ tests and evaluate LLMs with some specific\ncognitive tasks in Section 5.\nCost Estimation Method. Due to the considerable computa-\ntional expense of LLMs, we also emphasize their associated\ncosts in our experimental results. However, it is difficult\nto directly compare the actual cost of LLMs due to their\ndifferent infrastructures and prices. To objectively compare\ntraining costs, we use the FLOPs for training as the cost\nestimation index, which is estimated from the model‚Äôs hy-\nperparameters, configuration, and training data (Narayanan\net al. 2021). Since most models do not release the complete\ntraining configuration, we estimate FLOPs within a range6.\nFor monolingual English LLMs, the computational cost of\nGPT-3 is calculated as 376.41 ( ¬±53.77) zettaFLOPs, and\nLlama-2 (13B) as 210.37 ( ¬±28.77) zettaFLOPs. For bilin-\ngual or multilingual models, it is necessary to estimate the\ncost for each language. The total cost of GLM-130B is com-\nputed as 421.60 zettaFLOPs. As the data ratio of English\n5https://huggingface.co/spaces/HuggingFaceH4/\nopen llm leaderboard.\n6This range originates from the use of checkpoint activation.\nPlease check (Narayanan et al. 2021) for more details.\n\nTable 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.We list the estimated floating-\npoint operations (zetta = 10 21) of the training process for reference.\nModel Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA\nLlama-2 (13B) 201.37 ( ¬±28.77) 58.66 59.39 82.13 55.77 37.38\nLlama-2 (7B) 106.60 ( ¬±15.23) 54.32 53.07 78.59 46.87 38.76\nLlama (13B) 94.81 ( ¬±13.54) 56.08 56.23 80.93 47.67 39.48\nLlama (7B) 49.54 ( ¬±7.08) 49.72 51.02 77.82 35.71 34.33\nGLM-130B 210.80 48.11 42.15 67.91 42.59 39.80\nFLM-101B 28.22 43.94 39.76 66.23 28.30 ‚àó 41.47\n‚àó44.50 for a knowledge-enhanced eFLM-16B (Section 2.2, 4.2).\nTable 5: Performance of eFLM-16B and baselines on C-eval.In this table, eFLM-16B refers to the professional-knowledge-\nenhanced FLM-16B. Note that C-Eval leaderboard only keeps one decimal place for the evaluation results.\nModel Average Average (Hard) STEM Social Science Humanities Others\nGPT-4 68.7 54.9 67.1 77.6 64.5 67.8\nChatGPT 54.4 41.4 52.9 61.8 50.9 53.6\nGLM-130B 44.0 30.7 36.7 55.8 47.7 43.0\neFLM-16B 46.1 28.9 38.3 53.7 46.8 52.6\nand Chinese is reported to be 1:1, the cost of GLM-130B\nfor English is 210.80 zettaFLOPs, and the same for Chinese.\nThe data ratio of FLM-101B is 53.5% : 46 .5% for English\nand Chinese. The total cost of FLM-101B is computed as\n52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54\nfor Chinese).\nCarbon Footprint Analysis. An important measurement of\na model‚Äôs environmental impact (Schwartz et al. 2020) is the\ncarbon footprints originated from the pre-training process.\nWe estimate carbon emission with the methods provided in\n(Patterson et al. 2021). We summarize the carbon footprint\nstatistics of FLM-101B and well-known LLMs in Table 3.\nOur model yields only 1/10 pre-training carbon footprint of a\ntypical LLM.\n4.1 Open LLM Evaluation\nOpen LLM Leaderboard is an open-source project to eval-\nuate the open-sourced LLMs and chatbots. By the time\nFLM-101B is trained, Open LLM contains four tasks: ARC-\nChallenge (ARC for short) (Clark et al. 2018), HellaSwag\n(Zellers et al. 2019), MMLU (Hendrycks et al. 2021), and\nTruthfulQA (Lin, Hilton, and Evans 2022). The Open LLM\nLeaderboard applies the average score as a metric. All the\nfour tasks require intense knowledge to solve: ARC, Hel-\nlaSwag, and TruthfulQA depend on commonsense knowl-\nedge and Wikipedia, while MMLU contains some questions\n(i.e., STEM) that require domain-specific professional knowl-\nedge and intricate reasoning.\nTable 4 details the performance of FLM-101B and strong\nbaselines, including Llama series and GLM-130B 7. GLM-\n130B results are achieved by our run on an open-sourced\ncheckpoint.\n7We exclude GPT-3 because it is closed-source. Probability val-\nues are unavailable for fair comparison.\nResults. On average, FLM-101B achieves a score of 43.94,\nreaching over 90% of the performance of GLM-130B, which\nhas 7 times more FLOPs. Both model underperform the\nLlama series, potentially due to the first-generation model\narchitectures and less well-refined training data. Note that the\nFLOPs of FLM-101B is even lower than a 7B Llama model.\nGoing deeper into the nature of these tasks, we further have\nthe following observations:\n(i) MMLU typically requires domain knowledge to solve.\nIn our training, no English textbook or exam data is intention-\nally used. Nevertheless, our eFLM-16B (Section 2.2) variant,\nincorporated with these knowledge via FreeLM objectives,\noutperforms GLM-130B with only 16B parameters.\n(ii) As aforementioned, TruthfulQA, ARC, and HellaSwag\nemphasize more on common sense and Wiki-level knowl-\nedge; their performances improve with the increased amount\nof data and the reduction of training loss. With less than 0.16T\nEnglish data (about 1/10 of Llama-2), FLM-101B already\nachieves the best accuracy of 41.47 among all the baselines\non TruthfulQA. On ARC and HellaSwag, FLM-101B is com-\nparable to GLM-130B with a similar amount of English data\n(approximately 0.2T). Also, the training data of GLM-130B\nincludes ARC and HellaSwag, as expressly claimed in (Zeng\net al. 2023). In our understanding, for FLM-101B, improve-\nment can be expected on these three tasks if exposed to more\ntraining data.\n4.2 Evaluation on the Professional\nKnowledge-Enhanced Model\nWe conduct experiments on a knowledge-enhanced ver-\nsion (eFLM-16B, detailed in Section 2.2) of the FLM to vali-\ndate the effect of domain-specific knowledge data on bench-\nmark results. We continue to train the smallest FLM-16B\nwith teacher signals (Li et al. 2023) from a combination of (i)\npart of the auxiliary training data of MMLU (Hendrycks et al.\n\nTable 6: Performance of the three stages of FLM on Open LLM.To reduce the computational cost during evaluation, we\nsample 20% and 30% items for HellaSwag and MMLU tasks, respectively.\nParameters Training Data Average ARC Hellaswag MMLU TruthfulQA\n16B 245.37B 39.19 32.25 58.57 27.02 38.92\n51B 39.64B 41.79 35.32 64.04 27.66 40.12\n101B 26.54B 44.41 39.76 67.88 28.54 41.47\n2021), (ii) exam questions in similar domains and formats to\nC-Eval (Huang et al. 2023) 8, and (iii) other domain knowl-\nedge data. Note that eFLM-16B is not a typical fine-tuning\nwith instruct data which may affect the language modeling\ncapability of LLM. We preserve both language and teacher\nsignals with the corresponding data in this continue-training.\nThe MMLU result is in the footnote of Table 4. Table 5 lists\nthe result of eFLM-16B and baselines on C-Eval.\nResults. Enhanced with professional knowledge, significant\nimprovements are observed. On MMLU tasks, the incorpo-\nration of professional knowledge data results in a score of\n44.50 for eFLM-16B (see Table 4), which surpasses GLM-\n130B (42.59), a model that also incorporated multi-task data\nin the related domain (Zeng et al. 2023). For comparison,\nthe MMLU score is 27.02 for the un-enhanced FLM-16B.\nOn C-Eval tasks 9, we observe that eFLM-16B performs\nbetter than GLM-130B by about 2 points. For comparison,\nthe average C-Eval score of the vanilla FLM-16B is 27.0,\nwhich underperforms GLM-130B. These results suggest that\nevaluation with professional knowledge may not fully reflect\nthe capability of LLMs, particularly when different LLMs\nare trained with different data collections, and some may not\ncome with a clear list.\n4.3 Evaluation of the Growth Strategy\nOur core method for reducing computational cost is the\ngrowth strategy. We would like to answer the questions of\nwhether our growth strategy is effective in knowledge in-\nheritance, and how model capabilities grow with size. We\nevaluate the performance of FLM on all the stages: 16B, 51B,\nand 101B. Table 6 shows the performance of FLM models at\neach stage.\nResults. As expected, the performance of FLM improves\nwith the increase in model size. FLM-101B achieves the\nbest performance on almost all tasks. This means that our\nmodel inherits knowledge from the previous stage after each\ngrowth. We also observe that the 101B model improves the\nperformance scores more significantly than the 51B model,\nwith less data. This indicates that the models are successfully\nincorporating new weights in training after growth, and tak-\ning advantage of larger parameter counts. The performance\non ARC and HellaSwag increases steadily and significantly,\nwhich corresponds well to the decline of the model loss.\nAgain, as we expected in Section 4.1, as more training data\nis processed, FLM‚Äôs performance on Open LLM improves.\n8C-Eval can be considered as a Chinese version of MMLU.\n9The scores are achieved on the test set by submitting to the\nC-Eval platform.\n5 Evaluation Inspired by IQ Tests\nSection 4 presents the evaluation of existing benchmarks,\nfocusing on knowledge. As we discussed in Section 1 and\n4.2, knowledge could not fully reflect the Intelligence Quo-\ntient (IQ) of LLMs. As a supplement, we conduct a series\nof IQ-test task evaluation in this section. For IQ evaluation,\nwe make necessary modifications to existing datasets (Wei\net al. 2023; Weston et al. 2015; Srivastava et al. 2023) or gen-\nerate new synthetic datasets. Specifically, the IQ test mainly\nconsiders four aspects: symbolic mapping, rule understand-\ning, pattern mining, and anti-interference. A common key\nproperty of these tasks is that they are dependent on the in-\nference and generalization in a new context, instead of the\npreviously-learned knowledge.\nCompared Methods. Borrowing psychological ideas that\nthe measurement of IQ is dependent on age 10, we mainly\nconsider existing models trained with similar amounts of data\nto FLM-101B. As a milestone of LLM development, GPT-\n3 (175B) (Brown et al. 2020) proposed in-context learning\nfor the first time. GLM-130B (Zeng et al. 2023) is the first\nopen English-Chinese bilingual LLM. Hence, we select them\nas baseline models. Both models are trained with 300 Àú400\nbillion tokens, which are in the same range as ours. GPT-\n3 focuses on English, and is not included in the Chinese-\nrelated evaluation ( i.e., CLUE-IQ). The results of GPT-3\nare achieved by API. GLM-130B is evaluated with its open-\nsourced checkpoint.\nTasks, Data, and Results. We curate evaluation benchmarks\nregarding four cognitive capabilities. We summarize the data\nand results in this section. For details in task definition and\ndata collection process, please see Appendix A.1 to A.4.\nSymbolic Mapping. An existing study (Wei et al. 2023)\npoints out that textual classification tasks ( e.g., sentiment\nclassification) often lack generalization. Considering this, we\nuse a symbolic mapping method to replace the original cate-\ngorical labels with symbols that are unlikely to be seen in any\ntraining data. Hence, we can evaluate the LLMs‚Äô language\nunderstanding ability as well as the generalization abilities\nto a new context. We form our evaluation task as in-context\nlearning with few-shot examples for each label. We con-\nstruct two symbolic mapping datasets, namely SuperGLUE-\nIQ and CLUE-IQ, built on SuperGLUE (Wang et al. 2019)\nand CLUE (Xu et al. 2020), respectively. Examples are illus-\ntrated in the Appendix (Figure 1).\nResults on SuperGLUE-IQ and CLUE-IQ are presented in\nTable 7 and Table 8, respectively. With less computation\n10https://ocw.mit.edu/ans7870/9/9.00SC/MIT9 00SCF11 text.\npdf, page 367.\n\nby one magnitude, FLM-101B achieves comparable per-\nformance with GPT-3 on SuperGLUE-IQ and outperforms\nGLM-130B on CLUE-IQ.\nTable 7: Performance on SuperGLUE-IQ of GPT-3, GLM-\n130B, and FLM-101B. Cost is computed in zettaFLOPs.\nModel Cost Average BoolQ WiC RTE WSC\nGPT-3 376.41 (¬±53.77) 47.60 50.84 53.33 48.38 37.86\nGLM-130B 210.80 48.19 40.13 48.67 47.65 56.31\nFLM-101B 28.22 46.76 49.50 50.33 48.38 38.83\nTable 8: Performance on CLUE-IQ for GLM-130B and\nFLM-101B. Cost is computed in zettaFLOPs.\nModel Cost Average AFQMC CSL OCNLI\nCLUE\nWSC\n2020\nGLM-130B210.80 39.96 33.33 53.85 34.0 38.67\nFLM-101B 24.54 42.07 38.33 55.29 27.33 47.33\nRule Understanding. We consider the understanding and\nexecution of rules being a strong indication of reasoning ca-\npability. To this end, we design rule understanding evaluation.\nNote that this test is different from reasoning based on the\nchain of thought. Detailed discussion is provided in Appendix\nA.2.\nWe curate data for two subtasks: Counting (0-shot) and\nString replacement (4-shots).\nTable 9: Performance of FLM-101B, GPT-3, and GLM-\n130B on rule understanding tasks.\nModel Average Counting Replace\nLowercase\nReplace\nWord\nGPT-3 86.03 82.43 80.67 95.00\nGLM-130B 71.49 60.81 69.67 84.00\nFLM-101B 76.42 69.59 64.00 95.67\nTable 9 shows the performance of our proposed FLM-101B\nagainst GPT-3 and GLM-130B on rule understanding tasks.\nFor Counting, FLM-101B achieves 69.59%, about 9 points\nbetter than GLM-130B. GPT-3 wins the first place in counting\nand Replace-Lowercase, and second place in Replace-Word.\nThis is potentially because GPT-3 is the largest model.\nPattern Mining. Pattern Mining evaluation is common in\nIQ tests. In detail, it is the induction and deduction of the\npatterns emerging in a new context.\nWe build a benchmark with three tasks (i.e., Head & Tail,\nFull Repeating, and Head Slicing) for evaluation. Figure 2\nin the Appendix shows examples of these tasks. Each task is\n5-shot and contains 100 instances.\nTable 10 lists the experimental results of our FLM-101B\nagainst the baselines on pattern mining tasks. On all three\ntasks, FLM-101B outperforms GLM-130B by a large margin.\nFor the Head & Tail and Full Repeating tasks, FLM-101B\nis a few points behind GPT-3, but outperforms the latter on\nthe Head Slicing task. Considering the computational cost,\nFLM-101B exhibits noticeable abilities.\nTable 10: Performance of FLM-101B, GPT-3, and GLM-\n130B on pattern mining tasks.\nModel Average Head & Tail Full Repeating Head Slicing\nGPT-3 70.00 61.00 92.00 57.00\nGLM-130B 53.00 38.00 70.00 51.00\nFLM-101B 64.67 52.00 79.00 63.00\nAnti-interference. Anti-interference capability is critical\nfor finding and utilizing information that is truly related to a\nspecific goal, in an unseen and noisy context (Appendix Fig-\nure 3). As suggested by the cocktail party problem in speech\nrecognition (Qian et al. 2018), we consider anti-interference\nability to be important for intelligent agents. We conduct\nanti-interference evaluation in three task types: Multiple Key\nRetrieval, Single Supporting Fact Tracking, and Two Sup-\nporting Facts Tracking, as exemplified in Figure 3 in the\nAppendix.\nTable 11: Performance of FLM-101B, GPT-3, and GLM-\n130B on anti-interference evaluation.\nModel Average\nMultiple\nKey\nRetrieval\nSingle\nSupporting\nFact\nTwo\nSupporting\nFacts\nGPT-3 70.11 92.67 78.33 39.33\nGLM-130B 53.56 77.67 56.33 26.67\nFLM-101B 60.11 89.00 59.00 32.33\nTable 11 shows the evaluation results on anti-interference.\nFLM-101B achieves the second-best passing rates with an\nadvantage of around 7% compared to GLM-130B.\nIQ Test Conclusion. On our four additional evaluations in-\nspired by the IQ tests, FLM-101B outperforms GLM-130B\nand obtains competitive results compared to GPT-3 in some\ntasks with much lower costs. Except for the impacts of train-\ning data, the superiority may be owed to a story that in the\ngrowth strategy, the smaller models in early stages refine a\nmore efficient searching space, which keeps taking effect\nwhen the model grows larger with increased generalization\nability.\n6 Conclusions, Limitations, and Future Work\nIn this paper, we introduce FLM-101B, an open-sourced\nLLM that is successfully trained from scratch within a\n$100,000 budget. The key idea of reducing the training cost\nof FLM-101B is to break through the fixed number of model\nparameters via a growth strategy. Experimental results on\nknowledeg-oriented and IQ-related benchmarks show that\nFLM-101B is comparable to strong baseline models with\nless computational cost. Note that harmful contents may be\ninduced from the open-sourced checkpoint, which do not\nrepresent the opinions of the authors.\nDue to resource issues, the limitations of our work include\ninadequate exploration and comparison for different growth\nschedules, growth operators, and amount of data. For future\nwork, we believe that our exploration on the growth strategy\n\nas well as training stability would potentially be beneficial\nfor future attempts of further scaling up LLMs, e.g., beyond\n1T parameters.\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.;\nPassos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu,\nE.; Clark, J. H.; Shafey, L. E.; Huang, Y .; Meier-Hellstern,\nK.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.;\nRuder, S.; Tay, Y .; Xiao, K.; Xu, Y .; Zhang, Y .;¬¥Abrego, G. H.;\nAhn, J.; Austin, J.; Barham, P.; Botha, J. A.; Bradbury, J.;\nBrahma, S.; Brooks, K.; Catasta, M.; Cheng, Y .; Cherry, C.;\nChoquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave, S.;\nDehghani, M.; Dev, S.; Devlin, J.; D¬¥ƒ±az, M.; Du, N.; Dyer,\nE.; Feinberg, V .; Feng, F.; Fienber, V .; Freitag, M.; Garcia,\nX.; Gehrmann, S.; Gonzalez, L.; and et al. 2023. PaLM 2\nTechnical Report. CoRR, abs/2305.10403.\nBi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.;\nDing, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek\nllm: Scaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877‚Äì\n1901.\nChen, C.; Yin, Y .; Shang, L.; Jiang, X.; Qin, Y .; Wang, F.;\nWang, Z.; Chen, X.; Liu, Z.; and Liu, Q. 2022. bert2BERT:\nTowards Reusable Pretrained Language Models. In Mure-\nsan, S.; Nakov, P.; and Villavicencio, A., eds.,Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, 2134‚Äì2148. Association\nfor Computational Linguistics.\nChen, T.; Goodfellow, I. J.; and Shlens, J. 2016. Net2Net:\nAccelerating Learning via Knowledge Transfer. In Bengio,\nY .; and LeCun, Y ., eds.,4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico,\nMay 2-4, 2016, Conference Track Proceedings.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have Solved\nQuestion Answering? Try ARC, the AI2 Reasoning Chal-\nlenge. CoRR, abs/1803.05457.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), 4171‚Äì4186. Association for Com-\nputational Linguistics.\nEriksson, P. S.; Perfilieva, E.; Bj¬®ork-Eriksson, T.; Alborn, A.-\nM.; Nordborg, C.; Peterson, D. A.; and Gage, F. H. 1998. Neu-\nrogenesis in the adult human hippocampus. Nature medicine,\n4(11): 1313‚Äì1317.\nFan, S.; Wang, Y .; Li, J.; Zhang, Z.; Shang, S.; and Han,\nP. 2022. Interactive Information Extraction by Semantic\nInformation Graph. In Raedt, L. D., ed., Proceedings of\nthe Thirty-First International Joint Conference on Artificial\nIntelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022,\n4100‚Äì4106. ijcai.org.\nGong, L.; He, D.; Li, Z.; Qin, T.; Wang, L.; and Liu, T.\n2019. Efficient training of bert by progressively stacking. In\nInternational conference on machine learning, 2337‚Äì2346.\nPMLR.\nGu, X.; Liu, L.; Yu, H.; Li, J.; Chen, C.; and Han, J. 2021. On\nthe Transformer Growth for Progressive BERT Training. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 5174‚Äì5180.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multi-\ntask Language Understanding. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHenighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.;\nJackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.;\nHallacy, C.; Mann, B.; Radford, A.; Ramesh, A.; Ryder, N.;\nZiegler, D. M.; Schulman, J.; Amodei, D.; and McCandlish, S.\n2020. Scaling Laws for Autoregressive Generative Modeling.\nCoRR, abs/2010.14701.\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.;\nCai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.;\nWelbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.;\nvan den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.;\nSimonyan, K.; Elsen, E.; Vinyals, O.; Rae, J. W.; and Sifre,\nL. 2022. An empirical analysis of compute-optimal large\nlanguage model training. In NeurIPS.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He, J. 2023.\nC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models. CoRR, abs/2305.08322.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling Laws for Neural Language Mod-\nels. CoRR, abs/2001.08361.\nKorthikanti, V .; Casper, J.; Lym, S.; McAfee, L.; Ander-\nsch, M.; Shoeybi, M.; and Catanzaro, B. 2022. Reducing\nActivation Recomputation in Large Transformer Models.\narXiv:2205.05198.\nLi, X.; Jiang, X.; Meng, X.; Sun, A.; and Wang, Y .\n2023. FreeLM: Fine-Tuning-Free Language Model. CoRR,\nabs/2305.01616.\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Measur-\ning How Models Mimic Human Falsehoods. In Muresan, S.;\nNakov, P.; and Villavicencio, A., eds.,Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, 3214‚Äì3252. Association for Computational\nLinguistics.\nLittwin, E.; and Yang, G. 2023. Adaptive Optimization in the\n‚àû-Width Limit. In The Eleventh International Conference\n\non Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nLiu, Y .; Wang, Y .; Sun, A.; Meng, X.; Li, J.; and Guo, J. 2022.\nA Dual-Channel Framework for Sarcasm Recognition by\nDetecting Sentiment Conflict. In Carpuat, M.; de Marneffe,\nM.; and Ru ¬¥ƒ±z, I. V . M., eds., Findings of the Association\nfor Computational Linguistics: NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022, 1670‚Äì1680. Association for\nComputational Linguistics.\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay\nRegularization in Adam. CoRR, abs/1711.05101.\nMeng, X.; Lin, C.; Wang, Y .; and Zhang, Y . 2023. Net-\nGPT: Generative Pretrained Transformer for Network Traffic.\nCoRR, abs/2304.09513.\nMeta. 2024. Introducing Meta Llama 3: The most capable\nopenly available LLM to date. https://ai.meta.com/blog/meta-\nllama-3/.\nMistral. 2024. Mistral 8x22B. https://mistral.ai/news/mixtral-\n8x22b/.\nNarayanan, D.; Shoeybi, M.; Casper, J.; LeGresley, P.; Pat-\nwary, M.; Korthikanti, V .; Vainbrand, D.; Kashinkunti, P.;\nBernauer, J.; Catanzaro, B.; Phanishayee, A.; and Zaharia,\nM. 2021. Efficient Large-Scale Language Model Training on\nGPU Clusters. CoRR, abs/2104.04473.\nOpenAI. 2023. GPT-4 Technical Report. CoRR,\nabs/2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.;\nAskell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe,\nR. 2022. Training language models to follow instructions\nwith human feedback. In NeurIPS.\nPatterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia, L.-\nM.; Rothchild, D.; So, D.; Texier, M.; and Dean, J. 2021.\nCarbon emissions and large neural network training. arXiv\npreprint arXiv:2104.10350.\nPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cap-\npelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and\nLaunay, J. 2023. The RefinedWeb dataset for Falcon LLM:\noutperforming curated corpora with web data, and web data\nonly. arXiv preprint arXiv:2306.01116.\nQian, Y .; Weng, C.; Chang, X.; Wang, S.; and Yu, D. 2018.\nPast review, current progress, and challenges ahead on the\ncocktail party problem. Frontiers Inf. Technol. Electron. Eng.,\n19(1): 40‚Äì63.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised Mul-\ntitask Learners.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.;\nSong, H. F.; Aslanides, J.; Henderson, S.; Ring, R.; Young,\nS.; Rutherford, E.; Hennigan, T.; Menick, J.; Cassirer, A.;\nPowell, R.; van den Driessche, G.; Hendricks, L. A.; Rauh,\nM.; Huang, P.; Glaese, A.; Welbl, J.; Dathathri, S.; Huang, S.;\nUesato, J.; Mellor, J.; Higgins, I.; Creswell, A.; McAleese,\nN.; Wu, A.; Elsen, E.; Jayakumar, S. M.; Buchatskaya, E.;\nBudden, D.; Sutherland, E.; Simonyan, K.; Paganini, M.;\nSifre, L.; Martens, L.; Li, X. L.; Kuncoro, A.; Nematzadeh,\nA.; Gribovskaya, E.; Donato, D.; Lazaridou, A.; Mensch,\nA.; Lespiau, J.; Tsimpoukelli, M.; Grigorev, N.; Fritz, D.;\nSottiaux, T.; Pajarskas, M.; Pohlen, T.; Gong, Z.; Toyama,\nD.; de Masson d‚ÄôAutume, C.; Li, Y .; Terzi, T.; Mikulik, V .;\nBabuschkin, I.; Clark, A.; de Las Casas, D.; Guy, A.; Jones,\nC.; Bradbury, J.; Johnson, M. J.; Hechtman, B. A.; Weidinger,\nL.; Gabriel, I.; Isaac, W.; Lockhart, E.; Osindero, S.; Rimell,\nL.; Dyer, C.; Vinyals, O.; Ayoub, K.; Stanway, J.; Bennett, L.;\nHassabis, D.; Kavukcuoglu, K.; and Irving, G. 2021. Scal-\ning Language Models: Methods, Analysis & Insights from\nTraining Gopher. CoRR, abs/2112.11446.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21: 140:1‚Äì140:67.\nRajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y . 2019.\nZeRO: Memory Optimization Towards Training A Trillion\nParameter Models. CoRR, abs/1910.02054.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow,\nD.; Castagn¬¥e, R.; Luccioni, A. S.; Yvon, F.; Gall¬¥e, M.; Tow,\nJ.; Rush, A. M.; Biderman, S.; Webson, A.; Ammanamanchi,\nP. S.; Wang, T.; Sagot, B.; Muennighoff, N.; del Moral, A. V .;\nRuwase, O.; Bawden, R.; Bekman, S.; McMillan-Major, A.;\nBeltagy, I.; Nguyen, H.; Saulnier, L.; Tan, S.; Suarez, P. O.;\nSanh, V .; Laurenc ¬∏on, H.; Jernite, Y .; Launay, J.; Mitchell, M.;\nRaffel, C.; Gokaslan, A.; Simhi, A.; Soroa, A.; Aji, A. F.; Al-\nfassy, A.; Rogers, A.; Nitzav, A. K.; Xu, C.; Mou, C.; Emezue,\nC.; Klamm, C.; Leong, C.; van Strien, D.; Adelani, D. I.; and\net al. 2022. BLOOM: A 176B-Parameter Open-Access Mul-\ntilingual Language Model. CoRR, abs/2211.05100.\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2020.\nGreen ai. Communications of the ACM, 63(12): 54‚Äì63.\nShen, S.; Walsh, P.; Keutzer, K.; Dodge, J.; Peters, M. E.; and\nBeltagy, I. 2022. Staged Training for Transformer Language\nModels. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesv ¬¥ari,\nC.; Niu, G.; and Sabato, S., eds.,International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA , volume 162 of Proceedings of Machine\nLearning Research, 19893‚Äì19908. PMLR.\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,\nJ.; and Catanzaro, B. 2019. Megatron-LM: Training Multi-\nBillion Parameter Language Models Using Model Paral-\nlelism. CoRR, abs/1909.08053.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; et al. 2023. Beyond the Imitation Game: Quanti-\nfying and extrapolating the capabilities of language models.\nTransactions on Machine Learning Research.\nSu, J.; Lu, Y .; Pan, S.; Wen, B.; and Liu, Y . 2021. Ro-\nFormer: Enhanced Transformer with Rotary Position Em-\nbedding. CoRR, abs/2104.09864.\nSun, Y .; Dong, L.; Patra, B.; Ma, S.; Huang, S.; Benhaim,\nA.; Chaudhary, V .; Song, X.; and Wei, F. 2023. A Length-\n\nExtrapolatable Transformer. In Rogers, A.; Boyd-Graber,\nJ. L.; and Okazaki, N., eds., Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023, 14590‚Äì14604. Association for Computational\nLinguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023a. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cu-\ncurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller,\nB.; Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hos-\nseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa,\nM.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poulton,\nA.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva,\nR.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Tay-\nlor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.;\nZhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,\nA.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama\n2: Open Foundation and Fine-Tuned Chat Models. CoRR,\nabs/2307.09288.\nValiant, L. G. 1990. A Bridging Model for Parallel Computa-\ntion. Commun. ACM, 33(8): 103‚Äì111.\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.; Michael,\nJ.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Un-\nderstanding Systems. In Wallach, H. M.; Larochelle, H.;\nBeygelzimer, A.; d‚ÄôAlch¬¥e-Buc, F.; Fox, E. B.; and Garnett,\nR., eds., Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, 3261‚Äì3275.\nWang, Y .; Li, X.; Sun, A.; Meng, X.; Liao, H.; and Guo, J.\n2022a. CofeNet: Context and Former-Label Enhanced Net for\nComplicated Quotation Extraction. In Calzolari, N.; Huang,\nC.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.; Ryu, P.;\nChen, H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue,\nN.; Kim, S.; Hahm, Y .; He, Z.; Lee, T. K.; Santus, E.; Bond,\nF.; and Na, S., eds., Proceedings of the 29th International\nConference on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022, 2438‚Äì\n2449. International Committee on Computational Linguistics.\nWang, Y .; Zhang, H.; Sun, A.; and Meng, X. 2022b. CORT:\nA New Baseline for Comparative Opinion Classification by\nDual Prompts. In Goldberg, Y .; Kozareva, Z.; and Zhang,\nY ., eds.,Findings of the Association for Computational Lin-\nguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, 7064‚Äì7075. Association for Computa-\ntional Linguistics.\nWatkins, C. E.; Campbell, V . L.; Nieberding, R.; and Hall-\nmark, R. 1995. Contemporary practice of psychological as-\nsessment by clinical psychologists. Professional psychology:\nResearch and practice, 26(1): 54.\nWei, J. W.; Hou, L.; Lampinen, A. K.; Chen, X.; Huang,\nD.; Tay, Y .; Chen, X.; Lu, Y .; Zhou, D.; Ma, T.; and Le,\nQ. V . 2023. Symbol tuning improves in-context learning in\nlanguage models. CoRR, abs/2305.08298.\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.;\nVan Merri¬®enboer, B.; Joulin, A.; and Mikolov, T. 2015. To-\nwards ai-complete question answering: A set of prerequisite\ntoy tasks. arXiv preprint arXiv:1502.05698.\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;\nSun, K.; Yu, D.; Yu, C.; Tian, Y .; Dong, Q.; Liu, W.; Shi,\nB.; Cui, Y .; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y .; Pat-\nterson, Y .; Tian, Z.; Zhang, Y .; Zhou, H.; Liu, S.; Zhao, Z.;\nZhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and\nLan, Z. 2020. CLUE: A Chinese Language Understanding\nEvaluation Benchmark. In Scott, D.; Bel, N.; and Zong, C.,\neds., Proceedings of the 28th International Conference on\nComputational Linguistics, COLING 2020, Barcelona, Spain\n(Online), December 8-13, 2020 , 4762‚Äì4772. International\nCommittee on Computational Linguistics.\nYang, G.; and Hu, E. J. 2021. Tensor Programs IV: Feature\nLearning in Infinite-Width Neural Networks. In Meila, M.;\nand Zhang, T., eds., Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of Machine\nLearning Research, 11727‚Äì11737. PMLR.\nYang, G.; Hu, E. J.; Babuschkin, I.; Sidor, S.; Liu, X.; Farhi,\nD.; Ryder, N.; Pachocki, J.; Chen, W.; and Gao, J. 2021. Tun-\ning Large Neural Networks via Zero-Shot Hyperparameter\nTransfer. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.;\nLiang, P.; and Vaughan, J. W., eds.,Advances in Neural In-\nformation Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, 17084‚Äì17097.\nYao, Y .; and Wang, Y . 2023. Research without Re-search:\nMaximal Update Parametrization Yields Accurate Loss Pre-\ndiction across Scales. CoRR, abs/2304.06875.\nYao, Y .; Zhang, Z.; Li, J.; and Wang, Y . 2024. Masked Struc-\ntural Growth for 2x Faster Language Model Pre-training. In\nThe Twelfth International Conference on Learning Represen-\ntations.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. HellaSwag: Can a Machine Really Finish Your\nSentence? In Korhonen, A.; Traum, D. R.; and M`arquez, L.,\neds., Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers, 4791‚Äì4800.\nAssociation for Computational Linguistics.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;\nYang, Z.; Xu, Y .; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.;\nXue, Y .; Zhai, J.; Chen, W.; Liu, Z.; Zhang, P.; Dong, Y .;\nand Tang, J. 2023. GLM-130B: An Open Bilingual Pre-\ntrained Model. In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\n\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.; and Wen, J. 2023. A Survey of Large\nLanguage Models. CoRR, abs/2303.18223.", "metadata": {"url": "https://arxiv.org/pdf/2309.03852", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "FLM-101B: An Open LLM and How to Train It with $100K Budget\nXiang Li1‚Ä†, Yiqun Yao1‚Ä†, Xin Jiang1‚Ä†, Xuezhi Fang1‚Ä†, Xuying Meng2,\nSiqi Fan3, Peng Han3, Jing Li4, Li Du1, Bowen Qin1, Zheng Zhang1,\nAixin Sun5, Yequan Wang1‚àó\n1Beijing Academy of Artificial Intelligence, Beijing, China\n2Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n3University of Electronic Science and Technology of China, Chengdu, China\n4Harbin Institute of Technology, Shenzhen, China\n5School of Computer Science and Engineering, Nanyang Technological University, Singapore\nAbstract\nLarge language models (LLMs) are considered important ap-\nproaches towards foundational machine intelligence, achiev-\ning remarkable success in Natural Language Processing and\nmultimodal tasks, among others. However, the carbon foot-\nprints and financial costs originating from heavy pre-training\ncomputation is a non-negligible issue. Progressive training\nmethods, inspired by the neurogenesis process that grows\nneural structures, have shown potential to accelerate LLM\npre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B pa-\nrameters remain underexplored. In this paper, we show that\nour model, namely FLM-101B, trained with our growth strat-\negy under a budget of $100K, reaches 80% of the baselines‚Äô\nperformances with only 10% of their floating-point operations.\nWe believe that further studies on progressive training will ben-\nefit the community by cutting down the costs and promoting\ngreen AI. The checkpoint of FLM-101B is publicly available.\n1 Introduction\nLarge language models (LLMs) (Radford et al. 2018; Tou-\nvron et al. 2023a; Devlin et al. 2019; Raffel et al. 2020) have\nconsistently demonstrated their efficacy across a spectrum of\napplications, especially in language processing (Wang et al.\n2022b,a; Fan et al. 2022; Liu et al. 2022) and multimodal\ntasks (Zhao et al. 2023; Meng et al. 2023). Despite variations\nin architectural designs, a universal challenge confronting\nall LLMs is the escalating cost associated with their train-\ning. Recent trends indicate a shift towards utilizing larger\namounts of data (e.g., 1.4T tokens for Llama-1 (Touvron et al.\n2023a), 2T tokens for Llama-2 (Touvron et al. 2023b), and\n15T tokens for Llama-3 (Meta 2024)). Meanwhile, the sizes\nof open-sourced models continue to increase (Penedo et al.\n2023; Bi et al. 2024; Mistral 2024). Consequently, a major\nfocus within LLM research is the development of innovative\nmethodologies that effectively mitigate training expenses,\naligning with the broader objectives of Green AI (Schwartz\net al. 2020).\nIn this paper, we present our exploration to train an LLM\nat the 100B-parameter scale using a growth strategy inspired\nby previous research on progressive learning (Gong et al.\n2019; Gu et al. 2021; Yao et al. 2024) and neurogenesis\nNon-Growth vs. three Growth Strategies\n(a) Without growth\ntokens (Trillion)\nparameters (Billion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nThe shaded area in the graph represents the training cost\n(b) Linear growth strategy  cost saving = 50\n(c) Superlinear growth  strategy  cost saving > 50%\n(d) Sublinear growth strategy  cost saving < 50%\nparameters (Billion)\ntokens (Trillion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nSublinear\nSuperlinear\n  Linear\nFigure 1: An overview of different growth strategies. (a):\na baseline with constant number of parameters. (b): a straight-\nforward linear growth strategy, cost-saving being exactly\n50%; (c): a superlinear strategy with > 50% cost saving; (d):\nsublinear strategy saving the cost by less than 50%.\n(Eriksson et al. 1998). ‚ÄúGrowth‚Äù means dynamic expansion of\nthe parameter number count, from small to large, through the\ntraining progresses. Figure 1 illustrates three typical growth\nstrategies: linear, sublinear, and superlinear. As the FLOPs\nof LLMs are approximately proportional to their number\nof parameters (Hoffmann et al. 2022), the area under the\nparameter curve represents the computational cost of training.\nWhile existing studies on scaling laws (Hoffmann et al.\n2022) suggest that training a smaller model with more data\nmay potentially result in higher scores on some tasks un-\nder a fixed FLOPs budget, they mainly consider the scenar-\nios where model sizes are fixed through training. We be-\nlieve that verifying the feasibility of a growth strategy (Gu\net al. 2021; Shen et al. 2022; Chen et al. 2022; Yao et al.\n2024) for extremely large models would be an important\ncompletion to scaling laws. To maximize computational effi-\nciency, we strategically focus on implementing an aggressive\ngrowth strategy (Figure 1 (c)) for sanity check. We adapt the\nMSG (Yao et al. 2024) growth operators to train a model at\n100B+ scale. We fix our budget to be$100K with 192 A800\nGPUs.\nAt the 100B scale, it is impractical to conduct strict head-\narXiv:2309.03852v3  [cs.CL]  14 Jan 2025", "sentences": [{"text": "FLM-101B: An Open LLM and How to Train It with $100K Budget\nXiang Li1‚Ä†, Yiqun Yao1‚Ä†, Xin Jiang1‚Ä†, Xuezhi Fang1‚Ä†, Xuying Meng2,\nSiqi Fan3, Peng Han3, Jing Li4, Li Du1, Bowen Qin1, Zheng Zhang1,\nAixin Sun5, Yequan Wang1‚àó\n1Beijing Academy of Artificial Intelligence, Beijing, China\n2Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n3University of Electronic Science and Technology of China, Chengdu, China\n4Harbin Institute of Technology, Shenzhen, China\n5School of Computer Science and Engineering, Nanyang Technological University, Singapore\nAbstract\nLarge language models (LLMs) are considered important ap-\nproaches towards foundational machine intelligence, achiev-\ning remarkable success in Natural Language Processing and\nmultimodal tasks, among others.", "metadata": {}}, {"text": "However, the carbon foot-\nprints and financial costs originating from heavy pre-training\ncomputation is a non-negligible issue.", "metadata": {}}, {"text": "Progressive training\nmethods, inspired by the neurogenesis process that grows\nneural structures, have shown potential to accelerate LLM\npre-training.", "metadata": {}}, {"text": "However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B pa-\nrameters remain underexplored.", "metadata": {}}, {"text": "In this paper, we show that\nour model, namely FLM-101B, trained with our growth strat-\negy under a budget of $100K, reaches 80% of the baselines‚Äô\nperformances with only 10% of their floating-point operations.", "metadata": {}}, {"text": "We believe that further studies on progressive training will ben-\nefit the community by cutting down the costs and promoting\ngreen AI.", "metadata": {}}, {"text": "The checkpoint of FLM-101B is publicly available.", "metadata": {}}, {"text": "1 Introduction\nLarge language models (LLMs) (Radford et al.", "metadata": {}}, {"text": "2018;", "metadata": {}}, {"text": "Tou-\nvron et al.", "metadata": {}}, {"text": "2023a;", "metadata": {}}, {"text": "Devlin et al.", "metadata": {}}, {"text": "2019;", "metadata": {}}, {"text": "Raffel et al.", "metadata": {}}, {"text": "2020) have\nconsistently demonstrated their efficacy across a spectrum of\napplications, especially in language processing (Wang et al.", "metadata": {}}, {"text": "2022b,a;", "metadata": {}}, {"text": "Fan et al.", "metadata": {}}, {"text": "2022;", "metadata": {}}, {"text": "Liu et al.", "metadata": {}}, {"text": "2022) and multimodal\ntasks (Zhao et al.", "metadata": {}}, {"text": "2023;", "metadata": {}}, {"text": "Meng et al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "Despite variations\nin architectural designs, a universal challenge confronting\nall LLMs is the escalating cost associated with their train-\ning.", "metadata": {}}, {"text": "Recent trends indicate a shift towards utilizing larger\namounts of data (e.g., 1.4T tokens for Llama-1 (Touvron et al.", "metadata": {}}, {"text": "2023a), 2T tokens for Llama-2 (Touvron et al.", "metadata": {}}, {"text": "2023b), and\n15T tokens for Llama-3 (Meta 2024)).", "metadata": {}}, {"text": "Meanwhile, the sizes\nof open-sourced models continue to increase (Penedo et al.", "metadata": {}}, {"text": "2023;", "metadata": {}}, {"text": "Bi et al.", "metadata": {}}, {"text": "2024;", "metadata": {}}, {"text": "Mistral 2024).", "metadata": {}}, {"text": "Consequently, a major\nfocus within LLM research is the development of innovative\nmethodologies that effectively mitigate training expenses,\naligning with the broader objectives of Green AI (Schwartz\net al.", "metadata": {}}, {"text": "2020).", "metadata": {}}, {"text": "In this paper, we present our exploration to train an LLM\nat the 100B-parameter scale using a growth strategy inspired\nby previous research on progressive learning (Gong et al.", "metadata": {}}, {"text": "2019;", "metadata": {}}, {"text": "Gu et al.", "metadata": {}}, {"text": "2021;", "metadata": {}}, {"text": "Yao et al.", "metadata": {}}, {"text": "2024) and neurogenesis\nNon-Growth vs.", "metadata": {}}, {"text": "three Growth Strategies\n(a) Without growth\ntokens (Trillion)\nparameters (Billion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nThe shaded area in the graph represents the training cost\n(b) Linear growth strategy  cost saving = 50\n(c) Superlinear growth  strategy  cost saving > 50%\n(d) Sublinear growth strategy  cost saving < 50%\nparameters (Billion)\ntokens (Trillion)\n0\n20\n40\n60\n0.750.500.250.00 1.00\n80\n100\nSublinear\nSuperlinear\n  Linear\nFigure 1: An overview of different growth strategies.", "metadata": {}}, {"text": "(a):\na baseline with constant number of parameters.", "metadata": {}}, {"text": "(b): a straight-\nforward linear growth strategy, cost-saving being exactly\n50%;", "metadata": {}}, {"text": "(c): a superlinear strategy with > 50% cost saving;", "metadata": {}}, {"text": "(d):\nsublinear strategy saving the cost by less than 50%.", "metadata": {}}, {"text": "(Eriksson et al.", "metadata": {}}, {"text": "1998).", "metadata": {}}, {"text": "‚ÄúGrowth‚Äù means dynamic expansion of\nthe parameter number count, from small to large, through the\ntraining progresses.", "metadata": {}}, {"text": "Figure 1 illustrates three typical growth\nstrategies: linear, sublinear, and superlinear.", "metadata": {}}, {"text": "As the FLOPs\nof LLMs are approximately proportional to their number\nof parameters (Hoffmann et al.", "metadata": {}}, {"text": "2022), the area under the\nparameter curve represents the computational cost of training.", "metadata": {}}, {"text": "While existing studies on scaling laws (Hoffmann et al.", "metadata": {}}, {"text": "2022) suggest that training a smaller model with more data\nmay potentially result in higher scores on some tasks un-\nder a fixed FLOPs budget, they mainly consider the scenar-\nios where model sizes are fixed through training.", "metadata": {}}, {"text": "We be-\nlieve that verifying the feasibility of a growth strategy (Gu\net al.", "metadata": {}}, {"text": "2021;", "metadata": {}}, {"text": "Shen et al.", "metadata": {}}, {"text": "2022;", "metadata": {}}, {"text": "Chen et al.", "metadata": {}}, {"text": "2022;", "metadata": {}}, {"text": "Yao et al.", "metadata": {}}, {"text": "2024) for extremely large models would be an important\ncompletion to scaling laws.", "metadata": {}}, {"text": "To maximize computational effi-\nciency, we strategically focus on implementing an aggressive\ngrowth strategy (Figure 1 (c)) for sanity check.", "metadata": {}}, {"text": "We adapt the\nMSG (Yao et al.", "metadata": {}}, {"text": "2024) growth operators to train a model at\n100B+ scale.", "metadata": {}}, {"text": "We fix our budget to be$100K with 192 A800\nGPUs.", "metadata": {}}, {"text": "At the 100B scale, it is impractical to conduct strict head-\narXiv:2309.03852v3  [cs.CL]  14 Jan 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "to-head comparison with the same model trained with fixed\nsize from scratch. Instead, we compare our grown model,\nnamely FLM-101B, with the existing first-generation 100B+\nlanguage models (Brown et al. 2020; Zeng et al. 2023). Our\nmodel is trained with around 300 billion English and Chi-\nnese tokens, aligning it with these predecessors in terms\nof data scale. We first evaluate on the knowledge-oriented\nbenchmarks (i.e., MMLU (Hendrycks et al. 2021) and C-\nEval (Huang et al. 2023)). Nevertheless, such evaluation may\nnot comprehensively reflect the models‚Äô capability: it is diffi-\ncult to distinguish whether the models recall a piece of knowl-\nedge or possess the capacity for reasoning and/or inference.\nBorrowing some ideas from Intelligence Quotient (IQ) tests\n(i.e., Perceptual Reasoning and Working Memory (Watkins\net al. 1995)), we consolidate another range of evaluation,\nincluding symbolic mapping (Wei et al. 2023), rule under-\nstanding, pattern mining, and anti-interference evaluations.\nWe believe these tasks are less likely to be affected by data\nleakage or memorization, offering a more nuanced insight\ninto the model‚Äôs cognitive abilities beyond mere knowledge\nretrieval.\nTo summarize, the paper has made the following contri-\nbutions. First, to the best of our knowledge, this is the first\nattempt to use a growth strategy to train an LLM with 100B+\nparameters from scratch. The training costs only 100,000\nUS dollars. Second, we demonstrate details for addressing\nthe instability issues via improving training objectives, hyper-\nparameter search, and function-preserving growth. Third, we\nconduct extensive evaluations, including both the commonly\nused knowledge-oriented benchmarks and the new range of\nevaluations inspired by IQ tests. Experimental results show\nthat, despite its low training cost, FLM-101B is competitive\nand robust. Lastly, we will release the model checkpoints,\nas well as some related code and tools, to promote related\nresearch. Related literature is reviewed in Appendix B.\n2 Design Overview of FLM-101B\nIn this section, we provide an outline of FLM-101B, detail-\ning its architecture, pre-training methods, and configuration\nspecifics.\n2.1 Architecture\nBackbone. Among the many existing model architectures,\nwe adopt FreeLM (Li et al. 2023) as the backbone for\nour models, with modifications. FreeLM is based on GPT\n(Radford et al. 2019), a transformer-like architecture with\na decoder-only configuration. Different from GPT, FreeLM\nfeatures two pre-training objectives: the language objective\nand the teacher objective (Section 2.2). We preserve the GPT-\n3-style transformer block designs without incorporating the\nlater modifications from Llama series. We employ the tok-\nenizer derived from GPT-4, characterized by a vocabulary\nsize of 100, 256.\nxPos Integration. To enhance long sequence modeling, we\nintegrate the Extrapolatable Position Embedding (xPos) (Sun\net al. 2023). This innovation draws inspiration from RoPE (Su\net al. 2021), which aims to improve the length extrapolation\nability by introducing an exponential decay into the rotation\nmatrix.\nModel Sizes. Benefiting from our growth strategy, the we\nproduce three models with 16B, 51B, and 101B (i.e., FLM-\n101B) parameters in a single training. The training process is\ncarried out in a progressive manner by growing a 16B model\nto 51B, and then 101B.\n2.2 Pre-Training Setup\nFLM-101B. By design, FLM-101B is an English-Chinese\nbilingual model. It mixes English and Chinese corpora at a\nratio of approximately 53.5% : 46 .5%. Inspired by the find-\ning that instruction data can augment LLMs‚Äô comprehension\ncapabilities (Ouyang et al. 2022), we integrate multi-task\ninstructionally prompted data: OIG (Open Instruction Gen-\neralist) 1 and COIG (Chinese Open Instruction Generalist) 2,\nin the pre-training stage.\neFLM-16B. To analyse the effect of domain-specific knowl-\nedge data (Section 4.2), we apply the FreeLM teacher signals\n(Li et al. 2023) to enhance factual capability. Due to computa-\ntional cost, we incorporate these signals only in the smallest\n16B model. This enhanced model is named eFLM-16B.\nSpecifically, we employ two emojis:\nüòà (U+1F621) and\nüò° \n(U+1F608) 3, from the vocabulary to replace the original\nbinary classification labels (Li et al. 2023). For the teacher\nsignal, we supervise only on the emoji tokens, unifying the\nobjective with language modeling. Moreover, we discard the\noriginal multi-task alternating approach and completely mix\nthe samples from both sides in every batch. This strategy can\nenhance the consistency of data sampling distribution as well\nas improve training stability.\nTable 1: Partial configurations for different growth stages.\nParams Learning Warmup Batch Tokens Time Tokens\n(billion) Rate (samples) (million) (day) (billion)\n16 4e‚àí4 4,608,000 4.72 9.63 245.37\n51 3.4e‚àí4 230,400 4.72 5.37 39.64\n101 2e‚àí4 230,400 4.31 6.54 26.54\n2.3 Growth Strategy\nThe essence of the low cost in scaling up FLM-101B is the\ngrowth strategy. Specifically, we train three models, with\n16B, 51B, and 101B parameters, respectively, in a sequential\nmanner. Each model inherits knowledge from its predecessor.\nThis is contrary to the common practice that the models\nof different sizes are trained independently (Touvron et al.\n2023a,b).\nFunction-preserving Growth. Function preservation means\nthat before and after growth, the models yield consistent out-\nputs given the same arbitrary inputs. This property has proven\n1https://huggingface.co/datasets/laion/OIG\n2https://huggingface.co/datasets/BAAI/COIG\n3https://apps.timwhitlock.info/emoji/tables/unicode", "sentences": [{"text": "to-head comparison with the same model trained with fixed\nsize from scratch.", "metadata": {}}, {"text": "Instead, we compare our grown model,\nnamely FLM-101B, with the existing first-generation 100B+\nlanguage models (Brown et al.", "metadata": {}}, {"text": "2020;", "metadata": {}}, {"text": "Zeng et al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "Our\nmodel is trained with around 300 billion English and Chi-\nnese tokens, aligning it with these predecessors in terms\nof data scale.", "metadata": {}}, {"text": "We first evaluate on the knowledge-oriented\nbenchmarks (i.e., MMLU (Hendrycks et al.", "metadata": {}}, {"text": "2021) and C-\nEval (Huang et al.", "metadata": {}}, {"text": "2023)).", "metadata": {}}, {"text": "Nevertheless, such evaluation may\nnot comprehensively reflect the models‚Äô capability: it is diffi-\ncult to distinguish whether the models recall a piece of knowl-\nedge or possess the capacity for reasoning and/or inference.", "metadata": {}}, {"text": "Borrowing some ideas from Intelligence Quotient (IQ) tests\n(i.e., Perceptual Reasoning and Working Memory (Watkins\net al.", "metadata": {}}, {"text": "1995)), we consolidate another range of evaluation,\nincluding symbolic mapping (Wei et al.", "metadata": {}}, {"text": "2023), rule under-\nstanding, pattern mining, and anti-interference evaluations.", "metadata": {}}, {"text": "We believe these tasks are less likely to be affected by data\nleakage or memorization, offering a more nuanced insight\ninto the model‚Äôs cognitive abilities beyond mere knowledge\nretrieval.", "metadata": {}}, {"text": "To summarize, the paper has made the following contri-\nbutions.", "metadata": {}}, {"text": "First, to the best of our knowledge, this is the first\nattempt to use a growth strategy to train an LLM with 100B+\nparameters from scratch.", "metadata": {}}, {"text": "The training costs only 100,000\nUS dollars.", "metadata": {}}, {"text": "Second, we demonstrate details for addressing\nthe instability issues via improving training objectives, hyper-\nparameter search, and function-preserving growth.", "metadata": {}}, {"text": "Third, we\nconduct extensive evaluations, including both the commonly\nused knowledge-oriented benchmarks and the new range of\nevaluations inspired by IQ tests.", "metadata": {}}, {"text": "Experimental results show\nthat, despite its low training cost, FLM-101B is competitive\nand robust.", "metadata": {}}, {"text": "Lastly, we will release the model checkpoints,\nas well as some related code and tools, to promote related\nresearch.", "metadata": {}}, {"text": "Related literature is reviewed in Appendix B.", "metadata": {}}, {"text": "2 Design Overview of FLM-101B\nIn this section, we provide an outline of FLM-101B, detail-\ning its architecture, pre-training methods, and configuration\nspecifics.", "metadata": {}}, {"text": "2.1 Architecture\nBackbone.", "metadata": {}}, {"text": "Among the many existing model architectures,\nwe adopt FreeLM (Li et al.", "metadata": {}}, {"text": "2023) as the backbone for\nour models, with modifications.", "metadata": {}}, {"text": "FreeLM is based on GPT\n(Radford et al.", "metadata": {}}, {"text": "2019), a transformer-like architecture with\na decoder-only configuration.", "metadata": {}}, {"text": "Different from GPT, FreeLM\nfeatures two pre-training objectives: the language objective\nand the teacher objective (Section 2.2).", "metadata": {}}, {"text": "We preserve the GPT-\n3-style transformer block designs without incorporating the\nlater modifications from Llama series.", "metadata": {}}, {"text": "We employ the tok-\nenizer derived from GPT-4, characterized by a vocabulary\nsize of 100, 256.", "metadata": {}}, {"text": "xPos Integration.", "metadata": {}}, {"text": "To enhance long sequence modeling, we\nintegrate the Extrapolatable Position Embedding (xPos) (Sun\net al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "This innovation draws inspiration from RoPE (Su\net al.", "metadata": {}}, {"text": "2021), which aims to improve the length extrapolation\nability by introducing an exponential decay into the rotation\nmatrix.", "metadata": {}}, {"text": "Model Sizes.", "metadata": {}}, {"text": "Benefiting from our growth strategy, the we\nproduce three models with 16B, 51B, and 101B (i.e., FLM-\n101B) parameters in a single training.", "metadata": {}}, {"text": "The training process is\ncarried out in a progressive manner by growing a 16B model\nto 51B, and then 101B.", "metadata": {}}, {"text": "2.2 Pre-Training Setup\nFLM-101B.", "metadata": {}}, {"text": "By design, FLM-101B is an English-Chinese\nbilingual model.", "metadata": {}}, {"text": "It mixes English and Chinese corpora at a\nratio of approximately 53.5% : 46 .5%.", "metadata": {}}, {"text": "Inspired by the find-\ning that instruction data can augment LLMs‚Äô comprehension\ncapabilities (Ouyang et al.", "metadata": {}}, {"text": "2022), we integrate multi-task\ninstructionally prompted data: OIG (Open Instruction Gen-\neralist) 1 and COIG (Chinese Open Instruction Generalist) 2,\nin the pre-training stage.", "metadata": {}}, {"text": "eFLM-16B.", "metadata": {}}, {"text": "To analyse the effect of domain-specific knowl-\nedge data (Section 4.2), we apply the FreeLM teacher signals\n(Li et al.", "metadata": {}}, {"text": "2023) to enhance factual capability.", "metadata": {}}, {"text": "Due to computa-\ntional cost, we incorporate these signals only in the smallest\n16B model.", "metadata": {}}, {"text": "This enhanced model is named eFLM-16B.", "metadata": {}}, {"text": "Specifically, we employ two emojis:\nüòà (U+1F621) and\nüò° \n(U+1F608) 3, from the vocabulary to replace the original\nbinary classification labels (Li et al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "For the teacher\nsignal, we supervise only on the emoji tokens, unifying the\nobjective with language modeling.", "metadata": {}}, {"text": "Moreover, we discard the\noriginal multi-task alternating approach and completely mix\nthe samples from both sides in every batch.", "metadata": {}}, {"text": "This strategy can\nenhance the consistency of data sampling distribution as well\nas improve training stability.", "metadata": {}}, {"text": "Table 1: Partial configurations for different growth stages.", "metadata": {}}, {"text": "Params Learning Warmup Batch Tokens Time Tokens\n(billion) Rate (samples) (million) (day) (billion)\n16 4e‚àí4 4,608,000 4.72 9.63 245.37\n51 3.4e‚àí4 230,400 4.72 5.37 39.64\n101 2e‚àí4 230,400 4.31 6.54 26.54\n2.3 Growth Strategy\nThe essence of the low cost in scaling up FLM-101B is the\ngrowth strategy.", "metadata": {}}, {"text": "Specifically, we train three models, with\n16B, 51B, and 101B parameters, respectively, in a sequential\nmanner.", "metadata": {}}, {"text": "Each model inherits knowledge from its predecessor.", "metadata": {}}, {"text": "This is contrary to the common practice that the models\nof different sizes are trained independently (Touvron et al.", "metadata": {}}, {"text": "2023a,b).", "metadata": {}}, {"text": "Function-preserving Growth.", "metadata": {}}, {"text": "Function preservation means\nthat before and after growth, the models yield consistent out-\nputs given the same arbitrary inputs.", "metadata": {}}, {"text": "This property has proven\n1https://huggingface.co/datasets/laion/OIG\n2https://huggingface.co/datasets/BAAI/COIG\n3https://apps.timwhitlock.info/emoji/tables/unicode", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.png] Size: 160x160, Data: 33553 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.png] Size: 160x160, Data: 33553 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.png", "image_width": 160, "image_height": 160, "attachment_type": "image", "has_image_data": true, "image_data_size": 33553}}, {"text": "[Image page=2 idx=2 name=Im1.png] Size: 160x160, Data: 21053 bytes", "sentences": [{"text": "[Image page=2 idx=2 name=Im1.png] Size: 160x160, Data: 21053 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 2, "image_name": "Im1.png", "image_width": 160, "image_height": 160, "attachment_type": "image", "has_image_data": true, "image_data_size": 21053}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical\nFLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.\nParams Tensor Pipeline Data Number Batch teraFLOP/s FLOPs\n(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization\n16 2 1 96 192 2304 162 51.90%\n51 4 2 24 192 2304 160 51.30%\n101 4 4 12 192 2160 165 52.88%\nbeneficial for both knowledge inheritance (Chen, Goodfel-\nlow, and Shlens 2016; Chen et al. 2022; Shen et al. 2022)\nand training stability (Yao et al. 2024). The growth oper-\nators used in FLM-101B training originate from Masked\nStructural Growth (MSG) (Yao et al. 2024), with adaptation.\nSpecifically, to adapt these operators to the multi-node 3D\nparallel framework, we implement them by extending the\nmodel structures offline and reloading the checkpoint when\nthe next stage starts.\nSchedules and Cost-Effectiveness. Model growth schedul-\ning is a trade-off between the pros and cons inherent to\nmodels of different sizes (Yao et al. 2024): a smaller model\nis faster in computation, enabling more rapid consumption\nof training data for broader commonsense knowledge; con-\nversely, a larger model is better in the reduction of loss per\nstep, indicating a deeper understanding of the nuanced lin-\nguistic patterns. Based on the speed test results and total\nbudget, we train the 16B model with 245.37B tokens, the\n51B model with 39.64B tokens, and the 101B model with\n26.54B tokens. The billion tokens per day of different sizes\nare listed in Table 1. Under this growth schedule, the total\ntime cost for training FLM-101B is 21.54 days, which is\n72% time-saving (or a 3.56x speedup) compared to training\na 101B model from scratch (76.74 days estimated). This is\nconsistent with our motivations depicted in Figure 1.\n2.4 The Parallelism Setup and Model\nConfigurations\nThe Parallel Strategies. FLM-101B is trained on a clus-\nter of 24 DGX-A800 GPU (8 √ó80G) servers. We employ a\n3D parallel strategy for optimal throughput, including the\nstandard data parallelism (Valiant 1990), tensor model par-\nallelism (Shoeybi et al. 2019), and pipeline model paral-\nlelism (Narayanan et al. 2021). Moreover, by employing\nsequence parallelism (Korthikanti et al. 2022), we slice the\ninputs to the Transformer core‚Äôs LayerNorm and Dropout\nlayers along the sequence length dimension, leading to addi-\ntional savings in GPU computational resources and memory\nutilization. We also utilize the Megetron-LM 4 implemen-\ntation of the distributed optimizer (Rajbhandari et al. 2019)\nto further reduce GPU memory consumption, which evenly\ndistributes the optimizer states across data parallel ranks.\nTable 2 shows the parallelism configurations and train-\ning throughput in each stage of FLM-101B training. In\ndifferent stages, we configure different Tensor Parallel √ó\n4https://github.com/NVIDIA/Megatron-LM\n16B stage\n51B stage\n101B stage\nProcessed Tokens (Billions)\nTraining Loss\nFigure 2: Training loss for FLM-101B models.\nPipeline Parallel sizes to achieve higher efficiency. The single-\nGPU throughput for all three training stages consistently ex-\nceeds 160 teraFLOPs/sec with a utilization rate of at least\n51.3%. For comparison, GLM-130B achieves 135 teraFLOP-\ns/sec (Zeng et al. 2023) with a 42.27% utilization rate. We\ncan also find that FLM-101B has a higher FLOP utilization\nrate than Megatron-LM (Korthikanti et al. 2022) under a\nsimilar model size.\nFLM-101B Configurations. The FLM-101B model is struc-\ntured with a hidden state dimension of 10, 240, a layer num-\nber of 80, a context window of 2,048 tokens, 80 attention\nheads, and a vocabulary size of 100, 256. FLM-101B uses\nthe AdamW optimizer (Loshchilov and Hutter 2017) with\nŒ≤1 = 0.9 and Œ≤2 = 0.95. A cosine learning rate schedule is\nemployed, leading to a final learning rate of 6e ‚àí 6. We use a\nweight decay of 0.1 and gradient clipping of 1.0.\nTable 1 presents part of the hyperparameters used in differ-\nent growth stages. In each growth stage, we approximately\ninherit the previous learning rate and adhere to the same\nschedule. The learning rate at the beginning of each stage is\nreported in the table. In the 16B stage, 4,608k samples are\nused for learning rate warmup, while in later growth stages,\nwe use fewer samples of 230.4k. Note that we do not apply\nbatch size warmup because we address the stability issue in\na different manner, detailed in Section 3.\n3 Training Stability of FLM-101B\nModels beyond 100B parameters (Scao et al. 2022; Zeng\net al. 2023) usually suffer from a bunch of notorious stability\nissues including loss divergence, gradient explosion, and nu-\nmerical overflow/underflow. This not only inflates the cost of\nsearching for feasible hyperparameters like optimal learning", "sentences": [{"text": "Table 2: Parallel strategies and throughput for different growth stages.", "metadata": {}}, {"text": "For NVIDIA A800 GPUs, the peak theoretical\nFLOPs per second is 312 teraFLOPs/sec.", "metadata": {}}, {"text": "Gradient accumulation is applied for the large global batch size.", "metadata": {}}, {"text": "Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs\n(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization\n16 2 1 96 192 2304 162 51.90%\n51 4 2 24 192 2304 160 51.30%\n101 4 4 12 192 2160 165 52.88%\nbeneficial for both knowledge inheritance (Chen, Goodfel-\nlow, and Shlens 2016;", "metadata": {}}, {"text": "Chen et al.", "metadata": {}}, {"text": "2022;", "metadata": {}}, {"text": "Shen et al.", "metadata": {}}, {"text": "2022)\nand training stability (Yao et al.", "metadata": {}}, {"text": "2024).", "metadata": {}}, {"text": "The growth oper-\nators used in FLM-101B training originate from Masked\nStructural Growth (MSG) (Yao et al.", "metadata": {}}, {"text": "2024), with adaptation.", "metadata": {}}, {"text": "Specifically, to adapt these operators to the multi-node 3D\nparallel framework, we implement them by extending the\nmodel structures offline and reloading the checkpoint when\nthe next stage starts.", "metadata": {}}, {"text": "Schedules and Cost-Effectiveness.", "metadata": {}}, {"text": "Model growth schedul-\ning is a trade-off between the pros and cons inherent to\nmodels of different sizes (Yao et al.", "metadata": {}}, {"text": "2024): a smaller model\nis faster in computation, enabling more rapid consumption\nof training data for broader commonsense knowledge;", "metadata": {}}, {"text": "con-\nversely, a larger model is better in the reduction of loss per\nstep, indicating a deeper understanding of the nuanced lin-\nguistic patterns.", "metadata": {}}, {"text": "Based on the speed test results and total\nbudget, we train the 16B model with 245.37B tokens, the\n51B model with 39.64B tokens, and the 101B model with\n26.54B tokens.", "metadata": {}}, {"text": "The billion tokens per day of different sizes\nare listed in Table 1.", "metadata": {}}, {"text": "Under this growth schedule, the total\ntime cost for training FLM-101B is 21.54 days, which is\n72% time-saving (or a 3.56x speedup) compared to training\na 101B model from scratch (76.74 days estimated).", "metadata": {}}, {"text": "This is\nconsistent with our motivations depicted in Figure 1.", "metadata": {}}, {"text": "2.4 The Parallelism Setup and Model\nConfigurations\nThe Parallel Strategies.", "metadata": {}}, {"text": "FLM-101B is trained on a clus-\nter of 24 DGX-A800 GPU (8 √ó80G) servers.", "metadata": {}}, {"text": "We employ a\n3D parallel strategy for optimal throughput, including the\nstandard data parallelism (Valiant 1990), tensor model par-\nallelism (Shoeybi et al.", "metadata": {}}, {"text": "2019), and pipeline model paral-\nlelism (Narayanan et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "Moreover, by employing\nsequence parallelism (Korthikanti et al.", "metadata": {}}, {"text": "2022), we slice the\ninputs to the Transformer core‚Äôs LayerNorm and Dropout\nlayers along the sequence length dimension, leading to addi-\ntional savings in GPU computational resources and memory\nutilization.", "metadata": {}}, {"text": "We also utilize the Megetron-LM 4 implemen-\ntation of the distributed optimizer (Rajbhandari et al.", "metadata": {}}, {"text": "2019)\nto further reduce GPU memory consumption, which evenly\ndistributes the optimizer states across data parallel ranks.", "metadata": {}}, {"text": "Table 2 shows the parallelism configurations and train-\ning throughput in each stage of FLM-101B training.", "metadata": {}}, {"text": "In\ndifferent stages, we configure different Tensor Parallel √ó\n4https://github.com/NVIDIA/Megatron-LM\n16B stage\n51B stage\n101B stage\nProcessed Tokens (Billions)\nTraining Loss\nFigure 2: Training loss for FLM-101B models.", "metadata": {}}, {"text": "Pipeline Parallel sizes to achieve higher efficiency.", "metadata": {}}, {"text": "The single-\nGPU throughput for all three training stages consistently ex-\nceeds 160 teraFLOPs/sec with a utilization rate of at least\n51.3%.", "metadata": {}}, {"text": "For comparison, GLM-130B achieves 135 teraFLOP-\ns/sec (Zeng et al.", "metadata": {}}, {"text": "2023) with a 42.27% utilization rate.", "metadata": {}}, {"text": "We\ncan also find that FLM-101B has a higher FLOP utilization\nrate than Megatron-LM (Korthikanti et al.", "metadata": {}}, {"text": "2022) under a\nsimilar model size.", "metadata": {}}, {"text": "FLM-101B Configurations.", "metadata": {}}, {"text": "The FLM-101B model is struc-\ntured with a hidden state dimension of 10, 240, a layer num-\nber of 80, a context window of 2,048 tokens, 80 attention\nheads, and a vocabulary size of 100, 256.", "metadata": {}}, {"text": "FLM-101B uses\nthe AdamW optimizer (Loshchilov and Hutter 2017) with\nŒ≤1 = 0.9 and Œ≤2 = 0.95.", "metadata": {}}, {"text": "A cosine learning rate schedule is\nemployed, leading to a final learning rate of 6e ‚àí 6.", "metadata": {}}, {"text": "We use a\nweight decay of 0.1 and gradient clipping of 1.0.", "metadata": {}}, {"text": "Table 1 presents part of the hyperparameters used in differ-\nent growth stages.", "metadata": {}}, {"text": "In each growth stage, we approximately\ninherit the previous learning rate and adhere to the same\nschedule.", "metadata": {}}, {"text": "The learning rate at the beginning of each stage is\nreported in the table.", "metadata": {}}, {"text": "In the 16B stage, 4,608k samples are\nused for learning rate warmup, while in later growth stages,\nwe use fewer samples of 230.4k.", "metadata": {}}, {"text": "Note that we do not apply\nbatch size warmup because we address the stability issue in\na different manner, detailed in Section 3.", "metadata": {}}, {"text": "3 Training Stability of FLM-101B\nModels beyond 100B parameters (Scao et al.", "metadata": {}}, {"text": "2022;", "metadata": {}}, {"text": "Zeng\net al.", "metadata": {}}, {"text": "2023) usually suffer from a bunch of notorious stability\nissues including loss divergence, gradient explosion, and nu-\nmerical overflow/underflow.", "metadata": {}}, {"text": "This not only inflates the cost of\nsearching for feasible hyperparameters like optimal learning", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=I1.png] Size: 1500x600, Data: 50847 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=I1.png] Size: 1500x600, Data: 50847 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "I1.png", "image_width": 1500, "image_height": 600, "attachment_type": "image", "has_image_data": true, "image_data_size": 50847}}, {"text": "[Image page=3 idx=2 name=I2.png] Size: 4x1189, Data: 3607 bytes", "sentences": [{"text": "[Image page=3 idx=2 name=I2.png] Size: 4x1189, Data: 3607 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 2, "image_name": "I2.png", "image_width": 4, "image_height": 1189, "attachment_type": "image", "has_image_data": true, "image_data_size": 3607}}, {"text": "[Image page=3 idx=3 name=I3.png] Size: 4x1189, Data: 3607 bytes", "sentences": [{"text": "[Image page=3 idx=3 name=I3.png] Size: 4x1189, Data: 3607 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 3, "image_name": "I3.png", "image_width": 4, "image_height": 1189, "attachment_type": "image", "has_image_data": true, "image_data_size": 3607}}, {"text": "[Image page=3 idx=4 name=I4.png] Size: 4x1189, Data: 3077 bytes", "sentences": [{"text": "[Image page=3 idx=4 name=I4.png] Size: 4x1189, Data: 3077 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 4, "image_name": "I4.png", "image_width": 4, "image_height": 1189, "attachment_type": "image", "has_image_data": true, "image_data_size": 3077}}, {"text": "[Image page=3 idx=5 name=I5.png] Size: 4x1189, Data: 3694 bytes", "sentences": [{"text": "[Image page=3 idx=5 name=I5.png] Size: 4x1189, Data: 3694 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 5, "image_name": "I5.png", "image_width": 4, "image_height": 1189, "attachment_type": "image", "has_image_data": true, "image_data_size": 3694}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the\ncorresponding references. The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al. 2021).\nModel GPT-3\n(Brown et al. 2020)\nGopher\n(Rae et al. 2021)\nPaLM\n(Anil et al. 2023)\nGLM-130B\n(Zeng et al. 2023)\nLlama-2\n(Touvron et al. 2023b)FLM-101B\nParams 175B 280B 540B 130B 70B 101B\nGPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5\nChip Power/TDP 330 283 378.5 400 400 400\nEnergy (MkWh) 1171 1066 3179 444 688 40\nnet tCO2e 552 380 271 257 291 26\nrates, but also intensifies ongoing maintenance during train-\ning, such as babysitting, issue resolution, data adjustment,\nand rebooting. Moreover, this makes the budget of the whole\nproject unpredictable. In this section, we introduce details for\nmitigating these issues.\nPredictable Scaling. The Tensor Programs theories (Yang\nand Hu 2021; Littwin and Yang 2023) unveil the universal\nrelations of the training dynamics with the model width tend-\ning to infinite. This results in a parameterized mapping for\ncertain classes of hyperparameters between a small model\nand its larger counterparts, which is termed ¬µP (Yang et al.\n2021). Two important insights are: (i) The wider, the better:\ntheoretically, under ¬µP transfer, a wider model will always\nyield lower loss than its narrower counterparts when exposed\nto identical data (Yang et al. 2021). As a direct corollary, if a\nnarrow model converges, its wider counterparts will always\nconverge. (ii) Loss prediction: the loss value of a large model\nis predictable using the loss of its smaller counterparts (Ope-\nnAI 2023). ¬µScaling (Yao and Wang 2023) open-sources a\nmethodology through which loss prediction can be achieved\nby combining ¬µP (Yang et al. 2021) and (a modified) scaling\nlaw (Kaplan et al. 2020; Henighan et al. 2020; Hoffmann\net al. 2022).\nBased on these findings, our method to solve training sta-\nbility is as follows: we first determine the data distribution\nbefore the FLM-16B training starts. Next, we perform a grid\nsearch on three hyperparameters including the learning rate,\ninitialization standard deviation, and the softmax temperature\nin the output layer. This grid search is performed with a small\nproxy model (less than 100M) with a hidden state dimension\n(‚Äúmodel width‚Äù) of 256 and a head number of 2. All the other\nstructural configurations and training data are identical to\nthose of FLM-16B. A single run of grid search takes 24.6\nhours with data parallelism on 6 nodes, which is equivalent to\n6 hours per run given our 24-node infrastructure. Finally, we\nfind a group of well-performing hyperparameters: learning\nrate = 4e ‚àí 4, standard deviation = 1.6e ‚àí 2, and softmax\ntemperature = 2.0. Transferring these hyperparameters to the\n16B model via ¬µP (Yang et al. 2021) led to a seamless train-\ning experience devoid of instabilities. Combined with MSG\n(Yao et al. 2024), we also witness no post-growth divergence\nin FLM-51B and FLM-101B.\nOur implementations of ¬µP are largely consistent with\nthose in ¬µScaling (Yao and Wang 2023), with modifications\nto handle the rotary embedding. Thus, the value range of\nFLM-16B loss is also predictable with the results from multi-\nple proxy widths at the same steps. Mixed-precision training\nis applied to save run-time memory and reduce time costs.\nSpecifically, we chooseBfloat16 instead of FP16 due to its su-\nperior precision for values approaching zero, making it more\nsuitable for ¬µP. As a result, we do not encounter the FP16\nunderflow issue reported by (Yang et al. 2021). Moreover,\nBfloat16 negates the need for loss scale adjustments, making\nour training procedure more promising and reproducible.\nThe full training loss curve is presented in Figure 2. We\nobserve that the loss curve becomes steeper after each growth.\nIt matches the intuition that a larger model is better in loss\nreduction per step. The whole training procedure is robust\nand predictable: even though the 51B stage is short with only\n40B tokens, the 101B training remains stable. This supports\nthe effectiveness of the growth strategy.\n4 Benchmark Evaluation\nMany existing benchmarks ( e.g., Open LLM5) focus on as-\nsessing the knowledgeability of LLMs. In this section, we\ndiscuss the results of FLM on these benchmarks. We be-\nlieve that knowledge alone might not comprehensively reflect\nLLM‚Äôs capability (see Section 4.2 for more details). Thus, in\naddition to the common benchmark evaluation, we borrow\nthe concept of IQ tests and evaluate LLMs with some specific\ncognitive tasks in Section 5.\nCost Estimation Method. Due to the considerable computa-\ntional expense of LLMs, we also emphasize their associated\ncosts in our experimental results. However, it is difficult\nto directly compare the actual cost of LLMs due to their\ndifferent infrastructures and prices. To objectively compare\ntraining costs, we use the FLOPs for training as the cost\nestimation index, which is estimated from the model‚Äôs hy-\nperparameters, configuration, and training data (Narayanan\net al. 2021). Since most models do not release the complete\ntraining configuration, we estimate FLOPs within a range6.\nFor monolingual English LLMs, the computational cost of\nGPT-3 is calculated as 376.41 ( ¬±53.77) zettaFLOPs, and\nLlama-2 (13B) as 210.37 ( ¬±28.77) zettaFLOPs. For bilin-\ngual or multilingual models, it is necessary to estimate the\ncost for each language. The total cost of GLM-130B is com-\nputed as 421.60 zettaFLOPs. As the data ratio of English\n5https://huggingface.co/spaces/HuggingFaceH4/\nopen llm leaderboard.\n6This range originates from the use of checkpoint activation.\nPlease check (Narayanan et al. 2021) for more details.", "sentences": [{"text": "Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.", "metadata": {}}, {"text": "For details, please see the\ncorresponding references.", "metadata": {}}, {"text": "The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "Model GPT-3\n(Brown et al.", "metadata": {}}, {"text": "2020)\nGopher\n(Rae et al.", "metadata": {}}, {"text": "2021)\nPaLM\n(Anil et al.", "metadata": {}}, {"text": "2023)\nGLM-130B\n(Zeng et al.", "metadata": {}}, {"text": "2023)\nLlama-2\n(Touvron et al.", "metadata": {}}, {"text": "2023b)FLM-101B\nParams 175B 280B 540B 130B 70B 101B\nGPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5\nChip Power/TDP 330 283 378.5 400 400 400\nEnergy (MkWh) 1171 1066 3179 444 688 40\nnet tCO2e 552 380 271 257 291 26\nrates, but also intensifies ongoing maintenance during train-\ning, such as babysitting, issue resolution, data adjustment,\nand rebooting.", "metadata": {}}, {"text": "Moreover, this makes the budget of the whole\nproject unpredictable.", "metadata": {}}, {"text": "In this section, we introduce details for\nmitigating these issues.", "metadata": {}}, {"text": "Predictable Scaling.", "metadata": {}}, {"text": "The Tensor Programs theories (Yang\nand Hu 2021;", "metadata": {}}, {"text": "Littwin and Yang 2023) unveil the universal\nrelations of the training dynamics with the model width tend-\ning to infinite.", "metadata": {}}, {"text": "This results in a parameterized mapping for\ncertain classes of hyperparameters between a small model\nand its larger counterparts, which is termed ¬µP (Yang et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "Two important insights are: (i) The wider, the better:\ntheoretically, under ¬µP transfer, a wider model will always\nyield lower loss than its narrower counterparts when exposed\nto identical data (Yang et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "As a direct corollary, if a\nnarrow model converges, its wider counterparts will always\nconverge.", "metadata": {}}, {"text": "(ii) Loss prediction: the loss value of a large model\nis predictable using the loss of its smaller counterparts (Ope-\nnAI 2023).", "metadata": {}}, {"text": "¬µScaling (Yao and Wang 2023) open-sources a\nmethodology through which loss prediction can be achieved\nby combining ¬µP (Yang et al.", "metadata": {}}, {"text": "2021) and (a modified) scaling\nlaw (Kaplan et al.", "metadata": {}}, {"text": "2020;", "metadata": {}}, {"text": "Henighan et al.", "metadata": {}}, {"text": "2020;", "metadata": {}}, {"text": "Hoffmann\net al.", "metadata": {}}, {"text": "2022).", "metadata": {}}, {"text": "Based on these findings, our method to solve training sta-\nbility is as follows: we first determine the data distribution\nbefore the FLM-16B training starts.", "metadata": {}}, {"text": "Next, we perform a grid\nsearch on three hyperparameters including the learning rate,\ninitialization standard deviation, and the softmax temperature\nin the output layer.", "metadata": {}}, {"text": "This grid search is performed with a small\nproxy model (less than 100M) with a hidden state dimension\n(‚Äúmodel width‚Äù) of 256 and a head number of 2.", "metadata": {}}, {"text": "All the other\nstructural configurations and training data are identical to\nthose of FLM-16B.", "metadata": {}}, {"text": "A single run of grid search takes 24.6\nhours with data parallelism on 6 nodes, which is equivalent to\n6 hours per run given our 24-node infrastructure.", "metadata": {}}, {"text": "Finally, we\nfind a group of well-performing hyperparameters: learning\nrate = 4e ‚àí 4, standard deviation = 1.6e ‚àí 2, and softmax\ntemperature = 2.0.", "metadata": {}}, {"text": "Transferring these hyperparameters to the\n16B model via ¬µP (Yang et al.", "metadata": {}}, {"text": "2021) led to a seamless train-\ning experience devoid of instabilities.", "metadata": {}}, {"text": "Combined with MSG\n(Yao et al.", "metadata": {}}, {"text": "2024), we also witness no post-growth divergence\nin FLM-51B and FLM-101B.", "metadata": {}}, {"text": "Our implementations of ¬µP are largely consistent with\nthose in ¬µScaling (Yao and Wang 2023), with modifications\nto handle the rotary embedding.", "metadata": {}}, {"text": "Thus, the value range of\nFLM-16B loss is also predictable with the results from multi-\nple proxy widths at the same steps.", "metadata": {}}, {"text": "Mixed-precision training\nis applied to save run-time memory and reduce time costs.", "metadata": {}}, {"text": "Specifically, we chooseBfloat16 instead of FP16 due to its su-\nperior precision for values approaching zero, making it more\nsuitable for ¬µP.", "metadata": {}}, {"text": "As a result, we do not encounter the FP16\nunderflow issue reported by (Yang et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "Moreover,\nBfloat16 negates the need for loss scale adjustments, making\nour training procedure more promising and reproducible.", "metadata": {}}, {"text": "The full training loss curve is presented in Figure 2.", "metadata": {}}, {"text": "We\nobserve that the loss curve becomes steeper after each growth.", "metadata": {}}, {"text": "It matches the intuition that a larger model is better in loss\nreduction per step.", "metadata": {}}, {"text": "The whole training procedure is robust\nand predictable: even though the 51B stage is short with only\n40B tokens, the 101B training remains stable.", "metadata": {}}, {"text": "This supports\nthe effectiveness of the growth strategy.", "metadata": {}}, {"text": "4 Benchmark Evaluation\nMany existing benchmarks ( e.g., Open LLM5) focus on as-\nsessing the knowledgeability of LLMs.", "metadata": {}}, {"text": "In this section, we\ndiscuss the results of FLM on these benchmarks.", "metadata": {}}, {"text": "We be-\nlieve that knowledge alone might not comprehensively reflect\nLLM‚Äôs capability (see Section 4.2 for more details).", "metadata": {}}, {"text": "Thus, in\naddition to the common benchmark evaluation, we borrow\nthe concept of IQ tests and evaluate LLMs with some specific\ncognitive tasks in Section 5.", "metadata": {}}, {"text": "Cost Estimation Method.", "metadata": {}}, {"text": "Due to the considerable computa-\ntional expense of LLMs, we also emphasize their associated\ncosts in our experimental results.", "metadata": {}}, {"text": "However, it is difficult\nto directly compare the actual cost of LLMs due to their\ndifferent infrastructures and prices.", "metadata": {}}, {"text": "To objectively compare\ntraining costs, we use the FLOPs for training as the cost\nestimation index, which is estimated from the model‚Äôs hy-\nperparameters, configuration, and training data (Narayanan\net al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "Since most models do not release the complete\ntraining configuration, we estimate FLOPs within a range6.", "metadata": {}}, {"text": "For monolingual English LLMs, the computational cost of\nGPT-3 is calculated as 376.41 ( ¬±53.77) zettaFLOPs, and\nLlama-2 (13B) as 210.37 ( ¬±28.77) zettaFLOPs.", "metadata": {}}, {"text": "For bilin-\ngual or multilingual models, it is necessary to estimate the\ncost for each language.", "metadata": {}}, {"text": "The total cost of GLM-130B is com-\nputed as 421.60 zettaFLOPs.", "metadata": {}}, {"text": "As the data ratio of English\n5https://huggingface.co/spaces/HuggingFaceH4/\nopen llm leaderboard.", "metadata": {}}, {"text": "6This range originates from the use of checkpoint activation.", "metadata": {}}, {"text": "Please check (Narayanan et al.", "metadata": {}}, {"text": "2021) for more details.", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.We list the estimated floating-\npoint operations (zetta = 10 21) of the training process for reference.\nModel Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA\nLlama-2 (13B) 201.37 ( ¬±28.77) 58.66 59.39 82.13 55.77 37.38\nLlama-2 (7B) 106.60 ( ¬±15.23) 54.32 53.07 78.59 46.87 38.76\nLlama (13B) 94.81 ( ¬±13.54) 56.08 56.23 80.93 47.67 39.48\nLlama (7B) 49.54 ( ¬±7.08) 49.72 51.02 77.82 35.71 34.33\nGLM-130B 210.80 48.11 42.15 67.91 42.59 39.80\nFLM-101B 28.22 43.94 39.76 66.23 28.30 ‚àó 41.47\n‚àó44.50 for a knowledge-enhanced eFLM-16B (Section 2.2, 4.2).\nTable 5: Performance of eFLM-16B and baselines on C-eval.In this table, eFLM-16B refers to the professional-knowledge-\nenhanced FLM-16B. Note that C-Eval leaderboard only keeps one decimal place for the evaluation results.\nModel Average Average (Hard) STEM Social Science Humanities Others\nGPT-4 68.7 54.9 67.1 77.6 64.5 67.8\nChatGPT 54.4 41.4 52.9 61.8 50.9 53.6\nGLM-130B 44.0 30.7 36.7 55.8 47.7 43.0\neFLM-16B 46.1 28.9 38.3 53.7 46.8 52.6\nand Chinese is reported to be 1:1, the cost of GLM-130B\nfor English is 210.80 zettaFLOPs, and the same for Chinese.\nThe data ratio of FLM-101B is 53.5% : 46 .5% for English\nand Chinese. The total cost of FLM-101B is computed as\n52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54\nfor Chinese).\nCarbon Footprint Analysis. An important measurement of\na model‚Äôs environmental impact (Schwartz et al. 2020) is the\ncarbon footprints originated from the pre-training process.\nWe estimate carbon emission with the methods provided in\n(Patterson et al. 2021). We summarize the carbon footprint\nstatistics of FLM-101B and well-known LLMs in Table 3.\nOur model yields only 1/10 pre-training carbon footprint of a\ntypical LLM.\n4.1 Open LLM Evaluation\nOpen LLM Leaderboard is an open-source project to eval-\nuate the open-sourced LLMs and chatbots. By the time\nFLM-101B is trained, Open LLM contains four tasks: ARC-\nChallenge (ARC for short) (Clark et al. 2018), HellaSwag\n(Zellers et al. 2019), MMLU (Hendrycks et al. 2021), and\nTruthfulQA (Lin, Hilton, and Evans 2022). The Open LLM\nLeaderboard applies the average score as a metric. All the\nfour tasks require intense knowledge to solve: ARC, Hel-\nlaSwag, and TruthfulQA depend on commonsense knowl-\nedge and Wikipedia, while MMLU contains some questions\n(i.e., STEM) that require domain-specific professional knowl-\nedge and intricate reasoning.\nTable 4 details the performance of FLM-101B and strong\nbaselines, including Llama series and GLM-130B 7. GLM-\n130B results are achieved by our run on an open-sourced\ncheckpoint.\n7We exclude GPT-3 because it is closed-source. Probability val-\nues are unavailable for fair comparison.\nResults. On average, FLM-101B achieves a score of 43.94,\nreaching over 90% of the performance of GLM-130B, which\nhas 7 times more FLOPs. Both model underperform the\nLlama series, potentially due to the first-generation model\narchitectures and less well-refined training data. Note that the\nFLOPs of FLM-101B is even lower than a 7B Llama model.\nGoing deeper into the nature of these tasks, we further have\nthe following observations:\n(i) MMLU typically requires domain knowledge to solve.\nIn our training, no English textbook or exam data is intention-\nally used. Nevertheless, our eFLM-16B (Section 2.2) variant,\nincorporated with these knowledge via FreeLM objectives,\noutperforms GLM-130B with only 16B parameters.\n(ii) As aforementioned, TruthfulQA, ARC, and HellaSwag\nemphasize more on common sense and Wiki-level knowl-\nedge; their performances improve with the increased amount\nof data and the reduction of training loss. With less than 0.16T\nEnglish data (about 1/10 of Llama-2), FLM-101B already\nachieves the best accuracy of 41.47 among all the baselines\non TruthfulQA. On ARC and HellaSwag, FLM-101B is com-\nparable to GLM-130B with a similar amount of English data\n(approximately 0.2T). Also, the training data of GLM-130B\nincludes ARC and HellaSwag, as expressly claimed in (Zeng\net al. 2023). In our understanding, for FLM-101B, improve-\nment can be expected on these three tasks if exposed to more\ntraining data.\n4.2 Evaluation on the Professional\nKnowledge-Enhanced Model\nWe conduct experiments on a knowledge-enhanced ver-\nsion (eFLM-16B, detailed in Section 2.2) of the FLM to vali-\ndate the effect of domain-specific knowledge data on bench-\nmark results. We continue to train the smallest FLM-16B\nwith teacher signals (Li et al. 2023) from a combination of (i)\npart of the auxiliary training data of MMLU (Hendrycks et al.", "sentences": [{"text": "Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.We list the estimated floating-\npoint operations (zetta = 10 21) of the training process for reference.", "metadata": {}}, {"text": "Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA\nLlama-2 (13B) 201.37 ( ¬±28.77) 58.66 59.39 82.13 55.77 37.38\nLlama-2 (7B) 106.60 ( ¬±15.23) 54.32 53.07 78.59 46.87 38.76\nLlama (13B) 94.81 ( ¬±13.54) 56.08 56.23 80.93 47.67 39.48\nLlama (7B) 49.54 ( ¬±7.08) 49.72 51.02 77.82 35.71 34.33\nGLM-130B 210.80 48.11 42.15 67.91 42.59 39.80\nFLM-101B 28.22 43.94 39.76 66.23 28.30 ‚àó 41.47\n‚àó44.50 for a knowledge-enhanced eFLM-16B (Section 2.2, 4.2).", "metadata": {}}, {"text": "Table 5: Performance of eFLM-16B and baselines on C-eval.In this table, eFLM-16B refers to the professional-knowledge-\nenhanced FLM-16B.", "metadata": {}}, {"text": "Note that C-Eval leaderboard only keeps one decimal place for the evaluation results.", "metadata": {}}, {"text": "Model Average Average (Hard) STEM Social Science Humanities Others\nGPT-4 68.7 54.9 67.1 77.6 64.5 67.8\nChatGPT 54.4 41.4 52.9 61.8 50.9 53.6\nGLM-130B 44.0 30.7 36.7 55.8 47.7 43.0\neFLM-16B 46.1 28.9 38.3 53.7 46.8 52.6\nand Chinese is reported to be 1:1, the cost of GLM-130B\nfor English is 210.80 zettaFLOPs, and the same for Chinese.", "metadata": {}}, {"text": "The data ratio of FLM-101B is 53.5% : 46 .5% for English\nand Chinese.", "metadata": {}}, {"text": "The total cost of FLM-101B is computed as\n52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54\nfor Chinese).", "metadata": {}}, {"text": "Carbon Footprint Analysis.", "metadata": {}}, {"text": "An important measurement of\na model‚Äôs environmental impact (Schwartz et al.", "metadata": {}}, {"text": "2020) is the\ncarbon footprints originated from the pre-training process.", "metadata": {}}, {"text": "We estimate carbon emission with the methods provided in\n(Patterson et al.", "metadata": {}}, {"text": "2021).", "metadata": {}}, {"text": "We summarize the carbon footprint\nstatistics of FLM-101B and well-known LLMs in Table 3.", "metadata": {}}, {"text": "Our model yields only 1/10 pre-training carbon footprint of a\ntypical LLM.", "metadata": {}}, {"text": "4.1 Open LLM Evaluation\nOpen LLM Leaderboard is an open-source project to eval-\nuate the open-sourced LLMs and chatbots.", "metadata": {}}, {"text": "By the time\nFLM-101B is trained, Open LLM contains four tasks: ARC-\nChallenge (ARC for short) (Clark et al.", "metadata": {}}, {"text": "2018), HellaSwag\n(Zellers et al.", "metadata": {}}, {"text": "2019), MMLU (Hendrycks et al.", "metadata": {}}, {"text": "2021), and\nTruthfulQA (Lin, Hilton, and Evans 2022).", "metadata": {}}, {"text": "The Open LLM\nLeaderboard applies the average score as a metric.", "metadata": {}}, {"text": "All the\nfour tasks require intense knowledge to solve: ARC, Hel-\nlaSwag, and TruthfulQA depend on commonsense knowl-\nedge and Wikipedia, while MMLU contains some questions\n(i.e., STEM) that require domain-specific professional knowl-\nedge and intricate reasoning.", "metadata": {}}, {"text": "Table 4 details the performance of FLM-101B and strong\nbaselines, including Llama series and GLM-130B 7.", "metadata": {}}, {"text": "GLM-\n130B results are achieved by our run on an open-sourced\ncheckpoint.", "metadata": {}}, {"text": "7We exclude GPT-3 because it is closed-source.", "metadata": {}}, {"text": "Probability val-\nues are unavailable for fair comparison.", "metadata": {}}, {"text": "Results.", "metadata": {}}, {"text": "On average, FLM-101B achieves a score of 43.94,\nreaching over 90% of the performance of GLM-130B, which\nhas 7 times more FLOPs.", "metadata": {}}, {"text": "Both model underperform the\nLlama series, potentially due to the first-generation model\narchitectures and less well-refined training data.", "metadata": {}}, {"text": "Note that the\nFLOPs of FLM-101B is even lower than a 7B Llama model.", "metadata": {}}, {"text": "Going deeper into the nature of these tasks, we further have\nthe following observations:\n(i) MMLU typically requires domain knowledge to solve.", "metadata": {}}, {"text": "In our training, no English textbook or exam data is intention-\nally used.", "metadata": {}}, {"text": "Nevertheless, our eFLM-16B (Section 2.2) variant,\nincorporated with these knowledge via FreeLM objectives,\noutperforms GLM-130B with only 16B parameters.", "metadata": {}}, {"text": "(ii) As aforementioned, TruthfulQA, ARC, and HellaSwag\nemphasize more on common sense and Wiki-level knowl-\nedge;", "metadata": {}}, {"text": "their performances improve with the increased amount\nof data and the reduction of training loss.", "metadata": {}}, {"text": "With less than 0.16T\nEnglish data (about 1/10 of Llama-2), FLM-101B already\nachieves the best accuracy of 41.47 among all the baselines\non TruthfulQA.", "metadata": {}}, {"text": "On ARC and HellaSwag, FLM-101B is com-\nparable to GLM-130B with a similar amount of English data\n(approximately 0.2T).", "metadata": {}}, {"text": "Also, the training data of GLM-130B\nincludes ARC and HellaSwag, as expressly claimed in (Zeng\net al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "In our understanding, for FLM-101B, improve-\nment can be expected on these three tasks if exposed to more\ntraining data.", "metadata": {}}, {"text": "4.2 Evaluation on the Professional\nKnowledge-Enhanced Model\nWe conduct experiments on a knowledge-enhanced ver-\nsion (eFLM-16B, detailed in Section 2.2) of the FLM to vali-\ndate the effect of domain-specific knowledge data on bench-\nmark results.", "metadata": {}}, {"text": "We continue to train the smallest FLM-16B\nwith teacher signals (Li et al.", "metadata": {}}, {"text": "2023) from a combination of (i)\npart of the auxiliary training data of MMLU (Hendrycks et al.", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Table 6: Performance of the three stages of FLM on Open LLM.To reduce the computational cost during evaluation, we\nsample 20% and 30% items for HellaSwag and MMLU tasks, respectively.\nParameters Training Data Average ARC Hellaswag MMLU TruthfulQA\n16B 245.37B 39.19 32.25 58.57 27.02 38.92\n51B 39.64B 41.79 35.32 64.04 27.66 40.12\n101B 26.54B 44.41 39.76 67.88 28.54 41.47\n2021), (ii) exam questions in similar domains and formats to\nC-Eval (Huang et al. 2023) 8, and (iii) other domain knowl-\nedge data. Note that eFLM-16B is not a typical fine-tuning\nwith instruct data which may affect the language modeling\ncapability of LLM. We preserve both language and teacher\nsignals with the corresponding data in this continue-training.\nThe MMLU result is in the footnote of Table 4. Table 5 lists\nthe result of eFLM-16B and baselines on C-Eval.\nResults. Enhanced with professional knowledge, significant\nimprovements are observed. On MMLU tasks, the incorpo-\nration of professional knowledge data results in a score of\n44.50 for eFLM-16B (see Table 4), which surpasses GLM-\n130B (42.59), a model that also incorporated multi-task data\nin the related domain (Zeng et al. 2023). For comparison,\nthe MMLU score is 27.02 for the un-enhanced FLM-16B.\nOn C-Eval tasks 9, we observe that eFLM-16B performs\nbetter than GLM-130B by about 2 points. For comparison,\nthe average C-Eval score of the vanilla FLM-16B is 27.0,\nwhich underperforms GLM-130B. These results suggest that\nevaluation with professional knowledge may not fully reflect\nthe capability of LLMs, particularly when different LLMs\nare trained with different data collections, and some may not\ncome with a clear list.\n4.3 Evaluation of the Growth Strategy\nOur core method for reducing computational cost is the\ngrowth strategy. We would like to answer the questions of\nwhether our growth strategy is effective in knowledge in-\nheritance, and how model capabilities grow with size. We\nevaluate the performance of FLM on all the stages: 16B, 51B,\nand 101B. Table 6 shows the performance of FLM models at\neach stage.\nResults. As expected, the performance of FLM improves\nwith the increase in model size. FLM-101B achieves the\nbest performance on almost all tasks. This means that our\nmodel inherits knowledge from the previous stage after each\ngrowth. We also observe that the 101B model improves the\nperformance scores more significantly than the 51B model,\nwith less data. This indicates that the models are successfully\nincorporating new weights in training after growth, and tak-\ning advantage of larger parameter counts. The performance\non ARC and HellaSwag increases steadily and significantly,\nwhich corresponds well to the decline of the model loss.\nAgain, as we expected in Section 4.1, as more training data\nis processed, FLM‚Äôs performance on Open LLM improves.\n8C-Eval can be considered as a Chinese version of MMLU.\n9The scores are achieved on the test set by submitting to the\nC-Eval platform.\n5 Evaluation Inspired by IQ Tests\nSection 4 presents the evaluation of existing benchmarks,\nfocusing on knowledge. As we discussed in Section 1 and\n4.2, knowledge could not fully reflect the Intelligence Quo-\ntient (IQ) of LLMs. As a supplement, we conduct a series\nof IQ-test task evaluation in this section. For IQ evaluation,\nwe make necessary modifications to existing datasets (Wei\net al. 2023; Weston et al. 2015; Srivastava et al. 2023) or gen-\nerate new synthetic datasets. Specifically, the IQ test mainly\nconsiders four aspects: symbolic mapping, rule understand-\ning, pattern mining, and anti-interference. A common key\nproperty of these tasks is that they are dependent on the in-\nference and generalization in a new context, instead of the\npreviously-learned knowledge.\nCompared Methods. Borrowing psychological ideas that\nthe measurement of IQ is dependent on age 10, we mainly\nconsider existing models trained with similar amounts of data\nto FLM-101B. As a milestone of LLM development, GPT-\n3 (175B) (Brown et al. 2020) proposed in-context learning\nfor the first time. GLM-130B (Zeng et al. 2023) is the first\nopen English-Chinese bilingual LLM. Hence, we select them\nas baseline models. Both models are trained with 300 Àú400\nbillion tokens, which are in the same range as ours. GPT-\n3 focuses on English, and is not included in the Chinese-\nrelated evaluation ( i.e., CLUE-IQ). The results of GPT-3\nare achieved by API. GLM-130B is evaluated with its open-\nsourced checkpoint.\nTasks, Data, and Results. We curate evaluation benchmarks\nregarding four cognitive capabilities. We summarize the data\nand results in this section. For details in task definition and\ndata collection process, please see Appendix A.1 to A.4.\nSymbolic Mapping. An existing study (Wei et al. 2023)\npoints out that textual classification tasks ( e.g., sentiment\nclassification) often lack generalization. Considering this, we\nuse a symbolic mapping method to replace the original cate-\ngorical labels with symbols that are unlikely to be seen in any\ntraining data. Hence, we can evaluate the LLMs‚Äô language\nunderstanding ability as well as the generalization abilities\nto a new context. We form our evaluation task as in-context\nlearning with few-shot examples for each label. We con-\nstruct two symbolic mapping datasets, namely SuperGLUE-\nIQ and CLUE-IQ, built on SuperGLUE (Wang et al. 2019)\nand CLUE (Xu et al. 2020), respectively. Examples are illus-\ntrated in the Appendix (Figure 1).\nResults on SuperGLUE-IQ and CLUE-IQ are presented in\nTable 7 and Table 8, respectively. With less computation\n10https://ocw.mit.edu/ans7870/9/9.00SC/MIT9 00SCF11 text.\npdf, page 367.", "sentences": [{"text": "Table 6: Performance of the three stages of FLM on Open LLM.To reduce the computational cost during evaluation, we\nsample 20% and 30% items for HellaSwag and MMLU tasks, respectively.", "metadata": {}}, {"text": "Parameters Training Data Average ARC Hellaswag MMLU TruthfulQA\n16B 245.37B 39.19 32.25 58.57 27.02 38.92\n51B 39.64B 41.79 35.32 64.04 27.66 40.12\n101B 26.54B 44.41 39.76 67.88 28.54 41.47\n2021), (ii) exam questions in similar domains and formats to\nC-Eval (Huang et al.", "metadata": {}}, {"text": "2023) 8, and (iii) other domain knowl-\nedge data.", "metadata": {}}, {"text": "Note that eFLM-16B is not a typical fine-tuning\nwith instruct data which may affect the language modeling\ncapability of LLM.", "metadata": {}}, {"text": "We preserve both language and teacher\nsignals with the corresponding data in this continue-training.", "metadata": {}}, {"text": "The MMLU result is in the footnote of Table 4.", "metadata": {}}, {"text": "Table 5 lists\nthe result of eFLM-16B and baselines on C-Eval.", "metadata": {}}, {"text": "Results.", "metadata": {}}, {"text": "Enhanced with professional knowledge, significant\nimprovements are observed.", "metadata": {}}, {"text": "On MMLU tasks, the incorpo-\nration of professional knowledge data results in a score of\n44.50 for eFLM-16B (see Table 4), which surpasses GLM-\n130B (42.59), a model that also incorporated multi-task data\nin the related domain (Zeng et al.", "metadata": {}}, {"text": "2023).", "metadata": {}}, {"text": "For comparison,\nthe MMLU score is 27.02 for the un-enhanced FLM-16B.", "metadata": {}}, {"text": "On C-Eval tasks 9, we observe that eFLM-16B performs\nbetter than GLM-130B by about 2 points.", "metadata": {}}, {"text": "For comparison,\nthe average C-Eval score of the vanilla FLM-16B is 27.0,\nwhich underperforms GLM-130B.", "metadata": {}}, {"text": "These results suggest that\nevaluation with professional knowledge may not fully reflect\nthe capability of LLMs, particularly when different LLMs\nare trained with different data collections, and some may not\ncome with a clear list.", "metadata": {}}, {"text": "4.3 Evaluation of the Growth Strategy\nOur core method for reducing computational cost is the\ngrowth strategy.", "metadata": {}}, {"text": "We would like to answer the questions of\nwhether our growth strategy is effective in knowledge in-\nheritance, and how model capabilities grow with size.", "metadata": {}}, {"text": "We\nevaluate the performance of FLM on all the stages: 16B, 51B,\nand 101B.", "metadata": {}}, {"text": "Table 6 shows the performance of FLM models at\neach stage.", "metadata": {}}, {"text": "Results.", "metadata": {}}, {"text": "As expected, the performance of FLM improves\nwith the increase in model size.", "metadata": {}}, {"text": "FLM-101B achieves the\nbest performance on almost all tasks.", "metadata": {}}, {"text": "This means that our\nmodel inherits knowledge from the previous stage after each\ngrowth.", "metadata": {}}, {"text": "We also observe that the 101B model improves the\nperformance scores more significantly than the 51B model,\nwith less data.", "metadata": {}}, {"text": "This indicates that the models are successfully\nincorporating new weights in training after growth, and tak-\ning advantage of larger parameter counts.", "metadata": {}}, {"text": "The performance\non ARC and HellaSwag increases steadily and significantly,\nwhich corresponds well to the decline of the model loss.", "metadata": {}}, {"text": "Again, as we expected in Section 4.1, as more training data\nis processed, FLM‚Äôs performance on Open LLM improves.", "metadata": {}}, {"text": "8C-Eval can be considered as a Chinese version of MMLU.", "metadata": {}}, {"text": "9The scores are achieved on the test set by submitting to the\nC-Eval platform.", "metadata": {}}, {"text": "5 Evaluation Inspired by IQ Tests\nSection 4 presents the evaluation of existing benchmarks,\nfocusing on knowledge.", "metadata": {}}, {"text": "As we discussed in Section 1 and\n4.2, knowledge could not fully reflect the Intelligence Quo-\ntient (IQ) of LLMs.", "metadata": {}}, {"text": "As a supplement, we conduct a series\nof IQ-test task evaluation in this section.", "metadata": {}}, {"text": "For IQ evaluation,\nwe make necessary modifications to existing datasets (Wei\net al.", "metadata": {}}, {"text": "2023;", "metadata": {}}, {"text": "Weston et al.", "metadata": {}}, {"text": "2015;", "metadata": {}}, {"text": "Srivastava et al.", "metadata": {}}, {"text": "2023) or gen-\nerate new synthetic datasets.", "metadata": {}}, {"text": "Specifically, the IQ test mainly\nconsiders four aspects: symbolic mapping, rule understand-\ning, pattern mining, and anti-interference.", "metadata": {}}, {"text": "A common key\nproperty of these tasks is that they are dependent on the in-\nference and generalization in a new context, instead of the\npreviously-learned knowledge.", "metadata": {}}, {"text": "Compared Methods.", "metadata": {}}, {"text": "Borrowing psychological ideas that\nthe measurement of IQ is dependent on age 10, we mainly\nconsider existing models trained with similar amounts of data\nto FLM-101B.", "metadata": {}}, {"text": "As a milestone of LLM development, GPT-\n3 (175B) (Brown et al.", "metadata": {}}, {"text": "2020) proposed in-context learning\nfor the first time.", "metadata": {}}, {"text": "GLM-130B (Zeng et al.", "metadata": {}}, {"text": "2023) is the first\nopen English-Chinese bilingual LLM.", "metadata": {}}, {"text": "Hence, we select them\nas baseline models.", "metadata": {}}, {"text": "Both models are trained with 300 Àú400\nbillion tokens, which are in the same range as ours.", "metadata": {}}, {"text": "GPT-\n3 focuses on English, and is not included in the Chinese-\nrelated evaluation ( i.e., CLUE-IQ).", "metadata": {}}, {"text": "The results of GPT-3\nare achieved by API.", "metadata": {}}, {"text": "GLM-130B is evaluated with its open-\nsourced checkpoint.", "metadata": {}}, {"text": "Tasks, Data, and Results.", "metadata": {}}, {"text": "We curate evaluation benchmarks\nregarding four cognitive capabilities.", "metadata": {}}, {"text": "We summarize the data\nand results in this section.", "metadata": {}}, {"text": "For details in task definition and\ndata collection process, please see Appendix A.1 to A.4.", "metadata": {}}, {"text": "Symbolic Mapping.", "metadata": {}}, {"text": "An existing study (Wei et al.", "metadata": {}}, {"text": "2023)\npoints out that textual classification tasks ( e.g., sentiment\nclassification) often lack generalization.", "metadata": {}}, {"text": "Considering this, we\nuse a symbolic mapping method to replace the original cate-\ngorical labels with symbols that are unlikely to be seen in any\ntraining data.", "metadata": {}}, {"text": "Hence, we can evaluate the LLMs‚Äô language\nunderstanding ability as well as the generalization abilities\nto a new context.", "metadata": {}}, {"text": "We form our evaluation task as in-context\nlearning with few-shot examples for each label.", "metadata": {}}, {"text": "We con-\nstruct two symbolic mapping datasets, namely SuperGLUE-\nIQ and CLUE-IQ, built on SuperGLUE (Wang et al.", "metadata": {}}, {"text": "2019)\nand CLUE (Xu et al.", "metadata": {}}, {"text": "2020), respectively.", "metadata": {}}, {"text": "Examples are illus-\ntrated in the Appendix (Figure 1).", "metadata": {}}, {"text": "Results on SuperGLUE-IQ and CLUE-IQ are presented in\nTable 7 and Table 8, respectively.", "metadata": {}}, {"text": "With less computation\n10https://ocw.mit.edu/ans7870/9/9.00SC/MIT9 00SCF11 text.", "metadata": {}}, {"text": "pdf, page 367.", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "by one magnitude, FLM-101B achieves comparable per-\nformance with GPT-3 on SuperGLUE-IQ and outperforms\nGLM-130B on CLUE-IQ.\nTable 7: Performance on SuperGLUE-IQ of GPT-3, GLM-\n130B, and FLM-101B. Cost is computed in zettaFLOPs.\nModel Cost Average BoolQ WiC RTE WSC\nGPT-3 376.41 (¬±53.77) 47.60 50.84 53.33 48.38 37.86\nGLM-130B 210.80 48.19 40.13 48.67 47.65 56.31\nFLM-101B 28.22 46.76 49.50 50.33 48.38 38.83\nTable 8: Performance on CLUE-IQ for GLM-130B and\nFLM-101B. Cost is computed in zettaFLOPs.\nModel Cost Average AFQMC CSL OCNLI\nCLUE\nWSC\n2020\nGLM-130B210.80 39.96 33.33 53.85 34.0 38.67\nFLM-101B 24.54 42.07 38.33 55.29 27.33 47.33\nRule Understanding. We consider the understanding and\nexecution of rules being a strong indication of reasoning ca-\npability. To this end, we design rule understanding evaluation.\nNote that this test is different from reasoning based on the\nchain of thought. Detailed discussion is provided in Appendix\nA.2.\nWe curate data for two subtasks: Counting (0-shot) and\nString replacement (4-shots).\nTable 9: Performance of FLM-101B, GPT-3, and GLM-\n130B on rule understanding tasks.\nModel Average Counting Replace\nLowercase\nReplace\nWord\nGPT-3 86.03 82.43 80.67 95.00\nGLM-130B 71.49 60.81 69.67 84.00\nFLM-101B 76.42 69.59 64.00 95.67\nTable 9 shows the performance of our proposed FLM-101B\nagainst GPT-3 and GLM-130B on rule understanding tasks.\nFor Counting, FLM-101B achieves 69.59%, about 9 points\nbetter than GLM-130B. GPT-3 wins the first place in counting\nand Replace-Lowercase, and second place in Replace-Word.\nThis is potentially because GPT-3 is the largest model.\nPattern Mining. Pattern Mining evaluation is common in\nIQ tests. In detail, it is the induction and deduction of the\npatterns emerging in a new context.\nWe build a benchmark with three tasks (i.e., Head & Tail,\nFull Repeating, and Head Slicing) for evaluation. Figure 2\nin the Appendix shows examples of these tasks. Each task is\n5-shot and contains 100 instances.\nTable 10 lists the experimental results of our FLM-101B\nagainst the baselines on pattern mining tasks. On all three\ntasks, FLM-101B outperforms GLM-130B by a large margin.\nFor the Head & Tail and Full Repeating tasks, FLM-101B\nis a few points behind GPT-3, but outperforms the latter on\nthe Head Slicing task. Considering the computational cost,\nFLM-101B exhibits noticeable abilities.\nTable 10: Performance of FLM-101B, GPT-3, and GLM-\n130B on pattern mining tasks.\nModel Average Head & Tail Full Repeating Head Slicing\nGPT-3 70.00 61.00 92.00 57.00\nGLM-130B 53.00 38.00 70.00 51.00\nFLM-101B 64.67 52.00 79.00 63.00\nAnti-interference. Anti-interference capability is critical\nfor finding and utilizing information that is truly related to a\nspecific goal, in an unseen and noisy context (Appendix Fig-\nure 3). As suggested by the cocktail party problem in speech\nrecognition (Qian et al. 2018), we consider anti-interference\nability to be important for intelligent agents. We conduct\nanti-interference evaluation in three task types: Multiple Key\nRetrieval, Single Supporting Fact Tracking, and Two Sup-\nporting Facts Tracking, as exemplified in Figure 3 in the\nAppendix.\nTable 11: Performance of FLM-101B, GPT-3, and GLM-\n130B on anti-interference evaluation.\nModel Average\nMultiple\nKey\nRetrieval\nSingle\nSupporting\nFact\nTwo\nSupporting\nFacts\nGPT-3 70.11 92.67 78.33 39.33\nGLM-130B 53.56 77.67 56.33 26.67\nFLM-101B 60.11 89.00 59.00 32.33\nTable 11 shows the evaluation results on anti-interference.\nFLM-101B achieves the second-best passing rates with an\nadvantage of around 7% compared to GLM-130B.\nIQ Test Conclusion. On our four additional evaluations in-\nspired by the IQ tests, FLM-101B outperforms GLM-130B\nand obtains competitive results compared to GPT-3 in some\ntasks with much lower costs. Except for the impacts of train-\ning data, the superiority may be owed to a story that in the\ngrowth strategy, the smaller models in early stages refine a\nmore efficient searching space, which keeps taking effect\nwhen the model grows larger with increased generalization\nability.\n6 Conclusions, Limitations, and Future Work\nIn this paper, we introduce FLM-101B, an open-sourced\nLLM that is successfully trained from scratch within a\n$100,000 budget. The key idea of reducing the training cost\nof FLM-101B is to break through the fixed number of model\nparameters via a growth strategy. Experimental results on\nknowledeg-oriented and IQ-related benchmarks show that\nFLM-101B is comparable to strong baseline models with\nless computational cost. Note that harmful contents may be\ninduced from the open-sourced checkpoint, which do not\nrepresent the opinions of the authors.\nDue to resource issues, the limitations of our work include\ninadequate exploration and comparison for different growth\nschedules, growth operators, and amount of data. For future\nwork, we believe that our exploration on the growth strategy", "sentences": [{"text": "by one magnitude, FLM-101B achieves comparable per-\nformance with GPT-3 on SuperGLUE-IQ and outperforms\nGLM-130B on CLUE-IQ.", "metadata": {}}, {"text": "Table 7: Performance on SuperGLUE-IQ of GPT-3, GLM-\n130B, and FLM-101B.", "metadata": {}}, {"text": "Cost is computed in zettaFLOPs.", "metadata": {}}, {"text": "Model Cost Average BoolQ WiC RTE WSC\nGPT-3 376.41 (¬±53.77) 47.60 50.84 53.33 48.38 37.86\nGLM-130B 210.80 48.19 40.13 48.67 47.65 56.31\nFLM-101B 28.22 46.76 49.50 50.33 48.38 38.83\nTable 8: Performance on CLUE-IQ for GLM-130B and\nFLM-101B.", "metadata": {}}, {"text": "Cost is computed in zettaFLOPs.", "metadata": {}}, {"text": "Model Cost Average AFQMC CSL OCNLI\nCLUE\nWSC\n2020\nGLM-130B210.80 39.96 33.33 53.85 34.0 38.67\nFLM-101B 24.54 42.07 38.33 55.29 27.33 47.33\nRule Understanding.", "metadata": {}}, {"text": "We consider the understanding and\nexecution of rules being a strong indication of reasoning ca-\npability.", "metadata": {}}, {"text": "To this end, we design rule understanding evaluation.", "metadata": {}}, {"text": "Note that this test is different from reasoning based on the\nchain of thought.", "metadata": {}}, {"text": "Detailed discussion is provided in Appendix\nA.2.", "metadata": {}}, {"text": "We curate data for two subtasks: Counting (0-shot) and\nString replacement (4-shots).", "metadata": {}}, {"text": "Table 9: Performance of FLM-101B, GPT-3, and GLM-\n130B on rule understanding tasks.", "metadata": {}}, {"text": "Model Average Counting Replace\nLowercase\nReplace\nWord\nGPT-3 86.03 82.43 80.67 95.00\nGLM-130B 71.49 60.81 69.67 84.00\nFLM-101B 76.42 69.59 64.00 95.67\nTable 9 shows the performance of our proposed FLM-101B\nagainst GPT-3 and GLM-130B on rule understanding tasks.", "metadata": {}}, {"text": "For Counting, FLM-101B achieves 69.59%, about 9 points\nbetter than GLM-130B.", "metadata": {}}, {"text": "GPT-3 wins the first place in counting\nand Replace-Lowercase, and second place in Replace-Word.", "metadata": {}}, {"text": "This is potentially because GPT-3 is the largest model.", "metadata": {}}, {"text": "Pattern Mining.", "metadata": {}}, {"text": "Pattern Mining evaluation is common in\nIQ tests.", "metadata": {}}, {"text": "In detail, it is the induction and deduction of the\npatterns emerging in a new context.", "metadata": {}}, {"text": "We build a benchmark with three tasks (i.e., Head & Tail,\nFull Repeating, and Head Slicing) for evaluation.", "metadata": {}}, {"text": "Figure 2\nin the Appendix shows examples of these tasks.", "metadata": {}}, {"text": "Each task is\n5-shot and contains 100 instances.", "metadata": {}}, {"text": "Table 10 lists the experimental results of our FLM-101B\nagainst the baselines on pattern mining tasks.", "metadata": {}}, {"text": "On all three\ntasks, FLM-101B outperforms GLM-130B by a large margin.", "metadata": {}}, {"text": "For the Head & Tail and Full Repeating tasks, FLM-101B\nis a few points behind GPT-3, but outperforms the latter on\nthe Head Slicing task.", "metadata": {}}, {"text": "Considering the computational cost,\nFLM-101B exhibits noticeable abilities.", "metadata": {}}, {"text": "Table 10: Performance of FLM-101B, GPT-3, and GLM-\n130B on pattern mining tasks.", "metadata": {}}, {"text": "Model Average Head & Tail Full Repeating Head Slicing\nGPT-3 70.00 61.00 92.00 57.00\nGLM-130B 53.00 38.00 70.00 51.00\nFLM-101B 64.67 52.00 79.00 63.00\nAnti-interference.", "metadata": {}}, {"text": "Anti-interference capability is critical\nfor finding and utilizing information that is truly related to a\nspecific goal, in an unseen and noisy context (Appendix Fig-\nure 3).", "metadata": {}}, {"text": "As suggested by the cocktail party problem in speech\nrecognition (Qian et al.", "metadata": {}}, {"text": "2018), we consider anti-interference\nability to be important for intelligent agents.", "metadata": {}}, {"text": "We conduct\nanti-interference evaluation in three task types: Multiple Key\nRetrieval, Single Supporting Fact Tracking, and Two Sup-\nporting Facts Tracking, as exemplified in Figure 3 in the\nAppendix.", "metadata": {}}, {"text": "Table 11: Performance of FLM-101B, GPT-3, and GLM-\n130B on anti-interference evaluation.", "metadata": {}}, {"text": "Model Average\nMultiple\nKey\nRetrieval\nSingle\nSupporting\nFact\nTwo\nSupporting\nFacts\nGPT-3 70.11 92.67 78.33 39.33\nGLM-130B 53.56 77.67 56.33 26.67\nFLM-101B 60.11 89.00 59.00 32.33\nTable 11 shows the evaluation results on anti-interference.", "metadata": {}}, {"text": "FLM-101B achieves the second-best passing rates with an\nadvantage of around 7% compared to GLM-130B.", "metadata": {}}, {"text": "IQ Test Conclusion.", "metadata": {}}, {"text": "On our four additional evaluations in-\nspired by the IQ tests, FLM-101B outperforms GLM-130B\nand obtains competitive results compared to GPT-3 in some\ntasks with much lower costs.", "metadata": {}}, {"text": "Except for the impacts of train-\ning data, the superiority may be owed to a story that in the\ngrowth strategy, the smaller models in early stages refine a\nmore efficient searching space, which keeps taking effect\nwhen the model grows larger with increased generalization\nability.", "metadata": {}}, {"text": "6 Conclusions, Limitations, and Future Work\nIn this paper, we introduce FLM-101B, an open-sourced\nLLM that is successfully trained from scratch within a\n$100,000 budget.", "metadata": {}}, {"text": "The key idea of reducing the training cost\nof FLM-101B is to break through the fixed number of model\nparameters via a growth strategy.", "metadata": {}}, {"text": "Experimental results on\nknowledeg-oriented and IQ-related benchmarks show that\nFLM-101B is comparable to strong baseline models with\nless computational cost.", "metadata": {}}, {"text": "Note that harmful contents may be\ninduced from the open-sourced checkpoint, which do not\nrepresent the opinions of the authors.", "metadata": {}}, {"text": "Due to resource issues, the limitations of our work include\ninadequate exploration and comparison for different growth\nschedules, growth operators, and amount of data.", "metadata": {}}, {"text": "For future\nwork, we believe that our exploration on the growth strategy", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "as well as training stability would potentially be beneficial\nfor future attempts of further scaling up LLMs, e.g., beyond\n1T parameters.\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.;\nPassos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu,\nE.; Clark, J. H.; Shafey, L. E.; Huang, Y .; Meier-Hellstern,\nK.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.;\nRuder, S.; Tay, Y .; Xiao, K.; Xu, Y .; Zhang, Y .;¬¥Abrego, G. H.;\nAhn, J.; Austin, J.; Barham, P.; Botha, J. A.; Bradbury, J.;\nBrahma, S.; Brooks, K.; Catasta, M.; Cheng, Y .; Cherry, C.;\nChoquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave, S.;\nDehghani, M.; Dev, S.; Devlin, J.; D¬¥ƒ±az, M.; Du, N.; Dyer,\nE.; Feinberg, V .; Feng, F.; Fienber, V .; Freitag, M.; Garcia,\nX.; Gehrmann, S.; Gonzalez, L.; and et al. 2023. PaLM 2\nTechnical Report. CoRR, abs/2305.10403.\nBi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.;\nDing, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek\nllm: Scaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877‚Äì\n1901.\nChen, C.; Yin, Y .; Shang, L.; Jiang, X.; Qin, Y .; Wang, F.;\nWang, Z.; Chen, X.; Liu, Z.; and Liu, Q. 2022. bert2BERT:\nTowards Reusable Pretrained Language Models. In Mure-\nsan, S.; Nakov, P.; and Villavicencio, A., eds.,Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, 2134‚Äì2148. Association\nfor Computational Linguistics.\nChen, T.; Goodfellow, I. J.; and Shlens, J. 2016. Net2Net:\nAccelerating Learning via Knowledge Transfer. In Bengio,\nY .; and LeCun, Y ., eds.,4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico,\nMay 2-4, 2016, Conference Track Proceedings.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have Solved\nQuestion Answering? Try ARC, the AI2 Reasoning Chal-\nlenge. CoRR, abs/1803.05457.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), 4171‚Äì4186. Association for Com-\nputational Linguistics.\nEriksson, P. S.; Perfilieva, E.; Bj¬®ork-Eriksson, T.; Alborn, A.-\nM.; Nordborg, C.; Peterson, D. A.; and Gage, F. H. 1998. Neu-\nrogenesis in the adult human hippocampus. Nature medicine,\n4(11): 1313‚Äì1317.\nFan, S.; Wang, Y .; Li, J.; Zhang, Z.; Shang, S.; and Han,\nP. 2022. Interactive Information Extraction by Semantic\nInformation Graph. In Raedt, L. D., ed., Proceedings of\nthe Thirty-First International Joint Conference on Artificial\nIntelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022,\n4100‚Äì4106. ijcai.org.\nGong, L.; He, D.; Li, Z.; Qin, T.; Wang, L.; and Liu, T.\n2019. Efficient training of bert by progressively stacking. In\nInternational conference on machine learning, 2337‚Äì2346.\nPMLR.\nGu, X.; Liu, L.; Yu, H.; Li, J.; Chen, C.; and Han, J. 2021. On\nthe Transformer Growth for Progressive BERT Training. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 5174‚Äì5180.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multi-\ntask Language Understanding. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHenighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.;\nJackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.;\nHallacy, C.; Mann, B.; Radford, A.; Ramesh, A.; Ryder, N.;\nZiegler, D. M.; Schulman, J.; Amodei, D.; and McCandlish, S.\n2020. Scaling Laws for Autoregressive Generative Modeling.\nCoRR, abs/2010.14701.\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.;\nCai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.;\nWelbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.;\nvan den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.;\nSimonyan, K.; Elsen, E.; Vinyals, O.; Rae, J. W.; and Sifre,\nL. 2022. An empirical analysis of compute-optimal large\nlanguage model training. In NeurIPS.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He, J. 2023.\nC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models. CoRR, abs/2305.08322.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling Laws for Neural Language Mod-\nels. CoRR, abs/2001.08361.\nKorthikanti, V .; Casper, J.; Lym, S.; McAfee, L.; Ander-\nsch, M.; Shoeybi, M.; and Catanzaro, B. 2022. Reducing\nActivation Recomputation in Large Transformer Models.\narXiv:2205.05198.\nLi, X.; Jiang, X.; Meng, X.; Sun, A.; and Wang, Y .\n2023. FreeLM: Fine-Tuning-Free Language Model. CoRR,\nabs/2305.01616.\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Measur-\ning How Models Mimic Human Falsehoods. In Muresan, S.;\nNakov, P.; and Villavicencio, A., eds.,Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, 3214‚Äì3252. Association for Computational\nLinguistics.\nLittwin, E.; and Yang, G. 2023. Adaptive Optimization in the\n‚àû-Width Limit. In The Eleventh International Conference", "sentences": [{"text": "as well as training stability would potentially be beneficial\nfor future attempts of further scaling up LLMs, e.g., beyond\n1T parameters.", "metadata": {}}, {"text": "References\nAnil, R.;", "metadata": {}}, {"text": "Dai, A.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Firat, O.;", "metadata": {}}, {"text": "Johnson, M.;", "metadata": {}}, {"text": "Lepikhin, D.;", "metadata": {}}, {"text": "Passos, A.;", "metadata": {}}, {"text": "Shakeri, S.;", "metadata": {}}, {"text": "Taropa, E.;", "metadata": {}}, {"text": "Bailey, P.;", "metadata": {}}, {"text": "Chen, Z.;", "metadata": {}}, {"text": "Chu,\nE.;", "metadata": {}}, {"text": "Clark, J.", "metadata": {}}, {"text": "H.;", "metadata": {}}, {"text": "Shafey, L.", "metadata": {}}, {"text": "E.;", "metadata": {}}, {"text": "Huang, Y .;", "metadata": {}}, {"text": "Meier-Hellstern,\nK.;", "metadata": {}}, {"text": "Mishra, G.;", "metadata": {}}, {"text": "Moreira, E.;", "metadata": {}}, {"text": "Omernick, M.;", "metadata": {}}, {"text": "Robinson, K.;", "metadata": {}}, {"text": "Ruder, S.;", "metadata": {}}, {"text": "Tay, Y .;", "metadata": {}}, {"text": "Xiao, K.;", "metadata": {}}, {"text": "Xu, Y .;", "metadata": {}}, {"text": "Zhang, Y .;¬¥Abrego, G.", "metadata": {}}, {"text": "H.;", "metadata": {}}, {"text": "Ahn, J.;", "metadata": {}}, {"text": "Austin, J.;", "metadata": {}}, {"text": "Barham, P.;", "metadata": {}}, {"text": "Botha, J.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "Bradbury, J.;", "metadata": {}}, {"text": "Brahma, S.;", "metadata": {}}, {"text": "Brooks, K.;", "metadata": {}}, {"text": "Catasta, M.;", "metadata": {}}, {"text": "Cheng, Y .;", "metadata": {}}, {"text": "Cherry, C.;", "metadata": {}}, {"text": "Choquette-Choo, C.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "Chowdhery, A.;", "metadata": {}}, {"text": "Crepy, C.;", "metadata": {}}, {"text": "Dave, S.;", "metadata": {}}, {"text": "Dehghani, M.;", "metadata": {}}, {"text": "Dev, S.;", "metadata": {}}, {"text": "Devlin, J.;", "metadata": {}}, {"text": "D¬¥ƒ±az, M.;", "metadata": {}}, {"text": "Du, N.;", "metadata": {}}, {"text": "Dyer,\nE.;", "metadata": {}}, {"text": "Feinberg, V .;", "metadata": {}}, {"text": "Feng, F.;", "metadata": {}}, {"text": "Fienber, V .;", "metadata": {}}, {"text": "Freitag, M.;", "metadata": {}}, {"text": "Garcia,\nX.;", "metadata": {}}, {"text": "Gehrmann, S.;", "metadata": {}}, {"text": "Gonzalez, L.;", "metadata": {}}, {"text": "and et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "PaLM 2\nTechnical Report.", "metadata": {}}, {"text": "CoRR, abs/2305.10403.", "metadata": {}}, {"text": "Bi, X.;", "metadata": {}}, {"text": "Chen, D.;", "metadata": {}}, {"text": "Chen, G.;", "metadata": {}}, {"text": "Chen, S.;", "metadata": {}}, {"text": "Dai, D.;", "metadata": {}}, {"text": "Deng, C.;", "metadata": {}}, {"text": "Ding, H.;", "metadata": {}}, {"text": "Dong, K.;", "metadata": {}}, {"text": "Du, Q.;", "metadata": {}}, {"text": "Fu, Z.;", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Deepseek\nllm: Scaling open-source language models with longtermism.", "metadata": {}}, {"text": "arXiv preprint arXiv:2401.02954.", "metadata": {}}, {"text": "Brown, T.;", "metadata": {}}, {"text": "Mann, B.;", "metadata": {}}, {"text": "Ryder, N.;", "metadata": {}}, {"text": "Subbiah, M.;", "metadata": {}}, {"text": "Kaplan, J.", "metadata": {}}, {"text": "D.;", "metadata": {}}, {"text": "Dhariwal, P.;", "metadata": {}}, {"text": "Neelakantan, A.;", "metadata": {}}, {"text": "Shyam, P.;", "metadata": {}}, {"text": "Sastry, G.;", "metadata": {}}, {"text": "Askell,\nA.;", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Language models are few-shot learners.", "metadata": {}}, {"text": "Advances in neural information processing systems, 33: 1877‚Äì\n1901.", "metadata": {}}, {"text": "Chen, C.;", "metadata": {}}, {"text": "Yin, Y .;", "metadata": {}}, {"text": "Shang, L.;", "metadata": {}}, {"text": "Jiang, X.;", "metadata": {}}, {"text": "Qin, Y .;", "metadata": {}}, {"text": "Wang, F.;", "metadata": {}}, {"text": "Wang, Z.;", "metadata": {}}, {"text": "Chen, X.;", "metadata": {}}, {"text": "Liu, Z.;", "metadata": {}}, {"text": "and Liu, Q.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "bert2BERT:\nTowards Reusable Pretrained Language Models.", "metadata": {}}, {"text": "In Mure-\nsan, S.;", "metadata": {}}, {"text": "Nakov, P.;", "metadata": {}}, {"text": "and Villavicencio, A., eds.,Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, 2134‚Äì2148.", "metadata": {}}, {"text": "Association\nfor Computational Linguistics.", "metadata": {}}, {"text": "Chen, T.;", "metadata": {}}, {"text": "Goodfellow, I.", "metadata": {}}, {"text": "J.;", "metadata": {}}, {"text": "and Shlens, J.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Net2Net:\nAccelerating Learning via Knowledge Transfer.", "metadata": {}}, {"text": "In Bengio,\nY .;", "metadata": {}}, {"text": "and LeCun, Y ., eds.,4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico,\nMay 2-4, 2016, Conference Track Proceedings.", "metadata": {}}, {"text": "Clark, P.;", "metadata": {}}, {"text": "Cowhey, I.;", "metadata": {}}, {"text": "Etzioni, O.;", "metadata": {}}, {"text": "Khot, T.;", "metadata": {}}, {"text": "Sabharwal, A.;", "metadata": {}}, {"text": "Schoenick, C.;", "metadata": {}}, {"text": "and Tafjord, O.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Think you have Solved\nQuestion Answering?", "metadata": {}}, {"text": "Try ARC, the AI2 Reasoning Chal-\nlenge.", "metadata": {}}, {"text": "CoRR, abs/1803.05457.", "metadata": {}}, {"text": "Devlin, J.;", "metadata": {}}, {"text": "Chang, M.;", "metadata": {}}, {"text": "Lee, K.;", "metadata": {}}, {"text": "and Toutanova, K.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding.", "metadata": {}}, {"text": "In Burstein, J.;", "metadata": {}}, {"text": "Doran, C.;", "metadata": {}}, {"text": "and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), 4171‚Äì4186.", "metadata": {}}, {"text": "Association for Com-\nputational Linguistics.", "metadata": {}}, {"text": "Eriksson, P.", "metadata": {}}, {"text": "S.;", "metadata": {}}, {"text": "Perfilieva, E.;", "metadata": {}}, {"text": "Bj¬®ork-Eriksson, T.;", "metadata": {}}, {"text": "Alborn, A.-\nM.;", "metadata": {}}, {"text": "Nordborg, C.;", "metadata": {}}, {"text": "Peterson, D.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "and Gage, F.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "1998.", "metadata": {}}, {"text": "Neu-\nrogenesis in the adult human hippocampus.", "metadata": {}}, {"text": "Nature medicine,\n4(11): 1313‚Äì1317.", "metadata": {}}, {"text": "Fan, S.;", "metadata": {}}, {"text": "Wang, Y .;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "Zhang, Z.;", "metadata": {}}, {"text": "Shang, S.;", "metadata": {}}, {"text": "and Han,\nP.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Interactive Information Extraction by Semantic\nInformation Graph.", "metadata": {}}, {"text": "In Raedt, L.", "metadata": {}}, {"text": "D., ed., Proceedings of\nthe Thirty-First International Joint Conference on Artificial\nIntelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022,\n4100‚Äì4106.", "metadata": {}}, {"text": "ijcai.org.", "metadata": {}}, {"text": "Gong, L.;", "metadata": {}}, {"text": "He, D.;", "metadata": {}}, {"text": "Li, Z.;", "metadata": {}}, {"text": "Qin, T.;", "metadata": {}}, {"text": "Wang, L.;", "metadata": {}}, {"text": "and Liu, T.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Efficient training of bert by progressively stacking.", "metadata": {}}, {"text": "In\nInternational conference on machine learning, 2337‚Äì2346.", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "Gu, X.;", "metadata": {}}, {"text": "Liu, L.;", "metadata": {}}, {"text": "Yu, H.;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "Chen, C.;", "metadata": {}}, {"text": "and Han, J.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "On\nthe Transformer Growth for Progressive BERT Training.", "metadata": {}}, {"text": "In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 5174‚Äì5180.", "metadata": {}}, {"text": "Hendrycks, D.;", "metadata": {}}, {"text": "Burns, C.;", "metadata": {}}, {"text": "Basart, S.;", "metadata": {}}, {"text": "Zou, A.;", "metadata": {}}, {"text": "Mazeika, M.;", "metadata": {}}, {"text": "Song, D.;", "metadata": {}}, {"text": "and Steinhardt, J.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Measuring Massive Multi-\ntask Language Understanding.", "metadata": {}}, {"text": "In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021.", "metadata": {}}, {"text": "OpenReview.net.", "metadata": {}}, {"text": "Henighan, T.;", "metadata": {}}, {"text": "Kaplan, J.;", "metadata": {}}, {"text": "Katz, M.;", "metadata": {}}, {"text": "Chen, M.;", "metadata": {}}, {"text": "Hesse, C.;", "metadata": {}}, {"text": "Jackson, J.;", "metadata": {}}, {"text": "Jun, H.;", "metadata": {}}, {"text": "Brown, T.", "metadata": {}}, {"text": "B.;", "metadata": {}}, {"text": "Dhariwal, P.;", "metadata": {}}, {"text": "Gray, S.;", "metadata": {}}, {"text": "Hallacy, C.;", "metadata": {}}, {"text": "Mann, B.;", "metadata": {}}, {"text": "Radford, A.;", "metadata": {}}, {"text": "Ramesh, A.;", "metadata": {}}, {"text": "Ryder, N.;", "metadata": {}}, {"text": "Ziegler, D.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Schulman, J.;", "metadata": {}}, {"text": "Amodei, D.;", "metadata": {}}, {"text": "and McCandlish, S.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Scaling Laws for Autoregressive Generative Modeling.", "metadata": {}}, {"text": "CoRR, abs/2010.14701.", "metadata": {}}, {"text": "Hoffmann, J.;", "metadata": {}}, {"text": "Borgeaud, S.;", "metadata": {}}, {"text": "Mensch, A.;", "metadata": {}}, {"text": "Buchatskaya, E.;", "metadata": {}}, {"text": "Cai, T.;", "metadata": {}}, {"text": "Rutherford, E.;", "metadata": {}}, {"text": "de Las Casas, D.;", "metadata": {}}, {"text": "Hendricks, L.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "Welbl, J.;", "metadata": {}}, {"text": "Clark, A.;", "metadata": {}}, {"text": "Hennigan, T.;", "metadata": {}}, {"text": "Noland, E.;", "metadata": {}}, {"text": "Millican, K.;", "metadata": {}}, {"text": "van den Driessche, G.;", "metadata": {}}, {"text": "Damoc, B.;", "metadata": {}}, {"text": "Guy, A.;", "metadata": {}}, {"text": "Osindero, S.;", "metadata": {}}, {"text": "Simonyan, K.;", "metadata": {}}, {"text": "Elsen, E.;", "metadata": {}}, {"text": "Vinyals, O.;", "metadata": {}}, {"text": "Rae, J.", "metadata": {}}, {"text": "W.;", "metadata": {}}, {"text": "and Sifre,\nL.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "An empirical analysis of compute-optimal large\nlanguage model training.", "metadata": {}}, {"text": "In NeurIPS.", "metadata": {}}, {"text": "Huang, Y .;", "metadata": {}}, {"text": "Bai, Y .;", "metadata": {}}, {"text": "Zhu, Z.;", "metadata": {}}, {"text": "Zhang, J.;", "metadata": {}}, {"text": "Zhang, J.;", "metadata": {}}, {"text": "Su, T.;", "metadata": {}}, {"text": "Liu,\nJ.;", "metadata": {}}, {"text": "Lv, C.;", "metadata": {}}, {"text": "Zhang, Y .;", "metadata": {}}, {"text": "Lei, J.;", "metadata": {}}, {"text": "Fu, Y .;", "metadata": {}}, {"text": "Sun, M.;", "metadata": {}}, {"text": "and He, J.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models.", "metadata": {}}, {"text": "CoRR, abs/2305.08322.", "metadata": {}}, {"text": "Kaplan, J.;", "metadata": {}}, {"text": "McCandlish, S.;", "metadata": {}}, {"text": "Henighan, T.;", "metadata": {}}, {"text": "Brown, T.", "metadata": {}}, {"text": "B.;", "metadata": {}}, {"text": "Chess, B.;", "metadata": {}}, {"text": "Child, R.;", "metadata": {}}, {"text": "Gray, S.;", "metadata": {}}, {"text": "Radford, A.;", "metadata": {}}, {"text": "Wu, J.;", "metadata": {}}, {"text": "and\nAmodei, D.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Scaling Laws for Neural Language Mod-\nels.", "metadata": {}}, {"text": "CoRR, abs/2001.08361.", "metadata": {}}, {"text": "Korthikanti, V .;", "metadata": {}}, {"text": "Casper, J.;", "metadata": {}}, {"text": "Lym, S.;", "metadata": {}}, {"text": "McAfee, L.;", "metadata": {}}, {"text": "Ander-\nsch, M.;", "metadata": {}}, {"text": "Shoeybi, M.;", "metadata": {}}, {"text": "and Catanzaro, B.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Reducing\nActivation Recomputation in Large Transformer Models.", "metadata": {}}, {"text": "arXiv:2205.05198.", "metadata": {}}, {"text": "Li, X.;", "metadata": {}}, {"text": "Jiang, X.;", "metadata": {}}, {"text": "Meng, X.;", "metadata": {}}, {"text": "Sun, A.;", "metadata": {}}, {"text": "and Wang, Y .", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "FreeLM: Fine-Tuning-Free Language Model.", "metadata": {}}, {"text": "CoRR,\nabs/2305.01616.", "metadata": {}}, {"text": "Lin, S.;", "metadata": {}}, {"text": "Hilton, J.;", "metadata": {}}, {"text": "and Evans, O.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "TruthfulQA: Measur-\ning How Models Mimic Human Falsehoods.", "metadata": {}}, {"text": "In Muresan, S.;", "metadata": {}}, {"text": "Nakov, P.;", "metadata": {}}, {"text": "and Villavicencio, A., eds.,Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, 3214‚Äì3252.", "metadata": {}}, {"text": "Association for Computational\nLinguistics.", "metadata": {}}, {"text": "Littwin, E.;", "metadata": {}}, {"text": "and Yang, G.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Adaptive Optimization in the\n‚àû-Width Limit.", "metadata": {}}, {"text": "In The Eleventh International Conference", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "on Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nLiu, Y .; Wang, Y .; Sun, A.; Meng, X.; Li, J.; and Guo, J. 2022.\nA Dual-Channel Framework for Sarcasm Recognition by\nDetecting Sentiment Conflict. In Carpuat, M.; de Marneffe,\nM.; and Ru ¬¥ƒ±z, I. V . M., eds., Findings of the Association\nfor Computational Linguistics: NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022, 1670‚Äì1680. Association for\nComputational Linguistics.\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay\nRegularization in Adam. CoRR, abs/1711.05101.\nMeng, X.; Lin, C.; Wang, Y .; and Zhang, Y . 2023. Net-\nGPT: Generative Pretrained Transformer for Network Traffic.\nCoRR, abs/2304.09513.\nMeta. 2024. Introducing Meta Llama 3: The most capable\nopenly available LLM to date. https://ai.meta.com/blog/meta-\nllama-3/.\nMistral. 2024. Mistral 8x22B. https://mistral.ai/news/mixtral-\n8x22b/.\nNarayanan, D.; Shoeybi, M.; Casper, J.; LeGresley, P.; Pat-\nwary, M.; Korthikanti, V .; Vainbrand, D.; Kashinkunti, P.;\nBernauer, J.; Catanzaro, B.; Phanishayee, A.; and Zaharia,\nM. 2021. Efficient Large-Scale Language Model Training on\nGPU Clusters. CoRR, abs/2104.04473.\nOpenAI. 2023. GPT-4 Technical Report. CoRR,\nabs/2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.;\nAskell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe,\nR. 2022. Training language models to follow instructions\nwith human feedback. In NeurIPS.\nPatterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia, L.-\nM.; Rothchild, D.; So, D.; Texier, M.; and Dean, J. 2021.\nCarbon emissions and large neural network training. arXiv\npreprint arXiv:2104.10350.\nPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cap-\npelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and\nLaunay, J. 2023. The RefinedWeb dataset for Falcon LLM:\noutperforming curated corpora with web data, and web data\nonly. arXiv preprint arXiv:2306.01116.\nQian, Y .; Weng, C.; Chang, X.; Wang, S.; and Yu, D. 2018.\nPast review, current progress, and challenges ahead on the\ncocktail party problem. Frontiers Inf. Technol. Electron. Eng.,\n19(1): 40‚Äì63.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised Mul-\ntitask Learners.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.;\nSong, H. F.; Aslanides, J.; Henderson, S.; Ring, R.; Young,\nS.; Rutherford, E.; Hennigan, T.; Menick, J.; Cassirer, A.;\nPowell, R.; van den Driessche, G.; Hendricks, L. A.; Rauh,\nM.; Huang, P.; Glaese, A.; Welbl, J.; Dathathri, S.; Huang, S.;\nUesato, J.; Mellor, J.; Higgins, I.; Creswell, A.; McAleese,\nN.; Wu, A.; Elsen, E.; Jayakumar, S. M.; Buchatskaya, E.;\nBudden, D.; Sutherland, E.; Simonyan, K.; Paganini, M.;\nSifre, L.; Martens, L.; Li, X. L.; Kuncoro, A.; Nematzadeh,\nA.; Gribovskaya, E.; Donato, D.; Lazaridou, A.; Mensch,\nA.; Lespiau, J.; Tsimpoukelli, M.; Grigorev, N.; Fritz, D.;\nSottiaux, T.; Pajarskas, M.; Pohlen, T.; Gong, Z.; Toyama,\nD.; de Masson d‚ÄôAutume, C.; Li, Y .; Terzi, T.; Mikulik, V .;\nBabuschkin, I.; Clark, A.; de Las Casas, D.; Guy, A.; Jones,\nC.; Bradbury, J.; Johnson, M. J.; Hechtman, B. A.; Weidinger,\nL.; Gabriel, I.; Isaac, W.; Lockhart, E.; Osindero, S.; Rimell,\nL.; Dyer, C.; Vinyals, O.; Ayoub, K.; Stanway, J.; Bennett, L.;\nHassabis, D.; Kavukcuoglu, K.; and Irving, G. 2021. Scal-\ning Language Models: Methods, Analysis & Insights from\nTraining Gopher. CoRR, abs/2112.11446.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21: 140:1‚Äì140:67.\nRajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y . 2019.\nZeRO: Memory Optimization Towards Training A Trillion\nParameter Models. CoRR, abs/1910.02054.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow,\nD.; Castagn¬¥e, R.; Luccioni, A. S.; Yvon, F.; Gall¬¥e, M.; Tow,\nJ.; Rush, A. M.; Biderman, S.; Webson, A.; Ammanamanchi,\nP. S.; Wang, T.; Sagot, B.; Muennighoff, N.; del Moral, A. V .;\nRuwase, O.; Bawden, R.; Bekman, S.; McMillan-Major, A.;\nBeltagy, I.; Nguyen, H.; Saulnier, L.; Tan, S.; Suarez, P. O.;\nSanh, V .; Laurenc ¬∏on, H.; Jernite, Y .; Launay, J.; Mitchell, M.;\nRaffel, C.; Gokaslan, A.; Simhi, A.; Soroa, A.; Aji, A. F.; Al-\nfassy, A.; Rogers, A.; Nitzav, A. K.; Xu, C.; Mou, C.; Emezue,\nC.; Klamm, C.; Leong, C.; van Strien, D.; Adelani, D. I.; and\net al. 2022. BLOOM: A 176B-Parameter Open-Access Mul-\ntilingual Language Model. CoRR, abs/2211.05100.\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2020.\nGreen ai. Communications of the ACM, 63(12): 54‚Äì63.\nShen, S.; Walsh, P.; Keutzer, K.; Dodge, J.; Peters, M. E.; and\nBeltagy, I. 2022. Staged Training for Transformer Language\nModels. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesv ¬¥ari,\nC.; Niu, G.; and Sabato, S., eds.,International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA , volume 162 of Proceedings of Machine\nLearning Research, 19893‚Äì19908. PMLR.\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,\nJ.; and Catanzaro, B. 2019. Megatron-LM: Training Multi-\nBillion Parameter Language Models Using Model Paral-\nlelism. CoRR, abs/1909.08053.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; et al. 2023. Beyond the Imitation Game: Quanti-\nfying and extrapolating the capabilities of language models.\nTransactions on Machine Learning Research.\nSu, J.; Lu, Y .; Pan, S.; Wen, B.; and Liu, Y . 2021. Ro-\nFormer: Enhanced Transformer with Rotary Position Em-\nbedding. CoRR, abs/2104.09864.\nSun, Y .; Dong, L.; Patra, B.; Ma, S.; Huang, S.; Benhaim,\nA.; Chaudhary, V .; Song, X.; and Wei, F. 2023. A Length-", "sentences": [{"text": "on Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023.", "metadata": {}}, {"text": "OpenReview.net.", "metadata": {}}, {"text": "Liu, Y .;", "metadata": {}}, {"text": "Wang, Y .;", "metadata": {}}, {"text": "Sun, A.;", "metadata": {}}, {"text": "Meng, X.;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "and Guo, J.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "A Dual-Channel Framework for Sarcasm Recognition by\nDetecting Sentiment Conflict.", "metadata": {}}, {"text": "In Carpuat, M.;", "metadata": {}}, {"text": "de Marneffe,\nM.;", "metadata": {}}, {"text": "and Ru ¬¥ƒ±z, I.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "M., eds., Findings of the Association\nfor Computational Linguistics: NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022, 1670‚Äì1680.", "metadata": {}}, {"text": "Association for\nComputational Linguistics.", "metadata": {}}, {"text": "Loshchilov, I.;", "metadata": {}}, {"text": "and Hutter, F.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Fixing Weight Decay\nRegularization in Adam.", "metadata": {}}, {"text": "CoRR, abs/1711.05101.", "metadata": {}}, {"text": "Meng, X.;", "metadata": {}}, {"text": "Lin, C.;", "metadata": {}}, {"text": "Wang, Y .;", "metadata": {}}, {"text": "and Zhang, Y .", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Net-\nGPT: Generative Pretrained Transformer for Network Traffic.", "metadata": {}}, {"text": "CoRR, abs/2304.09513.", "metadata": {}}, {"text": "Meta.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Introducing Meta Llama 3: The most capable\nopenly available LLM to date.", "metadata": {}}, {"text": "https://ai.meta.com/blog/meta-\nllama-3/.", "metadata": {}}, {"text": "Mistral.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Mistral 8x22B.", "metadata": {}}, {"text": "https://mistral.ai/news/mixtral-\n8x22b/.", "metadata": {}}, {"text": "Narayanan, D.;", "metadata": {}}, {"text": "Shoeybi, M.;", "metadata": {}}, {"text": "Casper, J.;", "metadata": {}}, {"text": "LeGresley, P.;", "metadata": {}}, {"text": "Pat-\nwary, M.;", "metadata": {}}, {"text": "Korthikanti, V .;", "metadata": {}}, {"text": "Vainbrand, D.;", "metadata": {}}, {"text": "Kashinkunti, P.;", "metadata": {}}, {"text": "Bernauer, J.;", "metadata": {}}, {"text": "Catanzaro, B.;", "metadata": {}}, {"text": "Phanishayee, A.;", "metadata": {}}, {"text": "and Zaharia,\nM.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Efficient Large-Scale Language Model Training on\nGPU Clusters.", "metadata": {}}, {"text": "CoRR, abs/2104.04473.", "metadata": {}}, {"text": "OpenAI.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "GPT-4 Technical Report.", "metadata": {}}, {"text": "CoRR,\nabs/2303.08774.", "metadata": {}}, {"text": "Ouyang, L.;", "metadata": {}}, {"text": "Wu, J.;", "metadata": {}}, {"text": "Jiang, X.;", "metadata": {}}, {"text": "Almeida, D.;", "metadata": {}}, {"text": "Wainwright,\nC.", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "Mishkin, P.;", "metadata": {}}, {"text": "Zhang, C.;", "metadata": {}}, {"text": "Agarwal, S.;", "metadata": {}}, {"text": "Slama, K.;", "metadata": {}}, {"text": "Ray,\nA.;", "metadata": {}}, {"text": "Schulman, J.;", "metadata": {}}, {"text": "Hilton, J.;", "metadata": {}}, {"text": "Kelton, F.;", "metadata": {}}, {"text": "Miller, L.;", "metadata": {}}, {"text": "Simens, M.;", "metadata": {}}, {"text": "Askell, A.;", "metadata": {}}, {"text": "Welinder, P.;", "metadata": {}}, {"text": "Christiano, P.", "metadata": {}}, {"text": "F.;", "metadata": {}}, {"text": "Leike, J.;", "metadata": {}}, {"text": "and Lowe,\nR.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Training language models to follow instructions\nwith human feedback.", "metadata": {}}, {"text": "In NeurIPS.", "metadata": {}}, {"text": "Patterson, D.;", "metadata": {}}, {"text": "Gonzalez, J.;", "metadata": {}}, {"text": "Le, Q.;", "metadata": {}}, {"text": "Liang, C.;", "metadata": {}}, {"text": "Munguia, L.-\nM.;", "metadata": {}}, {"text": "Rothchild, D.;", "metadata": {}}, {"text": "So, D.;", "metadata": {}}, {"text": "Texier, M.;", "metadata": {}}, {"text": "and Dean, J.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2104.10350.", "metadata": {}}, {"text": "Penedo, G.;", "metadata": {}}, {"text": "Malartic, Q.;", "metadata": {}}, {"text": "Hesslow, D.;", "metadata": {}}, {"text": "Cojocaru, R.;", "metadata": {}}, {"text": "Cap-\npelli, A.;", "metadata": {}}, {"text": "Alobeidli, H.;", "metadata": {}}, {"text": "Pannier, B.;", "metadata": {}}, {"text": "Almazrouei, E.;", "metadata": {}}, {"text": "and\nLaunay, J.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The RefinedWeb dataset for Falcon LLM:\noutperforming curated corpora with web data, and web data\nonly.", "metadata": {}}, {"text": "arXiv preprint arXiv:2306.01116.", "metadata": {}}, {"text": "Qian, Y .;", "metadata": {}}, {"text": "Weng, C.;", "metadata": {}}, {"text": "Chang, X.;", "metadata": {}}, {"text": "Wang, S.;", "metadata": {}}, {"text": "and Yu, D.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Past review, current progress, and challenges ahead on the\ncocktail party problem.", "metadata": {}}, {"text": "Frontiers Inf.", "metadata": {}}, {"text": "Technol.", "metadata": {}}, {"text": "Electron.", "metadata": {}}, {"text": "Eng.,\n19(1): 40‚Äì63.", "metadata": {}}, {"text": "Radford, A.;", "metadata": {}}, {"text": "Narasimhan, K.;", "metadata": {}}, {"text": "Salimans, T.;", "metadata": {}}, {"text": "Sutskever, I.;", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Improving language understanding by generative\npre-training.", "metadata": {}}, {"text": "Radford, A.;", "metadata": {}}, {"text": "Wu, J.;", "metadata": {}}, {"text": "Child, R.;", "metadata": {}}, {"text": "Luan, D.;", "metadata": {}}, {"text": "Amodei, D.;", "metadata": {}}, {"text": "and\nSutskever, I.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Language Models are Unsupervised Mul-\ntitask Learners.", "metadata": {}}, {"text": "Rae, J.", "metadata": {}}, {"text": "W.;", "metadata": {}}, {"text": "Borgeaud, S.;", "metadata": {}}, {"text": "Cai, T.;", "metadata": {}}, {"text": "Millican, K.;", "metadata": {}}, {"text": "Hoffmann, J.;", "metadata": {}}, {"text": "Song, H.", "metadata": {}}, {"text": "F.;", "metadata": {}}, {"text": "Aslanides, J.;", "metadata": {}}, {"text": "Henderson, S.;", "metadata": {}}, {"text": "Ring, R.;", "metadata": {}}, {"text": "Young,\nS.;", "metadata": {}}, {"text": "Rutherford, E.;", "metadata": {}}, {"text": "Hennigan, T.;", "metadata": {}}, {"text": "Menick, J.;", "metadata": {}}, {"text": "Cassirer, A.;", "metadata": {}}, {"text": "Powell, R.;", "metadata": {}}, {"text": "van den Driessche, G.;", "metadata": {}}, {"text": "Hendricks, L.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "Rauh,\nM.;", "metadata": {}}, {"text": "Huang, P.;", "metadata": {}}, {"text": "Glaese, A.;", "metadata": {}}, {"text": "Welbl, J.;", "metadata": {}}, {"text": "Dathathri, S.;", "metadata": {}}, {"text": "Huang, S.;", "metadata": {}}, {"text": "Uesato, J.;", "metadata": {}}, {"text": "Mellor, J.;", "metadata": {}}, {"text": "Higgins, I.;", "metadata": {}}, {"text": "Creswell, A.;", "metadata": {}}, {"text": "McAleese,\nN.;", "metadata": {}}, {"text": "Wu, A.;", "metadata": {}}, {"text": "Elsen, E.;", "metadata": {}}, {"text": "Jayakumar, S.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Buchatskaya, E.;", "metadata": {}}, {"text": "Budden, D.;", "metadata": {}}, {"text": "Sutherland, E.;", "metadata": {}}, {"text": "Simonyan, K.;", "metadata": {}}, {"text": "Paganini, M.;", "metadata": {}}, {"text": "Sifre, L.;", "metadata": {}}, {"text": "Martens, L.;", "metadata": {}}, {"text": "Li, X.", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "Kuncoro, A.;", "metadata": {}}, {"text": "Nematzadeh,\nA.;", "metadata": {}}, {"text": "Gribovskaya, E.;", "metadata": {}}, {"text": "Donato, D.;", "metadata": {}}, {"text": "Lazaridou, A.;", "metadata": {}}, {"text": "Mensch,\nA.;", "metadata": {}}, {"text": "Lespiau, J.;", "metadata": {}}, {"text": "Tsimpoukelli, M.;", "metadata": {}}, {"text": "Grigorev, N.;", "metadata": {}}, {"text": "Fritz, D.;", "metadata": {}}, {"text": "Sottiaux, T.;", "metadata": {}}, {"text": "Pajarskas, M.;", "metadata": {}}, {"text": "Pohlen, T.;", "metadata": {}}, {"text": "Gong, Z.;", "metadata": {}}, {"text": "Toyama,\nD.;", "metadata": {}}, {"text": "de Masson d‚ÄôAutume, C.;", "metadata": {}}, {"text": "Li, Y .;", "metadata": {}}, {"text": "Terzi, T.;", "metadata": {}}, {"text": "Mikulik, V .;", "metadata": {}}, {"text": "Babuschkin, I.;", "metadata": {}}, {"text": "Clark, A.;", "metadata": {}}, {"text": "de Las Casas, D.;", "metadata": {}}, {"text": "Guy, A.;", "metadata": {}}, {"text": "Jones,\nC.;", "metadata": {}}, {"text": "Bradbury, J.;", "metadata": {}}, {"text": "Johnson, M.", "metadata": {}}, {"text": "J.;", "metadata": {}}, {"text": "Hechtman, B.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "Weidinger,\nL.;", "metadata": {}}, {"text": "Gabriel, I.;", "metadata": {}}, {"text": "Isaac, W.;", "metadata": {}}, {"text": "Lockhart, E.;", "metadata": {}}, {"text": "Osindero, S.;", "metadata": {}}, {"text": "Rimell,\nL.;", "metadata": {}}, {"text": "Dyer, C.;", "metadata": {}}, {"text": "Vinyals, O.;", "metadata": {}}, {"text": "Ayoub, K.;", "metadata": {}}, {"text": "Stanway, J.;", "metadata": {}}, {"text": "Bennett, L.;", "metadata": {}}, {"text": "Hassabis, D.;", "metadata": {}}, {"text": "Kavukcuoglu, K.;", "metadata": {}}, {"text": "and Irving, G.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Scal-\ning Language Models: Methods, Analysis & Insights from\nTraining Gopher.", "metadata": {}}, {"text": "CoRR, abs/2112.11446.", "metadata": {}}, {"text": "Raffel, C.;", "metadata": {}}, {"text": "Shazeer, N.;", "metadata": {}}, {"text": "Roberts, A.;", "metadata": {}}, {"text": "Lee, K.;", "metadata": {}}, {"text": "Narang, S.;", "metadata": {}}, {"text": "Matena, M.;", "metadata": {}}, {"text": "Zhou, Y .;", "metadata": {}}, {"text": "Li, W.;", "metadata": {}}, {"text": "and Liu, P.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Mach.", "metadata": {}}, {"text": "Learn.", "metadata": {}}, {"text": "Res., 21: 140:1‚Äì140:67.", "metadata": {}}, {"text": "Rajbhandari, S.;", "metadata": {}}, {"text": "Rasley, J.;", "metadata": {}}, {"text": "Ruwase, O.;", "metadata": {}}, {"text": "and He, Y .", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "ZeRO: Memory Optimization Towards Training A Trillion\nParameter Models.", "metadata": {}}, {"text": "CoRR, abs/1910.02054.", "metadata": {}}, {"text": "Scao, T.", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "Fan, A.;", "metadata": {}}, {"text": "Akiki, C.;", "metadata": {}}, {"text": "Pavlick, E.;", "metadata": {}}, {"text": "Ilic, S.;", "metadata": {}}, {"text": "Hesslow,\nD.;", "metadata": {}}, {"text": "Castagn¬¥e, R.;", "metadata": {}}, {"text": "Luccioni, A.", "metadata": {}}, {"text": "S.;", "metadata": {}}, {"text": "Yvon, F.;", "metadata": {}}, {"text": "Gall¬¥e, M.;", "metadata": {}}, {"text": "Tow,\nJ.;", "metadata": {}}, {"text": "Rush, A.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Biderman, S.;", "metadata": {}}, {"text": "Webson, A.;", "metadata": {}}, {"text": "Ammanamanchi,\nP.", "metadata": {}}, {"text": "S.;", "metadata": {}}, {"text": "Wang, T.;", "metadata": {}}, {"text": "Sagot, B.;", "metadata": {}}, {"text": "Muennighoff, N.;", "metadata": {}}, {"text": "del Moral, A.", "metadata": {}}, {"text": "V .;", "metadata": {}}, {"text": "Ruwase, O.;", "metadata": {}}, {"text": "Bawden, R.;", "metadata": {}}, {"text": "Bekman, S.;", "metadata": {}}, {"text": "McMillan-Major, A.;", "metadata": {}}, {"text": "Beltagy, I.;", "metadata": {}}, {"text": "Nguyen, H.;", "metadata": {}}, {"text": "Saulnier, L.;", "metadata": {}}, {"text": "Tan, S.;", "metadata": {}}, {"text": "Suarez, P.", "metadata": {}}, {"text": "O.;", "metadata": {}}, {"text": "Sanh, V .;", "metadata": {}}, {"text": "Laurenc ¬∏on, H.;", "metadata": {}}, {"text": "Jernite, Y .;", "metadata": {}}, {"text": "Launay, J.;", "metadata": {}}, {"text": "Mitchell, M.;", "metadata": {}}, {"text": "Raffel, C.;", "metadata": {}}, {"text": "Gokaslan, A.;", "metadata": {}}, {"text": "Simhi, A.;", "metadata": {}}, {"text": "Soroa, A.;", "metadata": {}}, {"text": "Aji, A.", "metadata": {}}, {"text": "F.;", "metadata": {}}, {"text": "Al-\nfassy, A.;", "metadata": {}}, {"text": "Rogers, A.;", "metadata": {}}, {"text": "Nitzav, A.", "metadata": {}}, {"text": "K.;", "metadata": {}}, {"text": "Xu, C.;", "metadata": {}}, {"text": "Mou, C.;", "metadata": {}}, {"text": "Emezue,\nC.;", "metadata": {}}, {"text": "Klamm, C.;", "metadata": {}}, {"text": "Leong, C.;", "metadata": {}}, {"text": "van Strien, D.;", "metadata": {}}, {"text": "Adelani, D.", "metadata": {}}, {"text": "I.;", "metadata": {}}, {"text": "and\net al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "BLOOM: A 176B-Parameter Open-Access Mul-\ntilingual Language Model.", "metadata": {}}, {"text": "CoRR, abs/2211.05100.", "metadata": {}}, {"text": "Schwartz, R.;", "metadata": {}}, {"text": "Dodge, J.;", "metadata": {}}, {"text": "Smith, N.", "metadata": {}}, {"text": "A.;", "metadata": {}}, {"text": "and Etzioni, O.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Green ai.", "metadata": {}}, {"text": "Communications of the ACM, 63(12): 54‚Äì63.", "metadata": {}}, {"text": "Shen, S.;", "metadata": {}}, {"text": "Walsh, P.;", "metadata": {}}, {"text": "Keutzer, K.;", "metadata": {}}, {"text": "Dodge, J.;", "metadata": {}}, {"text": "Peters, M.", "metadata": {}}, {"text": "E.;", "metadata": {}}, {"text": "and\nBeltagy, I.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Staged Training for Transformer Language\nModels.", "metadata": {}}, {"text": "In Chaudhuri, K.;", "metadata": {}}, {"text": "Jegelka, S.;", "metadata": {}}, {"text": "Song, L.;", "metadata": {}}, {"text": "Szepesv ¬¥ari,\nC.;", "metadata": {}}, {"text": "Niu, G.;", "metadata": {}}, {"text": "and Sabato, S., eds.,International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA , volume 162 of Proceedings of Machine\nLearning Research, 19893‚Äì19908.", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "Shoeybi, M.;", "metadata": {}}, {"text": "Patwary, M.;", "metadata": {}}, {"text": "Puri, R.;", "metadata": {}}, {"text": "LeGresley, P.;", "metadata": {}}, {"text": "Casper,\nJ.;", "metadata": {}}, {"text": "and Catanzaro, B.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Megatron-LM: Training Multi-\nBillion Parameter Language Models Using Model Paral-\nlelism.", "metadata": {}}, {"text": "CoRR, abs/1909.08053.", "metadata": {}}, {"text": "Srivastava, A.;", "metadata": {}}, {"text": "Rastogi, A.;", "metadata": {}}, {"text": "Rao, A.;", "metadata": {}}, {"text": "Shoeb, A.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Abid,\nA.;", "metadata": {}}, {"text": "Fisch, A.;", "metadata": {}}, {"text": "Brown, A.", "metadata": {}}, {"text": "R.;", "metadata": {}}, {"text": "Santoro, A.;", "metadata": {}}, {"text": "Gupta, A.;", "metadata": {}}, {"text": "Garriga-\nAlonso, A.;", "metadata": {}}, {"text": "et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Beyond the Imitation Game: Quanti-\nfying and extrapolating the capabilities of language models.", "metadata": {}}, {"text": "Transactions on Machine Learning Research.", "metadata": {}}, {"text": "Su, J.;", "metadata": {}}, {"text": "Lu, Y .;", "metadata": {}}, {"text": "Pan, S.;", "metadata": {}}, {"text": "Wen, B.;", "metadata": {}}, {"text": "and Liu, Y .", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Ro-\nFormer: Enhanced Transformer with Rotary Position Em-\nbedding.", "metadata": {}}, {"text": "CoRR, abs/2104.09864.", "metadata": {}}, {"text": "Sun, Y .;", "metadata": {}}, {"text": "Dong, L.;", "metadata": {}}, {"text": "Patra, B.;", "metadata": {}}, {"text": "Ma, S.;", "metadata": {}}, {"text": "Huang, S.;", "metadata": {}}, {"text": "Benhaim,\nA.;", "metadata": {}}, {"text": "Chaudhary, V .;", "metadata": {}}, {"text": "Song, X.;", "metadata": {}}, {"text": "and Wei, F.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "A Length-", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "Extrapolatable Transformer. In Rogers, A.; Boyd-Graber,\nJ. L.; and Okazaki, N., eds., Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023, 14590‚Äì14604. Association for Computational\nLinguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023a. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cu-\ncurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller,\nB.; Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hos-\nseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa,\nM.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poulton,\nA.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva,\nR.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Tay-\nlor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.;\nZhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,\nA.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama\n2: Open Foundation and Fine-Tuned Chat Models. CoRR,\nabs/2307.09288.\nValiant, L. G. 1990. A Bridging Model for Parallel Computa-\ntion. Commun. ACM, 33(8): 103‚Äì111.\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.; Michael,\nJ.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Un-\nderstanding Systems. In Wallach, H. M.; Larochelle, H.;\nBeygelzimer, A.; d‚ÄôAlch¬¥e-Buc, F.; Fox, E. B.; and Garnett,\nR., eds., Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, 3261‚Äì3275.\nWang, Y .; Li, X.; Sun, A.; Meng, X.; Liao, H.; and Guo, J.\n2022a. CofeNet: Context and Former-Label Enhanced Net for\nComplicated Quotation Extraction. In Calzolari, N.; Huang,\nC.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.; Ryu, P.;\nChen, H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue,\nN.; Kim, S.; Hahm, Y .; He, Z.; Lee, T. K.; Santus, E.; Bond,\nF.; and Na, S., eds., Proceedings of the 29th International\nConference on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022, 2438‚Äì\n2449. International Committee on Computational Linguistics.\nWang, Y .; Zhang, H.; Sun, A.; and Meng, X. 2022b. CORT:\nA New Baseline for Comparative Opinion Classification by\nDual Prompts. In Goldberg, Y .; Kozareva, Z.; and Zhang,\nY ., eds.,Findings of the Association for Computational Lin-\nguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, 7064‚Äì7075. Association for Computa-\ntional Linguistics.\nWatkins, C. E.; Campbell, V . L.; Nieberding, R.; and Hall-\nmark, R. 1995. Contemporary practice of psychological as-\nsessment by clinical psychologists. Professional psychology:\nResearch and practice, 26(1): 54.\nWei, J. W.; Hou, L.; Lampinen, A. K.; Chen, X.; Huang,\nD.; Tay, Y .; Chen, X.; Lu, Y .; Zhou, D.; Ma, T.; and Le,\nQ. V . 2023. Symbol tuning improves in-context learning in\nlanguage models. CoRR, abs/2305.08298.\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.;\nVan Merri¬®enboer, B.; Joulin, A.; and Mikolov, T. 2015. To-\nwards ai-complete question answering: A set of prerequisite\ntoy tasks. arXiv preprint arXiv:1502.05698.\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;\nSun, K.; Yu, D.; Yu, C.; Tian, Y .; Dong, Q.; Liu, W.; Shi,\nB.; Cui, Y .; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y .; Pat-\nterson, Y .; Tian, Z.; Zhang, Y .; Zhou, H.; Liu, S.; Zhao, Z.;\nZhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and\nLan, Z. 2020. CLUE: A Chinese Language Understanding\nEvaluation Benchmark. In Scott, D.; Bel, N.; and Zong, C.,\neds., Proceedings of the 28th International Conference on\nComputational Linguistics, COLING 2020, Barcelona, Spain\n(Online), December 8-13, 2020 , 4762‚Äì4772. International\nCommittee on Computational Linguistics.\nYang, G.; and Hu, E. J. 2021. Tensor Programs IV: Feature\nLearning in Infinite-Width Neural Networks. In Meila, M.;\nand Zhang, T., eds., Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of Machine\nLearning Research, 11727‚Äì11737. PMLR.\nYang, G.; Hu, E. J.; Babuschkin, I.; Sidor, S.; Liu, X.; Farhi,\nD.; Ryder, N.; Pachocki, J.; Chen, W.; and Gao, J. 2021. Tun-\ning Large Neural Networks via Zero-Shot Hyperparameter\nTransfer. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.;\nLiang, P.; and Vaughan, J. W., eds.,Advances in Neural In-\nformation Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, 17084‚Äì17097.\nYao, Y .; and Wang, Y . 2023. Research without Re-search:\nMaximal Update Parametrization Yields Accurate Loss Pre-\ndiction across Scales. CoRR, abs/2304.06875.\nYao, Y .; Zhang, Z.; Li, J.; and Wang, Y . 2024. Masked Struc-\ntural Growth for 2x Faster Language Model Pre-training. In\nThe Twelfth International Conference on Learning Represen-\ntations.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. HellaSwag: Can a Machine Really Finish Your\nSentence? In Korhonen, A.; Traum, D. R.; and M`arquez, L.,\neds., Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers, 4791‚Äì4800.\nAssociation for Computational Linguistics.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;\nYang, Z.; Xu, Y .; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.;\nXue, Y .; Zhai, J.; Chen, W.; Liu, Z.; Zhang, P.; Dong, Y .;\nand Tang, J. 2023. GLM-130B: An Open Bilingual Pre-\ntrained Model. In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.", "sentences": [{"text": "Extrapolatable Transformer.", "metadata": {}}, {"text": "In Rogers, A.;", "metadata": {}}, {"text": "Boyd-Graber,\nJ.", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "and Okazaki, N., eds., Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023, 14590‚Äì14604.", "metadata": {}}, {"text": "Association for Computational\nLinguistics.", "metadata": {}}, {"text": "Touvron, H.;", "metadata": {}}, {"text": "Lavril, T.;", "metadata": {}}, {"text": "Izacard, G.;", "metadata": {}}, {"text": "Martinet, X.;", "metadata": {}}, {"text": "Lachaux,\nM.;", "metadata": {}}, {"text": "Lacroix, T.;", "metadata": {}}, {"text": "Rozi`ere, B.;", "metadata": {}}, {"text": "Goyal, N.;", "metadata": {}}, {"text": "Hambro, E.;", "metadata": {}}, {"text": "Azhar,\nF.;", "metadata": {}}, {"text": "Rodriguez, A.;", "metadata": {}}, {"text": "Joulin, A.;", "metadata": {}}, {"text": "Grave, E.;", "metadata": {}}, {"text": "and Lample, G.", "metadata": {}}, {"text": "2023a.", "metadata": {}}, {"text": "LLaMA: Open and Efficient Foundation Language\nModels.", "metadata": {}}, {"text": "CoRR, abs/2302.13971.", "metadata": {}}, {"text": "Touvron, H.;", "metadata": {}}, {"text": "Martin, L.;", "metadata": {}}, {"text": "Stone, K.;", "metadata": {}}, {"text": "Albert, P.;", "metadata": {}}, {"text": "Almahairi, A.;", "metadata": {}}, {"text": "Babaei, Y .;", "metadata": {}}, {"text": "Bashlykov, N.;", "metadata": {}}, {"text": "Batra, S.;", "metadata": {}}, {"text": "Bhargava, P.;", "metadata": {}}, {"text": "Bhosale,\nS.;", "metadata": {}}, {"text": "Bikel, D.;", "metadata": {}}, {"text": "Blecher, L.;", "metadata": {}}, {"text": "Canton-Ferrer, C.;", "metadata": {}}, {"text": "Chen, M.;", "metadata": {}}, {"text": "Cu-\ncurull, G.;", "metadata": {}}, {"text": "Esiobu, D.;", "metadata": {}}, {"text": "Fernandes, J.;", "metadata": {}}, {"text": "Fu, J.;", "metadata": {}}, {"text": "Fu, W.;", "metadata": {}}, {"text": "Fuller,\nB.;", "metadata": {}}, {"text": "Gao, C.;", "metadata": {}}, {"text": "Goswami, V .;", "metadata": {}}, {"text": "Goyal, N.;", "metadata": {}}, {"text": "Hartshorn, A.;", "metadata": {}}, {"text": "Hos-\nseini, S.;", "metadata": {}}, {"text": "Hou, R.;", "metadata": {}}, {"text": "Inan, H.;", "metadata": {}}, {"text": "Kardas, M.;", "metadata": {}}, {"text": "Kerkez, V .;", "metadata": {}}, {"text": "Khabsa,\nM.;", "metadata": {}}, {"text": "Kloumann, I.;", "metadata": {}}, {"text": "Korenev, A.;", "metadata": {}}, {"text": "Koura, P.", "metadata": {}}, {"text": "S.;", "metadata": {}}, {"text": "Lachaux, M.;", "metadata": {}}, {"text": "Lavril, T.;", "metadata": {}}, {"text": "Lee, J.;", "metadata": {}}, {"text": "Liskovich, D.;", "metadata": {}}, {"text": "Lu, Y .;", "metadata": {}}, {"text": "Mao, Y .;", "metadata": {}}, {"text": "Martinet,\nX.;", "metadata": {}}, {"text": "Mihaylov, T.;", "metadata": {}}, {"text": "Mishra, P.;", "metadata": {}}, {"text": "Molybog, I.;", "metadata": {}}, {"text": "Nie, Y .;", "metadata": {}}, {"text": "Poulton,\nA.;", "metadata": {}}, {"text": "Reizenstein, J.;", "metadata": {}}, {"text": "Rungta, R.;", "metadata": {}}, {"text": "Saladi, K.;", "metadata": {}}, {"text": "Schelten, A.;", "metadata": {}}, {"text": "Silva,\nR.;", "metadata": {}}, {"text": "Smith, E.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Subramanian, R.;", "metadata": {}}, {"text": "Tan, X.", "metadata": {}}, {"text": "E.;", "metadata": {}}, {"text": "Tang, B.;", "metadata": {}}, {"text": "Tay-\nlor, R.;", "metadata": {}}, {"text": "Williams, A.;", "metadata": {}}, {"text": "Kuan, J.", "metadata": {}}, {"text": "X.;", "metadata": {}}, {"text": "Xu, P.;", "metadata": {}}, {"text": "Yan, Z.;", "metadata": {}}, {"text": "Zarov, I.;", "metadata": {}}, {"text": "Zhang, Y .;", "metadata": {}}, {"text": "Fan, A.;", "metadata": {}}, {"text": "Kambadur, M.;", "metadata": {}}, {"text": "Narang, S.;", "metadata": {}}, {"text": "Rodriguez,\nA.;", "metadata": {}}, {"text": "Stojnic, R.;", "metadata": {}}, {"text": "Edunov, S.;", "metadata": {}}, {"text": "and Scialom, T.", "metadata": {}}, {"text": "2023b.", "metadata": {}}, {"text": "Llama\n2: Open Foundation and Fine-Tuned Chat Models.", "metadata": {}}, {"text": "CoRR,\nabs/2307.09288.", "metadata": {}}, {"text": "Valiant, L.", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "1990.", "metadata": {}}, {"text": "A Bridging Model for Parallel Computa-\ntion.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM, 33(8): 103‚Äì111.", "metadata": {}}, {"text": "Wang, A.;", "metadata": {}}, {"text": "Pruksachatkun, Y .;", "metadata": {}}, {"text": "Nangia, N.;", "metadata": {}}, {"text": "Singh, A.;", "metadata": {}}, {"text": "Michael,\nJ.;", "metadata": {}}, {"text": "Hill, F.;", "metadata": {}}, {"text": "Levy, O.;", "metadata": {}}, {"text": "and Bowman, S.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Un-\nderstanding Systems.", "metadata": {}}, {"text": "In Wallach, H.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Larochelle, H.;", "metadata": {}}, {"text": "Beygelzimer, A.;", "metadata": {}}, {"text": "d‚ÄôAlch¬¥e-Buc, F.;", "metadata": {}}, {"text": "Fox, E.", "metadata": {}}, {"text": "B.;", "metadata": {}}, {"text": "and Garnett,\nR., eds., Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, 3261‚Äì3275.", "metadata": {}}, {"text": "Wang, Y .;", "metadata": {}}, {"text": "Li, X.;", "metadata": {}}, {"text": "Sun, A.;", "metadata": {}}, {"text": "Meng, X.;", "metadata": {}}, {"text": "Liao, H.;", "metadata": {}}, {"text": "and Guo, J.", "metadata": {}}, {"text": "2022a.", "metadata": {}}, {"text": "CofeNet: Context and Former-Label Enhanced Net for\nComplicated Quotation Extraction.", "metadata": {}}, {"text": "In Calzolari, N.;", "metadata": {}}, {"text": "Huang,\nC.;", "metadata": {}}, {"text": "Kim, H.;", "metadata": {}}, {"text": "Pustejovsky, J.;", "metadata": {}}, {"text": "Wanner, L.;", "metadata": {}}, {"text": "Choi, K.;", "metadata": {}}, {"text": "Ryu, P.;", "metadata": {}}, {"text": "Chen, H.;", "metadata": {}}, {"text": "Donatelli, L.;", "metadata": {}}, {"text": "Ji, H.;", "metadata": {}}, {"text": "Kurohashi, S.;", "metadata": {}}, {"text": "Paggio, P.;", "metadata": {}}, {"text": "Xue,\nN.;", "metadata": {}}, {"text": "Kim, S.;", "metadata": {}}, {"text": "Hahm, Y .;", "metadata": {}}, {"text": "He, Z.;", "metadata": {}}, {"text": "Lee, T.", "metadata": {}}, {"text": "K.;", "metadata": {}}, {"text": "Santus, E.;", "metadata": {}}, {"text": "Bond,\nF.;", "metadata": {}}, {"text": "and Na, S., eds., Proceedings of the 29th International\nConference on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022, 2438‚Äì\n2449.", "metadata": {}}, {"text": "International Committee on Computational Linguistics.", "metadata": {}}, {"text": "Wang, Y .;", "metadata": {}}, {"text": "Zhang, H.;", "metadata": {}}, {"text": "Sun, A.;", "metadata": {}}, {"text": "and Meng, X.", "metadata": {}}, {"text": "2022b.", "metadata": {}}, {"text": "CORT:\nA New Baseline for Comparative Opinion Classification by\nDual Prompts.", "metadata": {}}, {"text": "In Goldberg, Y .;", "metadata": {}}, {"text": "Kozareva, Z.;", "metadata": {}}, {"text": "and Zhang,\nY ., eds.,Findings of the Association for Computational Lin-\nguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, 7064‚Äì7075.", "metadata": {}}, {"text": "Association for Computa-\ntional Linguistics.", "metadata": {}}, {"text": "Watkins, C.", "metadata": {}}, {"text": "E.;", "metadata": {}}, {"text": "Campbell, V .", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "Nieberding, R.;", "metadata": {}}, {"text": "and Hall-\nmark, R.", "metadata": {}}, {"text": "1995.", "metadata": {}}, {"text": "Contemporary practice of psychological as-\nsessment by clinical psychologists.", "metadata": {}}, {"text": "Professional psychology:\nResearch and practice, 26(1): 54.", "metadata": {}}, {"text": "Wei, J.", "metadata": {}}, {"text": "W.;", "metadata": {}}, {"text": "Hou, L.;", "metadata": {}}, {"text": "Lampinen, A.", "metadata": {}}, {"text": "K.;", "metadata": {}}, {"text": "Chen, X.;", "metadata": {}}, {"text": "Huang,\nD.;", "metadata": {}}, {"text": "Tay, Y .;", "metadata": {}}, {"text": "Chen, X.;", "metadata": {}}, {"text": "Lu, Y .;", "metadata": {}}, {"text": "Zhou, D.;", "metadata": {}}, {"text": "Ma, T.;", "metadata": {}}, {"text": "and Le,\nQ.", "metadata": {}}, {"text": "V .", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Symbol tuning improves in-context learning in\nlanguage models.", "metadata": {}}, {"text": "CoRR, abs/2305.08298.", "metadata": {}}, {"text": "Weston, J.;", "metadata": {}}, {"text": "Bordes, A.;", "metadata": {}}, {"text": "Chopra, S.;", "metadata": {}}, {"text": "Rush, A.", "metadata": {}}, {"text": "M.;", "metadata": {}}, {"text": "Van Merri¬®enboer, B.;", "metadata": {}}, {"text": "Joulin, A.;", "metadata": {}}, {"text": "and Mikolov, T.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "To-\nwards ai-complete question answering: A set of prerequisite\ntoy tasks.", "metadata": {}}, {"text": "arXiv preprint arXiv:1502.05698.", "metadata": {}}, {"text": "Xu, L.;", "metadata": {}}, {"text": "Hu, H.;", "metadata": {}}, {"text": "Zhang, X.;", "metadata": {}}, {"text": "Li, L.;", "metadata": {}}, {"text": "Cao, C.;", "metadata": {}}, {"text": "Li, Y .;", "metadata": {}}, {"text": "Xu, Y .;", "metadata": {}}, {"text": "Sun, K.;", "metadata": {}}, {"text": "Yu, D.;", "metadata": {}}, {"text": "Yu, C.;", "metadata": {}}, {"text": "Tian, Y .;", "metadata": {}}, {"text": "Dong, Q.;", "metadata": {}}, {"text": "Liu, W.;", "metadata": {}}, {"text": "Shi,\nB.;", "metadata": {}}, {"text": "Cui, Y .;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "Zeng, J.;", "metadata": {}}, {"text": "Wang, R.;", "metadata": {}}, {"text": "Xie, W.;", "metadata": {}}, {"text": "Li, Y .;", "metadata": {}}, {"text": "Pat-\nterson, Y .;", "metadata": {}}, {"text": "Tian, Z.;", "metadata": {}}, {"text": "Zhang, Y .;", "metadata": {}}, {"text": "Zhou, H.;", "metadata": {}}, {"text": "Liu, S.;", "metadata": {}}, {"text": "Zhao, Z.;", "metadata": {}}, {"text": "Zhao, Q.;", "metadata": {}}, {"text": "Yue, C.;", "metadata": {}}, {"text": "Zhang, X.;", "metadata": {}}, {"text": "Yang, Z.;", "metadata": {}}, {"text": "Richardson, K.;", "metadata": {}}, {"text": "and\nLan, Z.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "CLUE: A Chinese Language Understanding\nEvaluation Benchmark.", "metadata": {}}, {"text": "In Scott, D.;", "metadata": {}}, {"text": "Bel, N.;", "metadata": {}}, {"text": "and Zong, C.,\neds., Proceedings of the 28th International Conference on\nComputational Linguistics, COLING 2020, Barcelona, Spain\n(Online), December 8-13, 2020 , 4762‚Äì4772.", "metadata": {}}, {"text": "International\nCommittee on Computational Linguistics.", "metadata": {}}, {"text": "Yang, G.;", "metadata": {}}, {"text": "and Hu, E.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Tensor Programs IV: Feature\nLearning in Infinite-Width Neural Networks.", "metadata": {}}, {"text": "In Meila, M.;", "metadata": {}}, {"text": "and Zhang, T., eds., Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of Machine\nLearning Research, 11727‚Äì11737.", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "Yang, G.;", "metadata": {}}, {"text": "Hu, E.", "metadata": {}}, {"text": "J.;", "metadata": {}}, {"text": "Babuschkin, I.;", "metadata": {}}, {"text": "Sidor, S.;", "metadata": {}}, {"text": "Liu, X.;", "metadata": {}}, {"text": "Farhi,\nD.;", "metadata": {}}, {"text": "Ryder, N.;", "metadata": {}}, {"text": "Pachocki, J.;", "metadata": {}}, {"text": "Chen, W.;", "metadata": {}}, {"text": "and Gao, J.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Tun-\ning Large Neural Networks via Zero-Shot Hyperparameter\nTransfer.", "metadata": {}}, {"text": "In Ranzato, M.;", "metadata": {}}, {"text": "Beygelzimer, A.;", "metadata": {}}, {"text": "Dauphin, Y .", "metadata": {}}, {"text": "N.;", "metadata": {}}, {"text": "Liang, P.;", "metadata": {}}, {"text": "and Vaughan, J.", "metadata": {}}, {"text": "W., eds.,Advances in Neural In-\nformation Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, 17084‚Äì17097.", "metadata": {}}, {"text": "Yao, Y .;", "metadata": {}}, {"text": "and Wang, Y .", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Research without Re-search:\nMaximal Update Parametrization Yields Accurate Loss Pre-\ndiction across Scales.", "metadata": {}}, {"text": "CoRR, abs/2304.06875.", "metadata": {}}, {"text": "Yao, Y .;", "metadata": {}}, {"text": "Zhang, Z.;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "and Wang, Y .", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Masked Struc-\ntural Growth for 2x Faster Language Model Pre-training.", "metadata": {}}, {"text": "In\nThe Twelfth International Conference on Learning Represen-\ntations.", "metadata": {}}, {"text": "Zellers, R.;", "metadata": {}}, {"text": "Holtzman, A.;", "metadata": {}}, {"text": "Bisk, Y .;", "metadata": {}}, {"text": "Farhadi, A.;", "metadata": {}}, {"text": "and Choi,\nY .", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "HellaSwag: Can a Machine Really Finish Your\nSentence?", "metadata": {}}, {"text": "In Korhonen, A.;", "metadata": {}}, {"text": "Traum, D.", "metadata": {}}, {"text": "R.;", "metadata": {}}, {"text": "and M`arquez, L.,\neds., Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers, 4791‚Äì4800.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "Zeng, A.;", "metadata": {}}, {"text": "Liu, X.;", "metadata": {}}, {"text": "Du, Z.;", "metadata": {}}, {"text": "Wang, Z.;", "metadata": {}}, {"text": "Lai, H.;", "metadata": {}}, {"text": "Ding, M.;", "metadata": {}}, {"text": "Yang, Z.;", "metadata": {}}, {"text": "Xu, Y .;", "metadata": {}}, {"text": "Zheng, W.;", "metadata": {}}, {"text": "Xia, X.;", "metadata": {}}, {"text": "Tam, W.", "metadata": {}}, {"text": "L.;", "metadata": {}}, {"text": "Ma, Z.;", "metadata": {}}, {"text": "Xue, Y .;", "metadata": {}}, {"text": "Zhai, J.;", "metadata": {}}, {"text": "Chen, W.;", "metadata": {}}, {"text": "Liu, Z.;", "metadata": {}}, {"text": "Zhang, P.;", "metadata": {}}, {"text": "Dong, Y .;", "metadata": {}}, {"text": "and Tang, J.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "GLM-130B: An Open Bilingual Pre-\ntrained Model.", "metadata": {}}, {"text": "In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023.", "metadata": {}}, {"text": "OpenReview.net.", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.; and Wen, J. 2023. A Survey of Large\nLanguage Models. CoRR, abs/2303.18223.", "sentences": [{"text": "Zhao, W.", "metadata": {}}, {"text": "X.;", "metadata": {}}, {"text": "Zhou, K.;", "metadata": {}}, {"text": "Li, J.;", "metadata": {}}, {"text": "Tang, T.;", "metadata": {}}, {"text": "Wang, X.;", "metadata": {}}, {"text": "Hou, Y .;", "metadata": {}}, {"text": "Min, Y .;", "metadata": {}}, {"text": "Zhang, B.;", "metadata": {}}, {"text": "Zhang, J.;", "metadata": {}}, {"text": "Dong, Z.;", "metadata": {}}, {"text": "Du, Y .;", "metadata": {}}, {"text": "Yang, C.;", "metadata": {}}, {"text": "Chen, Y .;", "metadata": {}}, {"text": "Chen, Z.;", "metadata": {}}, {"text": "Jiang, J.;", "metadata": {}}, {"text": "Ren, R.;", "metadata": {}}, {"text": "Li, Y .;", "metadata": {}}, {"text": "Tang, X.;", "metadata": {}}, {"text": "Liu,\nZ.;", "metadata": {}}, {"text": "Liu, P.;", "metadata": {}}, {"text": "Nie, J.;", "metadata": {}}, {"text": "and Wen, J.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "A Survey of Large\nLanguage Models.", "metadata": {}}, {"text": "CoRR, abs/2303.18223.", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}]}