{"document_id": "xia2024", "title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning", "text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract—Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI’s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n• Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024\n\nFig. 1. LLM model overview. We evaluate accuracy, throughput, runtime,\nand GPU characterization for different models, input datasets, and fine-\ntuning sparsity. The different colored expert boxes in MoE layer means\ndifferent sets of experts are activated according to the input token.\n• A detailed accuracy and runtime performance analysis\nto understand the LLM fine-tuning workload behavior.\n• Design and validation of an analytical model to estimate\nthe cost of LLM fine-tuning in the cloud.\nII. B ACKGROUND\nA. LLM and Finetuning\nThe decoder-only Transformer is designed to handle tasks\nwhere the output generation depends solely on the preceding\ntokens, making it particularly suited for auto-regressive tasks\nsuch as language modeling and text generation [9]. In the\nclassic decoder-only Transformer design, multiple decoder\nlayers are connected in sequence. Each decoder layer consists\nof a self-attention block followed by a feed-forward network\n(FFN). Fig. 1 presents an overview of the decoder-only\nTransformer model with a Mixture-of-Experts (MoE) design.\nIn this model, the FFN layers are divided into several smaller\nFFNs, referred to as experts, which are sparsely activated\nby a gating mechanism. The self-attention block can also\nbe replaced with a Mamba layer to improve performance in\nsequence modeling (a model known as state-space model).\nLLMs like GPT [10], [11], LLaMA [3], Claude [12], Mis-\ntral [13] have demonstrated their ability to excel in many\nnatural language processing (NLP) tasks Training an LLM\nmodel from scratch requires a large amount of hardware\nresources and budget.\nFine-tuning LLMs allows organizations to harness the full\npotential of advanced AI systems by tailoring them to specific\ntasks and domains. This customization involves training the\nmodel on domain-specific data, enabling it to understand\nand generate content that aligns closely with the unique\nneeds of the users. For instance, in the healthcare sector,\na fine-tuned LLM can assist in diagnosing conditions by\ninterpreting patient data and medical literature with high\nprecision. Another attractive feature of fine-tuning LLMs is\nthat it can be achieved at a cost-efficient manner. While pre-\ntraining LLMs require thousands of GPU hours, fine-tuning\ncan be achieved using a handful of GPUs in a relatively short\nTABLE I\nLLM M ODELS\n#params Mem consump. #layers #MoE layer\nMixtral 47B 23.35GB 32 8\nBlackMamba 2.8B 5.6GB 18 8\nTABLE II\nDATASETS\n#queries m. seq len type\nCommonsense 15K (CS) 15K 79 Common Sense\nMath 14K (MATH) 14K 174 Math\nHellaswag (HE) 10K 272 Common Sense\nGSM8K (GS) 1.3K 148 Math\namount of time [6]. This work uses case study of mathematics\nand common-sense question-answer datasets to demonstrate\nthe fine-tuning process of LLMs.\nB. LoRA\nLow-Rank Adaption (LoRA) is a technique that freezes\nthe pre-trained model weights and injects trainable rank de-\ncomposition into layers of the transformer architecture [14].\nLoRA significantly reduces the number of parameters,\nthereby decreasing the GPU memory footprint. LoRA can\nbe used independently of the aforementioned fine-tuning\ntechniques. In this work, we apply QLoRA [15] to the\nMixtral-8x7B model [4]; more details are provided in §III.\nC. Mixture of Experts (MoE)\nThe quality of an LLM is highly related to its scale. Given\na fixed computation budget, it is often desirable to train\na model with more parameters to achieve higher accuracy.\nMixture-of-Experts (MoE) is a technique that, instead of\nusing one large model for all tasks, combines multiple\nexpert sub-networks into a single, large model. As shown\nin Fig. 1, with MoE, different sets of experts are selectively\nactivated for different tokens. This approach can significantly\nreduce the amount of computation required for both training\nand inference, enabling the scaling up of model size and\nachieving better model accuracy [16].\nIII. E XPERIMENTAL SETUP\nModels. We fine-tune two pre-trained MoE models,\nMixtral-8x7B (Mixtral for short) [4] and BlackMamba-\n630M/2.8B (BlackMamba for short) [8]. The details of these\nmodels are shown in Table I. Both models incorporate eight\nexperts in their MoE layers. For dense fine-tuning, all experts\nare activated, whereas for sparse fine-tuning, only the top two\nexperts are selected for each token.\nThese models differ significantly in their transformer archi-\ntectures and sizes. Mixtral is a conventional MoE transformer\nmodel with a total of 47 billion parameters. In contrast,\nBlackMamba is a state-space model that replaces all at-\ntention layers with mamba layers and has only 2.8 billion\n\n[Image page=2 idx=1 name=Im1.png] Size: 1156x608, Data: 68592 bytes\n\nparameters. We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget. For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16. We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\ngradient checkpointing [18] to save memory usage.\nDatasets. Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]). The details of datasets\nare used in Table II. For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning. Each dataset consists of thousands\nof queries. We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.\nProfiling experiments. We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives. The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning. Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown. The hardware evaluation offers a detailed\nanalysis of GPU performance. Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency. These\nmetrics collectively offer a comprehensive overview of the\nmodels’ performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization. Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.\nIn contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata. Because profiling is time-consuming (approximately\n10,000× costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.\nWe present the sequence length distribution for the CS and\nMATH datasets in Fig. 2. The median sequence length is 79\nfor CS and 174 for MATH. Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect. We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.\nGPU platform. Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment. Therefore, we focus on fine-tuning these models on a\nsingle GPU. Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig. 2. Sequence length distribution for evaluated datasets.\n0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig. 3. Testing accuracy of Mixtral and BlackMamba. Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.\nNVIDIA A40 GPU with Ampere architecture. The GPU has\n48GB memory. While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory. We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\nIV. C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.\nA. Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels. Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and\n\nunder 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in §III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin §V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n§III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than\n\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (μs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n\n[Image page=5 idx=1 name=Im7.png] Size: 2368x1744, Data: 122833 bytes\n\nMixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 × when increasing the batch\nsize from 1 to 2, but only by 4.8 × when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n§IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is\n\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (§III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in §IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU’s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in §IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers’ memory\nutilization is batch-size-independent, while matmul layers’\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-\n\nFig. 12. Pseudo code for MoE layers.\nciency [27]. During the training process of MoE models,\neach token is dynamically assigned to the top-k experts based\non routing scores. This strategy often leads to most tokens\nbeing assigned to a small number of experts, resulting in load\nimbalance and slower training. Additionally, some experts\nreceive insufficient training, which degrades overall model\nperformance [28]. A na¨ıve approach to address this imbalance\nis to use token dropping and padding to ensure that the\nnumber of tokens assigned to each expert is equal [29].\nHowever, this method sacrifices model quality or leads to\nwasted computation. In this section, we analyze how fine-\ntuning influences the token distribution among experts. We\ncompare the token distribution of Mixtral and BlackMamba\nbefore and after fine-tuning to understand the impact of this\nprocess.\nWe extract 1,000 examples from both the CS and MATH\ndatasets to test the original models without tuning and the\nmodels after 10 epochs of tuning on these datasets. Fig. 12\nprovides the pseudo code for MoE layers with top-k gating.\nIn this process, the hidden states are first sent to the router\nof the MoE layer, which generates router logits. These logits\ndetermine the priority of each expert for each token. Based on\nthe router score for each token, tokens are grouped together\nand sent to their assigned experts. This top-k routing strategy\ncan lead to load imbalance if the model has not been pre-\ntrained for balance.\nFig. 11 evidently shows that fine-tuning causes load im-\nbalance in Mixtral for both datasets. Comparing variance\nbefore and after fine-tuning (e.g., HE vs. HE tuned), the\ntoken assignment variance increased from 55 to 112 for\nCS and from 21 to 79 for GS. Expert 3 became the\nmost frequently used and important expert post fine-tuning.\nConversely, there is a decrease in the variance of token\ndistribution for BlackMamba on the CS dataset, dropping\nfrom 150 to 93. For the GS dataset, the token distribution\nvariance for BlackMamba remains almost unchanged after\nfine-tuning. This suggests that load-imbalance has a less\ndisruptive impact on fine-tuning for BlackMamba compared\nto Mixtral. From Fig. 11, we can also observe that Mixtral\ndemonstrates better load balance in both tasks compared to\nBlackMamba, despite the increased load imbalance after fine-\ntuning. The increased level of imbalance after fine-tuning\nsuggests GPU load balancing techniques can be helpful.\nBoth single GPU load balancing [30] and multi-GPU load\nbalancing [31] have been proposed to address this issue.\nTakeaway 6. The effect of fine-tuning on expert\nload imbalance in the MoE layer is LLM model and\ndataset dependent.\n6) Sensitivity Study on Sequence Length: To further ana-\nlyze the effect of sequence length on the fine-tuning process,\nwe chose the batch size that would maximize the memory\nfor each sequence length (64, 128, 256, 512, and 1024) and\ncompared the latency, SM utilization, and DRAM utiliza-\ntion. Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively. This is due to the varying maximum batch sizes\nsupported by each sequence length, resulting in a similar\nnumber of tokens in each batch. Because latency remains\nconsistent with increasing sequence length and we can use\nlarger batch sizes, throughput is higher for shorter sequences.\nV. A NALYTICAL MODEL TO ESTIMATE THE COST OF\nFINE -T UNING LLM S\nWhile training LLMs from scratch is a cost-prohibitive\nprocess, fine-tuning LLMs offers an attractive solution to\nalign LLMs to desired behaviors. One such example is fine-\ntuning LLMs to a domain-specific use-cases, for example, to\nanswer math questions. §IV-A shows that it is possible to\nfine-tune pre-trained LLMs on domain-specific tasks to sig-\nnificant improve accuracy. While this is a desired approach,\ncurrently, no model exists that can predict the cost of fine-\ntuning LLMs.\nFine-tuning LLMs is complex, influenced by factors like\nmodel size, GPU memory, dataset sequence length, and MoE\nsparsity, all affecting batch size and throughput. By integrat-\ning these factors with GPU costs, we can identify the most\ncost-efficient GPU for pre-tuning tasks. This section presents\nan analytical model based on previous characterization.\nThis model estimates cloud-based fine-tuning costs for a\ngiven dataset and LLM. Developed from previous sections,\nit can be adapted for other LLMs by adjusting parameters. It\nassumes using the maximum batch size supported by GPU\nmemory to optimize cost. We first estimate this batch size,\nthen use it to evaluate throughput and fine-tuning costs.\nA. Estimating Maximum Batch Size\nThe maximum batch size is the maximum number of\nqueries that can fit in GPU memory at once. Our analytical\nmodel for maximum batch size is shown in (1).\nM ax BSZ = ⌊C0∗ GP U mem − model mem\nseq len ∗ ((1 − C1) + C1 ∗ sparsity) ⌋\n(1)\nIntuitively, larger GPU memory allows for higher batch\nsizes. In the meantime, the LLM model will take up a certain\n\n[Image page=8 idx=1 name=Im12.png] Size: 1598x352, Data: 191103 bytes\n\n0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(§III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, §V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in §IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 ∗ log( batch size\nsparsity ∗ C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU\n\n0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (§IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]–[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]–[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n\nREFERENCES\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,\nSebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou,\nDonald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large\nlanguage models, 2022.\n[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo\nFan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen\nChu. Dissecting the runtime performance of the training, fine-tuning,\nand inference of large language models, 2023.\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models, 2023.\n[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Men-\nsch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\nGuillaume Bour, Guillaume Lample, L ´elio Renard Lavaud, Lucile\nSaulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao, Th ´eophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth ´ee Lacroix, and William El\nSayed. Mixtral of experts, 2024.\n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,\nWilliam Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Sid-\ndhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-\nRos, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang,\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling\ninstruction-finetuned language models, 2022.\n[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling\ndown to scale up: A guide to parameter-efficient fine-tuning, 2023.\n[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi\nYang. Parameter-efficient fine-tuning design spaces, 2023.\n[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge.\nBlackmamba: Mixture of experts for state-space models, 2024.\n[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\nImproving language understanding by generative pre-training. 2018.\n[10] Introducing chatgpt. https://openai.com/index/chatgpt.\n[11] Josh Achiam et.al. Gpt-4 technical report, 2024.\n[12] Introducing the next generation of claude. https://www.anthropic.com/\nnews/claude-3-family.\n[13] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile Saulnier, L ´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth ´ee Lacroix, and William El Sayed.\nMistral 7b, 2023.\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank\nadaptation of large language models, 2021.\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\nQlora: Efficient finetuning of quantized llms, 2023.\n[16] Amin Vahdat. Societal infrastructure in the age of artificial general\nintelligence. ASPLOS 2024 Keynote , 2024.\n[17] Tri Dao. Flashattention-2: Faster attention with better parallelism and\nwork partitioning, 2023.\n[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training\ndeep nets with sublinear memory cost, 2016.\n[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan\nLuo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning\nof 100+ language models. arXiv preprint arXiv:2403.13372 , 2024.\n[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong\nBing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters:\nAn adapter family for parameter-efficient fine-tuning of large language\nmodels, 2023.\n[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,\nHeewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob\nHilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. Hellaswag: Can a machine really finish your sentence? In\nProceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2019.\n[23] Nvidia nsight compute. https://developer.nvidia.com/nsight-compute.\n[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and\nWenpeng Yin. Large language models for mathematical reasoning:\nProgresses and challenges, 2024.\n[25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with\nselective state spaces, 2024.\n[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You.\nOne student knows all experts know: From sparse to dense, 2022.\n[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu,\nZilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng\nCheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive\nmixture-of-experts at scale. CoRR, abs/2206.03382, June 2022.\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,\nQuoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer, 2017.\n[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen,\nOrhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and\nZhifeng Chen. Gshard: Scaling giant models with conditional compu-\ntation and automatic sharding, 2020.\n[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R. Gao.\nDynamic load balancing on single- and multi-gpu systems. In 2010\nIEEE International Symposium on Parallel & Distributed Processing\n(IPDPS), pages 1–12, 2010.\n[31] Mohamed Wahib, Muhammet Abdullah Soyt ¨urk, and Didem Unat.\nElastic load balancing for dynamic LLMs, 2024.\n[32] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland,\nTyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson,\nWarren Weckesser, Jonathan Bright, St ´efan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew\nR. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan\nPolat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\nCharles R. Harris, Anne M. Archibald, Ant ˆonio H. Ribeiro, Fabian\nPedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python. Nature\nMethods, 17:261–272, 2020.\n[33] CUDO compute: https://www.cudocompute.com.\n[34] Amazon AWS: https://aws.amazon.com.\n[35] Lambda: https://www.gpus.com.\n[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agar-\nwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning\nfrom complex explanation traces of gpt-4, 2023.\n[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-\nMageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled\nmodels from large-scale instructions, 2024.\n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank\nadaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,\nQuentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and\nSylvain Gelly. Parameter-efficient transfer learning for nlp. In Inter-\nnational conference on machine learning , pages 2790–2799. PMLR,\n2019.\n[40] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng\nTao. SparseAdapter: An easy approach for improving the parameter-\nefficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and\nYue Zhang, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2184–2190, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics.\n[41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-\nChiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora:\nWeight-decomposed low-rank adaptation, 2024.\n\n[42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima\nAnandkumar, and Yuandong Tian. Galore: Memory-efficient llm\ntraining by gradient low-rank projection, 2024.\n[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen\nHuang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang,\nand Fuzhen Zhuang. Mora: High-rank updating for parameter-efficient\nfine-tuning, 2024.\n[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan\nZhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense\ntraining, sparse inference: Rethinking training of mixture-of-experts\nlanguage models. arXiv preprint arXiv:2404.05567 , 2024.\n[45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen,\nYuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang\nZhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe:\nAlleviate world knowledge forgetting in large language models via\nmoe-style plugin, 2024.\n[46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent\nZhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon.\nMixture-of-experts with expert choice routing, 2022.\n[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao,\nDeli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y . Wu, Zhenda\nXie, Y . K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang. Deepseekmoe: Towards ultimate expert specialization\nin mixture-of-experts language models, 2024.\n[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta,\nSeyeon Kim, Yongin Kwon, and Sangtae Ha. Llmem: Estimating gpu\nmemory usage for fine-tuning pre-trained llms, 2024.\n\nAPPENDIX\nA. Abstract\nThis artifact reproduces the results presented in the Char-\nacterization Study. It includes a detailed three-level runtime\nbreakdown, analysis of SM and MEM utilization, and a\ncomprehensive study of throughput.\nB. Artifact check-list (meta-information)\n• Compilation: PyTorch\n• Model: Mixtral-8x7B and BlackMamba-630M/2.8B\n• Data set: Hellaswag, GSM8k, MATH 14k and common-\nsense 15k (provided in GitHub reopsitory)\n• Run-time environment: Ubuntu 20.04.6\n• Hardware: NVIDIA A40 (48GB) GPU\n• Output: Nsight Compute\n• Experiments: Fine-tune both models using different batch\nsizes and conduct a GPU characterization study\n• How much disk space required (approximately)?: 100GB\n• How much time is needed to prepare workflow (approxi-\nmately)?: 1 hour\n• How much time is needed to complete experiments (ap-\nproximately)?: Throughput and Runtime Breakdown experi-\nments can be completed within 2 hours, while Nsight Compute\nprofiling for SM and MEM utilization will take approximately\n80 hours\n• Publicly available?: Yes\n• Workflow framework used?: LLaMA-Factory\nC. Description\n1) How to access: Our source code can be found at\nhttps://github.com/stsxxx/finetune\n2) Hardware dependencies:\n• We conducted all experiments on a server equipped with\nan Intel® Xeon® Platinum 8380 CPU @ 2.30GHz and\nan NVIDIA A40 (48GB) GPU\n• Supported GPUs should have at least 48GB of memory\nand feature an Ampere architecture or newer\n3) Software dependencies:\n• A recent Linux release\n• Python 3.8.10\n• CUDA 11.8\n• PyTorch 2.1.0 compatible with CUDA 11.8\n• CUDA toolkit 11.8\n4) Data sets: Hellaswag, GSM8k, MATH 14k and com-\nmonsense 15k. We provide all of them in our GitHub repos-\nitory.\n5) Models: Mixtral-8x7B and BlackMamba-\n630M/2.8B. We provide the python script to download\nthem from Huggingface. Mixtral-8x7B is a gated\nmodel, access request should be submitted here\nhttps://huggingface.co/mistralai/Mixtral-8x7B-v0.1.\nD. Installation\nFor the Python environment, simply clone our repository\nand use conda to set up a new environment by running the\nfollowing command:\n#create a new conda environment\nconda create --name=ft python=3.8\nconda activate ft\n#install pytorch2.1.0+cu118\nconda install pytorch==2.1.0 \\\ntorchvision==0.16.0 torchaudio==2.1.0 \\\npytorch-cuda=11.8 -c pytorch -c nvidia\n#download the source code\ngit clone https://github.com/stsxxx\n/finetune.git\ncd finetune\n#install all other dependencies\npip install -r requirements.txt\nE. Experiment workflow\nFirst make sure the working directory is the LLaMA-\nFactory directory:\ncd LLaMA-Factory\nBefore running experiments, you should download both\ntwo models from Huggingface:\n#Add Blackmamba directory to your pythonpath\nexport PYTHONPATH=$PYTHONPATH:../BlackMamba\n#specify where you want to store models\nexport HF_HOME=\"path\"\n#download models, huggingface access token\nshould be entered in the terminal\npython3 model_download.py\nMake sure you change the transformers library path and\nmodel config file path before running each experiment bash\nscript, you can find an example in the README file:\n# change it to your transformers library path\ntransformers_path=\"xxxxx\"\n# change it to your model config path\nconfig_file_path=\"xxxxx\"\nTo reproduce the fine-tuning throughput results shown in\nFig. 8, you can run the following scripts:\n./mixtral_tp.sh\npython3 throughput.py ./profile_data/mixtral\n/throughput > mixtral_throughput.txt\n./mamba_tp.sh\npython3 throughput.py ./profile_data\n/blackmamba/throughput > mamba_throughput.txt\n\nHigh-level and layer-level latency breakdown results\nshown in Fig. 4 and 5 can be obtained by running:\n./mixtral_lt.sh\npython3 mixtral_latency.py ./profile_data\n/mixtral/latency > mixtral_latency_breakdown.txt\n./mamba_lt.sh\npython3 mamba_latency.py ./profile_data\n/blackmamba/latency > mamba_latency_breakdown.txt\nYou can also use Nsight Compute to profile and generate\nkernel-level latency breakdown, SM and MEM utilization\nresults shown in Fig. 6, 9 and 10 by running:\n./mixtral_pf.sh\npython3 sm_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_sm.txt\npython3 mem_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_mem.txt\n./mamba_pf.sh\npython3 sm_mamba.py ./profile_data/blackmamba\n/ncu > mamba_sm.txt\npython3 mem_mamba.py ./profile_data/blackmamba\n/ncu > mamba_mem.txt\npython3 sm_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_sm_backward.txt\npython3 mem_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_mem_backward.txt\nF . Evaluation and expected results\nThe generated results are stored in specific text files as\nindicated in the commands above, such as mixtral sm.txt for\nSM utilization data of the Mixtral model.\nG. Experiment customization\nCustomized experiments can be conducted with varying\nbatch sizes and query sequence lengths, both of which can\nbe adjusted in each bash script.\nH. Methodology\nSubmission, reviewing and badging methodology:\n• https://www.acm.org/publications/policies/\nartifact-review-and-badging-current\n• https://cTuning.org/ae", "metadata": {"url": "https://arxiv.org/pdf/2408.04693", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract—Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI’s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n• Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024", "sentences": [{"text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract—Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner.", "metadata": {}}, {"text": "In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU.", "metadata": {}}, {"text": "Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.", "metadata": {}}, {"text": "Our study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.", "metadata": {}}, {"text": "Using our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud.", "metadata": {}}, {"text": "This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.", "metadata": {}}, {"text": "I.", "metadata": {}}, {"text": "I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1].", "metadata": {}}, {"text": "Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training.", "metadata": {}}, {"text": "For in-\nstance, the estimated cost of training OpenAI’s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2].", "metadata": {}}, {"text": "Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].", "metadata": {}}, {"text": "Given the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6].", "metadata": {}}, {"text": "Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs.", "metadata": {}}, {"text": "Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].", "metadata": {}}, {"text": "This paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud.", "metadata": {}}, {"text": "Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.", "metadata": {}}, {"text": "Specifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8].", "metadata": {}}, {"text": "Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU.", "metadata": {}}, {"text": "We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance.", "metadata": {}}, {"text": "Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution.", "metadata": {}}, {"text": "The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.", "metadata": {}}, {"text": "Our characterization uncovers the following unique in-\nsights.", "metadata": {}}, {"text": "(1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts.", "metadata": {}}, {"text": "(2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning;", "metadata": {}}, {"text": "optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning.", "metadata": {}}, {"text": "(3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size.", "metadata": {}}, {"text": "Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning.", "metadata": {}}, {"text": "(4) The workload becomes\ncompute bound by increasing batch size;", "metadata": {}}, {"text": "improving compute\nresources will increase performance.", "metadata": {}}, {"text": "(5) Fine-tuning sparse\nmodel leads to more load imbalance.", "metadata": {}}, {"text": "Based on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture.", "metadata": {}}, {"text": "First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput.", "metadata": {}}, {"text": "We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55.", "metadata": {}}, {"text": "Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.", "metadata": {}}, {"text": "The contributions of this paper are as follows.", "metadata": {}}, {"text": "• Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.", "metadata": {}}, {"text": "arXiv:2408.04693v1  [cs.CL]  8 Aug 2024", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Fig. 1. LLM model overview. We evaluate accuracy, throughput, runtime,\nand GPU characterization for different models, input datasets, and fine-\ntuning sparsity. The different colored expert boxes in MoE layer means\ndifferent sets of experts are activated according to the input token.\n• A detailed accuracy and runtime performance analysis\nto understand the LLM fine-tuning workload behavior.\n• Design and validation of an analytical model to estimate\nthe cost of LLM fine-tuning in the cloud.\nII. B ACKGROUND\nA. LLM and Finetuning\nThe decoder-only Transformer is designed to handle tasks\nwhere the output generation depends solely on the preceding\ntokens, making it particularly suited for auto-regressive tasks\nsuch as language modeling and text generation [9]. In the\nclassic decoder-only Transformer design, multiple decoder\nlayers are connected in sequence. Each decoder layer consists\nof a self-attention block followed by a feed-forward network\n(FFN). Fig. 1 presents an overview of the decoder-only\nTransformer model with a Mixture-of-Experts (MoE) design.\nIn this model, the FFN layers are divided into several smaller\nFFNs, referred to as experts, which are sparsely activated\nby a gating mechanism. The self-attention block can also\nbe replaced with a Mamba layer to improve performance in\nsequence modeling (a model known as state-space model).\nLLMs like GPT [10], [11], LLaMA [3], Claude [12], Mis-\ntral [13] have demonstrated their ability to excel in many\nnatural language processing (NLP) tasks Training an LLM\nmodel from scratch requires a large amount of hardware\nresources and budget.\nFine-tuning LLMs allows organizations to harness the full\npotential of advanced AI systems by tailoring them to specific\ntasks and domains. This customization involves training the\nmodel on domain-specific data, enabling it to understand\nand generate content that aligns closely with the unique\nneeds of the users. For instance, in the healthcare sector,\na fine-tuned LLM can assist in diagnosing conditions by\ninterpreting patient data and medical literature with high\nprecision. Another attractive feature of fine-tuning LLMs is\nthat it can be achieved at a cost-efficient manner. While pre-\ntraining LLMs require thousands of GPU hours, fine-tuning\ncan be achieved using a handful of GPUs in a relatively short\nTABLE I\nLLM M ODELS\n#params Mem consump. #layers #MoE layer\nMixtral 47B 23.35GB 32 8\nBlackMamba 2.8B 5.6GB 18 8\nTABLE II\nDATASETS\n#queries m. seq len type\nCommonsense 15K (CS) 15K 79 Common Sense\nMath 14K (MATH) 14K 174 Math\nHellaswag (HE) 10K 272 Common Sense\nGSM8K (GS) 1.3K 148 Math\namount of time [6]. This work uses case study of mathematics\nand common-sense question-answer datasets to demonstrate\nthe fine-tuning process of LLMs.\nB. LoRA\nLow-Rank Adaption (LoRA) is a technique that freezes\nthe pre-trained model weights and injects trainable rank de-\ncomposition into layers of the transformer architecture [14].\nLoRA significantly reduces the number of parameters,\nthereby decreasing the GPU memory footprint. LoRA can\nbe used independently of the aforementioned fine-tuning\ntechniques. In this work, we apply QLoRA [15] to the\nMixtral-8x7B model [4]; more details are provided in §III.\nC. Mixture of Experts (MoE)\nThe quality of an LLM is highly related to its scale. Given\na fixed computation budget, it is often desirable to train\na model with more parameters to achieve higher accuracy.\nMixture-of-Experts (MoE) is a technique that, instead of\nusing one large model for all tasks, combines multiple\nexpert sub-networks into a single, large model. As shown\nin Fig. 1, with MoE, different sets of experts are selectively\nactivated for different tokens. This approach can significantly\nreduce the amount of computation required for both training\nand inference, enabling the scaling up of model size and\nachieving better model accuracy [16].\nIII. E XPERIMENTAL SETUP\nModels. We fine-tune two pre-trained MoE models,\nMixtral-8x7B (Mixtral for short) [4] and BlackMamba-\n630M/2.8B (BlackMamba for short) [8]. The details of these\nmodels are shown in Table I. Both models incorporate eight\nexperts in their MoE layers. For dense fine-tuning, all experts\nare activated, whereas for sparse fine-tuning, only the top two\nexperts are selected for each token.\nThese models differ significantly in their transformer archi-\ntectures and sizes. Mixtral is a conventional MoE transformer\nmodel with a total of 47 billion parameters. In contrast,\nBlackMamba is a state-space model that replaces all at-\ntention layers with mamba layers and has only 2.8 billion", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "LLM model overview.", "metadata": {}}, {"text": "We evaluate accuracy, throughput, runtime,\nand GPU characterization for different models, input datasets, and fine-\ntuning sparsity.", "metadata": {}}, {"text": "The different colored expert boxes in MoE layer means\ndifferent sets of experts are activated according to the input token.", "metadata": {}}, {"text": "• A detailed accuracy and runtime performance analysis\nto understand the LLM fine-tuning workload behavior.", "metadata": {}}, {"text": "• Design and validation of an analytical model to estimate\nthe cost of LLM fine-tuning in the cloud.", "metadata": {}}, {"text": "II.", "metadata": {}}, {"text": "B ACKGROUND\nA.", "metadata": {}}, {"text": "LLM and Finetuning\nThe decoder-only Transformer is designed to handle tasks\nwhere the output generation depends solely on the preceding\ntokens, making it particularly suited for auto-regressive tasks\nsuch as language modeling and text generation [9].", "metadata": {}}, {"text": "In the\nclassic decoder-only Transformer design, multiple decoder\nlayers are connected in sequence.", "metadata": {}}, {"text": "Each decoder layer consists\nof a self-attention block followed by a feed-forward network\n(FFN).", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "1 presents an overview of the decoder-only\nTransformer model with a Mixture-of-Experts (MoE) design.", "metadata": {}}, {"text": "In this model, the FFN layers are divided into several smaller\nFFNs, referred to as experts, which are sparsely activated\nby a gating mechanism.", "metadata": {}}, {"text": "The self-attention block can also\nbe replaced with a Mamba layer to improve performance in\nsequence modeling (a model known as state-space model).", "metadata": {}}, {"text": "LLMs like GPT [10], [11], LLaMA [3], Claude [12], Mis-\ntral [13] have demonstrated their ability to excel in many\nnatural language processing (NLP) tasks Training an LLM\nmodel from scratch requires a large amount of hardware\nresources and budget.", "metadata": {}}, {"text": "Fine-tuning LLMs allows organizations to harness the full\npotential of advanced AI systems by tailoring them to specific\ntasks and domains.", "metadata": {}}, {"text": "This customization involves training the\nmodel on domain-specific data, enabling it to understand\nand generate content that aligns closely with the unique\nneeds of the users.", "metadata": {}}, {"text": "For instance, in the healthcare sector,\na fine-tuned LLM can assist in diagnosing conditions by\ninterpreting patient data and medical literature with high\nprecision.", "metadata": {}}, {"text": "Another attractive feature of fine-tuning LLMs is\nthat it can be achieved at a cost-efficient manner.", "metadata": {}}, {"text": "While pre-\ntraining LLMs require thousands of GPU hours, fine-tuning\ncan be achieved using a handful of GPUs in a relatively short\nTABLE I\nLLM M ODELS\n#params Mem consump.", "metadata": {}}, {"text": "#layers #MoE layer\nMixtral 47B 23.35GB 32 8\nBlackMamba 2.8B 5.6GB 18 8\nTABLE II\nDATASETS\n#queries m.", "metadata": {}}, {"text": "seq len type\nCommonsense 15K (CS) 15K 79 Common Sense\nMath 14K (MATH) 14K 174 Math\nHellaswag (HE) 10K 272 Common Sense\nGSM8K (GS) 1.3K 148 Math\namount of time [6].", "metadata": {}}, {"text": "This work uses case study of mathematics\nand common-sense question-answer datasets to demonstrate\nthe fine-tuning process of LLMs.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "LoRA\nLow-Rank Adaption (LoRA) is a technique that freezes\nthe pre-trained model weights and injects trainable rank de-\ncomposition into layers of the transformer architecture [14].", "metadata": {}}, {"text": "LoRA significantly reduces the number of parameters,\nthereby decreasing the GPU memory footprint.", "metadata": {}}, {"text": "LoRA can\nbe used independently of the aforementioned fine-tuning\ntechniques.", "metadata": {}}, {"text": "In this work, we apply QLoRA [15] to the\nMixtral-8x7B model [4];", "metadata": {}}, {"text": "more details are provided in §III.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Mixture of Experts (MoE)\nThe quality of an LLM is highly related to its scale.", "metadata": {}}, {"text": "Given\na fixed computation budget, it is often desirable to train\na model with more parameters to achieve higher accuracy.", "metadata": {}}, {"text": "Mixture-of-Experts (MoE) is a technique that, instead of\nusing one large model for all tasks, combines multiple\nexpert sub-networks into a single, large model.", "metadata": {}}, {"text": "As shown\nin Fig.", "metadata": {}}, {"text": "1, with MoE, different sets of experts are selectively\nactivated for different tokens.", "metadata": {}}, {"text": "This approach can significantly\nreduce the amount of computation required for both training\nand inference, enabling the scaling up of model size and\nachieving better model accuracy [16].", "metadata": {}}, {"text": "III.", "metadata": {}}, {"text": "E XPERIMENTAL SETUP\nModels.", "metadata": {}}, {"text": "We fine-tune two pre-trained MoE models,\nMixtral-8x7B (Mixtral for short) [4] and BlackMamba-\n630M/2.8B (BlackMamba for short) [8].", "metadata": {}}, {"text": "The details of these\nmodels are shown in Table I.", "metadata": {}}, {"text": "Both models incorporate eight\nexperts in their MoE layers.", "metadata": {}}, {"text": "For dense fine-tuning, all experts\nare activated, whereas for sparse fine-tuning, only the top two\nexperts are selected for each token.", "metadata": {}}, {"text": "These models differ significantly in their transformer archi-\ntectures and sizes.", "metadata": {}}, {"text": "Mixtral is a conventional MoE transformer\nmodel with a total of 47 billion parameters.", "metadata": {}}, {"text": "In contrast,\nBlackMamba is a state-space model that replaces all at-\ntention layers with mamba layers and has only 2.8 billion", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.png] Size: 1156x608, Data: 68592 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.png] Size: 1156x608, Data: 68592 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.png", "image_width": 1156, "image_height": 608, "attachment_type": "image", "has_image_data": true, "image_data_size": 68592}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "parameters. We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget. For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16. We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\ngradient checkpointing [18] to save memory usage.\nDatasets. Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]). The details of datasets\nare used in Table II. For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning. Each dataset consists of thousands\nof queries. We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.\nProfiling experiments. We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives. The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning. Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown. The hardware evaluation offers a detailed\nanalysis of GPU performance. Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency. These\nmetrics collectively offer a comprehensive overview of the\nmodels’ performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization. Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.\nIn contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata. Because profiling is time-consuming (approximately\n10,000× costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.\nWe present the sequence length distribution for the CS and\nMATH datasets in Fig. 2. The median sequence length is 79\nfor CS and 174 for MATH. Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect. We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.\nGPU platform. Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment. Therefore, we focus on fine-tuning these models on a\nsingle GPU. Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig. 2. Sequence length distribution for evaluated datasets.\n0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig. 3. Testing accuracy of Mixtral and BlackMamba. Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.\nNVIDIA A40 GPU with Ampere architecture. The GPU has\n48GB memory. While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory. We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\nIV. C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.\nA. Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels. Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and", "sentences": [{"text": "parameters.", "metadata": {}}, {"text": "We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget.", "metadata": {}}, {"text": "For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16.", "metadata": {}}, {"text": "We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency.", "metadata": {}}, {"text": "Moreover, we use\ngradient checkpointing [18] to save memory usage.", "metadata": {}}, {"text": "Datasets.", "metadata": {}}, {"text": "Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs.", "metadata": {}}, {"text": "Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]).", "metadata": {}}, {"text": "The details of datasets\nare used in Table II.", "metadata": {}}, {"text": "For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning.", "metadata": {}}, {"text": "Each dataset consists of thousands\nof queries.", "metadata": {}}, {"text": "We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.", "metadata": {}}, {"text": "Profiling experiments.", "metadata": {}}, {"text": "We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives.", "metadata": {}}, {"text": "The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning.", "metadata": {}}, {"text": "Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown.", "metadata": {}}, {"text": "The hardware evaluation offers a detailed\nanalysis of GPU performance.", "metadata": {}}, {"text": "Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency.", "metadata": {}}, {"text": "These\nmetrics collectively offer a comprehensive overview of the\nmodels’ performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization.", "metadata": {}}, {"text": "Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.", "metadata": {}}, {"text": "In contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata.", "metadata": {}}, {"text": "Because profiling is time-consuming (approximately\n10,000× costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.", "metadata": {}}, {"text": "We present the sequence length distribution for the CS and\nMATH datasets in Fig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "The median sequence length is 79\nfor CS and 174 for MATH.", "metadata": {}}, {"text": "Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect.", "metadata": {}}, {"text": "We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.", "metadata": {}}, {"text": "GPU platform.", "metadata": {}}, {"text": "Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment.", "metadata": {}}, {"text": "Therefore, we focus on fine-tuning these models on a\nsingle GPU.", "metadata": {}}, {"text": "Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Sequence length distribution for evaluated datasets.", "metadata": {}}, {"text": "0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Testing accuracy of Mixtral and BlackMamba.", "metadata": {}}, {"text": "Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.", "metadata": {}}, {"text": "NVIDIA A40 GPU with Ampere architecture.", "metadata": {}}, {"text": "The GPU has\n48GB memory.", "metadata": {}}, {"text": "While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory.", "metadata": {}}, {"text": "We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.", "metadata": {}}, {"text": "IV.", "metadata": {}}, {"text": "C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels.", "metadata": {}}, {"text": "Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in §III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin §V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n§III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than", "sentences": [{"text": "under 10% on BlackMamba.", "metadata": {}}, {"text": "We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).", "metadata": {}}, {"text": "We fine-tune both models using the sparse and dense setups\ndescribed in §III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch.", "metadata": {}}, {"text": "We make the following\nobservations in Fig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "(1) Fine-tuning converges relatively\nquickly.", "metadata": {}}, {"text": "Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy.", "metadata": {}}, {"text": "On GS, both\nmodels are close to their peak accuracy at the first epoch.", "metadata": {}}, {"text": "(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE.", "metadata": {}}, {"text": "(3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.", "metadata": {}}, {"text": "(4) Both models perform better on the CS dataset HE than\non the GS dataset GS.", "metadata": {}}, {"text": "This is because math is harder for\nsmaller LLMs to learn [24].", "metadata": {}}, {"text": "The BlackMamba model is\ninadequate for fine-tuning GS.", "metadata": {}}, {"text": "This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba.", "metadata": {}}, {"text": "Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25].", "metadata": {}}, {"text": "Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH.", "metadata": {}}, {"text": "(5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE.", "metadata": {}}, {"text": "However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense;", "metadata": {}}, {"text": "we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].", "metadata": {}}, {"text": "Following the above insights, the key take-away of this\nanalysis can be summarized as follows.", "metadata": {}}, {"text": "Takeaway 1.", "metadata": {}}, {"text": "Sparse model can be trained as well\nas its dense counterpart.", "metadata": {}}, {"text": "Takeaway 2.", "metadata": {}}, {"text": "Fine-tuning generally takes less ten\nepochs to reach peak accuracy.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU.", "metadata": {}}, {"text": "This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis.", "metadata": {}}, {"text": "We also compare sparse\nand dense models.", "metadata": {}}, {"text": "Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.", "metadata": {}}, {"text": "1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity.", "metadata": {}}, {"text": "The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ;", "metadata": {}}, {"text": "D: DENSE\nAND S:SPARSE .", "metadata": {}}, {"text": "Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "Execution time breakdown.", "metadata": {}}, {"text": "occupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.", "metadata": {}}, {"text": "Longer sequence lengths consume more memory, and denser\nMoE configurations require additional memory space.", "metadata": {}}, {"text": "We\ndiscuss the heuristic for determining the maximum batch size\nin §V.", "metadata": {}}, {"text": "Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.", "metadata": {}}, {"text": "2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba.", "metadata": {}}, {"text": "The purpose of this study is to understand where\ndoes this workload spend most of its time.", "metadata": {}}, {"text": "As discussed in\n§III, we conduct this study using a sequence length of 128.", "metadata": {}}, {"text": "At a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer.", "metadata": {}}, {"text": "We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "4 illustrates the following insights.", "metadata": {}}, {"text": "(1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible.", "metadata": {}}, {"text": "The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning.", "metadata": {}}, {"text": "This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning.", "metadata": {}}, {"text": "(2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation.", "metadata": {}}, {"text": "(3) The backward stage\ntypically takes more time than the forward stage.", "metadata": {}}, {"text": "In Black-\nMamba, the backward stage demands more computation than", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (μs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because", "sentences": [{"text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm.", "metadata": {}}, {"text": "MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "Execution time breakdown in terms of different model layers.", "metadata": {}}, {"text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (μs)\nMixtral\nMamba\nFig.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "Execution breakdown of the MoE layer for different kernels.", "metadata": {}}, {"text": "the forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations.", "metadata": {}}, {"text": "In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it.", "metadata": {}}, {"text": "However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.", "metadata": {}}, {"text": "We further investigate the execution breakdown based\non various layers in two LLM models.", "metadata": {}}, {"text": "For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE.", "metadata": {}}, {"text": "In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE.", "metadata": {}}, {"text": "As shown in Fig.", "metadata": {}}, {"text": "5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average.", "metadata": {}}, {"text": "The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning.", "metadata": {}}, {"text": "Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.", "metadata": {}}, {"text": "To concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.", "metadata": {}}, {"text": "Each expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween.", "metadata": {}}, {"text": "In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig.", "metadata": {}}, {"text": "7.", "metadata": {}}, {"text": "Expert architectures for Mixtral (top) and BlackMamba (bottom).", "metadata": {}}, {"text": "matrix (W3) in parallel with W1.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "6 shows the kernel-level MoE time breakdown.", "metadata": {}}, {"text": "The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral.", "metadata": {}}, {"text": "As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency.", "metadata": {}}, {"text": "The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes.", "metadata": {}}, {"text": "While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization.", "metadata": {}}, {"text": "This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.", "metadata": {}}, {"text": "Takeaway 3.", "metadata": {}}, {"text": "Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.", "metadata": {}}, {"text": "3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig.", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning.", "metadata": {}}, {"text": "To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.", "metadata": {}}, {"text": "As illustrated in Fig.", "metadata": {}}, {"text": "8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning.", "metadata": {}}, {"text": "This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart.", "metadata": {}}, {"text": "Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because", "metadata": {}}], "metadata": {"page": 5}}, {"text": "[Image page=5 idx=1 name=Im7.png] Size: 2368x1744, Data: 122833 bytes", "sentences": [{"text": "[Image page=5 idx=1 name=Im7.png] Size: 2368x1744, Data: 122833 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 1, "image_name": "Im7.png", "image_width": 2368, "image_height": 1744, "attachment_type": "image", "has_image_data": true, "image_data_size": 122833}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 × when increasing the batch\nsize from 1 to 2, but only by 4.8 × when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n§IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is", "sentences": [{"text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig.", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "Query throughput of Mixtral and BlackMamba.", "metadata": {}}, {"text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig.", "metadata": {}}, {"text": "9.", "metadata": {}}, {"text": "GPU SM utilization of different kernels in the MoE layer for different batch sizes.", "metadata": {}}, {"text": "it involves fewer computational demands, resulting in lower\nlatency.", "metadata": {}}, {"text": "This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs.", "metadata": {}}, {"text": "sparse\n(0.7 qps) models.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "8 also shows that throughput does not increase linearly\nwith batch size.", "metadata": {}}, {"text": "For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 × when increasing the batch\nsize from 1 to 2, but only by 4.8 × when increasing from\n1 to 8.", "metadata": {}}, {"text": "With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel.", "metadata": {}}, {"text": "However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n§IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.", "metadata": {}}, {"text": "Takeaway 4.", "metadata": {}}, {"text": "Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.", "metadata": {}}, {"text": "4) Hardware characterization: As shown in Fig.", "metadata": {}}, {"text": "4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer.", "metadata": {}}, {"text": "To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer.", "metadata": {}}, {"text": "The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.", "metadata": {}}, {"text": "Compute resource utilization study.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer.", "metadata": {}}, {"text": "This utilization is", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (§III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in §IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU’s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in §IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers’ memory\nutilization is batch-size-independent, while matmul layers’\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-", "sentences": [{"text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig.", "metadata": {}}, {"text": "10.", "metadata": {}}, {"text": "GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.", "metadata": {}}, {"text": "weighted by the amount of time each kernel takes.", "metadata": {}}, {"text": "We\nuse a sequence length of 128 (§III).", "metadata": {}}, {"text": "Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in §IV-B6.", "metadata": {}}, {"text": "For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory;", "metadata": {}}, {"text": "for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights.", "metadata": {}}, {"text": "(1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity.", "metadata": {}}, {"text": "(2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism.", "metadata": {}}, {"text": "Consequently, sparse\nfine-tuning supports a higher maximum batch size.", "metadata": {}}, {"text": "Both\nachieve similar maximum SM utilization at their peak batch\nsizes.", "metadata": {}}, {"text": "(3) The de-quantization kernel maintains high SM\nutilization regardless of batch size.", "metadata": {}}, {"text": "(4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU’s parallel processing capabilities.", "metadata": {}}, {"text": "Memory resource utilization study.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion.", "metadata": {}}, {"text": "We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights.", "metadata": {}}, {"text": "(1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.", "metadata": {}}, {"text": "This is because the model parameters are loaded once and\nshared by all queries in a batch.", "metadata": {}}, {"text": "However, a larger batch\nincreases the execution time (as discussed in §IV-B6),\nleading to a lower average memory bandwidth utilization.", "metadata": {}}, {"text": "HE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num.", "metadata": {}}, {"text": "of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig.", "metadata": {}}, {"text": "11.", "metadata": {}}, {"text": "Token distribution to different experts.", "metadata": {}}, {"text": "(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times.", "metadata": {}}, {"text": "(3) Dequant layers’ memory\nutilization is batch-size-independent, while matmul layers’\nutilization decreases with larger batch sizes.", "metadata": {}}, {"text": "To maximize\nGPU memory usage, a sufficiently large batch size should be\nused.", "metadata": {}}, {"text": "With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.", "metadata": {}}, {"text": "Takeaway 5 .", "metadata": {}}, {"text": "As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.", "metadata": {}}, {"text": "5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "Fig. 12. Pseudo code for MoE layers.\nciency [27]. During the training process of MoE models,\neach token is dynamically assigned to the top-k experts based\non routing scores. This strategy often leads to most tokens\nbeing assigned to a small number of experts, resulting in load\nimbalance and slower training. Additionally, some experts\nreceive insufficient training, which degrades overall model\nperformance [28]. A na¨ıve approach to address this imbalance\nis to use token dropping and padding to ensure that the\nnumber of tokens assigned to each expert is equal [29].\nHowever, this method sacrifices model quality or leads to\nwasted computation. In this section, we analyze how fine-\ntuning influences the token distribution among experts. We\ncompare the token distribution of Mixtral and BlackMamba\nbefore and after fine-tuning to understand the impact of this\nprocess.\nWe extract 1,000 examples from both the CS and MATH\ndatasets to test the original models without tuning and the\nmodels after 10 epochs of tuning on these datasets. Fig. 12\nprovides the pseudo code for MoE layers with top-k gating.\nIn this process, the hidden states are first sent to the router\nof the MoE layer, which generates router logits. These logits\ndetermine the priority of each expert for each token. Based on\nthe router score for each token, tokens are grouped together\nand sent to their assigned experts. This top-k routing strategy\ncan lead to load imbalance if the model has not been pre-\ntrained for balance.\nFig. 11 evidently shows that fine-tuning causes load im-\nbalance in Mixtral for both datasets. Comparing variance\nbefore and after fine-tuning (e.g., HE vs. HE tuned), the\ntoken assignment variance increased from 55 to 112 for\nCS and from 21 to 79 for GS. Expert 3 became the\nmost frequently used and important expert post fine-tuning.\nConversely, there is a decrease in the variance of token\ndistribution for BlackMamba on the CS dataset, dropping\nfrom 150 to 93. For the GS dataset, the token distribution\nvariance for BlackMamba remains almost unchanged after\nfine-tuning. This suggests that load-imbalance has a less\ndisruptive impact on fine-tuning for BlackMamba compared\nto Mixtral. From Fig. 11, we can also observe that Mixtral\ndemonstrates better load balance in both tasks compared to\nBlackMamba, despite the increased load imbalance after fine-\ntuning. The increased level of imbalance after fine-tuning\nsuggests GPU load balancing techniques can be helpful.\nBoth single GPU load balancing [30] and multi-GPU load\nbalancing [31] have been proposed to address this issue.\nTakeaway 6. The effect of fine-tuning on expert\nload imbalance in the MoE layer is LLM model and\ndataset dependent.\n6) Sensitivity Study on Sequence Length: To further ana-\nlyze the effect of sequence length on the fine-tuning process,\nwe chose the batch size that would maximize the memory\nfor each sequence length (64, 128, 256, 512, and 1024) and\ncompared the latency, SM utilization, and DRAM utiliza-\ntion. Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively. This is due to the varying maximum batch sizes\nsupported by each sequence length, resulting in a similar\nnumber of tokens in each batch. Because latency remains\nconsistent with increasing sequence length and we can use\nlarger batch sizes, throughput is higher for shorter sequences.\nV. A NALYTICAL MODEL TO ESTIMATE THE COST OF\nFINE -T UNING LLM S\nWhile training LLMs from scratch is a cost-prohibitive\nprocess, fine-tuning LLMs offers an attractive solution to\nalign LLMs to desired behaviors. One such example is fine-\ntuning LLMs to a domain-specific use-cases, for example, to\nanswer math questions. §IV-A shows that it is possible to\nfine-tune pre-trained LLMs on domain-specific tasks to sig-\nnificant improve accuracy. While this is a desired approach,\ncurrently, no model exists that can predict the cost of fine-\ntuning LLMs.\nFine-tuning LLMs is complex, influenced by factors like\nmodel size, GPU memory, dataset sequence length, and MoE\nsparsity, all affecting batch size and throughput. By integrat-\ning these factors with GPU costs, we can identify the most\ncost-efficient GPU for pre-tuning tasks. This section presents\nan analytical model based on previous characterization.\nThis model estimates cloud-based fine-tuning costs for a\ngiven dataset and LLM. Developed from previous sections,\nit can be adapted for other LLMs by adjusting parameters. It\nassumes using the maximum batch size supported by GPU\nmemory to optimize cost. We first estimate this batch size,\nthen use it to evaluate throughput and fine-tuning costs.\nA. Estimating Maximum Batch Size\nThe maximum batch size is the maximum number of\nqueries that can fit in GPU memory at once. Our analytical\nmodel for maximum batch size is shown in (1).\nM ax BSZ = ⌊C0∗ GP U mem − model mem\nseq len ∗ ((1 − C1) + C1 ∗ sparsity) ⌋\n(1)\nIntuitively, larger GPU memory allows for higher batch\nsizes. In the meantime, the LLM model will take up a certain", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "12.", "metadata": {}}, {"text": "Pseudo code for MoE layers.", "metadata": {}}, {"text": "ciency [27].", "metadata": {}}, {"text": "During the training process of MoE models,\neach token is dynamically assigned to the top-k experts based\non routing scores.", "metadata": {}}, {"text": "This strategy often leads to most tokens\nbeing assigned to a small number of experts, resulting in load\nimbalance and slower training.", "metadata": {}}, {"text": "Additionally, some experts\nreceive insufficient training, which degrades overall model\nperformance [28].", "metadata": {}}, {"text": "A na¨ıve approach to address this imbalance\nis to use token dropping and padding to ensure that the\nnumber of tokens assigned to each expert is equal [29].", "metadata": {}}, {"text": "However, this method sacrifices model quality or leads to\nwasted computation.", "metadata": {}}, {"text": "In this section, we analyze how fine-\ntuning influences the token distribution among experts.", "metadata": {}}, {"text": "We\ncompare the token distribution of Mixtral and BlackMamba\nbefore and after fine-tuning to understand the impact of this\nprocess.", "metadata": {}}, {"text": "We extract 1,000 examples from both the CS and MATH\ndatasets to test the original models without tuning and the\nmodels after 10 epochs of tuning on these datasets.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "12\nprovides the pseudo code for MoE layers with top-k gating.", "metadata": {}}, {"text": "In this process, the hidden states are first sent to the router\nof the MoE layer, which generates router logits.", "metadata": {}}, {"text": "These logits\ndetermine the priority of each expert for each token.", "metadata": {}}, {"text": "Based on\nthe router score for each token, tokens are grouped together\nand sent to their assigned experts.", "metadata": {}}, {"text": "This top-k routing strategy\ncan lead to load imbalance if the model has not been pre-\ntrained for balance.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "11 evidently shows that fine-tuning causes load im-\nbalance in Mixtral for both datasets.", "metadata": {}}, {"text": "Comparing variance\nbefore and after fine-tuning (e.g., HE vs.", "metadata": {}}, {"text": "HE tuned), the\ntoken assignment variance increased from 55 to 112 for\nCS and from 21 to 79 for GS.", "metadata": {}}, {"text": "Expert 3 became the\nmost frequently used and important expert post fine-tuning.", "metadata": {}}, {"text": "Conversely, there is a decrease in the variance of token\ndistribution for BlackMamba on the CS dataset, dropping\nfrom 150 to 93.", "metadata": {}}, {"text": "For the GS dataset, the token distribution\nvariance for BlackMamba remains almost unchanged after\nfine-tuning.", "metadata": {}}, {"text": "This suggests that load-imbalance has a less\ndisruptive impact on fine-tuning for BlackMamba compared\nto Mixtral.", "metadata": {}}, {"text": "From Fig.", "metadata": {}}, {"text": "11, we can also observe that Mixtral\ndemonstrates better load balance in both tasks compared to\nBlackMamba, despite the increased load imbalance after fine-\ntuning.", "metadata": {}}, {"text": "The increased level of imbalance after fine-tuning\nsuggests GPU load balancing techniques can be helpful.", "metadata": {}}, {"text": "Both single GPU load balancing [30] and multi-GPU load\nbalancing [31] have been proposed to address this issue.", "metadata": {}}, {"text": "Takeaway 6.", "metadata": {}}, {"text": "The effect of fine-tuning on expert\nload imbalance in the MoE layer is LLM model and\ndataset dependent.", "metadata": {}}, {"text": "6) Sensitivity Study on Sequence Length: To further ana-\nlyze the effect of sequence length on the fine-tuning process,\nwe chose the batch size that would maximize the memory\nfor each sequence length (64, 128, 256, 512, and 1024) and\ncompared the latency, SM utilization, and DRAM utiliza-\ntion.", "metadata": {}}, {"text": "Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively.", "metadata": {}}, {"text": "This is due to the varying maximum batch sizes\nsupported by each sequence length, resulting in a similar\nnumber of tokens in each batch.", "metadata": {}}, {"text": "Because latency remains\nconsistent with increasing sequence length and we can use\nlarger batch sizes, throughput is higher for shorter sequences.", "metadata": {}}, {"text": "V.", "metadata": {}}, {"text": "A NALYTICAL MODEL TO ESTIMATE THE COST OF\nFINE -T UNING LLM S\nWhile training LLMs from scratch is a cost-prohibitive\nprocess, fine-tuning LLMs offers an attractive solution to\nalign LLMs to desired behaviors.", "metadata": {}}, {"text": "One such example is fine-\ntuning LLMs to a domain-specific use-cases, for example, to\nanswer math questions.", "metadata": {}}, {"text": "§IV-A shows that it is possible to\nfine-tune pre-trained LLMs on domain-specific tasks to sig-\nnificant improve accuracy.", "metadata": {}}, {"text": "While this is a desired approach,\ncurrently, no model exists that can predict the cost of fine-\ntuning LLMs.", "metadata": {}}, {"text": "Fine-tuning LLMs is complex, influenced by factors like\nmodel size, GPU memory, dataset sequence length, and MoE\nsparsity, all affecting batch size and throughput.", "metadata": {}}, {"text": "By integrat-\ning these factors with GPU costs, we can identify the most\ncost-efficient GPU for pre-tuning tasks.", "metadata": {}}, {"text": "This section presents\nan analytical model based on previous characterization.", "metadata": {}}, {"text": "This model estimates cloud-based fine-tuning costs for a\ngiven dataset and LLM.", "metadata": {}}, {"text": "Developed from previous sections,\nit can be adapted for other LLMs by adjusting parameters.", "metadata": {}}, {"text": "It\nassumes using the maximum batch size supported by GPU\nmemory to optimize cost.", "metadata": {}}, {"text": "We first estimate this batch size,\nthen use it to evaluate throughput and fine-tuning costs.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Estimating Maximum Batch Size\nThe maximum batch size is the maximum number of\nqueries that can fit in GPU memory at once.", "metadata": {}}, {"text": "Our analytical\nmodel for maximum batch size is shown in (1).", "metadata": {}}, {"text": "M ax BSZ = ⌊C0∗ GP U mem − model mem\nseq len ∗ ((1 − C1) + C1 ∗ sparsity) ⌋\n(1)\nIntuitively, larger GPU memory allows for higher batch\nsizes.", "metadata": {}}, {"text": "In the meantime, the LLM model will take up a certain", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im12.png] Size: 1598x352, Data: 191103 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im12.png] Size: 1598x352, Data: 191103 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im12.png", "image_width": 1598, "image_height": 352, "attachment_type": "image", "has_image_data": true, "image_data_size": 191103}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(§III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, §V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in §IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 ∗ log( batch size\nsparsity ∗ C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU", "sentences": [{"text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig.", "metadata": {}}, {"text": "13.", "metadata": {}}, {"text": "Projected maximum batch size of Mixtral for different GPUs.", "metadata": {}}, {"text": "amount of GPU memory, and need to be subtracted in the\nanalytical model.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.", "metadata": {}}, {"text": "Moreover, the sequence length and sparsity also affect the\nmaximum batch size.", "metadata": {}}, {"text": "Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient.", "metadata": {}}, {"text": "We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size.", "metadata": {}}, {"text": "Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant.", "metadata": {}}, {"text": "The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(§III), and generate different amounts of intermediate data\nfor each query.", "metadata": {}}, {"text": "The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.", "metadata": {}}, {"text": "Finally, we use floor to round it to the maximum integer.", "metadata": {}}, {"text": "The MoE coefficient and scaling coefficient vary across\nmodels.", "metadata": {}}, {"text": "These coefficients are independent of GPU microar-\nchitectural parameters.", "metadata": {}}, {"text": "We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients.", "metadata": {}}, {"text": "For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.", "metadata": {}}, {"text": "While we showcase these parameters for the models eval-\nuated, §V-D discusses how to generalize this approach for\nother models.", "metadata": {}}, {"text": "Using our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth.", "metadata": {}}, {"text": "While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support.", "metadata": {}}, {"text": "For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively.", "metadata": {}}, {"text": "Due to space limitations, we\nonly show the projection of Mixtral model.", "metadata": {}}, {"text": "TABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB.", "metadata": {}}, {"text": "Estimating Throughput\nAs discussed in §IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound.", "metadata": {}}, {"text": "When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.", "metadata": {}}, {"text": "However, when compute resources become constrained, the\nthroughput improvement gradually saturates.", "metadata": {}}, {"text": "We model this\nbehavior using a logarithmic relation between batch size and\nthroughput.", "metadata": {}}, {"text": "Our analytical model for maximum batch size is\nshown in (2).", "metadata": {}}, {"text": "T hroughput = C2 ∗ log( batch size\nsparsity ∗ C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4.", "metadata": {}}, {"text": "C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset.", "metadata": {}}, {"text": "The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe.", "metadata": {}}, {"text": "C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput.", "metadata": {}}, {"text": "MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation.", "metadata": {}}, {"text": "This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined.", "metadata": {}}, {"text": "C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one.", "metadata": {}}, {"text": "Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.", "metadata": {}}, {"text": "To estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model.", "metadata": {}}, {"text": "We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.", "metadata": {}}, {"text": "The figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100.", "metadata": {}}, {"text": "The RMSE is less than 0.6, close\nto that of A40.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs.", "metadata": {}}, {"text": "The cost of GPU", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (§IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]–[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]–[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.", "sentences": [{"text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig.", "metadata": {}}, {"text": "14.", "metadata": {}}, {"text": "Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU.", "metadata": {}}, {"text": "Dots represent ground truth and\nlines present the estimation.", "metadata": {}}, {"text": "0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig.", "metadata": {}}, {"text": "15.", "metadata": {}}, {"text": "Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.", "metadata": {}}, {"text": "resource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU.", "metadata": {}}, {"text": "However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35].", "metadata": {}}, {"text": "Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.", "metadata": {}}, {"text": "Enterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries.", "metadata": {}}, {"text": "For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets.", "metadata": {}}, {"text": "These models consider the characteristics of\nthe LLM, dataset, and GPU.", "metadata": {}}, {"text": "Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch.", "metadata": {}}, {"text": "In throughput estimation, based on the\nobservation we made (§IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.", "metadata": {}}, {"text": "This characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists.", "metadata": {}}, {"text": "The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).", "metadata": {}}, {"text": "The coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset;", "metadata": {}}, {"text": "however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.", "metadata": {}}, {"text": "Although it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost.", "metadata": {}}, {"text": "Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.", "metadata": {}}, {"text": "VI.", "metadata": {}}, {"text": "R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]–[43].", "metadata": {}}, {"text": "MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel.", "metadata": {}}, {"text": "[44]–[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM.", "metadata": {}}, {"text": "Kim et\nal.", "metadata": {}}, {"text": "[48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning.", "metadata": {}}, {"text": "The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.", "metadata": {}}, {"text": "VII.", "metadata": {}}, {"text": "C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner.", "metadata": {}}, {"text": "This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU.", "metadata": {}}, {"text": "Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off.", "metadata": {}}, {"text": "Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning.", "metadata": {}}, {"text": "Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training.", "metadata": {}}, {"text": "For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460.", "metadata": {}}, {"text": "A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.", "metadata": {}}, {"text": "While we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.", "metadata": {}}, {"text": "ACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC).", "metadata": {}}, {"text": "We thank all the anonymous\nreviewers for their valuable comments and suggestions.", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "REFERENCES\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,\nSebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou,\nDonald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large\nlanguage models, 2022.\n[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo\nFan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen\nChu. Dissecting the runtime performance of the training, fine-tuning,\nand inference of large language models, 2023.\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models, 2023.\n[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Men-\nsch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\nGuillaume Bour, Guillaume Lample, L ´elio Renard Lavaud, Lucile\nSaulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao, Th ´eophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth ´ee Lacroix, and William El\nSayed. Mixtral of experts, 2024.\n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,\nWilliam Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Sid-\ndhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-\nRos, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang,\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling\ninstruction-finetuned language models, 2022.\n[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling\ndown to scale up: A guide to parameter-efficient fine-tuning, 2023.\n[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi\nYang. Parameter-efficient fine-tuning design spaces, 2023.\n[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge.\nBlackmamba: Mixture of experts for state-space models, 2024.\n[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\nImproving language understanding by generative pre-training. 2018.\n[10] Introducing chatgpt. https://openai.com/index/chatgpt.\n[11] Josh Achiam et.al. Gpt-4 technical report, 2024.\n[12] Introducing the next generation of claude. https://www.anthropic.com/\nnews/claude-3-family.\n[13] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile Saulnier, L ´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth ´ee Lacroix, and William El Sayed.\nMistral 7b, 2023.\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank\nadaptation of large language models, 2021.\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\nQlora: Efficient finetuning of quantized llms, 2023.\n[16] Amin Vahdat. Societal infrastructure in the age of artificial general\nintelligence. ASPLOS 2024 Keynote , 2024.\n[17] Tri Dao. Flashattention-2: Faster attention with better parallelism and\nwork partitioning, 2023.\n[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training\ndeep nets with sublinear memory cost, 2016.\n[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan\nLuo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning\nof 100+ language models. arXiv preprint arXiv:2403.13372 , 2024.\n[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong\nBing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters:\nAn adapter family for parameter-efficient fine-tuning of large language\nmodels, 2023.\n[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,\nHeewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob\nHilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. Hellaswag: Can a machine really finish your sentence? In\nProceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2019.\n[23] Nvidia nsight compute. https://developer.nvidia.com/nsight-compute.\n[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and\nWenpeng Yin. Large language models for mathematical reasoning:\nProgresses and challenges, 2024.\n[25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with\nselective state spaces, 2024.\n[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You.\nOne student knows all experts know: From sparse to dense, 2022.\n[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu,\nZilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng\nCheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive\nmixture-of-experts at scale. CoRR, abs/2206.03382, June 2022.\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,\nQuoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer, 2017.\n[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen,\nOrhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and\nZhifeng Chen. Gshard: Scaling giant models with conditional compu-\ntation and automatic sharding, 2020.\n[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R. Gao.\nDynamic load balancing on single- and multi-gpu systems. In 2010\nIEEE International Symposium on Parallel & Distributed Processing\n(IPDPS), pages 1–12, 2010.\n[31] Mohamed Wahib, Muhammet Abdullah Soyt ¨urk, and Didem Unat.\nElastic load balancing for dynamic LLMs, 2024.\n[32] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland,\nTyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson,\nWarren Weckesser, Jonathan Bright, St ´efan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew\nR. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan\nPolat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\nCharles R. Harris, Anne M. Archibald, Ant ˆonio H. Ribeiro, Fabian\nPedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python. Nature\nMethods, 17:261–272, 2020.\n[33] CUDO compute: https://www.cudocompute.com.\n[34] Amazon AWS: https://aws.amazon.com.\n[35] Lambda: https://www.gpus.com.\n[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agar-\nwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning\nfrom complex explanation traces of gpt-4, 2023.\n[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-\nMageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled\nmodels from large-scale instructions, 2024.\n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank\nadaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,\nQuentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and\nSylvain Gelly. Parameter-efficient transfer learning for nlp. In Inter-\nnational conference on machine learning , pages 2790–2799. PMLR,\n2019.\n[40] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng\nTao. SparseAdapter: An easy approach for improving the parameter-\nefficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and\nYue Zhang, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2184–2190, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics.\n[41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-\nChiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora:\nWeight-decomposed low-rank adaptation, 2024.", "sentences": [{"text": "REFERENCES\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,\nSebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou,\nDonald Metzler, Ed H.", "metadata": {}}, {"text": "Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus.", "metadata": {}}, {"text": "Emergent abilities of large\nlanguage models, 2022.", "metadata": {}}, {"text": "[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo\nFan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen\nChu.", "metadata": {}}, {"text": "Dissecting the runtime performance of the training, fine-tuning,\nand inference of large language models, 2023.", "metadata": {}}, {"text": "[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample.", "metadata": {}}, {"text": "Llama: Open and\nefficient foundation language models, 2023.", "metadata": {}}, {"text": "[4] Albert Q.", "metadata": {}}, {"text": "Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Men-\nsch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\nGuillaume Bour, Guillaume Lample, L ´elio Renard Lavaud, Lucile\nSaulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao, Th ´eophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth ´ee Lacroix, and William El\nSayed.", "metadata": {}}, {"text": "Mixtral of experts, 2024.", "metadata": {}}, {"text": "[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,\nWilliam Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Sid-\ndhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-\nRos, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang,\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H.", "metadata": {}}, {"text": "Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V .", "metadata": {}}, {"text": "Le, and Jason Wei.", "metadata": {}}, {"text": "Scaling\ninstruction-finetuned language models, 2022.", "metadata": {}}, {"text": "[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky.", "metadata": {}}, {"text": "Scaling\ndown to scale up: A guide to parameter-efficient fine-tuning, 2023.", "metadata": {}}, {"text": "[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi\nYang.", "metadata": {}}, {"text": "Parameter-efficient fine-tuning design spaces, 2023.", "metadata": {}}, {"text": "[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge.", "metadata": {}}, {"text": "Blackmamba: Mixture of experts for state-space models, 2024.", "metadata": {}}, {"text": "[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.", "metadata": {}}, {"text": "Improving language understanding by generative pre-training.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "[10] Introducing chatgpt.", "metadata": {}}, {"text": "https://openai.com/index/chatgpt.", "metadata": {}}, {"text": "[11] Josh Achiam et.al.", "metadata": {}}, {"text": "Gpt-4 technical report, 2024.", "metadata": {}}, {"text": "[12] Introducing the next generation of claude.", "metadata": {}}, {"text": "https://www.anthropic.com/\nnews/claude-3-family.", "metadata": {}}, {"text": "[13] Albert Q.", "metadata": {}}, {"text": "Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile Saulnier, L ´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth ´ee Lacroix, and William El Sayed.", "metadata": {}}, {"text": "Mistral 7b, 2023.", "metadata": {}}, {"text": "[14] Edward J.", "metadata": {}}, {"text": "Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen.", "metadata": {}}, {"text": "Lora: Low-rank\nadaptation of large language models, 2021.", "metadata": {}}, {"text": "[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.", "metadata": {}}, {"text": "Qlora: Efficient finetuning of quantized llms, 2023.", "metadata": {}}, {"text": "[16] Amin Vahdat.", "metadata": {}}, {"text": "Societal infrastructure in the age of artificial general\nintelligence.", "metadata": {}}, {"text": "ASPLOS 2024 Keynote , 2024.", "metadata": {}}, {"text": "[17] Tri Dao.", "metadata": {}}, {"text": "Flashattention-2: Faster attention with better parallelism and\nwork partitioning, 2023.", "metadata": {}}, {"text": "[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.", "metadata": {}}, {"text": "Training\ndeep nets with sublinear memory cost, 2016.", "metadata": {}}, {"text": "[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan\nLuo, and Yongqiang Ma.", "metadata": {}}, {"text": "Llamafactory: Unified efficient fine-tuning\nof 100+ language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2403.13372 , 2024.", "metadata": {}}, {"text": "[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong\nBing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee.", "metadata": {}}, {"text": "Llm-adapters:\nAn adapter family for parameter-efficient fine-tuning of large language\nmodels, 2023.", "metadata": {}}, {"text": "[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,\nHeewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob\nHilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.", "metadata": {}}, {"text": "Training verifiers to solve math word problems.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2110.14168, 2021.", "metadata": {}}, {"text": "[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi.", "metadata": {}}, {"text": "Hellaswag: Can a machine really finish your sentence?", "metadata": {}}, {"text": "In\nProceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2019.", "metadata": {}}, {"text": "[23] Nvidia nsight compute.", "metadata": {}}, {"text": "https://developer.nvidia.com/nsight-compute.", "metadata": {}}, {"text": "[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and\nWenpeng Yin.", "metadata": {}}, {"text": "Large language models for mathematical reasoning:\nProgresses and challenges, 2024.", "metadata": {}}, {"text": "[25] Albert Gu and Tri Dao.", "metadata": {}}, {"text": "Mamba: Linear-time sequence modeling with\nselective state spaces, 2024.", "metadata": {}}, {"text": "[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You.", "metadata": {}}, {"text": "One student knows all experts know: From sparse to dense, 2022.", "metadata": {}}, {"text": "[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu,\nZilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng\nCheng, Fan Yang, Mao Yang, and Yongqiang Xiong.", "metadata": {}}, {"text": "Tutel: Adaptive\nmixture-of-experts at scale.", "metadata": {}}, {"text": "CoRR, abs/2206.03382, June 2022.", "metadata": {}}, {"text": "[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,\nQuoc Le, Geoffrey Hinton, and Jeff Dean.", "metadata": {}}, {"text": "Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer, 2017.", "metadata": {}}, {"text": "[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen,\nOrhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and\nZhifeng Chen.", "metadata": {}}, {"text": "Gshard: Scaling giant models with conditional compu-\ntation and automatic sharding, 2020.", "metadata": {}}, {"text": "[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R.", "metadata": {}}, {"text": "Gao.", "metadata": {}}, {"text": "Dynamic load balancing on single- and multi-gpu systems.", "metadata": {}}, {"text": "In 2010\nIEEE International Symposium on Parallel & Distributed Processing\n(IPDPS), pages 1–12, 2010.", "metadata": {}}, {"text": "[31] Mohamed Wahib, Muhammet Abdullah Soyt ¨urk, and Didem Unat.", "metadata": {}}, {"text": "Elastic load balancing for dynamic LLMs, 2024.", "metadata": {}}, {"text": "[32] Pauli Virtanen, Ralf Gommers, Travis E.", "metadata": {}}, {"text": "Oliphant, Matt Haberland,\nTyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson,\nWarren Weckesser, Jonathan Bright, St ´efan J.", "metadata": {}}, {"text": "van der Walt, Matthew\nBrett, Joshua Wilson, K.", "metadata": {}}, {"text": "Jarrod Millman, Nikolay Mayorov, Andrew\nR.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan\nPolat, Yu Feng, Eric W.", "metadata": {}}, {"text": "Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Quintero,\nCharles R.", "metadata": {}}, {"text": "Harris, Anne M.", "metadata": {}}, {"text": "Archibald, Ant ˆonio H.", "metadata": {}}, {"text": "Ribeiro, Fabian\nPedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors.", "metadata": {}}, {"text": "SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python.", "metadata": {}}, {"text": "Nature\nMethods, 17:261–272, 2020.", "metadata": {}}, {"text": "[33] CUDO compute: https://www.cudocompute.com.", "metadata": {}}, {"text": "[34] Amazon AWS: https://aws.amazon.com.", "metadata": {}}, {"text": "[35] Lambda: https://www.gpus.com.", "metadata": {}}, {"text": "[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agar-\nwal, Hamid Palangi, and Ahmed Awadallah.", "metadata": {}}, {"text": "Orca: Progressive learning\nfrom complex explanation traces of gpt-4, 2023.", "metadata": {}}, {"text": "[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-\nMageed, and Alham Fikri Aji.", "metadata": {}}, {"text": "Lamini-lm: A diverse herd of distilled\nmodels from large-scale instructions, 2024.", "metadata": {}}, {"text": "[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen.", "metadata": {}}, {"text": "Lora: Low-rank\nadaptation of large language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2106.09685,\n2021.", "metadata": {}}, {"text": "[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,\nQuentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and\nSylvain Gelly.", "metadata": {}}, {"text": "Parameter-efficient transfer learning for nlp.", "metadata": {}}, {"text": "In Inter-\nnational conference on machine learning , pages 2790–2799.", "metadata": {}}, {"text": "PMLR,\n2019.", "metadata": {}}, {"text": "[40] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng\nTao.", "metadata": {}}, {"text": "SparseAdapter: An easy approach for improving the parameter-\nefficiency of adapters.", "metadata": {}}, {"text": "In Yoav Goldberg, Zornitsa Kozareva, and\nYue Zhang, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2184–2190, Abu Dhabi, United Arab\nEmirates, December 2022.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "[41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-\nChiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.", "metadata": {}}, {"text": "Dora:\nWeight-decomposed low-rank adaptation, 2024.", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "[42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima\nAnandkumar, and Yuandong Tian. Galore: Memory-efficient llm\ntraining by gradient low-rank projection, 2024.\n[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen\nHuang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang,\nand Fuzhen Zhuang. Mora: High-rank updating for parameter-efficient\nfine-tuning, 2024.\n[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan\nZhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense\ntraining, sparse inference: Rethinking training of mixture-of-experts\nlanguage models. arXiv preprint arXiv:2404.05567 , 2024.\n[45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen,\nYuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang\nZhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe:\nAlleviate world knowledge forgetting in large language models via\nmoe-style plugin, 2024.\n[46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent\nZhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon.\nMixture-of-experts with expert choice routing, 2022.\n[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao,\nDeli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y . Wu, Zhenda\nXie, Y . K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang. Deepseekmoe: Towards ultimate expert specialization\nin mixture-of-experts language models, 2024.\n[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta,\nSeyeon Kim, Yongin Kwon, and Sangtae Ha. Llmem: Estimating gpu\nmemory usage for fine-tuning pre-trained llms, 2024.", "sentences": [{"text": "[42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima\nAnandkumar, and Yuandong Tian.", "metadata": {}}, {"text": "Galore: Memory-efficient llm\ntraining by gradient low-rank projection, 2024.", "metadata": {}}, {"text": "[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen\nHuang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang,\nand Fuzhen Zhuang.", "metadata": {}}, {"text": "Mora: High-rank updating for parameter-efficient\nfine-tuning, 2024.", "metadata": {}}, {"text": "[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan\nZhang, Aude Oliva, Colin Raffel, and Rameswar Panda.", "metadata": {}}, {"text": "Dense\ntraining, sparse inference: Rethinking training of mixture-of-experts\nlanguage models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2404.05567 , 2024.", "metadata": {}}, {"text": "[45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen,\nYuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang\nZhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang.", "metadata": {}}, {"text": "Loramoe:\nAlleviate world knowledge forgetting in large language models via\nmoe-style plugin, 2024.", "metadata": {}}, {"text": "[46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent\nZhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon.", "metadata": {}}, {"text": "Mixture-of-experts with expert choice routing, 2022.", "metadata": {}}, {"text": "[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R.", "metadata": {}}, {"text": "X.", "metadata": {}}, {"text": "Xu, Huazuo Gao,\nDeli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y .", "metadata": {}}, {"text": "Wu, Zhenda\nXie, Y .", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang.", "metadata": {}}, {"text": "Deepseekmoe: Towards ultimate expert specialization\nin mixture-of-experts language models, 2024.", "metadata": {}}, {"text": "[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta,\nSeyeon Kim, Yongin Kwon, and Sangtae Ha.", "metadata": {}}, {"text": "Llmem: Estimating gpu\nmemory usage for fine-tuning pre-trained llms, 2024.", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "APPENDIX\nA. Abstract\nThis artifact reproduces the results presented in the Char-\nacterization Study. It includes a detailed three-level runtime\nbreakdown, analysis of SM and MEM utilization, and a\ncomprehensive study of throughput.\nB. Artifact check-list (meta-information)\n• Compilation: PyTorch\n• Model: Mixtral-8x7B and BlackMamba-630M/2.8B\n• Data set: Hellaswag, GSM8k, MATH 14k and common-\nsense 15k (provided in GitHub reopsitory)\n• Run-time environment: Ubuntu 20.04.6\n• Hardware: NVIDIA A40 (48GB) GPU\n• Output: Nsight Compute\n• Experiments: Fine-tune both models using different batch\nsizes and conduct a GPU characterization study\n• How much disk space required (approximately)?: 100GB\n• How much time is needed to prepare workflow (approxi-\nmately)?: 1 hour\n• How much time is needed to complete experiments (ap-\nproximately)?: Throughput and Runtime Breakdown experi-\nments can be completed within 2 hours, while Nsight Compute\nprofiling for SM and MEM utilization will take approximately\n80 hours\n• Publicly available?: Yes\n• Workflow framework used?: LLaMA-Factory\nC. Description\n1) How to access: Our source code can be found at\nhttps://github.com/stsxxx/finetune\n2) Hardware dependencies:\n• We conducted all experiments on a server equipped with\nan Intel® Xeon® Platinum 8380 CPU @ 2.30GHz and\nan NVIDIA A40 (48GB) GPU\n• Supported GPUs should have at least 48GB of memory\nand feature an Ampere architecture or newer\n3) Software dependencies:\n• A recent Linux release\n• Python 3.8.10\n• CUDA 11.8\n• PyTorch 2.1.0 compatible with CUDA 11.8\n• CUDA toolkit 11.8\n4) Data sets: Hellaswag, GSM8k, MATH 14k and com-\nmonsense 15k. We provide all of them in our GitHub repos-\nitory.\n5) Models: Mixtral-8x7B and BlackMamba-\n630M/2.8B. We provide the python script to download\nthem from Huggingface. Mixtral-8x7B is a gated\nmodel, access request should be submitted here\nhttps://huggingface.co/mistralai/Mixtral-8x7B-v0.1.\nD. Installation\nFor the Python environment, simply clone our repository\nand use conda to set up a new environment by running the\nfollowing command:\n#create a new conda environment\nconda create --name=ft python=3.8\nconda activate ft\n#install pytorch2.1.0+cu118\nconda install pytorch==2.1.0 \\\ntorchvision==0.16.0 torchaudio==2.1.0 \\\npytorch-cuda=11.8 -c pytorch -c nvidia\n#download the source code\ngit clone https://github.com/stsxxx\n/finetune.git\ncd finetune\n#install all other dependencies\npip install -r requirements.txt\nE. Experiment workflow\nFirst make sure the working directory is the LLaMA-\nFactory directory:\ncd LLaMA-Factory\nBefore running experiments, you should download both\ntwo models from Huggingface:\n#Add Blackmamba directory to your pythonpath\nexport PYTHONPATH=$PYTHONPATH:../BlackMamba\n#specify where you want to store models\nexport HF_HOME=\"path\"\n#download models, huggingface access token\nshould be entered in the terminal\npython3 model_download.py\nMake sure you change the transformers library path and\nmodel config file path before running each experiment bash\nscript, you can find an example in the README file:\n# change it to your transformers library path\ntransformers_path=\"xxxxx\"\n# change it to your model config path\nconfig_file_path=\"xxxxx\"\nTo reproduce the fine-tuning throughput results shown in\nFig. 8, you can run the following scripts:\n./mixtral_tp.sh\npython3 throughput.py ./profile_data/mixtral\n/throughput > mixtral_throughput.txt\n./mamba_tp.sh\npython3 throughput.py ./profile_data\n/blackmamba/throughput > mamba_throughput.txt", "sentences": [{"text": "APPENDIX\nA.", "metadata": {}}, {"text": "Abstract\nThis artifact reproduces the results presented in the Char-\nacterization Study.", "metadata": {}}, {"text": "It includes a detailed three-level runtime\nbreakdown, analysis of SM and MEM utilization, and a\ncomprehensive study of throughput.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Artifact check-list (meta-information)\n• Compilation: PyTorch\n• Model: Mixtral-8x7B and BlackMamba-630M/2.8B\n• Data set: Hellaswag, GSM8k, MATH 14k and common-\nsense 15k (provided in GitHub reopsitory)\n• Run-time environment: Ubuntu 20.04.6\n• Hardware: NVIDIA A40 (48GB) GPU\n• Output: Nsight Compute\n• Experiments: Fine-tune both models using different batch\nsizes and conduct a GPU characterization study\n• How much disk space required (approximately)?: 100GB\n• How much time is needed to prepare workflow (approxi-\nmately)?: 1 hour\n• How much time is needed to complete experiments (ap-\nproximately)?: Throughput and Runtime Breakdown experi-\nments can be completed within 2 hours, while Nsight Compute\nprofiling for SM and MEM utilization will take approximately\n80 hours\n• Publicly available?: Yes\n• Workflow framework used?: LLaMA-Factory\nC.", "metadata": {}}, {"text": "Description\n1) How to access: Our source code can be found at\nhttps://github.com/stsxxx/finetune\n2) Hardware dependencies:\n• We conducted all experiments on a server equipped with\nan Intel® Xeon® Platinum 8380 CPU @ 2.30GHz and\nan NVIDIA A40 (48GB) GPU\n• Supported GPUs should have at least 48GB of memory\nand feature an Ampere architecture or newer\n3) Software dependencies:\n• A recent Linux release\n• Python 3.8.10\n• CUDA 11.8\n• PyTorch 2.1.0 compatible with CUDA 11.8\n• CUDA toolkit 11.8\n4) Data sets: Hellaswag, GSM8k, MATH 14k and com-\nmonsense 15k.", "metadata": {}}, {"text": "We provide all of them in our GitHub repos-\nitory.", "metadata": {}}, {"text": "5) Models: Mixtral-8x7B and BlackMamba-\n630M/2.8B.", "metadata": {}}, {"text": "We provide the python script to download\nthem from Huggingface.", "metadata": {}}, {"text": "Mixtral-8x7B is a gated\nmodel, access request should be submitted here\nhttps://huggingface.co/mistralai/Mixtral-8x7B-v0.1.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Installation\nFor the Python environment, simply clone our repository\nand use conda to set up a new environment by running the\nfollowing command:\n#create a new conda environment\nconda create --name=ft python=3.8\nconda activate ft\n#install pytorch2.1.0+cu118\nconda install pytorch==2.1.0 \\\ntorchvision==0.16.0 torchaudio==2.1.0 \\\npytorch-cuda=11.8 -c pytorch -c nvidia\n#download the source code\ngit clone https://github.com/stsxxx\n/finetune.git\ncd finetune\n#install all other dependencies\npip install -r requirements.txt\nE.", "metadata": {}}, {"text": "Experiment workflow\nFirst make sure the working directory is the LLaMA-\nFactory directory:\ncd LLaMA-Factory\nBefore running experiments, you should download both\ntwo models from Huggingface:\n#Add Blackmamba directory to your pythonpath\nexport PYTHONPATH=$PYTHONPATH:../BlackMamba\n#specify where you want to store models\nexport HF_HOME=\"path\"\n#download models, huggingface access token\nshould be entered in the terminal\npython3 model_download.py\nMake sure you change the transformers library path and\nmodel config file path before running each experiment bash\nscript, you can find an example in the README file:\n# change it to your transformers library path\ntransformers_path=\"xxxxx\"\n# change it to your model config path\nconfig_file_path=\"xxxxx\"\nTo reproduce the fine-tuning throughput results shown in\nFig.", "metadata": {}}, {"text": "8, you can run the following scripts:\n./mixtral_tp.sh\npython3 throughput.py ./profile_data/mixtral\n/throughput > mixtral_throughput.txt\n./mamba_tp.sh\npython3 throughput.py ./profile_data\n/blackmamba/throughput > mamba_throughput.txt", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "High-level and layer-level latency breakdown results\nshown in Fig. 4 and 5 can be obtained by running:\n./mixtral_lt.sh\npython3 mixtral_latency.py ./profile_data\n/mixtral/latency > mixtral_latency_breakdown.txt\n./mamba_lt.sh\npython3 mamba_latency.py ./profile_data\n/blackmamba/latency > mamba_latency_breakdown.txt\nYou can also use Nsight Compute to profile and generate\nkernel-level latency breakdown, SM and MEM utilization\nresults shown in Fig. 6, 9 and 10 by running:\n./mixtral_pf.sh\npython3 sm_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_sm.txt\npython3 mem_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_mem.txt\n./mamba_pf.sh\npython3 sm_mamba.py ./profile_data/blackmamba\n/ncu > mamba_sm.txt\npython3 mem_mamba.py ./profile_data/blackmamba\n/ncu > mamba_mem.txt\npython3 sm_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_sm_backward.txt\npython3 mem_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_mem_backward.txt\nF . Evaluation and expected results\nThe generated results are stored in specific text files as\nindicated in the commands above, such as mixtral sm.txt for\nSM utilization data of the Mixtral model.\nG. Experiment customization\nCustomized experiments can be conducted with varying\nbatch sizes and query sequence lengths, both of which can\nbe adjusted in each bash script.\nH. Methodology\nSubmission, reviewing and badging methodology:\n• https://www.acm.org/publications/policies/\nartifact-review-and-badging-current\n• https://cTuning.org/ae", "sentences": [{"text": "High-level and layer-level latency breakdown results\nshown in Fig.", "metadata": {}}, {"text": "4 and 5 can be obtained by running:\n./mixtral_lt.sh\npython3 mixtral_latency.py ./profile_data\n/mixtral/latency > mixtral_latency_breakdown.txt\n./mamba_lt.sh\npython3 mamba_latency.py ./profile_data\n/blackmamba/latency > mamba_latency_breakdown.txt\nYou can also use Nsight Compute to profile and generate\nkernel-level latency breakdown, SM and MEM utilization\nresults shown in Fig.", "metadata": {}}, {"text": "6, 9 and 10 by running:\n./mixtral_pf.sh\npython3 sm_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_sm.txt\npython3 mem_mixtral.py ./profile_data/mixtral\n/ncu > mixtral_mem.txt\n./mamba_pf.sh\npython3 sm_mamba.py ./profile_data/blackmamba\n/ncu > mamba_sm.txt\npython3 mem_mamba.py ./profile_data/blackmamba\n/ncu > mamba_mem.txt\npython3 sm_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_sm_backward.txt\npython3 mem_mamba_back.py ./profile_data\n/blackmamba/ncu_back > mamba_mem_backward.txt\nF .", "metadata": {}}, {"text": "Evaluation and expected results\nThe generated results are stored in specific text files as\nindicated in the commands above, such as mixtral sm.txt for\nSM utilization data of the Mixtral model.", "metadata": {}}, {"text": "G.", "metadata": {}}, {"text": "Experiment customization\nCustomized experiments can be conducted with varying\nbatch sizes and query sequence lengths, both of which can\nbe adjusted in each bash script.", "metadata": {}}, {"text": "H.", "metadata": {}}, {"text": "Methodology\nSubmission, reviewing and badging methodology:\n• https://www.acm.org/publications/policies/\nartifact-review-and-badging-current\n• https://cTuning.org/ae", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}]}