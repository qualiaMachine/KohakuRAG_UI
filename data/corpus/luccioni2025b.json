{"document_id": "luccioni2025b", "title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice", "text": "arXiv:2504.00797v1  [cs.CY]  1 Apr 2025\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI R esearch and\nPractice\nALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France\nRAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho\nAs the possibilities for Artiﬁcial Intelligence (AI) have grown, so hav e concerns regarding its impacts on society and the environment.\nHowever, these issues are often raised separately; i.e. carbonfootprint analyses of AI models typically do not consider how the pursuit\nof scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately\nharmful to the environment. On the other hand, model audits that aim t o evaluate model performance and disparate impacts mostly\nfail to engage with the environmental ramiﬁcations of AI models and how th ese ﬁt into their auditing approaches. In this separation,\nboth research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential\nsolutions for making informed choices that can be developed at their c onvergence. In this essay, we build upon work carried out in AI\nand in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such\nas generalizability, transparency, evaluation and equity across AI r esearch and practice. We argue that the eﬀorts aiming to study\nAI’s ethical ramiﬁcations should be made in tandem with those evalua ting its impacts on the environment, and we conclude with a\nproposal of best practices to better integrate AI ethics and susta inability in AI research and practice.\n1 INTRODUCTION\nIn recent years, AI systems have become pervasive, presente d as a key tool in the ﬁght against climate change [168]\nand in societally-beneﬁcial domains such as health and educ ation [64, 71, 87]. However, the training and deployment\nof AI systems also comes with a cost in terms of energy and natu ral resources [52, 117, 192] and can inadvertently\nresult in the ampliﬁcation of inequalities [183] and prolif eration of biases [6] when systems are put into practice.\nHistorically, the societal and environmental impacts of AI systems have been addressed separately, in two distinct\nareas of study – i.e. scholarship that aims to address the eth ics of AI models focuses on aspects such as bias evaluation\nor auditing [33, 201], typically overlooking models’ impac ts on natural resources and ecosystems [127]. Conversely,\nwork that aims to estimate the growing carbon footprint and e nergy consumption of AI models [121, 192] does not\ntypically address the contribution this has towards shifti ng the balance of power or amplifying inequalities [1, 2].\nSome recent scholarship has started to establish explicit l inks between ethics and sustainability often hones in on\nspeciﬁc applications, such as the emblematic \"Stochastic P arrots\" paper, which addresses both sustainability and ethics\nas issues in the context of large language models (LLMs) [16] . Apart from this exception and a precious few others,\nconversations around AI ethics and sustainability have tak en place separately, in diﬀerent venues and by diﬀerent sets\nof stakeholders. However, since in both AI sustainability a nd AI ethics research, the aim is to think through how we\nmight develop technologies which are useful and inclusive w hile we limit harm to people and environments, similar\nthemes emerge, often structured via concepts such as functi onality and eﬃciency just as much as justice, fairness and\nequity. In both of these disciplines.\nIn the present article, we highlight these transversal them es and argue that both the societal and environmental\nimpacts of AI systems should be considered in parallel in all aspects of AI theory, practice and governance. Drawing\nupon work from within the AI community as well as related ﬁeld s, we explore a series of use cases illustrating that\nAuthors’ addresses: Alexandra Sasha Luccioni; Giada Pisti lli, sasha.luccioni@huggingface.co, Hugging Face, Canad a/France; Raesetje Sefala; Nyalleng\nMoorosi, Distributed AI Research Institute, Canada/Lesot ho.\n1\n\n[Image page=1 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\n2 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nethics and sustainability go hand-in-glove when it comes to the development and the deployment of AI systems. By\ndoing so, we hope to shed light on the importance of pursuing r esearch that blends together diﬀerent perspectives to\nallow for a better understanding of the societal impacts of A I systems.\nGiven that our goal is to allow our work to be read and understo od by a variety of audiences, we start, in Section §2,\nby deﬁning the concepts and terms that are core to our subsequ ent analysis. We continue by examining the current\nstate of AI ethics and sustainability from 3 diﬀerent but com plementary perspectives: theoretical principles and fram e-\nworks (§3.1), AI research and practice(§3.2), and AI regula tion and governance (§3.3). Next, in §4, we identify four\ntransversal themes that we have found to be particularly cen tral to both AI ethics and sustainability: generalizabilit y,\nevaluation, transparency, and power. Finally, in §5, we pro pose ways forward at the intersection of these themes and\nthree directions of AI research and practice. We wrap up the a rticle with ideas for future work that can be pursued at\nthe nexus of AI ethics and sustainability and a brief conclus ion.\n2 KEY CONCEPTS AND DEFINITIONS\nSustainability\nWhen it comes to the concept of sustainability, one of its ﬁrs t and most widely-accepted deﬁnitions originates from\nthe 1987 Brundtland Report, which deﬁnes sustainable devel opment as “development that meets the needs of the present\nwithout compromising the ability of future generations to m eet their own needs” [1987][p.41]. This deﬁnition remains\ncentral to the ﬁeld of sustainability write large, informin g frameworks such as the UN Sustainable Development Goals\n(SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [199].\nHowever, also in 1987, environmental economist Edward Barb ier proposed an alternative deﬁnition to sustainability,\nstructuring it around three pillars: environmental, socie tal and economic, arguing that sustainable development can\nonly be truly achieved when both environmental stewardship , social equity and economic viability coexist and are\ninter-connected [1987].\nIn the context of AI, the term ‘sustainability’ is most often used to refer solely to environmental sustainability [41,\n61]. The umbrella term ‘Sustainable AI’ was initially propo sed by van Wynsberghe as a ﬁeld of practice that both\naims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI\napproaches themselves [203]. This proposal would then enco mpass the vast variety of work being done at the nexus\nof machine learning and ﬁelds such as biodiversity monitori ng, agriculture, transportation, etc. (for a review, see [2 06]\nand [168]). AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable\nAI Workshop, that have put the emphasis on eﬃcient methods an d the application of AI to sustainability-related\nproblems, as well as the Tackling Climate Change with Machin e Learning workshop, which aims to demonstrate that\nAI can be an invaluable tool in helping society adapt to and mi tigate the eﬀects of climate change.\nEthics\nInherently characterized by ongoing perplexity, ethics as pires for certainty and consensus, yet also remains dynamic\nand evolving. It has its origins in the work of philosophers s uch as Aristotle, who posited that ethics connects theory\nwith praxis, with the goal of guiding human actions towards eudaimonia (i.e. the highest human good) [8]. In the mil-\nlennia since Aristotle, the philosophical sub-domain of ap plied ethics has sought to establish normative principles f or\na variety of human activities and domains, which inexorably depend on the context of application and the individuals\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 3\ninvolved, leading to much debate regarding which norms shou ld be applicable in which contexts, as well as the deﬁni-\ntion of key ethical concepts such as fairness [89, 175], tran sparency [60, 147] and, indeed, the very deﬁnition of ethics\nitself [21, 195].\nLacking consensus, the ﬁeld of AI ethics often applies Weste rn moral theories ranging from utilitarianism [18, 134]\nto egalitarianism [209] and virtue ethics [7, 8] to propose w ays of assessing the ethicality of AI systems. However, the\napplication of these moral theories faces challenges given the diﬃculty of, e.g. quantifying the concept of utility in\nutilitarianism, assessing and comparing who is worse oﬀ in egalitarianism, or evaluating cultural variability in deﬁning\nvirtues in virtue ethics. This is also the case in terms of the application of these theories in modern-day contexts\ninvolving new types of AI-driven technologies such as robot s or autonomous vehicles, which can be limited without a\ncomprehensive understanding of both AI’s technical capabi lities (e.g. the limitations of the underlying models) as we ll\nas the diversity of life experiences of the people using thes e tools, who can interact with them in ways that are hard\nto predict or design for [102]. Given that these concepts enc ompass work from a multitude of domains that espouse\ndiﬀerent objectives, values and methods [23], their deﬁnit ion can have major consequences on the way in which\nthese concepts are operationalized in AI models and systems [10, 65, 183]. Work that addresses the ethical aspects of\nAI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a\ncross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.\n3 EXISTING SCHOLARSHIP IN AI ETHICS AND SUSTAINABILITY\nIn the sections below, we explore existing scholarship in or der to critically analyze how both ethics and sustainabilit y\nare deﬁned in theory and operationalized in practice within the diverse communities that pursue AI. First, we describe\nthe principles and frameworks that have been proposed to guide AI from both an ethical and environmental perspective;\nnext, we examine how these principles are applied, implicit ly and explicitly, in AI research and practice. Finally, we\nlook at several recent approaches for regulating and govern ing AI and the diﬀerent roles adopted by stakeholders and\norganizations.\n3.1 Principles and Frameworks\nA common starting point to ensure the ethical development an d deployment of AI systems is the deﬁnition of a struc-\nture for guiding this process, which can be operationalized via sets of principles or a framework. On the one hand,\nethical principles are often deﬁned at a high level, describ ing values and concepts outside of any speciﬁc context of\ndeployment. As such, multiple sets of guiding principles fo r ‘ethical AI’ have been proposed by diﬀerent organizations\nranging from research institutes to nonproﬁt and for-proﬁt entities. However, given the many diﬀerent types of AI\napproaches that exist, as well as contextual factors that in ﬂuence their application, it is diﬃcult to deﬁne universal, or\neven generalizable, guidelines. To this point, a 2019 analy sis by Jobin et al. reviewed 84 sets of guidelines mentioning\na variety of principles, ﬁnding very limited convergence be tween them but identifying the values of transparency,\nfairness, non-maleﬁcence, privacy and responsibility as b eing most common [2019]. Similarly, a multitude of ethical\nframeworks have been proposed, with most of them espousing s peciﬁc visions of ethics by putting an emphasis on\naspects ranging from human empowerment [64] to virtue ethic s [79]. In comparison to principles, ethical frameworks\noften aim to frame ethics from the perspective of implementa tion, identifying how challenges can be addressed and\nhow to build consensus around ethical values, often anchore d to speciﬁc contexts of deployment of AI systems, such\nas medicine [125] or autonomous vehicles [109]. However, th e technological mechanisms proposed to operationalize\n\n4 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nthese frameworks are often deﬁned in abstract terms, which h ave been found to be diﬃcult to implement in practice\nfrom an engineering perspective, making them diﬃcult to ope rationalize in practice [155]. In terms of environmental\nsustainability, the UN SDGs are most commonly used to inform AI frameworks [72], with inspiration from ﬁelds such\nas ecology to guide the deﬁnition of methods based on metrics and evaluation methods that enable a more holistic\nassessment of AI’s environmental impacts. [112]. However, similarly to AI principles, there is an equal multiplicity\nof AI frameworks, and multiple analyses have been carried ou t with the goal of establishing overlap and transversal\nconnections, which were found to be lacking [14, 155, 205].\nConversely, analyses of ethical AI principles have also obs erved a general lack of recognition of AI’s environmental\nimpacts within the diﬀerent sets of principles that have been deﬁned. Diﬀerent reasons have been proposed for this lack\nof connection, from the reliance of most AI principles on tra ditional Western ethical perspectives, which are human-\ncentered and assign intrinsic value to human beings above ot her living things [28], to the paucity of research on AI’s\nenvironmental impacts, which hinders the development of co herent principles [25]. When sustainability is addressed\nin ethical AI frameworks, it is once again only limited to env ironmental sustainability, notably carbon footprint es-\ntimation. For instance, several industry-led sets of princ iples speciﬁcally addressing the environmental sustainab ility\nof AI have been proposed in recent years by organizations suc h as Salesforce [2024] and the Green Software Founda-\ntion [2023]. While these propositions touch upon metrics su ch as energy eﬃciency, they do not address topics such as\nrebound eﬀects [120] 1, transparency and access to compute, which we consider to be core to these discussions and\nwhich we discuss in Section 4. Nonetheless, certain framewo rks, such as the UNESCO recommendations on the ethics\nof artiﬁcial intelligence [2021] do emphasize the importan ce of sustainability and of evaluating technologies based o n\ntheir environmental impacts via the UN SDGs, which, as discu ssed in the introduction, have sustainable development\nat their core – which we explore in more depth in subsequent se ctions.\n3.2 Research and Practice\nGiven that AI is a distributed ﬁeld consisting of a multitude of practitioners and organizations, the practical application\nof the principles and frameworks described in the previous s ection can diﬀer immensely. In a 2022 study of papers\nsubmitted to conferences such as ICML and NeurIPS, Birhane e t al. analyzed the values that were highlighted by their\nauthors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered\nexplicitly [2022a]. From the 59 values they identiﬁed, the m ost emphasis was put on aspects such as technical progress,\nquantitative evidence, and novelty, whereas ethical consi derations around values such as beneﬁcence, interpretabil ity\nand respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a\nfraction of papers. Also, not a single one of the values Birha ne et al. identiﬁed was explicitly linked to environmental\nsustainability, highlighting once again the lack of connection in the research community with sustainability writ lar ge.\nProgressing from this observation, while sustainability- oriented research has not been prominently featured in\nvenues that directly address AI ethics, it remains a topic of research that has been gathering momentum in recent\nyears. The ﬁrst research to formally address the environmen tal impacts of training AI models was the seminal 2019\narticle by Strubell et al. which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as\nreaching 626,155 pounds of /u1D436/u1D4422 emissions [192]. Follow-up work by other researchers has sh ed more light on this\nissue, revealing diﬀerent aspects of model architecture [1 53] and training procedure [53] that can impact its ensuing\n1The relationship between eﬃciency and sustainability is fa r from straightforward, given phenomena such as rebound eﬀe cts, in which improved\neﬃciency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resour ces – see [27, 120] for a more\nin-depth review.\n\n[Image page=4 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 5\ncarbon footprint. The ﬁrst proposal for “Green AI”, i.e. AI r esearch that takes environmental impacts into consideration\nwhen training AI models, was made by Schwartz et al. in 2020 [1 81] - it was subsequently broadened to include aspects\nsuch as hardware and scaling [215] and model deployment [121 ] more recently. However, this ﬁeld of research, while\nincreasingly proliﬁc, has failed to take ethical considera tions into account in its analyses, focusing solely on aspec ts\nsuch as carbon intensity and energy eﬃciency, and not on issu es such as the environmental impacts of increased\nconsumption due to the use of AI in targeted advertising [96] , or the application of AI in the oil and gas sector [77],\nwhich are liable to counter-balance any actual eﬃciency gai ns 2.\n3.3 Governance and Regulation\nGovernance and regulation aim to establish mechanisms for d ecision-making, guiding the development and deploy-\nment of AI systems, and outlining the roles and responsibili ties of each party involved [219]. There are many dis-\ntributed eﬀorts for governance in AI whose aim is to develop e thical guardrails, with some highlighting the impor-\ntance of international institutions [85], and others focus ing on the public sector [101] – reﬂecting that both bottom-u p\nand top-down endeavors are useful to establish functional m echanisms for governing AI. If we take a look at recent\ncommunity endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that estab-\nlished mechanisms for various ethical aspects of the projec t such as data governance, quality metrics, and fostering\nstakeholder collaboration and transparency [92], as well a s drafting a consensus-driven ethical framework for govern -\ning the resulting artifacts that encompasses both legal and technical dimensions [154]. Interestingly, Big Science wa s\none of the few projects that also considered and documented t he carbon footprint of model training, evaluation and\ndeployment, proposing a holistic, life cycle approach to es timating emissions [121]. There have also been proposals\narguing for putting sustainability in the center of AI devel opment and deployment [203], as well as frameworks for\ncertifying the sustainability of AI systems [25], across al l the diﬀerent pillars of sustainability (i.e. social, envi ronmen-\ntal and economic) [70]. However, we are still at the beginnin g of building governance mechanisms that oﬀer a more\ncomprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability.\nA pivotal example of top-down governance is the European Uni on’s AI Act, which draws from broadly deﬁned\nethical principles to inform its regulations [39]. In fact, the text of the Act demonstrates considerable progress foll ow-\ning the recent European dialogues, seeking to regulate AI ap plications that may infringe on human rights, adhering,\namong others, to the ethical principles of human oversight, human agency and transparency. Moreover, the AI Act’s\nfoundation on the premise that risk equates to potential hum an rights harms [152] echoes one of the longstanding tra-\nditions in AI ethics of prioritizing human rights [9, 216]. T his approach embodies the ethical commitment to safeguard\nfundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these im-\nportant principles. However, while both EU AI Act [2022], as well as similar regulatory initiatives in China [2023] and\nCanada [2023] point to the need to protect both fundamental h uman rights as well as to limit damage to environment,\nthere are no oﬃcial provisions regarding sustainability in any of their texts, and it remains to be seen how existing\nstandards for environmental impacts in all of these jurisdi ctions will apply to AI systems. Similarly, sustainability\nconsiderations were also lacking in the 2023 US Executive Or der regarding AI [20], which did not mention AI’s green-\nhouse gas emissions nor energy usage, as well as multi-natio n declarations such as the Bletchley Declaration [2023],\nillustrating the disconnect between sustainability and et hics in recent approaches to AI regulation.\n2Recent media coverage of Microsoft’s sustainability promi ses has estimated that a single contract to use AI to expand oi l production “could enable\ncarbon emissions adding up to 640 percent of the company’s ca rbon removal targets\" [191].\n\n[Image page=5 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\n6 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\n4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY\n\"Is it fair . . . that the residents of the Maldives (likely to b e underwater by 2100) or the 800,000 people in\nSudan aﬀected by drastic ﬂoods pay the environmental price o f training and deploying ever larger English\nLMs, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic? \"\nBender, Gebru et al. [2021]\nIn the current section, we deﬁne four recurring issues that w e have found to be particularly salient to discussions\naround both AI ethics and sustainability. These issues are i nspired by previous carried out by critical scholars such\nas Dobbe and Whittaker [2019], Birhane [2022b], van Wynsber ghe [2021] as well as Bender and Gebru [2021], as\ncited above. We start, in Section 4.1 with a discussion of the perils of assumptions of the generalizability of data and\nmodels in both ethics and sustainability. We follow, in Section 4.2 with a reﬂection upon the current state of evaluation\nof AI systems, what is measured, what is missing, and why that matters. Next, we remark upon the current lack of\ntransparency with regards to information relevant to both ethics and sustainabilityin Section 4.3. Finally, in Section 4.4,\nwe discuss the balance of power and the allocation of justice , and how existing inequalities can be further ampliﬁed\nby AI systems.\n4.1 Generalizability\nAI technologies function based on assumptions of generaliz ability and representativeness – i.e. that given suﬃcient\ndata, an AI model can learn to accurately represent (any) giv en process and even adapt to previously unseen data [74].\nFor instance, the concept of pre-training AI models on large datasets such as ImageNet [47] dates back to the early\n1990s [178] and has since become the dominant training parad igm in both computer vision [162, 193] and natural\nlanguage processing [50, 114]. In fact, pre-training is heavily dependent upon the assumption of representativeness -i.e.\nthat the huge amounts of training data used for pre-training represent the world as a whole, or at least a suﬃcient part\nof it to be useful for any kind of downstream application (i.e . ﬁne-tuning, transfer learning, etc). While the limitatio ns\nof such claims for both AI models and datasets have been previ ously shown (see Chasalow and Levy [36], Koch et al.\n[98], Raji et al. [157], Smith et al. [188]), the theory of gen eralizability, and the perception of certain types of AI\nmodels, ie. LLMs, as “general purpose technologies” contin ues to persist [57]. This can come with both ethical and\nenvironmental ramiﬁcations when systems trained under the pretense of generalizability are applied in contexts that\ndiﬀer from the ones represented in their training data – we di scuss some of these below.\nGiven the data that fuels AI models is produced by humans, it i s intrinsically laden with subjective judgments [132]\nand representative of speciﬁc worldviews [46]. Numerous st udies have shown that both the data used for training AI\nmodels [54, 157, 166] and the models themselves [16, 73, 213] are not, in fact, representative of the world at large and\nthat the biases contained in training data persist even if fu rther ﬁne-tuning is carried out [103], which can have ‘cas-\ncade’ eﬀects when models are deployed in production [173], w hich can contribute to perpetuating negative biases [68].\nSimilarly, oﬀ-the-shelf, proprietary technologies that a re marketed as generic can fail at the tasks when applied in\ndiﬀering contexts, e.g. in applications such as facial reco gnition (when they fail to recognize people from population s\nunder-represented in training datasets [33]) and crime pre diction (where they have dismal accuracy rates across dif-\nferent locations [174]) – and yet, out-of-the-box AI system s for these tasks and many others continue to be built and\ndeployed under the assumptions that they will work no matter the context of their application. This can have devas-\ntating eﬀects on already marginalized communities when app lied for tasks such as criminal sentencing [6], and facial\nrecognition [33].\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 7\nSimilarly to data concerning human beings, environmental a nd ecological data can also contain biases, for instance\nin its temporal coverage and geographical spread, which can be as damaging to biomes 3 of plants and animals as they\nare to communities of humans [95, 187]. From a modeling persp ective, AI models trained to carry out biodiversity\nmonitoring on one ecosystem often fail to perform accuratel y on others, no matter the species [99, 184], and yet\necological bias assessments are not often carried out befor e model training. In fact, White et al. refer to geographical\ndiﬀerences in ecological datasets as the main factor limiti ng our capacity to predict how biodiversity will be impacted\nby future changes in the climate, notably due to missing data from regions such as Africa and South America, making\ndata-driven approaches such as AI unrepresentative of enti re continents [210].\nAn example of the dire consequences this can have can be found in the ﬁeld of short-term climate modeling and\ndisaster detection, which relies on data from sensors and we ather radar stations that allow for events such as ﬂoods\nand wildﬁres to be tracked in real-time across borders, ocea ns and continents. While geospatial data is inherently\nglobal (given that satellites circle the planet as a whole), it relies upon assumptions of generalizability that often\nfail to hold when applied in contexts that diﬀer from the trai ning data, or the fact that many regions are missing\nthese data sources for reasons ranging from insuﬃcient fund ing and aging technological infrastructure to lack of\nconnectivity [37, 198], which results in data gaps [182, 197 ]. This can come at a cost to living beings, both human and\nanimal, in the regions and contexts where these technologie s are applied – for instance in early warning systems for\nextreme weather events [150].and deforestation detection [97]. Of course, any assessment of representativity (or lac k\nthereof) hinges upon an appropriate evaluation of this assu mption – which we address in the following section.\n4.2 Evaluation\nA popular adage states that “you can’t improve what you don’t measure” 4; in the context of AI systems, this can be\ntranslated into the fact that the criteria that we use to eval uate AI systems and the way in which this evaluation are\ncarried out are important – i.e. the metrics we choose help us embed our values as communities about outcomes we\nwish to see and those that we put less emphasis on [136]. While leaderboards such as Papers With Code tend to only\nmeasure performance-based metrics such as accuracy or prec ision, factoring in other metrics can make comparisons\nbetween diﬀerent models more meaningful and actionable. Th is is due to the fact that real-world constraints on model\ndeployment often result in trade-oﬀs being made between diﬀ erent factors that include accuracy and eﬃciency [31],\nbut also robustness [217], inclusion [88] and data quality [ 11]. This means that in order to meaningfully assess the\nutility of AI systems in diﬀerent practical contexts, other measures must be considered in parallel to performance and\naccuracy [122]; and often the best people to do the assessmen ts for the trade-oﬀs are the populations who will use\nthese tools. For example, when it comes to the evaluation of g enerative technologies such as large language models,\nthese do not have a single well-established evaluation appr oach [139, 159]. Approaches that are used for evaluating\ntheir ethical limitations include red-teaming [66], exter nal audits [139, 158] as well as more holistic model evaluati ons\nthat reﬂect diﬀerent aspects of model performance [26, 67]. However, critiques of the approaches described above\nalso include their non-inclusivity of marginalized commun ities [48] as well as a lack of formalized approaches and\nstandards for model evaluation, making apples-to-apples c omparisons between models diﬃcult [40]. Also, as many of\nthe most widely deployed AI systems are currently proprieta ry and direct access to models is not possible, it is hard\nto exhaustively evaluate many popular commercial models fo r any meaningful evaluation to take place [118, 190].\n3A biome is a bio-geographical unit that corresponds to a comm unity of plants and animals that share a physical environmen t and a climate.\n4The quote is often attributed to Peter Drucker, although its exact origins are unclear.\n\n[Image page=7 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\n8 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nSimilarly, evaluating the environmental impacts of AI syst ems is far from straightforward, and we are still missing\nmany pieces of the puzzle needed in order to meaningfully est imate these impacts. For instance, most of the carbon\nfootprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192],\nbut which only represents a portion of models’ total environ mental impacts. In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach\nfor this evaluation, since it takes into account diﬀerent st ages of the model life cycle including the manufacturing of\ncomputing hardware, idle energy usage, and model deploymen t, ﬁnding that training accounted for only half of the\nmodel’s overall emissions [121], meaning that similar stud ies that only took training into account were potentially\nunderestimating their emissions by half. Also, while comme ndable in terms of its granularity, this kind of carbon\naccounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such\nas BLOOM towards amplifying the existing inequalities in th e ﬁeld of AI due to the increased amount of computing\nresources that they require, which are unattainable to many members of the AI community, as well as the propagation\nof biases via their usage. Furthermore, the authors themsel ves note that there is currently no information available\nabout the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the\noverall carbon footprint this represents. This highlights that the emphasis on environmental sustainability often fa ils\nto account for other aspects of AI’s global impacts – and any k ind of evaluation hinges upon transparency, which is\nsorely lacking in the ﬁeld of AI – we discuss this in more lengt h in the following section.\n4.3 Transparency\nTransparency is widely recognized as a fundamental princip le in science in general and AI in particular [63, 107, 208,\n212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based\nsystems are not inherently transparent or interpretable, given the complexity of the neural network architectures the y\nespouse and the number of parameters they contain [105, 113, 140]. Eﬀorts such as interpretability approaches are\nuseful and can help interpret the predictions of models post hoc [104, 164], whereas artifacts such as data sheets and\nmodel cards [69, 137] can contribute towards making AI syste ms more understandable for users, providing essential\ninformation about AI models in a user-friendly format. Thes e artifacts allow users to understand not just how an\nAI system functions, but also its limitations, potential bi ases, implications, and environmental impacts. However, even\nthough model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face)\nand provide important information about models, they are no t suﬃcient to guarantee, for instance, the reproducibility\nof reported results, which is a core tenet of scientiﬁc pract ice [146].\nIndeed, several studies of transparency found that an overw helming amount of results published at technical AI\nconferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation\nhighlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility\nof AI models and their ﬁndings. Such an approach would acknow ledge the connection between transparency and\nreproducibility: transparent research practices enable r eproducibility, which in turn facilitates independent scr utiny,\nvalidation, further development of research ﬁndings by other scientists [80]. The absence of transparency, especially in\nsharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing\nAI models’ scientiﬁc impact and impeding their adoption wit hin the wider scientiﬁc community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental\nimpacts of AI models and systems, with most work in this ﬁeld b eing done post-hoc by researchers who did not do the\ninitial model training and deployment (e.g. [116, 192]). Th e most common environmental sustainability metric for AI\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 9\nmodels, their carbon footprint, is rarely, if ever, disclos ed. While model cards of recent models such as BLOOM [214]\nand Stable Diﬀusion [169] have included carbon footprint in formation, it remains far from common information com-\nmunicated by model creators – recent work has found that the o verwhelming majority of models shared publicly do\nnot include this information [34]. In fact, most carbon foot print analyses gather the information manually by writing\nto authors. For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-\nmation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many\nauthors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023]. In\nfact, until recent years, the general emphasis in the AI comm unity was put on the eﬃciency and ‘greenness‘ of AI as\nopposed to its environmental costs, which are now slowly starting to gain traction as an important consideration for AI\nsystems [86, 141]. In fact, given the increasing size and com putational requirements of models being productionized in\nrecent years (especially LLMs), training them is only acces sible to a small fraction of the AI community, which means\nthat the organizations who have the necessary resources for this have a disproportionate inﬂuence on the ﬁeld as a\nwhole— we discuss this in the following section.\n4.4 Power and Equity\nModern AI research and practice are not equitable by design:their cost in terms of computer hardware as well as human\nskills means that only a small percentage of both academic and industrial organizations can contribute to many aspects\nof model development. With the recent advent of AI models of e ver-increasing scale and complexity, the digital divide\nin AI is only increasing, as it takes more compute and human sk ill to train and deploy AI models and systems [1, 19, 35,\n118]. This means that the future wide-sweeping beneﬁts that AI technologies are promised to have for humanity as a\nwhole [149] are contingent upon access to technologies that are fundamentally unequally distributed. Despite explici t\nproposals to pursue more equitable and explicitly de-colon ialist approaches [124, 138], the ‘bigger-is-better’ para digm\ncontinues to be central to AI research and practice [204]. In a similar fashion, the places where AI research is being\ncarried out are also skewed towards institutions from a hand ful of countries mostly located in the Global North [2],\nwhich inexorably impacts the choices made during the AI deve lopment and deployment process, introducing many\nbiases (which we have already addressed in previous section s). In fact, recent work has proposed that the very pursuit\nof sustainable AI has the opposite eﬀect, contributing towa rds maintaining the status quo and “securing the dominant\nsocio-economic interests of neo-liberal capitalism\" [180]. For instance, major technological corporations have dedicated\nsigniﬁcant resources towards solutions such as improving t he eﬃciency of their data centers, proposing numerous\ninitiatives towards technological sustainability [4, 75] , including research at the nexus of AI and the climate [44, 12 8].\nHowever, both Microsoft and Google announced that they woul d miss their 2024 sustainability targets, due in part to\nthe energy demands of the AI tools that they have been develop ing and deploying [130, 161]. The impacts of these\ncomputation-intensive data centers can further be expande d to include the mining of rare metals and the disposal of\ne-waste, both of which predominantly aﬀect countries from t he Global South which proﬁting technology companies\nfrom the Global North [86, 194].\nSimilarly to AI, issues of power, equity and justice are also central in the ﬁeld of sustainability, since climate change\nis an inherently inequitable phenomenon – with a handful of c ountries and regions in North America, East Asia and\nEurope responsible for a disproportionate portion of globa l emissions, while the impacts of sea level rise and extreme\nweather events being felt most strongly in countries with ve ry minimal carbon footprints, raising questions of equity\nand justice and how to address them [49, 126, 151, 177]. Simil arly, the majority of climate-focused AI solutions over-\nlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who\n\n10 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nstands to beneﬁt from them, or whether these issues can be sol ved with technology in the ﬁrst place [30].For instance,\nwhen AI systems that carry out precision agriculture are dev eloped, the emphasis is made on eﬃciency or increased\ncrop yields [185], and not the fact that these systems are lia ble to replace already underprivileged communities such as\nmigrant workers that traditionally harvested crops by hand, or the disparate impacts of proposed solutions on diﬀerent\ncommunities [55, 207, 218]. In addition, AI technologies ca n be seen as further exacerbating the existing inequalities\nin terms of the distribution of power and proﬁts [83] and perp etuating the existing extractionist approaches in terms\nof labor by exploiting already under-paid and marginalized workers for data collection and labeling tasks [165] as well\nas creating a new class of precarious crowd-sourced workers [211], which are often located in the Global South or in\nalready marginalized communities, where the impacts of cli mate change have reduced the viability of traditional pro-\nfessions such as farming [81, 163]. This is an additional exa mple for why we advocate for establishing more concerted\neﬀorts for integrating AI ethics and sustainability – we des cribe these in the following section.\n5 ESTABLISHING BEST PRACTICES FOR AI ETHICS AND SUSTAINABIL ITY\nHaving established a multitude of transversal topics that s pan AI ethics and sustainability, we now focus on proposing\nbest practices that would integrate the two in AI research an d practice. We draw upon existing work to show how we\ncan build upon it both within the AI community and in tandem wi th members of other communities ranging from\nsustainable development to policy-making and engineering .\nPrinciples and Frameworks Research and Practice Governance and Regulation\nGeneralizability\nShifting from the dominant\nWestern moral philosophies\nto include perspectives from\nnon-Western traditions\nStudying how well AI models\ngeneralize to diﬀerent populations\nof human and non-human living\nbeings\nEspousing bottom-up governance\napproaches based on the cultural,\nsocietal and geographical constraints\nof AI system deployment\nEvaluation\nDeveloping frameworks that\naccommodate both ethical and\nenvironmental responsibility\nCarrying out more holistic\nevaluation of models and systems,\nspanning both ethical and\nenvironmental criteria\nIntegrating both social and\nenvironmental assessments\ninto existing and in-progress\nAI regulation\nTransparency\nBroadening the scope of\ntransparency to include its\nsocial and environmental aspects\nCommunicate the costs\nand impacts of AI systems\non both the environment\nand society\nRequiring deployed AI systems\nto carry out audits and report\na minimum of metrics spanning\nboth bias and ethics\nPower\nDeveloping principles and\nframeworks that address\nboth human and ecological needs\nMake equity-informed\ntrade-oﬀs when developing\nand deploying AI\nInvolving multiple stakeholders,\nespecially ones from the concerned\ncommunities and areas, in the\ngovernance process\nTable 1. A summary of the proposed best practices for diﬀeren t axes that we cover in our article\n5.1 Principles\nIn the context of AI, there are multiple facets of AI technolo gies that have to be taken into consideration, given the\nintersection between AI and the broader societal context in which it operates. Importantly, integrating broader sus-\ntainability into AI guidelines ensures that justice and fai rness are not just about social dimensions but also include\nrespecting and protecting the environment – i.e. expanding the deﬁnition of sustainable AI to include the social and\neconomic pillars proposed by Barbier [12]. This integration also acknowledges that true justice in AI cannot be achieved\nwithout considering its environmental and societal implications, which helps build the bridge between complementary\napproaches in both AI ethics and (environmental) sustainab ility research.\n\n[Image page=10 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=2 name=~1~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=3 name=~2~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=4 name=~3~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=5 name=~4~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=6 name=~5~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=7 name=~6~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=8 name=~7~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=9 name=~8~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=10 name=~9~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=11 name=~10~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=12 name=~11~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=13 name=~12~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=14 name=~13~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=15 name=~14~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=16 name=~15~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=17 name=~16~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=18 name=~17~.png] Size: 1x1, Data: 67 bytes\n\n[Image page=10 idx=19 name=~18~.png] Size: 1x1, Data: 67 bytes\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 11\nGeneralizability. Given the observed disconnect between ethics and sustainability in the context of AI principles and\nframeworks, we ﬁnd that, while these oﬀer a valuable startin g point, they often fall short in addressing AI’s complex\nethical issues due to their lack of contextual sensitivity [ 142]. We also believe that improving upon this necessitates a\nmore nuanced and context-speciﬁc approach to AI ethics, one that embraces the varied ethical dimensions presented\nby AI, including its environmental implications. Current e thical charters in AI, often detached from this perplexity,\nrepresent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [5].\nBy recognizing these limitations, we intend not to discard these deﬁnitions and principles but to improve upon them. In\nthis context, environmental and sustainability challenges related to AI development and deployment are integral to the\nbroader ethical reﬂections within the ﬁeld. This integrati on between ethics and sustainability in AI calls for a holist ic\napproach, where ethical considerations are not viewed in is olation but are intrinsically linked with environmental an d\nsustainability oversight. For instance, shifting from the dominant Western moral philosophies to include perspectiv es\nfrom non-Western traditions such as relational ethics [131 ], Ubuntu ethics [145], and Confucian ethics [111] can oﬀer\nvaluable insights into community, social harmony, and the i nterconnectedness of beings, emphasizing the impact of\nAI on society and interpersonal relationships.\nEvaluation. There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of\nconcluding that an AI system is neither truly ethical nor sus tainable. Recent work has begun bridging the gap; for\ninstance, work by Lynch et al. is inspired by the concept of ur gent governance in environmental studies, which distin-\nguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure\nand technologies [123]. Raji et al. use a similar approach fo r their proposed end-to-end framework for internal algo-\nrithmic auditing of AI models, which takes into account both technical and ethical assessments [159]. In a similar vein,\na recent model evaluation framework by Rakova et al., propos es an environmental justice-oriented lens to carry out\nalgorithmic audits [160], that of Genovesi and Mönig places sustainability at the center of Ethical AI certiﬁcation [70 ],\nwhile that of Metcalf et al. uses environmental impact asses sments as an example of a formal mechanism that can be\nused to inspire the assessment of AI technologies [129]. All of these approaches acknowledge that ethical decisions in\nAI have environmental consequences and vice versa, thus nec essitating a framing that accommodates both ethical and\nenvironmental responsibility.\nTransparency. When viewed as a means to foster greater accountability, tra nsparency takes on a central role: it\nbecomes a principle that enhances ethical compliance and pr omotes environmental responsibility, contributing to\nsustainability. For instance, integrating social transpa rency and sustainability can be exempliﬁed by an AI system\ndesigned for urban planning, such as an AI tool developed to o ptimize city layouts for eﬃciency. In this example,\nto include sustainability approaches, developers would al so provide information on the environmental footprint of\nrunning the AI system, such as energy consumption during dat a processing and potential environmental beneﬁts of\nthe proposed urban layouts, like reduced carbon emissions from optimized traﬃc ﬂows or green spaces. In this way, by\ndeepening the concept of transparency to include social and environmental aspects, we would move towards creating\nAI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts\nthey have and making more informed decisions based on the inf ormation at our disposal [51].\nEquity and Power. Equity is about ensuring fair access and participation in the beneﬁts and governance of technology\nacross diﬀerent communities, especially those historically marginalized. Philosophical perspectives on equity, drawing\n\n12 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nfrom theories of distributive justice [106, 135], emphasiz e the necessity of equitable distribution of resources and r e-\nsponsibilities and risks associated with AI technologies. This principle is especially relevant in environmental jus tice,\nwhere the disproportionate impact of environmental harms on speciﬁc populations demands a reevaluation of AI tech-\nnologies are deployed at scale – for instance, Schlosberg’s theory of recognitional justice highlights the importance of\nrecognizing and respecting diverse community needs and val ues in environmental policies [176]. Additionally, equity\nrequires that AI development actively includes diverse voices in its creation and implementation phases, ensuring tha t\nAI systems do not perpetuate existing disparities but rathe r contribute to rectifying them. This approach draws upon\nenvironmental justice literature for developing principl es and frameworks that addresses both human and ecological\nneeds, thus framing equity as a matter of distribution, proc edural and interactional fairness [32].\n5.2 Research\nDespite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainabil-\nity and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given\ntheir socio-technicality and the consequences this can hav e on the human and non-human species residing in these\nregions.\nGeneralizability. At the nexus of AI ethics and sustainability, existing schol arship has already established that the\nmost disadvantaged and marginalized members of our societi es tend to be the least well-represented in the ‘Big Data’\nused in many AI models and systems [33, 43, 68, 186]; the same applies at the level of countries and regions, with ‘global’\ndatasets reﬂecting things like wealth and economic develop ment only being tested in a select few countries [24, 84]\nand the most extensive biodiversity datasets consisting of data collected in a subset of regions from the Global North,\nas well as regions close to cities and roads [15, 196]. Studyi ng the limits of application of AI systems both in terms\nof ethics and sustainability and how well they generalize to diﬀerent populations of human and non-human subjects\nis important to question existing assumptions. For instanc e, studies of emblematic datasets such as ImageNet found\nthat it to misrepresent both humans [2021] and other living b eings such as insects and ﬁsh [2023]. Developing new\ndatasets that are more representative of diverse populatio ns and contexts - such as the Dollar Street dataset [167] and\nCropHarvest [197] - and using this dataset in research and pr actice and help improve the applicability of AI systems\nand their ability to represent more diverse populations fro m both a societal and environmental perspective.\nEvaluation. As AI is increasingly used in the ﬁght against climate change , holistic evaluation of models and systems\nbecomes ever more relevant. For instance, in the high-stake s domain of solar geoengineering, which aims to develop\nnew ways for modifying the Earth’s climate to reduce the glob al warming eﬀect (i.e. by enhancing the reﬂexivity of\nclouds so that they reﬂect more of the sun’s rays), AI is often used to help model the potential far-reaching eﬀects\nof even minor interventions and understand how they will imp act local and global climate patterns [62, 148, 179].\nThis is because regional changes to the climate may trigger i ncreased fragility of some regions and not others, which\nis hard to quantify and therefore, to be optimized for in AI mo dels that aim to predict the consequences of climate\ninterventions [83]. For instance, diﬀerent thresholds of solar reﬂexivity are optimal for diﬀerent regions, and optimizing\nresults based on a given region (e.g. the continental United States) would make things worse for others (e.g. Western\nAfrica), which can suﬀer droughts and other forms of damage [ 126]. Considerations around the wider rebound eﬀects\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 13\nof proposed AI solutions are also important: for instance, in the case of AI systems that improve aircraft eﬃciency [108] ,\nmore eﬃcient aircraft can result in cheaper airfare and ther efore, more travel overall 5.\nTransparency. Falk and van Wynsberghe argue that transparency is a crucial factor for establishing “whether the\nnet environmental impact of AI for Sustainability is positi ve or how the positive impact on the respective sustainabili ty\ngoal can outweigh the very diﬀerent negative impact of the mo dels’ development on sustainability\" [61] – we would\nexpand this to include AI systems in general, which should us e a variety of mechanisms to communicate the costs and\npotential impacts of their systems on both the environment a nd society. In fact, as proposed by Ehsan et al., the notion\nof transparency in AI can be expanded to encompass \"social tr ansparency\", which involves integrating socio-technical\naspects in the description and understanding of AI systems [ 56]. Social transparency involves a portrayal of an AI\nsystem’s societal impacts, ethical considerations, and ev entually its environmental footprint. By doing so, it provi des\na more complete picture, making the AI system transparent bu t also understandable in a broader societal context.\nThis augmented view of transparency, which would integrate both social and environmental dimensions, resonates\nwith the increasing awareness that AI is not simply a technol ogical tool but a socio-technical system with extensive\nrepercussions, spanning both people and the environment [2 02].\nEquity. Recognizing that Green AI (or sustainable AI), i.e. the deve lopment and deployment of AI systems that puts\nthe emphasis solely on eﬃciency or the reduction of greenhou se gas emissions, is not necessarily inherently more\nequitable or just – for instance, if the more eﬃcient models a re not widely shared, or entail an increased usage of\ncompute due to their eﬃciency – is an important ﬁrst step towa rds improving the current direction towards bigger\nmodels. Recent research in both AI ethics and sustainabilit y has shed light on the extent to which AI systems enable\nthe ampliﬁcation of existing social inequities [17, 58, 115 ] as well as contributing to the preservation of the ecologic al\nstatus quo vis-a-vis to climate change [194]. Given these ﬁn dings, it is important to make equity-informed trade-oﬀs\nwhen developing and deploying AI – for instance by carrying o ut energy prediction in resource-constrained energy\ngrids, which are common in low- and middle-income countries [13], or by using AI to predict the impacts of changes\nin climate on societal aspects such as disease propagation a nd health [100].\n5.3 Governance\nAs the ﬁeld of AI ethics increasingly intersects with regula tion, including law and policy, it showcases its interdisci -\nplinary nature, meaning that in order to be successful, gove rnance initiatives must incorporate eﬀorts from diﬀerent\ndomains, depending on the context of the application.\nGeneralizability. While there is no single solution to complex questions invol ving governance over AI systems,\nvarious bottom-up governance approaches have been propose d based on the cultural, societal and geographical con-\nstraints of AI system deployment. Some of these follow the te nets of the M ¯aori culture, which is based on principles\nthat consider both impacts on nature as well as on fellow huma n beings, bridging the gap between ethics and sustain-\nability [82, 143]; others espouse those established by the I ndigenous AI community, which is based on both values and\npractices of social and environmental sustainability, bot h core to many Indigenous epistemologies [110]. Also, regu-\nlating the deployment of out-of-the-box AI solutions that o perate on the premise of generalizability without taking\n5This is often referred to as Jevons paradox, which observes that when technological progress improves the eﬃciency of technology, this actually results\nin its increased usage and increases overall resource use [9 3].\n\n[Image page=13 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes\n\n14 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\ncontext into account can help ensure that systems that are me ant to be widely applicable are truly representative of\nthe context of application – where evaluations, as explaine d below, will play a crucial role.\nEvaluation. As noted by Metcalf et al. [2021], there is a parallel between environmental impact assessments and AI\nethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such\nas energy consumption and carbon emissions. Requiring audi ts of commercial AI systems before their deployment in\npractice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but\nalso in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental\nimpacts, will require the development of new governance app roaches. For instance, attempting to evaluate the wider\nrebound eﬀects of AI tools and their impacts on consumption a nd human behavior is important to represent their\nbroader impacts on both society and the environment [86, 96, 194]. Finally, integrating both social and environmental\nassessments into existing and in-progress regulation and d eveloping new approaches to evaluate these impacts can\nhelp ensure that the deployment of AI systems is carried out i n a way that is ethically sound and sustainable across\nmultiple dimensions.\nTransparency. Recent years have seen less transparency in AI research and practice, especially in terms of generative\nAI models [189]. However, as these systems are increasingly being deployed in society, having more information\nregarding how these systems were created and deployed remains paramount. Ensuring that enough details are provided\nboth regarding the energy consumed and greenhouse gasses em itted during model training and deployment can help\ntrack how the environmental impacts of AI are evolving over t ime. Mandating transparency for already deployed AI\nsystems can help establish audits, red teaming eﬀorts and AI energy score ratings to raise users’ awareness around the\nimpacts of the systems they use [29], contributing to what can be termed \"usable transparency\" [144]. By ensuring that\nthe processes and results of these practices are well-docum ented and publicly accessible, stakeholders would be bette r\nequipped to understand and evaluate the ethical and sustain able aspects of AI systems. Such policies would promote\na culture of openness in the AI industry, encouraging develo pers to prioritize ethical considerations and sustainabil ity\nalongside technical advancements.\nEquity. Involving multiple stakeholders, especially ones from the concerned communities and domains, in this pro-\ncess is important to ensure that diﬀerent perspectives and l ived experiences are reﬂected in the development and\ndeployment of AI systems [170, 171] as well as the diﬀerent co mmunities and can inﬂuence existing and future prac-\ntices [45, 126]. From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society)\nproject proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI\nsystems [133], whereas the White House Oﬃce of Management an d Budget (OMB)’s ﬁrst government-wide policy\naround the usage of AI includes, inter alia, clauses that stipulate that government agencies should tak e both envi-\nronmental impacts and bias and fairness into account when pr ocuring AI-enabled services [59] - exhibiting thought\nleadership that will hopefully have wider repercussions.\n6 CONCLUSION\nWe recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies\nlike AI, it can be diﬃcult to deﬁne what progress looks like an d how it can be achieved. We do not pretend to have\ndeveloped a universal approach for either of these issues (a nd do not believe that one can exist) – but by adopting\na multitude of endeavors such as the ones described in the par agraphs above can help involve diﬀerent actors and\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 15\nhopefully build momentum across the AI community. The beaut y of making transversal connections that go above\nand beyond the silos in which many AI technologists tend to op erate is that we can also be inspired by the multitude\nof rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and\nclimate science, to propose ways forward that would allow the AI community to improve systems from the perspective\nof both ethics and sustainability. Looking forward, the ﬁeld of AI ethics is rapidly evolving, with insights racing to keep\nup with the rapid pace of technological advancements in AI. T his dynamic landscape presents an ongoing challenge:\nto develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts. Furthermore, it is\nbecoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a\nholistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical\nsocietal and environmental consequences. Therefore, inte grating AI ethics with sustainability is not just beneﬁcial\nbut necessary, ensuring that AI advancements are not only te chnologically innovative and ethically sound but also\nmaximizing their potential to engender sustainable advanc ement.\n\n16 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoo die Project: Big tobacco, big tech, and the threat on academic integrity. InProceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 287–297.\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie N évéol, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. Th e elephant in the\nroom: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n[3] AI Safety Summit. 2023. The Bletchley Declaration by Cou ntries Attending the AI Safety Summit, 1-2 November 2023. (2 023).\nhttps://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchl ey-declaration-by-countries-attending-the-ai-safety -summit-1- \n[4] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud.\n[5] Daniel Andler. 2023. Intelligence artiﬁcielle, intelligence humaine: le doubl e énigme. Gallimard, Paris.\n[6] Julia Angwin, Jeﬀ Larson, Surya Mattu, and Lauren Kirchn er. 2022. Machine bias. In Ethics of data and analytics. Auerbach Publications, 254–264.\n[7] Thomas Aquinas. 1702. Summa theologica. J. Mentelin.\n[8] Aristotle. 350. Nicomachean Ethics. Original work published in 350 B.C.E..\n[9] Hutan Ashraﬁan. 2015. Intelligent robots must uphold hu man rights. Nature 519 (2015), 391. https://doi.org/10.1038/519391a\n[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and De borah Raji. 2022. Disentangling the Components of Ethical Research in Machine Learning.\nIn Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul, Republic of Korea) (FAccT ’22) . Association for\nComputing Machinery, New York, NY, USA, 2057–2068. https:/ /doi.org/10.1145/3531146.3533781\n[11] Ricardo Baeza-Yates and Zeinab Liaghat. 2017. Quality -eﬃciency trade-oﬀs in machine learning for text processin g. In 2017 IEEE international\nconference on big data (big data) . IEEE, 897–904.\n[12] Edward B Barbier. 1987. The concept of sustainable econ omic development. Environmental conservation 14, 2 (1987), 101–110.\n[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Jo yce, and Margaret Odero. 2023. Topology Estimation from Vol tage Edge Sensing for\nResource-Constrained Grids. Tackling Climate Change with Machine Learning Workshop - IC LR 2023 (2023).\n[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante , and Azzurra Ragone. 2023. A Rapid Review of Responsible AI f rameworks: How to guide\nthe development of ethical AI. In Proceedings of the 27th International Conference on Evaluat ion and Assessment in Software Engineering . 358–367.\n[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfga ng Schwanghart. 2014. Spatial bias in the GBIF database and i ts eﬀect on modeling\nspecies’ geographic distributions. Ecological Informatics 19 (2014), 10–15.\n[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Majo r, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accounta bility, and Transparency (Virtual Event, Canada) (FAccT ’21) .\nAssociation for Computing Machinery, New York, NY, USA, 610 –623. https://doi.org/10.1145/3442188.3445922\n[17] Ruha Benjamin. 2023. Race after technology. In Social Theory Re-Wired. Routledge, 405–415.\n[18] Jeremy Bentham. 1789. An Introduction to the Principles of Morals and Legislation . T. Payne and Son.\n[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michae l, Lennart Heim, Xueyun Luo, and Neil Thompson. 2024. The Com pute Divide in\nMachine Learning: A Threat to Academic Contribution and Scr utiny? arXiv preprint arXiv:2401.02452 (2024).\n[20] Joseph R Biden. 2023. Executive order on the safe, secur e, and trustworthy development and use of artiﬁcial intelli gence. (2023).\n[21] Elettra Bietti. 2020. From Ethics Washing to Ethics Bas hing: A View on Tech Ethics from within Moral Philosophy. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency(Barcelona, Spain) (FAT* ’20) . Association for Computing Machinery, New York, NY, USA,\n210–219. https://doi.org/10.1145/3351095.3372860\n[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, Willia m Agnew, Ravit Dotan, and Michelle Bao. 2022. The Values Enco ded in Machine Learning\nResearch. arXiv:2106.15590 [cs.LG]\n[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S . Brown, Johnathan Flowers, Anthony Ventresque, and Christ opher L. Dancy. 2022.\nThe forgotten margins of AI ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency. 948–958.\n[24] Joshua Blumenstock. 2018. Don’t forget people in the us e of big data for development.\n[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van W ynsberghe. 2022. From an Ethics of Carefulness to an Ethics o f Desirability: Going\nBeyond Current Ethics Approaches to Sustainable AI. Sustainability 14, 8 (2022). https://doi.org/10.3390/su14084472\n[26] Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holis tic evaluation of language models. Annals of the New York Academy of Sciences 1525, 1\n(2023), 140–146.\n[27] Mioara Borza. 2014. The connection between eﬃciency an d sustainability–a theoretical approach. Procedia Economics and Finance 15 (2014),\n1355–1363.\n[28] Andrew Brennan and Norva Y. S. Lo. 2022. Environmental E thics. In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N. Zalta\n(Ed.). Metaphysics Research Lab, Stanford University.\n[29] Benedetta Brevini. 2020. Black boxes, not green: Mytho logizing artiﬁcial intelligence and omitting the environm ent. Big Data & Society 7, 2\n(2020), 2053951720935141. https://doi.org/10.1177/205 3951720935141\n[30] Benedetta Brevini. 2023. Myths, techno solutionism an d artiﬁcial intelligence: reclaiming AI materiality and it s massive environmental costs. In\nHandbook of Critical Studies of Artiﬁcial Intelligence . Edward Elgar Publishing, 869–877.\n[31] Alexander EI Brownlee, Jason Adair, Saemundur O Harald sson, and John Jabbo. 2021. Exploring the accuracy–energy t rade-oﬀ in machine\nlearning. In 2021 IEEE/ACM International Workshop on Genetic Improveme nt (GI). IEEE, 11–18.\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 17\n[32] Robert D. Bullard, Glenn S. Johnson, and Beverly H. Wrig ht. 1997. Confronting Environmental Injustice: It’s the Ri ght Thing to Do. Race, Gender\n& Class 5, 1 (1997), 63–79.\n[33] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: I ntersectional accuracy disparities in commercial gender c lassiﬁcation. In Conference\non fairness, accountability and transparency . PMLR, 77–91.\n[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Fr anch, and Justus Bogner. 2023. Exploring the Carbon Footprint of Hugging Face’s ML Models:\nA Repository Mining Study. arXiv preprint arXiv:2305.11164 (2023).\n[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelin a Wang. 2021. The limits of global inclusion in AI developmen t. arXiv preprint\narXiv:2102.01265 (2021).\n[36] Kyla Chasalow and Karen Levy. 2021. Representativenes s in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency . 77–89.\n[37] Rachel Chason and Rael Ombuor. 2021. A lack of weather da ta in Africa is thwarting critical climate research.\nhttps://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/\n[38] Chinese Data Law Alliance. 2023. Chinese Artiﬁcial Int elligence Law, v. 1.0. https://mp.weixin.qq.com/s/85D8T jMkN9Tl-oWjq15JiQ\n[39] Concil of EU. 2022. Artiﬁcial Intelligence Act: Counci l calls for promoting safe AI that respects fundamental righ ts.\nhttps://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artiﬁcial-intelligence-act-council- calls-for-promoting-safe-ai-that-respects-fundament al-rights/\n[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy B uolamwini. 2022. Who Audits the Auditors? Recommendations from a ﬁeld scan of the\nalgorithmic auditing ecosystem. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency. 1571–1583.\n[41] Josh Cowls, Andreas Tsamados, Mariarosaria Taddeo, an d Luciano Floridi. 2023. The AI gambit: leveraging artiﬁcial intelligence to combat climate\nchange—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.\n[42] Kate Crawford and Trevor Paglen. 2021. Excavating AI: T he politics of images in machine learning training sets. Ai & Society 36, 4 (2021),\n1105–1116.\n[43] Rowena Cullen. 2001. Addressing the digital divide. Online information review 25, 5 (2001), 311–320.\n[44] Amane Dannouni, Stefan A. Deutscher, Ghita Dezzaz, Ada m Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid\nMaher, David Patterson, Edmond Rhys Jones, Juliet Rothenbe rg, Hamza Tber, Maud Texier, and Ali Ziat. 2023. Acceleratin g Climate Action with\nAI. https://www.gstatic.com/gumdrop/sustainability/a ccelerating-climate-action-ai.pdf\n[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur. 2022. Recommendations for ethical and responsible use of artiﬁcial intelligence in digital\nagriculture. Frontiers in Artiﬁcial Intelligence 5 (2022), 884192.\n[46] Hannah Davis. 2020. A Dataset is a Worldview. Towards Data Science (2020).\n[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, an d Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical im age database. In 2009 IEEE\nconference on computer vision and pattern recognition . IEEE, 248–255.\n[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Sol daini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Gho sh, Kyra Yee, Irene Font\nPeradejordi, et al. 2023. Bound by the Bounty: Collaborativ ely Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023\nAAAI/ACM Conference on AI, Ethics, and Society . 375–386.\n[49] Fatma Denton. 2002. Climate change vulnerability, imp acts, and adaptation: Why does gender matter? Gender & Development 10, 2 (2002), 10–20.\n[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[51] Nicholas Diakopoulos. 2020. Accountability, transpa rency, and algorithms. The Oxford handbook of ethics of AI 17, 4 (2020), 197.\n[52] Roel Dobbe and Meredith Whittaker. 2019. AI and climate change: how they’re connected, and what we can do about it. AI Now Institute 17\n(2019).\n[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Er ika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Lu ccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carb on intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency. 1877–1894.\n[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew , Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Docu-\nmenting large webtext corpora: A case study on the colossal c lean crawled corpus. arXiv preprint arXiv:2104.08758 (2021).\n[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma. 2018. Neo-colonialism and Millennium Development Goals (M DGs) in Africa: A\nblend of an old wine in a new bottle. African Journal of Science, Technology, Innovation and Dev elopment 10, 3 (2018), 355–366.\n[56] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towar ds Social Transparency\nin AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Com puting Systems (Yokohama, Japan) (CHI ’21) . Association for\nComputing Machinery, New York, NY, USA, Article 82, 19 pages . https://doi.org/10.1145/3411764.3445188\n[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market im pact potential of\nlarge language models. arXiv preprint arXiv:2303.10130 (2023).\n[58] Virginia Eubanks. 2018. Automating inequality: How high-tech tools proﬁle, police , and punish the poor . St. Martin’s Press.\n[59] Executive Oﬃce of the President, Oﬃce of Management and Budget. 2024. Memorandum for the heads of executive depart-\nments and agencies on Advancing Governance, Innovation, an d Risk Management for Agency Use of Artiﬁcial Intelligence.\nhttps://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk -Management-for-Agency-Use-of-Artiﬁcial-Intelligenc e.pdf\n\n18 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[60] Florian Eyert and Paola Lopez. 2023. Rethinking Transp arency as a Communicative Constellation. In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ’23) . Association for Computing Machinery, New York, NY, USA, 44 4–454.\nhttps://doi.org/10.1145/3593013.3594010\n[61] Sophia Falk and Aimee van Wynsberghe. 2023. Challengin g AI for Sustainability: what ought it mean? AI and Ethics (2023), 1–11.\n[62] Alec Feinberg. 2022. Solar Geoengineering Modeling an d Applications for Mitigating Global Warming: Assessing Ke y Parameters and the Urban\nHeat Island Inﬂuence. Frontiers in Climate 4 (2022), 870071.\n[63] H. Felzmann, E. Fosch-Villaronga, C. Lutz, et al. 2020. Towards Transparency by Design for Artiﬁcial Intelligence . Science and Engineering Ethics\n26 (2020), 3333–3361. https://doi.org/10.1007/s11948-0 20-00276-4\n[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Lu etge, Robert Madelin, Ugo\nPagallo, Francesca Rossi, et al. 2018. AI4People—an ethica l framework for a good AI society: opportunities, risks, principles, and recommendations.\nMinds and machines 28 (2018), 689–707.\n[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Ven katasubramanian. 2021. The (im) possibility of fairness: D iﬀerent value systems require\ndiﬀerent mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136–143.\n[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda As kell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Ni cholas Schiefer, Ka-\nmal Ndousse, et al. 2022. Red teaming language models to redu ce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint\narXiv:2209.07858 (2022).\n[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, S id Black, Anthony DiPoﬁ, Charles Foster, Laurence Golding, Jeﬀrey Hsu, Alain\nLe Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoﬀ, Chr is Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wa ng, and Andy Zou. 2023. A framework for few-shot language mod el evaluation.\nhttps://doi.org/10.5281/zenodo.10256836\n[68] Timnit Gebru. 2019. Oxford handbook on AI ethics book ch apter on race and gender. arXiv preprint arXiv:1908.06165 (2019).\n[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Je nnifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Ka te Crawford. 2021.\nDatasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n[70] Sergio Genovesi and Julia Maria Mönig. 2022. Acknowled ging Sustainability in the Framework of Ethical Certiﬁcati on for AI. Sustainability 14,\n7 (2022). https://doi.org/10.3390/su14074157\n[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, And rew L Beam, Irene Y Chen, and Rajesh Ranganath. 2020. A review of challenges and\nopportunities in machine learning for health. AMIA Summits on Translational Science Proceedings 2020 (2020), 191.\n[72] Amandeep S Gill and Stefan Germann. 2022. Conceptual an d normative approaches to AI governance for a global digital ecosystem supportive\nof the UN Sustainable Development Goals (SDGs). AI and Ethics 2, 2 (2022), 293–301.\n[73] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: De biasing methods cover up systematic gender biases in word em beddings but do not\nremove them. arXiv preprint arXiv:1903.03862 (2019).\n[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 20 16. Deep learning. MIT press.\n[75] Google. 2022. Carbon free energy for Google Cloud regio ns. https://cloud.google.com/sustainability/region-c arbon.\n[76] Green Software Foundation. 2023. Can AI be Truly Green? https://greensoftware.foundation/articles/can-ai-tr uly-be-green\n[77] GreenPeace. 2020. Oil in the Cloud: How Tech Companies a re Helping Big Oil Proﬁt from Climate Destructions.\nhttps://www.greenpeace.org/usa/reports/oil-in-the-c loud/\n[78] Odd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the art: Reproducibility in artiﬁcial intelligence. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, Vol. 32.\n[79] Thilo Hagendorﬀ. 2022. A virtue-based framework to sup port putting AI ethics into practice. Philosophy & Technology 35, 3 (2022), 55.\n[80] Benjamin Haibe-Kains et al. 2020. Transparency and rep roducibility in artiﬁcial intelligence. Nature 586, 7829 (2020), E14–E16.\n[81] Karen Hao. 2022. Artiﬁcial intelligence is creating a n ew colonial world order. MIT Technology Review (2022).\n[82] Karen Hao. 2022. A new vision of artiﬁcial intelligence for the people. MIT Technology Review (2022).\n[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Na gel. 2023. Beware of sustainable AI! Uses and abuses of a wort hy goal. AI and Ethics\n(2023), 1–12.\n[84] Martin Hilbert. 2016. Big data for development: A revie w of promises and challenges. Development Policy Review 34, 1 (2016), 135–174.\n[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengi o, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadﬁeld,\nMargaret Levi, et al. 2023. International institutions for advanced AI. arXiv preprint arXiv:2307.04699 (2023).\n[86] Mél Hogan. 2018. Big data ecologies. Ephemera 18, 3 (2018), 631.\n[87] Wayne Holmes and Ilkka Tuomi. 2022. State of the art and p ractice in AI in education. European Journal of Education 57, 4 (2022), 542–570.\n[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Ben gio, and Emily Denton. 2020. Characterising bias in compres sed models. arXiv preprint\narXiv:2010.03058 (2020).\n[89] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)Fairness: Lessons for Machine Learning. In Proceedings of the Conference\non Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) . Association for Computing Machinery, New York, NY, USA, 49 –58.\nhttps://doi.org/10.1145/3287560.3287600\n[90] Strategic Imperatives. 1987. Report of the World Commi ssion on Environment and Development: Our common future. Accessed Feb 10 (1987),\n1–300.\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 19\n[91] Innovation, Science and Economic Development Canada. 2023. The Artiﬁcial Intelligence and Data Act (AIDA) – Compa nion document.\nhttps://ised-isde.canada.ca/site/innovation-better- canada/en/artiﬁcial-intelligence-and-data-act-aida- companion-document\n[92] Yacine Jernite et al. 2022. Data governance in the age of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference\non Fairness, Accountability, and Transparency . ACM.\n[93] W Stanley Jevons. 1866. The coal question. In The Economics of Population . Routledge, 193–204.\n[94] Anna Jobin, Marcello Ienca, and Eﬀy Vayena. 2019. The gl obal landscape of AI ethics guidelines. Nature machine intelligence 1, 9 (2019), 389–399.\n[95] LN Joppa, Brian O’Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoﬀmann, James EM Watson, Stuart HM Butchart, Malika\nVirah-Sawmy, Benjamin S Halpern, et al. 2016. Filling in bio diversity threat gaps. Science 352, 6284 (2016), 416–418.\n[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiy a, Felix Creutzig, and David Rolnick. 2022. Aligning artiﬁcial intelligence with climate\nchange mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Meg an Mills-Novoa, Elizabeth Shoﬀner, and Katherine Siegel. 2 022. Biases and limitations\nof Global Forest Change and author-generated land cover map s in detecting deforestation in the Amazon. PLoS One 17, 7 (2022), e0268970.\n[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Fost er. 2021. Reduced, reused and recycled: The life of a dataset in machine learning\nresearch. arXiv preprint arXiv:2112.01716 (2021).\n[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Mich ael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Mich ihiro Yasunaga,\nRichard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benc hmark of in-the-wild distribution shifts. In International Conference on Machine\nLearning. PMLR, 5637–5664.\n[100] Julian Kuehnert, Deborah McGlynn, Sekou L. Remy, Aish a Walcott-Bryant, and Anne Jones. 2022. Surrogate Ensemble Forecasting for Dynamic\nClimate Impact Models. arXiv:2204.05795 [cs.LG]\n[101] Maciej Kuziemski and Gianluca Misuraca. 2020. AI gove rnance in the public sector: Three tales from the frontiers o f automated decision-making\nin democratic settings. Telecommunications policy 44, 6 (2020), 101976.\n[102] Travis LaCroix and Alexandra Sasha Luccioni. 2022. Me taethical perspectives on’Benchmarking’AI ethics. arXiv preprint arXiv:2204.05151 (2022).\n[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhan g, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto . 2023. When Do Pre-\nTraining Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics , Andreas Vlachos and Isabelle Augenstein (Eds.). Associat ion for Computational Linguis-\ntics, Dubrovnik, Croatia, 3206–3219. https://doi.org/10 .18653/v1/2023.eacl-main.234\n[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskove c. 2016. Interpretable decision sets: A joint framework for description and prediction.\nIn Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 1675–1684.\n[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jur e Leskovec. 2019. Faithful and customizable explanations o f black box models. In\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, an d Society. 131–138.\n[106] Julian Lamont (Ed.). 2017. Distributive Justice. Routledge.\n[107] S. Larsson and F. Heintz. 2020. Transparency in artiﬁc ial intelligence. Internet Policy Review 9, 2 (2020). https://doi.org/10.14763/2020.2.1469\n[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elis abeth Cross, Alessandro Parente, and Ricardo Vinuesa. 2023. Improving aircraft performance\nusing machine learning: a review. Aerospace Science and Technology (2023), 108354.\n[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva. 2019. Ethical framework for designing autonomous intellig ent systems. Journal of Open\nInnovation: Technology, Market, and Complexity 5, 1 (2019), 18.\n[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Ka ipulaumakaniolono Baker, Scott Benesiinaabandan, Michel le Brown, Melanie Cheung,\nMeredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artiﬁcial intelligence position p aper. (2020).\n[111] Chenyang Li. 2013. The Confucian philosophy of harmony . Vol. 10. Routledge.\n[112] Mochen Liao, Kai Lan, and Yuan Yao. 2022. Sustainabili ty implications of artiﬁcial intelligence in the chemical i ndustry: A conceptual framework.\nJournal of industrial ecology 26, 1 (2022), 164–182.\n[113] Zachary C Lipton. 2018. The mythos of model interpreta bility: In machine learning, the concept of interpretability is both important and slippery.\nQueue 16, 3 (2018), 31–57.\n[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar J oshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, a nd Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining appro ach. arXiv preprint arXiv:1907.11692 (2019).\n[115] Kirsten Lloyd. 2018. Bias ampliﬁcation in artiﬁcial i ntelligence systems. arXiv preprint arXiv:1809.07842 (2018).\n[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors inﬂuencing the em issions of machine learning.\narXiv preprint arXiv:2302.08476 (2023).\n[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma St rubell. 2023. Power Hungry Processing: Watts Driving the Co st of AI Deployment?\narXiv:2311.16863 [cs.LG]\n[118] Alexandra Sasha Luccioni and Anna Rogers. 2023. Mind y our Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice.\narXiv preprint arXiv:2308.07120 (2023).\n[119] Alexandra Sasha Luccioni and David Rolnick. 2023. Bug s in the data: How ImageNet misrepresents biodiversity. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , Vol. 37. 14382–14390.\n[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Cra wford. 2025. From Eﬃciency Gains to Rebound Eﬀects: The Prob lem of Jevons’ Paradox\nin AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025).\n\n20 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne- Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n[122] Federica Lucivero. 2020. Big data, big waste? A reﬂect ion on the environmental sustainability of big data initiatives. Science and engineering ethics\n26, 2 (2020), 1009–1030.\n[123] Amanda H Lynch and Siri Veland. 2018. Urgency in the Anthropocene. MIT Press.\n[124] Mirca Madianou. 2021. Nonhuman humanitarianism: whe n’AI for good’can be harmful. Information, Communication & Society 24, 6 (2021),\n850–868.\n[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi,\nTee Garnett, et al. 2023. What’s fair is. . . fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in\nthe integration of clinical machine learning: JustEFAB. In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency.\n1505–1519.\n[126] Duncan P McLaren. 2018. Whose climate and whose ethics ? Conceptions of justice in solar geoengineering modelling . Energy research & social\nscience 44 (2018), 209–221.\n[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, K ristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning.\nACM computing surveys (CSUR) 54, 6 (2021), 1–35.\n[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Mu ratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Sca ling deep learning\nfor materials discovery. Nature 624, 7990 (2023), 80–85.\n[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. 2021. Algorithmic Impact Assessments and\nAccountability: The Co-Construction of Impacts. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual\nEvent, Canada) (FAccT ’21) . Association for Computing Machinery, New York, NY, USA, 73 5–746. https://doi.org/10.1145/3442188.3445935\n[130] Rachel Metz. 2024. Google’s Emissions Shot Up 48% Over Five Years Due to AI. Bloomberg (2024).\n[131] Thaddeus Metz and Scott C. Miller. 2016. Relational et hics. In The international encyclopedia of ethics . 1–10.\n[132] Milagros Miceli, Julian Posada, and Tianling Yang. 20 22. Studying up machine learning data: Why talk about bias wh en we mean power?\nProceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.\n[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija. 2019. Ethical AI for the Governance of the Society: Ch allenges and Opportunities.\nIn CEUR Workshop Proceedings, Vol. 2505. 20–26.\n[134] John Stuart Mill. 1863. Utilitarianism. Parker, Son, and Bourn.\n[135] David Miller and Michael Walzer (Eds.). 1995. Pluralism, Justice, and Equality . Oxford University Press.\n[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Em ily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Ja mie Morgenstern. 2020.\nDiversity and inclusion metrics in subset selection. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Soc iety. 117–123.\n[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parke r Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In ioluwa Deborah Raji, and\nTimnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.\n[138] Shakir Mohamed, Marie-Therese Png, and William Isaac . 2020. Decolonial AI: Decolonial theory as sociotechnical foresight in artiﬁcial intelli-\ngence. Philosophy & Technology 33 (2020), 659–684.\n[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and L uciano Floridi. 2023. Auditing large language models: a thr ee-layered approach. AI and\nEthics (2023), 1–31.\n[140] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.\n[141] Steven Gonzalez Monserrate. 2022. The cloud is materi al: On the environmental impacts of computation and data sto rage. (2022).\n[142] L. Munn. 2022. The Uselessness of AI Ethics. AI Ethics (2022). https://doi.org/10.1007/s43681-022-00209-w\n[143] Luke Munn. 2023. The ﬁve tests: designing and evaluati ng AI according to indigenous M ¯aori principles. AI & SOCIETY (2023), 1–9.\n[144] Patrick Murmann and Simone Fischer-Hübner. 2017. Usa ble transparency enhancing tools: A literature review. (20 17).\n[145] Mechthild Nagel. 2022. Ludic ubuntu ethics: Decolonizing justice . Taylor & Francis.\n[146] National Academies of Sciences, Engineering and Medi cine. 2019. Reproducibility and replicability in science. (2019).\n[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing Info rmation Disclosures to Support\nMeaningful Transparency and Accountability. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul,\nRepublic of Korea) (FAccT ’22) . Association for Computing Machinery, New York, NY, USA, 67 9–690. https://doi.org/10.1145/3531146.3533133\n[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Lu ke Abraham, John Pyle, and Apostolos Voulgarakis. 2018. Usi ng machine learning to\nbuild temperature-based ozone parameterizations for clim ate sensitivity simulations. Environmental Research Letters 13, 10 (2018), 104016.\n[149] Victor Ordonez, Taylor Dunn, and Eric Noll. 2023. Open AI CEO Sam Altman says AI will reshape society, acknowledges risks:‘A little bit scared\nof this’. (2023). https://abcnews.go.com/Technology/op enai-ceo-sam-altman-ai-reshapesociety-acknowledges/ story\n[150] Friederike Otto. 2023. Without Warning: A Lack of Weat her Stations Is Costing African Lives.\nhttps://e360.yale.edu/features/africa-weather-stati ons-climate-change\n[151] Edward A Page. 2008. Distributing the burdens of clima te change. Environmental Politics 17, 4 (2008), 556–575.\n[152] European Parliament. 2023. Artiﬁcial Intelligence A ct: deal on comprehensive rules for trustworthy AI. (2023).\nhttps://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artiﬁcial-intelligence-act-deal-on- comprehensive-rules-for-trustworthy-ai\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 21\n[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang , Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Te xier, and Jeﬀ Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jerni te, and Margaret Mitchell. 2023. Stronger Together: on the A rticulation of Ethical Charters,\nLegal Tools, and Technical Documentation in ML. In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency (<conf-\nloc>, <city>Chicago</city>, <state>IL</state>, <countr y>USA</country>, </conf-loc>) (FAccT ’23) . Association for Computing Machinery, New\nYork, NY, USA, 343–354. https://doi.org/10.1145/3593013 .3594002\n[155] Erich Prem. 2023. From ethical AI frameworks to tools: a review of approaches. AI and Ethics 3, 3 (2023), 699–716.\n[156] Edward Raﬀ. 2019. A Step Toward Quantifying Independe ntly Reproducible Machine Learning Research. arXiv:1909. 06674 [cs.LG]\n[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Pa ullada, Emily Denton, and Alex Hanna. 2021. AI and the everyt hing in the whole wide\nworld benchmark. arXiv preprint arXiv:2111.15366 (2021).\n[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buo lamwini. 2023. Change from the outside: Towards credible th ird-party audits of ai\nsystems. Missing links in AI governance (2023), 5.\n[159] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, et al. 2020. Closing the AI accountability ga p: Deﬁning an end-to-end\nframework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountabil ity, and Transparency. ACM, 33–44.\n[160] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Soc ial-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic\nAudits. In 2023 ACM Conference on Fairness, Accountability, and Trans parency. ACM. https://doi.org/10.1145/3593013.3594014\n[161] Akshat Rathi and Dina Bass. 2024. Microsoft’s AI Push I mperils Climate Goal as Carbon Emissions Jump 30%. Bloomberg (2024).\n[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Al i Farhadi. 2016. You Only Look Once: Uniﬁed, Real-Time Objec t Detection.\narXiv:1506.02640 [cs.CV]\n[163] Jennifer Rhee. 2018. The robotic imaginary: The human and the price of dehumanize d labor. U of Minnesota Press.\n[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestri n. 2016. Model-agnostic interpretability of machine learn ing. arXiv preprint\narXiv:1606.05386 (2016).\n[165] Paola Ricaurte. 2022. Ethics for the majority world: A I and the question of violence at scale. Media, Culture & Society 44, 4 (2022), 726–745.\n[166] Anna Rogers. 2021. Changing the world by changing the d ata. arXiv preprint arXiv:2105.13947 (2021).\n[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranja n Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2 022. The Dollar Street\ndataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track .\n[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kocha nski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross , Nikola Milojevic-Dupont,\nNatasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022),\n1–96.\n[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Pa trick Esser, and Björn Ommer. 2022. High-Resolution Image S ynthesis With Latent\nDiﬀusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision an d Pattern Recognition (CVPR) . 10684–10695.\n[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Eli zabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin , Hannah Tait Neufeld,\nAndrew Nixon, et al. 2019. Automated pastures and the digita l divide: How agricultural technologies are shaping labour and rural communities.\nJournal of Rural Studies 68 (2019), 112–122.\n[171] Mark Ryan. 2022. The social and ethical impacts of arti ﬁcial intelligence in agriculture: mapping the agricultur al AI literature. AI & SOCIETY\n(2022), 1–13.\n[172] Salesforce. 2024. Sustainable AI Policy Principles. https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-susta inable-ai-policy-principles.pdf\n[173] Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “Eve ryone wants to do the\nmodel work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Co mputing\nSystems. 1–15.\n[174] Aaron Sankin and Surya Mattu. 2023. Predictive Polici ng Software Terrible At Predicting Crimes.\nhttps://themarkup.org/prediction-bias/2023/10/02/pr edictive-policing-software-terrible-at-predicting-c rimes.\n[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Go ran Radanovic, David C. Parkes, and Yang Liu. 2019. How Do Fai rness Deﬁnitions Fare?\nExamining Public Attitudes Towards Algorithmic Deﬁnitions of Fairness. InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\n(Honolulu, HI, USA) (AIES ’19) . Association for Computing Machinery, New York, NY, USA, 99 –106. https://doi.org/10.1145/3306618.3314248\n[176] David Schlosberg. 2012. Climate Justice and Capabili ties: A Framework for Adaptation Policy. Ethics & International Aﬀairs 26, 4 (2012), 445–461.\n[177] David Schlosberg and Lisette B Collins. 2014. From env ironmental to climate justice: climate change and the disco urse of environmental justice.\nWiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.\n[178] Jurgen Schmidhuber. 1991. Neural Sequence Chunkers. Technical Report FKI-148-91 (1991).\n[179] Christian Schroeder de Witt and Thomas Hornigold. 201 9. Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. arXiv\ne-prints (2019), arXiv–1905.\n[180] Paul Schütze. 2024. The Problem of Sustainable AI: A Cr itical Assessment of an Emerging Phenomenon. Weizenbaum Journal of the Digital\nSociety 4, 1 (2024).\n[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzio ni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.\n\n22 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalle ng Moorosi, and Richard Klein. 2021. Constructing a visual d ataset to study the eﬀects of\nspatial apartheid in South Africa. In Thirty-ﬁfth conference on neural information processing s ystems datasets and benchmarks track (round 2) .\n[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Sures h Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical\nsystems. In Proceedings of the conference on fairness, accountability, and transparency. 59–68.\n[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinc k, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes. 2023. L imits to the accurate\nand generalizable use of soundscapes to monitor biodiversi ty. Nature Ecology & Evolution 7, 9 (2023), 1373–1378.\n[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay C howdary. 2020. Machine learning applications for precision agriculture: A comprehensive\nreview. IEEE Access 9 (2020), 4843–4873.\n[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romai n Bosc, and Li Wenwei. 2017. Digital infrastructure: Overco ming the digital divide in\nemerging economies. G20 Insights 3 (2017), 1–36.\n[187] Ahmed AH Siddig. 2019. Why is biodiversity data-deﬁci ency an ongoing conservation dilemma in Africa? Journal for Nature Conservation 50\n(2019), 125719.\n[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Real ML: Recog nizing, exploring, and\narticulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency .\n587–597.\n[189] Irene Solaiman. 2023. The gradient of generative AI re lease: Methods and considerations. In Proceedings of the 2023 ACM Conference on Fairness,\nAccountability, and Transparency. 111–122.\n[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahm ad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dod ge, Ellie Evans, Sara\nHooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and\nApostol Vassilev. 2023. Evaluating the Social Impact of Gen erative AI Systems in Systems and Society. arXiv:2306.0594 9 [cs.CY]\n[191] Maddie Stone. 2024. Microsoft employees spent years ﬁ ghting the tech giant’s oil ties. Now, they’re speaking out.\nhttps://grist.org/accountability/microsoft-employees-spent-years-ﬁghting-the-tech-giants-oil-ties-now- theyre-speaking-out/\n[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 20 19. Energy and policy considerations for deep learning in NL P. arXiv preprint\narXiv:1906.02243 (2019).\n[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Serm anet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincen t Vanhoucke, and Andrew\nRabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pat tern recognition. 1–9.\n[194] Sy Taﬀel, Laura Bedford, and Monique Mann. 2019. Ecoci de isn’t ethical: Political ecology and capitalist AI ethic s. ECONOMIES OF VIRTUE 20\n(2019), 58.\n[195] Petros Terzis. 2020. Onward for the Freedom of Others: Marching beyond the AI Ethics. In Proceedings of the 2020 Conference on Fair-\nness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20) . Association for Computing Machinery, New York, NY, USA, 22 0–229.\nhttps://doi.org/10.1145/3351095.3373152\n[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre. 2017. Taxonom ic bias in biodiversity data and\nsocietal preferences. Scientiﬁc reports 7, 1 (2017), 9132.\n[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakale mbe, and Hannah Kerner. 2021. CropHarvest: A global dataset for crop-\ntype classiﬁcation. In Thirty-ﬁfth Conference on Neural Information Processing Sy stems Datasets and Benchmarks Track (Round 2) .\nhttps://openreview.net/forum?id=JtjzUXPEaCu\n[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshav a, Patricia Nying’uro, Herbert Misiani, Jemimah G Ongoma, Y oav Yair, Yacob Mulugetta,\nand Amadou T Gaye. 2023. How to reduce Africa’s undue exposur e to climate risks. Nature 620, 7974 (2023), 488–491.\n[199] UN General Assembly. 2015. Transforming our world : th e 2030 Agenda for Sustainable Development.\nhttps://www.refworld.org/legal/resolution/unga/2015/en/111816\n[200] UNESCO. 2021. Recommendation on the ethics of artiﬁci al intelligence.\n[201] Prasetya Ajie Utama, Naﬁse Sadat Moosavi, and Iryna Gu revych. 2020. Towards debiasing NLU models from unknown bia ses. arXiv preprint\narXiv:2009.12303 (2020).\n[202] I. van de Poel. 2020. Embedding Values in Artiﬁcial Int elligence (AI) Systems. Minds & Machines 30 (2020), 385–409.\nhttps://doi.org/10.1007/s11023-020-09537-4\n[203] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sus tainability and the sustainability of AI. AI and Ethics 1, 3 (2021), 213–218.\n[204] GaÃG , l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker. 2024. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm\nin AI. arXiv preprint arXiv:2409.14160 (2024).\n[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexa ndre Polvora. 2020. Societal and ethical impacts of artiﬁci al intelligence: Critical notes on\nEuropean policy frameworks. Telecommunications Policy 44, 6 (2020), 101961.\n[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Ma deline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. The role of art iﬁcial intelligence in achieving the Sustainable Developm ent Goals. Nature\ncommunications 11, 1 (2020), 1–10.\n[207] Johannes M Waldmueller. 2015. Agriculture, knowledg e and the ‘colonial matrix of power’: approaching sustainab ilities from the Global South.\nJournal of Global Ethics 11, 3 (2015), 294–302.\n\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 23\n[208] J. Walmsley. 2021. Artiﬁcial Intelligence and the Val ue of Transparency. AI & Society 36 (2021), 585–595.\nhttps://doi.org/10.1007/s00146-020-01066-z\n[209] Laura Weidinger, Kevin R. McKee, Richard Everett, Saﬀ ron Huang, Tina O. Zhu, Martin J. Chadwick, Christopher Summ erﬁeld, and Iason Gabriel.\n2023. Using the Veil of Ignorance to align AI systems with pri nciples of justice. Proceedings of the National Academy of Sciences 120, 18 (2023),\ne2213709120. https://doi.org/10.1073/pnas.2213709120 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120\n[210] Craig R White, Dustin J Marshall, Steven L Chown, Susan a Clusella-Trullas, Steven J Portugal, Craig E Franklin, an d Frank Seebacher. 2021.\nGeographical bias in physiological data limits prediction s of global change impacts. Functional Ecology 35, 7 (2021), 1572–1578.\n[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru . 2022. The exploited labor behind artiﬁcial intelligence. Noema Magazine 13 (2022).\n[212] T. Wischmeyer. 2020. Artiﬁcial Intelligence and Tran sparency: Opening the Black Box. In Regulating Artiﬁcial Intelligence. Springer, 75–101.\n[213] Robert Wolfe and Aylin Caliskan. 2022. American== whi te in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society . 800–812.\n[214] BigScience Workshop, Teven Le Scao, Angela Fan, Chris topher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha\nLuccioni, François Yvon, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Imp lications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[216] D. Zeng. 2015. AI Ethics: Science Fiction Meets Techno logical Reality. IEEE Intelligent Systems 30, 03 (may 2015), 2–5.\nhttps://doi.org/10.1109/MIS.2015.53\n[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, L aurent El Ghaoui, and Michael Jordan. 2019. Theoretically p rincipled trade-oﬀ between\nrobustness and accuracy. In International conference on machine learning . PMLR, 7472–7482.\n[218] Aram Ziai. 2016. Development discourse and global history: From colonialis m to the sustainable development goals . Taylor & Francis.\n[219] Ofer Zwikael and John Smyrk. 2015. Project governance : Balancing control and trust in dealing with risk. International Journal of Project\nManagement 33, 4 (2015), 852–862.", "metadata": {"url": "https://arxiv.org/pdf/2504.00797", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "arXiv:2504.00797v1  [cs.CY]  1 Apr 2025\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI R esearch and\nPractice\nALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France\nRAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho\nAs the possibilities for Artiﬁcial Intelligence (AI) have grown, so hav e concerns regarding its impacts on society and the environment.\nHowever, these issues are often raised separately; i.e. carbonfootprint analyses of AI models typically do not consider how the pursuit\nof scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately\nharmful to the environment. On the other hand, model audits that aim t o evaluate model performance and disparate impacts mostly\nfail to engage with the environmental ramiﬁcations of AI models and how th ese ﬁt into their auditing approaches. In this separation,\nboth research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential\nsolutions for making informed choices that can be developed at their c onvergence. In this essay, we build upon work carried out in AI\nand in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such\nas generalizability, transparency, evaluation and equity across AI r esearch and practice. We argue that the eﬀorts aiming to study\nAI’s ethical ramiﬁcations should be made in tandem with those evalua ting its impacts on the environment, and we conclude with a\nproposal of best practices to better integrate AI ethics and susta inability in AI research and practice.\n1 INTRODUCTION\nIn recent years, AI systems have become pervasive, presente d as a key tool in the ﬁght against climate change [168]\nand in societally-beneﬁcial domains such as health and educ ation [64, 71, 87]. However, the training and deployment\nof AI systems also comes with a cost in terms of energy and natu ral resources [52, 117, 192] and can inadvertently\nresult in the ampliﬁcation of inequalities [183] and prolif eration of biases [6] when systems are put into practice.\nHistorically, the societal and environmental impacts of AI systems have been addressed separately, in two distinct\nareas of study – i.e. scholarship that aims to address the eth ics of AI models focuses on aspects such as bias evaluation\nor auditing [33, 201], typically overlooking models’ impac ts on natural resources and ecosystems [127]. Conversely,\nwork that aims to estimate the growing carbon footprint and e nergy consumption of AI models [121, 192] does not\ntypically address the contribution this has towards shifti ng the balance of power or amplifying inequalities [1, 2].\nSome recent scholarship has started to establish explicit l inks between ethics and sustainability often hones in on\nspeciﬁc applications, such as the emblematic \"Stochastic P arrots\" paper, which addresses both sustainability and ethics\nas issues in the context of large language models (LLMs) [16] . Apart from this exception and a precious few others,\nconversations around AI ethics and sustainability have tak en place separately, in diﬀerent venues and by diﬀerent sets\nof stakeholders. However, since in both AI sustainability a nd AI ethics research, the aim is to think through how we\nmight develop technologies which are useful and inclusive w hile we limit harm to people and environments, similar\nthemes emerge, often structured via concepts such as functi onality and eﬃciency just as much as justice, fairness and\nequity. In both of these disciplines.\nIn the present article, we highlight these transversal them es and argue that both the societal and environmental\nimpacts of AI systems should be considered in parallel in all aspects of AI theory, practice and governance. Drawing\nupon work from within the AI community as well as related ﬁeld s, we explore a series of use cases illustrating that\nAuthors’ addresses: Alexandra Sasha Luccioni; Giada Pisti lli, sasha.luccioni@huggingface.co, Hugging Face, Canad a/France; Raesetje Sefala; Nyalleng\nMoorosi, Distributed AI Research Institute, Canada/Lesot ho.\n1", "sentences": [{"text": "arXiv:2504.00797v1  [cs.CY]  1 Apr 2025\nBridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI R esearch and\nPractice\nALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France\nRAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho\nAs the possibilities for Artiﬁcial Intelligence (AI) have grown, so hav e concerns regarding its impacts on society and the environment.", "metadata": {}}, {"text": "However, these issues are often raised separately;", "metadata": {}}, {"text": "i.e.", "metadata": {}}, {"text": "carbonfootprint analyses of AI models typically do not consider how the pursuit\nof scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately\nharmful to the environment.", "metadata": {}}, {"text": "On the other hand, model audits that aim t o evaluate model performance and disparate impacts mostly\nfail to engage with the environmental ramiﬁcations of AI models and how th ese ﬁt into their auditing approaches.", "metadata": {}}, {"text": "In this separation,\nboth research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential\nsolutions for making informed choices that can be developed at their c onvergence.", "metadata": {}}, {"text": "In this essay, we build upon work carried out in AI\nand in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such\nas generalizability, transparency, evaluation and equity across AI r esearch and practice.", "metadata": {}}, {"text": "We argue that the eﬀorts aiming to study\nAI’s ethical ramiﬁcations should be made in tandem with those evalua ting its impacts on the environment, and we conclude with a\nproposal of best practices to better integrate AI ethics and susta inability in AI research and practice.", "metadata": {}}, {"text": "1 INTRODUCTION\nIn recent years, AI systems have become pervasive, presente d as a key tool in the ﬁght against climate change [168]\nand in societally-beneﬁcial domains such as health and educ ation [64, 71, 87].", "metadata": {}}, {"text": "However, the training and deployment\nof AI systems also comes with a cost in terms of energy and natu ral resources [52, 117, 192] and can inadvertently\nresult in the ampliﬁcation of inequalities [183] and prolif eration of biases [6] when systems are put into practice.", "metadata": {}}, {"text": "Historically, the societal and environmental impacts of AI systems have been addressed separately, in two distinct\nareas of study – i.e.", "metadata": {}}, {"text": "scholarship that aims to address the eth ics of AI models focuses on aspects such as bias evaluation\nor auditing [33, 201], typically overlooking models’ impac ts on natural resources and ecosystems [127].", "metadata": {}}, {"text": "Conversely,\nwork that aims to estimate the growing carbon footprint and e nergy consumption of AI models [121, 192] does not\ntypically address the contribution this has towards shifti ng the balance of power or amplifying inequalities [1, 2].", "metadata": {}}, {"text": "Some recent scholarship has started to establish explicit l inks between ethics and sustainability often hones in on\nspeciﬁc applications, such as the emblematic \"Stochastic P arrots\" paper, which addresses both sustainability and ethics\nas issues in the context of large language models (LLMs) [16] .", "metadata": {}}, {"text": "Apart from this exception and a precious few others,\nconversations around AI ethics and sustainability have tak en place separately, in diﬀerent venues and by diﬀerent sets\nof stakeholders.", "metadata": {}}, {"text": "However, since in both AI sustainability a nd AI ethics research, the aim is to think through how we\nmight develop technologies which are useful and inclusive w hile we limit harm to people and environments, similar\nthemes emerge, often structured via concepts such as functi onality and eﬃciency just as much as justice, fairness and\nequity.", "metadata": {}}, {"text": "In both of these disciplines.", "metadata": {}}, {"text": "In the present article, we highlight these transversal them es and argue that both the societal and environmental\nimpacts of AI systems should be considered in parallel in all aspects of AI theory, practice and governance.", "metadata": {}}, {"text": "Drawing\nupon work from within the AI community as well as related ﬁeld s, we explore a series of use cases illustrating that\nAuthors’ addresses: Alexandra Sasha Luccioni;", "metadata": {}}, {"text": "Giada Pisti lli, sasha.luccioni@huggingface.co, Hugging Face, Canad a/France;", "metadata": {}}, {"text": "Raesetje Sefala;", "metadata": {}}, {"text": "Nyalleng\nMoorosi, Distributed AI Research Institute, Canada/Lesot ho.", "metadata": {}}, {"text": "1", "metadata": {}}], "metadata": {"page": 1}}, {"text": "[Image page=1 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=1 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 1, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "2 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nethics and sustainability go hand-in-glove when it comes to the development and the deployment of AI systems. By\ndoing so, we hope to shed light on the importance of pursuing r esearch that blends together diﬀerent perspectives to\nallow for a better understanding of the societal impacts of A I systems.\nGiven that our goal is to allow our work to be read and understo od by a variety of audiences, we start, in Section §2,\nby deﬁning the concepts and terms that are core to our subsequ ent analysis. We continue by examining the current\nstate of AI ethics and sustainability from 3 diﬀerent but com plementary perspectives: theoretical principles and fram e-\nworks (§3.1), AI research and practice(§3.2), and AI regula tion and governance (§3.3). Next, in §4, we identify four\ntransversal themes that we have found to be particularly cen tral to both AI ethics and sustainability: generalizabilit y,\nevaluation, transparency, and power. Finally, in §5, we pro pose ways forward at the intersection of these themes and\nthree directions of AI research and practice. We wrap up the a rticle with ideas for future work that can be pursued at\nthe nexus of AI ethics and sustainability and a brief conclus ion.\n2 KEY CONCEPTS AND DEFINITIONS\nSustainability\nWhen it comes to the concept of sustainability, one of its ﬁrs t and most widely-accepted deﬁnitions originates from\nthe 1987 Brundtland Report, which deﬁnes sustainable devel opment as “development that meets the needs of the present\nwithout compromising the ability of future generations to m eet their own needs” [1987][p.41]. This deﬁnition remains\ncentral to the ﬁeld of sustainability write large, informin g frameworks such as the UN Sustainable Development Goals\n(SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [199].\nHowever, also in 1987, environmental economist Edward Barb ier proposed an alternative deﬁnition to sustainability,\nstructuring it around three pillars: environmental, socie tal and economic, arguing that sustainable development can\nonly be truly achieved when both environmental stewardship , social equity and economic viability coexist and are\ninter-connected [1987].\nIn the context of AI, the term ‘sustainability’ is most often used to refer solely to environmental sustainability [41,\n61]. The umbrella term ‘Sustainable AI’ was initially propo sed by van Wynsberghe as a ﬁeld of practice that both\naims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI\napproaches themselves [203]. This proposal would then enco mpass the vast variety of work being done at the nexus\nof machine learning and ﬁelds such as biodiversity monitori ng, agriculture, transportation, etc. (for a review, see [2 06]\nand [168]). AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable\nAI Workshop, that have put the emphasis on eﬃcient methods an d the application of AI to sustainability-related\nproblems, as well as the Tackling Climate Change with Machin e Learning workshop, which aims to demonstrate that\nAI can be an invaluable tool in helping society adapt to and mi tigate the eﬀects of climate change.\nEthics\nInherently characterized by ongoing perplexity, ethics as pires for certainty and consensus, yet also remains dynamic\nand evolving. It has its origins in the work of philosophers s uch as Aristotle, who posited that ethics connects theory\nwith praxis, with the goal of guiding human actions towards eudaimonia (i.e. the highest human good) [8]. In the mil-\nlennia since Aristotle, the philosophical sub-domain of ap plied ethics has sought to establish normative principles f or\na variety of human activities and domains, which inexorably depend on the context of application and the individuals", "sentences": [{"text": "2 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nethics and sustainability go hand-in-glove when it comes to the development and the deployment of AI systems.", "metadata": {}}, {"text": "By\ndoing so, we hope to shed light on the importance of pursuing r esearch that blends together diﬀerent perspectives to\nallow for a better understanding of the societal impacts of A I systems.", "metadata": {}}, {"text": "Given that our goal is to allow our work to be read and understo od by a variety of audiences, we start, in Section §2,\nby deﬁning the concepts and terms that are core to our subsequ ent analysis.", "metadata": {}}, {"text": "We continue by examining the current\nstate of AI ethics and sustainability from 3 diﬀerent but com plementary perspectives: theoretical principles and fram e-\nworks (§3.1), AI research and practice(§3.2), and AI regula tion and governance (§3.3).", "metadata": {}}, {"text": "Next, in §4, we identify four\ntransversal themes that we have found to be particularly cen tral to both AI ethics and sustainability: generalizabilit y,\nevaluation, transparency, and power.", "metadata": {}}, {"text": "Finally, in §5, we pro pose ways forward at the intersection of these themes and\nthree directions of AI research and practice.", "metadata": {}}, {"text": "We wrap up the a rticle with ideas for future work that can be pursued at\nthe nexus of AI ethics and sustainability and a brief conclus ion.", "metadata": {}}, {"text": "2 KEY CONCEPTS AND DEFINITIONS\nSustainability\nWhen it comes to the concept of sustainability, one of its ﬁrs t and most widely-accepted deﬁnitions originates from\nthe 1987 Brundtland Report, which deﬁnes sustainable devel opment as “development that meets the needs of the present\nwithout compromising the ability of future generations to m eet their own needs” [1987][p.41].", "metadata": {}}, {"text": "This deﬁnition remains\ncentral to the ﬁeld of sustainability write large, informin g frameworks such as the UN Sustainable Development Goals\n(SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [199].", "metadata": {}}, {"text": "However, also in 1987, environmental economist Edward Barb ier proposed an alternative deﬁnition to sustainability,\nstructuring it around three pillars: environmental, socie tal and economic, arguing that sustainable development can\nonly be truly achieved when both environmental stewardship , social equity and economic viability coexist and are\ninter-connected [1987].", "metadata": {}}, {"text": "In the context of AI, the term ‘sustainability’ is most often used to refer solely to environmental sustainability [41,\n61].", "metadata": {}}, {"text": "The umbrella term ‘Sustainable AI’ was initially propo sed by van Wynsberghe as a ﬁeld of practice that both\naims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI\napproaches themselves [203].", "metadata": {}}, {"text": "This proposal would then enco mpass the vast variety of work being done at the nexus\nof machine learning and ﬁelds such as biodiversity monitori ng, agriculture, transportation, etc.", "metadata": {}}, {"text": "(for a review, see [2 06]\nand [168]).", "metadata": {}}, {"text": "AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable\nAI Workshop, that have put the emphasis on eﬃcient methods an d the application of AI to sustainability-related\nproblems, as well as the Tackling Climate Change with Machin e Learning workshop, which aims to demonstrate that\nAI can be an invaluable tool in helping society adapt to and mi tigate the eﬀects of climate change.", "metadata": {}}, {"text": "Ethics\nInherently characterized by ongoing perplexity, ethics as pires for certainty and consensus, yet also remains dynamic\nand evolving.", "metadata": {}}, {"text": "It has its origins in the work of philosophers s uch as Aristotle, who posited that ethics connects theory\nwith praxis, with the goal of guiding human actions towards eudaimonia (i.e.", "metadata": {}}, {"text": "the highest human good) [8].", "metadata": {}}, {"text": "In the mil-\nlennia since Aristotle, the philosophical sub-domain of ap plied ethics has sought to establish normative principles f or\na variety of human activities and domains, which inexorably depend on the context of application and the individuals", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 3\ninvolved, leading to much debate regarding which norms shou ld be applicable in which contexts, as well as the deﬁni-\ntion of key ethical concepts such as fairness [89, 175], tran sparency [60, 147] and, indeed, the very deﬁnition of ethics\nitself [21, 195].\nLacking consensus, the ﬁeld of AI ethics often applies Weste rn moral theories ranging from utilitarianism [18, 134]\nto egalitarianism [209] and virtue ethics [7, 8] to propose w ays of assessing the ethicality of AI systems. However, the\napplication of these moral theories faces challenges given the diﬃculty of, e.g. quantifying the concept of utility in\nutilitarianism, assessing and comparing who is worse oﬀ in egalitarianism, or evaluating cultural variability in deﬁning\nvirtues in virtue ethics. This is also the case in terms of the application of these theories in modern-day contexts\ninvolving new types of AI-driven technologies such as robot s or autonomous vehicles, which can be limited without a\ncomprehensive understanding of both AI’s technical capabi lities (e.g. the limitations of the underlying models) as we ll\nas the diversity of life experiences of the people using thes e tools, who can interact with them in ways that are hard\nto predict or design for [102]. Given that these concepts enc ompass work from a multitude of domains that espouse\ndiﬀerent objectives, values and methods [23], their deﬁnit ion can have major consequences on the way in which\nthese concepts are operationalized in AI models and systems [10, 65, 183]. Work that addresses the ethical aspects of\nAI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a\ncross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.\n3 EXISTING SCHOLARSHIP IN AI ETHICS AND SUSTAINABILITY\nIn the sections below, we explore existing scholarship in or der to critically analyze how both ethics and sustainabilit y\nare deﬁned in theory and operationalized in practice within the diverse communities that pursue AI. First, we describe\nthe principles and frameworks that have been proposed to guide AI from both an ethical and environmental perspective;\nnext, we examine how these principles are applied, implicit ly and explicitly, in AI research and practice. Finally, we\nlook at several recent approaches for regulating and govern ing AI and the diﬀerent roles adopted by stakeholders and\norganizations.\n3.1 Principles and Frameworks\nA common starting point to ensure the ethical development an d deployment of AI systems is the deﬁnition of a struc-\nture for guiding this process, which can be operationalized via sets of principles or a framework. On the one hand,\nethical principles are often deﬁned at a high level, describ ing values and concepts outside of any speciﬁc context of\ndeployment. As such, multiple sets of guiding principles fo r ‘ethical AI’ have been proposed by diﬀerent organizations\nranging from research institutes to nonproﬁt and for-proﬁt entities. However, given the many diﬀerent types of AI\napproaches that exist, as well as contextual factors that in ﬂuence their application, it is diﬃcult to deﬁne universal, or\neven generalizable, guidelines. To this point, a 2019 analy sis by Jobin et al. reviewed 84 sets of guidelines mentioning\na variety of principles, ﬁnding very limited convergence be tween them but identifying the values of transparency,\nfairness, non-maleﬁcence, privacy and responsibility as b eing most common [2019]. Similarly, a multitude of ethical\nframeworks have been proposed, with most of them espousing s peciﬁc visions of ethics by putting an emphasis on\naspects ranging from human empowerment [64] to virtue ethic s [79]. In comparison to principles, ethical frameworks\noften aim to frame ethics from the perspective of implementa tion, identifying how challenges can be addressed and\nhow to build consensus around ethical values, often anchore d to speciﬁc contexts of deployment of AI systems, such\nas medicine [125] or autonomous vehicles [109]. However, th e technological mechanisms proposed to operationalize", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 3\ninvolved, leading to much debate regarding which norms shou ld be applicable in which contexts, as well as the deﬁni-\ntion of key ethical concepts such as fairness [89, 175], tran sparency [60, 147] and, indeed, the very deﬁnition of ethics\nitself [21, 195].", "metadata": {}}, {"text": "Lacking consensus, the ﬁeld of AI ethics often applies Weste rn moral theories ranging from utilitarianism [18, 134]\nto egalitarianism [209] and virtue ethics [7, 8] to propose w ays of assessing the ethicality of AI systems.", "metadata": {}}, {"text": "However, the\napplication of these moral theories faces challenges given the diﬃculty of, e.g.", "metadata": {}}, {"text": "quantifying the concept of utility in\nutilitarianism, assessing and comparing who is worse oﬀ in egalitarianism, or evaluating cultural variability in deﬁning\nvirtues in virtue ethics.", "metadata": {}}, {"text": "This is also the case in terms of the application of these theories in modern-day contexts\ninvolving new types of AI-driven technologies such as robot s or autonomous vehicles, which can be limited without a\ncomprehensive understanding of both AI’s technical capabi lities (e.g.", "metadata": {}}, {"text": "the limitations of the underlying models) as we ll\nas the diversity of life experiences of the people using thes e tools, who can interact with them in ways that are hard\nto predict or design for [102].", "metadata": {}}, {"text": "Given that these concepts enc ompass work from a multitude of domains that espouse\ndiﬀerent objectives, values and methods [23], their deﬁnit ion can have major consequences on the way in which\nthese concepts are operationalized in AI models and systems [10, 65, 183].", "metadata": {}}, {"text": "Work that addresses the ethical aspects of\nAI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a\ncross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.", "metadata": {}}, {"text": "3 EXISTING SCHOLARSHIP IN AI ETHICS AND SUSTAINABILITY\nIn the sections below, we explore existing scholarship in or der to critically analyze how both ethics and sustainabilit y\nare deﬁned in theory and operationalized in practice within the diverse communities that pursue AI.", "metadata": {}}, {"text": "First, we describe\nthe principles and frameworks that have been proposed to guide AI from both an ethical and environmental perspective;", "metadata": {}}, {"text": "next, we examine how these principles are applied, implicit ly and explicitly, in AI research and practice.", "metadata": {}}, {"text": "Finally, we\nlook at several recent approaches for regulating and govern ing AI and the diﬀerent roles adopted by stakeholders and\norganizations.", "metadata": {}}, {"text": "3.1 Principles and Frameworks\nA common starting point to ensure the ethical development an d deployment of AI systems is the deﬁnition of a struc-\nture for guiding this process, which can be operationalized via sets of principles or a framework.", "metadata": {}}, {"text": "On the one hand,\nethical principles are often deﬁned at a high level, describ ing values and concepts outside of any speciﬁc context of\ndeployment.", "metadata": {}}, {"text": "As such, multiple sets of guiding principles fo r ‘ethical AI’ have been proposed by diﬀerent organizations\nranging from research institutes to nonproﬁt and for-proﬁt entities.", "metadata": {}}, {"text": "However, given the many diﬀerent types of AI\napproaches that exist, as well as contextual factors that in ﬂuence their application, it is diﬃcult to deﬁne universal, or\neven generalizable, guidelines.", "metadata": {}}, {"text": "To this point, a 2019 analy sis by Jobin et al.", "metadata": {}}, {"text": "reviewed 84 sets of guidelines mentioning\na variety of principles, ﬁnding very limited convergence be tween them but identifying the values of transparency,\nfairness, non-maleﬁcence, privacy and responsibility as b eing most common [2019].", "metadata": {}}, {"text": "Similarly, a multitude of ethical\nframeworks have been proposed, with most of them espousing s peciﬁc visions of ethics by putting an emphasis on\naspects ranging from human empowerment [64] to virtue ethic s [79].", "metadata": {}}, {"text": "In comparison to principles, ethical frameworks\noften aim to frame ethics from the perspective of implementa tion, identifying how challenges can be addressed and\nhow to build consensus around ethical values, often anchore d to speciﬁc contexts of deployment of AI systems, such\nas medicine [125] or autonomous vehicles [109].", "metadata": {}}, {"text": "However, th e technological mechanisms proposed to operationalize", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "4 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nthese frameworks are often deﬁned in abstract terms, which h ave been found to be diﬃcult to implement in practice\nfrom an engineering perspective, making them diﬃcult to ope rationalize in practice [155]. In terms of environmental\nsustainability, the UN SDGs are most commonly used to inform AI frameworks [72], with inspiration from ﬁelds such\nas ecology to guide the deﬁnition of methods based on metrics and evaluation methods that enable a more holistic\nassessment of AI’s environmental impacts. [112]. However, similarly to AI principles, there is an equal multiplicity\nof AI frameworks, and multiple analyses have been carried ou t with the goal of establishing overlap and transversal\nconnections, which were found to be lacking [14, 155, 205].\nConversely, analyses of ethical AI principles have also obs erved a general lack of recognition of AI’s environmental\nimpacts within the diﬀerent sets of principles that have been deﬁned. Diﬀerent reasons have been proposed for this lack\nof connection, from the reliance of most AI principles on tra ditional Western ethical perspectives, which are human-\ncentered and assign intrinsic value to human beings above ot her living things [28], to the paucity of research on AI’s\nenvironmental impacts, which hinders the development of co herent principles [25]. When sustainability is addressed\nin ethical AI frameworks, it is once again only limited to env ironmental sustainability, notably carbon footprint es-\ntimation. For instance, several industry-led sets of princ iples speciﬁcally addressing the environmental sustainab ility\nof AI have been proposed in recent years by organizations suc h as Salesforce [2024] and the Green Software Founda-\ntion [2023]. While these propositions touch upon metrics su ch as energy eﬃciency, they do not address topics such as\nrebound eﬀects [120] 1, transparency and access to compute, which we consider to be core to these discussions and\nwhich we discuss in Section 4. Nonetheless, certain framewo rks, such as the UNESCO recommendations on the ethics\nof artiﬁcial intelligence [2021] do emphasize the importan ce of sustainability and of evaluating technologies based o n\ntheir environmental impacts via the UN SDGs, which, as discu ssed in the introduction, have sustainable development\nat their core – which we explore in more depth in subsequent se ctions.\n3.2 Research and Practice\nGiven that AI is a distributed ﬁeld consisting of a multitude of practitioners and organizations, the practical application\nof the principles and frameworks described in the previous s ection can diﬀer immensely. In a 2022 study of papers\nsubmitted to conferences such as ICML and NeurIPS, Birhane e t al. analyzed the values that were highlighted by their\nauthors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered\nexplicitly [2022a]. From the 59 values they identiﬁed, the m ost emphasis was put on aspects such as technical progress,\nquantitative evidence, and novelty, whereas ethical consi derations around values such as beneﬁcence, interpretabil ity\nand respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a\nfraction of papers. Also, not a single one of the values Birha ne et al. identiﬁed was explicitly linked to environmental\nsustainability, highlighting once again the lack of connection in the research community with sustainability writ lar ge.\nProgressing from this observation, while sustainability- oriented research has not been prominently featured in\nvenues that directly address AI ethics, it remains a topic of research that has been gathering momentum in recent\nyears. The ﬁrst research to formally address the environmen tal impacts of training AI models was the seminal 2019\narticle by Strubell et al. which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as\nreaching 626,155 pounds of /u1D436/u1D4422 emissions [192]. Follow-up work by other researchers has sh ed more light on this\nissue, revealing diﬀerent aspects of model architecture [1 53] and training procedure [53] that can impact its ensuing\n1The relationship between eﬃciency and sustainability is fa r from straightforward, given phenomena such as rebound eﬀe cts, in which improved\neﬃciency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resour ces – see [27, 120] for a more\nin-depth review.", "sentences": [{"text": "4 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nthese frameworks are often deﬁned in abstract terms, which h ave been found to be diﬃcult to implement in practice\nfrom an engineering perspective, making them diﬃcult to ope rationalize in practice [155].", "metadata": {}}, {"text": "In terms of environmental\nsustainability, the UN SDGs are most commonly used to inform AI frameworks [72], with inspiration from ﬁelds such\nas ecology to guide the deﬁnition of methods based on metrics and evaluation methods that enable a more holistic\nassessment of AI’s environmental impacts.", "metadata": {}}, {"text": "[112].", "metadata": {}}, {"text": "However, similarly to AI principles, there is an equal multiplicity\nof AI frameworks, and multiple analyses have been carried ou t with the goal of establishing overlap and transversal\nconnections, which were found to be lacking [14, 155, 205].", "metadata": {}}, {"text": "Conversely, analyses of ethical AI principles have also obs erved a general lack of recognition of AI’s environmental\nimpacts within the diﬀerent sets of principles that have been deﬁned.", "metadata": {}}, {"text": "Diﬀerent reasons have been proposed for this lack\nof connection, from the reliance of most AI principles on tra ditional Western ethical perspectives, which are human-\ncentered and assign intrinsic value to human beings above ot her living things [28], to the paucity of research on AI’s\nenvironmental impacts, which hinders the development of co herent principles [25].", "metadata": {}}, {"text": "When sustainability is addressed\nin ethical AI frameworks, it is once again only limited to env ironmental sustainability, notably carbon footprint es-\ntimation.", "metadata": {}}, {"text": "For instance, several industry-led sets of princ iples speciﬁcally addressing the environmental sustainab ility\nof AI have been proposed in recent years by organizations suc h as Salesforce [2024] and the Green Software Founda-\ntion [2023].", "metadata": {}}, {"text": "While these propositions touch upon metrics su ch as energy eﬃciency, they do not address topics such as\nrebound eﬀects [120] 1, transparency and access to compute, which we consider to be core to these discussions and\nwhich we discuss in Section 4.", "metadata": {}}, {"text": "Nonetheless, certain framewo rks, such as the UNESCO recommendations on the ethics\nof artiﬁcial intelligence [2021] do emphasize the importan ce of sustainability and of evaluating technologies based o n\ntheir environmental impacts via the UN SDGs, which, as discu ssed in the introduction, have sustainable development\nat their core – which we explore in more depth in subsequent se ctions.", "metadata": {}}, {"text": "3.2 Research and Practice\nGiven that AI is a distributed ﬁeld consisting of a multitude of practitioners and organizations, the practical application\nof the principles and frameworks described in the previous s ection can diﬀer immensely.", "metadata": {}}, {"text": "In a 2022 study of papers\nsubmitted to conferences such as ICML and NeurIPS, Birhane e t al.", "metadata": {}}, {"text": "analyzed the values that were highlighted by their\nauthors – i.e.", "metadata": {}}, {"text": "the positive attributes of their project that they emphasized and the negative impacts they considered\nexplicitly [2022a].", "metadata": {}}, {"text": "From the 59 values they identiﬁed, the m ost emphasis was put on aspects such as technical progress,\nquantitative evidence, and novelty, whereas ethical consi derations around values such as beneﬁcence, interpretabil ity\nand respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a\nfraction of papers.", "metadata": {}}, {"text": "Also, not a single one of the values Birha ne et al.", "metadata": {}}, {"text": "identiﬁed was explicitly linked to environmental\nsustainability, highlighting once again the lack of connection in the research community with sustainability writ lar ge.", "metadata": {}}, {"text": "Progressing from this observation, while sustainability- oriented research has not been prominently featured in\nvenues that directly address AI ethics, it remains a topic of research that has been gathering momentum in recent\nyears.", "metadata": {}}, {"text": "The ﬁrst research to formally address the environmen tal impacts of training AI models was the seminal 2019\narticle by Strubell et al.", "metadata": {}}, {"text": "which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as\nreaching 626,155 pounds of /u1D436/u1D4422 emissions [192].", "metadata": {}}, {"text": "Follow-up work by other researchers has sh ed more light on this\nissue, revealing diﬀerent aspects of model architecture [1 53] and training procedure [53] that can impact its ensuing\n1The relationship between eﬃciency and sustainability is fa r from straightforward, given phenomena such as rebound eﬀe cts, in which improved\neﬃciency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resour ces – see [27, 120] for a more\nin-depth review.", "metadata": {}}], "metadata": {"page": 4}}, {"text": "[Image page=4 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=4 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 4, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 5\ncarbon footprint. The ﬁrst proposal for “Green AI”, i.e. AI r esearch that takes environmental impacts into consideration\nwhen training AI models, was made by Schwartz et al. in 2020 [1 81] - it was subsequently broadened to include aspects\nsuch as hardware and scaling [215] and model deployment [121 ] more recently. However, this ﬁeld of research, while\nincreasingly proliﬁc, has failed to take ethical considera tions into account in its analyses, focusing solely on aspec ts\nsuch as carbon intensity and energy eﬃciency, and not on issu es such as the environmental impacts of increased\nconsumption due to the use of AI in targeted advertising [96] , or the application of AI in the oil and gas sector [77],\nwhich are liable to counter-balance any actual eﬃciency gai ns 2.\n3.3 Governance and Regulation\nGovernance and regulation aim to establish mechanisms for d ecision-making, guiding the development and deploy-\nment of AI systems, and outlining the roles and responsibili ties of each party involved [219]. There are many dis-\ntributed eﬀorts for governance in AI whose aim is to develop e thical guardrails, with some highlighting the impor-\ntance of international institutions [85], and others focus ing on the public sector [101] – reﬂecting that both bottom-u p\nand top-down endeavors are useful to establish functional m echanisms for governing AI. If we take a look at recent\ncommunity endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that estab-\nlished mechanisms for various ethical aspects of the projec t such as data governance, quality metrics, and fostering\nstakeholder collaboration and transparency [92], as well a s drafting a consensus-driven ethical framework for govern -\ning the resulting artifacts that encompasses both legal and technical dimensions [154]. Interestingly, Big Science wa s\none of the few projects that also considered and documented t he carbon footprint of model training, evaluation and\ndeployment, proposing a holistic, life cycle approach to es timating emissions [121]. There have also been proposals\narguing for putting sustainability in the center of AI devel opment and deployment [203], as well as frameworks for\ncertifying the sustainability of AI systems [25], across al l the diﬀerent pillars of sustainability (i.e. social, envi ronmen-\ntal and economic) [70]. However, we are still at the beginnin g of building governance mechanisms that oﬀer a more\ncomprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability.\nA pivotal example of top-down governance is the European Uni on’s AI Act, which draws from broadly deﬁned\nethical principles to inform its regulations [39]. In fact, the text of the Act demonstrates considerable progress foll ow-\ning the recent European dialogues, seeking to regulate AI ap plications that may infringe on human rights, adhering,\namong others, to the ethical principles of human oversight, human agency and transparency. Moreover, the AI Act’s\nfoundation on the premise that risk equates to potential hum an rights harms [152] echoes one of the longstanding tra-\nditions in AI ethics of prioritizing human rights [9, 216]. T his approach embodies the ethical commitment to safeguard\nfundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these im-\nportant principles. However, while both EU AI Act [2022], as well as similar regulatory initiatives in China [2023] and\nCanada [2023] point to the need to protect both fundamental h uman rights as well as to limit damage to environment,\nthere are no oﬃcial provisions regarding sustainability in any of their texts, and it remains to be seen how existing\nstandards for environmental impacts in all of these jurisdi ctions will apply to AI systems. Similarly, sustainability\nconsiderations were also lacking in the 2023 US Executive Or der regarding AI [20], which did not mention AI’s green-\nhouse gas emissions nor energy usage, as well as multi-natio n declarations such as the Bletchley Declaration [2023],\nillustrating the disconnect between sustainability and et hics in recent approaches to AI regulation.\n2Recent media coverage of Microsoft’s sustainability promi ses has estimated that a single contract to use AI to expand oi l production “could enable\ncarbon emissions adding up to 640 percent of the company’s ca rbon removal targets\" [191].", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 5\ncarbon footprint.", "metadata": {}}, {"text": "The ﬁrst proposal for “Green AI”, i.e.", "metadata": {}}, {"text": "AI r esearch that takes environmental impacts into consideration\nwhen training AI models, was made by Schwartz et al.", "metadata": {}}, {"text": "in 2020 [1 81] - it was subsequently broadened to include aspects\nsuch as hardware and scaling [215] and model deployment [121 ] more recently.", "metadata": {}}, {"text": "However, this ﬁeld of research, while\nincreasingly proliﬁc, has failed to take ethical considera tions into account in its analyses, focusing solely on aspec ts\nsuch as carbon intensity and energy eﬃciency, and not on issu es such as the environmental impacts of increased\nconsumption due to the use of AI in targeted advertising [96] , or the application of AI in the oil and gas sector [77],\nwhich are liable to counter-balance any actual eﬃciency gai ns 2.", "metadata": {}}, {"text": "3.3 Governance and Regulation\nGovernance and regulation aim to establish mechanisms for d ecision-making, guiding the development and deploy-\nment of AI systems, and outlining the roles and responsibili ties of each party involved [219].", "metadata": {}}, {"text": "There are many dis-\ntributed eﬀorts for governance in AI whose aim is to develop e thical guardrails, with some highlighting the impor-\ntance of international institutions [85], and others focus ing on the public sector [101] – reﬂecting that both bottom-u p\nand top-down endeavors are useful to establish functional m echanisms for governing AI.", "metadata": {}}, {"text": "If we take a look at recent\ncommunity endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that estab-\nlished mechanisms for various ethical aspects of the projec t such as data governance, quality metrics, and fostering\nstakeholder collaboration and transparency [92], as well a s drafting a consensus-driven ethical framework for govern -\ning the resulting artifacts that encompasses both legal and technical dimensions [154].", "metadata": {}}, {"text": "Interestingly, Big Science wa s\none of the few projects that also considered and documented t he carbon footprint of model training, evaluation and\ndeployment, proposing a holistic, life cycle approach to es timating emissions [121].", "metadata": {}}, {"text": "There have also been proposals\narguing for putting sustainability in the center of AI devel opment and deployment [203], as well as frameworks for\ncertifying the sustainability of AI systems [25], across al l the diﬀerent pillars of sustainability (i.e.", "metadata": {}}, {"text": "social, envi ronmen-\ntal and economic) [70].", "metadata": {}}, {"text": "However, we are still at the beginnin g of building governance mechanisms that oﬀer a more\ncomprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability.", "metadata": {}}, {"text": "A pivotal example of top-down governance is the European Uni on’s AI Act, which draws from broadly deﬁned\nethical principles to inform its regulations [39].", "metadata": {}}, {"text": "In fact, the text of the Act demonstrates considerable progress foll ow-\ning the recent European dialogues, seeking to regulate AI ap plications that may infringe on human rights, adhering,\namong others, to the ethical principles of human oversight, human agency and transparency.", "metadata": {}}, {"text": "Moreover, the AI Act’s\nfoundation on the premise that risk equates to potential hum an rights harms [152] echoes one of the longstanding tra-\nditions in AI ethics of prioritizing human rights [9, 216].", "metadata": {}}, {"text": "T his approach embodies the ethical commitment to safeguard\nfundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these im-\nportant principles.", "metadata": {}}, {"text": "However, while both EU AI Act [2022], as well as similar regulatory initiatives in China [2023] and\nCanada [2023] point to the need to protect both fundamental h uman rights as well as to limit damage to environment,\nthere are no oﬃcial provisions regarding sustainability in any of their texts, and it remains to be seen how existing\nstandards for environmental impacts in all of these jurisdi ctions will apply to AI systems.", "metadata": {}}, {"text": "Similarly, sustainability\nconsiderations were also lacking in the 2023 US Executive Or der regarding AI [20], which did not mention AI’s green-\nhouse gas emissions nor energy usage, as well as multi-natio n declarations such as the Bletchley Declaration [2023],\nillustrating the disconnect between sustainability and et hics in recent approaches to AI regulation.", "metadata": {}}, {"text": "2Recent media coverage of Microsoft’s sustainability promi ses has estimated that a single contract to use AI to expand oi l production “could enable\ncarbon emissions adding up to 640 percent of the company’s ca rbon removal targets\" [191].", "metadata": {}}], "metadata": {"page": 5}}, {"text": "[Image page=5 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=5 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "6 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\n4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY\n\"Is it fair . . . that the residents of the Maldives (likely to b e underwater by 2100) or the 800,000 people in\nSudan aﬀected by drastic ﬂoods pay the environmental price o f training and deploying ever larger English\nLMs, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic? \"\nBender, Gebru et al. [2021]\nIn the current section, we deﬁne four recurring issues that w e have found to be particularly salient to discussions\naround both AI ethics and sustainability. These issues are i nspired by previous carried out by critical scholars such\nas Dobbe and Whittaker [2019], Birhane [2022b], van Wynsber ghe [2021] as well as Bender and Gebru [2021], as\ncited above. We start, in Section 4.1 with a discussion of the perils of assumptions of the generalizability of data and\nmodels in both ethics and sustainability. We follow, in Section 4.2 with a reﬂection upon the current state of evaluation\nof AI systems, what is measured, what is missing, and why that matters. Next, we remark upon the current lack of\ntransparency with regards to information relevant to both ethics and sustainabilityin Section 4.3. Finally, in Section 4.4,\nwe discuss the balance of power and the allocation of justice , and how existing inequalities can be further ampliﬁed\nby AI systems.\n4.1 Generalizability\nAI technologies function based on assumptions of generaliz ability and representativeness – i.e. that given suﬃcient\ndata, an AI model can learn to accurately represent (any) giv en process and even adapt to previously unseen data [74].\nFor instance, the concept of pre-training AI models on large datasets such as ImageNet [47] dates back to the early\n1990s [178] and has since become the dominant training parad igm in both computer vision [162, 193] and natural\nlanguage processing [50, 114]. In fact, pre-training is heavily dependent upon the assumption of representativeness -i.e.\nthat the huge amounts of training data used for pre-training represent the world as a whole, or at least a suﬃcient part\nof it to be useful for any kind of downstream application (i.e . ﬁne-tuning, transfer learning, etc). While the limitatio ns\nof such claims for both AI models and datasets have been previ ously shown (see Chasalow and Levy [36], Koch et al.\n[98], Raji et al. [157], Smith et al. [188]), the theory of gen eralizability, and the perception of certain types of AI\nmodels, ie. LLMs, as “general purpose technologies” contin ues to persist [57]. This can come with both ethical and\nenvironmental ramiﬁcations when systems trained under the pretense of generalizability are applied in contexts that\ndiﬀer from the ones represented in their training data – we di scuss some of these below.\nGiven the data that fuels AI models is produced by humans, it i s intrinsically laden with subjective judgments [132]\nand representative of speciﬁc worldviews [46]. Numerous st udies have shown that both the data used for training AI\nmodels [54, 157, 166] and the models themselves [16, 73, 213] are not, in fact, representative of the world at large and\nthat the biases contained in training data persist even if fu rther ﬁne-tuning is carried out [103], which can have ‘cas-\ncade’ eﬀects when models are deployed in production [173], w hich can contribute to perpetuating negative biases [68].\nSimilarly, oﬀ-the-shelf, proprietary technologies that a re marketed as generic can fail at the tasks when applied in\ndiﬀering contexts, e.g. in applications such as facial reco gnition (when they fail to recognize people from population s\nunder-represented in training datasets [33]) and crime pre diction (where they have dismal accuracy rates across dif-\nferent locations [174]) – and yet, out-of-the-box AI system s for these tasks and many others continue to be built and\ndeployed under the assumptions that they will work no matter the context of their application. This can have devas-\ntating eﬀects on already marginalized communities when app lied for tasks such as criminal sentencing [6], and facial\nrecognition [33].", "sentences": [{"text": "6 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\n4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY\n\"Is it fair .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "that the residents of the Maldives (likely to b e underwater by 2100) or the 800,000 people in\nSudan aﬀected by drastic ﬂoods pay the environmental price o f training and deploying ever larger English\nLMs, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic?", "metadata": {}}, {"text": "\"\nBender, Gebru et al.", "metadata": {}}, {"text": "[2021]\nIn the current section, we deﬁne four recurring issues that w e have found to be particularly salient to discussions\naround both AI ethics and sustainability.", "metadata": {}}, {"text": "These issues are i nspired by previous carried out by critical scholars such\nas Dobbe and Whittaker [2019], Birhane [2022b], van Wynsber ghe [2021] as well as Bender and Gebru [2021], as\ncited above.", "metadata": {}}, {"text": "We start, in Section 4.1 with a discussion of the perils of assumptions of the generalizability of data and\nmodels in both ethics and sustainability.", "metadata": {}}, {"text": "We follow, in Section 4.2 with a reﬂection upon the current state of evaluation\nof AI systems, what is measured, what is missing, and why that matters.", "metadata": {}}, {"text": "Next, we remark upon the current lack of\ntransparency with regards to information relevant to both ethics and sustainabilityin Section 4.3.", "metadata": {}}, {"text": "Finally, in Section 4.4,\nwe discuss the balance of power and the allocation of justice , and how existing inequalities can be further ampliﬁed\nby AI systems.", "metadata": {}}, {"text": "4.1 Generalizability\nAI technologies function based on assumptions of generaliz ability and representativeness – i.e.", "metadata": {}}, {"text": "that given suﬃcient\ndata, an AI model can learn to accurately represent (any) giv en process and even adapt to previously unseen data [74].", "metadata": {}}, {"text": "For instance, the concept of pre-training AI models on large datasets such as ImageNet [47] dates back to the early\n1990s [178] and has since become the dominant training parad igm in both computer vision [162, 193] and natural\nlanguage processing [50, 114].", "metadata": {}}, {"text": "In fact, pre-training is heavily dependent upon the assumption of representativeness -i.e.", "metadata": {}}, {"text": "that the huge amounts of training data used for pre-training represent the world as a whole, or at least a suﬃcient part\nof it to be useful for any kind of downstream application (i.e .", "metadata": {}}, {"text": "ﬁne-tuning, transfer learning, etc).", "metadata": {}}, {"text": "While the limitatio ns\nof such claims for both AI models and datasets have been previ ously shown (see Chasalow and Levy [36], Koch et al.", "metadata": {}}, {"text": "[98], Raji et al.", "metadata": {}}, {"text": "[157], Smith et al.", "metadata": {}}, {"text": "[188]), the theory of gen eralizability, and the perception of certain types of AI\nmodels, ie.", "metadata": {}}, {"text": "LLMs, as “general purpose technologies” contin ues to persist [57].", "metadata": {}}, {"text": "This can come with both ethical and\nenvironmental ramiﬁcations when systems trained under the pretense of generalizability are applied in contexts that\ndiﬀer from the ones represented in their training data – we di scuss some of these below.", "metadata": {}}, {"text": "Given the data that fuels AI models is produced by humans, it i s intrinsically laden with subjective judgments [132]\nand representative of speciﬁc worldviews [46].", "metadata": {}}, {"text": "Numerous st udies have shown that both the data used for training AI\nmodels [54, 157, 166] and the models themselves [16, 73, 213] are not, in fact, representative of the world at large and\nthat the biases contained in training data persist even if fu rther ﬁne-tuning is carried out [103], which can have ‘cas-\ncade’ eﬀects when models are deployed in production [173], w hich can contribute to perpetuating negative biases [68].", "metadata": {}}, {"text": "Similarly, oﬀ-the-shelf, proprietary technologies that a re marketed as generic can fail at the tasks when applied in\ndiﬀering contexts, e.g.", "metadata": {}}, {"text": "in applications such as facial reco gnition (when they fail to recognize people from population s\nunder-represented in training datasets [33]) and crime pre diction (where they have dismal accuracy rates across dif-\nferent locations [174]) – and yet, out-of-the-box AI system s for these tasks and many others continue to be built and\ndeployed under the assumptions that they will work no matter the context of their application.", "metadata": {}}, {"text": "This can have devas-\ntating eﬀects on already marginalized communities when app lied for tasks such as criminal sentencing [6], and facial\nrecognition [33].", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 7\nSimilarly to data concerning human beings, environmental a nd ecological data can also contain biases, for instance\nin its temporal coverage and geographical spread, which can be as damaging to biomes 3 of plants and animals as they\nare to communities of humans [95, 187]. From a modeling persp ective, AI models trained to carry out biodiversity\nmonitoring on one ecosystem often fail to perform accuratel y on others, no matter the species [99, 184], and yet\necological bias assessments are not often carried out befor e model training. In fact, White et al. refer to geographical\ndiﬀerences in ecological datasets as the main factor limiti ng our capacity to predict how biodiversity will be impacted\nby future changes in the climate, notably due to missing data from regions such as Africa and South America, making\ndata-driven approaches such as AI unrepresentative of enti re continents [210].\nAn example of the dire consequences this can have can be found in the ﬁeld of short-term climate modeling and\ndisaster detection, which relies on data from sensors and we ather radar stations that allow for events such as ﬂoods\nand wildﬁres to be tracked in real-time across borders, ocea ns and continents. While geospatial data is inherently\nglobal (given that satellites circle the planet as a whole), it relies upon assumptions of generalizability that often\nfail to hold when applied in contexts that diﬀer from the trai ning data, or the fact that many regions are missing\nthese data sources for reasons ranging from insuﬃcient fund ing and aging technological infrastructure to lack of\nconnectivity [37, 198], which results in data gaps [182, 197 ]. This can come at a cost to living beings, both human and\nanimal, in the regions and contexts where these technologie s are applied – for instance in early warning systems for\nextreme weather events [150].and deforestation detection [97]. Of course, any assessment of representativity (or lac k\nthereof) hinges upon an appropriate evaluation of this assu mption – which we address in the following section.\n4.2 Evaluation\nA popular adage states that “you can’t improve what you don’t measure” 4; in the context of AI systems, this can be\ntranslated into the fact that the criteria that we use to eval uate AI systems and the way in which this evaluation are\ncarried out are important – i.e. the metrics we choose help us embed our values as communities about outcomes we\nwish to see and those that we put less emphasis on [136]. While leaderboards such as Papers With Code tend to only\nmeasure performance-based metrics such as accuracy or prec ision, factoring in other metrics can make comparisons\nbetween diﬀerent models more meaningful and actionable. Th is is due to the fact that real-world constraints on model\ndeployment often result in trade-oﬀs being made between diﬀ erent factors that include accuracy and eﬃciency [31],\nbut also robustness [217], inclusion [88] and data quality [ 11]. This means that in order to meaningfully assess the\nutility of AI systems in diﬀerent practical contexts, other measures must be considered in parallel to performance and\naccuracy [122]; and often the best people to do the assessmen ts for the trade-oﬀs are the populations who will use\nthese tools. For example, when it comes to the evaluation of g enerative technologies such as large language models,\nthese do not have a single well-established evaluation appr oach [139, 159]. Approaches that are used for evaluating\ntheir ethical limitations include red-teaming [66], exter nal audits [139, 158] as well as more holistic model evaluati ons\nthat reﬂect diﬀerent aspects of model performance [26, 67]. However, critiques of the approaches described above\nalso include their non-inclusivity of marginalized commun ities [48] as well as a lack of formalized approaches and\nstandards for model evaluation, making apples-to-apples c omparisons between models diﬃcult [40]. Also, as many of\nthe most widely deployed AI systems are currently proprieta ry and direct access to models is not possible, it is hard\nto exhaustively evaluate many popular commercial models fo r any meaningful evaluation to take place [118, 190].\n3A biome is a bio-geographical unit that corresponds to a comm unity of plants and animals that share a physical environmen t and a climate.\n4The quote is often attributed to Peter Drucker, although its exact origins are unclear.", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 7\nSimilarly to data concerning human beings, environmental a nd ecological data can also contain biases, for instance\nin its temporal coverage and geographical spread, which can be as damaging to biomes 3 of plants and animals as they\nare to communities of humans [95, 187].", "metadata": {}}, {"text": "From a modeling persp ective, AI models trained to carry out biodiversity\nmonitoring on one ecosystem often fail to perform accuratel y on others, no matter the species [99, 184], and yet\necological bias assessments are not often carried out befor e model training.", "metadata": {}}, {"text": "In fact, White et al.", "metadata": {}}, {"text": "refer to geographical\ndiﬀerences in ecological datasets as the main factor limiti ng our capacity to predict how biodiversity will be impacted\nby future changes in the climate, notably due to missing data from regions such as Africa and South America, making\ndata-driven approaches such as AI unrepresentative of enti re continents [210].", "metadata": {}}, {"text": "An example of the dire consequences this can have can be found in the ﬁeld of short-term climate modeling and\ndisaster detection, which relies on data from sensors and we ather radar stations that allow for events such as ﬂoods\nand wildﬁres to be tracked in real-time across borders, ocea ns and continents.", "metadata": {}}, {"text": "While geospatial data is inherently\nglobal (given that satellites circle the planet as a whole), it relies upon assumptions of generalizability that often\nfail to hold when applied in contexts that diﬀer from the trai ning data, or the fact that many regions are missing\nthese data sources for reasons ranging from insuﬃcient fund ing and aging technological infrastructure to lack of\nconnectivity [37, 198], which results in data gaps [182, 197 ].", "metadata": {}}, {"text": "This can come at a cost to living beings, both human and\nanimal, in the regions and contexts where these technologie s are applied – for instance in early warning systems for\nextreme weather events [150].and deforestation detection [97].", "metadata": {}}, {"text": "Of course, any assessment of representativity (or lac k\nthereof) hinges upon an appropriate evaluation of this assu mption – which we address in the following section.", "metadata": {}}, {"text": "4.2 Evaluation\nA popular adage states that “you can’t improve what you don’t measure” 4;", "metadata": {}}, {"text": "in the context of AI systems, this can be\ntranslated into the fact that the criteria that we use to eval uate AI systems and the way in which this evaluation are\ncarried out are important – i.e.", "metadata": {}}, {"text": "the metrics we choose help us embed our values as communities about outcomes we\nwish to see and those that we put less emphasis on [136].", "metadata": {}}, {"text": "While leaderboards such as Papers With Code tend to only\nmeasure performance-based metrics such as accuracy or prec ision, factoring in other metrics can make comparisons\nbetween diﬀerent models more meaningful and actionable.", "metadata": {}}, {"text": "Th is is due to the fact that real-world constraints on model\ndeployment often result in trade-oﬀs being made between diﬀ erent factors that include accuracy and eﬃciency [31],\nbut also robustness [217], inclusion [88] and data quality [ 11].", "metadata": {}}, {"text": "This means that in order to meaningfully assess the\nutility of AI systems in diﬀerent practical contexts, other measures must be considered in parallel to performance and\naccuracy [122];", "metadata": {}}, {"text": "and often the best people to do the assessmen ts for the trade-oﬀs are the populations who will use\nthese tools.", "metadata": {}}, {"text": "For example, when it comes to the evaluation of g enerative technologies such as large language models,\nthese do not have a single well-established evaluation appr oach [139, 159].", "metadata": {}}, {"text": "Approaches that are used for evaluating\ntheir ethical limitations include red-teaming [66], exter nal audits [139, 158] as well as more holistic model evaluati ons\nthat reﬂect diﬀerent aspects of model performance [26, 67].", "metadata": {}}, {"text": "However, critiques of the approaches described above\nalso include their non-inclusivity of marginalized commun ities [48] as well as a lack of formalized approaches and\nstandards for model evaluation, making apples-to-apples c omparisons between models diﬃcult [40].", "metadata": {}}, {"text": "Also, as many of\nthe most widely deployed AI systems are currently proprieta ry and direct access to models is not possible, it is hard\nto exhaustively evaluate many popular commercial models fo r any meaningful evaluation to take place [118, 190].", "metadata": {}}, {"text": "3A biome is a bio-geographical unit that corresponds to a comm unity of plants and animals that share a physical environmen t and a climate.", "metadata": {}}, {"text": "4The quote is often attributed to Peter Drucker, although its exact origins are unclear.", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "8 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nSimilarly, evaluating the environmental impacts of AI syst ems is far from straightforward, and we are still missing\nmany pieces of the puzzle needed in order to meaningfully est imate these impacts. For instance, most of the carbon\nfootprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192],\nbut which only represents a portion of models’ total environ mental impacts. In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach\nfor this evaluation, since it takes into account diﬀerent st ages of the model life cycle including the manufacturing of\ncomputing hardware, idle energy usage, and model deploymen t, ﬁnding that training accounted for only half of the\nmodel’s overall emissions [121], meaning that similar stud ies that only took training into account were potentially\nunderestimating their emissions by half. Also, while comme ndable in terms of its granularity, this kind of carbon\naccounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such\nas BLOOM towards amplifying the existing inequalities in th e ﬁeld of AI due to the increased amount of computing\nresources that they require, which are unattainable to many members of the AI community, as well as the propagation\nof biases via their usage. Furthermore, the authors themsel ves note that there is currently no information available\nabout the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the\noverall carbon footprint this represents. This highlights that the emphasis on environmental sustainability often fa ils\nto account for other aspects of AI’s global impacts – and any k ind of evaluation hinges upon transparency, which is\nsorely lacking in the ﬁeld of AI – we discuss this in more lengt h in the following section.\n4.3 Transparency\nTransparency is widely recognized as a fundamental princip le in science in general and AI in particular [63, 107, 208,\n212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based\nsystems are not inherently transparent or interpretable, given the complexity of the neural network architectures the y\nespouse and the number of parameters they contain [105, 113, 140]. Eﬀorts such as interpretability approaches are\nuseful and can help interpret the predictions of models post hoc [104, 164], whereas artifacts such as data sheets and\nmodel cards [69, 137] can contribute towards making AI syste ms more understandable for users, providing essential\ninformation about AI models in a user-friendly format. Thes e artifacts allow users to understand not just how an\nAI system functions, but also its limitations, potential bi ases, implications, and environmental impacts. However, even\nthough model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face)\nand provide important information about models, they are no t suﬃcient to guarantee, for instance, the reproducibility\nof reported results, which is a core tenet of scientiﬁc pract ice [146].\nIndeed, several studies of transparency found that an overw helming amount of results published at technical AI\nconferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation\nhighlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility\nof AI models and their ﬁndings. Such an approach would acknow ledge the connection between transparency and\nreproducibility: transparent research practices enable r eproducibility, which in turn facilitates independent scr utiny,\nvalidation, further development of research ﬁndings by other scientists [80]. The absence of transparency, especially in\nsharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing\nAI models’ scientiﬁc impact and impeding their adoption wit hin the wider scientiﬁc community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental\nimpacts of AI models and systems, with most work in this ﬁeld b eing done post-hoc by researchers who did not do the\ninitial model training and deployment (e.g. [116, 192]). Th e most common environmental sustainability metric for AI", "sentences": [{"text": "8 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nSimilarly, evaluating the environmental impacts of AI syst ems is far from straightforward, and we are still missing\nmany pieces of the puzzle needed in order to meaningfully est imate these impacts.", "metadata": {}}, {"text": "For instance, most of the carbon\nfootprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192],\nbut which only represents a portion of models’ total environ mental impacts.", "metadata": {}}, {"text": "In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al.", "metadata": {}}, {"text": "proposed using a Life Cycle Assessment approach\nfor this evaluation, since it takes into account diﬀerent st ages of the model life cycle including the manufacturing of\ncomputing hardware, idle energy usage, and model deploymen t, ﬁnding that training accounted for only half of the\nmodel’s overall emissions [121], meaning that similar stud ies that only took training into account were potentially\nunderestimating their emissions by half.", "metadata": {}}, {"text": "Also, while comme ndable in terms of its granularity, this kind of carbon\naccounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such\nas BLOOM towards amplifying the existing inequalities in th e ﬁeld of AI due to the increased amount of computing\nresources that they require, which are unattainable to many members of the AI community, as well as the propagation\nof biases via their usage.", "metadata": {}}, {"text": "Furthermore, the authors themsel ves note that there is currently no information available\nabout the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the\noverall carbon footprint this represents.", "metadata": {}}, {"text": "This highlights that the emphasis on environmental sustainability often fa ils\nto account for other aspects of AI’s global impacts – and any k ind of evaluation hinges upon transparency, which is\nsorely lacking in the ﬁeld of AI – we discuss this in more lengt h in the following section.", "metadata": {}}, {"text": "4.3 Transparency\nTransparency is widely recognized as a fundamental princip le in science in general and AI in particular [63, 107, 208,\n212] but actualizing it in practice can be challenging.", "metadata": {}}, {"text": "This is, in part, due to the fact that machine learning-based\nsystems are not inherently transparent or interpretable, given the complexity of the neural network architectures the y\nespouse and the number of parameters they contain [105, 113, 140].", "metadata": {}}, {"text": "Eﬀorts such as interpretability approaches are\nuseful and can help interpret the predictions of models post hoc [104, 164], whereas artifacts such as data sheets and\nmodel cards [69, 137] can contribute towards making AI syste ms more understandable for users, providing essential\ninformation about AI models in a user-friendly format.", "metadata": {}}, {"text": "Thes e artifacts allow users to understand not just how an\nAI system functions, but also its limitations, potential bi ases, implications, and environmental impacts.", "metadata": {}}, {"text": "However, even\nthough model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face)\nand provide important information about models, they are no t suﬃcient to guarantee, for instance, the reproducibility\nof reported results, which is a core tenet of scientiﬁc pract ice [146].", "metadata": {}}, {"text": "Indeed, several studies of transparency found that an overw helming amount of results published at technical AI\nconferences do not document all of the variables necessary to reproduce the results they report [78, 156].", "metadata": {}}, {"text": "This situation\nhighlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility\nof AI models and their ﬁndings.", "metadata": {}}, {"text": "Such an approach would acknow ledge the connection between transparency and\nreproducibility: transparent research practices enable r eproducibility, which in turn facilitates independent scr utiny,\nvalidation, further development of research ﬁndings by other scientists [80].", "metadata": {}}, {"text": "The absence of transparency, especially in\nsharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing\nAI models’ scientiﬁc impact and impeding their adoption wit hin the wider scientiﬁc community [80].", "metadata": {}}, {"text": "In terms of sustainability, the AI community has historically been even less transparent regarding the environmental\nimpacts of AI models and systems, with most work in this ﬁeld b eing done post-hoc by researchers who did not do the\ninitial model training and deployment (e.g.", "metadata": {}}, {"text": "[116, 192]).", "metadata": {}}, {"text": "Th e most common environmental sustainability metric for AI", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 9\nmodels, their carbon footprint, is rarely, if ever, disclos ed. While model cards of recent models such as BLOOM [214]\nand Stable Diﬀusion [169] have included carbon footprint in formation, it remains far from common information com-\nmunicated by model creators – recent work has found that the o verwhelming majority of models shared publicly do\nnot include this information [34]. In fact, most carbon foot print analyses gather the information manually by writing\nto authors. For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-\nmation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many\nauthors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023]. In\nfact, until recent years, the general emphasis in the AI comm unity was put on the eﬃciency and ‘greenness‘ of AI as\nopposed to its environmental costs, which are now slowly starting to gain traction as an important consideration for AI\nsystems [86, 141]. In fact, given the increasing size and com putational requirements of models being productionized in\nrecent years (especially LLMs), training them is only acces sible to a small fraction of the AI community, which means\nthat the organizations who have the necessary resources for this have a disproportionate inﬂuence on the ﬁeld as a\nwhole— we discuss this in the following section.\n4.4 Power and Equity\nModern AI research and practice are not equitable by design:their cost in terms of computer hardware as well as human\nskills means that only a small percentage of both academic and industrial organizations can contribute to many aspects\nof model development. With the recent advent of AI models of e ver-increasing scale and complexity, the digital divide\nin AI is only increasing, as it takes more compute and human sk ill to train and deploy AI models and systems [1, 19, 35,\n118]. This means that the future wide-sweeping beneﬁts that AI technologies are promised to have for humanity as a\nwhole [149] are contingent upon access to technologies that are fundamentally unequally distributed. Despite explici t\nproposals to pursue more equitable and explicitly de-colon ialist approaches [124, 138], the ‘bigger-is-better’ para digm\ncontinues to be central to AI research and practice [204]. In a similar fashion, the places where AI research is being\ncarried out are also skewed towards institutions from a hand ful of countries mostly located in the Global North [2],\nwhich inexorably impacts the choices made during the AI deve lopment and deployment process, introducing many\nbiases (which we have already addressed in previous section s). In fact, recent work has proposed that the very pursuit\nof sustainable AI has the opposite eﬀect, contributing towa rds maintaining the status quo and “securing the dominant\nsocio-economic interests of neo-liberal capitalism\" [180]. For instance, major technological corporations have dedicated\nsigniﬁcant resources towards solutions such as improving t he eﬃciency of their data centers, proposing numerous\ninitiatives towards technological sustainability [4, 75] , including research at the nexus of AI and the climate [44, 12 8].\nHowever, both Microsoft and Google announced that they woul d miss their 2024 sustainability targets, due in part to\nthe energy demands of the AI tools that they have been develop ing and deploying [130, 161]. The impacts of these\ncomputation-intensive data centers can further be expande d to include the mining of rare metals and the disposal of\ne-waste, both of which predominantly aﬀect countries from t he Global South which proﬁting technology companies\nfrom the Global North [86, 194].\nSimilarly to AI, issues of power, equity and justice are also central in the ﬁeld of sustainability, since climate change\nis an inherently inequitable phenomenon – with a handful of c ountries and regions in North America, East Asia and\nEurope responsible for a disproportionate portion of globa l emissions, while the impacts of sea level rise and extreme\nweather events being felt most strongly in countries with ve ry minimal carbon footprints, raising questions of equity\nand justice and how to address them [49, 126, 151, 177]. Simil arly, the majority of climate-focused AI solutions over-\nlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 9\nmodels, their carbon footprint, is rarely, if ever, disclos ed.", "metadata": {}}, {"text": "While model cards of recent models such as BLOOM [214]\nand Stable Diﬀusion [169] have included carbon footprint in formation, it remains far from common information com-\nmunicated by model creators – recent work has found that the o verwhelming majority of models shared publicly do\nnot include this information [34].", "metadata": {}}, {"text": "In fact, most carbon foot print analyses gather the information manually by writing\nto authors.", "metadata": {}}, {"text": "For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-\nmation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many\nauthors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].", "metadata": {}}, {"text": "In\nfact, until recent years, the general emphasis in the AI comm unity was put on the eﬃciency and ‘greenness‘ of AI as\nopposed to its environmental costs, which are now slowly starting to gain traction as an important consideration for AI\nsystems [86, 141].", "metadata": {}}, {"text": "In fact, given the increasing size and com putational requirements of models being productionized in\nrecent years (especially LLMs), training them is only acces sible to a small fraction of the AI community, which means\nthat the organizations who have the necessary resources for this have a disproportionate inﬂuence on the ﬁeld as a\nwhole— we discuss this in the following section.", "metadata": {}}, {"text": "4.4 Power and Equity\nModern AI research and practice are not equitable by design:their cost in terms of computer hardware as well as human\nskills means that only a small percentage of both academic and industrial organizations can contribute to many aspects\nof model development.", "metadata": {}}, {"text": "With the recent advent of AI models of e ver-increasing scale and complexity, the digital divide\nin AI is only increasing, as it takes more compute and human sk ill to train and deploy AI models and systems [1, 19, 35,\n118].", "metadata": {}}, {"text": "This means that the future wide-sweeping beneﬁts that AI technologies are promised to have for humanity as a\nwhole [149] are contingent upon access to technologies that are fundamentally unequally distributed.", "metadata": {}}, {"text": "Despite explici t\nproposals to pursue more equitable and explicitly de-colon ialist approaches [124, 138], the ‘bigger-is-better’ para digm\ncontinues to be central to AI research and practice [204].", "metadata": {}}, {"text": "In a similar fashion, the places where AI research is being\ncarried out are also skewed towards institutions from a hand ful of countries mostly located in the Global North [2],\nwhich inexorably impacts the choices made during the AI deve lopment and deployment process, introducing many\nbiases (which we have already addressed in previous section s).", "metadata": {}}, {"text": "In fact, recent work has proposed that the very pursuit\nof sustainable AI has the opposite eﬀect, contributing towa rds maintaining the status quo and “securing the dominant\nsocio-economic interests of neo-liberal capitalism\" [180].", "metadata": {}}, {"text": "For instance, major technological corporations have dedicated\nsigniﬁcant resources towards solutions such as improving t he eﬃciency of their data centers, proposing numerous\ninitiatives towards technological sustainability [4, 75] , including research at the nexus of AI and the climate [44, 12 8].", "metadata": {}}, {"text": "However, both Microsoft and Google announced that they woul d miss their 2024 sustainability targets, due in part to\nthe energy demands of the AI tools that they have been develop ing and deploying [130, 161].", "metadata": {}}, {"text": "The impacts of these\ncomputation-intensive data centers can further be expande d to include the mining of rare metals and the disposal of\ne-waste, both of which predominantly aﬀect countries from t he Global South which proﬁting technology companies\nfrom the Global North [86, 194].", "metadata": {}}, {"text": "Similarly to AI, issues of power, equity and justice are also central in the ﬁeld of sustainability, since climate change\nis an inherently inequitable phenomenon – with a handful of c ountries and regions in North America, East Asia and\nEurope responsible for a disproportionate portion of globa l emissions, while the impacts of sea level rise and extreme\nweather events being felt most strongly in countries with ve ry minimal carbon footprints, raising questions of equity\nand justice and how to address them [49, 126, 151, 177].", "metadata": {}}, {"text": "Simil arly, the majority of climate-focused AI solutions over-\nlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "10 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nstands to beneﬁt from them, or whether these issues can be sol ved with technology in the ﬁrst place [30].For instance,\nwhen AI systems that carry out precision agriculture are dev eloped, the emphasis is made on eﬃciency or increased\ncrop yields [185], and not the fact that these systems are lia ble to replace already underprivileged communities such as\nmigrant workers that traditionally harvested crops by hand, or the disparate impacts of proposed solutions on diﬀerent\ncommunities [55, 207, 218]. In addition, AI technologies ca n be seen as further exacerbating the existing inequalities\nin terms of the distribution of power and proﬁts [83] and perp etuating the existing extractionist approaches in terms\nof labor by exploiting already under-paid and marginalized workers for data collection and labeling tasks [165] as well\nas creating a new class of precarious crowd-sourced workers [211], which are often located in the Global South or in\nalready marginalized communities, where the impacts of cli mate change have reduced the viability of traditional pro-\nfessions such as farming [81, 163]. This is an additional exa mple for why we advocate for establishing more concerted\neﬀorts for integrating AI ethics and sustainability – we des cribe these in the following section.\n5 ESTABLISHING BEST PRACTICES FOR AI ETHICS AND SUSTAINABIL ITY\nHaving established a multitude of transversal topics that s pan AI ethics and sustainability, we now focus on proposing\nbest practices that would integrate the two in AI research an d practice. We draw upon existing work to show how we\ncan build upon it both within the AI community and in tandem wi th members of other communities ranging from\nsustainable development to policy-making and engineering .\nPrinciples and Frameworks Research and Practice Governance and Regulation\nGeneralizability\nShifting from the dominant\nWestern moral philosophies\nto include perspectives from\nnon-Western traditions\nStudying how well AI models\ngeneralize to diﬀerent populations\nof human and non-human living\nbeings\nEspousing bottom-up governance\napproaches based on the cultural,\nsocietal and geographical constraints\nof AI system deployment\nEvaluation\nDeveloping frameworks that\naccommodate both ethical and\nenvironmental responsibility\nCarrying out more holistic\nevaluation of models and systems,\nspanning both ethical and\nenvironmental criteria\nIntegrating both social and\nenvironmental assessments\ninto existing and in-progress\nAI regulation\nTransparency\nBroadening the scope of\ntransparency to include its\nsocial and environmental aspects\nCommunicate the costs\nand impacts of AI systems\non both the environment\nand society\nRequiring deployed AI systems\nto carry out audits and report\na minimum of metrics spanning\nboth bias and ethics\nPower\nDeveloping principles and\nframeworks that address\nboth human and ecological needs\nMake equity-informed\ntrade-oﬀs when developing\nand deploying AI\nInvolving multiple stakeholders,\nespecially ones from the concerned\ncommunities and areas, in the\ngovernance process\nTable 1. A summary of the proposed best practices for diﬀeren t axes that we cover in our article\n5.1 Principles\nIn the context of AI, there are multiple facets of AI technolo gies that have to be taken into consideration, given the\nintersection between AI and the broader societal context in which it operates. Importantly, integrating broader sus-\ntainability into AI guidelines ensures that justice and fai rness are not just about social dimensions but also include\nrespecting and protecting the environment – i.e. expanding the deﬁnition of sustainable AI to include the social and\neconomic pillars proposed by Barbier [12]. This integration also acknowledges that true justice in AI cannot be achieved\nwithout considering its environmental and societal implications, which helps build the bridge between complementary\napproaches in both AI ethics and (environmental) sustainab ility research.", "sentences": [{"text": "10 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nstands to beneﬁt from them, or whether these issues can be sol ved with technology in the ﬁrst place [30].For instance,\nwhen AI systems that carry out precision agriculture are dev eloped, the emphasis is made on eﬃciency or increased\ncrop yields [185], and not the fact that these systems are lia ble to replace already underprivileged communities such as\nmigrant workers that traditionally harvested crops by hand, or the disparate impacts of proposed solutions on diﬀerent\ncommunities [55, 207, 218].", "metadata": {}}, {"text": "In addition, AI technologies ca n be seen as further exacerbating the existing inequalities\nin terms of the distribution of power and proﬁts [83] and perp etuating the existing extractionist approaches in terms\nof labor by exploiting already under-paid and marginalized workers for data collection and labeling tasks [165] as well\nas creating a new class of precarious crowd-sourced workers [211], which are often located in the Global South or in\nalready marginalized communities, where the impacts of cli mate change have reduced the viability of traditional pro-\nfessions such as farming [81, 163].", "metadata": {}}, {"text": "This is an additional exa mple for why we advocate for establishing more concerted\neﬀorts for integrating AI ethics and sustainability – we des cribe these in the following section.", "metadata": {}}, {"text": "5 ESTABLISHING BEST PRACTICES FOR AI ETHICS AND SUSTAINABIL ITY\nHaving established a multitude of transversal topics that s pan AI ethics and sustainability, we now focus on proposing\nbest practices that would integrate the two in AI research an d practice.", "metadata": {}}, {"text": "We draw upon existing work to show how we\ncan build upon it both within the AI community and in tandem wi th members of other communities ranging from\nsustainable development to policy-making and engineering .", "metadata": {}}, {"text": "Principles and Frameworks Research and Practice Governance and Regulation\nGeneralizability\nShifting from the dominant\nWestern moral philosophies\nto include perspectives from\nnon-Western traditions\nStudying how well AI models\ngeneralize to diﬀerent populations\nof human and non-human living\nbeings\nEspousing bottom-up governance\napproaches based on the cultural,\nsocietal and geographical constraints\nof AI system deployment\nEvaluation\nDeveloping frameworks that\naccommodate both ethical and\nenvironmental responsibility\nCarrying out more holistic\nevaluation of models and systems,\nspanning both ethical and\nenvironmental criteria\nIntegrating both social and\nenvironmental assessments\ninto existing and in-progress\nAI regulation\nTransparency\nBroadening the scope of\ntransparency to include its\nsocial and environmental aspects\nCommunicate the costs\nand impacts of AI systems\non both the environment\nand society\nRequiring deployed AI systems\nto carry out audits and report\na minimum of metrics spanning\nboth bias and ethics\nPower\nDeveloping principles and\nframeworks that address\nboth human and ecological needs\nMake equity-informed\ntrade-oﬀs when developing\nand deploying AI\nInvolving multiple stakeholders,\nespecially ones from the concerned\ncommunities and areas, in the\ngovernance process\nTable 1.", "metadata": {}}, {"text": "A summary of the proposed best practices for diﬀeren t axes that we cover in our article\n5.1 Principles\nIn the context of AI, there are multiple facets of AI technolo gies that have to be taken into consideration, given the\nintersection between AI and the broader societal context in which it operates.", "metadata": {}}, {"text": "Importantly, integrating broader sus-\ntainability into AI guidelines ensures that justice and fai rness are not just about social dimensions but also include\nrespecting and protecting the environment – i.e.", "metadata": {}}, {"text": "expanding the deﬁnition of sustainable AI to include the social and\neconomic pillars proposed by Barbier [12].", "metadata": {}}, {"text": "This integration also acknowledges that true justice in AI cannot be achieved\nwithout considering its environmental and societal implications, which helps build the bridge between complementary\napproaches in both AI ethics and (environmental) sustainab ility research.", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=2 name=~1~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=2 name=~1~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 2, "image_name": "~1~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=3 name=~2~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=3 name=~2~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 3, "image_name": "~2~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=4 name=~3~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=4 name=~3~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 4, "image_name": "~3~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=5 name=~4~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=5 name=~4~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 5, "image_name": "~4~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=6 name=~5~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=6 name=~5~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 6, "image_name": "~5~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=7 name=~6~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=7 name=~6~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 7, "image_name": "~6~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=8 name=~7~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=8 name=~7~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 8, "image_name": "~7~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=9 name=~8~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=9 name=~8~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 9, "image_name": "~8~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=10 name=~9~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=10 name=~9~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 10, "image_name": "~9~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=11 name=~10~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=11 name=~10~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 11, "image_name": "~10~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=12 name=~11~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=12 name=~11~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 12, "image_name": "~11~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=13 name=~12~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=13 name=~12~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 13, "image_name": "~12~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=14 name=~13~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=14 name=~13~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 14, "image_name": "~13~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=15 name=~14~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=15 name=~14~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 15, "image_name": "~14~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=16 name=~15~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=16 name=~15~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 16, "image_name": "~15~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=17 name=~16~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=17 name=~16~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 17, "image_name": "~16~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=18 name=~17~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=18 name=~17~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 18, "image_name": "~17~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}, {"text": "[Image page=10 idx=19 name=~18~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=10 idx=19 name=~18~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 19, "image_name": "~18~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 11\nGeneralizability. Given the observed disconnect between ethics and sustainability in the context of AI principles and\nframeworks, we ﬁnd that, while these oﬀer a valuable startin g point, they often fall short in addressing AI’s complex\nethical issues due to their lack of contextual sensitivity [ 142]. We also believe that improving upon this necessitates a\nmore nuanced and context-speciﬁc approach to AI ethics, one that embraces the varied ethical dimensions presented\nby AI, including its environmental implications. Current e thical charters in AI, often detached from this perplexity,\nrepresent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [5].\nBy recognizing these limitations, we intend not to discard these deﬁnitions and principles but to improve upon them. In\nthis context, environmental and sustainability challenges related to AI development and deployment are integral to the\nbroader ethical reﬂections within the ﬁeld. This integrati on between ethics and sustainability in AI calls for a holist ic\napproach, where ethical considerations are not viewed in is olation but are intrinsically linked with environmental an d\nsustainability oversight. For instance, shifting from the dominant Western moral philosophies to include perspectiv es\nfrom non-Western traditions such as relational ethics [131 ], Ubuntu ethics [145], and Confucian ethics [111] can oﬀer\nvaluable insights into community, social harmony, and the i nterconnectedness of beings, emphasizing the impact of\nAI on society and interpersonal relationships.\nEvaluation. There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of\nconcluding that an AI system is neither truly ethical nor sus tainable. Recent work has begun bridging the gap; for\ninstance, work by Lynch et al. is inspired by the concept of ur gent governance in environmental studies, which distin-\nguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure\nand technologies [123]. Raji et al. use a similar approach fo r their proposed end-to-end framework for internal algo-\nrithmic auditing of AI models, which takes into account both technical and ethical assessments [159]. In a similar vein,\na recent model evaluation framework by Rakova et al., propos es an environmental justice-oriented lens to carry out\nalgorithmic audits [160], that of Genovesi and Mönig places sustainability at the center of Ethical AI certiﬁcation [70 ],\nwhile that of Metcalf et al. uses environmental impact asses sments as an example of a formal mechanism that can be\nused to inspire the assessment of AI technologies [129]. All of these approaches acknowledge that ethical decisions in\nAI have environmental consequences and vice versa, thus nec essitating a framing that accommodates both ethical and\nenvironmental responsibility.\nTransparency. When viewed as a means to foster greater accountability, tra nsparency takes on a central role: it\nbecomes a principle that enhances ethical compliance and pr omotes environmental responsibility, contributing to\nsustainability. For instance, integrating social transpa rency and sustainability can be exempliﬁed by an AI system\ndesigned for urban planning, such as an AI tool developed to o ptimize city layouts for eﬃciency. In this example,\nto include sustainability approaches, developers would al so provide information on the environmental footprint of\nrunning the AI system, such as energy consumption during dat a processing and potential environmental beneﬁts of\nthe proposed urban layouts, like reduced carbon emissions from optimized traﬃc ﬂows or green spaces. In this way, by\ndeepening the concept of transparency to include social and environmental aspects, we would move towards creating\nAI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts\nthey have and making more informed decisions based on the inf ormation at our disposal [51].\nEquity and Power. Equity is about ensuring fair access and participation in the beneﬁts and governance of technology\nacross diﬀerent communities, especially those historically marginalized. Philosophical perspectives on equity, drawing", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 11\nGeneralizability.", "metadata": {}}, {"text": "Given the observed disconnect between ethics and sustainability in the context of AI principles and\nframeworks, we ﬁnd that, while these oﬀer a valuable startin g point, they often fall short in addressing AI’s complex\nethical issues due to their lack of contextual sensitivity [ 142].", "metadata": {}}, {"text": "We also believe that improving upon this necessitates a\nmore nuanced and context-speciﬁc approach to AI ethics, one that embraces the varied ethical dimensions presented\nby AI, including its environmental implications.", "metadata": {}}, {"text": "Current e thical charters in AI, often detached from this perplexity,\nrepresent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [5].", "metadata": {}}, {"text": "By recognizing these limitations, we intend not to discard these deﬁnitions and principles but to improve upon them.", "metadata": {}}, {"text": "In\nthis context, environmental and sustainability challenges related to AI development and deployment are integral to the\nbroader ethical reﬂections within the ﬁeld.", "metadata": {}}, {"text": "This integrati on between ethics and sustainability in AI calls for a holist ic\napproach, where ethical considerations are not viewed in is olation but are intrinsically linked with environmental an d\nsustainability oversight.", "metadata": {}}, {"text": "For instance, shifting from the dominant Western moral philosophies to include perspectiv es\nfrom non-Western traditions such as relational ethics [131 ], Ubuntu ethics [145], and Confucian ethics [111] can oﬀer\nvaluable insights into community, social harmony, and the i nterconnectedness of beings, emphasizing the impact of\nAI on society and interpersonal relationships.", "metadata": {}}, {"text": "Evaluation.", "metadata": {}}, {"text": "There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of\nconcluding that an AI system is neither truly ethical nor sus tainable.", "metadata": {}}, {"text": "Recent work has begun bridging the gap;", "metadata": {}}, {"text": "for\ninstance, work by Lynch et al.", "metadata": {}}, {"text": "is inspired by the concept of ur gent governance in environmental studies, which distin-\nguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure\nand technologies [123].", "metadata": {}}, {"text": "Raji et al.", "metadata": {}}, {"text": "use a similar approach fo r their proposed end-to-end framework for internal algo-\nrithmic auditing of AI models, which takes into account both technical and ethical assessments [159].", "metadata": {}}, {"text": "In a similar vein,\na recent model evaluation framework by Rakova et al., propos es an environmental justice-oriented lens to carry out\nalgorithmic audits [160], that of Genovesi and Mönig places sustainability at the center of Ethical AI certiﬁcation [70 ],\nwhile that of Metcalf et al.", "metadata": {}}, {"text": "uses environmental impact asses sments as an example of a formal mechanism that can be\nused to inspire the assessment of AI technologies [129].", "metadata": {}}, {"text": "All of these approaches acknowledge that ethical decisions in\nAI have environmental consequences and vice versa, thus nec essitating a framing that accommodates both ethical and\nenvironmental responsibility.", "metadata": {}}, {"text": "Transparency.", "metadata": {}}, {"text": "When viewed as a means to foster greater accountability, tra nsparency takes on a central role: it\nbecomes a principle that enhances ethical compliance and pr omotes environmental responsibility, contributing to\nsustainability.", "metadata": {}}, {"text": "For instance, integrating social transpa rency and sustainability can be exempliﬁed by an AI system\ndesigned for urban planning, such as an AI tool developed to o ptimize city layouts for eﬃciency.", "metadata": {}}, {"text": "In this example,\nto include sustainability approaches, developers would al so provide information on the environmental footprint of\nrunning the AI system, such as energy consumption during dat a processing and potential environmental beneﬁts of\nthe proposed urban layouts, like reduced carbon emissions from optimized traﬃc ﬂows or green spaces.", "metadata": {}}, {"text": "In this way, by\ndeepening the concept of transparency to include social and environmental aspects, we would move towards creating\nAI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts\nthey have and making more informed decisions based on the inf ormation at our disposal [51].", "metadata": {}}, {"text": "Equity and Power.", "metadata": {}}, {"text": "Equity is about ensuring fair access and participation in the beneﬁts and governance of technology\nacross diﬀerent communities, especially those historically marginalized.", "metadata": {}}, {"text": "Philosophical perspectives on equity, drawing", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "12 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nfrom theories of distributive justice [106, 135], emphasiz e the necessity of equitable distribution of resources and r e-\nsponsibilities and risks associated with AI technologies. This principle is especially relevant in environmental jus tice,\nwhere the disproportionate impact of environmental harms on speciﬁc populations demands a reevaluation of AI tech-\nnologies are deployed at scale – for instance, Schlosberg’s theory of recognitional justice highlights the importance of\nrecognizing and respecting diverse community needs and val ues in environmental policies [176]. Additionally, equity\nrequires that AI development actively includes diverse voices in its creation and implementation phases, ensuring tha t\nAI systems do not perpetuate existing disparities but rathe r contribute to rectifying them. This approach draws upon\nenvironmental justice literature for developing principl es and frameworks that addresses both human and ecological\nneeds, thus framing equity as a matter of distribution, proc edural and interactional fairness [32].\n5.2 Research\nDespite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainabil-\nity and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given\ntheir socio-technicality and the consequences this can hav e on the human and non-human species residing in these\nregions.\nGeneralizability. At the nexus of AI ethics and sustainability, existing schol arship has already established that the\nmost disadvantaged and marginalized members of our societi es tend to be the least well-represented in the ‘Big Data’\nused in many AI models and systems [33, 43, 68, 186]; the same applies at the level of countries and regions, with ‘global’\ndatasets reﬂecting things like wealth and economic develop ment only being tested in a select few countries [24, 84]\nand the most extensive biodiversity datasets consisting of data collected in a subset of regions from the Global North,\nas well as regions close to cities and roads [15, 196]. Studyi ng the limits of application of AI systems both in terms\nof ethics and sustainability and how well they generalize to diﬀerent populations of human and non-human subjects\nis important to question existing assumptions. For instanc e, studies of emblematic datasets such as ImageNet found\nthat it to misrepresent both humans [2021] and other living b eings such as insects and ﬁsh [2023]. Developing new\ndatasets that are more representative of diverse populatio ns and contexts - such as the Dollar Street dataset [167] and\nCropHarvest [197] - and using this dataset in research and pr actice and help improve the applicability of AI systems\nand their ability to represent more diverse populations fro m both a societal and environmental perspective.\nEvaluation. As AI is increasingly used in the ﬁght against climate change , holistic evaluation of models and systems\nbecomes ever more relevant. For instance, in the high-stake s domain of solar geoengineering, which aims to develop\nnew ways for modifying the Earth’s climate to reduce the glob al warming eﬀect (i.e. by enhancing the reﬂexivity of\nclouds so that they reﬂect more of the sun’s rays), AI is often used to help model the potential far-reaching eﬀects\nof even minor interventions and understand how they will imp act local and global climate patterns [62, 148, 179].\nThis is because regional changes to the climate may trigger i ncreased fragility of some regions and not others, which\nis hard to quantify and therefore, to be optimized for in AI mo dels that aim to predict the consequences of climate\ninterventions [83]. For instance, diﬀerent thresholds of solar reﬂexivity are optimal for diﬀerent regions, and optimizing\nresults based on a given region (e.g. the continental United States) would make things worse for others (e.g. Western\nAfrica), which can suﬀer droughts and other forms of damage [ 126]. Considerations around the wider rebound eﬀects", "sentences": [{"text": "12 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nfrom theories of distributive justice [106, 135], emphasiz e the necessity of equitable distribution of resources and r e-\nsponsibilities and risks associated with AI technologies.", "metadata": {}}, {"text": "This principle is especially relevant in environmental jus tice,\nwhere the disproportionate impact of environmental harms on speciﬁc populations demands a reevaluation of AI tech-\nnologies are deployed at scale – for instance, Schlosberg’s theory of recognitional justice highlights the importance of\nrecognizing and respecting diverse community needs and val ues in environmental policies [176].", "metadata": {}}, {"text": "Additionally, equity\nrequires that AI development actively includes diverse voices in its creation and implementation phases, ensuring tha t\nAI systems do not perpetuate existing disparities but rathe r contribute to rectifying them.", "metadata": {}}, {"text": "This approach draws upon\nenvironmental justice literature for developing principl es and frameworks that addresses both human and ecological\nneeds, thus framing equity as a matter of distribution, proc edural and interactional fairness [32].", "metadata": {}}, {"text": "5.2 Research\nDespite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainabil-\nity and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given\ntheir socio-technicality and the consequences this can hav e on the human and non-human species residing in these\nregions.", "metadata": {}}, {"text": "Generalizability.", "metadata": {}}, {"text": "At the nexus of AI ethics and sustainability, existing schol arship has already established that the\nmost disadvantaged and marginalized members of our societi es tend to be the least well-represented in the ‘Big Data’\nused in many AI models and systems [33, 43, 68, 186];", "metadata": {}}, {"text": "the same applies at the level of countries and regions, with ‘global’\ndatasets reﬂecting things like wealth and economic develop ment only being tested in a select few countries [24, 84]\nand the most extensive biodiversity datasets consisting of data collected in a subset of regions from the Global North,\nas well as regions close to cities and roads [15, 196].", "metadata": {}}, {"text": "Studyi ng the limits of application of AI systems both in terms\nof ethics and sustainability and how well they generalize to diﬀerent populations of human and non-human subjects\nis important to question existing assumptions.", "metadata": {}}, {"text": "For instanc e, studies of emblematic datasets such as ImageNet found\nthat it to misrepresent both humans [2021] and other living b eings such as insects and ﬁsh [2023].", "metadata": {}}, {"text": "Developing new\ndatasets that are more representative of diverse populatio ns and contexts - such as the Dollar Street dataset [167] and\nCropHarvest [197] - and using this dataset in research and pr actice and help improve the applicability of AI systems\nand their ability to represent more diverse populations fro m both a societal and environmental perspective.", "metadata": {}}, {"text": "Evaluation.", "metadata": {}}, {"text": "As AI is increasingly used in the ﬁght against climate change , holistic evaluation of models and systems\nbecomes ever more relevant.", "metadata": {}}, {"text": "For instance, in the high-stake s domain of solar geoengineering, which aims to develop\nnew ways for modifying the Earth’s climate to reduce the glob al warming eﬀect (i.e.", "metadata": {}}, {"text": "by enhancing the reﬂexivity of\nclouds so that they reﬂect more of the sun’s rays), AI is often used to help model the potential far-reaching eﬀects\nof even minor interventions and understand how they will imp act local and global climate patterns [62, 148, 179].", "metadata": {}}, {"text": "This is because regional changes to the climate may trigger i ncreased fragility of some regions and not others, which\nis hard to quantify and therefore, to be optimized for in AI mo dels that aim to predict the consequences of climate\ninterventions [83].", "metadata": {}}, {"text": "For instance, diﬀerent thresholds of solar reﬂexivity are optimal for diﬀerent regions, and optimizing\nresults based on a given region (e.g.", "metadata": {}}, {"text": "the continental United States) would make things worse for others (e.g.", "metadata": {}}, {"text": "Western\nAfrica), which can suﬀer droughts and other forms of damage [ 126].", "metadata": {}}, {"text": "Considerations around the wider rebound eﬀects", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 13\nof proposed AI solutions are also important: for instance, in the case of AI systems that improve aircraft eﬃciency [108] ,\nmore eﬃcient aircraft can result in cheaper airfare and ther efore, more travel overall 5.\nTransparency. Falk and van Wynsberghe argue that transparency is a crucial factor for establishing “whether the\nnet environmental impact of AI for Sustainability is positi ve or how the positive impact on the respective sustainabili ty\ngoal can outweigh the very diﬀerent negative impact of the mo dels’ development on sustainability\" [61] – we would\nexpand this to include AI systems in general, which should us e a variety of mechanisms to communicate the costs and\npotential impacts of their systems on both the environment a nd society. In fact, as proposed by Ehsan et al., the notion\nof transparency in AI can be expanded to encompass \"social tr ansparency\", which involves integrating socio-technical\naspects in the description and understanding of AI systems [ 56]. Social transparency involves a portrayal of an AI\nsystem’s societal impacts, ethical considerations, and ev entually its environmental footprint. By doing so, it provi des\na more complete picture, making the AI system transparent bu t also understandable in a broader societal context.\nThis augmented view of transparency, which would integrate both social and environmental dimensions, resonates\nwith the increasing awareness that AI is not simply a technol ogical tool but a socio-technical system with extensive\nrepercussions, spanning both people and the environment [2 02].\nEquity. Recognizing that Green AI (or sustainable AI), i.e. the deve lopment and deployment of AI systems that puts\nthe emphasis solely on eﬃciency or the reduction of greenhou se gas emissions, is not necessarily inherently more\nequitable or just – for instance, if the more eﬃcient models a re not widely shared, or entail an increased usage of\ncompute due to their eﬃciency – is an important ﬁrst step towa rds improving the current direction towards bigger\nmodels. Recent research in both AI ethics and sustainabilit y has shed light on the extent to which AI systems enable\nthe ampliﬁcation of existing social inequities [17, 58, 115 ] as well as contributing to the preservation of the ecologic al\nstatus quo vis-a-vis to climate change [194]. Given these ﬁn dings, it is important to make equity-informed trade-oﬀs\nwhen developing and deploying AI – for instance by carrying o ut energy prediction in resource-constrained energy\ngrids, which are common in low- and middle-income countries [13], or by using AI to predict the impacts of changes\nin climate on societal aspects such as disease propagation a nd health [100].\n5.3 Governance\nAs the ﬁeld of AI ethics increasingly intersects with regula tion, including law and policy, it showcases its interdisci -\nplinary nature, meaning that in order to be successful, gove rnance initiatives must incorporate eﬀorts from diﬀerent\ndomains, depending on the context of the application.\nGeneralizability. While there is no single solution to complex questions invol ving governance over AI systems,\nvarious bottom-up governance approaches have been propose d based on the cultural, societal and geographical con-\nstraints of AI system deployment. Some of these follow the te nets of the M ¯aori culture, which is based on principles\nthat consider both impacts on nature as well as on fellow huma n beings, bridging the gap between ethics and sustain-\nability [82, 143]; others espouse those established by the I ndigenous AI community, which is based on both values and\npractices of social and environmental sustainability, bot h core to many Indigenous epistemologies [110]. Also, regu-\nlating the deployment of out-of-the-box AI solutions that o perate on the premise of generalizability without taking\n5This is often referred to as Jevons paradox, which observes that when technological progress improves the eﬃciency of technology, this actually results\nin its increased usage and increases overall resource use [9 3].", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 13\nof proposed AI solutions are also important: for instance, in the case of AI systems that improve aircraft eﬃciency [108] ,\nmore eﬃcient aircraft can result in cheaper airfare and ther efore, more travel overall 5.", "metadata": {}}, {"text": "Transparency.", "metadata": {}}, {"text": "Falk and van Wynsberghe argue that transparency is a crucial factor for establishing “whether the\nnet environmental impact of AI for Sustainability is positi ve or how the positive impact on the respective sustainabili ty\ngoal can outweigh the very diﬀerent negative impact of the mo dels’ development on sustainability\" [61] – we would\nexpand this to include AI systems in general, which should us e a variety of mechanisms to communicate the costs and\npotential impacts of their systems on both the environment a nd society.", "metadata": {}}, {"text": "In fact, as proposed by Ehsan et al., the notion\nof transparency in AI can be expanded to encompass \"social tr ansparency\", which involves integrating socio-technical\naspects in the description and understanding of AI systems [ 56].", "metadata": {}}, {"text": "Social transparency involves a portrayal of an AI\nsystem’s societal impacts, ethical considerations, and ev entually its environmental footprint.", "metadata": {}}, {"text": "By doing so, it provi des\na more complete picture, making the AI system transparent bu t also understandable in a broader societal context.", "metadata": {}}, {"text": "This augmented view of transparency, which would integrate both social and environmental dimensions, resonates\nwith the increasing awareness that AI is not simply a technol ogical tool but a socio-technical system with extensive\nrepercussions, spanning both people and the environment [2 02].", "metadata": {}}, {"text": "Equity.", "metadata": {}}, {"text": "Recognizing that Green AI (or sustainable AI), i.e.", "metadata": {}}, {"text": "the deve lopment and deployment of AI systems that puts\nthe emphasis solely on eﬃciency or the reduction of greenhou se gas emissions, is not necessarily inherently more\nequitable or just – for instance, if the more eﬃcient models a re not widely shared, or entail an increased usage of\ncompute due to their eﬃciency – is an important ﬁrst step towa rds improving the current direction towards bigger\nmodels.", "metadata": {}}, {"text": "Recent research in both AI ethics and sustainabilit y has shed light on the extent to which AI systems enable\nthe ampliﬁcation of existing social inequities [17, 58, 115 ] as well as contributing to the preservation of the ecologic al\nstatus quo vis-a-vis to climate change [194].", "metadata": {}}, {"text": "Given these ﬁn dings, it is important to make equity-informed trade-oﬀs\nwhen developing and deploying AI – for instance by carrying o ut energy prediction in resource-constrained energy\ngrids, which are common in low- and middle-income countries [13], or by using AI to predict the impacts of changes\nin climate on societal aspects such as disease propagation a nd health [100].", "metadata": {}}, {"text": "5.3 Governance\nAs the ﬁeld of AI ethics increasingly intersects with regula tion, including law and policy, it showcases its interdisci -\nplinary nature, meaning that in order to be successful, gove rnance initiatives must incorporate eﬀorts from diﬀerent\ndomains, depending on the context of the application.", "metadata": {}}, {"text": "Generalizability.", "metadata": {}}, {"text": "While there is no single solution to complex questions invol ving governance over AI systems,\nvarious bottom-up governance approaches have been propose d based on the cultural, societal and geographical con-\nstraints of AI system deployment.", "metadata": {}}, {"text": "Some of these follow the te nets of the M ¯aori culture, which is based on principles\nthat consider both impacts on nature as well as on fellow huma n beings, bridging the gap between ethics and sustain-\nability [82, 143];", "metadata": {}}, {"text": "others espouse those established by the I ndigenous AI community, which is based on both values and\npractices of social and environmental sustainability, bot h core to many Indigenous epistemologies [110].", "metadata": {}}, {"text": "Also, regu-\nlating the deployment of out-of-the-box AI solutions that o perate on the premise of generalizability without taking\n5This is often referred to as Jevons paradox, which observes that when technological progress improves the eﬃciency of technology, this actually results\nin its increased usage and increases overall resource use [9 3].", "metadata": {}}], "metadata": {"page": 13}}, {"text": "[Image page=13 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "sentences": [{"text": "[Image page=13 idx=1 name=~0~.png] Size: 1x1, Data: 67 bytes", "metadata": {}}], "metadata": {"page": 13, "image_index": 1, "image_name": "~0~.png", "image_width": 1, "image_height": 1, "attachment_type": "image", "has_image_data": true, "image_data_size": 67}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "14 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\ncontext into account can help ensure that systems that are me ant to be widely applicable are truly representative of\nthe context of application – where evaluations, as explaine d below, will play a crucial role.\nEvaluation. As noted by Metcalf et al. [2021], there is a parallel between environmental impact assessments and AI\nethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such\nas energy consumption and carbon emissions. Requiring audi ts of commercial AI systems before their deployment in\npractice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but\nalso in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental\nimpacts, will require the development of new governance app roaches. For instance, attempting to evaluate the wider\nrebound eﬀects of AI tools and their impacts on consumption a nd human behavior is important to represent their\nbroader impacts on both society and the environment [86, 96, 194]. Finally, integrating both social and environmental\nassessments into existing and in-progress regulation and d eveloping new approaches to evaluate these impacts can\nhelp ensure that the deployment of AI systems is carried out i n a way that is ethically sound and sustainable across\nmultiple dimensions.\nTransparency. Recent years have seen less transparency in AI research and practice, especially in terms of generative\nAI models [189]. However, as these systems are increasingly being deployed in society, having more information\nregarding how these systems were created and deployed remains paramount. Ensuring that enough details are provided\nboth regarding the energy consumed and greenhouse gasses em itted during model training and deployment can help\ntrack how the environmental impacts of AI are evolving over t ime. Mandating transparency for already deployed AI\nsystems can help establish audits, red teaming eﬀorts and AI energy score ratings to raise users’ awareness around the\nimpacts of the systems they use [29], contributing to what can be termed \"usable transparency\" [144]. By ensuring that\nthe processes and results of these practices are well-docum ented and publicly accessible, stakeholders would be bette r\nequipped to understand and evaluate the ethical and sustain able aspects of AI systems. Such policies would promote\na culture of openness in the AI industry, encouraging develo pers to prioritize ethical considerations and sustainabil ity\nalongside technical advancements.\nEquity. Involving multiple stakeholders, especially ones from the concerned communities and domains, in this pro-\ncess is important to ensure that diﬀerent perspectives and l ived experiences are reﬂected in the development and\ndeployment of AI systems [170, 171] as well as the diﬀerent co mmunities and can inﬂuence existing and future prac-\ntices [45, 126]. From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society)\nproject proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI\nsystems [133], whereas the White House Oﬃce of Management an d Budget (OMB)’s ﬁrst government-wide policy\naround the usage of AI includes, inter alia, clauses that stipulate that government agencies should tak e both envi-\nronmental impacts and bias and fairness into account when pr ocuring AI-enabled services [59] - exhibiting thought\nleadership that will hopefully have wider repercussions.\n6 CONCLUSION\nWe recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies\nlike AI, it can be diﬃcult to deﬁne what progress looks like an d how it can be achieved. We do not pretend to have\ndeveloped a universal approach for either of these issues (a nd do not believe that one can exist) – but by adopting\na multitude of endeavors such as the ones described in the par agraphs above can help involve diﬀerent actors and", "sentences": [{"text": "14 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\ncontext into account can help ensure that systems that are me ant to be widely applicable are truly representative of\nthe context of application – where evaluations, as explaine d below, will play a crucial role.", "metadata": {}}, {"text": "Evaluation.", "metadata": {}}, {"text": "As noted by Metcalf et al.", "metadata": {}}, {"text": "[2021], there is a parallel between environmental impact assessments and AI\nethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such\nas energy consumption and carbon emissions.", "metadata": {}}, {"text": "Requiring audi ts of commercial AI systems before their deployment in\npractice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but\nalso in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental\nimpacts, will require the development of new governance app roaches.", "metadata": {}}, {"text": "For instance, attempting to evaluate the wider\nrebound eﬀects of AI tools and their impacts on consumption a nd human behavior is important to represent their\nbroader impacts on both society and the environment [86, 96, 194].", "metadata": {}}, {"text": "Finally, integrating both social and environmental\nassessments into existing and in-progress regulation and d eveloping new approaches to evaluate these impacts can\nhelp ensure that the deployment of AI systems is carried out i n a way that is ethically sound and sustainable across\nmultiple dimensions.", "metadata": {}}, {"text": "Transparency.", "metadata": {}}, {"text": "Recent years have seen less transparency in AI research and practice, especially in terms of generative\nAI models [189].", "metadata": {}}, {"text": "However, as these systems are increasingly being deployed in society, having more information\nregarding how these systems were created and deployed remains paramount.", "metadata": {}}, {"text": "Ensuring that enough details are provided\nboth regarding the energy consumed and greenhouse gasses em itted during model training and deployment can help\ntrack how the environmental impacts of AI are evolving over t ime.", "metadata": {}}, {"text": "Mandating transparency for already deployed AI\nsystems can help establish audits, red teaming eﬀorts and AI energy score ratings to raise users’ awareness around the\nimpacts of the systems they use [29], contributing to what can be termed \"usable transparency\" [144].", "metadata": {}}, {"text": "By ensuring that\nthe processes and results of these practices are well-docum ented and publicly accessible, stakeholders would be bette r\nequipped to understand and evaluate the ethical and sustain able aspects of AI systems.", "metadata": {}}, {"text": "Such policies would promote\na culture of openness in the AI industry, encouraging develo pers to prioritize ethical considerations and sustainabil ity\nalongside technical advancements.", "metadata": {}}, {"text": "Equity.", "metadata": {}}, {"text": "Involving multiple stakeholders, especially ones from the concerned communities and domains, in this pro-\ncess is important to ensure that diﬀerent perspectives and l ived experiences are reﬂected in the development and\ndeployment of AI systems [170, 171] as well as the diﬀerent co mmunities and can inﬂuence existing and future prac-\ntices [45, 126].", "metadata": {}}, {"text": "From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society)\nproject proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI\nsystems [133], whereas the White House Oﬃce of Management an d Budget (OMB)’s ﬁrst government-wide policy\naround the usage of AI includes, inter alia, clauses that stipulate that government agencies should tak e both envi-\nronmental impacts and bias and fairness into account when pr ocuring AI-enabled services [59] - exhibiting thought\nleadership that will hopefully have wider repercussions.", "metadata": {}}, {"text": "6 CONCLUSION\nWe recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies\nlike AI, it can be diﬃcult to deﬁne what progress looks like an d how it can be achieved.", "metadata": {}}, {"text": "We do not pretend to have\ndeveloped a universal approach for either of these issues (a nd do not believe that one can exist) – but by adopting\na multitude of endeavors such as the ones described in the par agraphs above can help involve diﬀerent actors and", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 15\nhopefully build momentum across the AI community. The beaut y of making transversal connections that go above\nand beyond the silos in which many AI technologists tend to op erate is that we can also be inspired by the multitude\nof rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and\nclimate science, to propose ways forward that would allow the AI community to improve systems from the perspective\nof both ethics and sustainability. Looking forward, the ﬁeld of AI ethics is rapidly evolving, with insights racing to keep\nup with the rapid pace of technological advancements in AI. T his dynamic landscape presents an ongoing challenge:\nto develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts. Furthermore, it is\nbecoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a\nholistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical\nsocietal and environmental consequences. Therefore, inte grating AI ethics with sustainability is not just beneﬁcial\nbut necessary, ensuring that AI advancements are not only te chnologically innovative and ethically sound but also\nmaximizing their potential to engender sustainable advanc ement.", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 15\nhopefully build momentum across the AI community.", "metadata": {}}, {"text": "The beaut y of making transversal connections that go above\nand beyond the silos in which many AI technologists tend to op erate is that we can also be inspired by the multitude\nof rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and\nclimate science, to propose ways forward that would allow the AI community to improve systems from the perspective\nof both ethics and sustainability.", "metadata": {}}, {"text": "Looking forward, the ﬁeld of AI ethics is rapidly evolving, with insights racing to keep\nup with the rapid pace of technological advancements in AI.", "metadata": {}}, {"text": "T his dynamic landscape presents an ongoing challenge:\nto develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts.", "metadata": {}}, {"text": "Furthermore, it is\nbecoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a\nholistic societal impact.", "metadata": {}}, {"text": "The absence of either aspect leads to an incomplete perspective, potentially overlooking critical\nsocietal and environmental consequences.", "metadata": {}}, {"text": "Therefore, inte grating AI ethics with sustainability is not just beneﬁcial\nbut necessary, ensuring that AI advancements are not only te chnologically innovative and ethically sound but also\nmaximizing their potential to engender sustainable advanc ement.", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "16 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoo die Project: Big tobacco, big tech, and the threat on academic integrity. InProceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 287–297.\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie N évéol, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. Th e elephant in the\nroom: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n[3] AI Safety Summit. 2023. The Bletchley Declaration by Cou ntries Attending the AI Safety Summit, 1-2 November 2023. (2 023).\nhttps://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchl ey-declaration-by-countries-attending-the-ai-safety -summit-1- \n[4] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud.\n[5] Daniel Andler. 2023. Intelligence artiﬁcielle, intelligence humaine: le doubl e énigme. Gallimard, Paris.\n[6] Julia Angwin, Jeﬀ Larson, Surya Mattu, and Lauren Kirchn er. 2022. Machine bias. In Ethics of data and analytics. Auerbach Publications, 254–264.\n[7] Thomas Aquinas. 1702. Summa theologica. J. Mentelin.\n[8] Aristotle. 350. Nicomachean Ethics. Original work published in 350 B.C.E..\n[9] Hutan Ashraﬁan. 2015. Intelligent robots must uphold hu man rights. Nature 519 (2015), 391. https://doi.org/10.1038/519391a\n[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and De borah Raji. 2022. Disentangling the Components of Ethical Research in Machine Learning.\nIn Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul, Republic of Korea) (FAccT ’22) . Association for\nComputing Machinery, New York, NY, USA, 2057–2068. https:/ /doi.org/10.1145/3531146.3533781\n[11] Ricardo Baeza-Yates and Zeinab Liaghat. 2017. Quality -eﬃciency trade-oﬀs in machine learning for text processin g. In 2017 IEEE international\nconference on big data (big data) . IEEE, 897–904.\n[12] Edward B Barbier. 1987. The concept of sustainable econ omic development. Environmental conservation 14, 2 (1987), 101–110.\n[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Jo yce, and Margaret Odero. 2023. Topology Estimation from Vol tage Edge Sensing for\nResource-Constrained Grids. Tackling Climate Change with Machine Learning Workshop - IC LR 2023 (2023).\n[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante , and Azzurra Ragone. 2023. A Rapid Review of Responsible AI f rameworks: How to guide\nthe development of ethical AI. In Proceedings of the 27th International Conference on Evaluat ion and Assessment in Software Engineering . 358–367.\n[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfga ng Schwanghart. 2014. Spatial bias in the GBIF database and i ts eﬀect on modeling\nspecies’ geographic distributions. Ecological Informatics 19 (2014), 10–15.\n[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Majo r, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accounta bility, and Transparency (Virtual Event, Canada) (FAccT ’21) .\nAssociation for Computing Machinery, New York, NY, USA, 610 –623. https://doi.org/10.1145/3442188.3445922\n[17] Ruha Benjamin. 2023. Race after technology. In Social Theory Re-Wired. Routledge, 405–415.\n[18] Jeremy Bentham. 1789. An Introduction to the Principles of Morals and Legislation . T. Payne and Son.\n[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michae l, Lennart Heim, Xueyun Luo, and Neil Thompson. 2024. The Com pute Divide in\nMachine Learning: A Threat to Academic Contribution and Scr utiny? arXiv preprint arXiv:2401.02452 (2024).\n[20] Joseph R Biden. 2023. Executive order on the safe, secur e, and trustworthy development and use of artiﬁcial intelli gence. (2023).\n[21] Elettra Bietti. 2020. From Ethics Washing to Ethics Bas hing: A View on Tech Ethics from within Moral Philosophy. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency(Barcelona, Spain) (FAT* ’20) . Association for Computing Machinery, New York, NY, USA,\n210–219. https://doi.org/10.1145/3351095.3372860\n[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, Willia m Agnew, Ravit Dotan, and Michelle Bao. 2022. The Values Enco ded in Machine Learning\nResearch. arXiv:2106.15590 [cs.LG]\n[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S . Brown, Johnathan Flowers, Anthony Ventresque, and Christ opher L. Dancy. 2022.\nThe forgotten margins of AI ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency. 948–958.\n[24] Joshua Blumenstock. 2018. Don’t forget people in the us e of big data for development.\n[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van W ynsberghe. 2022. From an Ethics of Carefulness to an Ethics o f Desirability: Going\nBeyond Current Ethics Approaches to Sustainable AI. Sustainability 14, 8 (2022). https://doi.org/10.3390/su14084472\n[26] Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holis tic evaluation of language models. Annals of the New York Academy of Sciences 1525, 1\n(2023), 140–146.\n[27] Mioara Borza. 2014. The connection between eﬃciency an d sustainability–a theoretical approach. Procedia Economics and Finance 15 (2014),\n1355–1363.\n[28] Andrew Brennan and Norva Y. S. Lo. 2022. Environmental E thics. In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N. Zalta\n(Ed.). Metaphysics Research Lab, Stanford University.\n[29] Benedetta Brevini. 2020. Black boxes, not green: Mytho logizing artiﬁcial intelligence and omitting the environm ent. Big Data & Society 7, 2\n(2020), 2053951720935141. https://doi.org/10.1177/205 3951720935141\n[30] Benedetta Brevini. 2023. Myths, techno solutionism an d artiﬁcial intelligence: reclaiming AI materiality and it s massive environmental costs. In\nHandbook of Critical Studies of Artiﬁcial Intelligence . Edward Elgar Publishing, 869–877.\n[31] Alexander EI Brownlee, Jason Adair, Saemundur O Harald sson, and John Jabbo. 2021. Exploring the accuracy–energy t rade-oﬀ in machine\nlearning. In 2021 IEEE/ACM International Workshop on Genetic Improveme nt (GI). IEEE, 11–18.", "sentences": [{"text": "16 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The Grey Hoo die Project: Big tobacco, big tech, and the threat on academic integrity.", "metadata": {}}, {"text": "InProceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society .", "metadata": {}}, {"text": "287–297.", "metadata": {}}, {"text": "[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie N évéol, Fanny Ducel, Saif M Mohammad, and Karën Fort.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Th e elephant in the\nroom: Analyzing the presence of big tech in natural language processing research.", "metadata": {}}, {"text": "arXiv preprint arXiv:2305.02797 (2023).", "metadata": {}}, {"text": "[3] AI Safety Summit.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The Bletchley Declaration by Cou ntries Attending the AI Safety Summit, 1-2 November 2023.", "metadata": {}}, {"text": "(2 023).", "metadata": {}}, {"text": "https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchl ey-declaration-by-countries-attending-the-ai-safety -summit-1- \n[4] Amazon Web Services.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Sustainability in the Cloud.", "metadata": {}}, {"text": "https://sustainability.aboutamazon.com/environment/the-cloud.", "metadata": {}}, {"text": "[5] Daniel Andler.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Intelligence artiﬁcielle, intelligence humaine: le doubl e énigme.", "metadata": {}}, {"text": "Gallimard, Paris.", "metadata": {}}, {"text": "[6] Julia Angwin, Jeﬀ Larson, Surya Mattu, and Lauren Kirchn er.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Machine bias.", "metadata": {}}, {"text": "In Ethics of data and analytics.", "metadata": {}}, {"text": "Auerbach Publications, 254–264.", "metadata": {}}, {"text": "[7] Thomas Aquinas.", "metadata": {}}, {"text": "1702.", "metadata": {}}, {"text": "Summa theologica.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Mentelin.", "metadata": {}}, {"text": "[8] Aristotle.", "metadata": {}}, {"text": "350.", "metadata": {}}, {"text": "Nicomachean Ethics.", "metadata": {}}, {"text": "Original work published in 350 B.C.E..", "metadata": {}}, {"text": "[9] Hutan Ashraﬁan.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Intelligent robots must uphold hu man rights.", "metadata": {}}, {"text": "Nature 519 (2015), 391.", "metadata": {}}, {"text": "https://doi.org/10.1038/519391a\n[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and De borah Raji.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Disentangling the Components of Ethical Research in Machine Learning.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul, Republic of Korea) (FAccT ’22) .", "metadata": {}}, {"text": "Association for\nComputing Machinery, New York, NY, USA, 2057–2068.", "metadata": {}}, {"text": "https:/ /doi.org/10.1145/3531146.3533781\n[11] Ricardo Baeza-Yates and Zeinab Liaghat.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Quality -eﬃciency trade-oﬀs in machine learning for text processin g.", "metadata": {}}, {"text": "In 2017 IEEE international\nconference on big data (big data) .", "metadata": {}}, {"text": "IEEE, 897–904.", "metadata": {}}, {"text": "[12] Edward B Barbier.", "metadata": {}}, {"text": "1987.", "metadata": {}}, {"text": "The concept of sustainable econ omic development.", "metadata": {}}, {"text": "Environmental conservation 14, 2 (1987), 101–110.", "metadata": {}}, {"text": "[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Jo yce, and Margaret Odero.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Topology Estimation from Vol tage Edge Sensing for\nResource-Constrained Grids.", "metadata": {}}, {"text": "Tackling Climate Change with Machine Learning Workshop - IC LR 2023 (2023).", "metadata": {}}, {"text": "[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante , and Azzurra Ragone.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "A Rapid Review of Responsible AI f rameworks: How to guide\nthe development of ethical AI.", "metadata": {}}, {"text": "In Proceedings of the 27th International Conference on Evaluat ion and Assessment in Software Engineering .", "metadata": {}}, {"text": "358–367.", "metadata": {}}, {"text": "[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfga ng Schwanghart.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Spatial bias in the GBIF database and i ts eﬀect on modeling\nspecies’ geographic distributions.", "metadata": {}}, {"text": "Ecological Informatics 19 (2014), 10–15.", "metadata": {}}, {"text": "[16] Emily M.", "metadata": {}}, {"text": "Bender, Timnit Gebru, Angelina McMillan-Majo r, and Shmargaret Shmitchell.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?.", "metadata": {}}, {"text": "In Proceedings of the 2021 ACM Conference on Fairness, Accounta bility, and Transparency (Virtual Event, Canada) (FAccT ’21) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 610 –623.", "metadata": {}}, {"text": "https://doi.org/10.1145/3442188.3445922\n[17] Ruha Benjamin.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Race after technology.", "metadata": {}}, {"text": "In Social Theory Re-Wired.", "metadata": {}}, {"text": "Routledge, 405–415.", "metadata": {}}, {"text": "[18] Jeremy Bentham.", "metadata": {}}, {"text": "1789.", "metadata": {}}, {"text": "An Introduction to the Principles of Morals and Legislation .", "metadata": {}}, {"text": "T.", "metadata": {}}, {"text": "Payne and Son.", "metadata": {}}, {"text": "[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michae l, Lennart Heim, Xueyun Luo, and Neil Thompson.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "The Com pute Divide in\nMachine Learning: A Threat to Academic Contribution and Scr utiny?", "metadata": {}}, {"text": "arXiv preprint arXiv:2401.02452 (2024).", "metadata": {}}, {"text": "[20] Joseph R Biden.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Executive order on the safe, secur e, and trustworthy development and use of artiﬁcial intelli gence.", "metadata": {}}, {"text": "(2023).", "metadata": {}}, {"text": "[21] Elettra Bietti.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "From Ethics Washing to Ethics Bas hing: A View on Tech Ethics from within Moral Philosophy.", "metadata": {}}, {"text": "In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency(Barcelona, Spain) (FAT* ’20) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA,\n210–219.", "metadata": {}}, {"text": "https://doi.org/10.1145/3351095.3372860\n[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, Willia m Agnew, Ravit Dotan, and Michelle Bao.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The Values Enco ded in Machine Learning\nResearch.", "metadata": {}}, {"text": "arXiv:2106.15590 [cs.LG]\n[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S .", "metadata": {}}, {"text": "Brown, Johnathan Flowers, Anthony Ventresque, and Christ opher L.", "metadata": {}}, {"text": "Dancy.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The forgotten margins of AI ethics.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency.", "metadata": {}}, {"text": "948–958.", "metadata": {}}, {"text": "[24] Joshua Blumenstock.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Don’t forget people in the us e of big data for development.", "metadata": {}}, {"text": "[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van W ynsberghe.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "From an Ethics of Carefulness to an Ethics o f Desirability: Going\nBeyond Current Ethics Approaches to Sustainable AI.", "metadata": {}}, {"text": "Sustainability 14, 8 (2022).", "metadata": {}}, {"text": "https://doi.org/10.3390/su14084472\n[26] Rishi Bommasani, Percy Liang, and Tony Lee.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Holis tic evaluation of language models.", "metadata": {}}, {"text": "Annals of the New York Academy of Sciences 1525, 1\n(2023), 140–146.", "metadata": {}}, {"text": "[27] Mioara Borza.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "The connection between eﬃciency an d sustainability–a theoretical approach.", "metadata": {}}, {"text": "Procedia Economics and Finance 15 (2014),\n1355–1363.", "metadata": {}}, {"text": "[28] Andrew Brennan and Norva Y.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Lo.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Environmental E thics.", "metadata": {}}, {"text": "In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N.", "metadata": {}}, {"text": "Zalta\n(Ed.).", "metadata": {}}, {"text": "Metaphysics Research Lab, Stanford University.", "metadata": {}}, {"text": "[29] Benedetta Brevini.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Black boxes, not green: Mytho logizing artiﬁcial intelligence and omitting the environm ent.", "metadata": {}}, {"text": "Big Data & Society 7, 2\n(2020), 2053951720935141.", "metadata": {}}, {"text": "https://doi.org/10.1177/205 3951720935141\n[30] Benedetta Brevini.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Myths, techno solutionism an d artiﬁcial intelligence: reclaiming AI materiality and it s massive environmental costs.", "metadata": {}}, {"text": "In\nHandbook of Critical Studies of Artiﬁcial Intelligence .", "metadata": {}}, {"text": "Edward Elgar Publishing, 869–877.", "metadata": {}}, {"text": "[31] Alexander EI Brownlee, Jason Adair, Saemundur O Harald sson, and John Jabbo.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Exploring the accuracy–energy t rade-oﬀ in machine\nlearning.", "metadata": {}}, {"text": "In 2021 IEEE/ACM International Workshop on Genetic Improveme nt (GI).", "metadata": {}}, {"text": "IEEE, 11–18.", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 17\n[32] Robert D. Bullard, Glenn S. Johnson, and Beverly H. Wrig ht. 1997. Confronting Environmental Injustice: It’s the Ri ght Thing to Do. Race, Gender\n& Class 5, 1 (1997), 63–79.\n[33] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: I ntersectional accuracy disparities in commercial gender c lassiﬁcation. In Conference\non fairness, accountability and transparency . PMLR, 77–91.\n[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Fr anch, and Justus Bogner. 2023. Exploring the Carbon Footprint of Hugging Face’s ML Models:\nA Repository Mining Study. arXiv preprint arXiv:2305.11164 (2023).\n[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelin a Wang. 2021. The limits of global inclusion in AI developmen t. arXiv preprint\narXiv:2102.01265 (2021).\n[36] Kyla Chasalow and Karen Levy. 2021. Representativenes s in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency . 77–89.\n[37] Rachel Chason and Rael Ombuor. 2021. A lack of weather da ta in Africa is thwarting critical climate research.\nhttps://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/\n[38] Chinese Data Law Alliance. 2023. Chinese Artiﬁcial Int elligence Law, v. 1.0. https://mp.weixin.qq.com/s/85D8T jMkN9Tl-oWjq15JiQ\n[39] Concil of EU. 2022. Artiﬁcial Intelligence Act: Counci l calls for promoting safe AI that respects fundamental righ ts.\nhttps://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artiﬁcial-intelligence-act-council- calls-for-promoting-safe-ai-that-respects-fundament al-rights/\n[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy B uolamwini. 2022. Who Audits the Auditors? Recommendations from a ﬁeld scan of the\nalgorithmic auditing ecosystem. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency. 1571–1583.\n[41] Josh Cowls, Andreas Tsamados, Mariarosaria Taddeo, an d Luciano Floridi. 2023. The AI gambit: leveraging artiﬁcial intelligence to combat climate\nchange—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.\n[42] Kate Crawford and Trevor Paglen. 2021. Excavating AI: T he politics of images in machine learning training sets. Ai & Society 36, 4 (2021),\n1105–1116.\n[43] Rowena Cullen. 2001. Addressing the digital divide. Online information review 25, 5 (2001), 311–320.\n[44] Amane Dannouni, Stefan A. Deutscher, Ghita Dezzaz, Ada m Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid\nMaher, David Patterson, Edmond Rhys Jones, Juliet Rothenbe rg, Hamza Tber, Maud Texier, and Ali Ziat. 2023. Acceleratin g Climate Action with\nAI. https://www.gstatic.com/gumdrop/sustainability/a ccelerating-climate-action-ai.pdf\n[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur. 2022. Recommendations for ethical and responsible use of artiﬁcial intelligence in digital\nagriculture. Frontiers in Artiﬁcial Intelligence 5 (2022), 884192.\n[46] Hannah Davis. 2020. A Dataset is a Worldview. Towards Data Science (2020).\n[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, an d Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical im age database. In 2009 IEEE\nconference on computer vision and pattern recognition . IEEE, 248–255.\n[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Sol daini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Gho sh, Kyra Yee, Irene Font\nPeradejordi, et al. 2023. Bound by the Bounty: Collaborativ ely Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023\nAAAI/ACM Conference on AI, Ethics, and Society . 375–386.\n[49] Fatma Denton. 2002. Climate change vulnerability, imp acts, and adaptation: Why does gender matter? Gender & Development 10, 2 (2002), 10–20.\n[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[51] Nicholas Diakopoulos. 2020. Accountability, transpa rency, and algorithms. The Oxford handbook of ethics of AI 17, 4 (2020), 197.\n[52] Roel Dobbe and Meredith Whittaker. 2019. AI and climate change: how they’re connected, and what we can do about it. AI Now Institute 17\n(2019).\n[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Er ika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Lu ccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carb on intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency. 1877–1894.\n[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew , Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Docu-\nmenting large webtext corpora: A case study on the colossal c lean crawled corpus. arXiv preprint arXiv:2104.08758 (2021).\n[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma. 2018. Neo-colonialism and Millennium Development Goals (M DGs) in Africa: A\nblend of an old wine in a new bottle. African Journal of Science, Technology, Innovation and Dev elopment 10, 3 (2018), 355–366.\n[56] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towar ds Social Transparency\nin AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Com puting Systems (Yokohama, Japan) (CHI ’21) . Association for\nComputing Machinery, New York, NY, USA, Article 82, 19 pages . https://doi.org/10.1145/3411764.3445188\n[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market im pact potential of\nlarge language models. arXiv preprint arXiv:2303.10130 (2023).\n[58] Virginia Eubanks. 2018. Automating inequality: How high-tech tools proﬁle, police , and punish the poor . St. Martin’s Press.\n[59] Executive Oﬃce of the President, Oﬃce of Management and Budget. 2024. Memorandum for the heads of executive depart-\nments and agencies on Advancing Governance, Innovation, an d Risk Management for Agency Use of Artiﬁcial Intelligence.\nhttps://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk -Management-for-Agency-Use-of-Artiﬁcial-Intelligenc e.pdf", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 17\n[32] Robert D.", "metadata": {}}, {"text": "Bullard, Glenn S.", "metadata": {}}, {"text": "Johnson, and Beverly H.", "metadata": {}}, {"text": "Wrig ht.", "metadata": {}}, {"text": "1997.", "metadata": {}}, {"text": "Confronting Environmental Injustice: It’s the Ri ght Thing to Do.", "metadata": {}}, {"text": "Race, Gender\n& Class 5, 1 (1997), 63–79.", "metadata": {}}, {"text": "[33] Joy Buolamwini and Timnit Gebru.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Gender shades: I ntersectional accuracy disparities in commercial gender c lassiﬁcation.", "metadata": {}}, {"text": "In Conference\non fairness, accountability and transparency .", "metadata": {}}, {"text": "PMLR, 77–91.", "metadata": {}}, {"text": "[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Fr anch, and Justus Bogner.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Exploring the Carbon Footprint of Hugging Face’s ML Models:\nA Repository Mining Study.", "metadata": {}}, {"text": "arXiv preprint arXiv:2305.11164 (2023).", "metadata": {}}, {"text": "[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelin a Wang.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The limits of global inclusion in AI developmen t.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2102.01265 (2021).", "metadata": {}}, {"text": "[36] Kyla Chasalow and Karen Levy.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Representativenes s in statistics, politics, and machine learning.", "metadata": {}}, {"text": "In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency .", "metadata": {}}, {"text": "77–89.", "metadata": {}}, {"text": "[37] Rachel Chason and Rael Ombuor.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "A lack of weather da ta in Africa is thwarting critical climate research.", "metadata": {}}, {"text": "https://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/\n[38] Chinese Data Law Alliance.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Chinese Artiﬁcial Int elligence Law, v.", "metadata": {}}, {"text": "1.0.", "metadata": {}}, {"text": "https://mp.weixin.qq.com/s/85D8T jMkN9Tl-oWjq15JiQ\n[39] Concil of EU.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Artiﬁcial Intelligence Act: Counci l calls for promoting safe AI that respects fundamental righ ts.", "metadata": {}}, {"text": "https://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artiﬁcial-intelligence-act-council- calls-for-promoting-safe-ai-that-respects-fundament al-rights/\n[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy B uolamwini.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Who Audits the Auditors?", "metadata": {}}, {"text": "Recommendations from a ﬁeld scan of the\nalgorithmic auditing ecosystem.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency.", "metadata": {}}, {"text": "1571–1583.", "metadata": {}}, {"text": "[41] Josh Cowls, Andreas Tsamados, Mariarosaria Taddeo, an d Luciano Floridi.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The AI gambit: leveraging artiﬁcial intelligence to combat climate\nchange—opportunities, challenges, and recommendations.", "metadata": {}}, {"text": "Ai & Society (2023), 1–25.", "metadata": {}}, {"text": "[42] Kate Crawford and Trevor Paglen.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Excavating AI: T he politics of images in machine learning training sets.", "metadata": {}}, {"text": "Ai & Society 36, 4 (2021),\n1105–1116.", "metadata": {}}, {"text": "[43] Rowena Cullen.", "metadata": {}}, {"text": "2001.", "metadata": {}}, {"text": "Addressing the digital divide.", "metadata": {}}, {"text": "Online information review 25, 5 (2001), 311–320.", "metadata": {}}, {"text": "[44] Amane Dannouni, Stefan A.", "metadata": {}}, {"text": "Deutscher, Ghita Dezzaz, Ada m Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid\nMaher, David Patterson, Edmond Rhys Jones, Juliet Rothenbe rg, Hamza Tber, Maud Texier, and Ali Ziat.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Acceleratin g Climate Action with\nAI.", "metadata": {}}, {"text": "https://www.gstatic.com/gumdrop/sustainability/a ccelerating-climate-action-ai.pdf\n[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Recommendations for ethical and responsible use of artiﬁcial intelligence in digital\nagriculture.", "metadata": {}}, {"text": "Frontiers in Artiﬁcial Intelligence 5 (2022), 884192.", "metadata": {}}, {"text": "[46] Hannah Davis.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "A Dataset is a Worldview.", "metadata": {}}, {"text": "Towards Data Science (2020).", "metadata": {}}, {"text": "[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, an d Li Fei-Fei.", "metadata": {}}, {"text": "2009.", "metadata": {}}, {"text": "ImageNet: A large-scale hierarchical im age database.", "metadata": {}}, {"text": "In 2009 IEEE\nconference on computer vision and pattern recognition .", "metadata": {}}, {"text": "IEEE, 248–255.", "metadata": {}}, {"text": "[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Sol daini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Gho sh, Kyra Yee, Irene Font\nPeradejordi, et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Bound by the Bounty: Collaborativ ely Shaping Evaluation Processes for Queer AI Harms.", "metadata": {}}, {"text": "In Proceedings of the 2023\nAAAI/ACM Conference on AI, Ethics, and Society .", "metadata": {}}, {"text": "375–386.", "metadata": {}}, {"text": "[49] Fatma Denton.", "metadata": {}}, {"text": "2002.", "metadata": {}}, {"text": "Climate change vulnerability, imp acts, and adaptation: Why does gender matter?", "metadata": {}}, {"text": "Gender & Development 10, 2 (2002), 10–20.", "metadata": {}}, {"text": "[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "BERT: Pre-training of deep bidirectional transformers for language\nunderstanding.", "metadata": {}}, {"text": "arXiv preprint arXiv:1810.04805 (2018).", "metadata": {}}, {"text": "[51] Nicholas Diakopoulos.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Accountability, transpa rency, and algorithms.", "metadata": {}}, {"text": "The Oxford handbook of ethics of AI 17, 4 (2020), 197.", "metadata": {}}, {"text": "[52] Roel Dobbe and Meredith Whittaker.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "AI and climate change: how they’re connected, and what we can do about it.", "metadata": {}}, {"text": "AI Now Institute 17\n(2019).", "metadata": {}}, {"text": "[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Er ika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Lu ccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Measuring the carb on intensity of AI in cloud instances.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency.", "metadata": {}}, {"text": "1877–1894.", "metadata": {}}, {"text": "[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew , Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Docu-\nmenting large webtext corpora: A case study on the colossal c lean crawled corpus.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.08758 (2021).", "metadata": {}}, {"text": "[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Neo-colonialism and Millennium Development Goals (M DGs) in Africa: A\nblend of an old wine in a new bottle.", "metadata": {}}, {"text": "African Journal of Science, Technology, Innovation and Dev elopment 10, 3 (2018), 355–366.", "metadata": {}}, {"text": "[56] Upol Ehsan, Q.", "metadata": {}}, {"text": "Vera Liao, Michael Muller, Mark O.", "metadata": {}}, {"text": "Riedl, and Justin D.", "metadata": {}}, {"text": "Weisz.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Expanding Explainability: Towar ds Social Transparency\nin AI Systems.", "metadata": {}}, {"text": "In Proceedings of the 2021 CHI Conference on Human Factors in Com puting Systems (Yokohama, Japan) (CHI ’21) .", "metadata": {}}, {"text": "Association for\nComputing Machinery, New York, NY, USA, Article 82, 19 pages .", "metadata": {}}, {"text": "https://doi.org/10.1145/3411764.3445188\n[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "GPTs are GPTs: An early look at the labor market im pact potential of\nlarge language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2303.10130 (2023).", "metadata": {}}, {"text": "[58] Virginia Eubanks.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Automating inequality: How high-tech tools proﬁle, police , and punish the poor .", "metadata": {}}, {"text": "St.", "metadata": {}}, {"text": "Martin’s Press.", "metadata": {}}, {"text": "[59] Executive Oﬃce of the President, Oﬃce of Management and Budget.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Memorandum for the heads of executive depart-\nments and agencies on Advancing Governance, Innovation, an d Risk Management for Agency Use of Artiﬁcial Intelligence.", "metadata": {}}, {"text": "https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk -Management-for-Agency-Use-of-Artiﬁcial-Intelligenc e.pdf", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "18 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[60] Florian Eyert and Paola Lopez. 2023. Rethinking Transp arency as a Communicative Constellation. In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ’23) . Association for Computing Machinery, New York, NY, USA, 44 4–454.\nhttps://doi.org/10.1145/3593013.3594010\n[61] Sophia Falk and Aimee van Wynsberghe. 2023. Challengin g AI for Sustainability: what ought it mean? AI and Ethics (2023), 1–11.\n[62] Alec Feinberg. 2022. Solar Geoengineering Modeling an d Applications for Mitigating Global Warming: Assessing Ke y Parameters and the Urban\nHeat Island Inﬂuence. Frontiers in Climate 4 (2022), 870071.\n[63] H. Felzmann, E. Fosch-Villaronga, C. Lutz, et al. 2020. Towards Transparency by Design for Artiﬁcial Intelligence . Science and Engineering Ethics\n26 (2020), 3333–3361. https://doi.org/10.1007/s11948-0 20-00276-4\n[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Lu etge, Robert Madelin, Ugo\nPagallo, Francesca Rossi, et al. 2018. AI4People—an ethica l framework for a good AI society: opportunities, risks, principles, and recommendations.\nMinds and machines 28 (2018), 689–707.\n[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Ven katasubramanian. 2021. The (im) possibility of fairness: D iﬀerent value systems require\ndiﬀerent mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136–143.\n[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda As kell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Ni cholas Schiefer, Ka-\nmal Ndousse, et al. 2022. Red teaming language models to redu ce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint\narXiv:2209.07858 (2022).\n[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, S id Black, Anthony DiPoﬁ, Charles Foster, Laurence Golding, Jeﬀrey Hsu, Alain\nLe Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoﬀ, Chr is Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wa ng, and Andy Zou. 2023. A framework for few-shot language mod el evaluation.\nhttps://doi.org/10.5281/zenodo.10256836\n[68] Timnit Gebru. 2019. Oxford handbook on AI ethics book ch apter on race and gender. arXiv preprint arXiv:1908.06165 (2019).\n[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Je nnifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Ka te Crawford. 2021.\nDatasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n[70] Sergio Genovesi and Julia Maria Mönig. 2022. Acknowled ging Sustainability in the Framework of Ethical Certiﬁcati on for AI. Sustainability 14,\n7 (2022). https://doi.org/10.3390/su14074157\n[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, And rew L Beam, Irene Y Chen, and Rajesh Ranganath. 2020. A review of challenges and\nopportunities in machine learning for health. AMIA Summits on Translational Science Proceedings 2020 (2020), 191.\n[72] Amandeep S Gill and Stefan Germann. 2022. Conceptual an d normative approaches to AI governance for a global digital ecosystem supportive\nof the UN Sustainable Development Goals (SDGs). AI and Ethics 2, 2 (2022), 293–301.\n[73] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: De biasing methods cover up systematic gender biases in word em beddings but do not\nremove them. arXiv preprint arXiv:1903.03862 (2019).\n[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 20 16. Deep learning. MIT press.\n[75] Google. 2022. Carbon free energy for Google Cloud regio ns. https://cloud.google.com/sustainability/region-c arbon.\n[76] Green Software Foundation. 2023. Can AI be Truly Green? https://greensoftware.foundation/articles/can-ai-tr uly-be-green\n[77] GreenPeace. 2020. Oil in the Cloud: How Tech Companies a re Helping Big Oil Proﬁt from Climate Destructions.\nhttps://www.greenpeace.org/usa/reports/oil-in-the-c loud/\n[78] Odd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the art: Reproducibility in artiﬁcial intelligence. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, Vol. 32.\n[79] Thilo Hagendorﬀ. 2022. A virtue-based framework to sup port putting AI ethics into practice. Philosophy & Technology 35, 3 (2022), 55.\n[80] Benjamin Haibe-Kains et al. 2020. Transparency and rep roducibility in artiﬁcial intelligence. Nature 586, 7829 (2020), E14–E16.\n[81] Karen Hao. 2022. Artiﬁcial intelligence is creating a n ew colonial world order. MIT Technology Review (2022).\n[82] Karen Hao. 2022. A new vision of artiﬁcial intelligence for the people. MIT Technology Review (2022).\n[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Na gel. 2023. Beware of sustainable AI! Uses and abuses of a wort hy goal. AI and Ethics\n(2023), 1–12.\n[84] Martin Hilbert. 2016. Big data for development: A revie w of promises and challenges. Development Policy Review 34, 1 (2016), 135–174.\n[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengi o, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadﬁeld,\nMargaret Levi, et al. 2023. International institutions for advanced AI. arXiv preprint arXiv:2307.04699 (2023).\n[86] Mél Hogan. 2018. Big data ecologies. Ephemera 18, 3 (2018), 631.\n[87] Wayne Holmes and Ilkka Tuomi. 2022. State of the art and p ractice in AI in education. European Journal of Education 57, 4 (2022), 542–570.\n[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Ben gio, and Emily Denton. 2020. Characterising bias in compres sed models. arXiv preprint\narXiv:2010.03058 (2020).\n[89] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)Fairness: Lessons for Machine Learning. In Proceedings of the Conference\non Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) . Association for Computing Machinery, New York, NY, USA, 49 –58.\nhttps://doi.org/10.1145/3287560.3287600\n[90] Strategic Imperatives. 1987. Report of the World Commi ssion on Environment and Development: Our common future. Accessed Feb 10 (1987),\n1–300.", "sentences": [{"text": "18 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[60] Florian Eyert and Paola Lopez.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Rethinking Transp arency as a Communicative Constellation.", "metadata": {}}, {"text": "In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ’23) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 44 4–454.", "metadata": {}}, {"text": "https://doi.org/10.1145/3593013.3594010\n[61] Sophia Falk and Aimee van Wynsberghe.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Challengin g AI for Sustainability: what ought it mean?", "metadata": {}}, {"text": "AI and Ethics (2023), 1–11.", "metadata": {}}, {"text": "[62] Alec Feinberg.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Solar Geoengineering Modeling an d Applications for Mitigating Global Warming: Assessing Ke y Parameters and the Urban\nHeat Island Inﬂuence.", "metadata": {}}, {"text": "Frontiers in Climate 4 (2022), 870071.", "metadata": {}}, {"text": "[63] H.", "metadata": {}}, {"text": "Felzmann, E.", "metadata": {}}, {"text": "Fosch-Villaronga, C.", "metadata": {}}, {"text": "Lutz, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Towards Transparency by Design for Artiﬁcial Intelligence .", "metadata": {}}, {"text": "Science and Engineering Ethics\n26 (2020), 3333–3361.", "metadata": {}}, {"text": "https://doi.org/10.1007/s11948-0 20-00276-4\n[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Lu etge, Robert Madelin, Ugo\nPagallo, Francesca Rossi, et al.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "AI4People—an ethica l framework for a good AI society: opportunities, risks, principles, and recommendations.", "metadata": {}}, {"text": "Minds and machines 28 (2018), 689–707.", "metadata": {}}, {"text": "[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Ven katasubramanian.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The (im) possibility of fairness: D iﬀerent value systems require\ndiﬀerent mechanisms for fair decision making.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM 64, 4 (2021), 136–143.", "metadata": {}}, {"text": "[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda As kell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Ni cholas Schiefer, Ka-\nmal Ndousse, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Red teaming language models to redu ce harms: Methods, scaling behaviors, and lessons learned.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2209.07858 (2022).", "metadata": {}}, {"text": "[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, S id Black, Anthony DiPoﬁ, Charles Foster, Laurence Golding, Jeﬀrey Hsu, Alain\nLe Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoﬀ, Chr is Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wa ng, and Andy Zou.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "A framework for few-shot language mod el evaluation.", "metadata": {}}, {"text": "https://doi.org/10.5281/zenodo.10256836\n[68] Timnit Gebru.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Oxford handbook on AI ethics book ch apter on race and gender.", "metadata": {}}, {"text": "arXiv preprint arXiv:1908.06165 (2019).", "metadata": {}}, {"text": "[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Je nnifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Ka te Crawford.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Datasheets for datasets.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM 64, 12 (2021), 86–92.", "metadata": {}}, {"text": "[70] Sergio Genovesi and Julia Maria Mönig.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Acknowled ging Sustainability in the Framework of Ethical Certiﬁcati on for AI.", "metadata": {}}, {"text": "Sustainability 14,\n7 (2022).", "metadata": {}}, {"text": "https://doi.org/10.3390/su14074157\n[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, And rew L Beam, Irene Y Chen, and Rajesh Ranganath.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "A review of challenges and\nopportunities in machine learning for health.", "metadata": {}}, {"text": "AMIA Summits on Translational Science Proceedings 2020 (2020), 191.", "metadata": {}}, {"text": "[72] Amandeep S Gill and Stefan Germann.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Conceptual an d normative approaches to AI governance for a global digital ecosystem supportive\nof the UN Sustainable Development Goals (SDGs).", "metadata": {}}, {"text": "AI and Ethics 2, 2 (2022), 293–301.", "metadata": {}}, {"text": "[73] Hila Gonen and Yoav Goldberg.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Lipstick on a pig: De biasing methods cover up systematic gender biases in word em beddings but do not\nremove them.", "metadata": {}}, {"text": "arXiv preprint arXiv:1903.03862 (2019).", "metadata": {}}, {"text": "[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "metadata": {}}, {"text": "20 16.", "metadata": {}}, {"text": "Deep learning.", "metadata": {}}, {"text": "MIT press.", "metadata": {}}, {"text": "[75] Google.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Carbon free energy for Google Cloud regio ns.", "metadata": {}}, {"text": "https://cloud.google.com/sustainability/region-c arbon.", "metadata": {}}, {"text": "[76] Green Software Foundation.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Can AI be Truly Green?", "metadata": {}}, {"text": "https://greensoftware.foundation/articles/can-ai-tr uly-be-green\n[77] GreenPeace.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Oil in the Cloud: How Tech Companies a re Helping Big Oil Proﬁt from Climate Destructions.", "metadata": {}}, {"text": "https://www.greenpeace.org/usa/reports/oil-in-the-c loud/\n[78] Odd Erik Gundersen and Sigbjørn Kjensmo.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "State of the art: Reproducibility in artiﬁcial intelligence.", "metadata": {}}, {"text": "In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, Vol.", "metadata": {}}, {"text": "32.", "metadata": {}}, {"text": "[79] Thilo Hagendorﬀ.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "A virtue-based framework to sup port putting AI ethics into practice.", "metadata": {}}, {"text": "Philosophy & Technology 35, 3 (2022), 55.", "metadata": {}}, {"text": "[80] Benjamin Haibe-Kains et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Transparency and rep roducibility in artiﬁcial intelligence.", "metadata": {}}, {"text": "Nature 586, 7829 (2020), E14–E16.", "metadata": {}}, {"text": "[81] Karen Hao.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Artiﬁcial intelligence is creating a n ew colonial world order.", "metadata": {}}, {"text": "MIT Technology Review (2022).", "metadata": {}}, {"text": "[82] Karen Hao.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "A new vision of artiﬁcial intelligence for the people.", "metadata": {}}, {"text": "MIT Technology Review (2022).", "metadata": {}}, {"text": "[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Na gel.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Beware of sustainable AI!", "metadata": {}}, {"text": "Uses and abuses of a wort hy goal.", "metadata": {}}, {"text": "AI and Ethics\n(2023), 1–12.", "metadata": {}}, {"text": "[84] Martin Hilbert.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Big data for development: A revie w of promises and challenges.", "metadata": {}}, {"text": "Development Policy Review 34, 1 (2016), 135–174.", "metadata": {}}, {"text": "[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengi o, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadﬁeld,\nMargaret Levi, et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "International institutions for advanced AI.", "metadata": {}}, {"text": "arXiv preprint arXiv:2307.04699 (2023).", "metadata": {}}, {"text": "[86] Mél Hogan.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Big data ecologies.", "metadata": {}}, {"text": "Ephemera 18, 3 (2018), 631.", "metadata": {}}, {"text": "[87] Wayne Holmes and Ilkka Tuomi.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "State of the art and p ractice in AI in education.", "metadata": {}}, {"text": "European Journal of Education 57, 4 (2022), 542–570.", "metadata": {}}, {"text": "[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Ben gio, and Emily Denton.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Characterising bias in compres sed models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2010.03058 (2020).", "metadata": {}}, {"text": "[89] Ben Hutchinson and Margaret Mitchell.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "50 Years of Test (Un)Fairness: Lessons for Machine Learning.", "metadata": {}}, {"text": "In Proceedings of the Conference\non Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 49 –58.", "metadata": {}}, {"text": "https://doi.org/10.1145/3287560.3287600\n[90] Strategic Imperatives.", "metadata": {}}, {"text": "1987.", "metadata": {}}, {"text": "Report of the World Commi ssion on Environment and Development: Our common future.", "metadata": {}}, {"text": "Accessed Feb 10 (1987),\n1–300.", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 19\n[91] Innovation, Science and Economic Development Canada. 2023. The Artiﬁcial Intelligence and Data Act (AIDA) – Compa nion document.\nhttps://ised-isde.canada.ca/site/innovation-better- canada/en/artiﬁcial-intelligence-and-data-act-aida- companion-document\n[92] Yacine Jernite et al. 2022. Data governance in the age of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference\non Fairness, Accountability, and Transparency . ACM.\n[93] W Stanley Jevons. 1866. The coal question. In The Economics of Population . Routledge, 193–204.\n[94] Anna Jobin, Marcello Ienca, and Eﬀy Vayena. 2019. The gl obal landscape of AI ethics guidelines. Nature machine intelligence 1, 9 (2019), 389–399.\n[95] LN Joppa, Brian O’Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoﬀmann, James EM Watson, Stuart HM Butchart, Malika\nVirah-Sawmy, Benjamin S Halpern, et al. 2016. Filling in bio diversity threat gaps. Science 352, 6284 (2016), 416–418.\n[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiy a, Felix Creutzig, and David Rolnick. 2022. Aligning artiﬁcial intelligence with climate\nchange mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Meg an Mills-Novoa, Elizabeth Shoﬀner, and Katherine Siegel. 2 022. Biases and limitations\nof Global Forest Change and author-generated land cover map s in detecting deforestation in the Amazon. PLoS One 17, 7 (2022), e0268970.\n[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Fost er. 2021. Reduced, reused and recycled: The life of a dataset in machine learning\nresearch. arXiv preprint arXiv:2112.01716 (2021).\n[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Mich ael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Mich ihiro Yasunaga,\nRichard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benc hmark of in-the-wild distribution shifts. In International Conference on Machine\nLearning. PMLR, 5637–5664.\n[100] Julian Kuehnert, Deborah McGlynn, Sekou L. Remy, Aish a Walcott-Bryant, and Anne Jones. 2022. Surrogate Ensemble Forecasting for Dynamic\nClimate Impact Models. arXiv:2204.05795 [cs.LG]\n[101] Maciej Kuziemski and Gianluca Misuraca. 2020. AI gove rnance in the public sector: Three tales from the frontiers o f automated decision-making\nin democratic settings. Telecommunications policy 44, 6 (2020), 101976.\n[102] Travis LaCroix and Alexandra Sasha Luccioni. 2022. Me taethical perspectives on’Benchmarking’AI ethics. arXiv preprint arXiv:2204.05151 (2022).\n[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhan g, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto . 2023. When Do Pre-\nTraining Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics , Andreas Vlachos and Isabelle Augenstein (Eds.). Associat ion for Computational Linguis-\ntics, Dubrovnik, Croatia, 3206–3219. https://doi.org/10 .18653/v1/2023.eacl-main.234\n[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskove c. 2016. Interpretable decision sets: A joint framework for description and prediction.\nIn Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 1675–1684.\n[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jur e Leskovec. 2019. Faithful and customizable explanations o f black box models. In\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, an d Society. 131–138.\n[106] Julian Lamont (Ed.). 2017. Distributive Justice. Routledge.\n[107] S. Larsson and F. Heintz. 2020. Transparency in artiﬁc ial intelligence. Internet Policy Review 9, 2 (2020). https://doi.org/10.14763/2020.2.1469\n[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elis abeth Cross, Alessandro Parente, and Ricardo Vinuesa. 2023. Improving aircraft performance\nusing machine learning: a review. Aerospace Science and Technology (2023), 108354.\n[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva. 2019. Ethical framework for designing autonomous intellig ent systems. Journal of Open\nInnovation: Technology, Market, and Complexity 5, 1 (2019), 18.\n[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Ka ipulaumakaniolono Baker, Scott Benesiinaabandan, Michel le Brown, Melanie Cheung,\nMeredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artiﬁcial intelligence position p aper. (2020).\n[111] Chenyang Li. 2013. The Confucian philosophy of harmony . Vol. 10. Routledge.\n[112] Mochen Liao, Kai Lan, and Yuan Yao. 2022. Sustainabili ty implications of artiﬁcial intelligence in the chemical i ndustry: A conceptual framework.\nJournal of industrial ecology 26, 1 (2022), 164–182.\n[113] Zachary C Lipton. 2018. The mythos of model interpreta bility: In machine learning, the concept of interpretability is both important and slippery.\nQueue 16, 3 (2018), 31–57.\n[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar J oshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, a nd Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining appro ach. arXiv preprint arXiv:1907.11692 (2019).\n[115] Kirsten Lloyd. 2018. Bias ampliﬁcation in artiﬁcial i ntelligence systems. arXiv preprint arXiv:1809.07842 (2018).\n[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors inﬂuencing the em issions of machine learning.\narXiv preprint arXiv:2302.08476 (2023).\n[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma St rubell. 2023. Power Hungry Processing: Watts Driving the Co st of AI Deployment?\narXiv:2311.16863 [cs.LG]\n[118] Alexandra Sasha Luccioni and Anna Rogers. 2023. Mind y our Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice.\narXiv preprint arXiv:2308.07120 (2023).\n[119] Alexandra Sasha Luccioni and David Rolnick. 2023. Bug s in the data: How ImageNet misrepresents biodiversity. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , Vol. 37. 14382–14390.\n[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Cra wford. 2025. From Eﬃciency Gains to Rebound Eﬀects: The Prob lem of Jevons’ Paradox\nin AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025).", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 19\n[91] Innovation, Science and Economic Development Canada.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The Artiﬁcial Intelligence and Data Act (AIDA) – Compa nion document.", "metadata": {}}, {"text": "https://ised-isde.canada.ca/site/innovation-better- canada/en/artiﬁcial-intelligence-and-data-act-aida- companion-document\n[92] Yacine Jernite et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Data governance in the age of large-scale data-driven language technology.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference\non Fairness, Accountability, and Transparency .", "metadata": {}}, {"text": "ACM.", "metadata": {}}, {"text": "[93] W Stanley Jevons.", "metadata": {}}, {"text": "1866.", "metadata": {}}, {"text": "The coal question.", "metadata": {}}, {"text": "In The Economics of Population .", "metadata": {}}, {"text": "Routledge, 193–204.", "metadata": {}}, {"text": "[94] Anna Jobin, Marcello Ienca, and Eﬀy Vayena.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "The gl obal landscape of AI ethics guidelines.", "metadata": {}}, {"text": "Nature machine intelligence 1, 9 (2019), 389–399.", "metadata": {}}, {"text": "[95] LN Joppa, Brian O’Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoﬀmann, James EM Watson, Stuart HM Butchart, Malika\nVirah-Sawmy, Benjamin S Halpern, et al.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Filling in bio diversity threat gaps.", "metadata": {}}, {"text": "Science 352, 6284 (2016), 416–418.", "metadata": {}}, {"text": "[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiy a, Felix Creutzig, and David Rolnick.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Aligning artiﬁcial intelligence with climate\nchange mitigation.", "metadata": {}}, {"text": "Nature Climate Change 12, 6 (2022), 518–527.", "metadata": {}}, {"text": "[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Meg an Mills-Novoa, Elizabeth Shoﬀner, and Katherine Siegel.", "metadata": {}}, {"text": "2 022.", "metadata": {}}, {"text": "Biases and limitations\nof Global Forest Change and author-generated land cover map s in detecting deforestation in the Amazon.", "metadata": {}}, {"text": "PLoS One 17, 7 (2022), e0268970.", "metadata": {}}, {"text": "[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Fost er.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Reduced, reused and recycled: The life of a dataset in machine learning\nresearch.", "metadata": {}}, {"text": "arXiv preprint arXiv:2112.01716 (2021).", "metadata": {}}, {"text": "[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Mich ael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Mich ihiro Yasunaga,\nRichard Lanas Phillips, Irena Gao, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Wilds: A benc hmark of in-the-wild distribution shifts.", "metadata": {}}, {"text": "In International Conference on Machine\nLearning.", "metadata": {}}, {"text": "PMLR, 5637–5664.", "metadata": {}}, {"text": "[100] Julian Kuehnert, Deborah McGlynn, Sekou L.", "metadata": {}}, {"text": "Remy, Aish a Walcott-Bryant, and Anne Jones.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Surrogate Ensemble Forecasting for Dynamic\nClimate Impact Models.", "metadata": {}}, {"text": "arXiv:2204.05795 [cs.LG]\n[101] Maciej Kuziemski and Gianluca Misuraca.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "AI gove rnance in the public sector: Three tales from the frontiers o f automated decision-making\nin democratic settings.", "metadata": {}}, {"text": "Telecommunications policy 44, 6 (2020), 101976.", "metadata": {}}, {"text": "[102] Travis LaCroix and Alexandra Sasha Luccioni.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Me taethical perspectives on’Benchmarking’AI ethics.", "metadata": {}}, {"text": "arXiv preprint arXiv:2204.05151 (2022).", "metadata": {}}, {"text": "[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhan g, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto .", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "When Do Pre-\nTraining Biases Propagate to Downstream Tasks?", "metadata": {}}, {"text": "A Case Study in Text Summarization.", "metadata": {}}, {"text": "In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics , Andreas Vlachos and Isabelle Augenstein (Eds.).", "metadata": {}}, {"text": "Associat ion for Computational Linguis-\ntics, Dubrovnik, Croatia, 3206–3219.", "metadata": {}}, {"text": "https://doi.org/10 .18653/v1/2023.eacl-main.234\n[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskove c.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Interpretable decision sets: A joint framework for description and prediction.", "metadata": {}}, {"text": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining .", "metadata": {}}, {"text": "1675–1684.", "metadata": {}}, {"text": "[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jur e Leskovec.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Faithful and customizable explanations o f black box models.", "metadata": {}}, {"text": "In\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, an d Society.", "metadata": {}}, {"text": "131–138.", "metadata": {}}, {"text": "[106] Julian Lamont (Ed.).", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Distributive Justice.", "metadata": {}}, {"text": "Routledge.", "metadata": {}}, {"text": "[107] S.", "metadata": {}}, {"text": "Larsson and F.", "metadata": {}}, {"text": "Heintz.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Transparency in artiﬁc ial intelligence.", "metadata": {}}, {"text": "Internet Policy Review 9, 2 (2020).", "metadata": {}}, {"text": "https://doi.org/10.14763/2020.2.1469\n[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elis abeth Cross, Alessandro Parente, and Ricardo Vinuesa.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Improving aircraft performance\nusing machine learning: a review.", "metadata": {}}, {"text": "Aerospace Science and Technology (2023), 108354.", "metadata": {}}, {"text": "[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Ethical framework for designing autonomous intellig ent systems.", "metadata": {}}, {"text": "Journal of Open\nInnovation: Technology, Market, and Complexity 5, 1 (2019), 18.", "metadata": {}}, {"text": "[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Ka ipulaumakaniolono Baker, Scott Benesiinaabandan, Michel le Brown, Melanie Cheung,\nMeredith Coleman, Ashley Cordes, Joel Davison, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Indigenous protocol and artiﬁcial intelligence position p aper.", "metadata": {}}, {"text": "(2020).", "metadata": {}}, {"text": "[111] Chenyang Li.", "metadata": {}}, {"text": "2013.", "metadata": {}}, {"text": "The Confucian philosophy of harmony .", "metadata": {}}, {"text": "Vol.", "metadata": {}}, {"text": "10.", "metadata": {}}, {"text": "Routledge.", "metadata": {}}, {"text": "[112] Mochen Liao, Kai Lan, and Yuan Yao.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Sustainabili ty implications of artiﬁcial intelligence in the chemical i ndustry: A conceptual framework.", "metadata": {}}, {"text": "Journal of industrial ecology 26, 1 (2022), 164–182.", "metadata": {}}, {"text": "[113] Zachary C Lipton.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "The mythos of model interpreta bility: In machine learning, the concept of interpretability is both important and slippery.", "metadata": {}}, {"text": "Queue 16, 3 (2018), 31–57.", "metadata": {}}, {"text": "[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar J oshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, a nd Veselin Stoyanov.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Roberta: A robustly optimized bert pretraining appro ach.", "metadata": {}}, {"text": "arXiv preprint arXiv:1907.11692 (2019).", "metadata": {}}, {"text": "[115] Kirsten Lloyd.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Bias ampliﬁcation in artiﬁcial i ntelligence systems.", "metadata": {}}, {"text": "arXiv preprint arXiv:1809.07842 (2018).", "metadata": {}}, {"text": "[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Counting carbon: A survey of factors inﬂuencing the em issions of machine learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.08476 (2023).", "metadata": {}}, {"text": "[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma St rubell.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Power Hungry Processing: Watts Driving the Co st of AI Deployment?", "metadata": {}}, {"text": "arXiv:2311.16863 [cs.LG]\n[118] Alexandra Sasha Luccioni and Anna Rogers.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Mind y our Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice.", "metadata": {}}, {"text": "arXiv preprint arXiv:2308.07120 (2023).", "metadata": {}}, {"text": "[119] Alexandra Sasha Luccioni and David Rolnick.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Bug s in the data: How ImageNet misrepresents biodiversity.", "metadata": {}}, {"text": "In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , Vol.", "metadata": {}}, {"text": "37.", "metadata": {}}, {"text": "14382–14390.", "metadata": {}}, {"text": "[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Cra wford.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "From Eﬃciency Gains to Rebound Eﬀects: The Prob lem of Jevons’ Paradox\nin AI’s Polarized Environmental Debate.", "metadata": {}}, {"text": "arXiv preprint arXiv:2501.16548 (2025).", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "20 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne- Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel. arXiv preprint arXiv:2211.02001 (2022).\n[122] Federica Lucivero. 2020. Big data, big waste? A reﬂect ion on the environmental sustainability of big data initiatives. Science and engineering ethics\n26, 2 (2020), 1009–1030.\n[123] Amanda H Lynch and Siri Veland. 2018. Urgency in the Anthropocene. MIT Press.\n[124] Mirca Madianou. 2021. Nonhuman humanitarianism: whe n’AI for good’can be harmful. Information, Communication & Society 24, 6 (2021),\n850–868.\n[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi,\nTee Garnett, et al. 2023. What’s fair is. . . fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in\nthe integration of clinical machine learning: JustEFAB. In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency.\n1505–1519.\n[126] Duncan P McLaren. 2018. Whose climate and whose ethics ? Conceptions of justice in solar geoengineering modelling . Energy research & social\nscience 44 (2018), 209–221.\n[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, K ristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning.\nACM computing surveys (CSUR) 54, 6 (2021), 1–35.\n[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Mu ratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Sca ling deep learning\nfor materials discovery. Nature 624, 7990 (2023), 80–85.\n[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. 2021. Algorithmic Impact Assessments and\nAccountability: The Co-Construction of Impacts. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual\nEvent, Canada) (FAccT ’21) . Association for Computing Machinery, New York, NY, USA, 73 5–746. https://doi.org/10.1145/3442188.3445935\n[130] Rachel Metz. 2024. Google’s Emissions Shot Up 48% Over Five Years Due to AI. Bloomberg (2024).\n[131] Thaddeus Metz and Scott C. Miller. 2016. Relational et hics. In The international encyclopedia of ethics . 1–10.\n[132] Milagros Miceli, Julian Posada, and Tianling Yang. 20 22. Studying up machine learning data: Why talk about bias wh en we mean power?\nProceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.\n[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija. 2019. Ethical AI for the Governance of the Society: Ch allenges and Opportunities.\nIn CEUR Workshop Proceedings, Vol. 2505. 20–26.\n[134] John Stuart Mill. 1863. Utilitarianism. Parker, Son, and Bourn.\n[135] David Miller and Michael Walzer (Eds.). 1995. Pluralism, Justice, and Equality . Oxford University Press.\n[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Em ily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Ja mie Morgenstern. 2020.\nDiversity and inclusion metrics in subset selection. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Soc iety. 117–123.\n[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parke r Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In ioluwa Deborah Raji, and\nTimnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.\n[138] Shakir Mohamed, Marie-Therese Png, and William Isaac . 2020. Decolonial AI: Decolonial theory as sociotechnical foresight in artiﬁcial intelli-\ngence. Philosophy & Technology 33 (2020), 659–684.\n[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and L uciano Floridi. 2023. Auditing large language models: a thr ee-layered approach. AI and\nEthics (2023), 1–31.\n[140] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.\n[141] Steven Gonzalez Monserrate. 2022. The cloud is materi al: On the environmental impacts of computation and data sto rage. (2022).\n[142] L. Munn. 2022. The Uselessness of AI Ethics. AI Ethics (2022). https://doi.org/10.1007/s43681-022-00209-w\n[143] Luke Munn. 2023. The ﬁve tests: designing and evaluati ng AI according to indigenous M ¯aori principles. AI & SOCIETY (2023), 1–9.\n[144] Patrick Murmann and Simone Fischer-Hübner. 2017. Usa ble transparency enhancing tools: A literature review. (20 17).\n[145] Mechthild Nagel. 2022. Ludic ubuntu ethics: Decolonizing justice . Taylor & Francis.\n[146] National Academies of Sciences, Engineering and Medi cine. 2019. Reproducibility and replicability in science. (2019).\n[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing Info rmation Disclosures to Support\nMeaningful Transparency and Accountability. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul,\nRepublic of Korea) (FAccT ’22) . Association for Computing Machinery, New York, NY, USA, 67 9–690. https://doi.org/10.1145/3531146.3533133\n[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Lu ke Abraham, John Pyle, and Apostolos Voulgarakis. 2018. Usi ng machine learning to\nbuild temperature-based ozone parameterizations for clim ate sensitivity simulations. Environmental Research Letters 13, 10 (2018), 104016.\n[149] Victor Ordonez, Taylor Dunn, and Eric Noll. 2023. Open AI CEO Sam Altman says AI will reshape society, acknowledges risks:‘A little bit scared\nof this’. (2023). https://abcnews.go.com/Technology/op enai-ceo-sam-altman-ai-reshapesociety-acknowledges/ story\n[150] Friederike Otto. 2023. Without Warning: A Lack of Weat her Stations Is Costing African Lives.\nhttps://e360.yale.edu/features/africa-weather-stati ons-climate-change\n[151] Edward A Page. 2008. Distributing the burdens of clima te change. Environmental Politics 17, 4 (2008), 556–575.\n[152] European Parliament. 2023. Artiﬁcial Intelligence A ct: deal on comprehensive rules for trustworthy AI. (2023).\nhttps://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artiﬁcial-intelligence-act-deal-on- comprehensive-rules-for-trustworthy-ai", "sentences": [{"text": "20 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne- Laure Ligozat.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Estimating the carbon footprint of BLOOM, a 176B parameter language\nmodel.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.02001 (2022).", "metadata": {}}, {"text": "[122] Federica Lucivero.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Big data, big waste?", "metadata": {}}, {"text": "A reﬂect ion on the environmental sustainability of big data initiatives.", "metadata": {}}, {"text": "Science and engineering ethics\n26, 2 (2020), 1009–1030.", "metadata": {}}, {"text": "[123] Amanda H Lynch and Siri Veland.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Urgency in the Anthropocene.", "metadata": {}}, {"text": "MIT Press.", "metadata": {}}, {"text": "[124] Mirca Madianou.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Nonhuman humanitarianism: whe n’AI for good’can be harmful.", "metadata": {}}, {"text": "Information, Communication & Society 24, 6 (2021),\n850–868.", "metadata": {}}, {"text": "[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi,\nTee Garnett, et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "What’s fair is.", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "fair?", "metadata": {}}, {"text": "Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in\nthe integration of clinical machine learning: JustEFAB.", "metadata": {}}, {"text": "In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency.", "metadata": {}}, {"text": "1505–1519.", "metadata": {}}, {"text": "[126] Duncan P McLaren.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Whose climate and whose ethics ?", "metadata": {}}, {"text": "Conceptions of justice in solar geoengineering modelling .", "metadata": {}}, {"text": "Energy research & social\nscience 44 (2018), 209–221.", "metadata": {}}, {"text": "[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, K ristina Lerman, and Aram Galstyan.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "A survey on bias and fairness in machine learning.", "metadata": {}}, {"text": "ACM computing surveys (CSUR) 54, 6 (2021), 1–35.", "metadata": {}}, {"text": "[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Mu ratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Sca ling deep learning\nfor materials discovery.", "metadata": {}}, {"text": "Nature 624, 7990 (2023), 80–85.", "metadata": {}}, {"text": "[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Algorithmic Impact Assessments and\nAccountability: The Co-Construction of Impacts.", "metadata": {}}, {"text": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual\nEvent, Canada) (FAccT ’21) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 73 5–746.", "metadata": {}}, {"text": "https://doi.org/10.1145/3442188.3445935\n[130] Rachel Metz.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Google’s Emissions Shot Up 48% Over Five Years Due to AI.", "metadata": {}}, {"text": "Bloomberg (2024).", "metadata": {}}, {"text": "[131] Thaddeus Metz and Scott C.", "metadata": {}}, {"text": "Miller.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Relational et hics.", "metadata": {}}, {"text": "In The international encyclopedia of ethics .", "metadata": {}}, {"text": "1–10.", "metadata": {}}, {"text": "[132] Milagros Miceli, Julian Posada, and Tianling Yang.", "metadata": {}}, {"text": "20 22.", "metadata": {}}, {"text": "Studying up machine learning data: Why talk about bias wh en we mean power?", "metadata": {}}, {"text": "Proceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.", "metadata": {}}, {"text": "[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Ethical AI for the Governance of the Society: Ch allenges and Opportunities.", "metadata": {}}, {"text": "In CEUR Workshop Proceedings, Vol.", "metadata": {}}, {"text": "2505.", "metadata": {}}, {"text": "20–26.", "metadata": {}}, {"text": "[134] John Stuart Mill.", "metadata": {}}, {"text": "1863.", "metadata": {}}, {"text": "Utilitarianism.", "metadata": {}}, {"text": "Parker, Son, and Bourn.", "metadata": {}}, {"text": "[135] David Miller and Michael Walzer (Eds.).", "metadata": {}}, {"text": "1995.", "metadata": {}}, {"text": "Pluralism, Justice, and Equality .", "metadata": {}}, {"text": "Oxford University Press.", "metadata": {}}, {"text": "[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Em ily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Ja mie Morgenstern.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Diversity and inclusion metrics in subset selection.", "metadata": {}}, {"text": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Soc iety.", "metadata": {}}, {"text": "117–123.", "metadata": {}}, {"text": "[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parke r Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In ioluwa Deborah Raji, and\nTimnit Gebru.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Model cards for model reporting.", "metadata": {}}, {"text": "In Proceedings of the conference on fairness, accountability, and transparency.", "metadata": {}}, {"text": "220–229.", "metadata": {}}, {"text": "[138] Shakir Mohamed, Marie-Therese Png, and William Isaac .", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Decolonial AI: Decolonial theory as sociotechnical foresight in artiﬁcial intelli-\ngence.", "metadata": {}}, {"text": "Philosophy & Technology 33 (2020), 659–684.", "metadata": {}}, {"text": "[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and L uciano Floridi.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Auditing large language models: a thr ee-layered approach.", "metadata": {}}, {"text": "AI and\nEthics (2023), 1–31.", "metadata": {}}, {"text": "[140] Christoph Molnar.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Interpretable machine learning.", "metadata": {}}, {"text": "Lulu.", "metadata": {}}, {"text": "com.", "metadata": {}}, {"text": "[141] Steven Gonzalez Monserrate.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The cloud is materi al: On the environmental impacts of computation and data sto rage.", "metadata": {}}, {"text": "(2022).", "metadata": {}}, {"text": "[142] L.", "metadata": {}}, {"text": "Munn.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The Uselessness of AI Ethics.", "metadata": {}}, {"text": "AI Ethics (2022).", "metadata": {}}, {"text": "https://doi.org/10.1007/s43681-022-00209-w\n[143] Luke Munn.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The ﬁve tests: designing and evaluati ng AI according to indigenous M ¯aori principles.", "metadata": {}}, {"text": "AI & SOCIETY (2023), 1–9.", "metadata": {}}, {"text": "[144] Patrick Murmann and Simone Fischer-Hübner.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Usa ble transparency enhancing tools: A literature review.", "metadata": {}}, {"text": "(20 17).", "metadata": {}}, {"text": "[145] Mechthild Nagel.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Ludic ubuntu ethics: Decolonizing justice .", "metadata": {}}, {"text": "Taylor & Francis.", "metadata": {}}, {"text": "[146] National Academies of Sciences, Engineering and Medi cine.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Reproducibility and replicability in science.", "metadata": {}}, {"text": "(2019).", "metadata": {}}, {"text": "[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Disclosure by Design: Designing Info rmation Disclosures to Support\nMeaningful Transparency and Accountability.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency (Seoul,\nRepublic of Korea) (FAccT ’22) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 67 9–690.", "metadata": {}}, {"text": "https://doi.org/10.1145/3531146.3533133\n[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Lu ke Abraham, John Pyle, and Apostolos Voulgarakis.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Usi ng machine learning to\nbuild temperature-based ozone parameterizations for clim ate sensitivity simulations.", "metadata": {}}, {"text": "Environmental Research Letters 13, 10 (2018), 104016.", "metadata": {}}, {"text": "[149] Victor Ordonez, Taylor Dunn, and Eric Noll.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Open AI CEO Sam Altman says AI will reshape society, acknowledges risks:‘A little bit scared\nof this’.", "metadata": {}}, {"text": "(2023).", "metadata": {}}, {"text": "https://abcnews.go.com/Technology/op enai-ceo-sam-altman-ai-reshapesociety-acknowledges/ story\n[150] Friederike Otto.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Without Warning: A Lack of Weat her Stations Is Costing African Lives.", "metadata": {}}, {"text": "https://e360.yale.edu/features/africa-weather-stati ons-climate-change\n[151] Edward A Page.", "metadata": {}}, {"text": "2008.", "metadata": {}}, {"text": "Distributing the burdens of clima te change.", "metadata": {}}, {"text": "Environmental Politics 17, 4 (2008), 556–575.", "metadata": {}}, {"text": "[152] European Parliament.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Artiﬁcial Intelligence A ct: deal on comprehensive rules for trustworthy AI.", "metadata": {}}, {"text": "(2023).", "metadata": {}}, {"text": "https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artiﬁcial-intelligence-act-deal-on- comprehensive-rules-for-trustworthy-ai", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}, {"title": "Page 21", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 21\n[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang , Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Te xier, and Jeﬀ Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jerni te, and Margaret Mitchell. 2023. Stronger Together: on the A rticulation of Ethical Charters,\nLegal Tools, and Technical Documentation in ML. In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency (<conf-\nloc>, <city>Chicago</city>, <state>IL</state>, <countr y>USA</country>, </conf-loc>) (FAccT ’23) . Association for Computing Machinery, New\nYork, NY, USA, 343–354. https://doi.org/10.1145/3593013 .3594002\n[155] Erich Prem. 2023. From ethical AI frameworks to tools: a review of approaches. AI and Ethics 3, 3 (2023), 699–716.\n[156] Edward Raﬀ. 2019. A Step Toward Quantifying Independe ntly Reproducible Machine Learning Research. arXiv:1909. 06674 [cs.LG]\n[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Pa ullada, Emily Denton, and Alex Hanna. 2021. AI and the everyt hing in the whole wide\nworld benchmark. arXiv preprint arXiv:2111.15366 (2021).\n[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buo lamwini. 2023. Change from the outside: Towards credible th ird-party audits of ai\nsystems. Missing links in AI governance (2023), 5.\n[159] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, et al. 2020. Closing the AI accountability ga p: Deﬁning an end-to-end\nframework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountabil ity, and Transparency. ACM, 33–44.\n[160] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Soc ial-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic\nAudits. In 2023 ACM Conference on Fairness, Accountability, and Trans parency. ACM. https://doi.org/10.1145/3593013.3594014\n[161] Akshat Rathi and Dina Bass. 2024. Microsoft’s AI Push I mperils Climate Goal as Carbon Emissions Jump 30%. Bloomberg (2024).\n[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Al i Farhadi. 2016. You Only Look Once: Uniﬁed, Real-Time Objec t Detection.\narXiv:1506.02640 [cs.CV]\n[163] Jennifer Rhee. 2018. The robotic imaginary: The human and the price of dehumanize d labor. U of Minnesota Press.\n[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestri n. 2016. Model-agnostic interpretability of machine learn ing. arXiv preprint\narXiv:1606.05386 (2016).\n[165] Paola Ricaurte. 2022. Ethics for the majority world: A I and the question of violence at scale. Media, Culture & Society 44, 4 (2022), 726–745.\n[166] Anna Rogers. 2021. Changing the world by changing the d ata. arXiv preprint arXiv:2105.13947 (2021).\n[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranja n Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2 022. The Dollar Street\ndataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track .\n[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kocha nski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross , Nikola Milojevic-Dupont,\nNatasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022),\n1–96.\n[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Pa trick Esser, and Björn Ommer. 2022. High-Resolution Image S ynthesis With Latent\nDiﬀusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision an d Pattern Recognition (CVPR) . 10684–10695.\n[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Eli zabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin , Hannah Tait Neufeld,\nAndrew Nixon, et al. 2019. Automated pastures and the digita l divide: How agricultural technologies are shaping labour and rural communities.\nJournal of Rural Studies 68 (2019), 112–122.\n[171] Mark Ryan. 2022. The social and ethical impacts of arti ﬁcial intelligence in agriculture: mapping the agricultur al AI literature. AI & SOCIETY\n(2022), 1–13.\n[172] Salesforce. 2024. Sustainable AI Policy Principles. https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-susta inable-ai-policy-principles.pdf\n[173] Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “Eve ryone wants to do the\nmodel work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Co mputing\nSystems. 1–15.\n[174] Aaron Sankin and Surya Mattu. 2023. Predictive Polici ng Software Terrible At Predicting Crimes.\nhttps://themarkup.org/prediction-bias/2023/10/02/pr edictive-policing-software-terrible-at-predicting-c rimes.\n[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Go ran Radanovic, David C. Parkes, and Yang Liu. 2019. How Do Fai rness Deﬁnitions Fare?\nExamining Public Attitudes Towards Algorithmic Deﬁnitions of Fairness. InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\n(Honolulu, HI, USA) (AIES ’19) . Association for Computing Machinery, New York, NY, USA, 99 –106. https://doi.org/10.1145/3306618.3314248\n[176] David Schlosberg. 2012. Climate Justice and Capabili ties: A Framework for Adaptation Policy. Ethics & International Aﬀairs 26, 4 (2012), 445–461.\n[177] David Schlosberg and Lisette B Collins. 2014. From env ironmental to climate justice: climate change and the disco urse of environmental justice.\nWiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.\n[178] Jurgen Schmidhuber. 1991. Neural Sequence Chunkers. Technical Report FKI-148-91 (1991).\n[179] Christian Schroeder de Witt and Thomas Hornigold. 201 9. Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. arXiv\ne-prints (2019), arXiv–1905.\n[180] Paul Schütze. 2024. The Problem of Sustainable AI: A Cr itical Assessment of an Emerging Phenomenon. Weizenbaum Journal of the Digital\nSociety 4, 1 (2024).\n[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzio ni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 21\n[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang , Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Te xier, and Jeﬀ Dean.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.10350 (2021).", "metadata": {}}, {"text": "[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jerni te, and Margaret Mitchell.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Stronger Together: on the A rticulation of Ethical Charters,\nLegal Tools, and Technical Documentation in ML.", "metadata": {}}, {"text": "In Proceedings of the 2023 ACM Conference on Fairness, Accounta bility, and Transparency (<conf-\nloc>, <city>Chicago</city>, <state>IL</state>, <countr y>USA</country>, </conf-loc>) (FAccT ’23) .", "metadata": {}}, {"text": "Association for Computing Machinery, New\nYork, NY, USA, 343–354.", "metadata": {}}, {"text": "https://doi.org/10.1145/3593013 .3594002\n[155] Erich Prem.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "From ethical AI frameworks to tools: a review of approaches.", "metadata": {}}, {"text": "AI and Ethics 3, 3 (2023), 699–716.", "metadata": {}}, {"text": "[156] Edward Raﬀ.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "A Step Toward Quantifying Independe ntly Reproducible Machine Learning Research.", "metadata": {}}, {"text": "arXiv:1909.", "metadata": {}}, {"text": "06674 [cs.LG]\n[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Pa ullada, Emily Denton, and Alex Hanna.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "AI and the everyt hing in the whole wide\nworld benchmark.", "metadata": {}}, {"text": "arXiv preprint arXiv:2111.15366 (2021).", "metadata": {}}, {"text": "[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buo lamwini.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Change from the outside: Towards credible th ird-party audits of ai\nsystems.", "metadata": {}}, {"text": "Missing links in AI governance (2023), 5.", "metadata": {}}, {"text": "[159] I.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Raji, A.", "metadata": {}}, {"text": "Smart, R.", "metadata": {}}, {"text": "N.", "metadata": {}}, {"text": "White, M.", "metadata": {}}, {"text": "Mitchell, T.", "metadata": {}}, {"text": "Gebru, B.", "metadata": {}}, {"text": "Hutchinson, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Closing the AI accountability ga p: Deﬁning an end-to-end\nframework for internal algorithmic auditing.", "metadata": {}}, {"text": "In Proceedings of the 2020 Conference on Fairness, Accountabil ity, and Transparency.", "metadata": {}}, {"text": "ACM, 33–44.", "metadata": {}}, {"text": "[160] Bogdana Rakova and Roel Dobbe.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Algorithms as Soc ial-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic\nAudits.", "metadata": {}}, {"text": "In 2023 ACM Conference on Fairness, Accountability, and Trans parency.", "metadata": {}}, {"text": "ACM.", "metadata": {}}, {"text": "https://doi.org/10.1145/3593013.3594014\n[161] Akshat Rathi and Dina Bass.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Microsoft’s AI Push I mperils Climate Goal as Carbon Emissions Jump 30%.", "metadata": {}}, {"text": "Bloomberg (2024).", "metadata": {}}, {"text": "[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Al i Farhadi.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "You Only Look Once: Uniﬁed, Real-Time Objec t Detection.", "metadata": {}}, {"text": "arXiv:1506.02640 [cs.CV]\n[163] Jennifer Rhee.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "The robotic imaginary: The human and the price of dehumanize d labor.", "metadata": {}}, {"text": "U of Minnesota Press.", "metadata": {}}, {"text": "[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestri n.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Model-agnostic interpretability of machine learn ing.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1606.05386 (2016).", "metadata": {}}, {"text": "[165] Paola Ricaurte.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Ethics for the majority world: A I and the question of violence at scale.", "metadata": {}}, {"text": "Media, Culture & Society 44, 4 (2022), 726–745.", "metadata": {}}, {"text": "[166] Anna Rogers.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Changing the world by changing the d ata.", "metadata": {}}, {"text": "arXiv preprint arXiv:2105.13947 (2021).", "metadata": {}}, {"text": "[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranja n Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman.", "metadata": {}}, {"text": "2 022.", "metadata": {}}, {"text": "The Dollar Street\ndataset: Images representing the geographic and socioeconomic diversity of the world.", "metadata": {}}, {"text": "In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track .", "metadata": {}}, {"text": "[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kocha nski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross , Nikola Milojevic-Dupont,\nNatasha Jaques, Anna Waldman-Brown, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Tackling climate change with machine learning.", "metadata": {}}, {"text": "ACM Computing Surveys (CSUR) 55, 2 (2022),\n1–96.", "metadata": {}}, {"text": "[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Pa trick Esser, and Björn Ommer.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "High-Resolution Image S ynthesis With Latent\nDiﬀusion Models.", "metadata": {}}, {"text": "In Proceedings of the IEEE/CVF Conference on Computer Vision an d Pattern Recognition (CVPR) .", "metadata": {}}, {"text": "10684–10695.", "metadata": {}}, {"text": "[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Eli zabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin , Hannah Tait Neufeld,\nAndrew Nixon, et al.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Automated pastures and the digita l divide: How agricultural technologies are shaping labour and rural communities.", "metadata": {}}, {"text": "Journal of Rural Studies 68 (2019), 112–122.", "metadata": {}}, {"text": "[171] Mark Ryan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The social and ethical impacts of arti ﬁcial intelligence in agriculture: mapping the agricultur al AI literature.", "metadata": {}}, {"text": "AI & SOCIETY\n(2022), 1–13.", "metadata": {}}, {"text": "[172] Salesforce.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Sustainable AI Policy Principles.", "metadata": {}}, {"text": "https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-susta inable-ai-policy-principles.pdf\n[173] Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll, Diana Akrong, Praveen Paritosh, and Lora M Aroyo.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "“Eve ryone wants to do the\nmodel work, not the data work”: Data Cascades in High-Stakes AI.", "metadata": {}}, {"text": "In proceedings of the 2021 CHI Conference on Human Factors in Co mputing\nSystems.", "metadata": {}}, {"text": "1–15.", "metadata": {}}, {"text": "[174] Aaron Sankin and Surya Mattu.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Predictive Polici ng Software Terrible At Predicting Crimes.", "metadata": {}}, {"text": "https://themarkup.org/prediction-bias/2023/10/02/pr edictive-policing-software-terrible-at-predicting-c rimes.", "metadata": {}}, {"text": "[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Go ran Radanovic, David C.", "metadata": {}}, {"text": "Parkes, and Yang Liu.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "How Do Fai rness Deﬁnitions Fare?", "metadata": {}}, {"text": "Examining Public Attitudes Towards Algorithmic Deﬁnitions of Fairness.", "metadata": {}}, {"text": "InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\n(Honolulu, HI, USA) (AIES ’19) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 99 –106.", "metadata": {}}, {"text": "https://doi.org/10.1145/3306618.3314248\n[176] David Schlosberg.", "metadata": {}}, {"text": "2012.", "metadata": {}}, {"text": "Climate Justice and Capabili ties: A Framework for Adaptation Policy.", "metadata": {}}, {"text": "Ethics & International Aﬀairs 26, 4 (2012), 445–461.", "metadata": {}}, {"text": "[177] David Schlosberg and Lisette B Collins.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "From env ironmental to climate justice: climate change and the disco urse of environmental justice.", "metadata": {}}, {"text": "Wiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.", "metadata": {}}, {"text": "[178] Jurgen Schmidhuber.", "metadata": {}}, {"text": "1991.", "metadata": {}}, {"text": "Neural Sequence Chunkers.", "metadata": {}}, {"text": "Technical Report FKI-148-91 (1991).", "metadata": {}}, {"text": "[179] Christian Schroeder de Witt and Thomas Hornigold.", "metadata": {}}, {"text": "201 9.", "metadata": {}}, {"text": "Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem.", "metadata": {}}, {"text": "arXiv\ne-prints (2019), arXiv–1905.", "metadata": {}}, {"text": "[180] Paul Schütze.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "The Problem of Sustainable AI: A Cr itical Assessment of an Emerging Phenomenon.", "metadata": {}}, {"text": "Weizenbaum Journal of the Digital\nSociety 4, 1 (2024).", "metadata": {}}, {"text": "[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzio ni.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Green AI.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM 63, 12 (2020), 54–63.", "metadata": {}}], "metadata": {"page": 21}}], "metadata": {"page": 21}}, {"title": "Page 22", "paragraphs": [{"text": "22 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalle ng Moorosi, and Richard Klein. 2021. Constructing a visual d ataset to study the eﬀects of\nspatial apartheid in South Africa. In Thirty-ﬁfth conference on neural information processing s ystems datasets and benchmarks track (round 2) .\n[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Sures h Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical\nsystems. In Proceedings of the conference on fairness, accountability, and transparency. 59–68.\n[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinc k, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes. 2023. L imits to the accurate\nand generalizable use of soundscapes to monitor biodiversi ty. Nature Ecology & Evolution 7, 9 (2023), 1373–1378.\n[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay C howdary. 2020. Machine learning applications for precision agriculture: A comprehensive\nreview. IEEE Access 9 (2020), 4843–4873.\n[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romai n Bosc, and Li Wenwei. 2017. Digital infrastructure: Overco ming the digital divide in\nemerging economies. G20 Insights 3 (2017), 1–36.\n[187] Ahmed AH Siddig. 2019. Why is biodiversity data-deﬁci ency an ongoing conservation dilemma in Africa? Journal for Nature Conservation 50\n(2019), 125719.\n[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Real ML: Recog nizing, exploring, and\narticulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency .\n587–597.\n[189] Irene Solaiman. 2023. The gradient of generative AI re lease: Methods and considerations. In Proceedings of the 2023 ACM Conference on Fairness,\nAccountability, and Transparency. 111–122.\n[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahm ad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dod ge, Ellie Evans, Sara\nHooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and\nApostol Vassilev. 2023. Evaluating the Social Impact of Gen erative AI Systems in Systems and Society. arXiv:2306.0594 9 [cs.CY]\n[191] Maddie Stone. 2024. Microsoft employees spent years ﬁ ghting the tech giant’s oil ties. Now, they’re speaking out.\nhttps://grist.org/accountability/microsoft-employees-spent-years-ﬁghting-the-tech-giants-oil-ties-now- theyre-speaking-out/\n[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 20 19. Energy and policy considerations for deep learning in NL P. arXiv preprint\narXiv:1906.02243 (2019).\n[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Serm anet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincen t Vanhoucke, and Andrew\nRabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pat tern recognition. 1–9.\n[194] Sy Taﬀel, Laura Bedford, and Monique Mann. 2019. Ecoci de isn’t ethical: Political ecology and capitalist AI ethic s. ECONOMIES OF VIRTUE 20\n(2019), 58.\n[195] Petros Terzis. 2020. Onward for the Freedom of Others: Marching beyond the AI Ethics. In Proceedings of the 2020 Conference on Fair-\nness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20) . Association for Computing Machinery, New York, NY, USA, 22 0–229.\nhttps://doi.org/10.1145/3351095.3373152\n[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre. 2017. Taxonom ic bias in biodiversity data and\nsocietal preferences. Scientiﬁc reports 7, 1 (2017), 9132.\n[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakale mbe, and Hannah Kerner. 2021. CropHarvest: A global dataset for crop-\ntype classiﬁcation. In Thirty-ﬁfth Conference on Neural Information Processing Sy stems Datasets and Benchmarks Track (Round 2) .\nhttps://openreview.net/forum?id=JtjzUXPEaCu\n[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshav a, Patricia Nying’uro, Herbert Misiani, Jemimah G Ongoma, Y oav Yair, Yacob Mulugetta,\nand Amadou T Gaye. 2023. How to reduce Africa’s undue exposur e to climate risks. Nature 620, 7974 (2023), 488–491.\n[199] UN General Assembly. 2015. Transforming our world : th e 2030 Agenda for Sustainable Development.\nhttps://www.refworld.org/legal/resolution/unga/2015/en/111816\n[200] UNESCO. 2021. Recommendation on the ethics of artiﬁci al intelligence.\n[201] Prasetya Ajie Utama, Naﬁse Sadat Moosavi, and Iryna Gu revych. 2020. Towards debiasing NLU models from unknown bia ses. arXiv preprint\narXiv:2009.12303 (2020).\n[202] I. van de Poel. 2020. Embedding Values in Artiﬁcial Int elligence (AI) Systems. Minds & Machines 30 (2020), 385–409.\nhttps://doi.org/10.1007/s11023-020-09537-4\n[203] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sus tainability and the sustainability of AI. AI and Ethics 1, 3 (2021), 213–218.\n[204] GaÃG , l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker. 2024. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm\nin AI. arXiv preprint arXiv:2409.14160 (2024).\n[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexa ndre Polvora. 2020. Societal and ethical impacts of artiﬁci al intelligence: Critical notes on\nEuropean policy frameworks. Telecommunications Policy 44, 6 (2020), 101961.\n[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Ma deline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. The role of art iﬁcial intelligence in achieving the Sustainable Developm ent Goals. Nature\ncommunications 11, 1 (2020), 1–10.\n[207] Johannes M Waldmueller. 2015. Agriculture, knowledg e and the ‘colonial matrix of power’: approaching sustainab ilities from the Global South.\nJournal of Global Ethics 11, 3 (2015), 294–302.", "sentences": [{"text": "22 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefa la, and Nyalleng Moorosi\n[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalle ng Moorosi, and Richard Klein.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Constructing a visual d ataset to study the eﬀects of\nspatial apartheid in South Africa.", "metadata": {}}, {"text": "In Thirty-ﬁfth conference on neural information processing s ystems datasets and benchmarks track (round 2) .", "metadata": {}}, {"text": "[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Sures h Venkatasubramanian, and Janet Vertesi.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Fairness and abstraction in sociotechnical\nsystems.", "metadata": {}}, {"text": "In Proceedings of the conference on fairness, accountability, and transparency.", "metadata": {}}, {"text": "59–68.", "metadata": {}}, {"text": "[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinc k, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "L imits to the accurate\nand generalizable use of soundscapes to monitor biodiversi ty.", "metadata": {}}, {"text": "Nature Ecology & Evolution 7, 9 (2023), 1373–1378.", "metadata": {}}, {"text": "[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay C howdary.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Machine learning applications for precision agriculture: A comprehensive\nreview.", "metadata": {}}, {"text": "IEEE Access 9 (2020), 4843–4873.", "metadata": {}}, {"text": "[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romai n Bosc, and Li Wenwei.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Digital infrastructure: Overco ming the digital divide in\nemerging economies.", "metadata": {}}, {"text": "G20 Insights 3 (2017), 1–36.", "metadata": {}}, {"text": "[187] Ahmed AH Siddig.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Why is biodiversity data-deﬁci ency an ongoing conservation dilemma in Africa?", "metadata": {}}, {"text": "Journal for Nature Conservation 50\n(2019), 125719.", "metadata": {}}, {"text": "[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Real ML: Recog nizing, exploring, and\narticulating limitations of machine learning research.", "metadata": {}}, {"text": "In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility, and Transparency .", "metadata": {}}, {"text": "587–597.", "metadata": {}}, {"text": "[189] Irene Solaiman.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The gradient of generative AI re lease: Methods and considerations.", "metadata": {}}, {"text": "In Proceedings of the 2023 ACM Conference on Fairness,\nAccountability, and Transparency.", "metadata": {}}, {"text": "111–122.", "metadata": {}}, {"text": "[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahm ad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dod ge, Ellie Evans, Sara\nHooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and\nApostol Vassilev.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Evaluating the Social Impact of Gen erative AI Systems in Systems and Society.", "metadata": {}}, {"text": "arXiv:2306.0594 9 [cs.CY]\n[191] Maddie Stone.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Microsoft employees spent years ﬁ ghting the tech giant’s oil ties.", "metadata": {}}, {"text": "Now, they’re speaking out.", "metadata": {}}, {"text": "https://grist.org/accountability/microsoft-employees-spent-years-ﬁghting-the-tech-giants-oil-ties-now- theyre-speaking-out/\n[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "20 19.", "metadata": {}}, {"text": "Energy and policy considerations for deep learning in NL P.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1906.02243 (2019).", "metadata": {}}, {"text": "[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Serm anet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincen t Vanhoucke, and Andrew\nRabinovich.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Going deeper with convolutions.", "metadata": {}}, {"text": "In Proceedings of the IEEE conference on computer vision and pat tern recognition.", "metadata": {}}, {"text": "1–9.", "metadata": {}}, {"text": "[194] Sy Taﬀel, Laura Bedford, and Monique Mann.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Ecoci de isn’t ethical: Political ecology and capitalist AI ethic s.", "metadata": {}}, {"text": "ECONOMIES OF VIRTUE 20\n(2019), 58.", "metadata": {}}, {"text": "[195] Petros Terzis.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Onward for the Freedom of Others: Marching beyond the AI Ethics.", "metadata": {}}, {"text": "In Proceedings of the 2020 Conference on Fair-\nness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20) .", "metadata": {}}, {"text": "Association for Computing Machinery, New York, NY, USA, 22 0–229.", "metadata": {}}, {"text": "https://doi.org/10.1145/3351095.3373152\n[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Taxonom ic bias in biodiversity data and\nsocietal preferences.", "metadata": {}}, {"text": "Scientiﬁc reports 7, 1 (2017), 9132.", "metadata": {}}, {"text": "[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakale mbe, and Hannah Kerner.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "CropHarvest: A global dataset for crop-\ntype classiﬁcation.", "metadata": {}}, {"text": "In Thirty-ﬁfth Conference on Neural Information Processing Sy stems Datasets and Benchmarks Track (Round 2) .", "metadata": {}}, {"text": "https://openreview.net/forum?id=JtjzUXPEaCu\n[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshav a, Patricia Nying’uro, Herbert Misiani, Jemimah G Ongoma, Y oav Yair, Yacob Mulugetta,\nand Amadou T Gaye.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "How to reduce Africa’s undue exposur e to climate risks.", "metadata": {}}, {"text": "Nature 620, 7974 (2023), 488–491.", "metadata": {}}, {"text": "[199] UN General Assembly.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Transforming our world : th e 2030 Agenda for Sustainable Development.", "metadata": {}}, {"text": "https://www.refworld.org/legal/resolution/unga/2015/en/111816\n[200] UNESCO.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Recommendation on the ethics of artiﬁci al intelligence.", "metadata": {}}, {"text": "[201] Prasetya Ajie Utama, Naﬁse Sadat Moosavi, and Iryna Gu revych.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Towards debiasing NLU models from unknown bia ses.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2009.12303 (2020).", "metadata": {}}, {"text": "[202] I.", "metadata": {}}, {"text": "van de Poel.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Embedding Values in Artiﬁcial Int elligence (AI) Systems.", "metadata": {}}, {"text": "Minds & Machines 30 (2020), 385–409.", "metadata": {}}, {"text": "https://doi.org/10.1007/s11023-020-09537-4\n[203] Aimee Van Wynsberghe.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Sustainable AI: AI for sus tainability and the sustainability of AI.", "metadata": {}}, {"text": "AI and Ethics 1, 3 (2021), 213–218.", "metadata": {}}, {"text": "[204] GaÃG , l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm\nin AI.", "metadata": {}}, {"text": "arXiv preprint arXiv:2409.14160 (2024).", "metadata": {}}, {"text": "[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexa ndre Polvora.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Societal and ethical impacts of artiﬁci al intelligence: Critical notes on\nEuropean policy frameworks.", "metadata": {}}, {"text": "Telecommunications Policy 44, 6 (2020), 101961.", "metadata": {}}, {"text": "[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Ma deline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "The role of art iﬁcial intelligence in achieving the Sustainable Developm ent Goals.", "metadata": {}}, {"text": "Nature\ncommunications 11, 1 (2020), 1–10.", "metadata": {}}, {"text": "[207] Johannes M Waldmueller.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Agriculture, knowledg e and the ‘colonial matrix of power’: approaching sustainab ilities from the Global South.", "metadata": {}}, {"text": "Journal of Global Ethics 11, 3 (2015), 294–302.", "metadata": {}}], "metadata": {"page": 22}}], "metadata": {"page": 22}}, {"title": "Page 23", "paragraphs": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 23\n[208] J. Walmsley. 2021. Artiﬁcial Intelligence and the Val ue of Transparency. AI & Society 36 (2021), 585–595.\nhttps://doi.org/10.1007/s00146-020-01066-z\n[209] Laura Weidinger, Kevin R. McKee, Richard Everett, Saﬀ ron Huang, Tina O. Zhu, Martin J. Chadwick, Christopher Summ erﬁeld, and Iason Gabriel.\n2023. Using the Veil of Ignorance to align AI systems with pri nciples of justice. Proceedings of the National Academy of Sciences 120, 18 (2023),\ne2213709120. https://doi.org/10.1073/pnas.2213709120 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120\n[210] Craig R White, Dustin J Marshall, Steven L Chown, Susan a Clusella-Trullas, Steven J Portugal, Craig E Franklin, an d Frank Seebacher. 2021.\nGeographical bias in physiological data limits prediction s of global change impacts. Functional Ecology 35, 7 (2021), 1572–1578.\n[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru . 2022. The exploited labor behind artiﬁcial intelligence. Noema Magazine 13 (2022).\n[212] T. Wischmeyer. 2020. Artiﬁcial Intelligence and Tran sparency: Opening the Black Box. In Regulating Artiﬁcial Intelligence. Springer, 75–101.\n[213] Robert Wolfe and Aylin Caliskan. 2022. American== whi te in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society . 800–812.\n[214] BigScience Workshop, Teven Le Scao, Angela Fan, Chris topher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha\nLuccioni, François Yvon, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Imp lications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[216] D. Zeng. 2015. AI Ethics: Science Fiction Meets Techno logical Reality. IEEE Intelligent Systems 30, 03 (may 2015), 2–5.\nhttps://doi.org/10.1109/MIS.2015.53\n[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, L aurent El Ghaoui, and Michael Jordan. 2019. Theoretically p rincipled trade-oﬀ between\nrobustness and accuracy. In International conference on machine learning . PMLR, 7472–7482.\n[218] Aram Ziai. 2016. Development discourse and global history: From colonialis m to the sustainable development goals . Taylor & Francis.\n[219] Ofer Zwikael and John Smyrk. 2015. Project governance : Balancing control and trust in dealing with risk. International Journal of Project\nManagement 33, 4 (2015), 852–862.", "sentences": [{"text": "Bridging the Gap:\nIntegrating Ethics and Environmental Sustainability in AI Research and Practice 23\n[208] J.", "metadata": {}}, {"text": "Walmsley.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Artiﬁcial Intelligence and the Val ue of Transparency.", "metadata": {}}, {"text": "AI & Society 36 (2021), 585–595.", "metadata": {}}, {"text": "https://doi.org/10.1007/s00146-020-01066-z\n[209] Laura Weidinger, Kevin R.", "metadata": {}}, {"text": "McKee, Richard Everett, Saﬀ ron Huang, Tina O.", "metadata": {}}, {"text": "Zhu, Martin J.", "metadata": {}}, {"text": "Chadwick, Christopher Summ erﬁeld, and Iason Gabriel.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Using the Veil of Ignorance to align AI systems with pri nciples of justice.", "metadata": {}}, {"text": "Proceedings of the National Academy of Sciences 120, 18 (2023),\ne2213709120.", "metadata": {}}, {"text": "https://doi.org/10.1073/pnas.2213709120 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120\n[210] Craig R White, Dustin J Marshall, Steven L Chown, Susan a Clusella-Trullas, Steven J Portugal, Craig E Franklin, an d Frank Seebacher.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Geographical bias in physiological data limits prediction s of global change impacts.", "metadata": {}}, {"text": "Functional Ecology 35, 7 (2021), 1572–1578.", "metadata": {}}, {"text": "[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru .", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The exploited labor behind artiﬁcial intelligence.", "metadata": {}}, {"text": "Noema Magazine 13 (2022).", "metadata": {}}, {"text": "[212] T.", "metadata": {}}, {"text": "Wischmeyer.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Artiﬁcial Intelligence and Tran sparency: Opening the Black Box.", "metadata": {}}, {"text": "In Regulating Artiﬁcial Intelligence.", "metadata": {}}, {"text": "Springer, 75–101.", "metadata": {}}, {"text": "[213] Robert Wolfe and Aylin Caliskan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "American== whi te in multimodal language-and-image ai.", "metadata": {}}, {"text": "In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society .", "metadata": {}}, {"text": "800–812.", "metadata": {}}, {"text": "[214] BigScience Workshop, Teven Le Scao, Angela Fan, Chris topher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha\nLuccioni, François Yvon, et al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "BLOOM: A 176b-parameter open-access multilingual language model.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.05100 (2022).", "metadata": {}}, {"text": "[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Sustainable AI: Environmental Imp lications, Challenges and Opportunities.", "metadata": {}}, {"text": "arXiv preprint arXiv:2111.00364 (2021).", "metadata": {}}, {"text": "[216] D.", "metadata": {}}, {"text": "Zeng.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "AI Ethics: Science Fiction Meets Techno logical Reality.", "metadata": {}}, {"text": "IEEE Intelligent Systems 30, 03 (may 2015), 2–5.", "metadata": {}}, {"text": "https://doi.org/10.1109/MIS.2015.53\n[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, L aurent El Ghaoui, and Michael Jordan.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Theoretically p rincipled trade-oﬀ between\nrobustness and accuracy.", "metadata": {}}, {"text": "In International conference on machine learning .", "metadata": {}}, {"text": "PMLR, 7472–7482.", "metadata": {}}, {"text": "[218] Aram Ziai.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Development discourse and global history: From colonialis m to the sustainable development goals .", "metadata": {}}, {"text": "Taylor & Francis.", "metadata": {}}, {"text": "[219] Ofer Zwikael and John Smyrk.", "metadata": {}}, {"text": "2015.", "metadata": {}}, {"text": "Project governance : Balancing control and trust in dealing with risk.", "metadata": {}}, {"text": "International Journal of Project\nManagement 33, 4 (2015), 852–862.", "metadata": {}}], "metadata": {"page": 23}}], "metadata": {"page": 23}}]}