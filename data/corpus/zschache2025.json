{"document_id": "zschache2025", "title": "Comparing energy consumption and accuracy in text classification inference", "text": "Comparing energy consumption and accuracy in\ntext classification inference\nJohannes Zschache and Tilman Hartwig\nApplication Lab for AI and Big Data, German Environment Agency,\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de;\nContributing authors: johannes.zschache@uba.de;\nAbstract\nThe increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\nity. While prior research has largely focused on energy consumption during\nmodel training, the inference phase has received comparatively less attention.\nThis study systematically evaluates the trade-offs between model accuracy and\nenergy consumption in text classification inference across various model archi-\ntectures and hardware configurations. Our empirical analysis shows that the\nbest-performing model in terms of accuracy can also be energy-efficient, while\nlarger LLMs tend to consume significantly more energy with lower classifica-\ntion accuracy. We observe substantial variability in inference energy consumption\n(<mWh to >kWh), influenced by model type, model size, and hardware spec-\nifications. Additionally, we find a strong correlation between inference energy\nconsumption and model runtime, indicating that execution time can serve as\na practical proxy for energy usage in settings where direct measurement is not\nfeasible. These findings have implications for sustainable AI development, provid-\ning actionable insights for researchers, industry practitioners, and policymakers\nseeking to balance performance and resource efficiency in NLP applications.\nKeywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI\n1 Introduction\nArtificial intelligence (AI) systems, particularly large language models (LLMs), have\ndriven remarkable progress in Natural Language Processing (NLP) applications. This\n1\narXiv:2508.14170v1  [cs.CL]  19 Aug 2025\n\ndevelopment has been enabled by the Transformer architecture (Vaswani et al., 2017)\nand exemplified by the emergence of large-scale models such as GPT-3 (Brown et al.,\n2020), which have significantly advanced task performance. However, this progress\nhas come at a cost: the escalating energy demands of AI systems pose significant\nenvironmental and computational challenges. Data centers that support AI com-\nputations are major electricity consumers, often dependent on fossil fuels, thereby\ncontributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al.,\n2025). This increasing energy demand challenges global climate objectives such as\nthe Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable\nDevelopment Goals (SDGs), specifically Goal 13 on climate action (United Nations,\n2015b). Consequently, designing energy-efficient AI systems is imperative for aligning\ntechnological advancements with sustainability goals. Moreover, beyond sustainabil-\nity, energy-efficient models offer additional advantages, including reduced hardware\nrequirements, lower financial costs, and faster inference times.\nWhen evaluating machine learning models, most studies concentrate on the quality\nof the model responses by tracking e.g. the accuracy, the RMSE, or other measures.\nAnd even if the energy consumption is taken into account, prior research has mainly\nfocused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni\nand Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in\nreal world deployments, has received comparatively less attention. However, energy\nefficiency during the operational phase is an increasingly relevant topic as LLM appli-\ncations become ubiquitous and LLM models are trained to use additional test-time\ncompute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this\ngap, we present a systematic study on the energy consumption of language models\nduring inference, providing actionable insights for balancing accuracy with efficiency.\nA particularly popular machine learning task is text categorization, a task that\nlightweight models have been shown to handle effectively. For instance, Joulin et al.\n(2017) show that a simple classifier built on word embeddings is often as accurate as\ndeep learning classifiers. Despite this, some authors argue for the use of pre-trained\nLLMs for text classification because it reduces the need for model training and sim-\nplifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials\npromote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023),\nfurther encouraging their use even when more efficient alternatives exist. In order to\njustify the usage of LLM in relatively simple tasks such as text categorization, we\nadvocate a consequent comparison of a model’s response quality to its energy efficiency.\nGiven a practical use case that is occurring in public administration, our study\nempirically analyzes trade-offs between model accuracy and energy consumption across\nvarious language models and hardware configurations. We find that the best perform-\ning model is energy efficient while LLMs show higher energy usage with lower accuracy.\nGenerally, we see significant variability in inference energy consumption, influenced\nby model type, model size, and hardware specifications. Additionally, the energy con-\nsumption during inference is shown to highly correlate with the model’s runtime. This\nmakes the duration of computations a valuable proxy measure for energy consump-\ntion in settings where the latter cannot be traced. Our findings have implications\nfor researchers, industry practitioners, and policymakers advocating for sustainable\n2\n\nAI development (Kaack et al.; Luccioni et al., 2025). By systematically evaluating\ninference efficiency and runtime across architectures and hardware settings, we con-\ntribute to the ongoing discourse on AI’s environmental impact and provide actionable\nguidelines for optimizing NLP applications for both performance and sustainability.\n2 Previous research\nResearch on the environmental impact of machine learning (ML) has primarily focused\non the energy consumption and carbon emissions produced during the training phase of\nlarge-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint\nof NLP models, revealing that the training of a single large-scale transformer model\ncan emit as much carbon as five cars over their entire lifetimes (their measurements\ninclude thousands of hyperparameter tuning jobs, which makes it difficult to disen-\ntangle model-inherent efficiency from experimental setup). This seminal work spurred\nfurther investigations into the environmental costs of training neural networks, includ-\ning large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia,\n2023; Patterson et al., 2022).\nWhile training remains a significant contributor to energy consumption, recent\nstudies have begun to focus on the inference phase. Samsi et al. (2023) highlighted\nthe substantial energy demands of LLM inference but did not explore the relationship\nbetween energy consumption and task-specific performance. Liu et al. (2022) under-\nscore the importance of evaluating NLP models not just on efficiency metrics but also\non accuracy by introducing the Efficient Language Understanding Evaluation (ELUE)\nbenchmark. ELUE aims to establish a Pareto frontier that balances performance and\nefficiency. It includes various language understanding tasks, facilitating fair and com-\nprehensive comparisons among models. However, the framework adopts number of\nparameters and FLOPs as the metrics for model efficiency, disregarding hardware\nspecific factors. Similarly, Chien et al. (2023) estimate the energy consumption associ-\nated with the inference phase of generative AI applications based on the output word\ncount and several assumptions about the application such as the number of FLOPS\nper inference and the sampling rate.\nIn contrast, we promote energy-efficient NLP models by the direct measurement\nof the power consumed during inference. Hence, our work follows the approach of the\nSustaiNLP 2020 shared task (Wang and Wolf, 2020). SustaiNLP demonstrated that\nsubstantial energy savings are achievable with minimal performance loss. While this\nstudy was limited to the performance of a couple of small language models on a single\nbenchmark, we extend these efforts to a greater number of partially very large models\ndeployed to a practical inference scenario.\nThis makes our study very similar to the one by Alizadeh et al. (2025), who inves-\ntigated the trade-offs between accuracy and energy consumption when deploying large\nlanguage models (LLMs) for software development tasks. Besides the finding that\nlarger LLMs with higher energy consumption do not always yield significantly bet-\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\nlayer size and transformer block count, directly correlate with energy usage.\n3\n\nFinally, Luccioni et al. (2024) provide one of the most comprehensive analyses\nof energy consumption during ML model inference. Their study systematically com-\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\nsmaller task-specific and larger multi-purpose models. They found that the larger\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\nespecially for tasks involving text and image generation. Furthermore, their research\nunderscores the variability in energy consumption across tasks and model architec-\ntures. The authors advocate for increased transparency and sustainable deployment\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\nAI systems must be carefully weighed against their utility.\n3 Data and methods\nOur experiments are inspired by an occasionally occurring use case in public admin-\nistration: the management of objections that are submitted by the population. Due\nto a potentially very large amount of submissions, an automatic preprocessing of the\nobjections is of high value. One of the possible steps of an automated workflow is to\ncategorize each submission for optimal forwarding to the responsible department.\nThe data of our study originates from the process of selecting a repository site\nfor high-level radioactive waste in Germany. During the first phase, sub-areas were\nidentified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The\nstatements from the population were categorized, processed and published as the\nFKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is\ngiven by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains\nthe category of the text.\nWe scraped the dataset from the website and restricted it to entries for which the\ntopic occurs at least 10 times. The remaining 378 entries were split into half: 189\nentries for training and 189 entries for testing. This unusual 50:50 split was done so\nthat the test set should be sufficiently representative by containing enough examples\nof each of the 14 categories. Each of the following experiments was repeated 10 times\nwith different train-test-splits. To increase comparability, every experiment was run\nwith the same 10 train-test-splits.\nAn experiment run consists of a training phase and a testing phase. Since large\nlanguage models have been argued to be applicable to text categorization without\ntraining (zero-shot), we omit the training phase for these models and apply LLMs\nwithout fine-tuning. We report the energy consumption and accuracy only for the test\nphase as averages over all runs.\n3.1 Traditional models\nBesides LLMs, we initially run the experiments with lightweight NLP models that we\ncall traditional because they have been used for categorization tasks long before LLMs\nexisted. Specifically, we use a linear model (logistic regression) and a gradient boosting\nalgorithm (xgboost). Logistic regression is a simple, interpretable model that estimates\nthe probability of a class based on a linear combination of input features. XGBoost\n4\n\n(Extreme Gradient Boosting) is an efficient, scalable machine-learning algorithm that\ncombines predictions from multiple decision trees to improve accuracy.\nWe consider three different types of features: bag-of-words (BoW), term frequency-\ninverse document frequency (TF-IDF), and a pretrained multilingual sentence\nembedding. BoW represents text by counting word occurrences without consider-\ning order, while TF-IDF adjusts word counts by their importance across documents,\ncapturing rare but informative terms. The TF-IDF features are calculated on all\n2-gram and 3-gram character sequences, which capture local patterns in the text.\nThe multilingual sentence embedding (https://huggingface.co/sentence-transformers/\nparaphrase-multilingual-mpnet-base-v2) provides dense vector representations of text,\npreserving semantic meaning across languages. This embedding is not fine-tuned on\nthe training data. Both models are trained using the default parameters provided by\nsklearn.linear model.LogisticRegression and xgboost.XGBClassifier.\n3.2 Large language models\nLarge language models (LLMs) were applied without training (zero-shot) using the\ntest set only. Table 1 gives the names and sources of the models used. The LLMs were\nselected by the following criteria:\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architec-\nture that includes next to Transformer also Mamba layers (a state-space model). The\nDeepseek distillations (DS) were added to include models with reasoning capabilities\n(test-time compute).\nModel Link\nLlama 3.1 8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B https://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B https://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini https://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5 https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nT able 1 Selection of large language models\n3.3 Computing Resources\nWe used different computing systems for a comparative analysis of energy efficiency\nacross diverse hardware architectures. This enables the assessment of how architectural\n5\n\ndifferences - especially GPU tensor core capabilities - affect the inference speed and\npower usage. A diversity in computational infrastructure is crucial for generalizing\nfindings across different environments and ensuring the validity and replicability of\nexperimental results in machine learning research. Furthermore, insights gained from\nusing multiple platforms contribute to optimizing resource allocation strategies and\nimproving cost-effectiveness in large-scale machine learning projects.\nTo run our experiments, we were granted access to the high-performance computing\n(HPC) systems of TUD Dresden University of Technology (https://doc.zih.tu-dresden.\nde/) and Leipzig University (https://www.sc.uni-leipzig.de/). For GPU-accelerated\ncomputing, three different systems are available named Capella, Paula, and Clara\n(see Table 2). The main difference for our study is the GPU: while a node on the\nCapella cluster is equipped with 4 x H100, there are 8 x A30 on each node on Paula\nand 4 x V100 on Clara. This means that a large model such as Llama 3.1 70B or\nQwen 2.5 72B fits on a single node of Capella (requiring 2 GPUs) or Paula (requiring\nall 8 GPUs) but takes up two nodes of the Clara cluster (assuming a 16-bit floating\npoint representation of the parameters).\nCluster Capella Paula Clara\nHPC center TUD Dresden\nUniversity of Technology Leipzig University Leipzig University\nnumber of nodes 144 12 6\nCPU per node 2 x AMD (32 cores)\n2.7GHz\n2 x AMD (64 cores)\n2.0GHz\n1 x AMD (32 cores)\n2.0GHz\nRAM per node 768 GB 1 TB 512 GB\nGPU per node 4 x NVIDIA H100\n(94GB)\n8 x NVIDIA A30\n(24 GB)\n4 x NVIDIA V100\n(32GB)\nsingle GPU max\npower consumption 700W 165W 250W\nT able 2 HPC Resources\nLLMs were deployed using the vllm library (https://github.com/vllm-project/\nvllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.\nIf a model is too large to be deployed on a single GPU, the model weights are dis-\ntributed over multiple GPUs, which allow for a parallel computation of the activations\n(c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two\ncomputing nodes are needed, the model is split into two parts and executed sequen-\ntially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model\npart on the first node and then the model part on the second node.\nThe energy consumption and the runtime of the inference phase were measured\nby the CodeCarbon package (https://github.com/mlco2/codecarbon). This package\nuses the NVIDIA Management Library (NVML) and the Intel RAPL files to track\nthe power usage of GPU and CPU (https://mlco2.github.io/codecarbon/methodology.\nhtml#power-usage). The power consumption of the memory is flatly added with\n0.375W/GB of memory used. In settings where the model is deployed on more than\none node, the inference duration is taken as the maximum and the energy as the sum\nover all nodes.\n6\n\nVarious software tools have been created to monitor energy consump-\ntion during the application of machine learning models (https://github.com/\ntiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%\nB8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al.,\n2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy con-\nsumption by monitoring hardware usage. In some settings, CodeCarbon is considered\nmore accurate, yielding values closer to those obtained via physical wattmeters\n(Bouza et al., 2023). Comparing different tools of energy monitoring is beyond the\nscope of our paper.\n4 Results\nFor each model, we report accuracy, energy consumption, and inference duration.\nThe energy consumption and duration were measured only for the inference step, i.e.,\nafter the model and data were already loaded. One inference run involves classifying\n189 text samples from a test set. All tables and figures present the average results\nover 10 runs on different test sets, with the same 10 test sets used for each model.\nMeasurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both\nenergy consumption and duration (logarithmically scaled to base 10).\nFigure 1 illustrates the trade-off between energy consumption and accuracy across\nall models. For these experiments, a single node of the Capella system was used. The\nminimum number of H100 GPUs required varies by model (see Table B1).\nThe highest accuracy was achieved by a traditional linear model using pre-trained\nsentence embeddings. Notably, even the most energy-efficient model - a linear model\nwith TF-IDF features - outperformed several large language models (LLMs). Among\nLLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes\nseven times less energy than the most accurate model (Qwen 2.5 72B), with only\na minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive\nreasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs\nwhile consuming significantly more energy and taking longer to complete inference.\n4.1 Analysis of hardware settings\nThis section analyzes the impact of different hardware configurations (see Tab. 2) on\nenergy consumption. We focus on GPU usage due to its dominant role in machine\nlearning inference.\nAs shown in Figure 2, GPU consumption accounts for the largest share of total\nenergy usage in all experiments. The only exceptions are traditional models without\nembeddings, which do not use the GPU during inference.\n4.1.1 V arying the Number of GPUs\nWe examined the effect of varying the number of GPUs on energy consumption and\ninference duration. Most LLMs were tested on 1, 2, or 4 GPUs on a single Capella\nsystem node. Larger models (Qwen 72B, Phi MoE, Llama 70B, Jamba Mini, and DS\nLlama 70B) required either 2 or 4 GPUs. Increasing the number of GPUs consistently\n7\n\nFig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single\nnode). The energy consumption for the same task spans over six orders of magnitude with traditional\nmodels being the most energy-efficient models and reasoning models are most energy-consuming. The\nbest model for this specific task is a traditional model (Linear Embedding) with moderate energy\nconsumption.\nreduced inference duration but did not reduce energy consumption. In some cases,\nenergy consumption increased due to the additional GPUs in operation (see Figure 3).\n4.1.2 V arying the Number of Nodes\nWhile large models can often be executed on a single computing node, certain hardware\nlimitations or shared high-performance computing (HPC) environments may necessi-\ntate using multiple nodes. In shared systems, it is often easier to access two nodes\nwith half the number of available GPUs than a single node with all its GPUs, due\nto scheduling constraints and resource allocation policies. However, deploying mod-\nels across multiple nodes increases network communication overhead and significantly\nraises energy consumption.\nWe evaluated this effect for the largest models on theCapella system by comparing\na ‘single-node’ configuration (2 GPUs on one node) with a ‘double-node’ configuration\n(1 GPU on each of two nodes). For the double-node configuration, energy consumption\nwas summed across both nodes and averaged over 10 runs, while the reported duration\nreflects the average of the maximum value between the two nodes.\nAs shown in Figure 4, using two nodes increased energy consumption by a factor\nthat depends on the model (see also Table B2). This increase stems from the overhead\n8\n\n[Image page=8 idx=1 name=Im1.png] Size: 1152x768, Data: 77506 bytes\n\nFig. 2 Energy consumption of all models for the inference task on the Capella system (single node)\nFig. 3 Effects of the number of GPUs on the runtime and consumed energy ( Capella, single node).\nDeepseek models are not shown.\nof coordinating across nodes. Inference duration also increased by the same factor\ndue to the sequential execution of model components and the required inter-node\ncommunication.\n4.1.3 Comparing GPU Architectures\nFinally, we compared the energy efficiency of different GPU architectures (see Figure\n5 and Table B3). Interestingly, the expected efficiency gains from using the more pow-\nerful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models.\nThis discrepancy is likely to arise because Deepseek models engage in extended rea-\nsoning by generating a larger output of words before making a classification decision.\nConsequently, the efficiency of H100 GPUs becomes evident only when substantial\n9\n\n[Image page=9 idx=1 name=Im2.png] Size: 1344x576, Data: 61871 bytes\n\n[Image page=9 idx=2 name=Im3.png] Size: 1152x576, Data: 36496 bytes\n\nFig. 4 Comparison single node vs. double node deployment ( Capella).\ntext is generated. For models generating a single token per inference, a V100 or even\na A30 GPU is more efficient in inference.\nFig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment.\n4.2 Linear relationship between duration and energy\nIn most of the tables in appendix B, we report both the duration of each inference\nrun and its corresponding energy consumption. Since energy is the integral of power\nover time, these two measures exhibit a strong correlation. If the power is constant\nover time, this correlation should be linear. Figure 6 illustrates this relationship for\n10\n\n[Image page=10 idx=1 name=Im4.png] Size: 1152x576, Data: 29545 bytes\n\n[Image page=10 idx=2 name=Im5.png] Size: 1344x768, Data: 51427 bytes\n\nall experiments conducted on a single node of the Capella cluster. When controlling\nfor the number of GPUs used for model deployment, the relation between duration\nand energy is approximately linear. Therefore, the duration appears to serve as a good\nproxy for the energy consumed.\nFig. 6 Plotting the relationship between duration and energy consumption (single node onCapella).\nThe lines are added by running a linear regression model.\nTo further quantify the relationship between duration and energy consumption,\nwe performed a linear regression analysis for each hardware configuration (see Table\n3). This analysis includes all experiments, regardless of the number of nodes used\nfor model deployment. The consistently high R 2 values across all configurations indi-\ncate that, for a given hardware setup, duration and energy consumption are nearly\ninterchangeable as measures of computational effort.\nMoreover, when the regression coefficients are known for a specific computing sys-\ntem, energy consumption can be reliably estimated from the duration and the number\nof GPUs. Only the coefficients of duration ( a) and of the interaction term dura-\ntion:GPUs (c) are statistically significant. The other coefficients (b and d) are omitted\nfrom the approximation:\nEnergy ≈ (a + c · GPUs) · Duration. (1)\nFor instance, on the Capella system, the following approximation holds for any\ncomputation:\n11\n\n[Image page=11 idx=1 name=Im6.png] Size: 960x768, Data: 60337 bytes\n\nEnergy\n1 Wh ≈ (0.1 + 0.09 · GPUs) · Duration\n1 s . (2)\nThis relationship suggests that, under fixed hardware conditions, monitoring the\nduration of computations provides an efficient means of estimating energy usage with\nminimal additional measurement overhead.\nDependent variable: Energy\nCapella Clara Paula\n(1) (2) (3)\nDuration (a) 0.097 ∗∗∗ 0.061∗∗∗ 0.079∗∗∗\n(0.008) (0.002) (0.026)\nGPUs (b) −0.500 0.048 −2.195\n(2.297) (0.339) (3.472)\nDuration:GPUs (c) 0.090 ∗∗∗ 0.036∗∗∗ 0.054∗∗∗\n(0.004) (0.0002) (0.004)\nConstant (d) −6.205 −0.826 3.328\n(5.725) (1.368) (17.220)\nObservations 44 19 23\nR2 0.998 1.000 0.989\nAdjusted R2 0.998 1.000 0.987\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nT able 3 Linear regression of energy consumption on\nduration (table format by Hlavac, 2022). The numbers\n(coefficients) give the estimated effects of each predictor on\nthe dependent variable. A positive coefficient means the\nvariable increases the outcome, while a negative coefficient\nmeans it decreases the outcome. The standard error (in\nparenthesis) estimates the variability of the coefficient\nestimate. The p-value (given by the asterisks) indicates\nwhether the predictor is statistically significant (different\nfrom zero).\n5 Discussion\nWe would like to mention the limitations of our study, which also point to the areas\nof future research. First, while traditional models were trained on approximately 200\nexamples, the large language models (LLMs) were applied in a zero-shot setting,\nmeaning they had no access to labeled examples. Previous research has shown that\nfew-shot prompting - where representative examples are included in the prompt - can\nimprove performance (Brown et al., 2020). For the present study, we kept the prompt\n12\n\nas simple as possible (see section A). But in an actual application, we would add back-\nground information about the data and the categories. In general, prompt engineering,\nthe addition of representative examples to the prompt, or even fine-tuning an LLM\ncould yield higher accuracy rates. On the other hand, energy efficiency in LLMs can\nbe improved through model quantization, which reduces computational demands by\ncompressing model parameters (Jacob et al., 2017).\nSecond, we do not account for the energy costs associated with training the tradi-\ntional models because it is infeasible to compare them to the training costs of LLMs.\nThe LLMs used in this study were pre-trained by external organizations and made\npublicly available. As a result, the energy costs of training are distributed among all\nusers, making it difficult to estimate per-user energy consumption. Even if training\nenergy costs for an LLM were known, the number of users remains uncertain. Addi-\ntionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also\ncontribute to energy consumption. Deploying an LLM on a dedicated server (e.g.,\nusing vLLM) requires setup time and additional energy. Beyond inference, significant\ntime and computational resources are also required for development tasks, including\ndata processing, testing different models and prompts, parameter tuning, and debug-\nging - workloads that apply to both traditional models and LLMs. The measurement\nof additional related energy consumptions (such as network traffic or disk storage) is\nbeyond the scope of this paper.\nThird, energy consumption was measured using CodeCarbon, a tool recognized\nfor providing reliable estimates of a machine’s total energy use (Bouza et al., 2023).\nHowever, it does not allow for precise measurement of energy consumption at the\nlevel of individual processes. Moreover, power intake was recorded at 15-second\nintervals, meaning the accuracy of energy estimates improves with longer-running\nprocesses. Another limitation of CodeCarbon is that RAM energy consumption is\napproximated at 0.375W per GB of memory used. While the Running Average\nPower Limit (RAPL) framework can directly measure RAM power consumption, it\nis not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#\nissuecomment-2589805160). Additionally, in shared computing environments such as\nhigh-performance computing (HPC) clusters, measurements may be affected by other\nusers’ activities. Especially when an LLM was deployed across multiple nodes, varia-\ntions in network traffic at different times may have influenced energy measurements. A\nmore precise assessment of energy efficiency would benefit from using dedicated com-\nputing resources with physical wattmeters and high-resolution energy measurement\ntools(e.g. Ilsche et al., 2019).\nIn the following, we assess further limitations of the present study in more detail.\nMore specifically, we address our focus on a single dataset in section 5.1 and the\nlimitation to the text categorisation task in section 5.2. Subsequently, we contextualise\nour work in the broader context of planet-centered LLMs (section 5.3).\n5.1 Analysis on other datasets\nOur analysis was conducted on a highly specialized dataset. To assess the generaliz-\nability of our findings, we replicated the experiments using four additional, widely used\ndatasets (see table 4). These datasets were selected from the HuggingFace platform\n13\n\nbased on popularity and had to meet two criteria: suitability for text classification and\ninclusion of two columns - text and label. To maintain comparability with our initial\nanalysis, we randomly sampled 200 training examples and 200 test examples from each\ndataset. Using a slightly larger training set might have provided an advantage to tra-\nditional models, as the LLMs were applied in a zero-shot setting without fine-tuning.\nEach model experiment was repeated 10 times with different samples, ensuring that\neach model was tested on the same 10 sets.\nName Classification Task ID on https://huggingface.co/datasets\nnews news topics: World, Sports,\nBusiness, Sci/Tech\nfancyzhx/ag news\nyelp sentiment: 1-5 stars Yelp/yelp review full\ntomatoes sentiment: pos, neg cornell-movie-review-data/rotten tomatoes\nemotion emotion: anger, fear, joy,\nlove, sadness, surprise\ndair-ai/emotion\nT able 4 Selection of datasets for text classification tasks.\nFigure 7 visualizes the relationship between accuracy and energy consumption for\nthese additional text classification tasks. For clarity, we restricted the visualization to\nthe models with the three highest accuracy scores and included the linear model with\nsentence embeddings for comparison (see Tables B4 and B5 for details).\nSimilar to our findings with the FKTG dataset, the DeepSeek models do not\noutperform the best non-reasoning models in most cases. The only exception is the\nemotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0 .61, slightly\nsurpassing the best non-reasoning model, Phi 3.5 MoE (0 .60). However, unlike in the\nprevious analysis, for every dataset, at least one LLM outperforms the best traditional\nmodel. For the news dataset, Llama 3.1 70B achieves an accuracy 0 .05 points higher\nthan the best linear model (0 .88 vs. 0 .83). However, this comes at the cost of signif-\nicantly higher energy consumption (34 .15 Wh vs. 0 .0021 Wh), highlighting the need\nfor careful trade-off considerations.\nIn the case of sentiment analysis on the Yelp dataset, traditional models perform\nconsiderably worse than LLMs, justifying the energy costs of LLM deployment. In some\ncases, a smaller model, such as Qwen 2.5 7B, may be sufficient. While its accuracy is\nslightly lower than the version with 72B parameters (0 .60 vs. 0 .68), it consumes only\none-eighth of the energy. A similar pattern is observed for sentiment analysis on the\nRotten Tomatoes dataset, where traditional models fail to match LLM performance.\nAmong the larger models, Jamba Mini 1.5 stands out as one of the most efficient\nchoices, offering strong accuracy while consuming significantly less energy. Notably,\ndespite having nearly as many parameters as Llama 3.1 70B and Qwen 2.5 72B (51.6B\nvs. 70B/72B), Jamba Mini 1.5 requires only a quarter of the energy for the same task.\nFinally, for emotion classification, the linear model with sentence embeddings is\namong the top-performing models. In this case, a traditional model provides the most\nefficient solution. Hence, accuracy-energy trade-offs must be assessed on a case-by-\ncase basis. In some scenarios, traditional models are sufficient, while in others, LLMs\noffer justifiable benefits despite higher energy consumption. However, a reason for the\n14\n\nFig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the\nLinear Embedding model was added for comparison), Capella system, single node. See Tables B4\nand B5 for results of all models.\nsuperior performance of LLMs on some datasets might be that the data were included\nin the model’s training data. Our study uses data that are probably not part of\nany LLM training set. Nevertheless, test-time compute, as featured by the Deepseek\nmodels, has no benefits in text classification tasks, and the linear relationship between\ncomputation runtime and energy consumption holds across all datasets (see Table B6).\n5.2 Transferability to other tasks\nAnother limitation of the present study is its exclusive focus on the categorization\ntask, which confines the analysis to a narrow subset of machine learning challenges.\nWhile this focus allows for a straightforward measurement of a model’s performance\n(using the accuracy metric), it neglects the applicability of the results to other tasks.\nRecent studies suggest that similar comparisons in terms of efficiency and accuracy can\nbe insightful in a variety of domains beyond categorization. For instance, Clavi´ e et al.\n15\n\n[Image page=15 idx=1 name=Im7.png] Size: 1152x1152, Data: 98323 bytes\n\n(2025) demonstrate that simple encoder-based models can effectively tackle generative\ntasks, expanding the potential applications of smaller, less energy-hungry models.\nMoreover, a growing body of research highlights the advantages of fine-tuned small\nmodels for specialized tasks, where they often outperform larger models (Savvov,\n2024). This trend is evident in studies such as Wei et al. (2024), where an diabetes-\nspecific LLM - despite having significantly fewer parameters - outperforms both GPT-4\nand Claude-3.5 in processing various diabetes tasks. Similarly, Lu et al. (2023) report\nthat their fine-tuned models achieve performance levels comparable to GPT-4 on\ndomain-specific annotation tasks, yet with hundreds of times fewer parameters and\nsignificantly reduced computational costs. Zhan et al. (2025) further emphasize the\nsuperior performance of fine-tuned small models over zero-shot LLMs, particularly in\nin-domain content moderation tasks.\nThe study by Luccioni et al. (2024) provides additional insights into the balance\nbetween model size and efficiency while looking at ten different machine learning tasks\nincluding image classification and captioning, question answering, summarization, as\nwell as image and text generation. The authors demonstrate that smaller models can\nachieve high performance with considerably less resource consumption. Their initiative\nresulted into the AI Energy Score (https://huggingface.co/AIEnergyScore), a tool\ndesigned to assess the environmental impact of AI models on a range of tasks, and\nreinforces the growing importance of considering energy efficiency in model evaluation.\n5.3 Further Requirements of Planet-Centered LLMs\nWhile energy consumption and the associated carbon footprint remain crucial con-\nsiderations for sustainable AI, truly planet-centered LLMs must meet a broader\nset of requirements that go beyond mere efficiency. These include other limited\nresources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical\nconsiderations, and technical adaptability to ensure responsible and sustainable AI\ndeployment.\nTransparency in AI models is essential for trust and reproducibility (Raji et al.,\n2020). The predictions of traditional LLM models are generally more transparent than\nthose of LLMs. Open-source LLMs, where both model architectures and training data\nare publicly available, contribute to scientific progress, allow for direct model compar-\nisons such as this present study, and reduce dependency on proprietary technologies\n(Wei et al., 2023). Furthermore, the ability to inspect training data is crucial to assess\npotential biases and copyright compliance (Bender et al., 2021). Many proprietary\nmodels, such as GPT-4, lack such transparency, making it difficult to evaluate their\nfairness and ethical considerations. The EU AI Act will require providers of general-\npurpose AI models to publish a sufficiently detailed summary of their training data\nstarting in August 2025, which further highlights the call for transparency.\nLLMs vary significantly in size, ranging from lightweight models such as fast-\nText (Joulin et al., 2017) to massive architectures like BLOOM-176B, which require\nsubstantial GPU memory and network bandwidth (Luccioni et al., 2023). These com-\nputational demands translate into high operational costs and environmental impacts.\nMoreover, some models require proprietary hardware, limiting their accessibility and\n16\n\nlong-term sustainability. Future AI systems should prioritize modularity and adapt-\nability, enabling efficient integration into diverse infrastructures without excessive\nresource demands.\nThe relevance and fairness of AI-generated outputs depend on the quality and\nrecency of training data. Stale or biased datasets can lead to misleading results and\nreinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In par-\nticular, the presence of toxic content or hate speech in training data can result in\nmodels generating harmful or discriminatory outputs, which poses serious challenges\nfor their deployment in sensitive contexts such as education, healthcare, or public\nadministration. Moreover, safety concerns—such as the risk of models producing fac-\ntually incorrect, manipulative, or otherwise harmful content—are especially critical in\npublic-sector applications, where accountability and trust are paramount (Weidinger\net al., 2021). Addressing these challenges requires robust bias-mitigation strategies\nand transparent documentation of model behavior.\nTo align with global sustainability and ethical AI principles, future research should\nemphasize the development of adaptable, transparent, and energy-efficient LLMs. By\nintegrating principles of openness, fairness, and regulatory compliance, we can foster\nAI systems that not only minimize environmental impact but also promote responsible\nand equitable usage across sectors.\nAcknowledgements. We gratefully acknowledge the support provided by the Fed-\neral Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV).\nAdditionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their\nsupport and inspiration.\nThis work was supported by high-performance computer time and resources from\nthe Center for Information Services and High Performance Computing (ZIH) of TUD\nDresden University of Technology and the systems for scientific computing of Leipzig\nUniversity. We thank the Center for Scalable Data Analytics and Artificial Intelligence\n(ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.\nThe tool ChatGPT (OpenAI) was used to revise the text of the paper.\nAuthor contribution statements. T.H. conceived the study, initiated the project,\nled the research effort, and contributed to the literature review and manuscript writing.\nJ.Z. designed and implemented the experiments, developed the codebase, conducted\ndata analysis, and contributed to drafting the manuscript.\nCompeting interests. There are no competing interests.\nAvailability of data and code. All underlying data will be shared upon reasonable\nrequest to the corresponding author. The source code will be made public.\n17\n\nAppendix A LLM prompt\nFor the zero-shot classification, we prompted the LLM with the following instruction\n(originally in German):\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− . . .\nThe categories were a fixed set of 14 options that occurred in the train-\ning as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨ orungszonen’,\n‘ ¨Offentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des\nEndlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler\nLagerung’, ‘Datenverf¨ ugbarkeit’, ‘Modellierung’, ‘Referenzdatens¨ atze’, ‘Bereitstellung\nder Daten’, ‘Ausschlusskriterien’.\nSince we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the\nfinal prompt was automatically extended to the following:\n− r o l e : s y s t e m\nc o n t e n t : |−\nYour i n p u t f i e l d s a r e :\n1 . ‘ t e x t ‘ ( s t r )\nYour o u t p u t f i e l d s a r e :\n1 . ‘ c a t e g o r y ‘ ( s t r )\nA l l i n t e r a c t i o n s w i l l be s t r u c t u r e d i n t h e f o l l o w i n g way ,\nw i t h t h e a p p r o p r i a t e v a l u e s f i l l e d i n .\n[ [ # # t e x t # # ] ]\n{ t e x t}\n[ [ # # c a t e g o r y # # ] ]\n{ c a t e g o r y}\n[ [ # # c o m p l e t e d # # ] ]\nI n a d h e r i n g t o t h i s s t r u c t u r e , y o u r o b j e c t i v e i s :\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− . . .\n− r o l e : u s e r\nc o n t e n t : |−\n[ [ # # t e x t # # ] ]\n<t e x t>\nRespond w i t h t h e c o r r e s p o n d i n g o u t p u t f i e l d s , s t a r t i n g w i t h\nt h e f i e l d ‘ [ [ # # c a t e g o r y # # ] ] ‘ , and t h e n e n d i n g w i t h t h e\nm a r k e r f o r ‘ [ [ # # c o m p l e t e d # # ] ] ‘ .\n18\n\nAppendix B Tables\nModel GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)\nLinear BoW 1 <0.01 0.43 0.01 139.96\nLinear Tf-idf 1 <0.01 0.41 0.01 43.72\nLinear Embedding 1 0.12 0.57 1.64 259.41\nXGBoost BoW 1 <0.01 0.35 0.01 63.32\nXGBoost Tf-idf 1 <0.01 0.47 0.01 67.77\nXGBoost Embedding 1 0.21 0.47 2.87 259.94\nLlama 3.1 8B 1 5.86 0.35 36.88 572.49\nLlama 3.1 70B 2 48.60 0.48 161.59 1082.82\nQwen 2.5 7B 1 5.58 0.45 36.28 553.84\nQwen 2.5 72B 2 48.66 0.51 164.44 1065.31\nPhi 3.5 Mini 1 5.74 0.30 41.45 498.46\nPhi 3.5 MoE 2 11.00 0.40 55.51 713.34\nJamba Mini 1.5 2 17.42 0.34 78.61 797.94\nDS Llama 8B 1 79.64 0.37 517.83 553.67\nDS Llama 70B 2 702.06 0.46 2543.47 993.68\nDS Qwen 14B 1 155.20 0.39 981.35 569.33\nDS Qwen 32B 1 373.56 0.45 2255.99 596.11\nT able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system,\nsingle node, shown are averages over 10 runs\nModel Duration (s) Energy consumed (Wh)\nsingle double ratio single double ratio\nLlama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95\nQwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97\nJamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71\nDS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71\nT able B2 Comparison single vs. double node deployment, Capella system\nModel Duration (s) Energy consumed (Wh)\nA30 V100 H100 A30 V100 H100\nLlama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86\nQwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58\nPhi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74\nPhi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04\nDS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64\nDS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58\nDS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58\nT able B3 Comparison of different GPU cards, single node deployment.\n19\n\nDataset news yelp\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.65 <0.01 0.36\nLinear Tf-idf <0.01 0.65 <0.01 0.34\nLinear Embedding <0.01 0.83 0.04 0.43\nXGBoost BoW <0.01 0.48 <0.01 0.31\nXGBoost Tf-idf <0.01 0.52 <0.01 0.29\nXGBoost Embedding 0.03 0.74 0.01 0.40\nLlama 3.1 8B 4.31 0.71 4.73 0.58\nLlama 3.1 70B 34.15 0.88 36.71 0.67\nQwen 2.5 7B 4.21 0.01 4.52 0.60\nQwen 2.5 72B 33.75 0.79 38.20 0.68\nPhi 3.5 Mini 3.30 0.53 15.55 0.58\nPhi 3.5 MoE 8.53 0.78 8.32 0.58\nJamba Mini 1.5 9.34 0.78 11.45 0.56\nDS Llama 8B 60.58 0.82 97.18 0.62\nDS Llama 70B 483.73 0.83 707.03 0.67\nDS Qwen 14B 113.81 0.83 177.41 0.63\nDS Qwen 32B 271.92 0.83 358.62 0.63\nT able B4 Measurements of all models for the inference task on the news and\nyelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset tomatoes emotion\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.59 <0.01 0.36\nLinear Tf-idf <0.01 0.59 <0.01 0.40\nLinear Embedding 0.01 0.79 <0.01 0.59\nXGBoost BoW <0.01 0.54 <0.01 0.30\nXGBoost Tf-idf <0.01 0.55 <0.01 0.33\nXGBoost Embedding <0.01 0.76 <0.01 0.53\nLlama 3.1 8B 4.12 0.87 4.46 0.56\nLlama 3.1 70B 32.07 0.91 34.12 0.58\nQwen 2.5 7B 4.04 0.73 4.17 0.37\nQwen 2.5 72B 33.25 0.91 34.81 0.58\nPhi 3.5 Mini 7.20 0.87 5.13 0.53\nPhi 3.5 MoE 7.72 0.89 8.82 0.60\nJamba Mini 1.5 8.37 0.91 10.22 0.56\nDS Llama 8B 72.15 0.83 81.82 0.60\nDS Llama 70B 510.86 0.90 670.40 0.61\nDS Qwen 14B 134.02 0.89 148.20 0.60\nDS Qwen 32B 246.48 0.89 323.48 0.60\nT able B5 Measurements of all models for the inference task on the tomatoes and\nemotion datasets, Capella system, single node, shown are averages over 10 runs\n20\n\nDependent variable: Energy\ntomatoes emotion news yelp\n(1) (2) (3) (4)\nDuration 0.040 ∗∗∗ 0.043∗∗∗ 0.052∗∗∗ 0.045∗∗∗\n(0.002) (0.002) (0.003) (0.003)\nGPUs −0.079 −0.052 0.536 0.810\n(0.950) (1.011) (1.470) (1.545)\nDuration:GPUs 0.122 ∗∗∗ 0.120∗∗∗ 0.115∗∗∗ 0.120∗∗∗\n(0.001) (0.001) (0.002) (0.002)\nConstant −0.397 −0.464 −1.300 −1.773\n(1.290) (1.372) (1.985) (2.103)\nObservations 17 17 17 17\nR2 1.000 1.000 1.000 1.000\nAdjusted R2 1.000 1.000 1.000 1.000\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nT able B6 Linear regression of energy consumption on duration for\nthe datasets of section 5.1 (table format by Hlavac, 2022).\nReferences\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems, vol. 30 (2017)\nBrown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A.,et al.: Language models are few-shot\nlearners. In: Advances in Neural Information Processing Systems, vol. 33, pp.\n1877–1901 (2020)\nLacoste, A., Luccioni, A., Schmidt, V., Dandres, T.: Quantifying the Carbon Emissions\nof Machine Learning (2019). https://arxiv.org/abs/1910.09700\nAxenbeck, J., Kunkel, S., Blain, J., et al.: Global Embodied Emissions of Digital\nTechnologies: The Hidden 42%. Research Square. Preprint (Version 1) available\nat Research Square (2025). https://doi.org/10.21203/rs.3.rs-6479454/v1 . https://\nwww.researchsquare.com/article/rs-6479454/v1\nUnited Nations: Paris Agreement (2015). https://unfccc.int/sites/default/files/\nenglish paris agreement.pdf\nUnited Nations: Transforming our world: the 2030 Agenda for Sustain-\nable Development (2015). https://sustainabledevelopment.un.org/post2015/\n21\n\ntransformingourworld\nStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep\nLearning in NLP (2019). https://arxiv.org/abs/1906.02243\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D.,\nTexier, M., Dean, J.: Carbon Emissions and Large Neural Network Training (2021).\nhttps://arxiv.org/abs/2104.10350\nLuccioni, A.S., Hernandez-Garcia, A.: Counting Carbon: A Survey of Factors Influ-\nencing the Emissions of Machine Learning (2023). https://arxiv.org/abs/2302.\n08476\nOpenAI: Learning to reason with LLMs (2024). https://openai.com/index/\nlearning-to-reason-with-llms/\nDeepSeek-AI: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning (2025). https://arxiv.org/abs/2501.12948\nJoulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for efficient text\nclassification. In: Lapata, M., Blunsom, P., Koller, A. (eds.) Proceedings of the\n15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pp. 427–431. Association for Computational\nLinguistics, Valencia, Spain (2017). https://aclanthology.org/E17-2068/\nWang, Z., Pang, Y., Lin, Y., Zhu, X.: Adaptable and Reliable Text Classification using\nLarge Language Models (2024). https://arxiv.org/abs/2405.10523\nLangChain Team: Classification Tutorial – LangChain Documentation. https://\npython.langchain.com/docs/tutorials/classification/. Accessed: 2025-02-23 (2023)\nLamini Team: CAT Documentation – Lamini. https://docs.lamini.ai/cat/. Accessed:\n2025-02-23 (2023)\nKaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning\nartificial intelligence with climate change mitigation 12(6), 518–527 https://doi.\norg/10.1038/s41558-022-01377-7\nLuccioni, A.S., Strubell, E., Crawford, K.: From Efficiency Gains to Rebound Effects:\nThe Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate (2025).\nhttps://arxiv.org/abs/2501.16548\nPatterson, D., Gonzalez, J., H¨ olzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild,\nD., So, D., Texier, M., Dean, J.: The Carbon Footprint of Machine Learning Training\nWill Plateau, Then Shrink (2022). https://arxiv.org/abs/2204.05149\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W.,\nKepner, J., Tiwari, D., Gadepally, V.: From Words to Watts: Benchmarking the\n22\n\nEnergy Costs of Large Language Model Inference (2023). https://arxiv.org/abs/\n2310.03003\nLiu, X., Sun, T., He, J., Wu, J., Wu, L., Zhang, X., Jiang, H., Cao, Z., Huang, X., Qiu,\nX.: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (2022).\nhttps://arxiv.org/abs/2110.07038\nChien, A.A., Lin, L., Nguyen, H., Rao, V., Sharma, T., Wijayawardana, R.: Reducing\nthe carbon impact of generative ai inference (today and in 2035). In: Proceedings of\nthe 2nd Workshop on Sustainable Computer Systems. HotCarbon ’23. Association\nfor Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/\n3604930.3605705 . https://doi.org/10.1145/3604930.3605705\nWang, A., Wolf, T.: Overview of the sustainlp 2020 shared task. In: SUSTAINLP\n(2020). https://api.semanticscholar.org/CorpusID:226283937\nAlizadeh, N., Belchev, B., Saurabh, N., Kelbert, P., Castor, F.: Language Models in\nSoftware Development Tasks: An Experimental Analysis of Energy and Accuracy\n(2025). https://arxiv.org/abs/2412.00329\nLuccioni, S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the\ncost of ai deployment? In: Proceedings of the 2024 ACM Conference on Fairness,\nAccountability, and Transparency. FAccT ’24, pp. 85–99. Association for Computing\nMachinery, New York, NY, USA (2024). https://doi.org/10.1145/3630106.3658542\nBai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M.,\nZhang, Y., Song, X., Yang, C., Cheng, Y., Zhao, L.: Beyond Efficiency: A Systematic\nSurvey of Resource-Efficient Large Language Models (2024). https://arxiv.org/abs/\n2401.00625\nAnthony, L.F.W., Kanding, B., Selvan, R.: Carbontracker: Tracking and Predicting\nthe Carbon Footprint of Training Deep Learning Models (2020). https://arxiv.org/\nabs/2007.03051\nHenderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., Pineau, J.: Towards the\nsystematic reporting of the energy and carbon footprints of machine learning. J.\nMach. Learn. Res. 21(1) (2020)\nBouza, L., Bugeau, A., Lannelongue, L.: How to estimate carbon footprint when\ntraining deep learning models? a guide and review. Environmental Research\nCommunications 5(11), 115014 (2023) https://doi.org/10.1088/2515-7620/acf81b\nHlavac, M.: stargazer: Well-Formatted Regression and Summary Statistics Tables. R\npackage version 5.2.3 (2022). https://CRAN.R-project.org/package=stargazer\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\n23\n\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C.,\nChen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCan-\ndlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot\nlearners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.)\nAdvances in Neural Information Processing Systems, vol. 33, pp. 1877–1901. Curran\nAssociates, Inc., ??? (2020)\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,\nKalenichenko, D.: Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference (2017). https://arxiv.org/abs/1712.05877\nIlsche, T., Hackenberg, D., Sch¨ one, R., Bielert, M., H¨ opfner, F., E. Nagel, W.: Met-\nricq: A scalable infrastructure for processing high-resolution time series data. In:\n2019 IEEE/ACM Industry/University Joint International Workshop on Data-center\nAutomation, Analytics, and Control (DAAC), pp. 7–12 (2019). https://doi.org/10.\n1109/DAAC49578.2019.00007\nClavi´ e, B., Cooper, N., Warner, B.: It’s All in The [MASK]: Simple Instruction-Tuning\nEnables BERT-like Masked Language Models As Generative Classifiers (2025).\nhttps://arxiv.org/abs/2502.03793\nSavvov, S.: Your Company Needs Small Language Models (2024). https://\ntowardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9/\nWei, L., Ying, Z., He, M., Chen, Y., Yang, Q., Hong, Y., Lu, J., Li, X., Huang, W.,\nChen, Y.: An adapted large language model facilitates multiple medical tasks in\ndiabetes care (2024). https://arxiv.org/abs/2409.13191\nLu, Y., Yao, B., Zhang, S., Wang, Y., Zhang, P., Lu, T., Li, T.J.-J., Wang, D.: Human\nStill Wins over LLM: An Empirical Study of Active Learning on Domain-Specific\nAnnotation Tasks (2023). https://arxiv.org/abs/2311.09825\nZhan, X., Goyal, A., Chen, Y., Chandrasekharan, E., Saha, K.: SLM-Mod: Small\nLanguage Models Surpass LLMs at Content Moderation (2025). https://arxiv.org/\nabs/2410.13155\nRaji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving face:\nInvestigating the ethical concerns of facial recognition auditing. Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society (2020)\nWei, J., Bosma, M., Zhao, V.Y., et al.: Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971 (2023)\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of\nstochastic parrots: Can language models be too big? Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency (2021)\n24\n\nLuccioni, A.S., Viguier, S., Ligozat, A.-L.: Estimating the carbon footprint of bloom,\na 176b parameter language model. J. Mach. Learn. Res. 24(1) (2023)\nGehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: Realtoxicityprompts:\nEvaluating neural toxic degeneration in language models. Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP)\n(2020)\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359 (2021)\n25", "metadata": {"url": "https://arxiv.org/pdf/2508.14170", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Comparing energy consumption and accuracy in\ntext classification inference\nJohannes Zschache and Tilman Hartwig\nApplication Lab for AI and Big Data, German Environment Agency,\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de;\nContributing authors: johannes.zschache@uba.de;\nAbstract\nThe increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\nity. While prior research has largely focused on energy consumption during\nmodel training, the inference phase has received comparatively less attention.\nThis study systematically evaluates the trade-offs between model accuracy and\nenergy consumption in text classification inference across various model archi-\ntectures and hardware configurations. Our empirical analysis shows that the\nbest-performing model in terms of accuracy can also be energy-efficient, while\nlarger LLMs tend to consume significantly more energy with lower classifica-\ntion accuracy. We observe substantial variability in inference energy consumption\n(<mWh to >kWh), influenced by model type, model size, and hardware spec-\nifications. Additionally, we find a strong correlation between inference energy\nconsumption and model runtime, indicating that execution time can serve as\na practical proxy for energy usage in settings where direct measurement is not\nfeasible. These findings have implications for sustainable AI development, provid-\ning actionable insights for researchers, industry practitioners, and policymakers\nseeking to balance performance and resource efficiency in NLP applications.\nKeywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI\n1 Introduction\nArtificial intelligence (AI) systems, particularly large language models (LLMs), have\ndriven remarkable progress in Natural Language Processing (NLP) applications. This\n1\narXiv:2508.14170v1  [cs.CL]  19 Aug 2025", "sentences": [{"text": "Comparing energy consumption and accuracy in\ntext classification inference\nJohannes Zschache and Tilman Hartwig\nApplication Lab for AI and Big Data, German Environment Agency,\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.", "metadata": {}}, {"text": "*Corresponding author(s).", "metadata": {}}, {"text": "E-mail(s): tilman.hartwig@uba.de;", "metadata": {}}, {"text": "Contributing authors: johannes.zschache@uba.de;", "metadata": {}}, {"text": "Abstract\nThe increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\nity.", "metadata": {}}, {"text": "While prior research has largely focused on energy consumption during\nmodel training, the inference phase has received comparatively less attention.", "metadata": {}}, {"text": "This study systematically evaluates the trade-offs between model accuracy and\nenergy consumption in text classification inference across various model archi-\ntectures and hardware configurations.", "metadata": {}}, {"text": "Our empirical analysis shows that the\nbest-performing model in terms of accuracy can also be energy-efficient, while\nlarger LLMs tend to consume significantly more energy with lower classifica-\ntion accuracy.", "metadata": {}}, {"text": "We observe substantial variability in inference energy consumption\n(<mWh to >kWh), influenced by model type, model size, and hardware spec-\nifications.", "metadata": {}}, {"text": "Additionally, we find a strong correlation between inference energy\nconsumption and model runtime, indicating that execution time can serve as\na practical proxy for energy usage in settings where direct measurement is not\nfeasible.", "metadata": {}}, {"text": "These findings have implications for sustainable AI development, provid-\ning actionable insights for researchers, industry practitioners, and policymakers\nseeking to balance performance and resource efficiency in NLP applications.", "metadata": {}}, {"text": "Keywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI\n1 Introduction\nArtificial intelligence (AI) systems, particularly large language models (LLMs), have\ndriven remarkable progress in Natural Language Processing (NLP) applications.", "metadata": {}}, {"text": "This\n1\narXiv:2508.14170v1  [cs.CL]  19 Aug 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "development has been enabled by the Transformer architecture (Vaswani et al., 2017)\nand exemplified by the emergence of large-scale models such as GPT-3 (Brown et al.,\n2020), which have significantly advanced task performance. However, this progress\nhas come at a cost: the escalating energy demands of AI systems pose significant\nenvironmental and computational challenges. Data centers that support AI com-\nputations are major electricity consumers, often dependent on fossil fuels, thereby\ncontributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al.,\n2025). This increasing energy demand challenges global climate objectives such as\nthe Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable\nDevelopment Goals (SDGs), specifically Goal 13 on climate action (United Nations,\n2015b). Consequently, designing energy-efficient AI systems is imperative for aligning\ntechnological advancements with sustainability goals. Moreover, beyond sustainabil-\nity, energy-efficient models offer additional advantages, including reduced hardware\nrequirements, lower financial costs, and faster inference times.\nWhen evaluating machine learning models, most studies concentrate on the quality\nof the model responses by tracking e.g. the accuracy, the RMSE, or other measures.\nAnd even if the energy consumption is taken into account, prior research has mainly\nfocused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni\nand Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in\nreal world deployments, has received comparatively less attention. However, energy\nefficiency during the operational phase is an increasingly relevant topic as LLM appli-\ncations become ubiquitous and LLM models are trained to use additional test-time\ncompute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this\ngap, we present a systematic study on the energy consumption of language models\nduring inference, providing actionable insights for balancing accuracy with efficiency.\nA particularly popular machine learning task is text categorization, a task that\nlightweight models have been shown to handle effectively. For instance, Joulin et al.\n(2017) show that a simple classifier built on word embeddings is often as accurate as\ndeep learning classifiers. Despite this, some authors argue for the use of pre-trained\nLLMs for text classification because it reduces the need for model training and sim-\nplifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials\npromote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023),\nfurther encouraging their use even when more efficient alternatives exist. In order to\njustify the usage of LLM in relatively simple tasks such as text categorization, we\nadvocate a consequent comparison of a model’s response quality to its energy efficiency.\nGiven a practical use case that is occurring in public administration, our study\nempirically analyzes trade-offs between model accuracy and energy consumption across\nvarious language models and hardware configurations. We find that the best perform-\ning model is energy efficient while LLMs show higher energy usage with lower accuracy.\nGenerally, we see significant variability in inference energy consumption, influenced\nby model type, model size, and hardware specifications. Additionally, the energy con-\nsumption during inference is shown to highly correlate with the model’s runtime. This\nmakes the duration of computations a valuable proxy measure for energy consump-\ntion in settings where the latter cannot be traced. Our findings have implications\nfor researchers, industry practitioners, and policymakers advocating for sustainable\n2", "sentences": [{"text": "development has been enabled by the Transformer architecture (Vaswani et al., 2017)\nand exemplified by the emergence of large-scale models such as GPT-3 (Brown et al.,\n2020), which have significantly advanced task performance.", "metadata": {}}, {"text": "However, this progress\nhas come at a cost: the escalating energy demands of AI systems pose significant\nenvironmental and computational challenges.", "metadata": {}}, {"text": "Data centers that support AI com-\nputations are major electricity consumers, often dependent on fossil fuels, thereby\ncontributing to greenhouse gas emissions (Lacoste et al., 2019;", "metadata": {}}, {"text": "Axenbeck et al.,\n2025).", "metadata": {}}, {"text": "This increasing energy demand challenges global climate objectives such as\nthe Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable\nDevelopment Goals (SDGs), specifically Goal 13 on climate action (United Nations,\n2015b).", "metadata": {}}, {"text": "Consequently, designing energy-efficient AI systems is imperative for aligning\ntechnological advancements with sustainability goals.", "metadata": {}}, {"text": "Moreover, beyond sustainabil-\nity, energy-efficient models offer additional advantages, including reduced hardware\nrequirements, lower financial costs, and faster inference times.", "metadata": {}}, {"text": "When evaluating machine learning models, most studies concentrate on the quality\nof the model responses by tracking e.g.", "metadata": {}}, {"text": "the accuracy, the RMSE, or other measures.", "metadata": {}}, {"text": "And even if the energy consumption is taken into account, prior research has mainly\nfocused on the training phase (Strubell et al., 2019;", "metadata": {}}, {"text": "Patterson et al., 2021;", "metadata": {}}, {"text": "Luccioni\nand Hernandez-Garcia, 2023).", "metadata": {}}, {"text": "The inference phase, which is repeatedly executed in\nreal world deployments, has received comparatively less attention.", "metadata": {}}, {"text": "However, energy\nefficiency during the operational phase is an increasingly relevant topic as LLM appli-\ncations become ubiquitous and LLM models are trained to use additional test-time\ncompute to improve performance (OpenAI, 2024;", "metadata": {}}, {"text": "DeepSeek-AI, 2025).", "metadata": {}}, {"text": "Addressing this\ngap, we present a systematic study on the energy consumption of language models\nduring inference, providing actionable insights for balancing accuracy with efficiency.", "metadata": {}}, {"text": "A particularly popular machine learning task is text categorization, a task that\nlightweight models have been shown to handle effectively.", "metadata": {}}, {"text": "For instance, Joulin et al.", "metadata": {}}, {"text": "(2017) show that a simple classifier built on word embeddings is often as accurate as\ndeep learning classifiers.", "metadata": {}}, {"text": "Despite this, some authors argue for the use of pre-trained\nLLMs for text classification because it reduces the need for model training and sim-\nplifies data preprocessing (Wang et al., 2024).", "metadata": {}}, {"text": "Additionally, popular software tutorials\npromote LLMs for classification tasks (LangChain Team, 2023;", "metadata": {}}, {"text": "Lamini Team, 2023),\nfurther encouraging their use even when more efficient alternatives exist.", "metadata": {}}, {"text": "In order to\njustify the usage of LLM in relatively simple tasks such as text categorization, we\nadvocate a consequent comparison of a model’s response quality to its energy efficiency.", "metadata": {}}, {"text": "Given a practical use case that is occurring in public administration, our study\nempirically analyzes trade-offs between model accuracy and energy consumption across\nvarious language models and hardware configurations.", "metadata": {}}, {"text": "We find that the best perform-\ning model is energy efficient while LLMs show higher energy usage with lower accuracy.", "metadata": {}}, {"text": "Generally, we see significant variability in inference energy consumption, influenced\nby model type, model size, and hardware specifications.", "metadata": {}}, {"text": "Additionally, the energy con-\nsumption during inference is shown to highly correlate with the model’s runtime.", "metadata": {}}, {"text": "This\nmakes the duration of computations a valuable proxy measure for energy consump-\ntion in settings where the latter cannot be traced.", "metadata": {}}, {"text": "Our findings have implications\nfor researchers, industry practitioners, and policymakers advocating for sustainable\n2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "AI development (Kaack et al.; Luccioni et al., 2025). By systematically evaluating\ninference efficiency and runtime across architectures and hardware settings, we con-\ntribute to the ongoing discourse on AI’s environmental impact and provide actionable\nguidelines for optimizing NLP applications for both performance and sustainability.\n2 Previous research\nResearch on the environmental impact of machine learning (ML) has primarily focused\non the energy consumption and carbon emissions produced during the training phase of\nlarge-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint\nof NLP models, revealing that the training of a single large-scale transformer model\ncan emit as much carbon as five cars over their entire lifetimes (their measurements\ninclude thousands of hyperparameter tuning jobs, which makes it difficult to disen-\ntangle model-inherent efficiency from experimental setup). This seminal work spurred\nfurther investigations into the environmental costs of training neural networks, includ-\ning large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia,\n2023; Patterson et al., 2022).\nWhile training remains a significant contributor to energy consumption, recent\nstudies have begun to focus on the inference phase. Samsi et al. (2023) highlighted\nthe substantial energy demands of LLM inference but did not explore the relationship\nbetween energy consumption and task-specific performance. Liu et al. (2022) under-\nscore the importance of evaluating NLP models not just on efficiency metrics but also\non accuracy by introducing the Efficient Language Understanding Evaluation (ELUE)\nbenchmark. ELUE aims to establish a Pareto frontier that balances performance and\nefficiency. It includes various language understanding tasks, facilitating fair and com-\nprehensive comparisons among models. However, the framework adopts number of\nparameters and FLOPs as the metrics for model efficiency, disregarding hardware\nspecific factors. Similarly, Chien et al. (2023) estimate the energy consumption associ-\nated with the inference phase of generative AI applications based on the output word\ncount and several assumptions about the application such as the number of FLOPS\nper inference and the sampling rate.\nIn contrast, we promote energy-efficient NLP models by the direct measurement\nof the power consumed during inference. Hence, our work follows the approach of the\nSustaiNLP 2020 shared task (Wang and Wolf, 2020). SustaiNLP demonstrated that\nsubstantial energy savings are achievable with minimal performance loss. While this\nstudy was limited to the performance of a couple of small language models on a single\nbenchmark, we extend these efforts to a greater number of partially very large models\ndeployed to a practical inference scenario.\nThis makes our study very similar to the one by Alizadeh et al. (2025), who inves-\ntigated the trade-offs between accuracy and energy consumption when deploying large\nlanguage models (LLMs) for software development tasks. Besides the finding that\nlarger LLMs with higher energy consumption do not always yield significantly bet-\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\nlayer size and transformer block count, directly correlate with energy usage.\n3", "sentences": [{"text": "AI development (Kaack et al.;", "metadata": {}}, {"text": "Luccioni et al., 2025).", "metadata": {}}, {"text": "By systematically evaluating\ninference efficiency and runtime across architectures and hardware settings, we con-\ntribute to the ongoing discourse on AI’s environmental impact and provide actionable\nguidelines for optimizing NLP applications for both performance and sustainability.", "metadata": {}}, {"text": "2 Previous research\nResearch on the environmental impact of machine learning (ML) has primarily focused\non the energy consumption and carbon emissions produced during the training phase of\nlarge-scale models.", "metadata": {}}, {"text": "Most famously, Strubell et al.", "metadata": {}}, {"text": "(2019) quantify the carbon footprint\nof NLP models, revealing that the training of a single large-scale transformer model\ncan emit as much carbon as five cars over their entire lifetimes (their measurements\ninclude thousands of hyperparameter tuning jobs, which makes it difficult to disen-\ntangle model-inherent efficiency from experimental setup).", "metadata": {}}, {"text": "This seminal work spurred\nfurther investigations into the environmental costs of training neural networks, includ-\ning large language models (Patterson et al., 2021;", "metadata": {}}, {"text": "Luccioni and Hernandez-Garcia,\n2023;", "metadata": {}}, {"text": "Patterson et al., 2022).", "metadata": {}}, {"text": "While training remains a significant contributor to energy consumption, recent\nstudies have begun to focus on the inference phase.", "metadata": {}}, {"text": "Samsi et al.", "metadata": {}}, {"text": "(2023) highlighted\nthe substantial energy demands of LLM inference but did not explore the relationship\nbetween energy consumption and task-specific performance.", "metadata": {}}, {"text": "Liu et al.", "metadata": {}}, {"text": "(2022) under-\nscore the importance of evaluating NLP models not just on efficiency metrics but also\non accuracy by introducing the Efficient Language Understanding Evaluation (ELUE)\nbenchmark.", "metadata": {}}, {"text": "ELUE aims to establish a Pareto frontier that balances performance and\nefficiency.", "metadata": {}}, {"text": "It includes various language understanding tasks, facilitating fair and com-\nprehensive comparisons among models.", "metadata": {}}, {"text": "However, the framework adopts number of\nparameters and FLOPs as the metrics for model efficiency, disregarding hardware\nspecific factors.", "metadata": {}}, {"text": "Similarly, Chien et al.", "metadata": {}}, {"text": "(2023) estimate the energy consumption associ-\nated with the inference phase of generative AI applications based on the output word\ncount and several assumptions about the application such as the number of FLOPS\nper inference and the sampling rate.", "metadata": {}}, {"text": "In contrast, we promote energy-efficient NLP models by the direct measurement\nof the power consumed during inference.", "metadata": {}}, {"text": "Hence, our work follows the approach of the\nSustaiNLP 2020 shared task (Wang and Wolf, 2020).", "metadata": {}}, {"text": "SustaiNLP demonstrated that\nsubstantial energy savings are achievable with minimal performance loss.", "metadata": {}}, {"text": "While this\nstudy was limited to the performance of a couple of small language models on a single\nbenchmark, we extend these efforts to a greater number of partially very large models\ndeployed to a practical inference scenario.", "metadata": {}}, {"text": "This makes our study very similar to the one by Alizadeh et al.", "metadata": {}}, {"text": "(2025), who inves-\ntigated the trade-offs between accuracy and energy consumption when deploying large\nlanguage models (LLMs) for software development tasks.", "metadata": {}}, {"text": "Besides the finding that\nlarger LLMs with higher energy consumption do not always yield significantly bet-\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\nlayer size and transformer block count, directly correlate with energy usage.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses\nof energy consumption during ML model inference. Their study systematically com-\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\nsmaller task-specific and larger multi-purpose models. They found that the larger\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\nespecially for tasks involving text and image generation. Furthermore, their research\nunderscores the variability in energy consumption across tasks and model architec-\ntures. The authors advocate for increased transparency and sustainable deployment\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\nAI systems must be carefully weighed against their utility.\n3 Data and methods\nOur experiments are inspired by an occasionally occurring use case in public admin-\nistration: the management of objections that are submitted by the population. Due\nto a potentially very large amount of submissions, an automatic preprocessing of the\nobjections is of high value. One of the possible steps of an automated workflow is to\ncategorize each submission for optimal forwarding to the responsible department.\nThe data of our study originates from the process of selecting a repository site\nfor high-level radioactive waste in Germany. During the first phase, sub-areas were\nidentified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The\nstatements from the population were categorized, processed and published as the\nFKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is\ngiven by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains\nthe category of the text.\nWe scraped the dataset from the website and restricted it to entries for which the\ntopic occurs at least 10 times. The remaining 378 entries were split into half: 189\nentries for training and 189 entries for testing. This unusual 50:50 split was done so\nthat the test set should be sufficiently representative by containing enough examples\nof each of the 14 categories. Each of the following experiments was repeated 10 times\nwith different train-test-splits. To increase comparability, every experiment was run\nwith the same 10 train-test-splits.\nAn experiment run consists of a training phase and a testing phase. Since large\nlanguage models have been argued to be applicable to text categorization without\ntraining (zero-shot), we omit the training phase for these models and apply LLMs\nwithout fine-tuning. We report the energy consumption and accuracy only for the test\nphase as averages over all runs.\n3.1 Traditional models\nBesides LLMs, we initially run the experiments with lightweight NLP models that we\ncall traditional because they have been used for categorization tasks long before LLMs\nexisted. Specifically, we use a linear model (logistic regression) and a gradient boosting\nalgorithm (xgboost). Logistic regression is a simple, interpretable model that estimates\nthe probability of a class based on a linear combination of input features. XGBoost\n4", "sentences": [{"text": "Finally, Luccioni et al.", "metadata": {}}, {"text": "(2024) provide one of the most comprehensive analyses\nof energy consumption during ML model inference.", "metadata": {}}, {"text": "Their study systematically com-\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\nsmaller task-specific and larger multi-purpose models.", "metadata": {}}, {"text": "They found that the larger\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\nespecially for tasks involving text and image generation.", "metadata": {}}, {"text": "Furthermore, their research\nunderscores the variability in energy consumption across tasks and model architec-\ntures.", "metadata": {}}, {"text": "The authors advocate for increased transparency and sustainable deployment\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\nAI systems must be carefully weighed against their utility.", "metadata": {}}, {"text": "3 Data and methods\nOur experiments are inspired by an occasionally occurring use case in public admin-\nistration: the management of objections that are submitted by the population.", "metadata": {}}, {"text": "Due\nto a potentially very large amount of submissions, an automatic preprocessing of the\nobjections is of high value.", "metadata": {}}, {"text": "One of the possible steps of an automated workflow is to\ncategorize each submission for optimal forwarding to the responsible department.", "metadata": {}}, {"text": "The data of our study originates from the process of selecting a repository site\nfor high-level radioactive waste in Germany.", "metadata": {}}, {"text": "During the first phase, sub-areas were\nidentified and discussed in a process called FKTG (Fachkonferenz Teilgebiete).", "metadata": {}}, {"text": "The\nstatements from the population were categorized, processed and published as the\nFKTG-dataset (https://beteiligung.bge.de/index.php).", "metadata": {}}, {"text": "The text of the submission is\ngiven by the column ‘Beitrag’ (input).", "metadata": {}}, {"text": "The column ‘Themenkomplex’ (topic) contains\nthe category of the text.", "metadata": {}}, {"text": "We scraped the dataset from the website and restricted it to entries for which the\ntopic occurs at least 10 times.", "metadata": {}}, {"text": "The remaining 378 entries were split into half: 189\nentries for training and 189 entries for testing.", "metadata": {}}, {"text": "This unusual 50:50 split was done so\nthat the test set should be sufficiently representative by containing enough examples\nof each of the 14 categories.", "metadata": {}}, {"text": "Each of the following experiments was repeated 10 times\nwith different train-test-splits.", "metadata": {}}, {"text": "To increase comparability, every experiment was run\nwith the same 10 train-test-splits.", "metadata": {}}, {"text": "An experiment run consists of a training phase and a testing phase.", "metadata": {}}, {"text": "Since large\nlanguage models have been argued to be applicable to text categorization without\ntraining (zero-shot), we omit the training phase for these models and apply LLMs\nwithout fine-tuning.", "metadata": {}}, {"text": "We report the energy consumption and accuracy only for the test\nphase as averages over all runs.", "metadata": {}}, {"text": "3.1 Traditional models\nBesides LLMs, we initially run the experiments with lightweight NLP models that we\ncall traditional because they have been used for categorization tasks long before LLMs\nexisted.", "metadata": {}}, {"text": "Specifically, we use a linear model (logistic regression) and a gradient boosting\nalgorithm (xgboost).", "metadata": {}}, {"text": "Logistic regression is a simple, interpretable model that estimates\nthe probability of a class based on a linear combination of input features.", "metadata": {}}, {"text": "XGBoost\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "(Extreme Gradient Boosting) is an efficient, scalable machine-learning algorithm that\ncombines predictions from multiple decision trees to improve accuracy.\nWe consider three different types of features: bag-of-words (BoW), term frequency-\ninverse document frequency (TF-IDF), and a pretrained multilingual sentence\nembedding. BoW represents text by counting word occurrences without consider-\ning order, while TF-IDF adjusts word counts by their importance across documents,\ncapturing rare but informative terms. The TF-IDF features are calculated on all\n2-gram and 3-gram character sequences, which capture local patterns in the text.\nThe multilingual sentence embedding (https://huggingface.co/sentence-transformers/\nparaphrase-multilingual-mpnet-base-v2) provides dense vector representations of text,\npreserving semantic meaning across languages. This embedding is not fine-tuned on\nthe training data. Both models are trained using the default parameters provided by\nsklearn.linear model.LogisticRegression and xgboost.XGBClassifier.\n3.2 Large language models\nLarge language models (LLMs) were applied without training (zero-shot) using the\ntest set only. Table 1 gives the names and sources of the models used. The LLMs were\nselected by the following criteria:\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architec-\nture that includes next to Transformer also Mamba layers (a state-space model). The\nDeepseek distillations (DS) were added to include models with reasoning capabilities\n(test-time compute).\nModel Link\nLlama 3.1 8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B https://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B https://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini https://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5 https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nT able 1 Selection of large language models\n3.3 Computing Resources\nWe used different computing systems for a comparative analysis of energy efficiency\nacross diverse hardware architectures. This enables the assessment of how architectural\n5", "sentences": [{"text": "(Extreme Gradient Boosting) is an efficient, scalable machine-learning algorithm that\ncombines predictions from multiple decision trees to improve accuracy.", "metadata": {}}, {"text": "We consider three different types of features: bag-of-words (BoW), term frequency-\ninverse document frequency (TF-IDF), and a pretrained multilingual sentence\nembedding.", "metadata": {}}, {"text": "BoW represents text by counting word occurrences without consider-\ning order, while TF-IDF adjusts word counts by their importance across documents,\ncapturing rare but informative terms.", "metadata": {}}, {"text": "The TF-IDF features are calculated on all\n2-gram and 3-gram character sequences, which capture local patterns in the text.", "metadata": {}}, {"text": "The multilingual sentence embedding (https://huggingface.co/sentence-transformers/\nparaphrase-multilingual-mpnet-base-v2) provides dense vector representations of text,\npreserving semantic meaning across languages.", "metadata": {}}, {"text": "This embedding is not fine-tuned on\nthe training data.", "metadata": {}}, {"text": "Both models are trained using the default parameters provided by\nsklearn.linear model.LogisticRegression and xgboost.XGBClassifier.", "metadata": {}}, {"text": "3.2 Large language models\nLarge language models (LLMs) were applied without training (zero-shot) using the\ntest set only.", "metadata": {}}, {"text": "Table 1 gives the names and sources of the models used.", "metadata": {}}, {"text": "The LLMs were\nselected by the following criteria:\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architec-\nture that includes next to Transformer also Mamba layers (a state-space model).", "metadata": {}}, {"text": "The\nDeepseek distillations (DS) were added to include models with reasoning capabilities\n(test-time compute).", "metadata": {}}, {"text": "Model Link\nLlama 3.1 8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B https://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B https://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini https://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5 https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nT able 1 Selection of large language models\n3.3 Computing Resources\nWe used different computing systems for a comparative analysis of energy efficiency\nacross diverse hardware architectures.", "metadata": {}}, {"text": "This enables the assessment of how architectural\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "differences - especially GPU tensor core capabilities - affect the inference speed and\npower usage. A diversity in computational infrastructure is crucial for generalizing\nfindings across different environments and ensuring the validity and replicability of\nexperimental results in machine learning research. Furthermore, insights gained from\nusing multiple platforms contribute to optimizing resource allocation strategies and\nimproving cost-effectiveness in large-scale machine learning projects.\nTo run our experiments, we were granted access to the high-performance computing\n(HPC) systems of TUD Dresden University of Technology (https://doc.zih.tu-dresden.\nde/) and Leipzig University (https://www.sc.uni-leipzig.de/). For GPU-accelerated\ncomputing, three different systems are available named Capella, Paula, and Clara\n(see Table 2). The main difference for our study is the GPU: while a node on the\nCapella cluster is equipped with 4 x H100, there are 8 x A30 on each node on Paula\nand 4 x V100 on Clara. This means that a large model such as Llama 3.1 70B or\nQwen 2.5 72B fits on a single node of Capella (requiring 2 GPUs) or Paula (requiring\nall 8 GPUs) but takes up two nodes of the Clara cluster (assuming a 16-bit floating\npoint representation of the parameters).\nCluster Capella Paula Clara\nHPC center TUD Dresden\nUniversity of Technology Leipzig University Leipzig University\nnumber of nodes 144 12 6\nCPU per node 2 x AMD (32 cores)\n2.7GHz\n2 x AMD (64 cores)\n2.0GHz\n1 x AMD (32 cores)\n2.0GHz\nRAM per node 768 GB 1 TB 512 GB\nGPU per node 4 x NVIDIA H100\n(94GB)\n8 x NVIDIA A30\n(24 GB)\n4 x NVIDIA V100\n(32GB)\nsingle GPU max\npower consumption 700W 165W 250W\nT able 2 HPC Resources\nLLMs were deployed using the vllm library (https://github.com/vllm-project/\nvllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.\nIf a model is too large to be deployed on a single GPU, the model weights are dis-\ntributed over multiple GPUs, which allow for a parallel computation of the activations\n(c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two\ncomputing nodes are needed, the model is split into two parts and executed sequen-\ntially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model\npart on the first node and then the model part on the second node.\nThe energy consumption and the runtime of the inference phase were measured\nby the CodeCarbon package (https://github.com/mlco2/codecarbon). This package\nuses the NVIDIA Management Library (NVML) and the Intel RAPL files to track\nthe power usage of GPU and CPU (https://mlco2.github.io/codecarbon/methodology.\nhtml#power-usage). The power consumption of the memory is flatly added with\n0.375W/GB of memory used. In settings where the model is deployed on more than\none node, the inference duration is taken as the maximum and the energy as the sum\nover all nodes.\n6", "sentences": [{"text": "differences - especially GPU tensor core capabilities - affect the inference speed and\npower usage.", "metadata": {}}, {"text": "A diversity in computational infrastructure is crucial for generalizing\nfindings across different environments and ensuring the validity and replicability of\nexperimental results in machine learning research.", "metadata": {}}, {"text": "Furthermore, insights gained from\nusing multiple platforms contribute to optimizing resource allocation strategies and\nimproving cost-effectiveness in large-scale machine learning projects.", "metadata": {}}, {"text": "To run our experiments, we were granted access to the high-performance computing\n(HPC) systems of TUD Dresden University of Technology (https://doc.zih.tu-dresden.", "metadata": {}}, {"text": "de/) and Leipzig University (https://www.sc.uni-leipzig.de/).", "metadata": {}}, {"text": "For GPU-accelerated\ncomputing, three different systems are available named Capella, Paula, and Clara\n(see Table 2).", "metadata": {}}, {"text": "The main difference for our study is the GPU: while a node on the\nCapella cluster is equipped with 4 x H100, there are 8 x A30 on each node on Paula\nand 4 x V100 on Clara.", "metadata": {}}, {"text": "This means that a large model such as Llama 3.1 70B or\nQwen 2.5 72B fits on a single node of Capella (requiring 2 GPUs) or Paula (requiring\nall 8 GPUs) but takes up two nodes of the Clara cluster (assuming a 16-bit floating\npoint representation of the parameters).", "metadata": {}}, {"text": "Cluster Capella Paula Clara\nHPC center TUD Dresden\nUniversity of Technology Leipzig University Leipzig University\nnumber of nodes 144 12 6\nCPU per node 2 x AMD (32 cores)\n2.7GHz\n2 x AMD (64 cores)\n2.0GHz\n1 x AMD (32 cores)\n2.0GHz\nRAM per node 768 GB 1 TB 512 GB\nGPU per node 4 x NVIDIA H100\n(94GB)\n8 x NVIDIA A30\n(24 GB)\n4 x NVIDIA V100\n(32GB)\nsingle GPU max\npower consumption 700W 165W 250W\nT able 2 HPC Resources\nLLMs were deployed using the vllm library (https://github.com/vllm-project/\nvllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.", "metadata": {}}, {"text": "If a model is too large to be deployed on a single GPU, the model weights are dis-\ntributed over multiple GPUs, which allow for a parallel computation of the activations\n(c.f.", "metadata": {}}, {"text": "tensor model parallelism (TMP) in Bai et al., 2024, pp.16).", "metadata": {}}, {"text": "In cases where two\ncomputing nodes are needed, the model is split into two parts and executed sequen-\ntially (c.f.", "metadata": {}}, {"text": "pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model\npart on the first node and then the model part on the second node.", "metadata": {}}, {"text": "The energy consumption and the runtime of the inference phase were measured\nby the CodeCarbon package (https://github.com/mlco2/codecarbon).", "metadata": {}}, {"text": "This package\nuses the NVIDIA Management Library (NVML) and the Intel RAPL files to track\nthe power usage of GPU and CPU (https://mlco2.github.io/codecarbon/methodology.", "metadata": {}}, {"text": "html#power-usage).", "metadata": {}}, {"text": "The power consumption of the memory is flatly added with\n0.375W/GB of memory used.", "metadata": {}}, {"text": "In settings where the model is deployed on more than\none node, the inference duration is taken as the maximum and the energy as the sum\nover all nodes.", "metadata": {}}, {"text": "6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Various software tools have been created to monitor energy consump-\ntion during the application of machine learning models (https://github.com/\ntiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%\nB8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al.,\n2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy con-\nsumption by monitoring hardware usage. In some settings, CodeCarbon is considered\nmore accurate, yielding values closer to those obtained via physical wattmeters\n(Bouza et al., 2023). Comparing different tools of energy monitoring is beyond the\nscope of our paper.\n4 Results\nFor each model, we report accuracy, energy consumption, and inference duration.\nThe energy consumption and duration were measured only for the inference step, i.e.,\nafter the model and data were already loaded. One inference run involves classifying\n189 text samples from a test set. All tables and figures present the average results\nover 10 runs on different test sets, with the same 10 test sets used for each model.\nMeasurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both\nenergy consumption and duration (logarithmically scaled to base 10).\nFigure 1 illustrates the trade-off between energy consumption and accuracy across\nall models. For these experiments, a single node of the Capella system was used. The\nminimum number of H100 GPUs required varies by model (see Table B1).\nThe highest accuracy was achieved by a traditional linear model using pre-trained\nsentence embeddings. Notably, even the most energy-efficient model - a linear model\nwith TF-IDF features - outperformed several large language models (LLMs). Among\nLLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes\nseven times less energy than the most accurate model (Qwen 2.5 72B), with only\na minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive\nreasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs\nwhile consuming significantly more energy and taking longer to complete inference.\n4.1 Analysis of hardware settings\nThis section analyzes the impact of different hardware configurations (see Tab. 2) on\nenergy consumption. We focus on GPU usage due to its dominant role in machine\nlearning inference.\nAs shown in Figure 2, GPU consumption accounts for the largest share of total\nenergy usage in all experiments. The only exceptions are traditional models without\nembeddings, which do not use the GPU during inference.\n4.1.1 V arying the Number of GPUs\nWe examined the effect of varying the number of GPUs on energy consumption and\ninference duration. Most LLMs were tested on 1, 2, or 4 GPUs on a single Capella\nsystem node. Larger models (Qwen 72B, Phi MoE, Llama 70B, Jamba Mini, and DS\nLlama 70B) required either 2 or 4 GPUs. Increasing the number of GPUs consistently\n7", "sentences": [{"text": "Various software tools have been created to monitor energy consump-\ntion during the application of machine learning models (https://github.com/\ntiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%\nB8%8F-energy-metrics).", "metadata": {}}, {"text": "Similar to CodeCarbon, Carbontracker (Anthony et al.,\n2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy con-\nsumption by monitoring hardware usage.", "metadata": {}}, {"text": "In some settings, CodeCarbon is considered\nmore accurate, yielding values closer to those obtained via physical wattmeters\n(Bouza et al., 2023).", "metadata": {}}, {"text": "Comparing different tools of energy monitoring is beyond the\nscope of our paper.", "metadata": {}}, {"text": "4 Results\nFor each model, we report accuracy, energy consumption, and inference duration.", "metadata": {}}, {"text": "The energy consumption and duration were measured only for the inference step, i.e.,\nafter the model and data were already loaded.", "metadata": {}}, {"text": "One inference run involves classifying\n189 text samples from a test set.", "metadata": {}}, {"text": "All tables and figures present the average results\nover 10 runs on different test sets, with the same 10 test sets used for each model.", "metadata": {}}, {"text": "Measurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both\nenergy consumption and duration (logarithmically scaled to base 10).", "metadata": {}}, {"text": "Figure 1 illustrates the trade-off between energy consumption and accuracy across\nall models.", "metadata": {}}, {"text": "For these experiments, a single node of the Capella system was used.", "metadata": {}}, {"text": "The\nminimum number of H100 GPUs required varies by model (see Table B1).", "metadata": {}}, {"text": "The highest accuracy was achieved by a traditional linear model using pre-trained\nsentence embeddings.", "metadata": {}}, {"text": "Notably, even the most energy-efficient model - a linear model\nwith TF-IDF features - outperformed several large language models (LLMs).", "metadata": {}}, {"text": "Among\nLLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes\nseven times less energy than the most accurate model (Qwen 2.5 72B), with only\na minor accuracy reduction of 0.07 points.", "metadata": {}}, {"text": "Deepseek models, despite their extensive\nreasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs\nwhile consuming significantly more energy and taking longer to complete inference.", "metadata": {}}, {"text": "4.1 Analysis of hardware settings\nThis section analyzes the impact of different hardware configurations (see Tab.", "metadata": {}}, {"text": "2) on\nenergy consumption.", "metadata": {}}, {"text": "We focus on GPU usage due to its dominant role in machine\nlearning inference.", "metadata": {}}, {"text": "As shown in Figure 2, GPU consumption accounts for the largest share of total\nenergy usage in all experiments.", "metadata": {}}, {"text": "The only exceptions are traditional models without\nembeddings, which do not use the GPU during inference.", "metadata": {}}, {"text": "4.1.1 V arying the Number of GPUs\nWe examined the effect of varying the number of GPUs on energy consumption and\ninference duration.", "metadata": {}}, {"text": "Most LLMs were tested on 1, 2, or 4 GPUs on a single Capella\nsystem node.", "metadata": {}}, {"text": "Larger models (Qwen 72B, Phi MoE, Llama 70B, Jamba Mini, and DS\nLlama 70B) required either 2 or 4 GPUs.", "metadata": {}}, {"text": "Increasing the number of GPUs consistently\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "Fig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single\nnode). The energy consumption for the same task spans over six orders of magnitude with traditional\nmodels being the most energy-efficient models and reasoning models are most energy-consuming. The\nbest model for this specific task is a traditional model (Linear Embedding) with moderate energy\nconsumption.\nreduced inference duration but did not reduce energy consumption. In some cases,\nenergy consumption increased due to the additional GPUs in operation (see Figure 3).\n4.1.2 V arying the Number of Nodes\nWhile large models can often be executed on a single computing node, certain hardware\nlimitations or shared high-performance computing (HPC) environments may necessi-\ntate using multiple nodes. In shared systems, it is often easier to access two nodes\nwith half the number of available GPUs than a single node with all its GPUs, due\nto scheduling constraints and resource allocation policies. However, deploying mod-\nels across multiple nodes increases network communication overhead and significantly\nraises energy consumption.\nWe evaluated this effect for the largest models on theCapella system by comparing\na ‘single-node’ configuration (2 GPUs on one node) with a ‘double-node’ configuration\n(1 GPU on each of two nodes). For the double-node configuration, energy consumption\nwas summed across both nodes and averaged over 10 runs, while the reported duration\nreflects the average of the maximum value between the two nodes.\nAs shown in Figure 4, using two nodes increased energy consumption by a factor\nthat depends on the model (see also Table B2). This increase stems from the overhead\n8", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single\nnode).", "metadata": {}}, {"text": "The energy consumption for the same task spans over six orders of magnitude with traditional\nmodels being the most energy-efficient models and reasoning models are most energy-consuming.", "metadata": {}}, {"text": "The\nbest model for this specific task is a traditional model (Linear Embedding) with moderate energy\nconsumption.", "metadata": {}}, {"text": "reduced inference duration but did not reduce energy consumption.", "metadata": {}}, {"text": "In some cases,\nenergy consumption increased due to the additional GPUs in operation (see Figure 3).", "metadata": {}}, {"text": "4.1.2 V arying the Number of Nodes\nWhile large models can often be executed on a single computing node, certain hardware\nlimitations or shared high-performance computing (HPC) environments may necessi-\ntate using multiple nodes.", "metadata": {}}, {"text": "In shared systems, it is often easier to access two nodes\nwith half the number of available GPUs than a single node with all its GPUs, due\nto scheduling constraints and resource allocation policies.", "metadata": {}}, {"text": "However, deploying mod-\nels across multiple nodes increases network communication overhead and significantly\nraises energy consumption.", "metadata": {}}, {"text": "We evaluated this effect for the largest models on theCapella system by comparing\na ‘single-node’ configuration (2 GPUs on one node) with a ‘double-node’ configuration\n(1 GPU on each of two nodes).", "metadata": {}}, {"text": "For the double-node configuration, energy consumption\nwas summed across both nodes and averaged over 10 runs, while the reported duration\nreflects the average of the maximum value between the two nodes.", "metadata": {}}, {"text": "As shown in Figure 4, using two nodes increased energy consumption by a factor\nthat depends on the model (see also Table B2).", "metadata": {}}, {"text": "This increase stems from the overhead\n8", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im1.png] Size: 1152x768, Data: 77506 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im1.png] Size: 1152x768, Data: 77506 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im1.png", "image_width": 1152, "image_height": 768, "attachment_type": "image", "has_image_data": true, "image_data_size": 77506}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Fig. 2 Energy consumption of all models for the inference task on the Capella system (single node)\nFig. 3 Effects of the number of GPUs on the runtime and consumed energy ( Capella, single node).\nDeepseek models are not shown.\nof coordinating across nodes. Inference duration also increased by the same factor\ndue to the sequential execution of model components and the required inter-node\ncommunication.\n4.1.3 Comparing GPU Architectures\nFinally, we compared the energy efficiency of different GPU architectures (see Figure\n5 and Table B3). Interestingly, the expected efficiency gains from using the more pow-\nerful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models.\nThis discrepancy is likely to arise because Deepseek models engage in extended rea-\nsoning by generating a larger output of words before making a classification decision.\nConsequently, the efficiency of H100 GPUs becomes evident only when substantial\n9", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "2 Energy consumption of all models for the inference task on the Capella system (single node)\nFig.", "metadata": {}}, {"text": "3 Effects of the number of GPUs on the runtime and consumed energy ( Capella, single node).", "metadata": {}}, {"text": "Deepseek models are not shown.", "metadata": {}}, {"text": "of coordinating across nodes.", "metadata": {}}, {"text": "Inference duration also increased by the same factor\ndue to the sequential execution of model components and the required inter-node\ncommunication.", "metadata": {}}, {"text": "4.1.3 Comparing GPU Architectures\nFinally, we compared the energy efficiency of different GPU architectures (see Figure\n5 and Table B3).", "metadata": {}}, {"text": "Interestingly, the expected efficiency gains from using the more pow-\nerful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models.", "metadata": {}}, {"text": "This discrepancy is likely to arise because Deepseek models engage in extended rea-\nsoning by generating a larger output of words before making a classification decision.", "metadata": {}}, {"text": "Consequently, the efficiency of H100 GPUs becomes evident only when substantial\n9", "metadata": {}}], "metadata": {"page": 9}}, {"text": "[Image page=9 idx=1 name=Im2.png] Size: 1344x576, Data: 61871 bytes", "sentences": [{"text": "[Image page=9 idx=1 name=Im2.png] Size: 1344x576, Data: 61871 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 1, "image_name": "Im2.png", "image_width": 1344, "image_height": 576, "attachment_type": "image", "has_image_data": true, "image_data_size": 61871}}, {"text": "[Image page=9 idx=2 name=Im3.png] Size: 1152x576, Data: 36496 bytes", "sentences": [{"text": "[Image page=9 idx=2 name=Im3.png] Size: 1152x576, Data: 36496 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 2, "image_name": "Im3.png", "image_width": 1152, "image_height": 576, "attachment_type": "image", "has_image_data": true, "image_data_size": 36496}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "Fig. 4 Comparison single node vs. double node deployment ( Capella).\ntext is generated. For models generating a single token per inference, a V100 or even\na A30 GPU is more efficient in inference.\nFig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment.\n4.2 Linear relationship between duration and energy\nIn most of the tables in appendix B, we report both the duration of each inference\nrun and its corresponding energy consumption. Since energy is the integral of power\nover time, these two measures exhibit a strong correlation. If the power is constant\nover time, this correlation should be linear. Figure 6 illustrates this relationship for\n10", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "4 Comparison single node vs.", "metadata": {}}, {"text": "double node deployment ( Capella).", "metadata": {}}, {"text": "text is generated.", "metadata": {}}, {"text": "For models generating a single token per inference, a V100 or even\na A30 GPU is more efficient in inference.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "5 Comparison of different GPU cards: four exemplary LLMs.", "metadata": {}}, {"text": "Single node deployment.", "metadata": {}}, {"text": "4.2 Linear relationship between duration and energy\nIn most of the tables in appendix B, we report both the duration of each inference\nrun and its corresponding energy consumption.", "metadata": {}}, {"text": "Since energy is the integral of power\nover time, these two measures exhibit a strong correlation.", "metadata": {}}, {"text": "If the power is constant\nover time, this correlation should be linear.", "metadata": {}}, {"text": "Figure 6 illustrates this relationship for\n10", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=Im4.png] Size: 1152x576, Data: 29545 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=Im4.png] Size: 1152x576, Data: 29545 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "Im4.png", "image_width": 1152, "image_height": 576, "attachment_type": "image", "has_image_data": true, "image_data_size": 29545}}, {"text": "[Image page=10 idx=2 name=Im5.png] Size: 1344x768, Data: 51427 bytes", "sentences": [{"text": "[Image page=10 idx=2 name=Im5.png] Size: 1344x768, Data: 51427 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 2, "image_name": "Im5.png", "image_width": 1344, "image_height": 768, "attachment_type": "image", "has_image_data": true, "image_data_size": 51427}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "all experiments conducted on a single node of the Capella cluster. When controlling\nfor the number of GPUs used for model deployment, the relation between duration\nand energy is approximately linear. Therefore, the duration appears to serve as a good\nproxy for the energy consumed.\nFig. 6 Plotting the relationship between duration and energy consumption (single node onCapella).\nThe lines are added by running a linear regression model.\nTo further quantify the relationship between duration and energy consumption,\nwe performed a linear regression analysis for each hardware configuration (see Table\n3). This analysis includes all experiments, regardless of the number of nodes used\nfor model deployment. The consistently high R 2 values across all configurations indi-\ncate that, for a given hardware setup, duration and energy consumption are nearly\ninterchangeable as measures of computational effort.\nMoreover, when the regression coefficients are known for a specific computing sys-\ntem, energy consumption can be reliably estimated from the duration and the number\nof GPUs. Only the coefficients of duration ( a) and of the interaction term dura-\ntion:GPUs (c) are statistically significant. The other coefficients (b and d) are omitted\nfrom the approximation:\nEnergy ≈ (a + c · GPUs) · Duration. (1)\nFor instance, on the Capella system, the following approximation holds for any\ncomputation:\n11", "sentences": [{"text": "all experiments conducted on a single node of the Capella cluster.", "metadata": {}}, {"text": "When controlling\nfor the number of GPUs used for model deployment, the relation between duration\nand energy is approximately linear.", "metadata": {}}, {"text": "Therefore, the duration appears to serve as a good\nproxy for the energy consumed.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "6 Plotting the relationship between duration and energy consumption (single node onCapella).", "metadata": {}}, {"text": "The lines are added by running a linear regression model.", "metadata": {}}, {"text": "To further quantify the relationship between duration and energy consumption,\nwe performed a linear regression analysis for each hardware configuration (see Table\n3).", "metadata": {}}, {"text": "This analysis includes all experiments, regardless of the number of nodes used\nfor model deployment.", "metadata": {}}, {"text": "The consistently high R 2 values across all configurations indi-\ncate that, for a given hardware setup, duration and energy consumption are nearly\ninterchangeable as measures of computational effort.", "metadata": {}}, {"text": "Moreover, when the regression coefficients are known for a specific computing sys-\ntem, energy consumption can be reliably estimated from the duration and the number\nof GPUs.", "metadata": {}}, {"text": "Only the coefficients of duration ( a) and of the interaction term dura-\ntion:GPUs (c) are statistically significant.", "metadata": {}}, {"text": "The other coefficients (b and d) are omitted\nfrom the approximation:\nEnergy ≈ (a + c · GPUs) · Duration.", "metadata": {}}, {"text": "(1)\nFor instance, on the Capella system, the following approximation holds for any\ncomputation:\n11", "metadata": {}}], "metadata": {"page": 11}}, {"text": "[Image page=11 idx=1 name=Im6.png] Size: 960x768, Data: 60337 bytes", "sentences": [{"text": "[Image page=11 idx=1 name=Im6.png] Size: 960x768, Data: 60337 bytes", "metadata": {}}], "metadata": {"page": 11, "image_index": 1, "image_name": "Im6.png", "image_width": 960, "image_height": 768, "attachment_type": "image", "has_image_data": true, "image_data_size": 60337}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "Energy\n1 Wh ≈ (0.1 + 0.09 · GPUs) · Duration\n1 s . (2)\nThis relationship suggests that, under fixed hardware conditions, monitoring the\nduration of computations provides an efficient means of estimating energy usage with\nminimal additional measurement overhead.\nDependent variable: Energy\nCapella Clara Paula\n(1) (2) (3)\nDuration (a) 0.097 ∗∗∗ 0.061∗∗∗ 0.079∗∗∗\n(0.008) (0.002) (0.026)\nGPUs (b) −0.500 0.048 −2.195\n(2.297) (0.339) (3.472)\nDuration:GPUs (c) 0.090 ∗∗∗ 0.036∗∗∗ 0.054∗∗∗\n(0.004) (0.0002) (0.004)\nConstant (d) −6.205 −0.826 3.328\n(5.725) (1.368) (17.220)\nObservations 44 19 23\nR2 0.998 1.000 0.989\nAdjusted R2 0.998 1.000 0.987\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nT able 3 Linear regression of energy consumption on\nduration (table format by Hlavac, 2022). The numbers\n(coefficients) give the estimated effects of each predictor on\nthe dependent variable. A positive coefficient means the\nvariable increases the outcome, while a negative coefficient\nmeans it decreases the outcome. The standard error (in\nparenthesis) estimates the variability of the coefficient\nestimate. The p-value (given by the asterisks) indicates\nwhether the predictor is statistically significant (different\nfrom zero).\n5 Discussion\nWe would like to mention the limitations of our study, which also point to the areas\nof future research. First, while traditional models were trained on approximately 200\nexamples, the large language models (LLMs) were applied in a zero-shot setting,\nmeaning they had no access to labeled examples. Previous research has shown that\nfew-shot prompting - where representative examples are included in the prompt - can\nimprove performance (Brown et al., 2020). For the present study, we kept the prompt\n12", "sentences": [{"text": "Energy\n1 Wh ≈ (0.1 + 0.09 · GPUs) · Duration\n1 s .", "metadata": {}}, {"text": "(2)\nThis relationship suggests that, under fixed hardware conditions, monitoring the\nduration of computations provides an efficient means of estimating energy usage with\nminimal additional measurement overhead.", "metadata": {}}, {"text": "Dependent variable: Energy\nCapella Clara Paula\n(1) (2) (3)\nDuration (a) 0.097 ∗∗∗ 0.061∗∗∗ 0.079∗∗∗\n(0.008) (0.002) (0.026)\nGPUs (b) −0.500 0.048 −2.195\n(2.297) (0.339) (3.472)\nDuration:GPUs (c) 0.090 ∗∗∗ 0.036∗∗∗ 0.054∗∗∗\n(0.004) (0.0002) (0.004)\nConstant (d) −6.205 −0.826 3.328\n(5.725) (1.368) (17.220)\nObservations 44 19 23\nR2 0.998 1.000 0.989\nAdjusted R2 0.998 1.000 0.987\nNote: ∗p<0.1;", "metadata": {}}, {"text": "∗∗p<0.05;", "metadata": {}}, {"text": "∗∗∗p<0.01\nT able 3 Linear regression of energy consumption on\nduration (table format by Hlavac, 2022).", "metadata": {}}, {"text": "The numbers\n(coefficients) give the estimated effects of each predictor on\nthe dependent variable.", "metadata": {}}, {"text": "A positive coefficient means the\nvariable increases the outcome, while a negative coefficient\nmeans it decreases the outcome.", "metadata": {}}, {"text": "The standard error (in\nparenthesis) estimates the variability of the coefficient\nestimate.", "metadata": {}}, {"text": "The p-value (given by the asterisks) indicates\nwhether the predictor is statistically significant (different\nfrom zero).", "metadata": {}}, {"text": "5 Discussion\nWe would like to mention the limitations of our study, which also point to the areas\nof future research.", "metadata": {}}, {"text": "First, while traditional models were trained on approximately 200\nexamples, the large language models (LLMs) were applied in a zero-shot setting,\nmeaning they had no access to labeled examples.", "metadata": {}}, {"text": "Previous research has shown that\nfew-shot prompting - where representative examples are included in the prompt - can\nimprove performance (Brown et al., 2020).", "metadata": {}}, {"text": "For the present study, we kept the prompt\n12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "as simple as possible (see section A). But in an actual application, we would add back-\nground information about the data and the categories. In general, prompt engineering,\nthe addition of representative examples to the prompt, or even fine-tuning an LLM\ncould yield higher accuracy rates. On the other hand, energy efficiency in LLMs can\nbe improved through model quantization, which reduces computational demands by\ncompressing model parameters (Jacob et al., 2017).\nSecond, we do not account for the energy costs associated with training the tradi-\ntional models because it is infeasible to compare them to the training costs of LLMs.\nThe LLMs used in this study were pre-trained by external organizations and made\npublicly available. As a result, the energy costs of training are distributed among all\nusers, making it difficult to estimate per-user energy consumption. Even if training\nenergy costs for an LLM were known, the number of users remains uncertain. Addi-\ntionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also\ncontribute to energy consumption. Deploying an LLM on a dedicated server (e.g.,\nusing vLLM) requires setup time and additional energy. Beyond inference, significant\ntime and computational resources are also required for development tasks, including\ndata processing, testing different models and prompts, parameter tuning, and debug-\nging - workloads that apply to both traditional models and LLMs. The measurement\nof additional related energy consumptions (such as network traffic or disk storage) is\nbeyond the scope of this paper.\nThird, energy consumption was measured using CodeCarbon, a tool recognized\nfor providing reliable estimates of a machine’s total energy use (Bouza et al., 2023).\nHowever, it does not allow for precise measurement of energy consumption at the\nlevel of individual processes. Moreover, power intake was recorded at 15-second\nintervals, meaning the accuracy of energy estimates improves with longer-running\nprocesses. Another limitation of CodeCarbon is that RAM energy consumption is\napproximated at 0.375W per GB of memory used. While the Running Average\nPower Limit (RAPL) framework can directly measure RAM power consumption, it\nis not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#\nissuecomment-2589805160). Additionally, in shared computing environments such as\nhigh-performance computing (HPC) clusters, measurements may be affected by other\nusers’ activities. Especially when an LLM was deployed across multiple nodes, varia-\ntions in network traffic at different times may have influenced energy measurements. A\nmore precise assessment of energy efficiency would benefit from using dedicated com-\nputing resources with physical wattmeters and high-resolution energy measurement\ntools(e.g. Ilsche et al., 2019).\nIn the following, we assess further limitations of the present study in more detail.\nMore specifically, we address our focus on a single dataset in section 5.1 and the\nlimitation to the text categorisation task in section 5.2. Subsequently, we contextualise\nour work in the broader context of planet-centered LLMs (section 5.3).\n5.1 Analysis on other datasets\nOur analysis was conducted on a highly specialized dataset. To assess the generaliz-\nability of our findings, we replicated the experiments using four additional, widely used\ndatasets (see table 4). These datasets were selected from the HuggingFace platform\n13", "sentences": [{"text": "as simple as possible (see section A).", "metadata": {}}, {"text": "But in an actual application, we would add back-\nground information about the data and the categories.", "metadata": {}}, {"text": "In general, prompt engineering,\nthe addition of representative examples to the prompt, or even fine-tuning an LLM\ncould yield higher accuracy rates.", "metadata": {}}, {"text": "On the other hand, energy efficiency in LLMs can\nbe improved through model quantization, which reduces computational demands by\ncompressing model parameters (Jacob et al., 2017).", "metadata": {}}, {"text": "Second, we do not account for the energy costs associated with training the tradi-\ntional models because it is infeasible to compare them to the training costs of LLMs.", "metadata": {}}, {"text": "The LLMs used in this study were pre-trained by external organizations and made\npublicly available.", "metadata": {}}, {"text": "As a result, the energy costs of training are distributed among all\nusers, making it difficult to estimate per-user energy consumption.", "metadata": {}}, {"text": "Even if training\nenergy costs for an LLM were known, the number of users remains uncertain.", "metadata": {}}, {"text": "Addi-\ntionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also\ncontribute to energy consumption.", "metadata": {}}, {"text": "Deploying an LLM on a dedicated server (e.g.,\nusing vLLM) requires setup time and additional energy.", "metadata": {}}, {"text": "Beyond inference, significant\ntime and computational resources are also required for development tasks, including\ndata processing, testing different models and prompts, parameter tuning, and debug-\nging - workloads that apply to both traditional models and LLMs.", "metadata": {}}, {"text": "The measurement\nof additional related energy consumptions (such as network traffic or disk storage) is\nbeyond the scope of this paper.", "metadata": {}}, {"text": "Third, energy consumption was measured using CodeCarbon, a tool recognized\nfor providing reliable estimates of a machine’s total energy use (Bouza et al., 2023).", "metadata": {}}, {"text": "However, it does not allow for precise measurement of energy consumption at the\nlevel of individual processes.", "metadata": {}}, {"text": "Moreover, power intake was recorded at 15-second\nintervals, meaning the accuracy of energy estimates improves with longer-running\nprocesses.", "metadata": {}}, {"text": "Another limitation of CodeCarbon is that RAM energy consumption is\napproximated at 0.375W per GB of memory used.", "metadata": {}}, {"text": "While the Running Average\nPower Limit (RAPL) framework can directly measure RAM power consumption, it\nis not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#\nissuecomment-2589805160).", "metadata": {}}, {"text": "Additionally, in shared computing environments such as\nhigh-performance computing (HPC) clusters, measurements may be affected by other\nusers’ activities.", "metadata": {}}, {"text": "Especially when an LLM was deployed across multiple nodes, varia-\ntions in network traffic at different times may have influenced energy measurements.", "metadata": {}}, {"text": "A\nmore precise assessment of energy efficiency would benefit from using dedicated com-\nputing resources with physical wattmeters and high-resolution energy measurement\ntools(e.g.", "metadata": {}}, {"text": "Ilsche et al., 2019).", "metadata": {}}, {"text": "In the following, we assess further limitations of the present study in more detail.", "metadata": {}}, {"text": "More specifically, we address our focus on a single dataset in section 5.1 and the\nlimitation to the text categorisation task in section 5.2.", "metadata": {}}, {"text": "Subsequently, we contextualise\nour work in the broader context of planet-centered LLMs (section 5.3).", "metadata": {}}, {"text": "5.1 Analysis on other datasets\nOur analysis was conducted on a highly specialized dataset.", "metadata": {}}, {"text": "To assess the generaliz-\nability of our findings, we replicated the experiments using four additional, widely used\ndatasets (see table 4).", "metadata": {}}, {"text": "These datasets were selected from the HuggingFace platform\n13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "based on popularity and had to meet two criteria: suitability for text classification and\ninclusion of two columns - text and label. To maintain comparability with our initial\nanalysis, we randomly sampled 200 training examples and 200 test examples from each\ndataset. Using a slightly larger training set might have provided an advantage to tra-\nditional models, as the LLMs were applied in a zero-shot setting without fine-tuning.\nEach model experiment was repeated 10 times with different samples, ensuring that\neach model was tested on the same 10 sets.\nName Classification Task ID on https://huggingface.co/datasets\nnews news topics: World, Sports,\nBusiness, Sci/Tech\nfancyzhx/ag news\nyelp sentiment: 1-5 stars Yelp/yelp review full\ntomatoes sentiment: pos, neg cornell-movie-review-data/rotten tomatoes\nemotion emotion: anger, fear, joy,\nlove, sadness, surprise\ndair-ai/emotion\nT able 4 Selection of datasets for text classification tasks.\nFigure 7 visualizes the relationship between accuracy and energy consumption for\nthese additional text classification tasks. For clarity, we restricted the visualization to\nthe models with the three highest accuracy scores and included the linear model with\nsentence embeddings for comparison (see Tables B4 and B5 for details).\nSimilar to our findings with the FKTG dataset, the DeepSeek models do not\noutperform the best non-reasoning models in most cases. The only exception is the\nemotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0 .61, slightly\nsurpassing the best non-reasoning model, Phi 3.5 MoE (0 .60). However, unlike in the\nprevious analysis, for every dataset, at least one LLM outperforms the best traditional\nmodel. For the news dataset, Llama 3.1 70B achieves an accuracy 0 .05 points higher\nthan the best linear model (0 .88 vs. 0 .83). However, this comes at the cost of signif-\nicantly higher energy consumption (34 .15 Wh vs. 0 .0021 Wh), highlighting the need\nfor careful trade-off considerations.\nIn the case of sentiment analysis on the Yelp dataset, traditional models perform\nconsiderably worse than LLMs, justifying the energy costs of LLM deployment. In some\ncases, a smaller model, such as Qwen 2.5 7B, may be sufficient. While its accuracy is\nslightly lower than the version with 72B parameters (0 .60 vs. 0 .68), it consumes only\none-eighth of the energy. A similar pattern is observed for sentiment analysis on the\nRotten Tomatoes dataset, where traditional models fail to match LLM performance.\nAmong the larger models, Jamba Mini 1.5 stands out as one of the most efficient\nchoices, offering strong accuracy while consuming significantly less energy. Notably,\ndespite having nearly as many parameters as Llama 3.1 70B and Qwen 2.5 72B (51.6B\nvs. 70B/72B), Jamba Mini 1.5 requires only a quarter of the energy for the same task.\nFinally, for emotion classification, the linear model with sentence embeddings is\namong the top-performing models. In this case, a traditional model provides the most\nefficient solution. Hence, accuracy-energy trade-offs must be assessed on a case-by-\ncase basis. In some scenarios, traditional models are sufficient, while in others, LLMs\noffer justifiable benefits despite higher energy consumption. However, a reason for the\n14", "sentences": [{"text": "based on popularity and had to meet two criteria: suitability for text classification and\ninclusion of two columns - text and label.", "metadata": {}}, {"text": "To maintain comparability with our initial\nanalysis, we randomly sampled 200 training examples and 200 test examples from each\ndataset.", "metadata": {}}, {"text": "Using a slightly larger training set might have provided an advantage to tra-\nditional models, as the LLMs were applied in a zero-shot setting without fine-tuning.", "metadata": {}}, {"text": "Each model experiment was repeated 10 times with different samples, ensuring that\neach model was tested on the same 10 sets.", "metadata": {}}, {"text": "Name Classification Task ID on https://huggingface.co/datasets\nnews news topics: World, Sports,\nBusiness, Sci/Tech\nfancyzhx/ag news\nyelp sentiment: 1-5 stars Yelp/yelp review full\ntomatoes sentiment: pos, neg cornell-movie-review-data/rotten tomatoes\nemotion emotion: anger, fear, joy,\nlove, sadness, surprise\ndair-ai/emotion\nT able 4 Selection of datasets for text classification tasks.", "metadata": {}}, {"text": "Figure 7 visualizes the relationship between accuracy and energy consumption for\nthese additional text classification tasks.", "metadata": {}}, {"text": "For clarity, we restricted the visualization to\nthe models with the three highest accuracy scores and included the linear model with\nsentence embeddings for comparison (see Tables B4 and B5 for details).", "metadata": {}}, {"text": "Similar to our findings with the FKTG dataset, the DeepSeek models do not\noutperform the best non-reasoning models in most cases.", "metadata": {}}, {"text": "The only exception is the\nemotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0 .61, slightly\nsurpassing the best non-reasoning model, Phi 3.5 MoE (0 .60).", "metadata": {}}, {"text": "However, unlike in the\nprevious analysis, for every dataset, at least one LLM outperforms the best traditional\nmodel.", "metadata": {}}, {"text": "For the news dataset, Llama 3.1 70B achieves an accuracy 0 .05 points higher\nthan the best linear model (0 .88 vs.", "metadata": {}}, {"text": "0 .83).", "metadata": {}}, {"text": "However, this comes at the cost of signif-\nicantly higher energy consumption (34 .15 Wh vs.", "metadata": {}}, {"text": "0 .0021 Wh), highlighting the need\nfor careful trade-off considerations.", "metadata": {}}, {"text": "In the case of sentiment analysis on the Yelp dataset, traditional models perform\nconsiderably worse than LLMs, justifying the energy costs of LLM deployment.", "metadata": {}}, {"text": "In some\ncases, a smaller model, such as Qwen 2.5 7B, may be sufficient.", "metadata": {}}, {"text": "While its accuracy is\nslightly lower than the version with 72B parameters (0 .60 vs.", "metadata": {}}, {"text": "0 .68), it consumes only\none-eighth of the energy.", "metadata": {}}, {"text": "A similar pattern is observed for sentiment analysis on the\nRotten Tomatoes dataset, where traditional models fail to match LLM performance.", "metadata": {}}, {"text": "Among the larger models, Jamba Mini 1.5 stands out as one of the most efficient\nchoices, offering strong accuracy while consuming significantly less energy.", "metadata": {}}, {"text": "Notably,\ndespite having nearly as many parameters as Llama 3.1 70B and Qwen 2.5 72B (51.6B\nvs.", "metadata": {}}, {"text": "70B/72B), Jamba Mini 1.5 requires only a quarter of the energy for the same task.", "metadata": {}}, {"text": "Finally, for emotion classification, the linear model with sentence embeddings is\namong the top-performing models.", "metadata": {}}, {"text": "In this case, a traditional model provides the most\nefficient solution.", "metadata": {}}, {"text": "Hence, accuracy-energy trade-offs must be assessed on a case-by-\ncase basis.", "metadata": {}}, {"text": "In some scenarios, traditional models are sufficient, while in others, LLMs\noffer justifiable benefits despite higher energy consumption.", "metadata": {}}, {"text": "However, a reason for the\n14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Fig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the\nLinear Embedding model was added for comparison), Capella system, single node. See Tables B4\nand B5 for results of all models.\nsuperior performance of LLMs on some datasets might be that the data were included\nin the model’s training data. Our study uses data that are probably not part of\nany LLM training set. Nevertheless, test-time compute, as featured by the Deepseek\nmodels, has no benefits in text classification tasks, and the linear relationship between\ncomputation runtime and energy consumption holds across all datasets (see Table B6).\n5.2 Transferability to other tasks\nAnother limitation of the present study is its exclusive focus on the categorization\ntask, which confines the analysis to a narrow subset of machine learning challenges.\nWhile this focus allows for a straightforward measurement of a model’s performance\n(using the accuracy metric), it neglects the applicability of the results to other tasks.\nRecent studies suggest that similar comparisons in terms of efficiency and accuracy can\nbe insightful in a variety of domains beyond categorization. For instance, Clavi´ e et al.\n15", "sentences": [{"text": "Fig.", "metadata": {}}, {"text": "7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the\nLinear Embedding model was added for comparison), Capella system, single node.", "metadata": {}}, {"text": "See Tables B4\nand B5 for results of all models.", "metadata": {}}, {"text": "superior performance of LLMs on some datasets might be that the data were included\nin the model’s training data.", "metadata": {}}, {"text": "Our study uses data that are probably not part of\nany LLM training set.", "metadata": {}}, {"text": "Nevertheless, test-time compute, as featured by the Deepseek\nmodels, has no benefits in text classification tasks, and the linear relationship between\ncomputation runtime and energy consumption holds across all datasets (see Table B6).", "metadata": {}}, {"text": "5.2 Transferability to other tasks\nAnother limitation of the present study is its exclusive focus on the categorization\ntask, which confines the analysis to a narrow subset of machine learning challenges.", "metadata": {}}, {"text": "While this focus allows for a straightforward measurement of a model’s performance\n(using the accuracy metric), it neglects the applicability of the results to other tasks.", "metadata": {}}, {"text": "Recent studies suggest that similar comparisons in terms of efficiency and accuracy can\nbe insightful in a variety of domains beyond categorization.", "metadata": {}}, {"text": "For instance, Clavi´ e et al.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}, {"text": "[Image page=15 idx=1 name=Im7.png] Size: 1152x1152, Data: 98323 bytes", "sentences": [{"text": "[Image page=15 idx=1 name=Im7.png] Size: 1152x1152, Data: 98323 bytes", "metadata": {}}], "metadata": {"page": 15, "image_index": 1, "image_name": "Im7.png", "image_width": 1152, "image_height": 1152, "attachment_type": "image", "has_image_data": true, "image_data_size": 98323}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "(2025) demonstrate that simple encoder-based models can effectively tackle generative\ntasks, expanding the potential applications of smaller, less energy-hungry models.\nMoreover, a growing body of research highlights the advantages of fine-tuned small\nmodels for specialized tasks, where they often outperform larger models (Savvov,\n2024). This trend is evident in studies such as Wei et al. (2024), where an diabetes-\nspecific LLM - despite having significantly fewer parameters - outperforms both GPT-4\nand Claude-3.5 in processing various diabetes tasks. Similarly, Lu et al. (2023) report\nthat their fine-tuned models achieve performance levels comparable to GPT-4 on\ndomain-specific annotation tasks, yet with hundreds of times fewer parameters and\nsignificantly reduced computational costs. Zhan et al. (2025) further emphasize the\nsuperior performance of fine-tuned small models over zero-shot LLMs, particularly in\nin-domain content moderation tasks.\nThe study by Luccioni et al. (2024) provides additional insights into the balance\nbetween model size and efficiency while looking at ten different machine learning tasks\nincluding image classification and captioning, question answering, summarization, as\nwell as image and text generation. The authors demonstrate that smaller models can\nachieve high performance with considerably less resource consumption. Their initiative\nresulted into the AI Energy Score (https://huggingface.co/AIEnergyScore), a tool\ndesigned to assess the environmental impact of AI models on a range of tasks, and\nreinforces the growing importance of considering energy efficiency in model evaluation.\n5.3 Further Requirements of Planet-Centered LLMs\nWhile energy consumption and the associated carbon footprint remain crucial con-\nsiderations for sustainable AI, truly planet-centered LLMs must meet a broader\nset of requirements that go beyond mere efficiency. These include other limited\nresources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical\nconsiderations, and technical adaptability to ensure responsible and sustainable AI\ndeployment.\nTransparency in AI models is essential for trust and reproducibility (Raji et al.,\n2020). The predictions of traditional LLM models are generally more transparent than\nthose of LLMs. Open-source LLMs, where both model architectures and training data\nare publicly available, contribute to scientific progress, allow for direct model compar-\nisons such as this present study, and reduce dependency on proprietary technologies\n(Wei et al., 2023). Furthermore, the ability to inspect training data is crucial to assess\npotential biases and copyright compliance (Bender et al., 2021). Many proprietary\nmodels, such as GPT-4, lack such transparency, making it difficult to evaluate their\nfairness and ethical considerations. The EU AI Act will require providers of general-\npurpose AI models to publish a sufficiently detailed summary of their training data\nstarting in August 2025, which further highlights the call for transparency.\nLLMs vary significantly in size, ranging from lightweight models such as fast-\nText (Joulin et al., 2017) to massive architectures like BLOOM-176B, which require\nsubstantial GPU memory and network bandwidth (Luccioni et al., 2023). These com-\nputational demands translate into high operational costs and environmental impacts.\nMoreover, some models require proprietary hardware, limiting their accessibility and\n16", "sentences": [{"text": "(2025) demonstrate that simple encoder-based models can effectively tackle generative\ntasks, expanding the potential applications of smaller, less energy-hungry models.", "metadata": {}}, {"text": "Moreover, a growing body of research highlights the advantages of fine-tuned small\nmodels for specialized tasks, where they often outperform larger models (Savvov,\n2024).", "metadata": {}}, {"text": "This trend is evident in studies such as Wei et al.", "metadata": {}}, {"text": "(2024), where an diabetes-\nspecific LLM - despite having significantly fewer parameters - outperforms both GPT-4\nand Claude-3.5 in processing various diabetes tasks.", "metadata": {}}, {"text": "Similarly, Lu et al.", "metadata": {}}, {"text": "(2023) report\nthat their fine-tuned models achieve performance levels comparable to GPT-4 on\ndomain-specific annotation tasks, yet with hundreds of times fewer parameters and\nsignificantly reduced computational costs.", "metadata": {}}, {"text": "Zhan et al.", "metadata": {}}, {"text": "(2025) further emphasize the\nsuperior performance of fine-tuned small models over zero-shot LLMs, particularly in\nin-domain content moderation tasks.", "metadata": {}}, {"text": "The study by Luccioni et al.", "metadata": {}}, {"text": "(2024) provides additional insights into the balance\nbetween model size and efficiency while looking at ten different machine learning tasks\nincluding image classification and captioning, question answering, summarization, as\nwell as image and text generation.", "metadata": {}}, {"text": "The authors demonstrate that smaller models can\nachieve high performance with considerably less resource consumption.", "metadata": {}}, {"text": "Their initiative\nresulted into the AI Energy Score (https://huggingface.co/AIEnergyScore), a tool\ndesigned to assess the environmental impact of AI models on a range of tasks, and\nreinforces the growing importance of considering energy efficiency in model evaluation.", "metadata": {}}, {"text": "5.3 Further Requirements of Planet-Centered LLMs\nWhile energy consumption and the associated carbon footprint remain crucial con-\nsiderations for sustainable AI, truly planet-centered LLMs must meet a broader\nset of requirements that go beyond mere efficiency.", "metadata": {}}, {"text": "These include other limited\nresources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical\nconsiderations, and technical adaptability to ensure responsible and sustainable AI\ndeployment.", "metadata": {}}, {"text": "Transparency in AI models is essential for trust and reproducibility (Raji et al.,\n2020).", "metadata": {}}, {"text": "The predictions of traditional LLM models are generally more transparent than\nthose of LLMs.", "metadata": {}}, {"text": "Open-source LLMs, where both model architectures and training data\nare publicly available, contribute to scientific progress, allow for direct model compar-\nisons such as this present study, and reduce dependency on proprietary technologies\n(Wei et al., 2023).", "metadata": {}}, {"text": "Furthermore, the ability to inspect training data is crucial to assess\npotential biases and copyright compliance (Bender et al., 2021).", "metadata": {}}, {"text": "Many proprietary\nmodels, such as GPT-4, lack such transparency, making it difficult to evaluate their\nfairness and ethical considerations.", "metadata": {}}, {"text": "The EU AI Act will require providers of general-\npurpose AI models to publish a sufficiently detailed summary of their training data\nstarting in August 2025, which further highlights the call for transparency.", "metadata": {}}, {"text": "LLMs vary significantly in size, ranging from lightweight models such as fast-\nText (Joulin et al., 2017) to massive architectures like BLOOM-176B, which require\nsubstantial GPU memory and network bandwidth (Luccioni et al., 2023).", "metadata": {}}, {"text": "These com-\nputational demands translate into high operational costs and environmental impacts.", "metadata": {}}, {"text": "Moreover, some models require proprietary hardware, limiting their accessibility and\n16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "long-term sustainability. Future AI systems should prioritize modularity and adapt-\nability, enabling efficient integration into diverse infrastructures without excessive\nresource demands.\nThe relevance and fairness of AI-generated outputs depend on the quality and\nrecency of training data. Stale or biased datasets can lead to misleading results and\nreinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In par-\nticular, the presence of toxic content or hate speech in training data can result in\nmodels generating harmful or discriminatory outputs, which poses serious challenges\nfor their deployment in sensitive contexts such as education, healthcare, or public\nadministration. Moreover, safety concerns—such as the risk of models producing fac-\ntually incorrect, manipulative, or otherwise harmful content—are especially critical in\npublic-sector applications, where accountability and trust are paramount (Weidinger\net al., 2021). Addressing these challenges requires robust bias-mitigation strategies\nand transparent documentation of model behavior.\nTo align with global sustainability and ethical AI principles, future research should\nemphasize the development of adaptable, transparent, and energy-efficient LLMs. By\nintegrating principles of openness, fairness, and regulatory compliance, we can foster\nAI systems that not only minimize environmental impact but also promote responsible\nand equitable usage across sectors.\nAcknowledgements. We gratefully acknowledge the support provided by the Fed-\neral Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV).\nAdditionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their\nsupport and inspiration.\nThis work was supported by high-performance computer time and resources from\nthe Center for Information Services and High Performance Computing (ZIH) of TUD\nDresden University of Technology and the systems for scientific computing of Leipzig\nUniversity. We thank the Center for Scalable Data Analytics and Artificial Intelligence\n(ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.\nThe tool ChatGPT (OpenAI) was used to revise the text of the paper.\nAuthor contribution statements. T.H. conceived the study, initiated the project,\nled the research effort, and contributed to the literature review and manuscript writing.\nJ.Z. designed and implemented the experiments, developed the codebase, conducted\ndata analysis, and contributed to drafting the manuscript.\nCompeting interests. There are no competing interests.\nAvailability of data and code. All underlying data will be shared upon reasonable\nrequest to the corresponding author. The source code will be made public.\n17", "sentences": [{"text": "long-term sustainability.", "metadata": {}}, {"text": "Future AI systems should prioritize modularity and adapt-\nability, enabling efficient integration into diverse infrastructures without excessive\nresource demands.", "metadata": {}}, {"text": "The relevance and fairness of AI-generated outputs depend on the quality and\nrecency of training data.", "metadata": {}}, {"text": "Stale or biased datasets can lead to misleading results and\nreinforce harmful stereotypes (Bender et al., 2021;", "metadata": {}}, {"text": "Gehman et al., 2020).", "metadata": {}}, {"text": "In par-\nticular, the presence of toxic content or hate speech in training data can result in\nmodels generating harmful or discriminatory outputs, which poses serious challenges\nfor their deployment in sensitive contexts such as education, healthcare, or public\nadministration.", "metadata": {}}, {"text": "Moreover, safety concerns—such as the risk of models producing fac-\ntually incorrect, manipulative, or otherwise harmful content—are especially critical in\npublic-sector applications, where accountability and trust are paramount (Weidinger\net al., 2021).", "metadata": {}}, {"text": "Addressing these challenges requires robust bias-mitigation strategies\nand transparent documentation of model behavior.", "metadata": {}}, {"text": "To align with global sustainability and ethical AI principles, future research should\nemphasize the development of adaptable, transparent, and energy-efficient LLMs.", "metadata": {}}, {"text": "By\nintegrating principles of openness, fairness, and regulatory compliance, we can foster\nAI systems that not only minimize environmental impact but also promote responsible\nand equitable usage across sectors.", "metadata": {}}, {"text": "Acknowledgements.", "metadata": {}}, {"text": "We gratefully acknowledge the support provided by the Fed-\neral Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV).", "metadata": {}}, {"text": "Additionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their\nsupport and inspiration.", "metadata": {}}, {"text": "This work was supported by high-performance computer time and resources from\nthe Center for Information Services and High Performance Computing (ZIH) of TUD\nDresden University of Technology and the systems for scientific computing of Leipzig\nUniversity.", "metadata": {}}, {"text": "We thank the Center for Scalable Data Analytics and Artificial Intelligence\n(ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.", "metadata": {}}, {"text": "The tool ChatGPT (OpenAI) was used to revise the text of the paper.", "metadata": {}}, {"text": "Author contribution statements.", "metadata": {}}, {"text": "T.H.", "metadata": {}}, {"text": "conceived the study, initiated the project,\nled the research effort, and contributed to the literature review and manuscript writing.", "metadata": {}}, {"text": "J.Z.", "metadata": {}}, {"text": "designed and implemented the experiments, developed the codebase, conducted\ndata analysis, and contributed to drafting the manuscript.", "metadata": {}}, {"text": "Competing interests.", "metadata": {}}, {"text": "There are no competing interests.", "metadata": {}}, {"text": "Availability of data and code.", "metadata": {}}, {"text": "All underlying data will be shared upon reasonable\nrequest to the corresponding author.", "metadata": {}}, {"text": "The source code will be made public.", "metadata": {}}, {"text": "17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "Appendix A LLM prompt\nFor the zero-shot classification, we prompted the LLM with the following instruction\n(originally in German):\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− . . .\nThe categories were a fixed set of 14 options that occurred in the train-\ning as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨ orungszonen’,\n‘ ¨Offentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des\nEndlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler\nLagerung’, ‘Datenverf¨ ugbarkeit’, ‘Modellierung’, ‘Referenzdatens¨ atze’, ‘Bereitstellung\nder Daten’, ‘Ausschlusskriterien’.\nSince we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the\nfinal prompt was automatically extended to the following:\n− r o l e : s y s t e m\nc o n t e n t : |−\nYour i n p u t f i e l d s a r e :\n1 . ‘ t e x t ‘ ( s t r )\nYour o u t p u t f i e l d s a r e :\n1 . ‘ c a t e g o r y ‘ ( s t r )\nA l l i n t e r a c t i o n s w i l l be s t r u c t u r e d i n t h e f o l l o w i n g way ,\nw i t h t h e a p p r o p r i a t e v a l u e s f i l l e d i n .\n[ [ # # t e x t # # ] ]\n{ t e x t}\n[ [ # # c a t e g o r y # # ] ]\n{ c a t e g o r y}\n[ [ # # c o m p l e t e d # # ] ]\nI n a d h e r i n g t o t h i s s t r u c t u r e , y o u r o b j e c t i v e i s :\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− . . .\n− r o l e : u s e r\nc o n t e n t : |−\n[ [ # # t e x t # # ] ]\n<t e x t>\nRespond w i t h t h e c o r r e s p o n d i n g o u t p u t f i e l d s , s t a r t i n g w i t h\nt h e f i e l d ‘ [ [ # # c a t e g o r y # # ] ] ‘ , and t h e n e n d i n g w i t h t h e\nm a r k e r f o r ‘ [ [ # # c o m p l e t e d # # ] ] ‘ .\n18", "sentences": [{"text": "Appendix A LLM prompt\nFor the zero-shot classification, we prompted the LLM with the following instruction\n(originally in German):\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "The categories were a fixed set of 14 options that occurred in the train-\ning as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨ orungszonen’,\n‘ ¨Offentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des\nEndlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler\nLagerung’, ‘Datenverf¨ ugbarkeit’, ‘Modellierung’, ‘Referenzdatens¨ atze’, ‘Bereitstellung\nder Daten’, ‘Ausschlusskriterien’.", "metadata": {}}, {"text": "Since we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the\nfinal prompt was automatically extended to the following:\n− r o l e : s y s t e m\nc o n t e n t : |−\nYour i n p u t f i e l d s a r e :\n1 .", "metadata": {}}, {"text": "‘ t e x t ‘ ( s t r )\nYour o u t p u t f i e l d s a r e :\n1 .", "metadata": {}}, {"text": "‘ c a t e g o r y ‘ ( s t r )\nA l l i n t e r a c t i o n s w i l l be s t r u c t u r e d i n t h e f o l l o w i n g way ,\nw i t h t h e a p p r o p r i a t e v a l u e s f i l l e d i n .", "metadata": {}}, {"text": "[ [ # # t e x t # # ] ]\n{ t e x t}\n[ [ # # c a t e g o r y # # ] ]\n{ c a t e g o r y}\n[ [ # # c o m p l e t e d # # ] ]\nI n a d h e r i n g t o t h i s s t r u c t u r e , y o u r o b j e c t i v e i s :\nC l a s s i f y t h e t e x t a s one o f t h e f o l l o w i n g c a t e g o r i e s :\n− <c a t e g o r y 1 >\n− <c a t e g o r y 2 >\n− .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "− r o l e : u s e r\nc o n t e n t : |−\n[ [ # # t e x t # # ] ]\n<t e x t>\nRespond w i t h t h e c o r r e s p o n d i n g o u t p u t f i e l d s , s t a r t i n g w i t h\nt h e f i e l d ‘ [ [ # # c a t e g o r y # # ] ] ‘ , and t h e n e n d i n g w i t h t h e\nm a r k e r f o r ‘ [ [ # # c o m p l e t e d # # ] ] ‘ .", "metadata": {}}, {"text": "18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Appendix B Tables\nModel GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)\nLinear BoW 1 <0.01 0.43 0.01 139.96\nLinear Tf-idf 1 <0.01 0.41 0.01 43.72\nLinear Embedding 1 0.12 0.57 1.64 259.41\nXGBoost BoW 1 <0.01 0.35 0.01 63.32\nXGBoost Tf-idf 1 <0.01 0.47 0.01 67.77\nXGBoost Embedding 1 0.21 0.47 2.87 259.94\nLlama 3.1 8B 1 5.86 0.35 36.88 572.49\nLlama 3.1 70B 2 48.60 0.48 161.59 1082.82\nQwen 2.5 7B 1 5.58 0.45 36.28 553.84\nQwen 2.5 72B 2 48.66 0.51 164.44 1065.31\nPhi 3.5 Mini 1 5.74 0.30 41.45 498.46\nPhi 3.5 MoE 2 11.00 0.40 55.51 713.34\nJamba Mini 1.5 2 17.42 0.34 78.61 797.94\nDS Llama 8B 1 79.64 0.37 517.83 553.67\nDS Llama 70B 2 702.06 0.46 2543.47 993.68\nDS Qwen 14B 1 155.20 0.39 981.35 569.33\nDS Qwen 32B 1 373.56 0.45 2255.99 596.11\nT able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system,\nsingle node, shown are averages over 10 runs\nModel Duration (s) Energy consumed (Wh)\nsingle double ratio single double ratio\nLlama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95\nQwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97\nJamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71\nDS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71\nT able B2 Comparison single vs. double node deployment, Capella system\nModel Duration (s) Energy consumed (Wh)\nA30 V100 H100 A30 V100 H100\nLlama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86\nQwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58\nPhi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74\nPhi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04\nDS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64\nDS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58\nDS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58\nT able B3 Comparison of different GPU cards, single node deployment.\n19", "sentences": [{"text": "Appendix B Tables\nModel GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)\nLinear BoW 1 <0.01 0.43 0.01 139.96\nLinear Tf-idf 1 <0.01 0.41 0.01 43.72\nLinear Embedding 1 0.12 0.57 1.64 259.41\nXGBoost BoW 1 <0.01 0.35 0.01 63.32\nXGBoost Tf-idf 1 <0.01 0.47 0.01 67.77\nXGBoost Embedding 1 0.21 0.47 2.87 259.94\nLlama 3.1 8B 1 5.86 0.35 36.88 572.49\nLlama 3.1 70B 2 48.60 0.48 161.59 1082.82\nQwen 2.5 7B 1 5.58 0.45 36.28 553.84\nQwen 2.5 72B 2 48.66 0.51 164.44 1065.31\nPhi 3.5 Mini 1 5.74 0.30 41.45 498.46\nPhi 3.5 MoE 2 11.00 0.40 55.51 713.34\nJamba Mini 1.5 2 17.42 0.34 78.61 797.94\nDS Llama 8B 1 79.64 0.37 517.83 553.67\nDS Llama 70B 2 702.06 0.46 2543.47 993.68\nDS Qwen 14B 1 155.20 0.39 981.35 569.33\nDS Qwen 32B 1 373.56 0.45 2255.99 596.11\nT able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system,\nsingle node, shown are averages over 10 runs\nModel Duration (s) Energy consumed (Wh)\nsingle double ratio single double ratio\nLlama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95\nQwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97\nJamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71\nDS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71\nT able B2 Comparison single vs.", "metadata": {}}, {"text": "double node deployment, Capella system\nModel Duration (s) Energy consumed (Wh)\nA30 V100 H100 A30 V100 H100\nLlama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86\nQwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58\nPhi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74\nPhi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04\nDS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64\nDS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58\nDS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58\nT able B3 Comparison of different GPU cards, single node deployment.", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "Dataset news yelp\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.65 <0.01 0.36\nLinear Tf-idf <0.01 0.65 <0.01 0.34\nLinear Embedding <0.01 0.83 0.04 0.43\nXGBoost BoW <0.01 0.48 <0.01 0.31\nXGBoost Tf-idf <0.01 0.52 <0.01 0.29\nXGBoost Embedding 0.03 0.74 0.01 0.40\nLlama 3.1 8B 4.31 0.71 4.73 0.58\nLlama 3.1 70B 34.15 0.88 36.71 0.67\nQwen 2.5 7B 4.21 0.01 4.52 0.60\nQwen 2.5 72B 33.75 0.79 38.20 0.68\nPhi 3.5 Mini 3.30 0.53 15.55 0.58\nPhi 3.5 MoE 8.53 0.78 8.32 0.58\nJamba Mini 1.5 9.34 0.78 11.45 0.56\nDS Llama 8B 60.58 0.82 97.18 0.62\nDS Llama 70B 483.73 0.83 707.03 0.67\nDS Qwen 14B 113.81 0.83 177.41 0.63\nDS Qwen 32B 271.92 0.83 358.62 0.63\nT able B4 Measurements of all models for the inference task on the news and\nyelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset tomatoes emotion\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.59 <0.01 0.36\nLinear Tf-idf <0.01 0.59 <0.01 0.40\nLinear Embedding 0.01 0.79 <0.01 0.59\nXGBoost BoW <0.01 0.54 <0.01 0.30\nXGBoost Tf-idf <0.01 0.55 <0.01 0.33\nXGBoost Embedding <0.01 0.76 <0.01 0.53\nLlama 3.1 8B 4.12 0.87 4.46 0.56\nLlama 3.1 70B 32.07 0.91 34.12 0.58\nQwen 2.5 7B 4.04 0.73 4.17 0.37\nQwen 2.5 72B 33.25 0.91 34.81 0.58\nPhi 3.5 Mini 7.20 0.87 5.13 0.53\nPhi 3.5 MoE 7.72 0.89 8.82 0.60\nJamba Mini 1.5 8.37 0.91 10.22 0.56\nDS Llama 8B 72.15 0.83 81.82 0.60\nDS Llama 70B 510.86 0.90 670.40 0.61\nDS Qwen 14B 134.02 0.89 148.20 0.60\nDS Qwen 32B 246.48 0.89 323.48 0.60\nT able B5 Measurements of all models for the inference task on the tomatoes and\nemotion datasets, Capella system, single node, shown are averages over 10 runs\n20", "sentences": [{"text": "Dataset news yelp\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.65 <0.01 0.36\nLinear Tf-idf <0.01 0.65 <0.01 0.34\nLinear Embedding <0.01 0.83 0.04 0.43\nXGBoost BoW <0.01 0.48 <0.01 0.31\nXGBoost Tf-idf <0.01 0.52 <0.01 0.29\nXGBoost Embedding 0.03 0.74 0.01 0.40\nLlama 3.1 8B 4.31 0.71 4.73 0.58\nLlama 3.1 70B 34.15 0.88 36.71 0.67\nQwen 2.5 7B 4.21 0.01 4.52 0.60\nQwen 2.5 72B 33.75 0.79 38.20 0.68\nPhi 3.5 Mini 3.30 0.53 15.55 0.58\nPhi 3.5 MoE 8.53 0.78 8.32 0.58\nJamba Mini 1.5 9.34 0.78 11.45 0.56\nDS Llama 8B 60.58 0.82 97.18 0.62\nDS Llama 70B 483.73 0.83 707.03 0.67\nDS Qwen 14B 113.81 0.83 177.41 0.63\nDS Qwen 32B 271.92 0.83 358.62 0.63\nT able B4 Measurements of all models for the inference task on the news and\nyelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset tomatoes emotion\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.59 <0.01 0.36\nLinear Tf-idf <0.01 0.59 <0.01 0.40\nLinear Embedding 0.01 0.79 <0.01 0.59\nXGBoost BoW <0.01 0.54 <0.01 0.30\nXGBoost Tf-idf <0.01 0.55 <0.01 0.33\nXGBoost Embedding <0.01 0.76 <0.01 0.53\nLlama 3.1 8B 4.12 0.87 4.46 0.56\nLlama 3.1 70B 32.07 0.91 34.12 0.58\nQwen 2.5 7B 4.04 0.73 4.17 0.37\nQwen 2.5 72B 33.25 0.91 34.81 0.58\nPhi 3.5 Mini 7.20 0.87 5.13 0.53\nPhi 3.5 MoE 7.72 0.89 8.82 0.60\nJamba Mini 1.5 8.37 0.91 10.22 0.56\nDS Llama 8B 72.15 0.83 81.82 0.60\nDS Llama 70B 510.86 0.90 670.40 0.61\nDS Qwen 14B 134.02 0.89 148.20 0.60\nDS Qwen 32B 246.48 0.89 323.48 0.60\nT able B5 Measurements of all models for the inference task on the tomatoes and\nemotion datasets, Capella system, single node, shown are averages over 10 runs\n20", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}, {"title": "Page 21", "paragraphs": [{"text": "Dependent variable: Energy\ntomatoes emotion news yelp\n(1) (2) (3) (4)\nDuration 0.040 ∗∗∗ 0.043∗∗∗ 0.052∗∗∗ 0.045∗∗∗\n(0.002) (0.002) (0.003) (0.003)\nGPUs −0.079 −0.052 0.536 0.810\n(0.950) (1.011) (1.470) (1.545)\nDuration:GPUs 0.122 ∗∗∗ 0.120∗∗∗ 0.115∗∗∗ 0.120∗∗∗\n(0.001) (0.001) (0.002) (0.002)\nConstant −0.397 −0.464 −1.300 −1.773\n(1.290) (1.372) (1.985) (2.103)\nObservations 17 17 17 17\nR2 1.000 1.000 1.000 1.000\nAdjusted R2 1.000 1.000 1.000 1.000\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nT able B6 Linear regression of energy consumption on duration for\nthe datasets of section 5.1 (table format by Hlavac, 2022).\nReferences\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems, vol. 30 (2017)\nBrown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A.,et al.: Language models are few-shot\nlearners. In: Advances in Neural Information Processing Systems, vol. 33, pp.\n1877–1901 (2020)\nLacoste, A., Luccioni, A., Schmidt, V., Dandres, T.: Quantifying the Carbon Emissions\nof Machine Learning (2019). https://arxiv.org/abs/1910.09700\nAxenbeck, J., Kunkel, S., Blain, J., et al.: Global Embodied Emissions of Digital\nTechnologies: The Hidden 42%. Research Square. Preprint (Version 1) available\nat Research Square (2025). https://doi.org/10.21203/rs.3.rs-6479454/v1 . https://\nwww.researchsquare.com/article/rs-6479454/v1\nUnited Nations: Paris Agreement (2015). https://unfccc.int/sites/default/files/\nenglish paris agreement.pdf\nUnited Nations: Transforming our world: the 2030 Agenda for Sustain-\nable Development (2015). https://sustainabledevelopment.un.org/post2015/\n21", "sentences": [{"text": "Dependent variable: Energy\ntomatoes emotion news yelp\n(1) (2) (3) (4)\nDuration 0.040 ∗∗∗ 0.043∗∗∗ 0.052∗∗∗ 0.045∗∗∗\n(0.002) (0.002) (0.003) (0.003)\nGPUs −0.079 −0.052 0.536 0.810\n(0.950) (1.011) (1.470) (1.545)\nDuration:GPUs 0.122 ∗∗∗ 0.120∗∗∗ 0.115∗∗∗ 0.120∗∗∗\n(0.001) (0.001) (0.002) (0.002)\nConstant −0.397 −0.464 −1.300 −1.773\n(1.290) (1.372) (1.985) (2.103)\nObservations 17 17 17 17\nR2 1.000 1.000 1.000 1.000\nAdjusted R2 1.000 1.000 1.000 1.000\nNote: ∗p<0.1;", "metadata": {}}, {"text": "∗∗p<0.05;", "metadata": {}}, {"text": "∗∗∗p<0.01\nT able B6 Linear regression of energy consumption on duration for\nthe datasets of section 5.1 (table format by Hlavac, 2022).", "metadata": {}}, {"text": "References\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need.", "metadata": {}}, {"text": "In: Advances in Neural Information\nProcessing Systems, vol.", "metadata": {}}, {"text": "30 (2017)\nBrown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A.,et al.: Language models are few-shot\nlearners.", "metadata": {}}, {"text": "In: Advances in Neural Information Processing Systems, vol.", "metadata": {}}, {"text": "33, pp.", "metadata": {}}, {"text": "1877–1901 (2020)\nLacoste, A., Luccioni, A., Schmidt, V., Dandres, T.: Quantifying the Carbon Emissions\nof Machine Learning (2019).", "metadata": {}}, {"text": "https://arxiv.org/abs/1910.09700\nAxenbeck, J., Kunkel, S., Blain, J., et al.: Global Embodied Emissions of Digital\nTechnologies: The Hidden 42%.", "metadata": {}}, {"text": "Research Square.", "metadata": {}}, {"text": "Preprint (Version 1) available\nat Research Square (2025).", "metadata": {}}, {"text": "https://doi.org/10.21203/rs.3.rs-6479454/v1 .", "metadata": {}}, {"text": "https://\nwww.researchsquare.com/article/rs-6479454/v1\nUnited Nations: Paris Agreement (2015).", "metadata": {}}, {"text": "https://unfccc.int/sites/default/files/\nenglish paris agreement.pdf\nUnited Nations: Transforming our world: the 2030 Agenda for Sustain-\nable Development (2015).", "metadata": {}}, {"text": "https://sustainabledevelopment.un.org/post2015/\n21", "metadata": {}}], "metadata": {"page": 21}}], "metadata": {"page": 21}}, {"title": "Page 22", "paragraphs": [{"text": "transformingourworld\nStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep\nLearning in NLP (2019). https://arxiv.org/abs/1906.02243\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D.,\nTexier, M., Dean, J.: Carbon Emissions and Large Neural Network Training (2021).\nhttps://arxiv.org/abs/2104.10350\nLuccioni, A.S., Hernandez-Garcia, A.: Counting Carbon: A Survey of Factors Influ-\nencing the Emissions of Machine Learning (2023). https://arxiv.org/abs/2302.\n08476\nOpenAI: Learning to reason with LLMs (2024). https://openai.com/index/\nlearning-to-reason-with-llms/\nDeepSeek-AI: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning (2025). https://arxiv.org/abs/2501.12948\nJoulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for efficient text\nclassification. In: Lapata, M., Blunsom, P., Koller, A. (eds.) Proceedings of the\n15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pp. 427–431. Association for Computational\nLinguistics, Valencia, Spain (2017). https://aclanthology.org/E17-2068/\nWang, Z., Pang, Y., Lin, Y., Zhu, X.: Adaptable and Reliable Text Classification using\nLarge Language Models (2024). https://arxiv.org/abs/2405.10523\nLangChain Team: Classification Tutorial – LangChain Documentation. https://\npython.langchain.com/docs/tutorials/classification/. Accessed: 2025-02-23 (2023)\nLamini Team: CAT Documentation – Lamini. https://docs.lamini.ai/cat/. Accessed:\n2025-02-23 (2023)\nKaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning\nartificial intelligence with climate change mitigation 12(6), 518–527 https://doi.\norg/10.1038/s41558-022-01377-7\nLuccioni, A.S., Strubell, E., Crawford, K.: From Efficiency Gains to Rebound Effects:\nThe Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate (2025).\nhttps://arxiv.org/abs/2501.16548\nPatterson, D., Gonzalez, J., H¨ olzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild,\nD., So, D., Texier, M., Dean, J.: The Carbon Footprint of Machine Learning Training\nWill Plateau, Then Shrink (2022). https://arxiv.org/abs/2204.05149\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W.,\nKepner, J., Tiwari, D., Gadepally, V.: From Words to Watts: Benchmarking the\n22", "sentences": [{"text": "transformingourworld\nStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep\nLearning in NLP (2019).", "metadata": {}}, {"text": "https://arxiv.org/abs/1906.02243\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D.,\nTexier, M., Dean, J.: Carbon Emissions and Large Neural Network Training (2021).", "metadata": {}}, {"text": "https://arxiv.org/abs/2104.10350\nLuccioni, A.S., Hernandez-Garcia, A.: Counting Carbon: A Survey of Factors Influ-\nencing the Emissions of Machine Learning (2023).", "metadata": {}}, {"text": "https://arxiv.org/abs/2302.", "metadata": {}}, {"text": "08476\nOpenAI: Learning to reason with LLMs (2024).", "metadata": {}}, {"text": "https://openai.com/index/\nlearning-to-reason-with-llms/\nDeepSeek-AI: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning (2025).", "metadata": {}}, {"text": "https://arxiv.org/abs/2501.12948\nJoulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for efficient text\nclassification.", "metadata": {}}, {"text": "In: Lapata, M., Blunsom, P., Koller, A.", "metadata": {}}, {"text": "(eds.) Proceedings of the\n15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pp.", "metadata": {}}, {"text": "427–431.", "metadata": {}}, {"text": "Association for Computational\nLinguistics, Valencia, Spain (2017).", "metadata": {}}, {"text": "https://aclanthology.org/E17-2068/\nWang, Z., Pang, Y., Lin, Y., Zhu, X.: Adaptable and Reliable Text Classification using\nLarge Language Models (2024).", "metadata": {}}, {"text": "https://arxiv.org/abs/2405.10523\nLangChain Team: Classification Tutorial – LangChain Documentation.", "metadata": {}}, {"text": "https://\npython.langchain.com/docs/tutorials/classification/.", "metadata": {}}, {"text": "Accessed: 2025-02-23 (2023)\nLamini Team: CAT Documentation – Lamini.", "metadata": {}}, {"text": "https://docs.lamini.ai/cat/.", "metadata": {}}, {"text": "Accessed:\n2025-02-23 (2023)\nKaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning\nartificial intelligence with climate change mitigation 12(6), 518–527 https://doi.", "metadata": {}}, {"text": "org/10.1038/s41558-022-01377-7\nLuccioni, A.S., Strubell, E., Crawford, K.: From Efficiency Gains to Rebound Effects:\nThe Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate (2025).", "metadata": {}}, {"text": "https://arxiv.org/abs/2501.16548\nPatterson, D., Gonzalez, J., H¨ olzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild,\nD., So, D., Texier, M., Dean, J.: The Carbon Footprint of Machine Learning Training\nWill Plateau, Then Shrink (2022).", "metadata": {}}, {"text": "https://arxiv.org/abs/2204.05149\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W.,\nKepner, J., Tiwari, D., Gadepally, V.: From Words to Watts: Benchmarking the\n22", "metadata": {}}], "metadata": {"page": 22}}], "metadata": {"page": 22}}, {"title": "Page 23", "paragraphs": [{"text": "Energy Costs of Large Language Model Inference (2023). https://arxiv.org/abs/\n2310.03003\nLiu, X., Sun, T., He, J., Wu, J., Wu, L., Zhang, X., Jiang, H., Cao, Z., Huang, X., Qiu,\nX.: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (2022).\nhttps://arxiv.org/abs/2110.07038\nChien, A.A., Lin, L., Nguyen, H., Rao, V., Sharma, T., Wijayawardana, R.: Reducing\nthe carbon impact of generative ai inference (today and in 2035). In: Proceedings of\nthe 2nd Workshop on Sustainable Computer Systems. HotCarbon ’23. Association\nfor Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/\n3604930.3605705 . https://doi.org/10.1145/3604930.3605705\nWang, A., Wolf, T.: Overview of the sustainlp 2020 shared task. In: SUSTAINLP\n(2020). https://api.semanticscholar.org/CorpusID:226283937\nAlizadeh, N., Belchev, B., Saurabh, N., Kelbert, P., Castor, F.: Language Models in\nSoftware Development Tasks: An Experimental Analysis of Energy and Accuracy\n(2025). https://arxiv.org/abs/2412.00329\nLuccioni, S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the\ncost of ai deployment? In: Proceedings of the 2024 ACM Conference on Fairness,\nAccountability, and Transparency. FAccT ’24, pp. 85–99. Association for Computing\nMachinery, New York, NY, USA (2024). https://doi.org/10.1145/3630106.3658542\nBai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M.,\nZhang, Y., Song, X., Yang, C., Cheng, Y., Zhao, L.: Beyond Efficiency: A Systematic\nSurvey of Resource-Efficient Large Language Models (2024). https://arxiv.org/abs/\n2401.00625\nAnthony, L.F.W., Kanding, B., Selvan, R.: Carbontracker: Tracking and Predicting\nthe Carbon Footprint of Training Deep Learning Models (2020). https://arxiv.org/\nabs/2007.03051\nHenderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., Pineau, J.: Towards the\nsystematic reporting of the energy and carbon footprints of machine learning. J.\nMach. Learn. Res. 21(1) (2020)\nBouza, L., Bugeau, A., Lannelongue, L.: How to estimate carbon footprint when\ntraining deep learning models? a guide and review. Environmental Research\nCommunications 5(11), 115014 (2023) https://doi.org/10.1088/2515-7620/acf81b\nHlavac, M.: stargazer: Well-Formatted Regression and Summary Statistics Tables. R\npackage version 5.2.3 (2022). https://CRAN.R-project.org/package=stargazer\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\n23", "sentences": [{"text": "Energy Costs of Large Language Model Inference (2023).", "metadata": {}}, {"text": "https://arxiv.org/abs/\n2310.03003\nLiu, X., Sun, T., He, J., Wu, J., Wu, L., Zhang, X., Jiang, H., Cao, Z., Huang, X., Qiu,\nX.: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (2022).", "metadata": {}}, {"text": "https://arxiv.org/abs/2110.07038\nChien, A.A., Lin, L., Nguyen, H., Rao, V., Sharma, T., Wijayawardana, R.: Reducing\nthe carbon impact of generative ai inference (today and in 2035).", "metadata": {}}, {"text": "In: Proceedings of\nthe 2nd Workshop on Sustainable Computer Systems.", "metadata": {}}, {"text": "HotCarbon ’23.", "metadata": {}}, {"text": "Association\nfor Computing Machinery, New York, NY, USA (2023).", "metadata": {}}, {"text": "https://doi.org/10.1145/\n3604930.3605705 .", "metadata": {}}, {"text": "https://doi.org/10.1145/3604930.3605705\nWang, A., Wolf, T.: Overview of the sustainlp 2020 shared task.", "metadata": {}}, {"text": "In: SUSTAINLP\n(2020).", "metadata": {}}, {"text": "https://api.semanticscholar.org/CorpusID:226283937\nAlizadeh, N., Belchev, B., Saurabh, N., Kelbert, P., Castor, F.: Language Models in\nSoftware Development Tasks: An Experimental Analysis of Energy and Accuracy\n(2025).", "metadata": {}}, {"text": "https://arxiv.org/abs/2412.00329\nLuccioni, S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the\ncost of ai deployment?", "metadata": {}}, {"text": "In: Proceedings of the 2024 ACM Conference on Fairness,\nAccountability, and Transparency.", "metadata": {}}, {"text": "FAccT ’24, pp.", "metadata": {}}, {"text": "85–99.", "metadata": {}}, {"text": "Association for Computing\nMachinery, New York, NY, USA (2024).", "metadata": {}}, {"text": "https://doi.org/10.1145/3630106.3658542\nBai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M.,\nZhang, Y., Song, X., Yang, C., Cheng, Y., Zhao, L.: Beyond Efficiency: A Systematic\nSurvey of Resource-Efficient Large Language Models (2024).", "metadata": {}}, {"text": "https://arxiv.org/abs/\n2401.00625\nAnthony, L.F.W., Kanding, B., Selvan, R.: Carbontracker: Tracking and Predicting\nthe Carbon Footprint of Training Deep Learning Models (2020).", "metadata": {}}, {"text": "https://arxiv.org/\nabs/2007.03051\nHenderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., Pineau, J.: Towards the\nsystematic reporting of the energy and carbon footprints of machine learning.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Mach.", "metadata": {}}, {"text": "Learn.", "metadata": {}}, {"text": "Res.", "metadata": {}}, {"text": "21(1) (2020)\nBouza, L., Bugeau, A., Lannelongue, L.: How to estimate carbon footprint when\ntraining deep learning models?", "metadata": {}}, {"text": "a guide and review.", "metadata": {}}, {"text": "Environmental Research\nCommunications 5(11), 115014 (2023) https://doi.org/10.1088/2515-7620/acf81b\nHlavac, M.: stargazer: Well-Formatted Regression and Summary Statistics Tables.", "metadata": {}}, {"text": "R\npackage version 5.2.3 (2022).", "metadata": {}}, {"text": "https://CRAN.R-project.org/package=stargazer\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\n23", "metadata": {}}], "metadata": {"page": 23}}], "metadata": {"page": 23}}, {"title": "Page 24", "paragraphs": [{"text": "G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C.,\nChen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCan-\ndlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot\nlearners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.)\nAdvances in Neural Information Processing Systems, vol. 33, pp. 1877–1901. Curran\nAssociates, Inc., ??? (2020)\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,\nKalenichenko, D.: Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference (2017). https://arxiv.org/abs/1712.05877\nIlsche, T., Hackenberg, D., Sch¨ one, R., Bielert, M., H¨ opfner, F., E. Nagel, W.: Met-\nricq: A scalable infrastructure for processing high-resolution time series data. In:\n2019 IEEE/ACM Industry/University Joint International Workshop on Data-center\nAutomation, Analytics, and Control (DAAC), pp. 7–12 (2019). https://doi.org/10.\n1109/DAAC49578.2019.00007\nClavi´ e, B., Cooper, N., Warner, B.: It’s All in The [MASK]: Simple Instruction-Tuning\nEnables BERT-like Masked Language Models As Generative Classifiers (2025).\nhttps://arxiv.org/abs/2502.03793\nSavvov, S.: Your Company Needs Small Language Models (2024). https://\ntowardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9/\nWei, L., Ying, Z., He, M., Chen, Y., Yang, Q., Hong, Y., Lu, J., Li, X., Huang, W.,\nChen, Y.: An adapted large language model facilitates multiple medical tasks in\ndiabetes care (2024). https://arxiv.org/abs/2409.13191\nLu, Y., Yao, B., Zhang, S., Wang, Y., Zhang, P., Lu, T., Li, T.J.-J., Wang, D.: Human\nStill Wins over LLM: An Empirical Study of Active Learning on Domain-Specific\nAnnotation Tasks (2023). https://arxiv.org/abs/2311.09825\nZhan, X., Goyal, A., Chen, Y., Chandrasekharan, E., Saha, K.: SLM-Mod: Small\nLanguage Models Surpass LLMs at Content Moderation (2025). https://arxiv.org/\nabs/2410.13155\nRaji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving face:\nInvestigating the ethical concerns of facial recognition auditing. Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society (2020)\nWei, J., Bosma, M., Zhao, V.Y., et al.: Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971 (2023)\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of\nstochastic parrots: Can language models be too big? Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency (2021)\n24", "sentences": [{"text": "G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C.,\nChen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCan-\ndlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot\nlearners.", "metadata": {}}, {"text": "In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H.", "metadata": {}}, {"text": "(eds.)\nAdvances in Neural Information Processing Systems, vol.", "metadata": {}}, {"text": "33, pp.", "metadata": {}}, {"text": "1877–1901.", "metadata": {}}, {"text": "Curran\nAssociates, Inc., ???", "metadata": {}}, {"text": "(2020)\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,\nKalenichenko, D.: Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference (2017).", "metadata": {}}, {"text": "https://arxiv.org/abs/1712.05877\nIlsche, T., Hackenberg, D., Sch¨ one, R., Bielert, M., H¨ opfner, F., E.", "metadata": {}}, {"text": "Nagel, W.: Met-\nricq: A scalable infrastructure for processing high-resolution time series data.", "metadata": {}}, {"text": "In:\n2019 IEEE/ACM Industry/University Joint International Workshop on Data-center\nAutomation, Analytics, and Control (DAAC), pp.", "metadata": {}}, {"text": "7–12 (2019).", "metadata": {}}, {"text": "https://doi.org/10.", "metadata": {}}, {"text": "1109/DAAC49578.2019.00007\nClavi´ e, B., Cooper, N., Warner, B.: It’s All in The [MASK]: Simple Instruction-Tuning\nEnables BERT-like Masked Language Models As Generative Classifiers (2025).", "metadata": {}}, {"text": "https://arxiv.org/abs/2502.03793\nSavvov, S.: Your Company Needs Small Language Models (2024).", "metadata": {}}, {"text": "https://\ntowardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9/\nWei, L., Ying, Z., He, M., Chen, Y., Yang, Q., Hong, Y., Lu, J., Li, X., Huang, W.,\nChen, Y.: An adapted large language model facilitates multiple medical tasks in\ndiabetes care (2024).", "metadata": {}}, {"text": "https://arxiv.org/abs/2409.13191\nLu, Y., Yao, B., Zhang, S., Wang, Y., Zhang, P., Lu, T., Li, T.J.-J., Wang, D.: Human\nStill Wins over LLM: An Empirical Study of Active Learning on Domain-Specific\nAnnotation Tasks (2023).", "metadata": {}}, {"text": "https://arxiv.org/abs/2311.09825\nZhan, X., Goyal, A., Chen, Y., Chandrasekharan, E., Saha, K.: SLM-Mod: Small\nLanguage Models Surpass LLMs at Content Moderation (2025).", "metadata": {}}, {"text": "https://arxiv.org/\nabs/2410.13155\nRaji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving face:\nInvestigating the ethical concerns of facial recognition auditing.", "metadata": {}}, {"text": "Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society (2020)\nWei, J., Bosma, M., Zhao, V.Y., et al.: Llama: Open and efficient foundation language\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.13971 (2023)\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of\nstochastic parrots: Can language models be too big?", "metadata": {}}, {"text": "Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency (2021)\n24", "metadata": {}}], "metadata": {"page": 24}}], "metadata": {"page": 24}}, {"title": "Page 25", "paragraphs": [{"text": "Luccioni, A.S., Viguier, S., Ligozat, A.-L.: Estimating the carbon footprint of bloom,\na 176b parameter language model. J. Mach. Learn. Res. 24(1) (2023)\nGehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: Realtoxicityprompts:\nEvaluating neural toxic degeneration in language models. Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP)\n(2020)\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359 (2021)\n25", "sentences": [{"text": "Luccioni, A.S., Viguier, S., Ligozat, A.-L.: Estimating the carbon footprint of bloom,\na 176b parameter language model.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Mach.", "metadata": {}}, {"text": "Learn.", "metadata": {}}, {"text": "Res.", "metadata": {}}, {"text": "24(1) (2023)\nGehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: Realtoxicityprompts:\nEvaluating neural toxic degeneration in language models.", "metadata": {}}, {"text": "Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP)\n(2020)\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2112.04359 (2021)\n25", "metadata": {}}], "metadata": {"page": 25}}], "metadata": {"page": 25}}]}