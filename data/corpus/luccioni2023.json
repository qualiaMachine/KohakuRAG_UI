{"document_id": "luccioni2023", "title": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning", "text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine\nLearning\nALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada\nALEX HERNANDEZ-GARCIA, Mila, Universit√© de Montr√©al, Montreal, Canada\nMachine learning (ML) requires using energy to carry out computations during the model training process. The generation of this\nenergy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source.\nExisting research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not\nadequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95\nML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the\nenergy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model\nperformance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized\nrepository for reporting and tracking these emissions.\n1 INTRODUCTION\nIn recent years, machine learning (ML) models have achieved high performance in a multitude of tasks such as\nimage classification, machine translation, and object detection. However, this progress also comes with a cost in terms\nof energy, since developing and deploying ML models requires access to computational resources such as Graphical\nProcessing Units (GPUs) and therefore energy to power them. In turn, producing this energy comes with a cost to the\nenvironment, given that energy generation often entails the emission of greenhouse gases (GHG) such as carbon dioxide\n(CO2) [40]. On a global scale, electricity generation represents over a quarter of the global GHG emissions, adding up to\n33.1 gigatonnes of CO2 in 2019 [24]. Recent estimates put the contribution of the information and communications\ntechnology (ICT) sector ‚Äì which includes the data centers, devices and networks used for training and deploying ML\nmodels ‚Äì at 2‚Äì6 % of global GHG emissions, although the exact number is still debated [ 25, 32, 36]. In fact, there is\nlimited information about the overall energy consumption and carbon footprint of our field, how it is evolving, and\nhow it correlates with performance on different tasks.\nThe goal of the current paper is to analyze the main factors influencing the carbon emissions of our field, to study\nthe evolution across time, and to contribute towards a better understanding of the carbon emissions generated by ML\nmodels trained on different tasks and as a function of their performance. As such, our research aims to answer the\nfollowing research questions:\n(1) What are the main sources of energy used for training ML models?\n(2) What is the order of magnitude of CO 2 emissions produced by training ML models?\n(3) How do the CO 2 emissions produced by training ML models evolve over time?\n(4) Does more energy and CO 2 lead to better model performance?\nAuthors‚Äô addresses: Alexandra Sasha Luccioni, Hugging Face, Montreal, Canada, sasha.luccioni@huggingface.co; Alex Hernandez-Garcia, Mila, Universit√©\nde Montr√©al, Montreal, Canada, alex.hernandez-garcia@mila.quebec.\n2023. Manuscript pending review\n1\narXiv:2302.08476v1  [cs.LG]  16 Feb 2023\n\n2 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nWe start our article with a survey of related work in Section 2, followed by a presentation of our methodology\nin Section 3. In Section 4 we present our analysis, and we conclude with our proposals for future work, including a\ncentralized hub for reporting the carbon footprint of machine learning..\n2 RELATED WORK\nMeasuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering\nmomentum in recent years. In the current section, we present several directions pursued in this domain, from empirical\nstudies of specific models to the development of efficient algorithms and hardware.\nEmpirical studies on carbon emissions. A large proportion of research has focused on estimating the carbon emissions\nof specific model architectures and/or comparing the carbon emissions of two or more models and approaches. The\nfirst paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large\nTransformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the\nlifetime emissions of five US cars. [48]. This perspective has since been explored further via analyses of the carbon\nfootprint of different neural network architectures [31, 37, 38] and the relative efficiency of different methods [35, 56].\nThese empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research\nareas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain\nunexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale\nand variation of carbon emissions in our community.\nTools and approaches for measuring carbon emissions. Developing standardized approaches for estimating the carbon\nemissions of model training has also been the focus of much work [5, 20, 26, 27, 30, 45, 51]. As a result, there are several\ntools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the\nmodel training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate\nof the amount of carbon emitted. However, a recent study on different carbon estimation tools concluded that the\nestimates produced by different tools vary significantly and consistently under-report emissions [7]. To date, there\nis no single, accepted approach for estimating the carbon emissions of the field, making standardized reporting and\ncomparisons difficult [31].\nBroader impacts of ML models. Several papers have been written in recent years regarding the broader societal impacts\nof ML models, which includes their environmental footprint. This spans research on how the size and computational\ndemands of ML models in general [50] and large language models in particular [8, 11] have grown in recent years. Many\nstrategies and directions forward have been proposed, ranging from advocating for more environmentally-conscious\npractice of AI [46] to adopting a sustainability mindset for the community [54]. However, while the documentation\nof aspects such as bias and safety has begun to be described in reports and articles accompanying certain recent ML\nmodels (e.g. [12, 22]), environmental impacts have yet to be consistently tracked and reported. Notable exceptions\ninclude recent language models such as OPT [57], T0 [43] and BLOOM [31].\nEfficient algorithms and hardware. A related and complementary direction of research is the development of more\nefficient model architectures and approaches. For instance, approaches such as Eyeriss [13] and DistilBERT [42] have\nmade significant progress in terms of computing efficiency, enabling faster training and inference, which results in less\nenergy usage and, indirectly, less carbon emissions, during model training. This research is gathering attention within\nthe community, with workshops such as SustaiNLP and EMC2 growing in scope and popularity, although efficiency\nManuscript pending review\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 3\nhas yet to be a central consideration when it comes to evaluating and comparing models. However, energy-efficient\nbenchmarks such as HULK [58] have also been proposed, which take computational requirements and environmental\nimpacts into account during model evaluation, allowing a comparison of models based on multiple criteria.\nOther aspects of the carbon impact of ML. Finally, efforts have been made to quantify other factors that have an\ninfluence on the overall carbon footprint of the field of ML, including in-person versus virtual conference attendance [47],\nthe manufacturing of computing hardware [ 19], life cycle analysis of the entire ML development and deployment\ncycle [28], as well as some initial studies regarding the carbon footprint of model deployment in production settings [31].\nThe relative contribution of each of these factors is still unclear, which suggests that further research is needed in order\nto further disentangle these factors.\n3 METHODOLOGY\nAs stated in Section 1, the goal of this paper is descriptive ‚Äì to observe the evolution of the carbon emissions of our\nfield of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In\nthis section, we present the different aspects and details of our methodology.\n3.1 Data collection\nIn order to gather data from a diverse set of ML models from a variety of domains and tasks, we leveraged the dataset\ncollected by Thompson et al. [50] in the scope of a recent study on the computational requirements of ML. From this\ndataset, we equally sampled 500 papers published from 2012 to 2021 spanning 5 tasks: Image Classification, Object\nDetection, Machine Translation, Question Answering and Named Entity Recognition. We then contacted the first author\nof each of the papers and asked them to provide missing training details regarding their model (See Supplementary\nMaterials A.1 for the email text). We were able to collect information for a total of 95 models from 77 papers (since\nsome of the papers trained more than one model), which represents an author response rate of 15.4 %.\nTable 1. Summary of the models analyzed in our study\nTask Dataset Number of Models Publication dates\nImage Classification ImageNet [14] 35 2012-2021\nMachine Translation WMT2014 [10] 30 2016-2021\nNamed Entity Recognition CoNLL 2003 [41] 11 2015-2021\nQuestion Answering SQuAD 1.1 [39] 10 2016-2021\nObject Detection MS COCO [29] 9 2019-2021\nThe models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of\nconferences and journals. They all represent novel architectures at the time of publication, achieving high performance\nin their respective tasks: on average, the models are within 8 % of SOTA performance according to Papers With Code\nleaderboards at the time of their publication . This sample represents the largest amount of information regarding\nthe carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of\nangles, which we present in Section 4. In the remaining of this section, we describe our method for estimating carbon\nemissions.\nManuscript pending review\n\n4 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n3.2 Estimating carbon emissions\nThe unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents. This unit\nallows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of\nCO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) 1.\nThe amount of CO2eq (ùê∂) emitted during model training can be decomposed into three relevant factors: the power\nconsumption of the hardware used ( ùëÉ), the training time ( ùëá ) and the carbon intensity of the energy grid ( ùêº); or\nequivalently, the energy consumed (ùê∏) and the carbon intensity:\nùê∂ = ùëÉ √ó ùëá √ó ùêº = ùê∏ √ó ùêº . (1)\nFor instance, a model trained on a single GPU consuming 300 W for 100 hours on a grid that emits 500 gCO2eq/kWh will\nemit 0.3 kW √ó 100 h √ó 500 g/kWh = 15000 g = 15 kg of CO2eq. The same model trained on a less carbon-intensive\nenergy grid, emitting only 100 gCO2eq/kWh, will only emit 0.3 √ó 100 √ó 100 = 3000 g = 3 kg of CO2eq, i.e. five times\nless overall. In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the\nlocation of the computer or server where their model was trained (either cloud or local), the hardware used, and the\ntotal model training time. We describe how we estimate each of the relevant factors in the paragraphs below:\nCarbon Intensity. Based on the training location provided by authors, we were able to estimate the carbon intensity\nof the energy grid that was utilized, based on publicly-available sources such as the International Energy Agency and\nthe Energy Information Administration. The granularity of information available ranges widely depending on the\nlocation ‚Äì whereas in countries such as the United States, it is available at a sub-state (sometimes even at a sub-zip\ncode) level, in others such as China, only country-level information is available. The carbon intensity figures that we\nuse are yearly averages for the year the model was trained, given that these can evolve over time. In cases when the\nauthors indicated that they used a computing infrastructure internal to a company, we consulted company reports and\npublications (e.g. [16, 38]) to obtain more precise information regarding the carbon intensity, including the usage of\nlocal renewable energy sources. In cases when models were trained on commercial cloud computing platforms such\nas Google Cloud or Amazon Web Services (AWS), we used the information provided by the companies themselves to\nestimate emission factors [4, 18].\nHardware power. In order to calculate the power consumption of the hardware used for model training, we refer to\nits Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load. That is,\nthe higher the TDP, the more power is consumed. While in practice GPUs are not always fully utilized during all parts\nof the training process, gathering more precise information regarding real-time power consumption is only possible by\nusing a tool like Code Carbon during the training process [45]. Nonetheless, the TDP-based approach is often used in\npractice when estimating the carbon emissions of AI model training [38] and it remains a fair approximation of the\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\nfor training the models in our sample in Section A.2 of the Appendix.\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\n\"wall time\" of ML model training, since most models were trained on multiple units at once. For instance, if training a\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\nmethane per kWh will emit 28 grams of CO2eq per kWh.\nManuscript pending review\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\ntherefore have an equivalent training time.\n4 DATA ANALYSIS\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\n4.1 What are the main sources of energy used for training ML models?\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\nof that grid, in the face of the large differences between energy sources. For instance, renewable energy sources\nlike hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-\nrenewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive\n(ranging from 360 to 680 gCO2eq/kWh) [24, 44]. That means that the energy source that powers the hardware to train\nML models can result in differences of up to 60 times more CO2eq in terms of total emissions.\nTable 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]\nMain energy source Number of Models Low-Carbon? Average Carbon Intensity\n(gCO2eq/kWh)\nCoal 38 No 512.3\nNatural Gas 23 No 350.5\nHydroelectricity 19 Yes 100.6\nOil 12 No 453.6\nNuclear 3 Yes 147.2\nIn Table 2, we show the principal energy source used by the models from our sample, as well as its average carbon\nintensity. We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and\nnatural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy\nsources like hydroelectricity and nuclear energy 3. While the average carbon intensity used for training the models\nfrom our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still\nleaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss\nthis further in Section 5).\nIn Figure 1, we show the model training locations reported by authors on a country-level, with the median carbon\nintensity of each country indicated below. In terms of the model training locations reported by authors, we found a\nvery imbalanced distribution, with the vast majority of models being trained in a small number of countries ‚Äì half of\nthe models in our sample were trained in the United States (48), followed by China (18), with the rest of the models\ndistributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on\nthis country-level analysis of energy grids used for training the models in our sample, we found that most common\n2We have made the data used for our analysis available in a GitHub repository.\n3Although the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of electricity that currently exists. More\ninformation about nuclear energy and its long-term impacts on the environment can be found in [6] and [49].\nManuscript pending review\n\n6 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ncountries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum,\nwith emissions of 350 gCO2eq/kWh and above. On the other end, the countries with the lowest carbon intensity in our\nsample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which\nhas a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a\ntotal of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly\nindustrialized countries produce the majority of the world‚Äôs greenhouse gases [17].\nFig. 1. Map with the countries where the models in the data were trained, as reported by the authors. The colors code the median\ncarbon intensity of the energy used by the models trained in each country. The legend indicates the number of models trained in\neach country, as well as a colored patch marking the main energy source ‚Äì see bottom of the legend for the values.\nAnother observation that can be made based on our data is that none of the models from our sample were trained in\neither Africa nor South America ‚Äì in fact, the majority of the models from our sample (76) were trained in countries\nrepresenting the Global North. This is consistent with previous work examining the ‚Äòdigital divide‚Äô in ML and observing\nthe centralization of power in the field, which hinders researchers from underrepresented locations and groups from\ncontributing to the field, given the attribution of computing resources [2, 3, 9]. Generally speaking, emissions, matters\nof equity and accessibility are closely connected to those around climate change, and the centralization of resources\nremains a major problem [33, 34].\n4.2 What is the order of magnitude of CO 2 emissions produced by training ML models?\nAs explained in Section 3, there is a linear relationship between the energy consumed and the carbon emissions\nproduced, with the energy source (discussed in the Section above) influencing the magnitude of this relationship. In\nFigure 2, we plot the energy consumed (X axis, logarithmic scale) and the CO2 emitted (Y axis, logarithmic scale) of\nevery model in our data set, color-coded with the main energy source, which are the same as those presented in Table 2.\nFirst, we can observe differences of several orders of magnitude in the energy used by models in our sample, ranging\nfrom just about 10 kWh to more than 10,000 kWh, which results in similar differences in the total quantity of CO 2\nemitted. As expected, the relationship between energy consumed and carbon emitted is largely linear. However, Figure 2\nManuscript pending review\n\n[Image page=6 idx=1 name=Im1.png] Size: 1570x691, Data: 158742 bytes\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 7\nalso shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend,\nwith orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models\ntrained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.\nFig. 2. Estimated energy consumed (kWh) and CO 2 (kg) by each model in the data set, plotted in a log-log scale. Colors indicate the\nprincipal energy source, and the size of the dot carbon intensity. While the relationship between energy and carbon emissions is\nmostly linear, the data show that models trained with less carbon-intensive energy (e.g. hydroelectric) emit orders of magnitude less\ncarbon than those trained using more carbon-intensive energy (e.g. coal).\nFor instance, honing in on the central bottom portion of Figure 2, it can be seen that the models trained using\nhydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that\nconsumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange),\ngiven that the Y axis is on a logarithmic scale. Furthermore, the size of the dots varies as a function of the carbon\nintensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend,\nwith the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy\nconsumed. This further supports the analysis carried out in Section 4.1, suggesting that the primary energy source used\nfor training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a\nlow-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.\nBesides the primary energy source, carbon emissions are a function of power consumed by the hardware used and\nthe training time. The choice of hardware has a relatively small influence on the large variation of carbon emissions\nthat we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span\nfrom 105 kgCO2eq to even less than 10 kgCO 2eq (see Section A.2 of the appendix for further details). While using\nrenewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining\nfactor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training\ntime.\nManuscript pending review\n\n[Image page=7 idx=1 name=Im2.png] Size: 1237x527, Data: 70720 bytes\n\n8 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n4.3 How do the CO 2 emissions produced by training ML models evolve over time?\nSome recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that\nachieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [50], whereas\nothers have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [37].\nTherefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether\nthere are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically\ncompare whether new generations of ML models from our sample consistently used more energy and emitted more\ncarbon than previous ones.\nFig. 3. CO 2 emitted (in kg) by the all models included in the data set, on a logarithmic scale. Each small marker corresponds to a\nmodel and the large markers indicate the 99 % trimmed mean within each task and year(s) of publication. The error lines cover the\nbootstrapped 99 % confidence intervals. The gray line corresponds to the average over all tasks.\nIn Figure 3, we show the carbon emissions emitted by every model from our sample, disaggregated by task and by\nyear. While we cannot claim that the models and papers in our data set are fully representative of the whole machine\nlearning field, a sample of 95 models spanning 9 years can offer interesting insights. The first observation, related to the\nconclusions from the sections above, is that there is a large variability in the carbon emissions from ML models. Second,\nwe do not observe a consistent trend by which carbon emissions have systematically increased for each individual task.\nThis is the case, for instance, of image classification models (in blue) and question answering models (in yellow) from\nour sample. However, the carbon emissions of machine translation models have peaked in 2019 and has since decreased.\nIf we look at the aggregated data from all tasks (grand average curve, in light gray), we can observe that overall, the\ncarbon emissions per model have increased by a factor of about 100 (two orders of magnitude) from 2012 to recent\nyears, with slight fluctuations, as in 2020. It is important to note that the vertical axis of Figure 3 is on a logarithmic\nscale, in order to reflect the non-linearity introduced by the much larger models from recent years, even though they\ndo not represent a majority in the sample. In fact, the last three years of our sample (2019-2021), have seen models\nthat have emitted orders of magnitude more carbon than before: e.g. there are several vertical outliers in tasks such as\nImage Classification (shown in blue) and Question Answering (in yellow) that have set new records in terms of the\nManuscript pending review\n\n[Image page=8 idx=1 name=Im3.png] Size: 1237x531, Data: 66187 bytes\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 9\ntotal amount of emissions produced by model training, responsible for about 104 and 105 kilograms of CO2eq. There\nare several possible explanations for this, ranging from the widespread adoption of Transformers, which are using\nincreasing amounts of both labeled and unlabeled data [53], as well as computationally-expensive techniques such as\nNAS [59], which result in more carbon emissions [48]. It is hard to disentangle the influence of different factors on\nthe overall carbon emissions of ML models, as well as the relative contributions of different parts of the pre-training\nand fine-tuning process ‚Äì this requires further work, which we discuss in Section 5.3 ‚Äì however, it is worth noting the\nevolution of emissions in recent years, among the papers of our sample.\n4.4 Does more energy and CO 2 lead to better model performance?\nA final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of\ncarbon emitted by models to their performance on benchmark tasks such as image classification, machine translation\nand question answering. We compare the emissions of the models from our sample and their performance on four\ntasks: image recognition on ImageNet [14] (35 models), machine translation for English-French and English-German\non the 2014 WMT Translation tasks [10] (30 models), question answering on the SQuAD 1.1 dataset [39] (10 models),\nand named entity recognition on the CoNLL 2003 dataset [41] (11 models) 4. Our goal with this analysis is to validate\nwhether, generally speaking, the more carbon-intensive models from our sample achieved better performance on\ncommon benchmarks compared to the models with less incurred emissions.\nFigure 4 shows the performance of the models in these four tasks and the associated carbon emissions; we also\nrepresent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based\non our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the\nmodels that achieved the best accuracy for a given amount of CO 2eq emissions. In other words, all the data points\nunder the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite\nproducing the same or more carbon emissions.\nBased on the comparison between carbon emissions and performance, we can observe that the only task in which\nbetter performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top\nright subplot of Figure 4. Still, the relationship is far from being highly correlated (especially given that that the x-axis\nin on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are\nalso the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance\n(achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.\nFor other tasks, the trend is even less clear ‚Äì for instance, for the 30 models evaluated on the WMT translation task\n(top left plot of Figure 4), there is no clear link between CO2 emissions and BLEU score, for neither English-French or\nEnglish-German ‚Äì although the WMT English-French task seems to incur more carbon emissions than the English-\nGerman one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger\nthan the English-German one, which can require a longer training time and thus a higher energy consumption. For the\nfinal two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and\n11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many\nmodels from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD\ndataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).\n4We also had data from a fifth task, object detection, which is represented in Table 1 and Figure 3, but we did not have enough distinct data points to\nenable a meaningful comparison.\nManuscript pending review\n\n10 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nFig. 4. Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score\non the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on\nImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) andNamed Entity Recognition (bottom\nright, evaluated using F1 score on the CoNLL dataset) and the CO2 emitted for training models. The black curves correspond to the\nPareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that\nthe x axis is in logarithmic scale.\nDespite the lack of clear correlation between carbon intensity and model performance, there are some interesting\nobservations to be made based on Figure 4. While we did not expect to see a strong link between these two factors, we\nfind it worth noting that neither consuming more energy nor emitting more carbon seems to necessarily correlate with\na higher accuracy, even in tasks such as Machine Translation, where Transformer models are largely seen to do better\ncompared to other models 5.\n5 DISCUSSION AND FUTURE WORK\nIn the current section, we discuss the significance and the context of our analysis, its limitations, as well as promising\ndirections for future work to improve the transparency of carbon emissions reporting in our field.\n5We find a similar pattern between accuracy and energy consumption, which can be seen in Figure 5 in the Supplementary Materials.\nManuscript pending review\n\n[Image page=10 idx=1 name=Im4.png] Size: 1882x1293, Data: 202085 bytes\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 11\n5.1 Discussion of Results\nWhile the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic\nreporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community\nto acquire a better understanding of its environmental footprint and how to reduce it [28, 38]. Our study is the first\nanalysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source\nto performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions\nassociated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is\nabout 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to\nBeijing. While this may not seem like a large amount, the increase in emissions in recent years ‚Äì from an average of\n487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 ‚Äì as well as\nother trends that we observed in Section 4.3, indicate that the overall emissions due to ML model are rising.\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the\nemissions spectrum. Given that Transformer architectures are increasing in popularity ‚Äì especially in NLP but also for\nseveral Computer Vision tasks ‚Äì having a better idea of their energy consumption, carbon emissions, and the factors\nthat influence them is also crucial part of analyzing the current and future state of our field.\nAn important observation from our analysis is that better performance is not generally achieved by using more\nenergy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent\nyears has brought the possibility to train machine learning models efficiently. Image Classification is the task in our\nsample in which we observed the strongest correlation between performance and emissions. However, even in this\ntask we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the\nleft-hand-side of Figure 4). This highlights the availability of efficient approaches and architectures.\n5.2 Limitations\nThe analyses that we have carried out and the insights that they have provided us are useful towards a better under-\nstanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for\none, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models\nand architectures that exist and the speed at which our field is evolving. As we discussed in Section 3, despite our\nbest efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant\nManuscript pending review\n\n12 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ninformation with us. We also recognize that there are several factors that we are missing in order to be more precise in\nour estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the\nPower Usage Effectiveness (PUE) of the data centers used for model training (i.e. the overhead used for heating, cooling,\nInternet etc.), as well as the real-time energy consumption of the hardware used for training. We also do not account\nfor carbon offsets and power purchase agreements, which intend to bring computing centers closer to carbon neutrality\nand which are often taken into account by providers of cloud compute in their carbon accounting [18]. Despite this, the\napples-to-apples carbon analysis that we carried out in the current study provides useful insights about the current\nstate of carbon emissions in our field, as well as how this has evolved over time in the last 9 years.\nFurthermore, while this study and much of the related work in this field has focused on estimating the carbon\nemissions of model training, there are many pieces of other overall carbon footprint of our field which are still missing:\nfor instance, the carbon emissions of tasks such as data processing, data transfer, and data storage [28], as well as the\ncarbon footprint of manufacturing and maintaining the hardware used for training ML models [19], We are also lacking\ninformation regarding the carbon impact of model development and inference ‚Äì given that a model that is trained a\nsingle time can be deployed on-demand for millions of queries, this can ultimately add up to more emissions than those\nproduced by the initial model training [31]. These are all directions for future research, which we discuss in more detail\nbelow.\n5.3 Future Work\nThere is much interesting and exciting work to be done that would help us better understand the carbon emissions and\nbroader environmental implications of ML. This includes:\nAdditional empirical studies. There is still a lot of uncertainty around, for instance, the relative contribution of added\nparameters of ML to their energy consumption and carbon footprint, as well as the proportion of energy used for\npre-training versus fine-tuning ML models for different tasks and architectures. Furthering this research can benefit the\nfield both from the perspective of sustainability and overall efficiency.\nWidening the scope of ML life-cycle emissions. The overwhelming majority of work in carbon accounting for ML\nmodels has been limited to model training. However, both the upstream emissions (i.e. those incurred by manufacturing\nand transporting the required computing equipment) as well as the downstream ones (i.e. the emissions of model\ndeployment) warrant further exploration and better understanding.\nIncreased standardization and transparency in carbon emissions reporting. As stated in Section 5.2, we put in significant\nefforts in contacting authors and gathering data to carry out our study, and were still lacking much of the necessary\ninformation that we would have liked to have. While certain conferences such as NeurIPS are starting to include\ncompute information in submissions in submission checklists, there is still a lot of variability in carbon reporting, and\nfigures can vary widely depending on what factors are included. Having a more standardized approach, such as ISO\nstandards, to reporting the carbon emissions of ML can help better understand their evolution.\nConsidering the trade-off between sustainability and fairness. The environmental impacts of ML also come with\nconsequences in terms of fairness, given the interplay between fairness and sustainability, most recently discussed\nby Hessenthaler et al. [21]. This includes, for instance, the consideration of the environmental impacts of ML approaches\nwhen benchmarking models [58], but also, conversely, considering the impact on robustness and bias of model distilla-\ntion techniques that improve model efficiency [23, 55]. Generally speaking, given that many advances in ML from last\nManuscript pending review\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 13\nyears can be attributed to training increasingly deep and computationally expensive models, especially in fields such as\nnatural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from\nthe perspective of their energy consumption [8, 15], the attribution of computing resources [2, 3] or the influence of\ncorporate interests on research directions [1, 9].\nWhile discussions regarding the carbon footprint of our daily lives has started to become more common in many\ncommunities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat)\ncontribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts\nof the models we train. We hope that our work encourages better practices and more transparency in reporting the\ncomputational needs of the models and details of the energy used, and that our study will be a meaningful contribution\ntowards a better understanding of our impact as ML researchers and practitioners.\nManuscript pending review\n\n14 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 287‚Äì297.\n[2] Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource\nMachine Translation. arXiv preprint arXiv:2110.03036 (2021).\n[3] Nur Ahmed and Muntasir Wahed. 2020. The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research.\narXiv preprint arXiv:2010.15581 (2020).\n[4] Amazon Web Services. 2021. Delivering Progress Every Day : Amazon‚Äôs 2021 Sustainability Report. https://sustainability.aboutamazon.com/2021-\nsustainability-report.pdf\n[5] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of\nTraining Deep Learning Models. arXiv:2007.03051 [cs.CY]\n[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael. 2010. On the causal dynamics between emissions, nuclear energy,\nrenewable energy, and economic growth. Ecological Economics 69, 11 (2010), 2255‚Äì2260.\n[7] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools. In EMNLP, Workshop SustaiNLP.\n[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?\n . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency . 610‚Äì623.\n[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The Values Encoded in Machine Learning\nResearch. arXiv:2106.15590 [cs.LG]\n[10] Ond≈ôej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post,\nHerve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical\nmachine translation. 12‚Äì58.\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile\ndevices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292‚Äì308.\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE\nconference on computer vision and pattern recognition . Ieee, 248‚Äì255.\n[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In2022 ACM Conference on Fairness, Accountability,\nand Transparency. 1877‚Äì1894.\n[16] Facebook. 2020. Facebook 2020 Sustainability Report. https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf\n[17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens. 2020. This interactive chart shows changes in the world‚Äôs top 10 emitters. (2020).\n[18] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\nIEEE, 854‚Äì867.\n[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy\nand carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020), 1‚Äì43.\n[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging Fairness and Environmental Sustainability in Natural Language\nProcessing. arXiv preprint arXiv:2211.04256 (2022).\n[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon\nOsindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models.\nhttps://doi.org/10.48550/ARXIV.2203.15556\n[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint\narXiv:2010.03058 (2020).\n[24] IEA. 2019. Global Energy & CO2 Status Report 2019. IEA (International Energy Agency): Paris, France (2019). https://www.iea.org/reports/global-\nenergy-co2-status-report-2019\n[25] International Telecommunication Union. 2020. Greenhouse gas emissions trajectories for the information and communication technology sector\ncompatible with the UNFCCC Paris agreement: L. 1470. http://handle.itu.int/11.1002/1000/14084\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700 (2019).\nManuscript pending review\n\n[Image page=14 idx=1 name=Im5.png] Size: 44x100, Data: 6208 bytes\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 15\n[27] Lo√Øc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of computation. Advanced Science\n(2021), 2100707.\n[28] Anne-Laure Ligozat, Julien Lef√®vre, Aur√©lie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for\nenvironment. arXiv preprint arXiv:2110.11822 (2021).\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. Microsoft\nCOCO: Common objects in context. In European conference on computer vision . Springer, 740‚Äì755.\n[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic\naccountability. arXiv preprint arXiv:1911.08354 (2019).\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language\nModel. arXiv preprint arXiv:2211.02001 (2022).\n[32] Jens Malmodin and Dag Lund√©n. 2018. The energy and carbon footprint of the global ICT and E&M sectors 2010‚Äì2015. Sustainability 10, 9 (2018),\n3027.\n[33] Aaditya Mattoo and Arvind Subramanian. 2012. Equity in climate change: an analytical review. World Development 40, 6 (2012), 1083‚Äì1097.\n[34] Jennifer Morgan and David Waskow. 2014. A new look at climate equity in the UNFCCC. Climate Policy 14, 1 (2014), 17‚Äì22.\n[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam. 2021. Towards Quantifying the Carbon\nEmissions of Differentially Private Machine Learning. arXiv preprint arXiv:2107.06946 (2021).\n[36] Copenhagen Centre on Energy Efficiency. 2020. Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet]. https:\n//c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf\n[37] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. arXiv preprint arXiv:2204.05149 (2022).\n[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. arXiv\npreprint arXiv:1606.05250 (2016).\n[40] Henning Rodhe. 1990. A comparison of the contribution of various gases to the greenhouse effect. Science 248, 4960 (1990), 1217‚Äì1219.\n[41] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv\npreprint cs/0306050 (2003).\n[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv preprint arXiv:1910.01108 (2019).\n[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,\nArun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali\nBers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization.\nhttps://doi.org/10.48550/ARXIV.2110.08207\n[44] Steffen Schl√∂mer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims,\nPete Smith, et al. 2014. Annex III: Technology-specific cost and performance parameters. In Climate Change 2014: Mitigation of Climate Change:\nContribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change . Cambridge University Press,\n1329‚Äì1356.\n[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54‚Äì63.\n[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Mu√±oz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert\nNerenberg, Andrea Armani, Kasey M. Faust, and Manish Kumar. 2021. Conference demographics and footprint changed by virtual platforms. Nature\nSustainability 2398-9629 (2021).\n[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n[49] Siddharth Suman. 2018. Hybrid nuclear-renewable energy systems: A review. Journal of Cleaner Production 181 (2018), 166‚Äì177.\n[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint\narXiv:2007.05558 (2020).\n[51] Tristan Tr√©baol. 2020. CUMULATOR ‚Äî a tool to quantify and report the carbon footprint of machine learning computations and communication in\nacademia and healthcare. Technical Report.\n[52] United States Energy Information Administration. 2012-2021. Detailed State Data. https://www.eia.gov/electricity/data/state/\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information processing systems . 5998‚Äì6008.\nManuscript pending review\n\n16 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[55] Guangxuan Xu and Qingyuan Hu. 2022. Can model compression improve NLP fairness. arXiv preprint arXiv:2201.08542 (2022).\n[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh. 2021. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine\nTranslation. arXiv preprint arXiv:2109.12584 (2021).\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: Open Pre-trained Transformer Language Models. https://doi.org/10.48550/ARXIV.2205.01068\n[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for responsible natural\nlanguage processing. arXiv preprint arXiv:2002.05829 (2020).\n[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).\nManuscript pending review\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 17\nA SUPPLEMENTARY MATERIALS\nA.1 Emails sent to authors\nSubject: Information Request: Computing Infrastructure Used in your Paper\nHello,\nMy name is XXXX and I am a researcher working on the environmental impact of Machine Learning.\nI am trying to gather data regarding the carbon footprint of recent state-of-the-art research\npapers. This will help the ML community get a better idea of how much CO2 we are emitting when\ntraining models.\nIn order to help me on my mission, I was hoping you could give me more information about your\npaper entitled YYYY.\nMore specifically, could you tell me:\n- Where it was trained? If it was on a local computing cluster, could you tell me the location\nof the cluster? And if it was trained on the cloud, could you indicate the provider and server\nregion (e.g. \"Microsoft Azure, us-east1\")?\n- What hardware you used\n- The total training time of your models?\nThank you very much for this information,\nXXXX\nA.2 Information regarding training hardware\nTable 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and\ntheir Thermal Design Power (TDP).\nModel Number of models TDP Quantity used\nTesla V100 30 300 W 1-128\nTPU v3 9 450 W 1-1024\nRTX 2080 Ti 8 250 W 4-16\nTesla M40 5 250 W 8\nGTX 1080 4 180 W 1-8\nIn Table 3, we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by\nthe number of papers that used them, the range of quantities used, and their TDP. The Tesla V100 was by far the most\npopular piece of hardware, representing almost a third of the papers, followed by the TPU v3. The TDP of the hardware\nused in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU\nv3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and\nTPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of\nGPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize\na single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly. We\nanalyze the connection between energy usage and performance on different ML tasks in ¬ß 4.4, in order to determine\nwhether higher energy consumption helps achieve better performance in different ML tasks.\nManuscript pending review\n\n18 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nA.3 Energy Consumption by Task\nIn Figure 5 below, we plot the same four tasks as in Figure 3, representing the energy consumed instead of the\nCO2 emitted. We find largely similar trends as the ones we describe in Section 4.4, with better performance on tasks\nlike machine translation and image classification not necessarily being contingent on higher energy consumption.\nFig. 5. Comparison of the performance achieved by each model trained on Machine Translation tasks (BLEU score) and Image\nClassification (top-1 accuracy), and the energy consumed.\nManuscript pending review\n\n[Image page=18 idx=1 name=Im6.png] Size: 1876x1293, Data: 202136 bytes\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 19\nA.4 Carbon intensity over time\nIn Figure 6, we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the\nnumber of models trained with each energy source. We observe that, despite the need to address the climate crisis by\nusing cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of\nmodels trained with cleaner energy. On the contrary, we do observe a stark increase of models trained with coal.\n(a) Carbon intensity of the models per year and energy source.\n(b) Number of models trained with each energy source per year.\nFig. 6. Carbon intensity and energy sources over the years.\nManuscript pending review\n\n[Image page=19 idx=1 name=Im7.png] Size: 1239x531, Data: 51232 bytes\n\n[Image page=19 idx=2 name=Im8.png] Size: 1229x531, Data: 25028 bytes", "metadata": {"url": "https://arxiv.org/pdf/2302.08476", "type": "paper", "year": "2023"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine\nLearning\nALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada\nALEX HERNANDEZ-GARCIA, Mila, Universit√© de Montr√©al, Montreal, Canada\nMachine learning (ML) requires using energy to carry out computations during the model training process. The generation of this\nenergy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source.\nExisting research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not\nadequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95\nML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the\nenergy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model\nperformance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized\nrepository for reporting and tracking these emissions.\n1 INTRODUCTION\nIn recent years, machine learning (ML) models have achieved high performance in a multitude of tasks such as\nimage classification, machine translation, and object detection. However, this progress also comes with a cost in terms\nof energy, since developing and deploying ML models requires access to computational resources such as Graphical\nProcessing Units (GPUs) and therefore energy to power them. In turn, producing this energy comes with a cost to the\nenvironment, given that energy generation often entails the emission of greenhouse gases (GHG) such as carbon dioxide\n(CO2) [40]. On a global scale, electricity generation represents over a quarter of the global GHG emissions, adding up to\n33.1 gigatonnes of CO2 in 2019 [24]. Recent estimates put the contribution of the information and communications\ntechnology (ICT) sector ‚Äì which includes the data centers, devices and networks used for training and deploying ML\nmodels ‚Äì at 2‚Äì6 % of global GHG emissions, although the exact number is still debated [ 25, 32, 36]. In fact, there is\nlimited information about the overall energy consumption and carbon footprint of our field, how it is evolving, and\nhow it correlates with performance on different tasks.\nThe goal of the current paper is to analyze the main factors influencing the carbon emissions of our field, to study\nthe evolution across time, and to contribute towards a better understanding of the carbon emissions generated by ML\nmodels trained on different tasks and as a function of their performance. As such, our research aims to answer the\nfollowing research questions:\n(1) What are the main sources of energy used for training ML models?\n(2) What is the order of magnitude of CO 2 emissions produced by training ML models?\n(3) How do the CO 2 emissions produced by training ML models evolve over time?\n(4) Does more energy and CO 2 lead to better model performance?\nAuthors‚Äô addresses: Alexandra Sasha Luccioni, Hugging Face, Montreal, Canada, sasha.luccioni@huggingface.co; Alex Hernandez-Garcia, Mila, Universit√©\nde Montr√©al, Montreal, Canada, alex.hernandez-garcia@mila.quebec.\n2023. Manuscript pending review\n1\narXiv:2302.08476v1  [cs.LG]  16 Feb 2023", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine\nLearning\nALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada\nALEX HERNANDEZ-GARCIA, Mila, Universit√© de Montr√©al, Montreal, Canada\nMachine learning (ML) requires using energy to carry out computations during the model training process.", "metadata": {}}, {"text": "The generation of this\nenergy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source.", "metadata": {}}, {"text": "Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not\nadequately represent the diversity of ML models and tasks.", "metadata": {}}, {"text": "In the current study, we present a survey of the carbon emissions of 95\nML models across time and different tasks in natural language processing and computer vision.", "metadata": {}}, {"text": "We analyze them in terms of the\nenergy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model\nperformance.", "metadata": {}}, {"text": "We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized\nrepository for reporting and tracking these emissions.", "metadata": {}}, {"text": "1 INTRODUCTION\nIn recent years, machine learning (ML) models have achieved high performance in a multitude of tasks such as\nimage classification, machine translation, and object detection.", "metadata": {}}, {"text": "However, this progress also comes with a cost in terms\nof energy, since developing and deploying ML models requires access to computational resources such as Graphical\nProcessing Units (GPUs) and therefore energy to power them.", "metadata": {}}, {"text": "In turn, producing this energy comes with a cost to the\nenvironment, given that energy generation often entails the emission of greenhouse gases (GHG) such as carbon dioxide\n(CO2) [40].", "metadata": {}}, {"text": "On a global scale, electricity generation represents over a quarter of the global GHG emissions, adding up to\n33.1 gigatonnes of CO2 in 2019 [24].", "metadata": {}}, {"text": "Recent estimates put the contribution of the information and communications\ntechnology (ICT) sector ‚Äì which includes the data centers, devices and networks used for training and deploying ML\nmodels ‚Äì at 2‚Äì6 % of global GHG emissions, although the exact number is still debated [ 25, 32, 36].", "metadata": {}}, {"text": "In fact, there is\nlimited information about the overall energy consumption and carbon footprint of our field, how it is evolving, and\nhow it correlates with performance on different tasks.", "metadata": {}}, {"text": "The goal of the current paper is to analyze the main factors influencing the carbon emissions of our field, to study\nthe evolution across time, and to contribute towards a better understanding of the carbon emissions generated by ML\nmodels trained on different tasks and as a function of their performance.", "metadata": {}}, {"text": "As such, our research aims to answer the\nfollowing research questions:\n(1) What are the main sources of energy used for training ML models?", "metadata": {}}, {"text": "(2) What is the order of magnitude of CO 2 emissions produced by training ML models?", "metadata": {}}, {"text": "(3) How do the CO 2 emissions produced by training ML models evolve over time?", "metadata": {}}, {"text": "(4) Does more energy and CO 2 lead to better model performance?", "metadata": {}}, {"text": "Authors‚Äô addresses: Alexandra Sasha Luccioni, Hugging Face, Montreal, Canada, sasha.luccioni@huggingface.co;", "metadata": {}}, {"text": "Alex Hernandez-Garcia, Mila, Universit√©\nde Montr√©al, Montreal, Canada, alex.hernandez-garcia@mila.quebec.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Manuscript pending review\n1\narXiv:2302.08476v1  [cs.LG]  16 Feb 2023", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "2 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nWe start our article with a survey of related work in Section 2, followed by a presentation of our methodology\nin Section 3. In Section 4 we present our analysis, and we conclude with our proposals for future work, including a\ncentralized hub for reporting the carbon footprint of machine learning..\n2 RELATED WORK\nMeasuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering\nmomentum in recent years. In the current section, we present several directions pursued in this domain, from empirical\nstudies of specific models to the development of efficient algorithms and hardware.\nEmpirical studies on carbon emissions. A large proportion of research has focused on estimating the carbon emissions\nof specific model architectures and/or comparing the carbon emissions of two or more models and approaches. The\nfirst paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large\nTransformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the\nlifetime emissions of five US cars. [48]. This perspective has since been explored further via analyses of the carbon\nfootprint of different neural network architectures [31, 37, 38] and the relative efficiency of different methods [35, 56].\nThese empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research\nareas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain\nunexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale\nand variation of carbon emissions in our community.\nTools and approaches for measuring carbon emissions. Developing standardized approaches for estimating the carbon\nemissions of model training has also been the focus of much work [5, 20, 26, 27, 30, 45, 51]. As a result, there are several\ntools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the\nmodel training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate\nof the amount of carbon emitted. However, a recent study on different carbon estimation tools concluded that the\nestimates produced by different tools vary significantly and consistently under-report emissions [7]. To date, there\nis no single, accepted approach for estimating the carbon emissions of the field, making standardized reporting and\ncomparisons difficult [31].\nBroader impacts of ML models. Several papers have been written in recent years regarding the broader societal impacts\nof ML models, which includes their environmental footprint. This spans research on how the size and computational\ndemands of ML models in general [50] and large language models in particular [8, 11] have grown in recent years. Many\nstrategies and directions forward have been proposed, ranging from advocating for more environmentally-conscious\npractice of AI [46] to adopting a sustainability mindset for the community [54]. However, while the documentation\nof aspects such as bias and safety has begun to be described in reports and articles accompanying certain recent ML\nmodels (e.g. [12, 22]), environmental impacts have yet to be consistently tracked and reported. Notable exceptions\ninclude recent language models such as OPT [57], T0 [43] and BLOOM [31].\nEfficient algorithms and hardware. A related and complementary direction of research is the development of more\nefficient model architectures and approaches. For instance, approaches such as Eyeriss [13] and DistilBERT [42] have\nmade significant progress in terms of computing efficiency, enabling faster training and inference, which results in less\nenergy usage and, indirectly, less carbon emissions, during model training. This research is gathering attention within\nthe community, with workshops such as SustaiNLP and EMC2 growing in scope and popularity, although efficiency\nManuscript pending review", "sentences": [{"text": "2 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nWe start our article with a survey of related work in Section 2, followed by a presentation of our methodology\nin Section 3.", "metadata": {}}, {"text": "In Section 4 we present our analysis, and we conclude with our proposals for future work, including a\ncentralized hub for reporting the carbon footprint of machine learning..", "metadata": {}}, {"text": "2 RELATED WORK\nMeasuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering\nmomentum in recent years.", "metadata": {}}, {"text": "In the current section, we present several directions pursued in this domain, from empirical\nstudies of specific models to the development of efficient algorithms and hardware.", "metadata": {}}, {"text": "Empirical studies on carbon emissions.", "metadata": {}}, {"text": "A large proportion of research has focused on estimating the carbon emissions\nof specific model architectures and/or comparing the carbon emissions of two or more models and approaches.", "metadata": {}}, {"text": "The\nfirst paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large\nTransformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the\nlifetime emissions of five US cars.", "metadata": {}}, {"text": "[48].", "metadata": {}}, {"text": "This perspective has since been explored further via analyses of the carbon\nfootprint of different neural network architectures [31, 37, 38] and the relative efficiency of different methods [35, 56].", "metadata": {}}, {"text": "These empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research\nareas (i.e.", "metadata": {}}, {"text": "Natural Language Processing), and there are many aspects of the emissions of model training that remain\nunexplored.", "metadata": {}}, {"text": "In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale\nand variation of carbon emissions in our community.", "metadata": {}}, {"text": "Tools and approaches for measuring carbon emissions.", "metadata": {}}, {"text": "Developing standardized approaches for estimating the carbon\nemissions of model training has also been the focus of much work [5, 20, 26, 27, 30, 45, 51].", "metadata": {}}, {"text": "As a result, there are several\ntools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the\nmodel training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate\nof the amount of carbon emitted.", "metadata": {}}, {"text": "However, a recent study on different carbon estimation tools concluded that the\nestimates produced by different tools vary significantly and consistently under-report emissions [7].", "metadata": {}}, {"text": "To date, there\nis no single, accepted approach for estimating the carbon emissions of the field, making standardized reporting and\ncomparisons difficult [31].", "metadata": {}}, {"text": "Broader impacts of ML models.", "metadata": {}}, {"text": "Several papers have been written in recent years regarding the broader societal impacts\nof ML models, which includes their environmental footprint.", "metadata": {}}, {"text": "This spans research on how the size and computational\ndemands of ML models in general [50] and large language models in particular [8, 11] have grown in recent years.", "metadata": {}}, {"text": "Many\nstrategies and directions forward have been proposed, ranging from advocating for more environmentally-conscious\npractice of AI [46] to adopting a sustainability mindset for the community [54].", "metadata": {}}, {"text": "However, while the documentation\nof aspects such as bias and safety has begun to be described in reports and articles accompanying certain recent ML\nmodels (e.g.", "metadata": {}}, {"text": "[12, 22]), environmental impacts have yet to be consistently tracked and reported.", "metadata": {}}, {"text": "Notable exceptions\ninclude recent language models such as OPT [57], T0 [43] and BLOOM [31].", "metadata": {}}, {"text": "Efficient algorithms and hardware.", "metadata": {}}, {"text": "A related and complementary direction of research is the development of more\nefficient model architectures and approaches.", "metadata": {}}, {"text": "For instance, approaches such as Eyeriss [13] and DistilBERT [42] have\nmade significant progress in terms of computing efficiency, enabling faster training and inference, which results in less\nenergy usage and, indirectly, less carbon emissions, during model training.", "metadata": {}}, {"text": "This research is gathering attention within\nthe community, with workshops such as SustaiNLP and EMC2 growing in scope and popularity, although efficiency\nManuscript pending review", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 3\nhas yet to be a central consideration when it comes to evaluating and comparing models. However, energy-efficient\nbenchmarks such as HULK [58] have also been proposed, which take computational requirements and environmental\nimpacts into account during model evaluation, allowing a comparison of models based on multiple criteria.\nOther aspects of the carbon impact of ML. Finally, efforts have been made to quantify other factors that have an\ninfluence on the overall carbon footprint of the field of ML, including in-person versus virtual conference attendance [47],\nthe manufacturing of computing hardware [ 19], life cycle analysis of the entire ML development and deployment\ncycle [28], as well as some initial studies regarding the carbon footprint of model deployment in production settings [31].\nThe relative contribution of each of these factors is still unclear, which suggests that further research is needed in order\nto further disentangle these factors.\n3 METHODOLOGY\nAs stated in Section 1, the goal of this paper is descriptive ‚Äì to observe the evolution of the carbon emissions of our\nfield of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In\nthis section, we present the different aspects and details of our methodology.\n3.1 Data collection\nIn order to gather data from a diverse set of ML models from a variety of domains and tasks, we leveraged the dataset\ncollected by Thompson et al. [50] in the scope of a recent study on the computational requirements of ML. From this\ndataset, we equally sampled 500 papers published from 2012 to 2021 spanning 5 tasks: Image Classification, Object\nDetection, Machine Translation, Question Answering and Named Entity Recognition. We then contacted the first author\nof each of the papers and asked them to provide missing training details regarding their model (See Supplementary\nMaterials A.1 for the email text). We were able to collect information for a total of 95 models from 77 papers (since\nsome of the papers trained more than one model), which represents an author response rate of 15.4 %.\nTable 1. Summary of the models analyzed in our study\nTask Dataset Number of Models Publication dates\nImage Classification ImageNet [14] 35 2012-2021\nMachine Translation WMT2014 [10] 30 2016-2021\nNamed Entity Recognition CoNLL 2003 [41] 11 2015-2021\nQuestion Answering SQuAD 1.1 [39] 10 2016-2021\nObject Detection MS COCO [29] 9 2019-2021\nThe models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of\nconferences and journals. They all represent novel architectures at the time of publication, achieving high performance\nin their respective tasks: on average, the models are within 8 % of SOTA performance according to Papers With Code\nleaderboards at the time of their publication . This sample represents the largest amount of information regarding\nthe carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of\nangles, which we present in Section 4. In the remaining of this section, we describe our method for estimating carbon\nemissions.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 3\nhas yet to be a central consideration when it comes to evaluating and comparing models.", "metadata": {}}, {"text": "However, energy-efficient\nbenchmarks such as HULK [58] have also been proposed, which take computational requirements and environmental\nimpacts into account during model evaluation, allowing a comparison of models based on multiple criteria.", "metadata": {}}, {"text": "Other aspects of the carbon impact of ML.", "metadata": {}}, {"text": "Finally, efforts have been made to quantify other factors that have an\ninfluence on the overall carbon footprint of the field of ML, including in-person versus virtual conference attendance [47],\nthe manufacturing of computing hardware [ 19], life cycle analysis of the entire ML development and deployment\ncycle [28], as well as some initial studies regarding the carbon footprint of model deployment in production settings [31].", "metadata": {}}, {"text": "The relative contribution of each of these factors is still unclear, which suggests that further research is needed in order\nto further disentangle these factors.", "metadata": {}}, {"text": "3 METHODOLOGY\nAs stated in Section 1, the goal of this paper is descriptive ‚Äì to observe the evolution of the carbon emissions of our\nfield of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models.", "metadata": {}}, {"text": "In\nthis section, we present the different aspects and details of our methodology.", "metadata": {}}, {"text": "3.1 Data collection\nIn order to gather data from a diverse set of ML models from a variety of domains and tasks, we leveraged the dataset\ncollected by Thompson et al.", "metadata": {}}, {"text": "[50] in the scope of a recent study on the computational requirements of ML.", "metadata": {}}, {"text": "From this\ndataset, we equally sampled 500 papers published from 2012 to 2021 spanning 5 tasks: Image Classification, Object\nDetection, Machine Translation, Question Answering and Named Entity Recognition.", "metadata": {}}, {"text": "We then contacted the first author\nof each of the papers and asked them to provide missing training details regarding their model (See Supplementary\nMaterials A.1 for the email text).", "metadata": {}}, {"text": "We were able to collect information for a total of 95 models from 77 papers (since\nsome of the papers trained more than one model), which represents an author response rate of 15.4 %.", "metadata": {}}, {"text": "Table 1.", "metadata": {}}, {"text": "Summary of the models analyzed in our study\nTask Dataset Number of Models Publication dates\nImage Classification ImageNet [14] 35 2012-2021\nMachine Translation WMT2014 [10] 30 2016-2021\nNamed Entity Recognition CoNLL 2003 [41] 11 2015-2021\nQuestion Answering SQuAD 1.1 [39] 10 2016-2021\nObject Detection MS COCO [29] 9 2019-2021\nThe models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of\nconferences and journals.", "metadata": {}}, {"text": "They all represent novel architectures at the time of publication, achieving high performance\nin their respective tasks: on average, the models are within 8 % of SOTA performance according to Papers With Code\nleaderboards at the time of their publication .", "metadata": {}}, {"text": "This sample represents the largest amount of information regarding\nthe carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of\nangles, which we present in Section 4.", "metadata": {}}, {"text": "In the remaining of this section, we describe our method for estimating carbon\nemissions.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "4 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n3.2 Estimating carbon emissions\nThe unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents. This unit\nallows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of\nCO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) 1.\nThe amount of CO2eq (ùê∂) emitted during model training can be decomposed into three relevant factors: the power\nconsumption of the hardware used ( ùëÉ), the training time ( ùëá ) and the carbon intensity of the energy grid ( ùêº); or\nequivalently, the energy consumed (ùê∏) and the carbon intensity:\nùê∂ = ùëÉ √ó ùëá √ó ùêº = ùê∏ √ó ùêº . (1)\nFor instance, a model trained on a single GPU consuming 300 W for 100 hours on a grid that emits 500 gCO2eq/kWh will\nemit 0.3 kW √ó 100 h √ó 500 g/kWh = 15000 g = 15 kg of CO2eq. The same model trained on a less carbon-intensive\nenergy grid, emitting only 100 gCO2eq/kWh, will only emit 0.3 √ó 100 √ó 100 = 3000 g = 3 kg of CO2eq, i.e. five times\nless overall. In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the\nlocation of the computer or server where their model was trained (either cloud or local), the hardware used, and the\ntotal model training time. We describe how we estimate each of the relevant factors in the paragraphs below:\nCarbon Intensity. Based on the training location provided by authors, we were able to estimate the carbon intensity\nof the energy grid that was utilized, based on publicly-available sources such as the International Energy Agency and\nthe Energy Information Administration. The granularity of information available ranges widely depending on the\nlocation ‚Äì whereas in countries such as the United States, it is available at a sub-state (sometimes even at a sub-zip\ncode) level, in others such as China, only country-level information is available. The carbon intensity figures that we\nuse are yearly averages for the year the model was trained, given that these can evolve over time. In cases when the\nauthors indicated that they used a computing infrastructure internal to a company, we consulted company reports and\npublications (e.g. [16, 38]) to obtain more precise information regarding the carbon intensity, including the usage of\nlocal renewable energy sources. In cases when models were trained on commercial cloud computing platforms such\nas Google Cloud or Amazon Web Services (AWS), we used the information provided by the companies themselves to\nestimate emission factors [4, 18].\nHardware power. In order to calculate the power consumption of the hardware used for model training, we refer to\nits Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load. That is,\nthe higher the TDP, the more power is consumed. While in practice GPUs are not always fully utilized during all parts\nof the training process, gathering more precise information regarding real-time power consumption is only possible by\nusing a tool like Code Carbon during the training process [45]. Nonetheless, the TDP-based approach is often used in\npractice when estimating the carbon emissions of AI model training [38] and it remains a fair approximation of the\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\nfor training the models in our sample in Section A.2 of the Appendix.\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\n\"wall time\" of ML model training, since most models were trained on multiple units at once. For instance, if training a\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\nmethane per kWh will emit 28 grams of CO2eq per kWh.\nManuscript pending review", "sentences": [{"text": "4 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n3.2 Estimating carbon emissions\nThe unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents.", "metadata": {}}, {"text": "This unit\nallows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of\nCO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) 1.", "metadata": {}}, {"text": "The amount of CO2eq (ùê∂) emitted during model training can be decomposed into three relevant factors: the power\nconsumption of the hardware used ( ùëÉ), the training time ( ùëá ) and the carbon intensity of the energy grid ( ùêº);", "metadata": {}}, {"text": "or\nequivalently, the energy consumed (ùê∏) and the carbon intensity:\nùê∂ = ùëÉ √ó ùëá √ó ùêº = ùê∏ √ó ùêº .", "metadata": {}}, {"text": "(1)\nFor instance, a model trained on a single GPU consuming 300 W for 100 hours on a grid that emits 500 gCO2eq/kWh will\nemit 0.3 kW √ó 100 h √ó 500 g/kWh = 15000 g = 15 kg of CO2eq.", "metadata": {}}, {"text": "The same model trained on a less carbon-intensive\nenergy grid, emitting only 100 gCO2eq/kWh, will only emit 0.3 √ó 100 √ó 100 = 3000 g = 3 kg of CO2eq, i.e.", "metadata": {}}, {"text": "five times\nless overall.", "metadata": {}}, {"text": "In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the\nlocation of the computer or server where their model was trained (either cloud or local), the hardware used, and the\ntotal model training time.", "metadata": {}}, {"text": "We describe how we estimate each of the relevant factors in the paragraphs below:\nCarbon Intensity.", "metadata": {}}, {"text": "Based on the training location provided by authors, we were able to estimate the carbon intensity\nof the energy grid that was utilized, based on publicly-available sources such as the International Energy Agency and\nthe Energy Information Administration.", "metadata": {}}, {"text": "The granularity of information available ranges widely depending on the\nlocation ‚Äì whereas in countries such as the United States, it is available at a sub-state (sometimes even at a sub-zip\ncode) level, in others such as China, only country-level information is available.", "metadata": {}}, {"text": "The carbon intensity figures that we\nuse are yearly averages for the year the model was trained, given that these can evolve over time.", "metadata": {}}, {"text": "In cases when the\nauthors indicated that they used a computing infrastructure internal to a company, we consulted company reports and\npublications (e.g.", "metadata": {}}, {"text": "[16, 38]) to obtain more precise information regarding the carbon intensity, including the usage of\nlocal renewable energy sources.", "metadata": {}}, {"text": "In cases when models were trained on commercial cloud computing platforms such\nas Google Cloud or Amazon Web Services (AWS), we used the information provided by the companies themselves to\nestimate emission factors [4, 18].", "metadata": {}}, {"text": "Hardware power.", "metadata": {}}, {"text": "In order to calculate the power consumption of the hardware used for model training, we refer to\nits Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load.", "metadata": {}}, {"text": "That is,\nthe higher the TDP, the more power is consumed.", "metadata": {}}, {"text": "While in practice GPUs are not always fully utilized during all parts\nof the training process, gathering more precise information regarding real-time power consumption is only possible by\nusing a tool like Code Carbon during the training process [45].", "metadata": {}}, {"text": "Nonetheless, the TDP-based approach is often used in\npractice when estimating the carbon emissions of AI model training [38] and it remains a fair approximation of the\nactual energy consumption of many hardware models.", "metadata": {}}, {"text": "We provide more information about TDP and the hardware used\nfor training the models in our sample in Section A.2 of the Appendix.", "metadata": {}}, {"text": "Training Time.", "metadata": {}}, {"text": "Training time was computed as the total number of hardware hours, which is different from the\n\"wall time\" of ML model training, since most models were trained on multiple units at once.", "metadata": {}}, {"text": "For instance, if training a\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\nmethane per kWh will emit 28 grams of CO2eq per kWh.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\ntherefore have an equivalent training time.\n4 DATA ANALYSIS\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\n4.1 What are the main sources of energy used for training ML models?\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\nof that grid, in the face of the large differences between energy sources. For instance, renewable energy sources\nlike hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-\nrenewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive\n(ranging from 360 to 680 gCO2eq/kWh) [24, 44]. That means that the energy source that powers the hardware to train\nML models can result in differences of up to 60 times more CO2eq in terms of total emissions.\nTable 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]\nMain energy source Number of Models Low-Carbon? Average Carbon Intensity\n(gCO2eq/kWh)\nCoal 38 No 512.3\nNatural Gas 23 No 350.5\nHydroelectricity 19 Yes 100.6\nOil 12 No 453.6\nNuclear 3 Yes 147.2\nIn Table 2, we show the principal energy source used by the models from our sample, as well as its average carbon\nintensity. We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and\nnatural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy\nsources like hydroelectricity and nuclear energy 3. While the average carbon intensity used for training the models\nfrom our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still\nleaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss\nthis further in Section 5).\nIn Figure 1, we show the model training locations reported by authors on a country-level, with the median carbon\nintensity of each country indicated below. In terms of the model training locations reported by authors, we found a\nvery imbalanced distribution, with the vast majority of models being trained in a small number of countries ‚Äì half of\nthe models in our sample were trained in the United States (48), followed by China (18), with the rest of the models\ndistributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on\nthis country-level analysis of energy grids used for training the models in our sample, we found that most common\n2We have made the data used for our analysis available in a GitHub repository.\n3Although the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of electricity that currently exists. More\ninformation about nuclear energy and its long-term impacts on the environment can be found in [6] and [49].\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ;", "metadata": {}}, {"text": "a model using 8 GPUs for 48 hours will\ntherefore have an equivalent training time.", "metadata": {}}, {"text": "4 DATA ANALYSIS\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.", "metadata": {}}, {"text": "4.1 What are the main sources of energy used for training ML models?", "metadata": {}}, {"text": "The primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\nof that grid, in the face of the large differences between energy sources.", "metadata": {}}, {"text": "For instance, renewable energy sources\nlike hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-\nrenewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive\n(ranging from 360 to 680 gCO2eq/kWh) [24, 44].", "metadata": {}}, {"text": "That means that the energy source that powers the hardware to train\nML models can result in differences of up to 60 times more CO2eq in terms of total emissions.", "metadata": {}}, {"text": "Table 2.", "metadata": {}}, {"text": "Main Energy Sources for the models analyzed and their carbon intensities [24, 52]\nMain energy source Number of Models Low-Carbon?", "metadata": {}}, {"text": "Average Carbon Intensity\n(gCO2eq/kWh)\nCoal 38 No 512.3\nNatural Gas 23 No 350.5\nHydroelectricity 19 Yes 100.6\nOil 12 No 453.6\nNuclear 3 Yes 147.2\nIn Table 2, we show the principal energy source used by the models from our sample, as well as its average carbon\nintensity.", "metadata": {}}, {"text": "We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and\nnatural gas as their primary energy source.", "metadata": {}}, {"text": "whereas less than a quarter of the models (34) used low-carbon energy\nsources like hydroelectricity and nuclear energy 3.", "metadata": {}}, {"text": "While the average carbon intensity used for training the models\nfrom our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still\nleaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss\nthis further in Section 5).", "metadata": {}}, {"text": "In Figure 1, we show the model training locations reported by authors on a country-level, with the median carbon\nintensity of each country indicated below.", "metadata": {}}, {"text": "In terms of the model training locations reported by authors, we found a\nvery imbalanced distribution, with the vast majority of models being trained in a small number of countries ‚Äì half of\nthe models in our sample were trained in the United States (48), followed by China (18), with the rest of the models\ndistributed across 9 other countries, with only a few papers in each.", "metadata": {}}, {"text": "Regarding the primary energy sources, based on\nthis country-level analysis of energy grids used for training the models in our sample, we found that most common\n2We have made the data used for our analysis available in a GitHub repository.", "metadata": {}}, {"text": "3Although the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of electricity that currently exists.", "metadata": {}}, {"text": "More\ninformation about nuclear energy and its long-term impacts on the environment can be found in [6] and [49].", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "6 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ncountries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum,\nwith emissions of 350 gCO2eq/kWh and above. On the other end, the countries with the lowest carbon intensity in our\nsample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which\nhas a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a\ntotal of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly\nindustrialized countries produce the majority of the world‚Äôs greenhouse gases [17].\nFig. 1. Map with the countries where the models in the data were trained, as reported by the authors. The colors code the median\ncarbon intensity of the energy used by the models trained in each country. The legend indicates the number of models trained in\neach country, as well as a colored patch marking the main energy source ‚Äì see bottom of the legend for the values.\nAnother observation that can be made based on our data is that none of the models from our sample were trained in\neither Africa nor South America ‚Äì in fact, the majority of the models from our sample (76) were trained in countries\nrepresenting the Global North. This is consistent with previous work examining the ‚Äòdigital divide‚Äô in ML and observing\nthe centralization of power in the field, which hinders researchers from underrepresented locations and groups from\ncontributing to the field, given the attribution of computing resources [2, 3, 9]. Generally speaking, emissions, matters\nof equity and accessibility are closely connected to those around climate change, and the centralization of resources\nremains a major problem [33, 34].\n4.2 What is the order of magnitude of CO 2 emissions produced by training ML models?\nAs explained in Section 3, there is a linear relationship between the energy consumed and the carbon emissions\nproduced, with the energy source (discussed in the Section above) influencing the magnitude of this relationship. In\nFigure 2, we plot the energy consumed (X axis, logarithmic scale) and the CO2 emitted (Y axis, logarithmic scale) of\nevery model in our data set, color-coded with the main energy source, which are the same as those presented in Table 2.\nFirst, we can observe differences of several orders of magnitude in the energy used by models in our sample, ranging\nfrom just about 10 kWh to more than 10,000 kWh, which results in similar differences in the total quantity of CO 2\nemitted. As expected, the relationship between energy consumed and carbon emitted is largely linear. However, Figure 2\nManuscript pending review", "sentences": [{"text": "6 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ncountries where model training was carried out (e.g.", "metadata": {}}, {"text": "the US and China), are on the high end of the carbon spectrum,\nwith emissions of 350 gCO2eq/kWh and above.", "metadata": {}}, {"text": "On the other end, the countries with the lowest carbon intensity in our\nsample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which\nhas a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a\ntotal of 7 models from our sample.", "metadata": {}}, {"text": "This is similar to patterns in emissions worldwide, where a small number of highly\nindustrialized countries produce the majority of the world‚Äôs greenhouse gases [17].", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "Map with the countries where the models in the data were trained, as reported by the authors.", "metadata": {}}, {"text": "The colors code the median\ncarbon intensity of the energy used by the models trained in each country.", "metadata": {}}, {"text": "The legend indicates the number of models trained in\neach country, as well as a colored patch marking the main energy source ‚Äì see bottom of the legend for the values.", "metadata": {}}, {"text": "Another observation that can be made based on our data is that none of the models from our sample were trained in\neither Africa nor South America ‚Äì in fact, the majority of the models from our sample (76) were trained in countries\nrepresenting the Global North.", "metadata": {}}, {"text": "This is consistent with previous work examining the ‚Äòdigital divide‚Äô in ML and observing\nthe centralization of power in the field, which hinders researchers from underrepresented locations and groups from\ncontributing to the field, given the attribution of computing resources [2, 3, 9].", "metadata": {}}, {"text": "Generally speaking, emissions, matters\nof equity and accessibility are closely connected to those around climate change, and the centralization of resources\nremains a major problem [33, 34].", "metadata": {}}, {"text": "4.2 What is the order of magnitude of CO 2 emissions produced by training ML models?", "metadata": {}}, {"text": "As explained in Section 3, there is a linear relationship between the energy consumed and the carbon emissions\nproduced, with the energy source (discussed in the Section above) influencing the magnitude of this relationship.", "metadata": {}}, {"text": "In\nFigure 2, we plot the energy consumed (X axis, logarithmic scale) and the CO2 emitted (Y axis, logarithmic scale) of\nevery model in our data set, color-coded with the main energy source, which are the same as those presented in Table 2.", "metadata": {}}, {"text": "First, we can observe differences of several orders of magnitude in the energy used by models in our sample, ranging\nfrom just about 10 kWh to more than 10,000 kWh, which results in similar differences in the total quantity of CO 2\nemitted.", "metadata": {}}, {"text": "As expected, the relationship between energy consumed and carbon emitted is largely linear.", "metadata": {}}, {"text": "However, Figure 2\nManuscript pending review", "metadata": {}}], "metadata": {"page": 6}}, {"text": "[Image page=6 idx=1 name=Im1.png] Size: 1570x691, Data: 158742 bytes", "sentences": [{"text": "[Image page=6 idx=1 name=Im1.png] Size: 1570x691, Data: 158742 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 1, "image_name": "Im1.png", "image_width": 1570, "image_height": 691, "attachment_type": "image", "has_image_data": true, "image_data_size": 158742}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 7\nalso shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend,\nwith orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models\ntrained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.\nFig. 2. Estimated energy consumed (kWh) and CO 2 (kg) by each model in the data set, plotted in a log-log scale. Colors indicate the\nprincipal energy source, and the size of the dot carbon intensity. While the relationship between energy and carbon emissions is\nmostly linear, the data show that models trained with less carbon-intensive energy (e.g. hydroelectric) emit orders of magnitude less\ncarbon than those trained using more carbon-intensive energy (e.g. coal).\nFor instance, honing in on the central bottom portion of Figure 2, it can be seen that the models trained using\nhydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that\nconsumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange),\ngiven that the Y axis is on a logarithmic scale. Furthermore, the size of the dots varies as a function of the carbon\nintensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend,\nwith the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy\nconsumed. This further supports the analysis carried out in Section 4.1, suggesting that the primary energy source used\nfor training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a\nlow-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.\nBesides the primary energy source, carbon emissions are a function of power consumed by the hardware used and\nthe training time. The choice of hardware has a relatively small influence on the large variation of carbon emissions\nthat we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span\nfrom 105 kgCO2eq to even less than 10 kgCO 2eq (see Section A.2 of the appendix for further details). While using\nrenewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining\nfactor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training\ntime.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 7\nalso shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend,\nwith orders of magnitude less carbon emissions compared to models trained using coal and gas.", "metadata": {}}, {"text": "In other words, models\ntrained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Estimated energy consumed (kWh) and CO 2 (kg) by each model in the data set, plotted in a log-log scale.", "metadata": {}}, {"text": "Colors indicate the\nprincipal energy source, and the size of the dot carbon intensity.", "metadata": {}}, {"text": "While the relationship between energy and carbon emissions is\nmostly linear, the data show that models trained with less carbon-intensive energy (e.g.", "metadata": {}}, {"text": "hydroelectric) emit orders of magnitude less\ncarbon than those trained using more carbon-intensive energy (e.g.", "metadata": {}}, {"text": "coal).", "metadata": {}}, {"text": "For instance, honing in on the central bottom portion of Figure 2, it can be seen that the models trained using\nhydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that\nconsumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange),\ngiven that the Y axis is on a logarithmic scale.", "metadata": {}}, {"text": "Furthermore, the size of the dots varies as a function of the carbon\nintensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend,\nwith the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy\nconsumed.", "metadata": {}}, {"text": "This further supports the analysis carried out in Section 4.1, suggesting that the primary energy source used\nfor training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a\nlow-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.", "metadata": {}}, {"text": "Besides the primary energy source, carbon emissions are a function of power consumed by the hardware used and\nthe training time.", "metadata": {}}, {"text": "The choice of hardware has a relatively small influence on the large variation of carbon emissions\nthat we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span\nfrom 105 kgCO2eq to even less than 10 kgCO 2eq (see Section A.2 of the appendix for further details).", "metadata": {}}, {"text": "While using\nrenewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining\nfactor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training\ntime.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=Im2.png] Size: 1237x527, Data: 70720 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=Im2.png] Size: 1237x527, Data: 70720 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "Im2.png", "image_width": 1237, "image_height": 527, "attachment_type": "image", "has_image_data": true, "image_data_size": 70720}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "8 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n4.3 How do the CO 2 emissions produced by training ML models evolve over time?\nSome recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that\nachieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [50], whereas\nothers have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [37].\nTherefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether\nthere are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically\ncompare whether new generations of ML models from our sample consistently used more energy and emitted more\ncarbon than previous ones.\nFig. 3. CO 2 emitted (in kg) by the all models included in the data set, on a logarithmic scale. Each small marker corresponds to a\nmodel and the large markers indicate the 99 % trimmed mean within each task and year(s) of publication. The error lines cover the\nbootstrapped 99 % confidence intervals. The gray line corresponds to the average over all tasks.\nIn Figure 3, we show the carbon emissions emitted by every model from our sample, disaggregated by task and by\nyear. While we cannot claim that the models and papers in our data set are fully representative of the whole machine\nlearning field, a sample of 95 models spanning 9 years can offer interesting insights. The first observation, related to the\nconclusions from the sections above, is that there is a large variability in the carbon emissions from ML models. Second,\nwe do not observe a consistent trend by which carbon emissions have systematically increased for each individual task.\nThis is the case, for instance, of image classification models (in blue) and question answering models (in yellow) from\nour sample. However, the carbon emissions of machine translation models have peaked in 2019 and has since decreased.\nIf we look at the aggregated data from all tasks (grand average curve, in light gray), we can observe that overall, the\ncarbon emissions per model have increased by a factor of about 100 (two orders of magnitude) from 2012 to recent\nyears, with slight fluctuations, as in 2020. It is important to note that the vertical axis of Figure 3 is on a logarithmic\nscale, in order to reflect the non-linearity introduced by the much larger models from recent years, even though they\ndo not represent a majority in the sample. In fact, the last three years of our sample (2019-2021), have seen models\nthat have emitted orders of magnitude more carbon than before: e.g. there are several vertical outliers in tasks such as\nImage Classification (shown in blue) and Question Answering (in yellow) that have set new records in terms of the\nManuscript pending review", "sentences": [{"text": "8 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n4.3 How do the CO 2 emissions produced by training ML models evolve over time?", "metadata": {}}, {"text": "Some recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that\nachieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [50], whereas\nothers have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [37].", "metadata": {}}, {"text": "Therefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether\nthere are clear trends.", "metadata": {}}, {"text": "Given that the papers from our study span from 2012 to the present time, we aimed to specifically\ncompare whether new generations of ML models from our sample consistently used more energy and emitted more\ncarbon than previous ones.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "CO 2 emitted (in kg) by the all models included in the data set, on a logarithmic scale.", "metadata": {}}, {"text": "Each small marker corresponds to a\nmodel and the large markers indicate the 99 % trimmed mean within each task and year(s) of publication.", "metadata": {}}, {"text": "The error lines cover the\nbootstrapped 99 % confidence intervals.", "metadata": {}}, {"text": "The gray line corresponds to the average over all tasks.", "metadata": {}}, {"text": "In Figure 3, we show the carbon emissions emitted by every model from our sample, disaggregated by task and by\nyear.", "metadata": {}}, {"text": "While we cannot claim that the models and papers in our data set are fully representative of the whole machine\nlearning field, a sample of 95 models spanning 9 years can offer interesting insights.", "metadata": {}}, {"text": "The first observation, related to the\nconclusions from the sections above, is that there is a large variability in the carbon emissions from ML models.", "metadata": {}}, {"text": "Second,\nwe do not observe a consistent trend by which carbon emissions have systematically increased for each individual task.", "metadata": {}}, {"text": "This is the case, for instance, of image classification models (in blue) and question answering models (in yellow) from\nour sample.", "metadata": {}}, {"text": "However, the carbon emissions of machine translation models have peaked in 2019 and has since decreased.", "metadata": {}}, {"text": "If we look at the aggregated data from all tasks (grand average curve, in light gray), we can observe that overall, the\ncarbon emissions per model have increased by a factor of about 100 (two orders of magnitude) from 2012 to recent\nyears, with slight fluctuations, as in 2020.", "metadata": {}}, {"text": "It is important to note that the vertical axis of Figure 3 is on a logarithmic\nscale, in order to reflect the non-linearity introduced by the much larger models from recent years, even though they\ndo not represent a majority in the sample.", "metadata": {}}, {"text": "In fact, the last three years of our sample (2019-2021), have seen models\nthat have emitted orders of magnitude more carbon than before: e.g.", "metadata": {}}, {"text": "there are several vertical outliers in tasks such as\nImage Classification (shown in blue) and Question Answering (in yellow) that have set new records in terms of the\nManuscript pending review", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im3.png] Size: 1237x531, Data: 66187 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im3.png] Size: 1237x531, Data: 66187 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im3.png", "image_width": 1237, "image_height": 531, "attachment_type": "image", "has_image_data": true, "image_data_size": 66187}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 9\ntotal amount of emissions produced by model training, responsible for about 104 and 105 kilograms of CO2eq. There\nare several possible explanations for this, ranging from the widespread adoption of Transformers, which are using\nincreasing amounts of both labeled and unlabeled data [53], as well as computationally-expensive techniques such as\nNAS [59], which result in more carbon emissions [48]. It is hard to disentangle the influence of different factors on\nthe overall carbon emissions of ML models, as well as the relative contributions of different parts of the pre-training\nand fine-tuning process ‚Äì this requires further work, which we discuss in Section 5.3 ‚Äì however, it is worth noting the\nevolution of emissions in recent years, among the papers of our sample.\n4.4 Does more energy and CO 2 lead to better model performance?\nA final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of\ncarbon emitted by models to their performance on benchmark tasks such as image classification, machine translation\nand question answering. We compare the emissions of the models from our sample and their performance on four\ntasks: image recognition on ImageNet [14] (35 models), machine translation for English-French and English-German\non the 2014 WMT Translation tasks [10] (30 models), question answering on the SQuAD 1.1 dataset [39] (10 models),\nand named entity recognition on the CoNLL 2003 dataset [41] (11 models) 4. Our goal with this analysis is to validate\nwhether, generally speaking, the more carbon-intensive models from our sample achieved better performance on\ncommon benchmarks compared to the models with less incurred emissions.\nFigure 4 shows the performance of the models in these four tasks and the associated carbon emissions; we also\nrepresent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based\non our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the\nmodels that achieved the best accuracy for a given amount of CO 2eq emissions. In other words, all the data points\nunder the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite\nproducing the same or more carbon emissions.\nBased on the comparison between carbon emissions and performance, we can observe that the only task in which\nbetter performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top\nright subplot of Figure 4. Still, the relationship is far from being highly correlated (especially given that that the x-axis\nin on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are\nalso the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance\n(achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.\nFor other tasks, the trend is even less clear ‚Äì for instance, for the 30 models evaluated on the WMT translation task\n(top left plot of Figure 4), there is no clear link between CO2 emissions and BLEU score, for neither English-French or\nEnglish-German ‚Äì although the WMT English-French task seems to incur more carbon emissions than the English-\nGerman one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger\nthan the English-German one, which can require a longer training time and thus a higher energy consumption. For the\nfinal two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and\n11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many\nmodels from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD\ndataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).\n4We also had data from a fifth task, object detection, which is represented in Table 1 and Figure 3, but we did not have enough distinct data points to\nenable a meaningful comparison.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 9\ntotal amount of emissions produced by model training, responsible for about 104 and 105 kilograms of CO2eq.", "metadata": {}}, {"text": "There\nare several possible explanations for this, ranging from the widespread adoption of Transformers, which are using\nincreasing amounts of both labeled and unlabeled data [53], as well as computationally-expensive techniques such as\nNAS [59], which result in more carbon emissions [48].", "metadata": {}}, {"text": "It is hard to disentangle the influence of different factors on\nthe overall carbon emissions of ML models, as well as the relative contributions of different parts of the pre-training\nand fine-tuning process ‚Äì this requires further work, which we discuss in Section 5.3 ‚Äì however, it is worth noting the\nevolution of emissions in recent years, among the papers of our sample.", "metadata": {}}, {"text": "4.4 Does more energy and CO 2 lead to better model performance?", "metadata": {}}, {"text": "A final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of\ncarbon emitted by models to their performance on benchmark tasks such as image classification, machine translation\nand question answering.", "metadata": {}}, {"text": "We compare the emissions of the models from our sample and their performance on four\ntasks: image recognition on ImageNet [14] (35 models), machine translation for English-French and English-German\non the 2014 WMT Translation tasks [10] (30 models), question answering on the SQuAD 1.1 dataset [39] (10 models),\nand named entity recognition on the CoNLL 2003 dataset [41] (11 models) 4.", "metadata": {}}, {"text": "Our goal with this analysis is to validate\nwhether, generally speaking, the more carbon-intensive models from our sample achieved better performance on\ncommon benchmarks compared to the models with less incurred emissions.", "metadata": {}}, {"text": "Figure 4 shows the performance of the models in these four tasks and the associated carbon emissions;", "metadata": {}}, {"text": "we also\nrepresent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based\non our data.", "metadata": {}}, {"text": "We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the\nmodels that achieved the best accuracy for a given amount of CO 2eq emissions.", "metadata": {}}, {"text": "In other words, all the data points\nunder the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite\nproducing the same or more carbon emissions.", "metadata": {}}, {"text": "Based on the comparison between carbon emissions and performance, we can observe that the only task in which\nbetter performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top\nright subplot of Figure 4.", "metadata": {}}, {"text": "Still, the relationship is far from being highly correlated (especially given that that the x-axis\nin on a logarithmic scale).", "metadata": {}}, {"text": "For example, out of the 35 models analyzed, the top two models in terms of performance are\nalso the most carbon-emitting.", "metadata": {}}, {"text": "However, the third most carbon-intensive model is on the lower end of the performance\n(achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.", "metadata": {}}, {"text": "For other tasks, the trend is even less clear ‚Äì for instance, for the 30 models evaluated on the WMT translation task\n(top left plot of Figure 4), there is no clear link between CO2 emissions and BLEU score, for neither English-French or\nEnglish-German ‚Äì although the WMT English-French task seems to incur more carbon emissions than the English-\nGerman one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger\nthan the English-German one, which can require a longer training time and thus a higher energy consumption.", "metadata": {}}, {"text": "For the\nfinal two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and\n11 for the latter), and the connection between carbon emissions and accuracy is very unclear.", "metadata": {}}, {"text": "For both tasks, many\nmodels from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD\ndataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).", "metadata": {}}, {"text": "4We also had data from a fifth task, object detection, which is represented in Table 1 and Figure 3, but we did not have enough distinct data points to\nenable a meaningful comparison.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "10 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nFig. 4. Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score\non the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on\nImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) andNamed Entity Recognition (bottom\nright, evaluated using F1 score on the CoNLL dataset) and the CO2 emitted for training models. The black curves correspond to the\nPareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that\nthe x axis is in logarithmic scale.\nDespite the lack of clear correlation between carbon intensity and model performance, there are some interesting\nobservations to be made based on Figure 4. While we did not expect to see a strong link between these two factors, we\nfind it worth noting that neither consuming more energy nor emitting more carbon seems to necessarily correlate with\na higher accuracy, even in tasks such as Machine Translation, where Transformer models are largely seen to do better\ncompared to other models 5.\n5 DISCUSSION AND FUTURE WORK\nIn the current section, we discuss the significance and the context of our analysis, its limitations, as well as promising\ndirections for future work to improve the transparency of carbon emissions reporting in our field.\n5We find a similar pattern between accuracy and energy consumption, which can be seen in Figure 5 in the Supplementary Materials.\nManuscript pending review", "sentences": [{"text": "10 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nFig.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score\non the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on\nImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) andNamed Entity Recognition (bottom\nright, evaluated using F1 score on the CoNLL dataset) and the CO2 emitted for training models.", "metadata": {}}, {"text": "The black curves correspond to the\nPareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that\nthe x axis is in logarithmic scale.", "metadata": {}}, {"text": "Despite the lack of clear correlation between carbon intensity and model performance, there are some interesting\nobservations to be made based on Figure 4.", "metadata": {}}, {"text": "While we did not expect to see a strong link between these two factors, we\nfind it worth noting that neither consuming more energy nor emitting more carbon seems to necessarily correlate with\na higher accuracy, even in tasks such as Machine Translation, where Transformer models are largely seen to do better\ncompared to other models 5.", "metadata": {}}, {"text": "5 DISCUSSION AND FUTURE WORK\nIn the current section, we discuss the significance and the context of our analysis, its limitations, as well as promising\ndirections for future work to improve the transparency of carbon emissions reporting in our field.", "metadata": {}}, {"text": "5We find a similar pattern between accuracy and energy consumption, which can be seen in Figure 5 in the Supplementary Materials.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=Im4.png] Size: 1882x1293, Data: 202085 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=Im4.png] Size: 1882x1293, Data: 202085 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "Im4.png", "image_width": 1882, "image_height": 1293, "attachment_type": "image", "has_image_data": true, "image_data_size": 202085}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 11\n5.1 Discussion of Results\nWhile the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic\nreporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community\nto acquire a better understanding of its environmental footprint and how to reduce it [28, 38]. Our study is the first\nanalysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source\nto performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions\nassociated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is\nabout 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to\nBeijing. While this may not seem like a large amount, the increase in emissions in recent years ‚Äì from an average of\n487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 ‚Äì as well as\nother trends that we observed in Section 4.3, indicate that the overall emissions due to ML model are rising.\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the\nemissions spectrum. Given that Transformer architectures are increasing in popularity ‚Äì especially in NLP but also for\nseveral Computer Vision tasks ‚Äì having a better idea of their energy consumption, carbon emissions, and the factors\nthat influence them is also crucial part of analyzing the current and future state of our field.\nAn important observation from our analysis is that better performance is not generally achieved by using more\nenergy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent\nyears has brought the possibility to train machine learning models efficiently. Image Classification is the task in our\nsample in which we observed the strongest correlation between performance and emissions. However, even in this\ntask we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the\nleft-hand-side of Figure 4). This highlights the availability of efficient approaches and architectures.\n5.2 Limitations\nThe analyses that we have carried out and the insights that they have provided us are useful towards a better under-\nstanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for\none, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models\nand architectures that exist and the speed at which our field is evolving. As we discussed in Section 3, despite our\nbest efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 11\n5.1 Discussion of Results\nWhile the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic\nreporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community\nto acquire a better understanding of its environmental footprint and how to reduce it [28, 38].", "metadata": {}}, {"text": "Our study is the first\nanalysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source\nto performance.", "metadata": {}}, {"text": "While our sample is only a small portion of the entire Machine Learning field, the carbon emissions\nassociated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is\nabout 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to\nBeijing.", "metadata": {}}, {"text": "While this may not seem like a large amount, the increase in emissions in recent years ‚Äì from an average of\n487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 ‚Äì as well as\nother trends that we observed in Section 4.3, indicate that the overall emissions due to ML model are rising.", "metadata": {}}, {"text": "In Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence.", "metadata": {}}, {"text": "In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample.", "metadata": {}}, {"text": "While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38].", "metadata": {}}, {"text": "Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources.", "metadata": {}}, {"text": "By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq.", "metadata": {}}, {"text": "Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the\nemissions spectrum.", "metadata": {}}, {"text": "Given that Transformer architectures are increasing in popularity ‚Äì especially in NLP but also for\nseveral Computer Vision tasks ‚Äì having a better idea of their energy consumption, carbon emissions, and the factors\nthat influence them is also crucial part of analyzing the current and future state of our field.", "metadata": {}}, {"text": "An important observation from our analysis is that better performance is not generally achieved by using more\nenergy.", "metadata": {}}, {"text": "In other words, good performance can be achieved with limited carbon emissions because the progress in recent\nyears has brought the possibility to train machine learning models efficiently.", "metadata": {}}, {"text": "Image Classification is the task in our\nsample in which we observed the strongest correlation between performance and emissions.", "metadata": {}}, {"text": "However, even in this\ntask we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the\nleft-hand-side of Figure 4).", "metadata": {}}, {"text": "This highlights the availability of efficient approaches and architectures.", "metadata": {}}, {"text": "5.2 Limitations\nThe analyses that we have carried out and the insights that they have provided us are useful towards a better under-\nstanding of the overall carbon emissions of ML model training.", "metadata": {}}, {"text": "We are also aware of the limitations of our study: for\none, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models\nand architectures that exist and the speed at which our field is evolving.", "metadata": {}}, {"text": "As we discussed in Section 3, despite our\nbest efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant\nManuscript pending review", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "12 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ninformation with us. We also recognize that there are several factors that we are missing in order to be more precise in\nour estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the\nPower Usage Effectiveness (PUE) of the data centers used for model training (i.e. the overhead used for heating, cooling,\nInternet etc.), as well as the real-time energy consumption of the hardware used for training. We also do not account\nfor carbon offsets and power purchase agreements, which intend to bring computing centers closer to carbon neutrality\nand which are often taken into account by providers of cloud compute in their carbon accounting [18]. Despite this, the\napples-to-apples carbon analysis that we carried out in the current study provides useful insights about the current\nstate of carbon emissions in our field, as well as how this has evolved over time in the last 9 years.\nFurthermore, while this study and much of the related work in this field has focused on estimating the carbon\nemissions of model training, there are many pieces of other overall carbon footprint of our field which are still missing:\nfor instance, the carbon emissions of tasks such as data processing, data transfer, and data storage [28], as well as the\ncarbon footprint of manufacturing and maintaining the hardware used for training ML models [19], We are also lacking\ninformation regarding the carbon impact of model development and inference ‚Äì given that a model that is trained a\nsingle time can be deployed on-demand for millions of queries, this can ultimately add up to more emissions than those\nproduced by the initial model training [31]. These are all directions for future research, which we discuss in more detail\nbelow.\n5.3 Future Work\nThere is much interesting and exciting work to be done that would help us better understand the carbon emissions and\nbroader environmental implications of ML. This includes:\nAdditional empirical studies. There is still a lot of uncertainty around, for instance, the relative contribution of added\nparameters of ML to their energy consumption and carbon footprint, as well as the proportion of energy used for\npre-training versus fine-tuning ML models for different tasks and architectures. Furthering this research can benefit the\nfield both from the perspective of sustainability and overall efficiency.\nWidening the scope of ML life-cycle emissions. The overwhelming majority of work in carbon accounting for ML\nmodels has been limited to model training. However, both the upstream emissions (i.e. those incurred by manufacturing\nand transporting the required computing equipment) as well as the downstream ones (i.e. the emissions of model\ndeployment) warrant further exploration and better understanding.\nIncreased standardization and transparency in carbon emissions reporting. As stated in Section 5.2, we put in significant\nefforts in contacting authors and gathering data to carry out our study, and were still lacking much of the necessary\ninformation that we would have liked to have. While certain conferences such as NeurIPS are starting to include\ncompute information in submissions in submission checklists, there is still a lot of variability in carbon reporting, and\nfigures can vary widely depending on what factors are included. Having a more standardized approach, such as ISO\nstandards, to reporting the carbon emissions of ML can help better understand their evolution.\nConsidering the trade-off between sustainability and fairness. The environmental impacts of ML also come with\nconsequences in terms of fairness, given the interplay between fairness and sustainability, most recently discussed\nby Hessenthaler et al. [21]. This includes, for instance, the consideration of the environmental impacts of ML approaches\nwhen benchmarking models [58], but also, conversely, considering the impact on robustness and bias of model distilla-\ntion techniques that improve model efficiency [23, 55]. Generally speaking, given that many advances in ML from last\nManuscript pending review", "sentences": [{"text": "12 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\ninformation with us.", "metadata": {}}, {"text": "We also recognize that there are several factors that we are missing in order to be more precise in\nour estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the\nPower Usage Effectiveness (PUE) of the data centers used for model training (i.e.", "metadata": {}}, {"text": "the overhead used for heating, cooling,\nInternet etc.), as well as the real-time energy consumption of the hardware used for training.", "metadata": {}}, {"text": "We also do not account\nfor carbon offsets and power purchase agreements, which intend to bring computing centers closer to carbon neutrality\nand which are often taken into account by providers of cloud compute in their carbon accounting [18].", "metadata": {}}, {"text": "Despite this, the\napples-to-apples carbon analysis that we carried out in the current study provides useful insights about the current\nstate of carbon emissions in our field, as well as how this has evolved over time in the last 9 years.", "metadata": {}}, {"text": "Furthermore, while this study and much of the related work in this field has focused on estimating the carbon\nemissions of model training, there are many pieces of other overall carbon footprint of our field which are still missing:\nfor instance, the carbon emissions of tasks such as data processing, data transfer, and data storage [28], as well as the\ncarbon footprint of manufacturing and maintaining the hardware used for training ML models [19], We are also lacking\ninformation regarding the carbon impact of model development and inference ‚Äì given that a model that is trained a\nsingle time can be deployed on-demand for millions of queries, this can ultimately add up to more emissions than those\nproduced by the initial model training [31].", "metadata": {}}, {"text": "These are all directions for future research, which we discuss in more detail\nbelow.", "metadata": {}}, {"text": "5.3 Future Work\nThere is much interesting and exciting work to be done that would help us better understand the carbon emissions and\nbroader environmental implications of ML.", "metadata": {}}, {"text": "This includes:\nAdditional empirical studies.", "metadata": {}}, {"text": "There is still a lot of uncertainty around, for instance, the relative contribution of added\nparameters of ML to their energy consumption and carbon footprint, as well as the proportion of energy used for\npre-training versus fine-tuning ML models for different tasks and architectures.", "metadata": {}}, {"text": "Furthering this research can benefit the\nfield both from the perspective of sustainability and overall efficiency.", "metadata": {}}, {"text": "Widening the scope of ML life-cycle emissions.", "metadata": {}}, {"text": "The overwhelming majority of work in carbon accounting for ML\nmodels has been limited to model training.", "metadata": {}}, {"text": "However, both the upstream emissions (i.e.", "metadata": {}}, {"text": "those incurred by manufacturing\nand transporting the required computing equipment) as well as the downstream ones (i.e.", "metadata": {}}, {"text": "the emissions of model\ndeployment) warrant further exploration and better understanding.", "metadata": {}}, {"text": "Increased standardization and transparency in carbon emissions reporting.", "metadata": {}}, {"text": "As stated in Section 5.2, we put in significant\nefforts in contacting authors and gathering data to carry out our study, and were still lacking much of the necessary\ninformation that we would have liked to have.", "metadata": {}}, {"text": "While certain conferences such as NeurIPS are starting to include\ncompute information in submissions in submission checklists, there is still a lot of variability in carbon reporting, and\nfigures can vary widely depending on what factors are included.", "metadata": {}}, {"text": "Having a more standardized approach, such as ISO\nstandards, to reporting the carbon emissions of ML can help better understand their evolution.", "metadata": {}}, {"text": "Considering the trade-off between sustainability and fairness.", "metadata": {}}, {"text": "The environmental impacts of ML also come with\nconsequences in terms of fairness, given the interplay between fairness and sustainability, most recently discussed\nby Hessenthaler et al.", "metadata": {}}, {"text": "[21].", "metadata": {}}, {"text": "This includes, for instance, the consideration of the environmental impacts of ML approaches\nwhen benchmarking models [58], but also, conversely, considering the impact on robustness and bias of model distilla-\ntion techniques that improve model efficiency [23, 55].", "metadata": {}}, {"text": "Generally speaking, given that many advances in ML from last\nManuscript pending review", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 13\nyears can be attributed to training increasingly deep and computationally expensive models, especially in fields such as\nnatural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from\nthe perspective of their energy consumption [8, 15], the attribution of computing resources [2, 3] or the influence of\ncorporate interests on research directions [1, 9].\nWhile discussions regarding the carbon footprint of our daily lives has started to become more common in many\ncommunities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat)\ncontribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts\nof the models we train. We hope that our work encourages better practices and more transparency in reporting the\ncomputational needs of the models and details of the energy used, and that our study will be a meaningful contribution\ntowards a better understanding of our impact as ML researchers and practitioners.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 13\nyears can be attributed to training increasingly deep and computationally expensive models, especially in fields such as\nnatural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from\nthe perspective of their energy consumption [8, 15], the attribution of computing resources [2, 3] or the influence of\ncorporate interests on research directions [1, 9].", "metadata": {}}, {"text": "While discussions regarding the carbon footprint of our daily lives has started to become more common in many\ncommunities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat)\ncontribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts\nof the models we train.", "metadata": {}}, {"text": "We hope that our work encourages better practices and more transparency in reporting the\ncomputational needs of the models and details of the energy used, and that our study will be a meaningful contribution\ntowards a better understanding of our impact as ML researchers and practitioners.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "14 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 287‚Äì297.\n[2] Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource\nMachine Translation. arXiv preprint arXiv:2110.03036 (2021).\n[3] Nur Ahmed and Muntasir Wahed. 2020. The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research.\narXiv preprint arXiv:2010.15581 (2020).\n[4] Amazon Web Services. 2021. Delivering Progress Every Day : Amazon‚Äôs 2021 Sustainability Report. https://sustainability.aboutamazon.com/2021-\nsustainability-report.pdf\n[5] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of\nTraining Deep Learning Models. arXiv:2007.03051 [cs.CY]\n[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael. 2010. On the causal dynamics between emissions, nuclear energy,\nrenewable energy, and economic growth. Ecological Economics 69, 11 (2010), 2255‚Äì2260.\n[7] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools. In EMNLP, Workshop SustaiNLP.\n[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?\n . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency . 610‚Äì623.\n[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The Values Encoded in Machine Learning\nResearch. arXiv:2106.15590 [cs.LG]\n[10] Ond≈ôej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post,\nHerve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical\nmachine translation. 12‚Äì58.\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile\ndevices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292‚Äì308.\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE\nconference on computer vision and pattern recognition . Ieee, 248‚Äì255.\n[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In2022 ACM Conference on Fairness, Accountability,\nand Transparency. 1877‚Äì1894.\n[16] Facebook. 2020. Facebook 2020 Sustainability Report. https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf\n[17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens. 2020. This interactive chart shows changes in the world‚Äôs top 10 emitters. (2020).\n[18] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\nIEEE, 854‚Äì867.\n[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy\nand carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020), 1‚Äì43.\n[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging Fairness and Environmental Sustainability in Natural Language\nProcessing. arXiv preprint arXiv:2211.04256 (2022).\n[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon\nOsindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models.\nhttps://doi.org/10.48550/ARXIV.2203.15556\n[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint\narXiv:2010.03058 (2020).\n[24] IEA. 2019. Global Energy & CO2 Status Report 2019. IEA (International Energy Agency): Paris, France (2019). https://www.iea.org/reports/global-\nenergy-co2-status-report-2019\n[25] International Telecommunication Union. 2020. Greenhouse gas emissions trajectories for the information and communication technology sector\ncompatible with the UNFCCC Paris agreement: L. 1470. http://handle.itu.int/11.1002/1000/14084\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700 (2019).\nManuscript pending review", "sentences": [{"text": "14 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nREFERENCES\n[1] Mohamed Abdalla and Moustafa Abdalla.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity.", "metadata": {}}, {"text": "In Proceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society .", "metadata": {}}, {"text": "287‚Äì297.", "metadata": {}}, {"text": "[2] Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource\nMachine Translation.", "metadata": {}}, {"text": "arXiv preprint arXiv:2110.03036 (2021).", "metadata": {}}, {"text": "[3] Nur Ahmed and Muntasir Wahed.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research.", "metadata": {}}, {"text": "arXiv preprint arXiv:2010.15581 (2020).", "metadata": {}}, {"text": "[4] Amazon Web Services.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Delivering Progress Every Day : Amazon‚Äôs 2021 Sustainability Report.", "metadata": {}}, {"text": "https://sustainability.aboutamazon.com/2021-\nsustainability-report.pdf\n[5] Lasse F.", "metadata": {}}, {"text": "Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Carbontracker: Tracking and Predicting the Carbon Footprint of\nTraining Deep Learning Models.", "metadata": {}}, {"text": "arXiv:2007.03051 [cs.CY]\n[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael.", "metadata": {}}, {"text": "2010.", "metadata": {}}, {"text": "On the causal dynamics between emissions, nuclear energy,\nrenewable energy, and economic growth.", "metadata": {}}, {"text": "Ecological Economics 69, 11 (2010), 2255‚Äì2260.", "metadata": {}}, {"text": "[7] Nesrine Bannour, Sahar Ghannay, Aur√©lie N√©v√©ol, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Evaluating the carbon footprint of NLP methods: a survey and\nanalysis of existing tools.", "metadata": {}}, {"text": "In EMNLP, Workshop SustaiNLP.", "metadata": {}}, {"text": "[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big?", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency .", "metadata": {}}, {"text": "610‚Äì623.", "metadata": {}}, {"text": "[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "The Values Encoded in Machine Learning\nResearch.", "metadata": {}}, {"text": "arXiv:2106.15590 [cs.LG]\n[10] Ond≈ôej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post,\nHerve Saint-Amand, et al.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Findings of the 2014 workshop on statistical machine translation.", "metadata": {}}, {"text": "In Proceedings of the ninth workshop on statistical\nmachine translation.", "metadata": {}}, {"text": "12‚Äì58.", "metadata": {}}, {"text": "[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "On the opportunities and risks of foundation models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2108.07258 (2021).", "metadata": {}}, {"text": "[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Language models are few-shot learners.", "metadata": {}}, {"text": "arXiv preprint arXiv:2005.14165 (2020).", "metadata": {}}, {"text": "[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile\ndevices.", "metadata": {}}, {"text": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292‚Äì308.", "metadata": {}}, {"text": "[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.", "metadata": {}}, {"text": "2009.", "metadata": {}}, {"text": "ImageNet: A large-scale hierarchical image database.", "metadata": {}}, {"text": "In 2009 IEEE\nconference on computer vision and pattern recognition .", "metadata": {}}, {"text": "Ieee, 248‚Äì255.", "metadata": {}}, {"text": "[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith,\nNicole DeCario, and Will Buchanan.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Measuring the carbon intensity of ai in cloud instances.", "metadata": {}}, {"text": "In2022 ACM Conference on Fairness, Accountability,\nand Transparency.", "metadata": {}}, {"text": "1877‚Äì1894.", "metadata": {}}, {"text": "[16] Facebook.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Facebook 2020 Sustainability Report.", "metadata": {}}, {"text": "https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf\n[17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "This interactive chart shows changes in the world‚Äôs top 10 emitters.", "metadata": {}}, {"text": "(2020).", "metadata": {}}, {"text": "[18] Google.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Carbon free energy for Google Cloud regions.", "metadata": {}}, {"text": "https://cloud.google.com/sustainability/region-carbon\n[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Chasing Carbon:\nThe Elusive Environmental Footprint of Computing.", "metadata": {}}, {"text": "In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .", "metadata": {}}, {"text": "IEEE, 854‚Äì867.", "metadata": {}}, {"text": "[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Towards the systematic reporting of the energy\nand carbon footprints of machine learning.", "metadata": {}}, {"text": "Journal of Machine Learning Research 21, 248 (2020), 1‚Äì43.", "metadata": {}}, {"text": "[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Bridging Fairness and Environmental Sustainability in Natural Language\nProcessing.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.04256 (2022).", "metadata": {}}, {"text": "[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon\nOsindero, Karen Simonyan, Erich Elsen, Jack W.", "metadata": {}}, {"text": "Rae, Oriol Vinyals, and Laurent Sifre.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Training Compute-Optimal Large Language Models.", "metadata": {}}, {"text": "https://doi.org/10.48550/ARXIV.2203.15556\n[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Characterising bias in compressed models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2010.03058 (2020).", "metadata": {}}, {"text": "[24] IEA.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Global Energy & CO2 Status Report 2019.", "metadata": {}}, {"text": "IEA (International Energy Agency): Paris, France (2019).", "metadata": {}}, {"text": "https://www.iea.org/reports/global-\nenergy-co2-status-report-2019\n[25] International Telecommunication Union.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Greenhouse gas emissions trajectories for the information and communication technology sector\ncompatible with the UNFCCC Paris agreement: L.", "metadata": {}}, {"text": "1470.", "metadata": {}}, {"text": "http://handle.itu.int/11.1002/1000/14084\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Quantifying the carbon emissions of machine learning.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:1910.09700 (2019).", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 14}}, {"text": "[Image page=14 idx=1 name=Im5.png] Size: 44x100, Data: 6208 bytes", "sentences": [{"text": "[Image page=14 idx=1 name=Im5.png] Size: 44x100, Data: 6208 bytes", "metadata": {}}], "metadata": {"page": 14, "image_index": 1, "image_name": "Im5.png", "image_width": 44, "image_height": 100, "attachment_type": "image", "has_image_data": true, "image_data_size": 6208}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 15\n[27] Lo√Øc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of computation. Advanced Science\n(2021), 2100707.\n[28] Anne-Laure Ligozat, Julien Lef√®vre, Aur√©lie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for\nenvironment. arXiv preprint arXiv:2110.11822 (2021).\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. Microsoft\nCOCO: Common objects in context. In European conference on computer vision . Springer, 740‚Äì755.\n[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic\naccountability. arXiv preprint arXiv:1911.08354 (2019).\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language\nModel. arXiv preprint arXiv:2211.02001 (2022).\n[32] Jens Malmodin and Dag Lund√©n. 2018. The energy and carbon footprint of the global ICT and E&M sectors 2010‚Äì2015. Sustainability 10, 9 (2018),\n3027.\n[33] Aaditya Mattoo and Arvind Subramanian. 2012. Equity in climate change: an analytical review. World Development 40, 6 (2012), 1083‚Äì1097.\n[34] Jennifer Morgan and David Waskow. 2014. A new look at climate equity in the UNFCCC. Climate Policy 14, 1 (2014), 17‚Äì22.\n[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam. 2021. Towards Quantifying the Carbon\nEmissions of Differentially Private Machine Learning. arXiv preprint arXiv:2107.06946 (2021).\n[36] Copenhagen Centre on Energy Efficiency. 2020. Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet]. https:\n//c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf\n[37] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. arXiv preprint arXiv:2204.05149 (2022).\n[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. arXiv\npreprint arXiv:1606.05250 (2016).\n[40] Henning Rodhe. 1990. A comparison of the contribution of various gases to the greenhouse effect. Science 248, 4960 (1990), 1217‚Äì1219.\n[41] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv\npreprint cs/0306050 (2003).\n[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv preprint arXiv:1910.01108 (2019).\n[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,\nArun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali\nBers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization.\nhttps://doi.org/10.48550/ARXIV.2110.08207\n[44] Steffen Schl√∂mer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims,\nPete Smith, et al. 2014. Annex III: Technology-specific cost and performance parameters. In Climate Change 2014: Mitigation of Climate Change:\nContribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change . Cambridge University Press,\n1329‚Äì1356.\n[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54‚Äì63.\n[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Mu√±oz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert\nNerenberg, Andrea Armani, Kasey M. Faust, and Manish Kumar. 2021. Conference demographics and footprint changed by virtual platforms. Nature\nSustainability 2398-9629 (2021).\n[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n[49] Siddharth Suman. 2018. Hybrid nuclear-renewable energy systems: A review. Journal of Cleaner Production 181 (2018), 166‚Äì177.\n[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint\narXiv:2007.05558 (2020).\n[51] Tristan Tr√©baol. 2020. CUMULATOR ‚Äî a tool to quantify and report the carbon footprint of machine learning computations and communication in\nacademia and healthcare. Technical Report.\n[52] United States Energy Information Administration. 2012-2021. Detailed State Data. https://www.eia.gov/electricity/data/state/\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information processing systems . 5998‚Äì6008.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 15\n[27] Lo√Øc Lannelongue, Jason Grealey, and Michael Inouye.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Green algorithms: Quantifying the carbon footprint of computation.", "metadata": {}}, {"text": "Advanced Science\n(2021), 2100707.", "metadata": {}}, {"text": "[28] Anne-Laure Ligozat, Julien Lef√®vre, Aur√©lie Bugeau, and Jacques Combaz.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Unraveling the hidden environmental impacts of AI solutions for\nenvironment.", "metadata": {}}, {"text": "arXiv preprint arXiv:2110.11822 (2021).", "metadata": {}}, {"text": "[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Microsoft\nCOCO: Common objects in context.", "metadata": {}}, {"text": "In European conference on computer vision .", "metadata": {}}, {"text": "Springer, 740‚Äì755.", "metadata": {}}, {"text": "[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Energy Usage Reports: Environmental awareness as part of algorithmic\naccountability.", "metadata": {}}, {"text": "arXiv preprint arXiv:1911.08354 (2019).", "metadata": {}}, {"text": "[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language\nModel.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.02001 (2022).", "metadata": {}}, {"text": "[32] Jens Malmodin and Dag Lund√©n.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "The energy and carbon footprint of the global ICT and E&M sectors 2010‚Äì2015.", "metadata": {}}, {"text": "Sustainability 10, 9 (2018),\n3027.", "metadata": {}}, {"text": "[33] Aaditya Mattoo and Arvind Subramanian.", "metadata": {}}, {"text": "2012.", "metadata": {}}, {"text": "Equity in climate change: an analytical review.", "metadata": {}}, {"text": "World Development 40, 6 (2012), 1083‚Äì1097.", "metadata": {}}, {"text": "[34] Jennifer Morgan and David Waskow.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "A new look at climate equity in the UNFCCC.", "metadata": {}}, {"text": "Climate Policy 14, 1 (2014), 17‚Äì22.", "metadata": {}}, {"text": "[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Towards Quantifying the Carbon\nEmissions of Differentially Private Machine Learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2107.06946 (2021).", "metadata": {}}, {"text": "[36] Copenhagen Centre on Energy Efficiency.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet].", "metadata": {}}, {"text": "https:\n//c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf\n[37] David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.", "metadata": {}}, {"text": "arXiv preprint arXiv:2204.05149 (2022).", "metadata": {}}, {"text": "[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.10350 (2021).", "metadata": {}}, {"text": "[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "SQuAD: 100,000+ questions for machine comprehension of text.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:1606.05250 (2016).", "metadata": {}}, {"text": "[40] Henning Rodhe.", "metadata": {}}, {"text": "1990.", "metadata": {}}, {"text": "A comparison of the contribution of various gases to the greenhouse effect.", "metadata": {}}, {"text": "Science 248, 4960 (1990), 1217‚Äì1219.", "metadata": {}}, {"text": "[41] Erik F Sang and Fien De Meulder.", "metadata": {}}, {"text": "2003.", "metadata": {}}, {"text": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.", "metadata": {}}, {"text": "arXiv\npreprint cs/0306050 (2003).", "metadata": {}}, {"text": "[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.", "metadata": {}}, {"text": "arXiv preprint arXiv:1910.01108 (2019).", "metadata": {}}, {"text": "[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H.", "metadata": {}}, {"text": "Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,\nArun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali\nBers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M.", "metadata": {}}, {"text": "Rush.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Multitask Prompted Training Enables Zero-Shot Task Generalization.", "metadata": {}}, {"text": "https://doi.org/10.48550/ARXIV.2110.08207\n[44] Steffen Schl√∂mer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims,\nPete Smith, et al.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Annex III: Technology-specific cost and performance parameters.", "metadata": {}}, {"text": "In Climate Change 2014: Mitigation of Climate Change:\nContribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change .", "metadata": {}}, {"text": "Cambridge University Press,\n1329‚Äì1356.", "metadata": {}}, {"text": "[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha\nLuccioni.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.", "metadata": {}}, {"text": "[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Green AI.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM 63, 12 (2020), 54‚Äì63.", "metadata": {}}, {"text": "[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Mu√±oz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert\nNerenberg, Andrea Armani, Kasey M.", "metadata": {}}, {"text": "Faust, and Manish Kumar.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Conference demographics and footprint changed by virtual platforms.", "metadata": {}}, {"text": "Nature\nSustainability 2398-9629 (2021).", "metadata": {}}, {"text": "[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Energy and policy considerations for deep learning in NLP.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1906.02243 (2019).", "metadata": {}}, {"text": "[49] Siddharth Suman.", "metadata": {}}, {"text": "2018.", "metadata": {}}, {"text": "Hybrid nuclear-renewable energy systems: A review.", "metadata": {}}, {"text": "Journal of Cleaner Production 181 (2018), 166‚Äì177.", "metadata": {}}, {"text": "[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "The computational limits of deep learning.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2007.05558 (2020).", "metadata": {}}, {"text": "[51] Tristan Tr√©baol.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "CUMULATOR ‚Äî a tool to quantify and report the carbon footprint of machine learning computations and communication in\nacademia and healthcare.", "metadata": {}}, {"text": "Technical Report.", "metadata": {}}, {"text": "[52] United States Energy Information Administration.", "metadata": {}}, {"text": "2012-2021.", "metadata": {}}, {"text": "Detailed State Data.", "metadata": {}}, {"text": "https://www.eia.gov/electricity/data/state/\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.", "metadata": {}}, {"text": "2017.", "metadata": {}}, {"text": "Attention is\nall you need.", "metadata": {}}, {"text": "In Advances in neural information processing systems .", "metadata": {}}, {"text": "5998‚Äì6008.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "16 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n[55] Guangxuan Xu and Qingyuan Hu. 2022. Can model compression improve NLP fairness. arXiv preprint arXiv:2201.08542 (2022).\n[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh. 2021. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine\nTranslation. arXiv preprint arXiv:2109.12584 (2021).\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: Open Pre-trained Transformer Language Models. https://doi.org/10.48550/ARXIV.2205.01068\n[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for responsible natural\nlanguage processing. arXiv preprint arXiv:2002.05829 (2020).\n[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).\nManuscript pending review", "sentences": [{"text": "16 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\n[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Sustainable AI: Environmental Implications, Challenges and Opportunities.", "metadata": {}}, {"text": "arXiv preprint arXiv:2111.00364 (2021).", "metadata": {}}, {"text": "[55] Guangxuan Xu and Qingyuan Hu.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Can model compression improve NLP fairness.", "metadata": {}}, {"text": "arXiv preprint arXiv:2201.08542 (2022).", "metadata": {}}, {"text": "[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine\nTranslation.", "metadata": {}}, {"text": "arXiv preprint arXiv:2109.12584 (2021).", "metadata": {}}, {"text": "[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "OPT: Open Pre-trained Transformer Language Models.", "metadata": {}}, {"text": "https://doi.org/10.48550/ARXIV.2205.01068\n[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Hulk: An energy efficiency benchmark platform for responsible natural\nlanguage processing.", "metadata": {}}, {"text": "arXiv preprint arXiv:2002.05829 (2020).", "metadata": {}}, {"text": "[59] Barret Zoph and Quoc V Le.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Neural architecture search with reinforcement learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:1611.01578 (2016).", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 17\nA SUPPLEMENTARY MATERIALS\nA.1 Emails sent to authors\nSubject: Information Request: Computing Infrastructure Used in your Paper\nHello,\nMy name is XXXX and I am a researcher working on the environmental impact of Machine Learning.\nI am trying to gather data regarding the carbon footprint of recent state-of-the-art research\npapers. This will help the ML community get a better idea of how much CO2 we are emitting when\ntraining models.\nIn order to help me on my mission, I was hoping you could give me more information about your\npaper entitled YYYY.\nMore specifically, could you tell me:\n- Where it was trained? If it was on a local computing cluster, could you tell me the location\nof the cluster? And if it was trained on the cloud, could you indicate the provider and server\nregion (e.g. \"Microsoft Azure, us-east1\")?\n- What hardware you used\n- The total training time of your models?\nThank you very much for this information,\nXXXX\nA.2 Information regarding training hardware\nTable 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and\ntheir Thermal Design Power (TDP).\nModel Number of models TDP Quantity used\nTesla V100 30 300 W 1-128\nTPU v3 9 450 W 1-1024\nRTX 2080 Ti 8 250 W 4-16\nTesla M40 5 250 W 8\nGTX 1080 4 180 W 1-8\nIn Table 3, we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by\nthe number of papers that used them, the range of quantities used, and their TDP. The Tesla V100 was by far the most\npopular piece of hardware, representing almost a third of the papers, followed by the TPU v3. The TDP of the hardware\nused in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU\nv3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and\nTPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of\nGPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize\na single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly. We\nanalyze the connection between energy usage and performance on different ML tasks in ¬ß 4.4, in order to determine\nwhether higher energy consumption helps achieve better performance in different ML tasks.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 17\nA SUPPLEMENTARY MATERIALS\nA.1 Emails sent to authors\nSubject: Information Request: Computing Infrastructure Used in your Paper\nHello,\nMy name is XXXX and I am a researcher working on the environmental impact of Machine Learning.", "metadata": {}}, {"text": "I am trying to gather data regarding the carbon footprint of recent state-of-the-art research\npapers.", "metadata": {}}, {"text": "This will help the ML community get a better idea of how much CO2 we are emitting when\ntraining models.", "metadata": {}}, {"text": "In order to help me on my mission, I was hoping you could give me more information about your\npaper entitled YYYY.", "metadata": {}}, {"text": "More specifically, could you tell me:\n- Where it was trained?", "metadata": {}}, {"text": "If it was on a local computing cluster, could you tell me the location\nof the cluster?", "metadata": {}}, {"text": "And if it was trained on the cloud, could you indicate the provider and server\nregion (e.g.", "metadata": {}}, {"text": "\"Microsoft Azure, us-east1\")?", "metadata": {}}, {"text": "- What hardware you used\n- The total training time of your models?", "metadata": {}}, {"text": "Thank you very much for this information,\nXXXX\nA.2 Information regarding training hardware\nTable 3.", "metadata": {}}, {"text": "The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and\ntheir Thermal Design Power (TDP).", "metadata": {}}, {"text": "Model Number of models TDP Quantity used\nTesla V100 30 300 W 1-128\nTPU v3 9 450 W 1-1024\nRTX 2080 Ti 8 250 W 4-16\nTesla M40 5 250 W 8\nGTX 1080 4 180 W 1-8\nIn Table 3, we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by\nthe number of papers that used them, the range of quantities used, and their TDP.", "metadata": {}}, {"text": "The Tesla V100 was by far the most\npopular piece of hardware, representing almost a third of the papers, followed by the TPU v3.", "metadata": {}}, {"text": "The TDP of the hardware\nused in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU\nv3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and\nTPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of\nGPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize\na single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly.", "metadata": {}}, {"text": "We\nanalyze the connection between energy usage and performance on different ML tasks in ¬ß 4.4, in order to determine\nwhether higher energy consumption helps achieve better performance in different ML tasks.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "18 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nA.3 Energy Consumption by Task\nIn Figure 5 below, we plot the same four tasks as in Figure 3, representing the energy consumed instead of the\nCO2 emitted. We find largely similar trends as the ones we describe in Section 4.4, with better performance on tasks\nlike machine translation and image classification not necessarily being contingent on higher energy consumption.\nFig. 5. Comparison of the performance achieved by each model trained on Machine Translation tasks (BLEU score) and Image\nClassification (top-1 accuracy), and the energy consumed.\nManuscript pending review", "sentences": [{"text": "18 Alexandra Sasha Luccioni and Alex Hernandez-Garcia\nA.3 Energy Consumption by Task\nIn Figure 5 below, we plot the same four tasks as in Figure 3, representing the energy consumed instead of the\nCO2 emitted.", "metadata": {}}, {"text": "We find largely similar trends as the ones we describe in Section 4.4, with better performance on tasks\nlike machine translation and image classification not necessarily being contingent on higher energy consumption.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "Comparison of the performance achieved by each model trained on Machine Translation tasks (BLEU score) and Image\nClassification (top-1 accuracy), and the energy consumed.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 18}}, {"text": "[Image page=18 idx=1 name=Im6.png] Size: 1876x1293, Data: 202136 bytes", "sentences": [{"text": "[Image page=18 idx=1 name=Im6.png] Size: 1876x1293, Data: 202136 bytes", "metadata": {}}], "metadata": {"page": 18, "image_index": 1, "image_name": "Im6.png", "image_width": 1876, "image_height": 1293, "attachment_type": "image", "has_image_data": true, "image_data_size": 202136}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 19\nA.4 Carbon intensity over time\nIn Figure 6, we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the\nnumber of models trained with each energy source. We observe that, despite the need to address the climate crisis by\nusing cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of\nmodels trained with cleaner energy. On the contrary, we do observe a stark increase of models trained with coal.\n(a) Carbon intensity of the models per year and energy source.\n(b) Number of models trained with each energy source per year.\nFig. 6. Carbon intensity and energy sources over the years.\nManuscript pending review", "sentences": [{"text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 19\nA.4 Carbon intensity over time\nIn Figure 6, we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the\nnumber of models trained with each energy source.", "metadata": {}}, {"text": "We observe that, despite the need to address the climate crisis by\nusing cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of\nmodels trained with cleaner energy.", "metadata": {}}, {"text": "On the contrary, we do observe a stark increase of models trained with coal.", "metadata": {}}, {"text": "(a) Carbon intensity of the models per year and energy source.", "metadata": {}}, {"text": "(b) Number of models trained with each energy source per year.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "Carbon intensity and energy sources over the years.", "metadata": {}}, {"text": "Manuscript pending review", "metadata": {}}], "metadata": {"page": 19}}, {"text": "[Image page=19 idx=1 name=Im7.png] Size: 1239x531, Data: 51232 bytes", "sentences": [{"text": "[Image page=19 idx=1 name=Im7.png] Size: 1239x531, Data: 51232 bytes", "metadata": {}}], "metadata": {"page": 19, "image_index": 1, "image_name": "Im7.png", "image_width": 1239, "image_height": 531, "attachment_type": "image", "has_image_data": true, "image_data_size": 51232}}, {"text": "[Image page=19 idx=2 name=Im8.png] Size: 1229x531, Data: 25028 bytes", "sentences": [{"text": "[Image page=19 idx=2 name=Im8.png] Size: 1229x531, Data: 25028 bytes", "metadata": {}}], "metadata": {"page": 19, "image_index": 2, "image_name": "Im8.png", "image_width": 1229, "image_height": 531, "attachment_type": "image", "has_image_data": true, "image_data_size": 25028}}], "metadata": {"page": 19}}]}