{"document_id": "chung2025", "title": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization", "text": "The ML.ENERGY Benchmark: Toward Automated\nInference Energy Measurement and Optimization\nJae-Won Chung Jeff J. Ma Ruofan Wu Jiachen Liu\nOh Jun Kweon Yuxuan Xia Zhiyu Wu Mosharaf Chowdhury\nUniversity of Michigan\nThe ML.ENERGY Initiative\nAbstract\nAs the adoption of Generative AI in real-world services grow explosively,energy\nhas emerged as a critical bottleneck resource. However, energy remains a metric\nthat is often overlooked, under-explored, or poorly understood in the context of\nbuilding ML systems. We present the ML.ENERGY Benchmark, a benchmark\nsuite and tool for measuring inference energy consumption under realistic service\nenvironments, and the corresponding ML.ENERGY Leaderboard, which have\nserved as a valuable resource for those hoping to understand and optimize the\nenergy consumption of their generative AI services. In this paper, we explain four\nkey design principles for benchmarking ML energy we have acquired over time,\nand then describe how they are implemented in the ML.ENERGY Benchmark. We\nthen highlight results from the early 2025 iteration of the benchmark, including\nenergy measurements of 40 widely used model architectures across 6 different\ntasks, case studies of how ML design choices impact energy consumption, and\nhow automated optimization recommendations can lead to significant (sometimes\nmore than 40%) energy savings without changing what is being computed by the\nmodel. The ML.ENERGY Benchmark is open-source and can be easily extended\nto various customized models and application scenarios.\n1 Introduction\nGenerative AI models have rapidly transitioned from research prototypes to real-world services such\nas ChatGPT [56], Character AI [6], Sora [57], and Midjourney [50]. However, exponential growth\nrarely continues without facing scaling bottlenecks; currently for generative AI, one of the most\ncrucial bottlenecks is theenergy bottleneck[4, 15 –17, 38, 48, 49, 51]. That is, even with fleets of latest\nGPUs and exploding demand for ML compute, getting access to the energy necessary to power these\nsystems is becoming increasingly costly, slow, and sometimes impossible. This particularly impacts\nserving real-world services as ML inference reportedly accounts for 80–90% of the total compute\ndemand [12, 32, 58, 60]. Left unaddressed, the energy bottleneck will not only hinder AI research and\ndevelopment progress [31], but also lead to energy being squeezed out of existing electricity grids\nand impacting availability and price [4].\nHowever, despite its growing importance, energy remains a secondary consideration compared to\ntraditional optimization objectives like time and accuracy. How much energy does a model consume\nduring inference? What is the right way for energy measurement and accounting during execution,\nlet alone optimization? To bridge this gap, we launched the ML.ENERGY Leaderboard,1 the first\ninference energy leaderboard for modern generative AI models to the best of our knowledge. We have\nbeen gradually expanding the Leaderboard in multiple dimensions to now include (1) 40 different\n1https://ml.energy/leaderboard\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks.\narXiv:2505.06371v2  [cs.LG]  16 Oct 2025\n\nBenchmark Optimization\nReusable Measurement Results Energy-Optimal Conﬁg \n……\nConﬁguration Space Latency TargetModel & Dataset\nConﬁgs\nA100 A100 A100 A100\nH100 H100\n……\nMax batch size = 512\nPipeline parallel = 1\nTensor parallel = 4\nGPU type = A100\n……\nMax batch size = 256\nPipeline parallel = 1\nTensor parallel = 2\nGPU type = H100\n1\n2\n3\nTime (s)\nEnergy (J)\nTarget\n4\nMeasured energy & latency\nFigure 1: Overview of the benchmarking and optimization flow of the ML.ENERGY Benchmark.\ngenerative AI model architectures across a wide range of tasks – including Large Language Model\n(LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video,\nand image-to-video generation using Diffusion models – and (2) more up-to-date hardware and\nsoftware stacks following rapid advancements in each area.\nIn this paper, we share the design principles we have established over time (Section 2) and present\nthe ML.ENERGY Benchmark that embodies them (Section 3). It provides two key functionalities:\n• Extensible benchmark: It provides an easily extensible benchmark suite and a comprehensive\nset of tools for measuring the inference energy consumption of generative AI models for various\ntasks underrealisticdeployment environments.\n• Automated optimization: Based on energy measurement results, it provides automated energy\noptimization recommendations for generative AI model deployment.\nFinally, we highlight notable results from the early 2025 iteration of the ML.ENERGY Leaderboard,\nshedding light on (1) how energy consumption varies across different generative AI models and tasks,\n(2) the complex trade-offs that involve energy, time, and model architecture design, and (3) the energy\nsavings opportunity unlocked by automated optimization (Section 4).\nThis paper describes the state of the ML.ENERGY Benchmark and Leaderboard as ofearly 2025.\nThe latest version of the ML.ENERGY Benchmark is open-source on GitHub,2 and the ML.ENERGY\nLeaderboard allows everyone to browse full results from the latest ML.ENERGY Benchmark.\n2 Design Principles\nThe design of the ML.ENERGY Benchmark is guided by four core principles. Our overarching goal\nis to create a benchmark that is representative of real-world generative AI service deployments, and\nto produce energy measurement results that are accurate, reusable, and ultimately actionable.\n2.1 Generalizability and Portability\nGoal.Every computer system is configured with different hardware and software components, and\nmeasurements from a particular system will never truly represent those from another system. For\ninstance, systems can be configured with different CPU and DRAM models, and running different\nLinux kernel versions with different daemons running in the background. Further, not all users have\n2https://github.com/ml-energy/benchmark\n2\n\nphysical access to the target system hardware, a common case for cloud-based environments. Still,\nwe wanted (1) the benchmark to run seamlessly on a wide variety of systems, and (2) measurement\nresults to provide generalizable insights and recommendations across a wide range of systems.\nOur approach.We focus on software-based GPU energy measurement for the following reasons:\n• GPUs are the dominant worker and energy consumer in a system running ML services, accounting\nfor 50–70% of the total provisioned power in the datacenter [52–54, 58].\n• Compared to other hardware components, GPU models are more standardized across different\nsystems [13], making measurements useful across systems that use the same GPU.\n• GPUs allow accurate software-based energy measurement [1, 2, 11, 81], allowing measurement\ntools to be portable across systems without requiring physical hardware access or modification.\n2.2 Representing Real-World Deployments\nGoal.Benchmarking results often inform real-world deployment optimizations, are used to plan\nfuture power capacity and energy usage, affect the design of new hardware and software systems, and\nserve as base numbers for long term projections that affect policymaking. Therefore, it is crucial that\nour measurements represent those from real-world deployments as closely as possible.\nOur approach.To obtain realistic measurements, we adhere to the following principles:\n• We adopt production-grade software and hardware (e.g., vLLM [39] on NVIDIA H100 GPUs)\nand run them with generation request workloads that are representative of real-world use cases.\n• During our measurement, we directly run or closely mimic the state of a serving system during\nlong term deployment. This allows us to capture thesteady stateenergy consumption of the\nservice while using a fixed-size benchmarking dataset.\n2.3 Energy Measurement at the Right Granularity\nGoal.Energy can be measured at different computation granularities. For instance, for LLM text\ngeneration, energy can be reported for the end-to-end benchmarking run, for each generated response,\nor for each token generated. Our goal is to measure and report energy consumption at a granularity\nthat is neither too coarse (as it only provides limited insight into the runtime behavior of the service)\nnor too fine (as it may miss important higher-level insights relevant to the service).\nOur approach.Also aligned with our goal of representing real-world deployments (Section 2.2),\nour approach is to mainly report energy consumption at the granularity of a single, whole generation\nresponse to a request (e.g., entire chat response, image, video). This is because any work less than\nthe full response (e.g., per token) is not considered a complete request, and may ignore model- and\ntask-specific characteristics. For instance, for LLM text generation, different models exhibit different\nverbosity(i.e., given the same prompt, different models respond with varying number of tokens), and\ndifferent tasks have vastly different output token length distributions (e.g., chat vs. code generation),\nall of which we want to capture in our measurements.\n2.4 Actionable Measurement Results\nGoal.While energy measurements are useful in themselves, they are even more useful when they\nlead to actionable insights and recommendations. For instance, how much is the potential energy\nsavings of your model without sacrificing accuracy or latency? If your service intends to guarantee a\nspecific generation latency deadline (e.g., 50 ms), what is the energy-optimal configuration, and how\nmuch is the potential energy savings?\nOur approach.The ML.ENERGY Benchmark allows users to provide computation latency con-\nstraints specific to their application scenario (e.g., LLM average Time Per Output Token), and will\nautomatically recommend (1) theenergy-optimalconfiguration that meets the latency constraints,\nand (2) the expected amount of energy savings. Due to the generalizability of our measurements\n(Section 2.1), these recommendations inform the optimization of a wide range of systems.\n3\n\n4\n3000 requests \n……\n……\n……\n……\n3000\nWaiting\nrequests\nRunning \nrequests\nSteady state (stable server utilization) \nLLM Inference Server Request\nTime\nMax\nbatch size\nFigure 2: LLM inference server and per-request energy accounting. The steady state is defined as the\nperiod when batch size is saturated at the server’s maximum configured batch size, and measurements\nduring the steady state represent that of a serving system during long-term deployment.\n3 The ML.ENERGY Benchmark\nThe ML.ENERGY Benchmark is a comprehensive tool for measuring and optimizing the inference\nenergy consumption of generative AI models, built upon our core design principles (Section 2).\nHere, we describe the overall flow of the ML.ENERGY Benchmark (Section 3.1), which includes\nservice-aware energy measurement and accounting (Section 3.2) and automated optimization recom-\nmendations (Section 3.3). Finally, we describe extension points of the ML.ENERGY Benchmark that\nallows users to easily benchmark their customized application scenarios (Section 3.4).\n3.1 Benchmark Flow\nFigure 1 provides an overview of the usage flow of the ML.ENERGY Benchmark. 1 First, the\ngenerative model to benchmark and the request dataset (set of inputs) to use are selected, alongside\nwith the set of configurations to sweep (e.g., GPU model, parallelism configuration, maximum\nbatch size). 2 Then the ML.ENERGY Benchmark runs configurations independently on designated\nhardware, and measures the time and energy consumption of each configuration using Zeus [2],\na library that provides programmatic energy measurement (Section 3.2). 3 After benchmarking\nis complete, users can specify a latency target based on their application requirements. 4 Given\nthat, the ML.ENERGY Benchmark constructs the time–energy Pareto frontier, and recommends the\nenergy-optimal configuration while satisfying the latency target (Section 3.3).\n3.2 Energy Measurement and Service-Aware Energy Accounting\nOur goal is to provide per-request energy measurements (Section 2.3) that are representative of\nreal-world deployments (Section 2.2). However, a realistic serving system batches together the\ngeneration of multiple requests (e.g., iteration-level batching [82] for LLM text generation), making\nthe energy consumption of a single request dependent on all other requests being processed at the\nsame time. Therefore, we implement measurement and energy accounting methods that capture the\nbatching behavior of different types of models.\nDiffusion models.We begin with the relatively more straightforward case of diffusion models,\nwhich are used for text-to-image, text-to-video, and image-to-video generation. Diffusion models are\ntypically batched as a whole, meaning that the energy consumption of a single request is:\nEnergyrequest = Energybatch\nB (1)\nwhere the batch consists ofBimage or video generation requests.\nLLM text generation.Request-level energy accounting is less straightforward for LLM inference,\nbecause iteration-level batching [82] is an essential optimization in any realistic, production-grade\nLLM serving system [39]. Figure 2 shows how requests are served by a serving system implementing\niteration-level batching and how the ML.ENERGY Benchmark performs energy accounting. Because\n4\n\nthe beginning and end of each request are often not aligned with each other, finding each request’s\nindividual energy consumption is non-trivial. For this, we first submit all requests in the request\ndataset, and as the system runs, identify thesteady stateas the time period where the batch size is\nsaturated at the server’s maximum configured batch size. This steady state is designed to closely\napproximate the state of a serving system when it is well-utilized during long-term deployment.\nParticularly, when the system is ramping up initially with a full queue or ramping down at the end\nwith an empty queue, the server runs with a smaller batch size and does not exhibit the same energy\namortization benefits as the steady state. With this, we can derive the average per-request energy\nconsumption with:\nEnergyrequest =\nEnergysteady\nTokenssteady\n× 1\nN\nX\ni\nTokensrequest,i.(2)\nIn essence, we compute the average energy consumption per token during the steady state and multiply\nit by the average number of output tokens to derive the average per-request energy consumption.\nIndividual requests’ energy consumption can also be computed by multiplying the average energy per\ntoken during the steady state by the number of output tokens for each request.\nAs we will see in Section 4, batch size is a critical configuration that significantly affects both\ngeneration time and energy consumption. By sweeping the batch size configuration, the ML.ENERGY\nbenchmark can capture varying levels of system utilization and collect various operation points with\ndifferent time and energy consumption.\n3.3 Automated Optimization Recommendation\nOur goal is to provide actionable insights beyond just energy measurements (Section 2.4) by rec-\nommending energy-optimal configurations for a given model and task. Central to the optimization\nrecommendation is the construction of thePareto frontierof energy vs. time, which is a collection of\nconfigurations where there are no other configurations that lead to both lower energy and lower time.\nThen, the energy-optimal configuration is selected based on user-specified latency constraints.\nLatency constraints inherently depend on the user’s or application’s needs. For example, for image\ngeneration with Diffusion models, computation results are useful only when the full image is\ngenerated, so latency constraints would be specified in terms of the time to generate the whole image.\nOn the other hand, for LLM text generation for chat, output tokens arestreamedto users (either in\nwritten text or synthesized speech) as they are generated. As such, for user-facing conversational\nAI services, as long as the average time per output token is at least as fast as the users’ reading or\nlistening speed, user experience will not be affected [44]. However, for LLM text generation for\ncoding, where code is likely only useful when it is fully generated, latency constraints would be\nspecified in terms of the time to generate the whole snippet, similar to the case of image generation.\nGiven the latency constraints, the time–energy Pareto frontier is used to suggest the minimum-energy\nconfiguration that satisfies the latency constraint.\n3.4 Extending the Benchmark\nThe ML.ENERGY Benchmark is designed to be easily extensible, allowing users to benchmark their\nown models or customized application scenarios.\nModel.The ML.ENERGY Benchmark already supports various popular architectures like\nLlama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A\nfor a full list). Models that are fine-tuned based on already-supported models work as is. Models\nwith different architectures should also work as is as long as they are supported by the underlying\nruntime, like vLLM [39], which supports arbitrary LLMs provided by Hugging Face Transformers.\nRequest dataset.For each task (e.g., LLM text generation for chat), the ML.ENERGY Benchmark\nprovides a default request dataset that contains a set of inputs representative of real-world usage (See\nAppendix A for a full list). Users can also provide their own request dataset, which can be used to\ninvoke the runtime and measure energy consumption.\nConfiguration space.The ML.ENERGY Benchmark provides a default set of configurations\nspecific to tasks. For instance, for LLM text generation, it supports maximum batch sizes and\n5\n\nparallelism configuration (e.g., tensor and pipeline parallelism). For diffusion models, it supports not\nonly batch size, but also changing the number of denoising steps, as it has a non-trivial impact on time,\nenergy, and output quality. Users can customize the range of values swept for each configuration,\nand also provide new configurations (e.g., GPU power limit [1, 81]) as long as they implement\nthe corresponding configuration interface in the top-level routine. More configuration dimensions\nand finer grained sweeps will lead to longer benchmarking time, but will also push the Pareto\nfrontier towards the lower left corner of the time–energy space, leading to the discovery of more\nenergy-efficient configurations.\nHardware.As long as the runtime used by the ML.ENERGY Benchmark (e.g., vLLM) is capable\nof running on the target hardware and Zeus [2] can measure energy consumption on the target\nhardware (e.g., NVIDIA/AMD GPUs, Intel/AMD CPUs, Apple Silicon, NVIDIA Jetson platforms),\nthe ML.ENERGY Benchmark can run on the target hardware as is.\nMetrics.Energy is a fundamental physical quantity that can be used to derive other useful metrics,\nthough these derived metrics arenotautomatically computed by default as they require context-\nspecific information. Below, we describe how these metrics might be computed based on the\nbenchmark’s outputs.\n• Average power draw (Watts): Average power draw over the steady state can be calculated by\ndividing total energy consumption during the steady state by the duration of the steady state.\n• Throughput per Watt: Work throughput, e.g., request or token generation throughput, divided\nby average power draw can describe how muchservice capacitycan be extracted from the system\ngiven a power budget, which is a critical quantity for datacenter power planning [38].\n• Monetary cost ($): The electricity cost of compute can be calculated by integrating over time the\nmultiplication of energy consumption and the electricity price in the region and time instance. If\nthere is a specific region and time frame the service is expected to run, choosing that electricity\nprice can simulate the operational electricity cost of deployment. Electricity prices can be obtained\nfrom sources like OpenEI.3 Calculating the electricity cost from energy is supported by Zeus [2],\nthe measurement library of choice for the benchmark.\n• Operational carbon emissions (gCO2e): This quantityestimatesthe greenhouse gas emissions\nassociated with the electricity consumed. It can be calculated by multiplying energy consumption\nby the carbon intensity (gCO 2e/kWh) of the particular region and time frame in which the\nbenchmark was run. Carbon intensity data can be obtained from sources like ElectricityMaps.4\nThis is also supported by Zeus [2], the energy measurement library employed by the benchmark.\n4 Results Highlight\nIn this section, we highlight notable results from the ML.ENERGY Benchmark; the full set of\nresults is available on the ML.ENERGY Leaderboard.5 The early 2025 iteration of the benchmark\nand leaderboard presents energy measurements across 40 models and 6 tasks (See Appendix A\nfor a full list). We ran the benchmark on NVIDIA A100 (40 GB) and H100 (80 GB) GPUs,\neach using AWS p4d.24xlarge and p5.48xlarge instances, respectively, and used vLLM [39] and\nDiffusers [77] as the inference runtime. In the following, we first present energy measurement results\nand discuss implications (Section 4.1), and then provide deeper understanding by showing how model\narchitecture choices affect their energy consumption (Section 4.2). Then, we present the energy\nsavings opportunities from our automated optimization recommendations (Section 4.3).\n4.1 Energy Measurements\nSignificant variation in energy consumption.The solid bars in Figure 3 (A100 GPUs in Figure 3a\nand H100 in Figure 3b) show the per-request energy consumption of various generative AI models\nacross different tasks. First, energy consumption varies widely across models. In particular, Diffusion\nmodels generally consume energy that is on par with larger LLMs (e.g., Mistral Large (123B)). This\n3https://openei.org/wiki/Utility_Rate_Database\n4https://electricitymaps.com/\n5https://ml.energy/leaderboard\n6\n\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n2.5x 1.8x\n1.7x\n2.1x\n3.3x\n2.5x\n2.2x 2.1x 1.5x 1.9x 1.8x\n2.6x\n2.7x\n3.2x\n1.6x 1.4x\n2.1x\n2.4x\n2.7x 1.9x 1.4x 3.4x 1.6x\n1.6x\n1.3x 1.5x 1.5x 1.5x\n1.4x 1.1x\n1.0x\n1.4x\n1.0x\n1.1x\n1.2x\n1.1x\n1.0x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n3.2x 2.5x\n2.4x\n2.6x\n2.5x\n2.5x\n2.7x 2.8x 2.1x 2.9x 2.3x\n3.2x\n3.3x\n3.3x\n2.3x 1.8x 2.0x\n2.3x\n3.4x 2.7x 2.1x 4.1x 2.4x\n2.0x 1.8x 1.9x 2.1x 2.1x\n1.9x 1.2x\n1.2x\n2.0x\n1.0x 1.1x\n1.5x\n1.2x\n1.1x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (H100)\n(b) NVIDIA H100\nFigure 3: Per-request energy consumption across various generative AI models. Black and orange\nrepresents text and vision modalities, respectively. Solid bars are energy measurements, whereas\ndimmed bars behind each solid bar are estimations based on the GPU’s TDP, with numbers showing\nthe ratio of overestimation. Note the log scale Y-axis.\nModel TP Max batch size\n4 8 16 32 64\nDeepSeek distilled Qwen 3 8B [23, 80] 1 9713.7 6010.1 4314.9 3340.8 2770.8\nPhi 4 reasoning plus 15B [3] 1 19974.4 12389.6 9347.3 7634.9 7595.4\nQwen 3 32B [80] 2 26419.7 15168.3 9140.5 6165.5 4520.6\nQwen 3 235B-A22B thinking [80] 8 122523.1 86491.5 56720.4 40275.5 33096.4\nTable 1: Energy per generation of reasoning models on GPQA [64] and NVIDIA H100 GPUs. TP is\nthe tensor parallelism degree, which is also equal to the number of GPUs used.\nis mainly because Diffusion models (1) draw higher power in general (more in Section 4.2) and (2)\ncannot perform as many concurrent generations compared to LLMs due to their long latency in real\nservices, preventing them from amortizing energy consumption across many generations.\nImportance of measuring.The dimmed bars behind each solid bar in Figure 3 show the estimated\nenergy consumption based on the GPU’s Thermal Design Power (TDP) instead of measuring the\nreal GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74]. Estimations using\nTDP are nearly always an overestimation since it is rare for a GPU – or any computing device –\nto draw its maximum power at every moment in time. In fact, such an estimation can lead to a\nworst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).\nInaccuracies may be overlooked when they influence downstream decisions and projections, leading\nto misleading conclusions. Accurate measurements that reflect production environments are crucial.\n4.2 Energy Implications of ML Design Decisions\nML decisions reflected in model architectures and trained models impact energy consumption. For\nthe interest of space, we defer systems implications on energy consumption to Appendix B.\nLLM response verbosity and energy.In Figure 3, we can see that energy consumption varies even\namong LLMs of similar sizes. This is because different LLMs generate responses of differentlength\neven when given the same prompt. Such differences inverbositycan be non-trivial; for instance,\nMistral Large’s responses were on average 36% longer than that of Mixtral 8×7B. As the number of\n7\n\n0 50 100 150 200 250 300\nBatch size\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPhi 3 mini (3.8B)\nPhi 3 small (7B)\n(a) Energy vs. Batch Size\n0 200 400 600 800\nMaximum batch size configuration\n0\n100\n200\n300\n400Batch size\nPhi 3 mini (3.8B)\nPhi 3 small (7B) (b) Batch Size vs. Max Batch Size config\nFigure 4: Phi-3 Mini and Small [26] benchmarked with the chat task on one NVIDIA A100 GPU.\n0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100\n(a) Llama 3.1 70B [73]\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (b) Stable Diffusion 3 Medium [25]\nFigure 5: Power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models.\noutput tokens equals the number of forward passes through the model, longer responses leads to a\nproportional increase in energy consumption. As humans are known to prefer longer responses [85],\nthis potentially introduces a trade-off between energy consumption and user satisfaction.\nThis is even more pronounced for reasoning models, which produce significantly more output tokens.\nTable 1 shows energy measurements for reasoning models on the GPQA dataset. Reasoning models\nproduce one to two orders of magnitude more output tokens per request compared to standard chat\nmodels, significantly increasing energy consumption per generation. Additionally, due to their long\noutput lengths, servers cannot run as large a batch size, preventing them from amortizing energy across\nmore requests. This leads to higher energy per token as well, further increasing energy consumption.\nAs long horizon reasoning and task decomposition become more common in real-world LLM-based\napplications, we expect this trend to continue.\nMemory consumption of operations and energy amortization.Generally, models with more\nparameters consume more energy, but this is not always the case. Figure 4 highlights the case of\nPhi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the\nleft plot shows that the larger Small model can consume less energy than Mini as batch size grows.\nThis happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped\nQuery Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3 × more memory than Small,\nwhich prevents it from scaling to larger batch sizes and amortizing energy consumption across more\ngenerations.\nCompute-intensity of operations and power draw.Figure 5 shows the power consumption of\nLlama 3.1 70B [73] and Stable Diffusion 3 Medium [25] on A100 and H100 GPUs. It can be\nseen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum,\nwhereas the Diffusion model’s power consumption is close to the maximum. This is because LLM\ndecoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations\n(e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the\nGPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s\ncomputation units being underutilized, leading to low power draw. Appendix C dives deeper into\npower consumption with measurements for all models and GPU power breakdowns over time.\n8\n\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nBatch size\n0\n1000\n2000\n3000Energy consumption (J)\nSDXL (1024x1024)\nSDXL Turbo (512x512)\n(a) Different Resolutions\n0 10 20 30 40 50\nNumber of denoising steps\n0\n1000\n2000\n3000Energy consumption (J)\nSXDL (1024x1024)\nSDXL Turbo (512x512) (b) Varying Denoising Steps\nFigure 6: Energy consumption of SDXL [61] and SDXL Turbo [7] on one NVIDIA A100 GPU.\n0.0 0.1 0.2 0.3 0.4\nAverage Time Per Output Token (s)\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPareto frontier\nA100 H100\n(a) Llama 3.1 8B [73]\n0 5 10 15 20 25\nGeneration latency (s)\n0\n100\n200\n300\n400\n500Energy consumption (J)\nPareto frontier\nA100 H100 (b) Stable Diffusion 2.1 [65]\nFigure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark.\nInference-time parameters and energy.Figure 6 shows the energy consumption of Stable Diffu-\nsion XL (SDXL) [61] and SDXL Turbo [7]. On the left, while SDXL and SDXL Turbo have identical\nmodel sizes and architectures, their energy consumption is significantly different. This is because\nSDXL Turbo is tuned to generate smaller resolution images (512×512) than SDXL (1024×1024),\nwhich leads to different latent sizes and amounts of computation. On the right, it can be seen that the\nnumber of denoising steps linearly increases energy consumption, as one denoising step requires one\nforward pass through the model. While simple in isolation, these inference-time parameters lead to\nnon-trivial design tradeoffs at the application-level. For instance, increasing the number of denoising\nsteps may improve final image quality, but beyond some point, it may be virtually indistinguishable\nto human users. Also, generating images in lower resolution and then upscaling them with a separate\nsuper-resolution model (e.g., DAT [19]) may consume less energy end-to-end.\n4.3 Automated Energy Optimization Recommendation\nFigure 7 shows the time–energy Pareto frontier constructed by the ML.ENERGY Benchmark mea-\nsurement results for Llama 3.1 8B and Stable Diffusion 2.1. In general, the Pareto frontier is convex,\nmeaning that by sacrificing some latency, one can achieve significant energy savings.\nConversational AI services like LLM-based chatbots achieve interactivity by streaming tokens to users\neither in written text or synthesized speech, making Time Per Output Token (TPOT) an important\nperformance metric that impacts user experience [44]. In this context, a chatbot provider can target\nan average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]),\nwhich is sufficient for most reading or listening speeds. This will land on the Pareto frontier at the\npoint where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared\nto the configuration that simply minimizes latency.\nHere, we note that for Llama 3.1 8B [73], the Pareto frontier is a mixture of configurations from both\nA100 and H100 GPUs. This is because LLM decoding does not fully exert the GPU’s compute units\nand are rather bound by memory, so going from A100 to H100 GPUs neither provides significantly\nhigher performance nor significantly increases power draw (See Appendix C for details). These two –\npower and time – multiplied, energy consumption is comparable across the two GPUs.\n9\n\nOn the other hand, for Stable Diffusion 2.1 [65], the Pareto frontier is dominated by configurations\non the H100 GPU. Diffusion models consume power close to the GPU’s TDP (See Appendix C\nfor details), which increases power draw significantly when going from A100 to H100. However,\nsince computation latency was reduced even more, configurations on H100 Pareto-dominate those on\nA100. If an application has a generation latency target of, for instance, 5 seconds, the energy-optimal\nconfiguration will lie on the Pareto frontier where latency is 3.63 seconds, which is 21% less energy\nthan the configuration that minimizes latency.\n5 Related Work\nML energy measurement.The Hugging Face LLM-Perf leaderboard [33] is specific to LLMs\nand reports theper-tokenenergy consumption of LLM text generation, which fails to capture the\nverbosity and task-specific output token length distribution difference of LLMs (Section 2.3). MLPerf\nPower [75] provides measurements for ML training and inference, but crucially, requires direct access\nto the system under test to physically install the power analyzer, which significantly limits who can\nrun the benchmarks (Section 2.1). Furthermore, it benchmarks at most a few model architectures for\neach task (sometimes only one), failing to provide insights on how ML design choices impact energy\nconsumption. The Hugging Face AI Energy Score leaderboard [27] provides measurement data for\nbroader AI tasks. However, it fixes the inference batch size to 1 for all models, failing to reflect how\nservices are deployed in the real world and thus their energy consumption (Section 2.2). Google\ndisclosed the median energy consumption of their AI service [24]. It provides a comprehensive\nscope of measurement, even including the energy consumption of idle machines provisioned for\nstable service operation. However, measurements and reports are based on internal Google systems,\nworkloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the\ngeneralizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the\nfirst inference energy benchmark for modern generative AI models, and empowers users to not only\nmeasure but also optimize the energy consumption of their models. See Appendix D for more details.\nML energy optimization.The ML.ENERGY Benchmark provides automated energy optimization\nrecommendations based on energy measurements (Section 3.3). There are several other efforts that\nalso provided automated energy optimizations – while preserving mathematical equivalence and/or\nmodel quality – for ML training and inference. Zeus [81], EnvPipe [20], and Perseus [21] optimizes\nthe energy consumption of ML training by adjusting GPU-level and training job-level configurations,\neither statically after profiling or dynamically during training. µ-Serve [63] and DynamoLLM [68]\nare also similar, but optimize energy consumption for ML inference clusters. Optimization recom-\nmendations by the ML.ENERGY Benchmark are complementary to the techniques proposed by these\nworks. Further, our results support the need for automatedcross-layerenergy optimizations that span\nall model, software, and hardware layers [22], as opposed to efforts siloed within a single layer.\n6 Conclusion\nIn this work, we described the ML.ENERGY Benchmark, a comprehensive energy benchmark for\ngenerative AI models that not only provides realistic energy measurements, but also automatically\nsuggests energy-optimal configurations based on user- and app-specific performance constraints.\nMeasurement results show that energy consumption is a metric that is impacted by design choices\nacross the whole AI stack, including application, model, software, and hardware, demonstrating the\nimportance of automatedcross-layerenergy optimizations instead of siloed optimizations within\na single layer. We are confident that the ML.ENERGY Benchmark will democratize the art of\nmeasuring, understanding, and optimizing ML energy consumption for the community.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Yunseok Jang and SymbioticLab members for helpful comments and sugges-\ntions on the paper. This work and its authors were in part supported by NSF grants CNS-2104243,\nCNS-2106184, and CNS-2450085, grants from VMware, the Mozilla Foundation, Cisco, Ford, and\nGitHub, and gifts from Salesforce and Google. Jae-Won Chung is additionally supported by the\nKwanjeong Educational Foundation.\n10\n\nReferences\n[1] NVIDIA Management Library (NVML). https://developer.nvidia.com/\nnvidia-management-library-nvml.\n[2] Zeus: Deep learning energy measurement and optimization. https://github.com/\nml-energy/zeus.\n[3] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl,\nLingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero\nKauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dim-\nitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue\nWu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report.arXiv preprint\narXiv:2504.21318, 2025.\n[4] International Energy Agency. Electricity 2025, 2025.\n[5] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-\nvani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in\nLLM inference with Sarathi-Serve. InOSDI, 2024.\n[6] Character AI. Character ai.https://character.ai, 2023.\n[7] Stability AI. Introducing SDXL turbo: A real-time text-to-image generation model, 2023.\n[8] AI@Meta. Llama 3 model card. 2024.\n[9] AI@Meta. Llama 4 model card. 2024.\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and\nSumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head\ncheckpoints.Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\n[11] Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly, Ayatelrahman Elsayed,\nAbdel-Hameed Badawy, Gopinath Chennupati, Stephan Eidenbenz, and Nandakishore Santhi.\nVerified instruction-level energy consumption measurement for NVIDIA GPUs.Proceedings of\nthe 17th ACM International Conference on Computing Frontiers, 2020.\n[12] Jeff Bar. Amazon EC2 update – inf1 instances with AWS inferentia chips for high performance\ncost-effective inferencing, 2019.\n[13] Nathan Beniach and Air Street Capital. State of AI report compute index. https://www.\nstateof.ai/compute.\n[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\nminik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, Varun Jampani, and Robin\nRombach. Stable video diffusion: Scaling latent video diffusion models to large datasets.arXiv\npreprint arXiv:2311.15127, 2023.\n[15] CBRE. Global data center trends 2023. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2023, 2023.\n[16] CBRE. Global data center trends 2024. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2024, 2024.\n[17] CBRE. Global data center trends 2025. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2025, 2025.\n[18] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong\nDuan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.\nSharegpt4video: Improving video understanding and generation with better captions.Advances\nin Neural Information Processing Systems Datasets and Benchmarks, 2024.\n[19] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual\naggregation transformer for image super-resolution.Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2023.\n[20] Sangjin Choi, Inhoe Koo, Jeongseob Ahn, Myeongjae Jeon, and Youngjin Kwon. EnvPipe:\nPerformance-preserving DNN training framework for saving energy.Proceedings of the 2023\nUSENIX Annual Technical Conference, 2023.\n11\n\n[21] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury.\nReducing energy bloat in large model training.Proceedings of the 30th ACM Symposium on\nOperating Systems Principles, 2024.\n[22] Jae-Won Chung, Nishil Talati, and Mosharaf Chowdhury. Toward cross-layer energy opti-\nmizations in AI systems.DOE ASCR Energy-Efficient Computing for Science Workshop,\n2024.\n[23] DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement\nlearning.arXiv preprint arXiv:2501.12948, 2025.\n[24] Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah\nGoodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes,\nand James Manyika. Measuring the environmental impact of delivering AI at google scale.\narXiv preprint arXiv:2508.15734, 2025.\n[25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion\nEnglish, and Robin Rombach. Scaling rectified flow transformers for high-resolution image\nsynthesis. InICML, 2024.\n[26] Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your\nphone.arXiv preprint arXiv:2404.14219, 2024.\n[27] Hugging Face. Ai energy score. https://huggingface.github.io/AIEnergyScore,\n2025.\n[28] Meta GenAI. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint, 2023.\n[29] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh\nAgrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image\ndiffusion models without specific tuning. InICLR, 2024.\n[30] Yatharth Gupta, Vishnu V . Jaddipal, Harish Prabhala, Sayak Paul, and Patrick V on Platen.\nProgressive knowledge distillation of stable diffusion xl using layer level loss.arXiv preprint\narXiv:2401.02677, 2024.\n[31] The White House. Fact sheet: President donald j. trump establishes the national energy\ndominance council, 2025.\n[32] HPCwire. AWS to offer NVIDIA’s T4 GPUs for AI inferencing, 2019.\n[33] Régis Pierrard Ilyas Moutawwakil. LLM-Perf leaderboard. https://huggingface.co/\nspaces/optimum/llm-perf-leaderboard, 2023.\n[34] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b.arXiv preprint\narXiv:2310.06825, 2023.\n[35] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier,\nMarie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,\nTeven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\nWilliam El Sayed. Mixtral of experts.arXiv preprint arXiv:2401.04088, 2024.\n[36] Heehoon Kim, Junyeol Ryu, and Jaejin Lee. TCCL: Discovering better communication paths\nfor PCIe GPU clusters.Proceedings of the 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume 3, 2024.\n[37] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan\nGenc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir\nGholami. Full stack optimization of transformer inference.Architecture and System Support\nfor Transformer Models, 2023.\n[38] Helen Kou. Power for AI: Easier said than built. https://about.bnef.com/insights/\ncommodities/power-for-ai-easier-said-than-built/, 2025.\n12\n\n[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language\nmodel serving with PagedAttention.Proceedings of the 29th Symposium on Operating Systems\nPrinciples, 2023.\n[40] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying\nthe carbon emissions of machine learning.arXiv preprint, 2019.\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. InCVPR, 2024.\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.\nLLaV A-NeXT: Improved reasoning, ocr, and world knowledge, 2024.\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.Advances\nin Neural Information Processing Systems, 2023.\n[44] Jiachen Liu, Jae-Won Chung, Zhiyu Wu, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.\nAndes: Defining and enhancing quality-of-experience in LLM-based text streaming services.\narXiv preprint arXiv:2404.16283, 2024.\n[45] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nchatGPT really correct? rigorous evaluation of large language models for code generation. In\nNeurIPS, 2023.\n[46] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noua-\nmane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack\nv2: The next generation.arXiv preprint arXiv:2402.19173, 2024.\n[47] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon\nfootprint of bloom, a 176b parameter language model.Journal of Machine Learning Research,\n2024.\n[48] McKinsey & Company. Investing in the rising data center economy. https:\n//www.mckinsey.com/industries/technology-media-and-telecommunications/\nour-insights/investing-in-the-rising-data-center-economy, 2023.\n[49] McKinsey & Company. How data centers and the energy sector can sate AI’s hunger for\npower. https://www.mckinsey.com/industries/private-capital/our-insights/\nhow-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power ,\n2024.\n[50] Midjourney. Midjourney.https://midjourney.com, 2022.\n[51] Sebastian Moss. Meta’s mark zuckerberg says energy constraints are holding back AI data\ncenter buildout, 2024.\n[52] NVIDIA. NVIDIA DGX A100 datasheet. https://www.nvidia.com/content/dam/\nen-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020.\n[53] NVIDIA. NVIDIA DGX H200 datasheet. https://resources.nvidia.com/\nen-us-dgx-systems/dgx-h200-datasheet, 2024.\n[54] NVIDIA. NVIDIA DGX B200 datasheet. https://resources.nvidia.com/\nen-us-dgx-systems/dgx-b200-datasheet, 2025.\n[55] OpenAI. What are tokens and how to count them? https://help.openai.com/en/\narticles/4936856-what-are-tokens-and-how-to-count-them.\n[56] OpenAI. ChatGPT.https://chatgpt.com, 2022.\n[57] OpenAI. Sora.https://openai.com/index/sora, 2024.\n[58] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam,\nand Ricardo Bianchini. Characterizing power management opportunities for llms in the cloud.\nASPLOS, 2024.\n[59] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and\nRicardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. InISCA,\n2024.\n13\n\n[60] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network\ntraining.arXiv preprint, 2021.\n[61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller,\nJoe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution\nimage synthesis. InICLR, 2024.\n[62] PromptHero. OpenJourney v4, 2023.\n[63] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus\nFranke, Zbigniew Kalbarczyk, Tamer Ba¸ sar, and Ravishankar K. Iyer. Power-aware deep\nlearning model serving with u-Serve. InATC, 2024.\n[64] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a\nbenchmark. InCoLM, 2024.\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer.\nHigh-resolution image synthesis with latent diffusion models. InCVPR, 2022.\n[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\nXiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for\ncode.arXiv preprint arXiv:2308.12950, 2024.\n[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training multi-billion parameter language models using model\nparallelism.arXiv preprint, 2019.\n[68] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM:\nDesigning llm inference clusters for performance and energy efficiency. InHPCA, 2025.\n[69] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models.arXiv preprint\narXiv:2405.09818, 2024.\n[70] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea\nHu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis,\nMateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham\nAgrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec,\nKelly Schaefer, and Scott Huffman. CodeGemma: Open code models based on gemma.arXiv\npreprint arXiv:2406.11409, 2024.\n[71] Gemma Team. Gemma 2: Improving open language models at a practical size.arXiv preprint\narXiv:2408.00118, 2024.\n[72] ShareGPT Team. ShareGPT.https://sharegpt.com/.\n[73] Llama team at Meta. The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open\nand efficient foundation language models.arXiv preprint, 2023.\n[75] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holle-\nman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver\nCurtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao\nPan, Manu Prasad Manmathan, Liz Raymond, Tom St. John, Arjun Suresh, Rowan Taubitz, Sean\nZhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi. MLPerf power: Benchmarking the\nenergy efficiency of machine learning systems from uWatts to MWatts for sustainable ai. In\nHPCA, 2025.\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in Neural Information\nProcessing Systems, 2017.\n[77] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas\n14\n\nWolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/\ndiffusers.\n[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.\nModelScope text-to-video technical report.arXiv preprint arXiv:2308.06571, 2023.\n[79] Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, and Xin Jin. ServeGen: Workload\ncharacterization and generation of large language model serving in production.arXiv preprint\narXiv:2505.09999, 2025.\n[80] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang,\nFeng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin\nYang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin\nZhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,\nXingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng\nZhou, and Zihan Qiu. Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.\n[81] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU\nenergy consumption of DNN training.NSDI, 2023.\n[82] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca:\nA distributed serving system for Transformer-Based generative models. InOSDI, 2022.\n[83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive\nmodels for content-rich text-to-image generation.Transactions on Machine Learning Research,\n2022.\n[84] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang,\nDeli Zhao, and Jingren Zhou. I2VGen-XL: High-quality image-to-video synthesis via cascaded\ndiffusion models.arXiv preprint arXiv:2311.04145, 2023.\n[85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena. InNeurIPS, 2023.\n[86] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang. DistServe: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving. InOSDI, 2024.\n15\n\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction reflect the paper’s contributions and scope.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Limitations are discussed in Appendix E.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\n16\n\nJustification: This paper does not include theoretical results.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.\nThe full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.\nenergy/leaderboard.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5.Open access to data and code\n17\n\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.\nThe full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.\nenergy/leaderboard.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We mention important details in the main paper and provide more details in\nthe Appendix. Full details are available in the code repository.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: We were not able to run the benchmark multiple times due to the high monetary\ncost of even a single run.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n18\n\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We mention compute resources in the beginning of Section 4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We confirm that we reviewed the NeurIPS Code of Ethics and that our research\nconforms to it.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss this in Appendix F.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n19\n\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: We do not believe safeguards are necessary for our work.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We extensively use models and datasets created by others in our benchmark.\nWe credit the authors through citation. Appendix A contains a comprehensive table. The\nbenchmark has default request datasets that we recommend, but does not come packaged\nwith any specific model or dataset.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n20\n\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: The benchmark code is available open-source and documented at https:\n//github.com/ml-energy/leaderboardunder the Apache-2.0 license.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n21\n\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16.Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: We have used LLMs to assist in editing the paper, generating figures, and writ-\ning code snippets, and its use does not impact the core methodology, scientific rigorousness,\nor originality of the research.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22\n\nTable 2: Model type, task, and default request dataset used in the ML.ENERGY Benchmark.\nModel architecture Task Request dataset\nLarge Language Model Chat ShareGPT [72]\nCode EvalPlus [45]\nVision Language Model Visual chat LLaV A instruction dataset [43]\nDiffusion Model\nText-to-image PartiPrompts [83]\nText-to-video Captions in ShareGPT4Video [18]\nImage-to-video Captions and first frames in ShareGPT4Video [18]\nTable 3: Model architectures supported by the ML.ENERGY Benchmark for each task.\nTask Model Architectures\nChat Gemma 2 2B/9B/27B [71], Llama 3.1 8B/70B/405B [73],\nPhi 3 Mini/Small/Medium [26], Mistral 7B/Nemo/Large [34],\nMixtral 8x7B/8x22B [35]\nCode CodeLlama 7B/13B/34B/70B [66], StarCoder 2 3B/7B/15B [46],\nCodeGemma 2B/7B [70]\nVisual chat LLaV A 1.5 7B/13B [41], LLaV A NeXT 8B [42], Phi 3 Vision [26],\nChameleon 7B/30B [69]\nText-to-image Stable Diffusion 2.1/XL/XL Turbo/3 Medium [7, 25, 61, 65],\nOpenJourney 4 [62], SSD 1B [30]\nText-to-video ModelScope T2V [78], AnimateDiff [29]\nImage-to-video I2VGen XL [84], Stable Video Diffusion and Stable Video Diffusion XT [14]\nA Tasks, Model Architectures, and Default Request Datasets\nTables 2 and 3 list the model architectures and tasks supported by current iteration of the ML.ENERGY\nBenchmark, along with the default request datasets for each task. We note that models that were\nfine-tuned based on the supported models are also supported as is, and the benchmark is designed to\nbe extensible (Section 3.4).\nThe ML.ENERGY Benchmark cannot avoid being outdated given the rapid pace of development\nin the generative AI field. As such, we have been updating the benchmark (and the accompanying\nLeaderboard) with new tasks, models, datasets, hardware, runtimes, and more, and we intend to\ncontinue doing so as long as resources allow.\nB Energy Implication of System Parameters\nThis section discusses the energy implication of different system-level configurations. System-level\nconfigurations are those that do not changewhatis computed but ratherhowit is computed by the\nunderlying software system.\nB.1 Request Preemption Mechanism\nEven with the model and inference parameters fixed, the software system used to serve inference\nrequests, which determines how model computations are executed on a given hardware, significantly\nimpacts energy consumption. As a concrete example, we will examine the effect of “preemption\nmechanism,” a configuration parameter for LLM inference servers. When a server is overloaded with\nmore requests than its capacity, it needs to temporarily remove (or, preempt) some requests from the\nsystem and then later bring them back (or, restore). For LLM inference, there are two widely-used\nmechanisms for preemption: Recomputation and Swapping [39]. Recomputation simply drops all\ntemporary request data or state on preemption and recomputes everything from scratch on restoration.\nOn the other hand, Swapping moves the request state to the CPU’s memory, and then returns it to\nthe GPU on restoration. The best preemption mechanism depends on the computing hardware and\nsoftware configuration and the LLM being served.\n23\n\n0 250 500 750 1000 1250 1500\nMaximum batch size configuration\n0\n20\n40\n60\n80\n100Energy consumption (J)\nServer is overloaded\nRecomputation\nSwapping\nFigure 8: Energy consumption per generation while varying the maximum batch size for Mistral\nNemo (12B). The LLM inference server’s preemption mechanism is compared.\n0 200 400 600 800 1000\nBatch size\n0\n100\n200\n300\n400Energy consumption (J)\n1 GPU\n2 GPUs\n4 GPUs\n8 GPUs\nFigure 9: Energy consumption per generation while varying batch size for Llama 3.1 8B. The number\nof NVIDIA A100 GPUs used to run the same model is scaled up.\nFigure 8, we compare the energy consumption per generation of the two preemption mechanisms with\nthe Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size\nconfiguration and causing preemption. It can be seen that when the server is overloaded, Swapping\nconsistently consumes less energy. This is because Recomputation performs extra computation\nwhen restoring requests whereas Swapping copies data without running computation, and the energy\nconsumption of computation is larger than memory operations (this will be further examined in\nthe next section). Furthermore, as the server gets more and more overloaded, energy consumption\ngenerally increases. This is because with higher overload, more preemptions – and thus more\nrecomputation or data movement – occur. Since preemptions do not directly contribute to the\ncompletion of the request, the extra energy consumption from preemptions increases the average\nenergy consumption of completing each request.\nB.2 Tensor Parallelism Scaling\nWe investigate the impact of communication overhead to energy consumption. This is important as\nmodern large models frequently do not fit within the memory capacity of a single GPU. This requires\nmultiple GPUs to execute inference for a single model, and GPUs must constantly communicate with\neach other to do so [67].\nIn order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary\nthe number of GPUs used (Figure 9). Because the amount of computation executed is the same\nregardless of the number of GPUs, energy consumption should ideally be constant. Indeed, energy\nconsumption barely changes when scaling from one GPU (no communication) to two, but when\nscaling further, energy consumption significantly increases. This is because, while the amount of\ncomputation decreases for each GPU, additional communication time between the GPUs offsets the\nreduction in computation time. Since communication time increases with the number of GPUs, using\ntoo many GPUs can lead to slowdowns in executing the same amount of computation and increase\nenergy consumption.\n24\n\nModel and deployment Request dataset\nInput mean 512\nOutput mean 512\nInput mean 512\nOutput mean 4096\nInput mean 4096\nOutput mean 512\nLlama 3.1 8B (TP=1, 1P3D) 37.71, 77.2% 665.77, 98.7% 208.34, 67.2%\nLlama 3.1 8B (TP=1, 2P2D) 36.22, 76.7% 706.27, 98.8% 151.75, 55.2%\nLlama 3.1 8B (TP=1, 3P1D) 37.26, 77.0% 748.45, 98.9% 158.85, 56.0%\nLlama 3.1 70B (TP=4, 1P1D) 276.93, 64.8% 907.60, 89.2% 1492.59, 50.0%\nTable 4: Energy per generation (Joules) and the percentage of decode energy consumption with\nPD disaggregation. Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean\nspecified in the table. TP means tensor parallelism degree, and xPyD means it was deployed with x\nprefill instances andydecode instances, each with TP-many GPUs.\nMax batch size Max batched tokens\n(sequences)32 64 128 256 512 1024 2048 4096 8192\n32 559.66 374.63 269.29 205.54 188.80 191.61 195.59 191.88 194.52\n64 362.49 266.98 200.43 168.27 165.52 170.17 168.78 169.58\n128 264.18 194.59 164.75 154.64 155.59 156.54 156.93\n256 194.39 161.87 153.97 155.11 157.13 159.25\n512 159.57 151.50 154.52 156.77 154.95\n1024 152.67 156.26 157.98 163.08\nTable 5: Energy per generation (Joules) of Llama 3.1 8B on a synthetic long context request dataset\nrunning on H100 GPUs. Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with mean 4,096 and alpha 2.5, and output lengths from an Exponential distribution with\nmean 512. Note that vLLM does not allow the max number of batched tokens to be smaller than the\nmax batch size, which is why the lower left triangle of the table is empty.\nFrom this scaling experiment, we can observe that the energy impact of communication overhead can\nbe large. This impact will be even more pronounced in hardware environments without sufficient or\nstate-of-the-art networking infrastructure, which is common in real world settings due to its cost [36].\nB.3 Prefill–Decode Disaggregation\nPrefill–decode (PD) disaggregation is a rising production deployment setting where prefill and decode\nphases are run on separate GPUs [59, 86]. This allows for independent scaling and optimization of\nprefill and decode phases based on workload characteristics, and leads to better latency deadline\nattainment. Table 4 shows energy measurements for different PD disaggregation configurations,\nwhere “xPyD” denotesxprefill instances andydecode instances.\nOverall, decode consumes the majority of energy, with some amount shifting to prefill when input\nlength is long. In our setup, PD disaggregation configurations did not have a large impact on absolute\nenergy consumption or the energy split as long as the throughput of prefill and decode instances are\nreasonably balanced.\nB.4 Chunked Prefill\nChunked prefill is a technique where long input prompts are split into chunks and processed alongside\ndecode iterations, improving GPU utilization and reducing the interference between long prefills and\ndecode iterations [5]. For chunked prefill, the max number of batched tokens is a key parameter that\ncontrols the chunk size. Table 5 shows the impact of this parameter on energy consumption.\nTable 5 shows that the more sequences or tokens you batch, the better the energy amortization you\nget and energy per generation decreases, and after a certain point, returns diminish.\n25\n\n0 200 400 600 800 1000\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(a) Llama 3.1 8B\n0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100 (b) Llama 3.1 70B\n0 20 40 60 80 100 120\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(c) LLaV A 1.5 7B\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (d) Stable Diffusion 3 Medium\n0 10 20 30 40 50 60\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(e) Stable Diffusion 2.1\n1 2 3 4\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (f) Stable Video Diffusion\nFigure 10: Power consumption of various models on A100 and H100 GPUs.\nC Power Consumption Analysis\nFigure 10 shows the power consumption of various models on A100 and H100 GPUs. Figure 11\nfurther shows the ratio of a model’s power consumption to the maximum GPU power draw across all\nmodels. Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because\nLLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully\nutilize the GPU’s compute resources. VLMs show slightly higher power consumption than LLMs\ndue to its additional modality encoder, which is compute-intensive. Diffusion models, on the other\nhand, consume nearly the maximum power of the GPU when batch size is not small. This is because\nDiffusion models are significantly more compute-intensive compared to LLM decoding.\nFigure 12 shows the GPU power draw breakdown over time on one NVIDIA H100 GPU. The GPU’s\npower is measured (1) in whole and (2) only for the VRAM while the ML.ENERGY Benchmark\nis running. First, for Llama 3.1 8B, the timeline shows the effect of the two phases in LLM text\ngeneration: Prefill and Decode. Prefill happens once at the beginning of a request to digest the input\nprompt, which is then followed by hundreds to thousands of Decode phases, each of which generates\none output token. Importantly, Prefill has high compute-intensity (and also high power draw) because\nit needs to digest the whole input prompt, whereas Decode has low compute-intensity (and low power\ndraw) as it does not entail very much computation. With this, we can first understand the initial spike\n26\n\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (H100)\n(b) NVIDIA H100\nFigure 11: Ratio of power consumption to maximum GPU power draw across various models.\n0 20 40 60 80\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Llama 3.1 8B on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(a) Llama 3.1 8B\n0 50 100 150 200 250 300\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Stable Video Diffusion XT on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(b) Stable Video Diffusion XT\nFigure 12: GPU power draw breakdown over time on one NVIDIA H100 GPU. “Entire GPU”\nand “Only VRAM” (memory) were measured, and the two were subtracted to derive “Entire GPU\nexcluding VRAM.”\nin power draw – when the benchmark begins, the server begins admitting new requests, creating a\nshort period where numerous Prefills are executed back-to-back, leading to high power draw. After\nthe initial spike, power draw repeats a periodic fluctuation. This is because, before each Prefill or\nDecode, the server must make numerous control decisions, including determining which requests are\nnow finished and which ones should run next. Since these decisions are executed by the CPU, this\ncreates a periodic time gap where the GPU is not running any computation. This GPU idle time leads\nto the periodic drop in GPU power draw.\nOn the other hand, Stable Video Diffusion XT shows a different power draw pattern. Diffusion\nmodels generally have three phases: Encode, Denoise, and Decode. The Encode phase digests the\n27\n\ninput prompt and passes it to the Denoise phase, which iteratively removes noise from a random\nvector. Finally, the Decode phase transforms the denoised vector into the final image or video.\nFrom the timeline, especially Denoise and Decode can be clearly distinguished. Denoise is the most\ncompute-intensive and consumes power close to the GPU’s TDP. For each batch, there are 25 local\npeaks that hit the GPU’s TDP, each of which corresponds to one denoising step in Denoise. During\nDecode, power draw generally decreases, with each local power peak corresponding to the two large\nlayers in the decoding module. On the other hand, VRAM power draw increases during Decode\nbecause it allocates a large chunk of memory and performs writes in order to generate the final video.\nFinally, as the final generated video is copied from the GPU’s memory to the CPU’s, the GPU does\nnot run any computation, resulting in a steep drop in power draw.\nFrom the power breakdown, we can observe that memory operations indeed draw significantly less\npower compared to computation, and thus computations with low compute-intensity should indeed\ndraw less power. Furthermore, we can observe that the power draw and energy consumption of a\nspecific hardware (GPU in this case) is not a function of just itself and the computations that it runs.\nRather, software and hardware components that are integrated in the same system stack impacts how\ncomputations are executed on each other, affecting their power draw and energy consumption.\nD The ML.ENERGY Leaderboard and Benchmark\nOn July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy\nleaderboard for modern generative AI models.6 Our goal was to measure and understand the energy\nconsumption of generative AI models, and we provided a web-based leaderboard to allow everyone\nto browse the results. The leaderboard started with only LLM chat with tens of different LLMs, but\ngradually expanded to include more tasks, models, and datasets. Our benchmarking suite to supply\ndata to the leaderboard is what we dub the ML.ENERGY Benchmark. This paper shares our design\nphilosophy and principles we have acquired over time by gradually maintaining and upgrading the\nML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained\nfrom the early 2025 iteration of the benchmark. Importantly, we plan to continuously update the\nbenchmark and the leaderboard as long as resources allow, and what is presented in this paper is only\na snapshot of the current state of the benchmark at the time of writing. We encourage readers to visit\nthe leaderboard website and benchmark repository for the latest results and updates.\nE Limitations\nThe ML.ENERGY Benchmark is not without limitations. First, we note that the benchmark is not\nexhaustive and does not cover all possible tasks, models, and datasets. This is particularly true as\ntime passes and new models and tasks are developed. We are aware of newer open-weight models\nand worthy tasks that were released after the early 2025 iteration of the benchmark was finalized.\nHowever, we cannot add each model or task one by one incrementally as they are released, due to the\nprohibitive monetary cost of running the benchmark on representative hardware; rather, we collect\nnew advances in a window of time and then mass-update the whole benchmark, accompanied by\nupgrades in hardware, software, and datasets. Second, the benchmark is not exhaustive in terms\nof hardware. We currently mainly support flagship NVIDIA GPUs, which arguably dominates the\nmarket especially when it comes to real-world generative AI services. Furthermore, we do not have\naccess to all possible hardware configurations, nor do they always provide a way for us to measure\nenergy consumption from software. Regardless, we are working to expand the benchmark to support\nmore hardware configurations.\nF Broader Impacts\nBy allowing everyone to accurate measure, understand, and optimize the energy consumption of\ngenerative AI models, we believe the ML.ENERGY Benchmark can enhance the understanding of\nenergy consumption of generative AI in the research community and the industry, and ultimately fuel\nworks that optimize energy consumption. Furthermore, energy is essentially throughput per watt,\n6https://github.com/ml-energy/leaderboard/releases/tag/2023-07-06\n28\n\nwhich is one factor that determines the cost of running generative AI services at the infrastructure\nlevel. By optimizing energy consumption, we can reduce the cost of running generative AI services,\nwhich can help democratize access to generative AI.\n29", "metadata": {"url": "https://arxiv.org/pdf/2505.06371", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "The ML.ENERGY Benchmark: Toward Automated\nInference Energy Measurement and Optimization\nJae-Won Chung Jeff J. Ma Ruofan Wu Jiachen Liu\nOh Jun Kweon Yuxuan Xia Zhiyu Wu Mosharaf Chowdhury\nUniversity of Michigan\nThe ML.ENERGY Initiative\nAbstract\nAs the adoption of Generative AI in real-world services grow explosively,energy\nhas emerged as a critical bottleneck resource. However, energy remains a metric\nthat is often overlooked, under-explored, or poorly understood in the context of\nbuilding ML systems. We present the ML.ENERGY Benchmark, a benchmark\nsuite and tool for measuring inference energy consumption under realistic service\nenvironments, and the corresponding ML.ENERGY Leaderboard, which have\nserved as a valuable resource for those hoping to understand and optimize the\nenergy consumption of their generative AI services. In this paper, we explain four\nkey design principles for benchmarking ML energy we have acquired over time,\nand then describe how they are implemented in the ML.ENERGY Benchmark. We\nthen highlight results from the early 2025 iteration of the benchmark, including\nenergy measurements of 40 widely used model architectures across 6 different\ntasks, case studies of how ML design choices impact energy consumption, and\nhow automated optimization recommendations can lead to significant (sometimes\nmore than 40%) energy savings without changing what is being computed by the\nmodel. The ML.ENERGY Benchmark is open-source and can be easily extended\nto various customized models and application scenarios.\n1 Introduction\nGenerative AI models have rapidly transitioned from research prototypes to real-world services such\nas ChatGPT [56], Character AI [6], Sora [57], and Midjourney [50]. However, exponential growth\nrarely continues without facing scaling bottlenecks; currently for generative AI, one of the most\ncrucial bottlenecks is theenergy bottleneck[4, 15 –17, 38, 48, 49, 51]. That is, even with fleets of latest\nGPUs and exploding demand for ML compute, getting access to the energy necessary to power these\nsystems is becoming increasingly costly, slow, and sometimes impossible. This particularly impacts\nserving real-world services as ML inference reportedly accounts for 80–90% of the total compute\ndemand [12, 32, 58, 60]. Left unaddressed, the energy bottleneck will not only hinder AI research and\ndevelopment progress [31], but also lead to energy being squeezed out of existing electricity grids\nand impacting availability and price [4].\nHowever, despite its growing importance, energy remains a secondary consideration compared to\ntraditional optimization objectives like time and accuracy. How much energy does a model consume\nduring inference? What is the right way for energy measurement and accounting during execution,\nlet alone optimization? To bridge this gap, we launched the ML.ENERGY Leaderboard,1 the first\ninference energy leaderboard for modern generative AI models to the best of our knowledge. We have\nbeen gradually expanding the Leaderboard in multiple dimensions to now include (1) 40 different\n1https://ml.energy/leaderboard\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks.\narXiv:2505.06371v2  [cs.LG]  16 Oct 2025", "sentences": [{"text": "The ML.ENERGY Benchmark: Toward Automated\nInference Energy Measurement and Optimization\nJae-Won Chung Jeff J.", "metadata": {}}, {"text": "Ma Ruofan Wu Jiachen Liu\nOh Jun Kweon Yuxuan Xia Zhiyu Wu Mosharaf Chowdhury\nUniversity of Michigan\nThe ML.ENERGY Initiative\nAbstract\nAs the adoption of Generative AI in real-world services grow explosively,energy\nhas emerged as a critical bottleneck resource.", "metadata": {}}, {"text": "However, energy remains a metric\nthat is often overlooked, under-explored, or poorly understood in the context of\nbuilding ML systems.", "metadata": {}}, {"text": "We present the ML.ENERGY Benchmark, a benchmark\nsuite and tool for measuring inference energy consumption under realistic service\nenvironments, and the corresponding ML.ENERGY Leaderboard, which have\nserved as a valuable resource for those hoping to understand and optimize the\nenergy consumption of their generative AI services.", "metadata": {}}, {"text": "In this paper, we explain four\nkey design principles for benchmarking ML energy we have acquired over time,\nand then describe how they are implemented in the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "We\nthen highlight results from the early 2025 iteration of the benchmark, including\nenergy measurements of 40 widely used model architectures across 6 different\ntasks, case studies of how ML design choices impact energy consumption, and\nhow automated optimization recommendations can lead to significant (sometimes\nmore than 40%) energy savings without changing what is being computed by the\nmodel.", "metadata": {}}, {"text": "The ML.ENERGY Benchmark is open-source and can be easily extended\nto various customized models and application scenarios.", "metadata": {}}, {"text": "1 Introduction\nGenerative AI models have rapidly transitioned from research prototypes to real-world services such\nas ChatGPT [56], Character AI [6], Sora [57], and Midjourney [50].", "metadata": {}}, {"text": "However, exponential growth\nrarely continues without facing scaling bottlenecks;", "metadata": {}}, {"text": "currently for generative AI, one of the most\ncrucial bottlenecks is theenergy bottleneck[4, 15 –17, 38, 48, 49, 51].", "metadata": {}}, {"text": "That is, even with fleets of latest\nGPUs and exploding demand for ML compute, getting access to the energy necessary to power these\nsystems is becoming increasingly costly, slow, and sometimes impossible.", "metadata": {}}, {"text": "This particularly impacts\nserving real-world services as ML inference reportedly accounts for 80–90% of the total compute\ndemand [12, 32, 58, 60].", "metadata": {}}, {"text": "Left unaddressed, the energy bottleneck will not only hinder AI research and\ndevelopment progress [31], but also lead to energy being squeezed out of existing electricity grids\nand impacting availability and price [4].", "metadata": {}}, {"text": "However, despite its growing importance, energy remains a secondary consideration compared to\ntraditional optimization objectives like time and accuracy.", "metadata": {}}, {"text": "How much energy does a model consume\nduring inference?", "metadata": {}}, {"text": "What is the right way for energy measurement and accounting during execution,\nlet alone optimization?", "metadata": {}}, {"text": "To bridge this gap, we launched the ML.ENERGY Leaderboard,1 the first\ninference energy leaderboard for modern generative AI models to the best of our knowledge.", "metadata": {}}, {"text": "We have\nbeen gradually expanding the Leaderboard in multiple dimensions to now include (1) 40 different\n1https://ml.energy/leaderboard\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks.", "metadata": {}}, {"text": "arXiv:2505.06371v2  [cs.LG]  16 Oct 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Benchmark Optimization\nReusable Measurement Results Energy-Optimal Conﬁg \n……\nConﬁguration Space Latency TargetModel & Dataset\nConﬁgs\nA100 A100 A100 A100\nH100 H100\n……\nMax batch size = 512\nPipeline parallel = 1\nTensor parallel = 4\nGPU type = A100\n……\nMax batch size = 256\nPipeline parallel = 1\nTensor parallel = 2\nGPU type = H100\n1\n2\n3\nTime (s)\nEnergy (J)\nTarget\n4\nMeasured energy & latency\nFigure 1: Overview of the benchmarking and optimization flow of the ML.ENERGY Benchmark.\ngenerative AI model architectures across a wide range of tasks – including Large Language Model\n(LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video,\nand image-to-video generation using Diffusion models – and (2) more up-to-date hardware and\nsoftware stacks following rapid advancements in each area.\nIn this paper, we share the design principles we have established over time (Section 2) and present\nthe ML.ENERGY Benchmark that embodies them (Section 3). It provides two key functionalities:\n• Extensible benchmark: It provides an easily extensible benchmark suite and a comprehensive\nset of tools for measuring the inference energy consumption of generative AI models for various\ntasks underrealisticdeployment environments.\n• Automated optimization: Based on energy measurement results, it provides automated energy\noptimization recommendations for generative AI model deployment.\nFinally, we highlight notable results from the early 2025 iteration of the ML.ENERGY Leaderboard,\nshedding light on (1) how energy consumption varies across different generative AI models and tasks,\n(2) the complex trade-offs that involve energy, time, and model architecture design, and (3) the energy\nsavings opportunity unlocked by automated optimization (Section 4).\nThis paper describes the state of the ML.ENERGY Benchmark and Leaderboard as ofearly 2025.\nThe latest version of the ML.ENERGY Benchmark is open-source on GitHub,2 and the ML.ENERGY\nLeaderboard allows everyone to browse full results from the latest ML.ENERGY Benchmark.\n2 Design Principles\nThe design of the ML.ENERGY Benchmark is guided by four core principles. Our overarching goal\nis to create a benchmark that is representative of real-world generative AI service deployments, and\nto produce energy measurement results that are accurate, reusable, and ultimately actionable.\n2.1 Generalizability and Portability\nGoal.Every computer system is configured with different hardware and software components, and\nmeasurements from a particular system will never truly represent those from another system. For\ninstance, systems can be configured with different CPU and DRAM models, and running different\nLinux kernel versions with different daemons running in the background. Further, not all users have\n2https://github.com/ml-energy/benchmark\n2", "sentences": [{"text": "Benchmark Optimization\nReusable Measurement Results Energy-Optimal Conﬁg \n……\nConﬁguration Space Latency TargetModel & Dataset\nConﬁgs\nA100 A100 A100 A100\nH100 H100\n……\nMax batch size = 512\nPipeline parallel = 1\nTensor parallel = 4\nGPU type = A100\n……\nMax batch size = 256\nPipeline parallel = 1\nTensor parallel = 2\nGPU type = H100\n1\n2\n3\nTime (s)\nEnergy (J)\nTarget\n4\nMeasured energy & latency\nFigure 1: Overview of the benchmarking and optimization flow of the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "generative AI model architectures across a wide range of tasks – including Large Language Model\n(LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video,\nand image-to-video generation using Diffusion models – and (2) more up-to-date hardware and\nsoftware stacks following rapid advancements in each area.", "metadata": {}}, {"text": "In this paper, we share the design principles we have established over time (Section 2) and present\nthe ML.ENERGY Benchmark that embodies them (Section 3).", "metadata": {}}, {"text": "It provides two key functionalities:\n• Extensible benchmark: It provides an easily extensible benchmark suite and a comprehensive\nset of tools for measuring the inference energy consumption of generative AI models for various\ntasks underrealisticdeployment environments.", "metadata": {}}, {"text": "• Automated optimization: Based on energy measurement results, it provides automated energy\noptimization recommendations for generative AI model deployment.", "metadata": {}}, {"text": "Finally, we highlight notable results from the early 2025 iteration of the ML.ENERGY Leaderboard,\nshedding light on (1) how energy consumption varies across different generative AI models and tasks,\n(2) the complex trade-offs that involve energy, time, and model architecture design, and (3) the energy\nsavings opportunity unlocked by automated optimization (Section 4).", "metadata": {}}, {"text": "This paper describes the state of the ML.ENERGY Benchmark and Leaderboard as ofearly 2025.", "metadata": {}}, {"text": "The latest version of the ML.ENERGY Benchmark is open-source on GitHub,2 and the ML.ENERGY\nLeaderboard allows everyone to browse full results from the latest ML.ENERGY Benchmark.", "metadata": {}}, {"text": "2 Design Principles\nThe design of the ML.ENERGY Benchmark is guided by four core principles.", "metadata": {}}, {"text": "Our overarching goal\nis to create a benchmark that is representative of real-world generative AI service deployments, and\nto produce energy measurement results that are accurate, reusable, and ultimately actionable.", "metadata": {}}, {"text": "2.1 Generalizability and Portability\nGoal.Every computer system is configured with different hardware and software components, and\nmeasurements from a particular system will never truly represent those from another system.", "metadata": {}}, {"text": "For\ninstance, systems can be configured with different CPU and DRAM models, and running different\nLinux kernel versions with different daemons running in the background.", "metadata": {}}, {"text": "Further, not all users have\n2https://github.com/ml-energy/benchmark\n2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "physical access to the target system hardware, a common case for cloud-based environments. Still,\nwe wanted (1) the benchmark to run seamlessly on a wide variety of systems, and (2) measurement\nresults to provide generalizable insights and recommendations across a wide range of systems.\nOur approach.We focus on software-based GPU energy measurement for the following reasons:\n• GPUs are the dominant worker and energy consumer in a system running ML services, accounting\nfor 50–70% of the total provisioned power in the datacenter [52–54, 58].\n• Compared to other hardware components, GPU models are more standardized across different\nsystems [13], making measurements useful across systems that use the same GPU.\n• GPUs allow accurate software-based energy measurement [1, 2, 11, 81], allowing measurement\ntools to be portable across systems without requiring physical hardware access or modification.\n2.2 Representing Real-World Deployments\nGoal.Benchmarking results often inform real-world deployment optimizations, are used to plan\nfuture power capacity and energy usage, affect the design of new hardware and software systems, and\nserve as base numbers for long term projections that affect policymaking. Therefore, it is crucial that\nour measurements represent those from real-world deployments as closely as possible.\nOur approach.To obtain realistic measurements, we adhere to the following principles:\n• We adopt production-grade software and hardware (e.g., vLLM [39] on NVIDIA H100 GPUs)\nand run them with generation request workloads that are representative of real-world use cases.\n• During our measurement, we directly run or closely mimic the state of a serving system during\nlong term deployment. This allows us to capture thesteady stateenergy consumption of the\nservice while using a fixed-size benchmarking dataset.\n2.3 Energy Measurement at the Right Granularity\nGoal.Energy can be measured at different computation granularities. For instance, for LLM text\ngeneration, energy can be reported for the end-to-end benchmarking run, for each generated response,\nor for each token generated. Our goal is to measure and report energy consumption at a granularity\nthat is neither too coarse (as it only provides limited insight into the runtime behavior of the service)\nnor too fine (as it may miss important higher-level insights relevant to the service).\nOur approach.Also aligned with our goal of representing real-world deployments (Section 2.2),\nour approach is to mainly report energy consumption at the granularity of a single, whole generation\nresponse to a request (e.g., entire chat response, image, video). This is because any work less than\nthe full response (e.g., per token) is not considered a complete request, and may ignore model- and\ntask-specific characteristics. For instance, for LLM text generation, different models exhibit different\nverbosity(i.e., given the same prompt, different models respond with varying number of tokens), and\ndifferent tasks have vastly different output token length distributions (e.g., chat vs. code generation),\nall of which we want to capture in our measurements.\n2.4 Actionable Measurement Results\nGoal.While energy measurements are useful in themselves, they are even more useful when they\nlead to actionable insights and recommendations. For instance, how much is the potential energy\nsavings of your model without sacrificing accuracy or latency? If your service intends to guarantee a\nspecific generation latency deadline (e.g., 50 ms), what is the energy-optimal configuration, and how\nmuch is the potential energy savings?\nOur approach.The ML.ENERGY Benchmark allows users to provide computation latency con-\nstraints specific to their application scenario (e.g., LLM average Time Per Output Token), and will\nautomatically recommend (1) theenergy-optimalconfiguration that meets the latency constraints,\nand (2) the expected amount of energy savings. Due to the generalizability of our measurements\n(Section 2.1), these recommendations inform the optimization of a wide range of systems.\n3", "sentences": [{"text": "physical access to the target system hardware, a common case for cloud-based environments.", "metadata": {}}, {"text": "Still,\nwe wanted (1) the benchmark to run seamlessly on a wide variety of systems, and (2) measurement\nresults to provide generalizable insights and recommendations across a wide range of systems.", "metadata": {}}, {"text": "Our approach.We focus on software-based GPU energy measurement for the following reasons:\n• GPUs are the dominant worker and energy consumer in a system running ML services, accounting\nfor 50–70% of the total provisioned power in the datacenter [52–54, 58].", "metadata": {}}, {"text": "• Compared to other hardware components, GPU models are more standardized across different\nsystems [13], making measurements useful across systems that use the same GPU.", "metadata": {}}, {"text": "• GPUs allow accurate software-based energy measurement [1, 2, 11, 81], allowing measurement\ntools to be portable across systems without requiring physical hardware access or modification.", "metadata": {}}, {"text": "2.2 Representing Real-World Deployments\nGoal.Benchmarking results often inform real-world deployment optimizations, are used to plan\nfuture power capacity and energy usage, affect the design of new hardware and software systems, and\nserve as base numbers for long term projections that affect policymaking.", "metadata": {}}, {"text": "Therefore, it is crucial that\nour measurements represent those from real-world deployments as closely as possible.", "metadata": {}}, {"text": "Our approach.To obtain realistic measurements, we adhere to the following principles:\n• We adopt production-grade software and hardware (e.g., vLLM [39] on NVIDIA H100 GPUs)\nand run them with generation request workloads that are representative of real-world use cases.", "metadata": {}}, {"text": "• During our measurement, we directly run or closely mimic the state of a serving system during\nlong term deployment.", "metadata": {}}, {"text": "This allows us to capture thesteady stateenergy consumption of the\nservice while using a fixed-size benchmarking dataset.", "metadata": {}}, {"text": "2.3 Energy Measurement at the Right Granularity\nGoal.Energy can be measured at different computation granularities.", "metadata": {}}, {"text": "For instance, for LLM text\ngeneration, energy can be reported for the end-to-end benchmarking run, for each generated response,\nor for each token generated.", "metadata": {}}, {"text": "Our goal is to measure and report energy consumption at a granularity\nthat is neither too coarse (as it only provides limited insight into the runtime behavior of the service)\nnor too fine (as it may miss important higher-level insights relevant to the service).", "metadata": {}}, {"text": "Our approach.Also aligned with our goal of representing real-world deployments (Section 2.2),\nour approach is to mainly report energy consumption at the granularity of a single, whole generation\nresponse to a request (e.g., entire chat response, image, video).", "metadata": {}}, {"text": "This is because any work less than\nthe full response (e.g., per token) is not considered a complete request, and may ignore model- and\ntask-specific characteristics.", "metadata": {}}, {"text": "For instance, for LLM text generation, different models exhibit different\nverbosity(i.e., given the same prompt, different models respond with varying number of tokens), and\ndifferent tasks have vastly different output token length distributions (e.g., chat vs.", "metadata": {}}, {"text": "code generation),\nall of which we want to capture in our measurements.", "metadata": {}}, {"text": "2.4 Actionable Measurement Results\nGoal.While energy measurements are useful in themselves, they are even more useful when they\nlead to actionable insights and recommendations.", "metadata": {}}, {"text": "For instance, how much is the potential energy\nsavings of your model without sacrificing accuracy or latency?", "metadata": {}}, {"text": "If your service intends to guarantee a\nspecific generation latency deadline (e.g., 50 ms), what is the energy-optimal configuration, and how\nmuch is the potential energy savings?", "metadata": {}}, {"text": "Our approach.The ML.ENERGY Benchmark allows users to provide computation latency con-\nstraints specific to their application scenario (e.g., LLM average Time Per Output Token), and will\nautomatically recommend (1) theenergy-optimalconfiguration that meets the latency constraints,\nand (2) the expected amount of energy savings.", "metadata": {}}, {"text": "Due to the generalizability of our measurements\n(Section 2.1), these recommendations inform the optimization of a wide range of systems.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "4\n3000 requests \n……\n……\n……\n……\n3000\nWaiting\nrequests\nRunning \nrequests\nSteady state (stable server utilization) \nLLM Inference Server Request\nTime\nMax\nbatch size\nFigure 2: LLM inference server and per-request energy accounting. The steady state is defined as the\nperiod when batch size is saturated at the server’s maximum configured batch size, and measurements\nduring the steady state represent that of a serving system during long-term deployment.\n3 The ML.ENERGY Benchmark\nThe ML.ENERGY Benchmark is a comprehensive tool for measuring and optimizing the inference\nenergy consumption of generative AI models, built upon our core design principles (Section 2).\nHere, we describe the overall flow of the ML.ENERGY Benchmark (Section 3.1), which includes\nservice-aware energy measurement and accounting (Section 3.2) and automated optimization recom-\nmendations (Section 3.3). Finally, we describe extension points of the ML.ENERGY Benchmark that\nallows users to easily benchmark their customized application scenarios (Section 3.4).\n3.1 Benchmark Flow\nFigure 1 provides an overview of the usage flow of the ML.ENERGY Benchmark. 1 First, the\ngenerative model to benchmark and the request dataset (set of inputs) to use are selected, alongside\nwith the set of configurations to sweep (e.g., GPU model, parallelism configuration, maximum\nbatch size). 2 Then the ML.ENERGY Benchmark runs configurations independently on designated\nhardware, and measures the time and energy consumption of each configuration using Zeus [2],\na library that provides programmatic energy measurement (Section 3.2). 3 After benchmarking\nis complete, users can specify a latency target based on their application requirements. 4 Given\nthat, the ML.ENERGY Benchmark constructs the time–energy Pareto frontier, and recommends the\nenergy-optimal configuration while satisfying the latency target (Section 3.3).\n3.2 Energy Measurement and Service-Aware Energy Accounting\nOur goal is to provide per-request energy measurements (Section 2.3) that are representative of\nreal-world deployments (Section 2.2). However, a realistic serving system batches together the\ngeneration of multiple requests (e.g., iteration-level batching [82] for LLM text generation), making\nthe energy consumption of a single request dependent on all other requests being processed at the\nsame time. Therefore, we implement measurement and energy accounting methods that capture the\nbatching behavior of different types of models.\nDiffusion models.We begin with the relatively more straightforward case of diffusion models,\nwhich are used for text-to-image, text-to-video, and image-to-video generation. Diffusion models are\ntypically batched as a whole, meaning that the energy consumption of a single request is:\nEnergyrequest = Energybatch\nB (1)\nwhere the batch consists ofBimage or video generation requests.\nLLM text generation.Request-level energy accounting is less straightforward for LLM inference,\nbecause iteration-level batching [82] is an essential optimization in any realistic, production-grade\nLLM serving system [39]. Figure 2 shows how requests are served by a serving system implementing\niteration-level batching and how the ML.ENERGY Benchmark performs energy accounting. Because\n4", "sentences": [{"text": "4\n3000 requests \n……\n……\n……\n……\n3000\nWaiting\nrequests\nRunning \nrequests\nSteady state (stable server utilization) \nLLM Inference Server Request\nTime\nMax\nbatch size\nFigure 2: LLM inference server and per-request energy accounting.", "metadata": {}}, {"text": "The steady state is defined as the\nperiod when batch size is saturated at the server’s maximum configured batch size, and measurements\nduring the steady state represent that of a serving system during long-term deployment.", "metadata": {}}, {"text": "3 The ML.ENERGY Benchmark\nThe ML.ENERGY Benchmark is a comprehensive tool for measuring and optimizing the inference\nenergy consumption of generative AI models, built upon our core design principles (Section 2).", "metadata": {}}, {"text": "Here, we describe the overall flow of the ML.ENERGY Benchmark (Section 3.1), which includes\nservice-aware energy measurement and accounting (Section 3.2) and automated optimization recom-\nmendations (Section 3.3).", "metadata": {}}, {"text": "Finally, we describe extension points of the ML.ENERGY Benchmark that\nallows users to easily benchmark their customized application scenarios (Section 3.4).", "metadata": {}}, {"text": "3.1 Benchmark Flow\nFigure 1 provides an overview of the usage flow of the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "1 First, the\ngenerative model to benchmark and the request dataset (set of inputs) to use are selected, alongside\nwith the set of configurations to sweep (e.g., GPU model, parallelism configuration, maximum\nbatch size).", "metadata": {}}, {"text": "2 Then the ML.ENERGY Benchmark runs configurations independently on designated\nhardware, and measures the time and energy consumption of each configuration using Zeus [2],\na library that provides programmatic energy measurement (Section 3.2).", "metadata": {}}, {"text": "3 After benchmarking\nis complete, users can specify a latency target based on their application requirements.", "metadata": {}}, {"text": "4 Given\nthat, the ML.ENERGY Benchmark constructs the time–energy Pareto frontier, and recommends the\nenergy-optimal configuration while satisfying the latency target (Section 3.3).", "metadata": {}}, {"text": "3.2 Energy Measurement and Service-Aware Energy Accounting\nOur goal is to provide per-request energy measurements (Section 2.3) that are representative of\nreal-world deployments (Section 2.2).", "metadata": {}}, {"text": "However, a realistic serving system batches together the\ngeneration of multiple requests (e.g., iteration-level batching [82] for LLM text generation), making\nthe energy consumption of a single request dependent on all other requests being processed at the\nsame time.", "metadata": {}}, {"text": "Therefore, we implement measurement and energy accounting methods that capture the\nbatching behavior of different types of models.", "metadata": {}}, {"text": "Diffusion models.We begin with the relatively more straightforward case of diffusion models,\nwhich are used for text-to-image, text-to-video, and image-to-video generation.", "metadata": {}}, {"text": "Diffusion models are\ntypically batched as a whole, meaning that the energy consumption of a single request is:\nEnergyrequest = Energybatch\nB (1)\nwhere the batch consists ofBimage or video generation requests.", "metadata": {}}, {"text": "LLM text generation.Request-level energy accounting is less straightforward for LLM inference,\nbecause iteration-level batching [82] is an essential optimization in any realistic, production-grade\nLLM serving system [39].", "metadata": {}}, {"text": "Figure 2 shows how requests are served by a serving system implementing\niteration-level batching and how the ML.ENERGY Benchmark performs energy accounting.", "metadata": {}}, {"text": "Because\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "the beginning and end of each request are often not aligned with each other, finding each request’s\nindividual energy consumption is non-trivial. For this, we first submit all requests in the request\ndataset, and as the system runs, identify thesteady stateas the time period where the batch size is\nsaturated at the server’s maximum configured batch size. This steady state is designed to closely\napproximate the state of a serving system when it is well-utilized during long-term deployment.\nParticularly, when the system is ramping up initially with a full queue or ramping down at the end\nwith an empty queue, the server runs with a smaller batch size and does not exhibit the same energy\namortization benefits as the steady state. With this, we can derive the average per-request energy\nconsumption with:\nEnergyrequest =\nEnergysteady\nTokenssteady\n× 1\nN\nX\ni\nTokensrequest,i.(2)\nIn essence, we compute the average energy consumption per token during the steady state and multiply\nit by the average number of output tokens to derive the average per-request energy consumption.\nIndividual requests’ energy consumption can also be computed by multiplying the average energy per\ntoken during the steady state by the number of output tokens for each request.\nAs we will see in Section 4, batch size is a critical configuration that significantly affects both\ngeneration time and energy consumption. By sweeping the batch size configuration, the ML.ENERGY\nbenchmark can capture varying levels of system utilization and collect various operation points with\ndifferent time and energy consumption.\n3.3 Automated Optimization Recommendation\nOur goal is to provide actionable insights beyond just energy measurements (Section 2.4) by rec-\nommending energy-optimal configurations for a given model and task. Central to the optimization\nrecommendation is the construction of thePareto frontierof energy vs. time, which is a collection of\nconfigurations where there are no other configurations that lead to both lower energy and lower time.\nThen, the energy-optimal configuration is selected based on user-specified latency constraints.\nLatency constraints inherently depend on the user’s or application’s needs. For example, for image\ngeneration with Diffusion models, computation results are useful only when the full image is\ngenerated, so latency constraints would be specified in terms of the time to generate the whole image.\nOn the other hand, for LLM text generation for chat, output tokens arestreamedto users (either in\nwritten text or synthesized speech) as they are generated. As such, for user-facing conversational\nAI services, as long as the average time per output token is at least as fast as the users’ reading or\nlistening speed, user experience will not be affected [44]. However, for LLM text generation for\ncoding, where code is likely only useful when it is fully generated, latency constraints would be\nspecified in terms of the time to generate the whole snippet, similar to the case of image generation.\nGiven the latency constraints, the time–energy Pareto frontier is used to suggest the minimum-energy\nconfiguration that satisfies the latency constraint.\n3.4 Extending the Benchmark\nThe ML.ENERGY Benchmark is designed to be easily extensible, allowing users to benchmark their\nown models or customized application scenarios.\nModel.The ML.ENERGY Benchmark already supports various popular architectures like\nLlama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A\nfor a full list). Models that are fine-tuned based on already-supported models work as is. Models\nwith different architectures should also work as is as long as they are supported by the underlying\nruntime, like vLLM [39], which supports arbitrary LLMs provided by Hugging Face Transformers.\nRequest dataset.For each task (e.g., LLM text generation for chat), the ML.ENERGY Benchmark\nprovides a default request dataset that contains a set of inputs representative of real-world usage (See\nAppendix A for a full list). Users can also provide their own request dataset, which can be used to\ninvoke the runtime and measure energy consumption.\nConfiguration space.The ML.ENERGY Benchmark provides a default set of configurations\nspecific to tasks. For instance, for LLM text generation, it supports maximum batch sizes and\n5", "sentences": [{"text": "the beginning and end of each request are often not aligned with each other, finding each request’s\nindividual energy consumption is non-trivial.", "metadata": {}}, {"text": "For this, we first submit all requests in the request\ndataset, and as the system runs, identify thesteady stateas the time period where the batch size is\nsaturated at the server’s maximum configured batch size.", "metadata": {}}, {"text": "This steady state is designed to closely\napproximate the state of a serving system when it is well-utilized during long-term deployment.", "metadata": {}}, {"text": "Particularly, when the system is ramping up initially with a full queue or ramping down at the end\nwith an empty queue, the server runs with a smaller batch size and does not exhibit the same energy\namortization benefits as the steady state.", "metadata": {}}, {"text": "With this, we can derive the average per-request energy\nconsumption with:\nEnergyrequest =\nEnergysteady\nTokenssteady\n× 1\nN\nX\ni\nTokensrequest,i.(2)\nIn essence, we compute the average energy consumption per token during the steady state and multiply\nit by the average number of output tokens to derive the average per-request energy consumption.", "metadata": {}}, {"text": "Individual requests’ energy consumption can also be computed by multiplying the average energy per\ntoken during the steady state by the number of output tokens for each request.", "metadata": {}}, {"text": "As we will see in Section 4, batch size is a critical configuration that significantly affects both\ngeneration time and energy consumption.", "metadata": {}}, {"text": "By sweeping the batch size configuration, the ML.ENERGY\nbenchmark can capture varying levels of system utilization and collect various operation points with\ndifferent time and energy consumption.", "metadata": {}}, {"text": "3.3 Automated Optimization Recommendation\nOur goal is to provide actionable insights beyond just energy measurements (Section 2.4) by rec-\nommending energy-optimal configurations for a given model and task.", "metadata": {}}, {"text": "Central to the optimization\nrecommendation is the construction of thePareto frontierof energy vs.", "metadata": {}}, {"text": "time, which is a collection of\nconfigurations where there are no other configurations that lead to both lower energy and lower time.", "metadata": {}}, {"text": "Then, the energy-optimal configuration is selected based on user-specified latency constraints.", "metadata": {}}, {"text": "Latency constraints inherently depend on the user’s or application’s needs.", "metadata": {}}, {"text": "For example, for image\ngeneration with Diffusion models, computation results are useful only when the full image is\ngenerated, so latency constraints would be specified in terms of the time to generate the whole image.", "metadata": {}}, {"text": "On the other hand, for LLM text generation for chat, output tokens arestreamedto users (either in\nwritten text or synthesized speech) as they are generated.", "metadata": {}}, {"text": "As such, for user-facing conversational\nAI services, as long as the average time per output token is at least as fast as the users’ reading or\nlistening speed, user experience will not be affected [44].", "metadata": {}}, {"text": "However, for LLM text generation for\ncoding, where code is likely only useful when it is fully generated, latency constraints would be\nspecified in terms of the time to generate the whole snippet, similar to the case of image generation.", "metadata": {}}, {"text": "Given the latency constraints, the time–energy Pareto frontier is used to suggest the minimum-energy\nconfiguration that satisfies the latency constraint.", "metadata": {}}, {"text": "3.4 Extending the Benchmark\nThe ML.ENERGY Benchmark is designed to be easily extensible, allowing users to benchmark their\nown models or customized application scenarios.", "metadata": {}}, {"text": "Model.The ML.ENERGY Benchmark already supports various popular architectures like\nLlama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A\nfor a full list).", "metadata": {}}, {"text": "Models that are fine-tuned based on already-supported models work as is.", "metadata": {}}, {"text": "Models\nwith different architectures should also work as is as long as they are supported by the underlying\nruntime, like vLLM [39], which supports arbitrary LLMs provided by Hugging Face Transformers.", "metadata": {}}, {"text": "Request dataset.For each task (e.g., LLM text generation for chat), the ML.ENERGY Benchmark\nprovides a default request dataset that contains a set of inputs representative of real-world usage (See\nAppendix A for a full list).", "metadata": {}}, {"text": "Users can also provide their own request dataset, which can be used to\ninvoke the runtime and measure energy consumption.", "metadata": {}}, {"text": "Configuration space.The ML.ENERGY Benchmark provides a default set of configurations\nspecific to tasks.", "metadata": {}}, {"text": "For instance, for LLM text generation, it supports maximum batch sizes and\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "parallelism configuration (e.g., tensor and pipeline parallelism). For diffusion models, it supports not\nonly batch size, but also changing the number of denoising steps, as it has a non-trivial impact on time,\nenergy, and output quality. Users can customize the range of values swept for each configuration,\nand also provide new configurations (e.g., GPU power limit [1, 81]) as long as they implement\nthe corresponding configuration interface in the top-level routine. More configuration dimensions\nand finer grained sweeps will lead to longer benchmarking time, but will also push the Pareto\nfrontier towards the lower left corner of the time–energy space, leading to the discovery of more\nenergy-efficient configurations.\nHardware.As long as the runtime used by the ML.ENERGY Benchmark (e.g., vLLM) is capable\nof running on the target hardware and Zeus [2] can measure energy consumption on the target\nhardware (e.g., NVIDIA/AMD GPUs, Intel/AMD CPUs, Apple Silicon, NVIDIA Jetson platforms),\nthe ML.ENERGY Benchmark can run on the target hardware as is.\nMetrics.Energy is a fundamental physical quantity that can be used to derive other useful metrics,\nthough these derived metrics arenotautomatically computed by default as they require context-\nspecific information. Below, we describe how these metrics might be computed based on the\nbenchmark’s outputs.\n• Average power draw (Watts): Average power draw over the steady state can be calculated by\ndividing total energy consumption during the steady state by the duration of the steady state.\n• Throughput per Watt: Work throughput, e.g., request or token generation throughput, divided\nby average power draw can describe how muchservice capacitycan be extracted from the system\ngiven a power budget, which is a critical quantity for datacenter power planning [38].\n• Monetary cost ($): The electricity cost of compute can be calculated by integrating over time the\nmultiplication of energy consumption and the electricity price in the region and time instance. If\nthere is a specific region and time frame the service is expected to run, choosing that electricity\nprice can simulate the operational electricity cost of deployment. Electricity prices can be obtained\nfrom sources like OpenEI.3 Calculating the electricity cost from energy is supported by Zeus [2],\nthe measurement library of choice for the benchmark.\n• Operational carbon emissions (gCO2e): This quantityestimatesthe greenhouse gas emissions\nassociated with the electricity consumed. It can be calculated by multiplying energy consumption\nby the carbon intensity (gCO 2e/kWh) of the particular region and time frame in which the\nbenchmark was run. Carbon intensity data can be obtained from sources like ElectricityMaps.4\nThis is also supported by Zeus [2], the energy measurement library employed by the benchmark.\n4 Results Highlight\nIn this section, we highlight notable results from the ML.ENERGY Benchmark; the full set of\nresults is available on the ML.ENERGY Leaderboard.5 The early 2025 iteration of the benchmark\nand leaderboard presents energy measurements across 40 models and 6 tasks (See Appendix A\nfor a full list). We ran the benchmark on NVIDIA A100 (40 GB) and H100 (80 GB) GPUs,\neach using AWS p4d.24xlarge and p5.48xlarge instances, respectively, and used vLLM [39] and\nDiffusers [77] as the inference runtime. In the following, we first present energy measurement results\nand discuss implications (Section 4.1), and then provide deeper understanding by showing how model\narchitecture choices affect their energy consumption (Section 4.2). Then, we present the energy\nsavings opportunities from our automated optimization recommendations (Section 4.3).\n4.1 Energy Measurements\nSignificant variation in energy consumption.The solid bars in Figure 3 (A100 GPUs in Figure 3a\nand H100 in Figure 3b) show the per-request energy consumption of various generative AI models\nacross different tasks. First, energy consumption varies widely across models. In particular, Diffusion\nmodels generally consume energy that is on par with larger LLMs (e.g., Mistral Large (123B)). This\n3https://openei.org/wiki/Utility_Rate_Database\n4https://electricitymaps.com/\n5https://ml.energy/leaderboard\n6", "sentences": [{"text": "parallelism configuration (e.g., tensor and pipeline parallelism).", "metadata": {}}, {"text": "For diffusion models, it supports not\nonly batch size, but also changing the number of denoising steps, as it has a non-trivial impact on time,\nenergy, and output quality.", "metadata": {}}, {"text": "Users can customize the range of values swept for each configuration,\nand also provide new configurations (e.g., GPU power limit [1, 81]) as long as they implement\nthe corresponding configuration interface in the top-level routine.", "metadata": {}}, {"text": "More configuration dimensions\nand finer grained sweeps will lead to longer benchmarking time, but will also push the Pareto\nfrontier towards the lower left corner of the time–energy space, leading to the discovery of more\nenergy-efficient configurations.", "metadata": {}}, {"text": "Hardware.As long as the runtime used by the ML.ENERGY Benchmark (e.g., vLLM) is capable\nof running on the target hardware and Zeus [2] can measure energy consumption on the target\nhardware (e.g., NVIDIA/AMD GPUs, Intel/AMD CPUs, Apple Silicon, NVIDIA Jetson platforms),\nthe ML.ENERGY Benchmark can run on the target hardware as is.", "metadata": {}}, {"text": "Metrics.Energy is a fundamental physical quantity that can be used to derive other useful metrics,\nthough these derived metrics arenotautomatically computed by default as they require context-\nspecific information.", "metadata": {}}, {"text": "Below, we describe how these metrics might be computed based on the\nbenchmark’s outputs.", "metadata": {}}, {"text": "• Average power draw (Watts): Average power draw over the steady state can be calculated by\ndividing total energy consumption during the steady state by the duration of the steady state.", "metadata": {}}, {"text": "• Throughput per Watt: Work throughput, e.g., request or token generation throughput, divided\nby average power draw can describe how muchservice capacitycan be extracted from the system\ngiven a power budget, which is a critical quantity for datacenter power planning [38].", "metadata": {}}, {"text": "• Monetary cost ($): The electricity cost of compute can be calculated by integrating over time the\nmultiplication of energy consumption and the electricity price in the region and time instance.", "metadata": {}}, {"text": "If\nthere is a specific region and time frame the service is expected to run, choosing that electricity\nprice can simulate the operational electricity cost of deployment.", "metadata": {}}, {"text": "Electricity prices can be obtained\nfrom sources like OpenEI.3 Calculating the electricity cost from energy is supported by Zeus [2],\nthe measurement library of choice for the benchmark.", "metadata": {}}, {"text": "• Operational carbon emissions (gCO2e): This quantityestimatesthe greenhouse gas emissions\nassociated with the electricity consumed.", "metadata": {}}, {"text": "It can be calculated by multiplying energy consumption\nby the carbon intensity (gCO 2e/kWh) of the particular region and time frame in which the\nbenchmark was run.", "metadata": {}}, {"text": "Carbon intensity data can be obtained from sources like ElectricityMaps.4\nThis is also supported by Zeus [2], the energy measurement library employed by the benchmark.", "metadata": {}}, {"text": "4 Results Highlight\nIn this section, we highlight notable results from the ML.ENERGY Benchmark;", "metadata": {}}, {"text": "the full set of\nresults is available on the ML.ENERGY Leaderboard.5 The early 2025 iteration of the benchmark\nand leaderboard presents energy measurements across 40 models and 6 tasks (See Appendix A\nfor a full list).", "metadata": {}}, {"text": "We ran the benchmark on NVIDIA A100 (40 GB) and H100 (80 GB) GPUs,\neach using AWS p4d.24xlarge and p5.48xlarge instances, respectively, and used vLLM [39] and\nDiffusers [77] as the inference runtime.", "metadata": {}}, {"text": "In the following, we first present energy measurement results\nand discuss implications (Section 4.1), and then provide deeper understanding by showing how model\narchitecture choices affect their energy consumption (Section 4.2).", "metadata": {}}, {"text": "Then, we present the energy\nsavings opportunities from our automated optimization recommendations (Section 4.3).", "metadata": {}}, {"text": "4.1 Energy Measurements\nSignificant variation in energy consumption.The solid bars in Figure 3 (A100 GPUs in Figure 3a\nand H100 in Figure 3b) show the per-request energy consumption of various generative AI models\nacross different tasks.", "metadata": {}}, {"text": "First, energy consumption varies widely across models.", "metadata": {}}, {"text": "In particular, Diffusion\nmodels generally consume energy that is on par with larger LLMs (e.g., Mistral Large (123B)).", "metadata": {}}, {"text": "This\n3https://openei.org/wiki/Utility_Rate_Database\n4https://electricitymaps.com/\n5https://ml.energy/leaderboard\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Gemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n2.5x 1.8x\n1.7x\n2.1x\n3.3x\n2.5x\n2.2x 2.1x 1.5x 1.9x 1.8x\n2.6x\n2.7x\n3.2x\n1.6x 1.4x\n2.1x\n2.4x\n2.7x 1.9x 1.4x 3.4x 1.6x\n1.6x\n1.3x 1.5x 1.5x 1.5x\n1.4x 1.1x\n1.0x\n1.4x\n1.0x\n1.1x\n1.2x\n1.1x\n1.0x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n3.2x 2.5x\n2.4x\n2.6x\n2.5x\n2.5x\n2.7x 2.8x 2.1x 2.9x 2.3x\n3.2x\n3.3x\n3.3x\n2.3x 1.8x 2.0x\n2.3x\n3.4x 2.7x 2.1x 4.1x 2.4x\n2.0x 1.8x 1.9x 2.1x 2.1x\n1.9x 1.2x\n1.2x\n2.0x\n1.0x 1.1x\n1.5x\n1.2x\n1.1x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (H100)\n(b) NVIDIA H100\nFigure 3: Per-request energy consumption across various generative AI models. Black and orange\nrepresents text and vision modalities, respectively. Solid bars are energy measurements, whereas\ndimmed bars behind each solid bar are estimations based on the GPU’s TDP, with numbers showing\nthe ratio of overestimation. Note the log scale Y-axis.\nModel TP Max batch size\n4 8 16 32 64\nDeepSeek distilled Qwen 3 8B [23, 80] 1 9713.7 6010.1 4314.9 3340.8 2770.8\nPhi 4 reasoning plus 15B [3] 1 19974.4 12389.6 9347.3 7634.9 7595.4\nQwen 3 32B [80] 2 26419.7 15168.3 9140.5 6165.5 4520.6\nQwen 3 235B-A22B thinking [80] 8 122523.1 86491.5 56720.4 40275.5 33096.4\nTable 1: Energy per generation of reasoning models on GPQA [64] and NVIDIA H100 GPUs. TP is\nthe tensor parallelism degree, which is also equal to the number of GPUs used.\nis mainly because Diffusion models (1) draw higher power in general (more in Section 4.2) and (2)\ncannot perform as many concurrent generations compared to LLMs due to their long latency in real\nservices, preventing them from amortizing energy consumption across many generations.\nImportance of measuring.The dimmed bars behind each solid bar in Figure 3 show the estimated\nenergy consumption based on the GPU’s Thermal Design Power (TDP) instead of measuring the\nreal GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74]. Estimations using\nTDP are nearly always an overestimation since it is rare for a GPU – or any computing device –\nto draw its maximum power at every moment in time. In fact, such an estimation can lead to a\nworst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).\nInaccuracies may be overlooked when they influence downstream decisions and projections, leading\nto misleading conclusions. Accurate measurements that reflect production environments are crucial.\n4.2 Energy Implications of ML Design Decisions\nML decisions reflected in model architectures and trained models impact energy consumption. For\nthe interest of space, we defer systems implications on energy consumption to Appendix B.\nLLM response verbosity and energy.In Figure 3, we can see that energy consumption varies even\namong LLMs of similar sizes. This is because different LLMs generate responses of differentlength\neven when given the same prompt. Such differences inverbositycan be non-trivial; for instance,\nMistral Large’s responses were on average 36% longer than that of Mixtral 8×7B. As the number of\n7", "sentences": [{"text": "Gemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n2.5x 1.8x\n1.7x\n2.1x\n3.3x\n2.5x\n2.2x 2.1x 1.5x 1.9x 1.8x\n2.6x\n2.7x\n3.2x\n1.6x 1.4x\n2.1x\n2.4x\n2.7x 1.9x 1.4x 3.4x 1.6x\n1.6x\n1.3x 1.5x 1.5x 1.5x\n1.4x 1.1x\n1.0x\n1.4x\n1.0x\n1.1x\n1.2x\n1.1x\n1.0x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n100\n101\n102\n103\n104\nEnergy consumption (J)\n3.2x 2.5x\n2.4x\n2.6x\n2.5x\n2.5x\n2.7x 2.8x 2.1x 2.9x 2.3x\n3.2x\n3.3x\n3.3x\n2.3x 1.8x 2.0x\n2.3x\n3.4x 2.7x 2.1x 4.1x 2.4x\n2.0x 1.8x 1.9x 2.1x 2.1x\n1.9x 1.2x\n1.2x\n2.0x\n1.0x 1.1x\n1.5x\n1.2x\n1.1x\n1.1x\n1.1x 1.1x\nEnergy Consumption Per Generation (H100)\n(b) NVIDIA H100\nFigure 3: Per-request energy consumption across various generative AI models.", "metadata": {}}, {"text": "Black and orange\nrepresents text and vision modalities, respectively.", "metadata": {}}, {"text": "Solid bars are energy measurements, whereas\ndimmed bars behind each solid bar are estimations based on the GPU’s TDP, with numbers showing\nthe ratio of overestimation.", "metadata": {}}, {"text": "Note the log scale Y-axis.", "metadata": {}}, {"text": "Model TP Max batch size\n4 8 16 32 64\nDeepSeek distilled Qwen 3 8B [23, 80] 1 9713.7 6010.1 4314.9 3340.8 2770.8\nPhi 4 reasoning plus 15B [3] 1 19974.4 12389.6 9347.3 7634.9 7595.4\nQwen 3 32B [80] 2 26419.7 15168.3 9140.5 6165.5 4520.6\nQwen 3 235B-A22B thinking [80] 8 122523.1 86491.5 56720.4 40275.5 33096.4\nTable 1: Energy per generation of reasoning models on GPQA [64] and NVIDIA H100 GPUs.", "metadata": {}}, {"text": "TP is\nthe tensor parallelism degree, which is also equal to the number of GPUs used.", "metadata": {}}, {"text": "is mainly because Diffusion models (1) draw higher power in general (more in Section 4.2) and (2)\ncannot perform as many concurrent generations compared to LLMs due to their long latency in real\nservices, preventing them from amortizing energy consumption across many generations.", "metadata": {}}, {"text": "Importance of measuring.The dimmed bars behind each solid bar in Figure 3 show the estimated\nenergy consumption based on the GPU’s Thermal Design Power (TDP) instead of measuring the\nreal GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74].", "metadata": {}}, {"text": "Estimations using\nTDP are nearly always an overestimation since it is rare for a GPU – or any computing device –\nto draw its maximum power at every moment in time.", "metadata": {}}, {"text": "In fact, such an estimation can lead to a\nworst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).", "metadata": {}}, {"text": "Inaccuracies may be overlooked when they influence downstream decisions and projections, leading\nto misleading conclusions.", "metadata": {}}, {"text": "Accurate measurements that reflect production environments are crucial.", "metadata": {}}, {"text": "4.2 Energy Implications of ML Design Decisions\nML decisions reflected in model architectures and trained models impact energy consumption.", "metadata": {}}, {"text": "For\nthe interest of space, we defer systems implications on energy consumption to Appendix B.", "metadata": {}}, {"text": "LLM response verbosity and energy.In Figure 3, we can see that energy consumption varies even\namong LLMs of similar sizes.", "metadata": {}}, {"text": "This is because different LLMs generate responses of differentlength\neven when given the same prompt.", "metadata": {}}, {"text": "Such differences inverbositycan be non-trivial;", "metadata": {}}, {"text": "for instance,\nMistral Large’s responses were on average 36% longer than that of Mixtral 8×7B.", "metadata": {}}, {"text": "As the number of\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "0 50 100 150 200 250 300\nBatch size\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPhi 3 mini (3.8B)\nPhi 3 small (7B)\n(a) Energy vs. Batch Size\n0 200 400 600 800\nMaximum batch size configuration\n0\n100\n200\n300\n400Batch size\nPhi 3 mini (3.8B)\nPhi 3 small (7B) (b) Batch Size vs. Max Batch Size config\nFigure 4: Phi-3 Mini and Small [26] benchmarked with the chat task on one NVIDIA A100 GPU.\n0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100\n(a) Llama 3.1 70B [73]\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (b) Stable Diffusion 3 Medium [25]\nFigure 5: Power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models.\noutput tokens equals the number of forward passes through the model, longer responses leads to a\nproportional increase in energy consumption. As humans are known to prefer longer responses [85],\nthis potentially introduces a trade-off between energy consumption and user satisfaction.\nThis is even more pronounced for reasoning models, which produce significantly more output tokens.\nTable 1 shows energy measurements for reasoning models on the GPQA dataset. Reasoning models\nproduce one to two orders of magnitude more output tokens per request compared to standard chat\nmodels, significantly increasing energy consumption per generation. Additionally, due to their long\noutput lengths, servers cannot run as large a batch size, preventing them from amortizing energy across\nmore requests. This leads to higher energy per token as well, further increasing energy consumption.\nAs long horizon reasoning and task decomposition become more common in real-world LLM-based\napplications, we expect this trend to continue.\nMemory consumption of operations and energy amortization.Generally, models with more\nparameters consume more energy, but this is not always the case. Figure 4 highlights the case of\nPhi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the\nleft plot shows that the larger Small model can consume less energy than Mini as batch size grows.\nThis happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped\nQuery Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3 × more memory than Small,\nwhich prevents it from scaling to larger batch sizes and amortizing energy consumption across more\ngenerations.\nCompute-intensity of operations and power draw.Figure 5 shows the power consumption of\nLlama 3.1 70B [73] and Stable Diffusion 3 Medium [25] on A100 and H100 GPUs. It can be\nseen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum,\nwhereas the Diffusion model’s power consumption is close to the maximum. This is because LLM\ndecoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations\n(e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the\nGPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s\ncomputation units being underutilized, leading to low power draw. Appendix C dives deeper into\npower consumption with measurements for all models and GPU power breakdowns over time.\n8", "sentences": [{"text": "0 50 100 150 200 250 300\nBatch size\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPhi 3 mini (3.8B)\nPhi 3 small (7B)\n(a) Energy vs.", "metadata": {}}, {"text": "Batch Size\n0 200 400 600 800\nMaximum batch size configuration\n0\n100\n200\n300\n400Batch size\nPhi 3 mini (3.8B)\nPhi 3 small (7B) (b) Batch Size vs.", "metadata": {}}, {"text": "Max Batch Size config\nFigure 4: Phi-3 Mini and Small [26] benchmarked with the chat task on one NVIDIA A100 GPU.", "metadata": {}}, {"text": "0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100\n(a) Llama 3.1 70B [73]\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (b) Stable Diffusion 3 Medium [25]\nFigure 5: Power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models.", "metadata": {}}, {"text": "output tokens equals the number of forward passes through the model, longer responses leads to a\nproportional increase in energy consumption.", "metadata": {}}, {"text": "As humans are known to prefer longer responses [85],\nthis potentially introduces a trade-off between energy consumption and user satisfaction.", "metadata": {}}, {"text": "This is even more pronounced for reasoning models, which produce significantly more output tokens.", "metadata": {}}, {"text": "Table 1 shows energy measurements for reasoning models on the GPQA dataset.", "metadata": {}}, {"text": "Reasoning models\nproduce one to two orders of magnitude more output tokens per request compared to standard chat\nmodels, significantly increasing energy consumption per generation.", "metadata": {}}, {"text": "Additionally, due to their long\noutput lengths, servers cannot run as large a batch size, preventing them from amortizing energy across\nmore requests.", "metadata": {}}, {"text": "This leads to higher energy per token as well, further increasing energy consumption.", "metadata": {}}, {"text": "As long horizon reasoning and task decomposition become more common in real-world LLM-based\napplications, we expect this trend to continue.", "metadata": {}}, {"text": "Memory consumption of operations and energy amortization.Generally, models with more\nparameters consume more energy, but this is not always the case.", "metadata": {}}, {"text": "Figure 4 highlights the case of\nPhi-3 Mini (3.8B) and Small (7B) [26].", "metadata": {}}, {"text": "Even though Small has nearly twice the parameters, the\nleft plot shows that the larger Small model can consume less energy than Mini as batch size grows.", "metadata": {}}, {"text": "This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped\nQuery Attention (GQA) [10].", "metadata": {}}, {"text": "Due to this, Mini’s KV cache uses 3 × more memory than Small,\nwhich prevents it from scaling to larger batch sizes and amortizing energy consumption across more\ngenerations.", "metadata": {}}, {"text": "Compute-intensity of operations and power draw.Figure 5 shows the power consumption of\nLlama 3.1 70B [73] and Stable Diffusion 3 Medium [25] on A100 and H100 GPUs.", "metadata": {}}, {"text": "It can be\nseen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum,\nwhereas the Diffusion model’s power consumption is close to the maximum.", "metadata": {}}, {"text": "This is because LLM\ndecoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations\n(e.g., multiplication and addition) per byte of memory loaded is low [37, 58].", "metadata": {}}, {"text": "This leads to the\nGPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s\ncomputation units being underutilized, leading to low power draw.", "metadata": {}}, {"text": "Appendix C dives deeper into\npower consumption with measurements for all models and GPU power breakdowns over time.", "metadata": {}}, {"text": "8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "0.0 2.5 5.0 7.5 10.0 12.5 15.0\nBatch size\n0\n1000\n2000\n3000Energy consumption (J)\nSDXL (1024x1024)\nSDXL Turbo (512x512)\n(a) Different Resolutions\n0 10 20 30 40 50\nNumber of denoising steps\n0\n1000\n2000\n3000Energy consumption (J)\nSXDL (1024x1024)\nSDXL Turbo (512x512) (b) Varying Denoising Steps\nFigure 6: Energy consumption of SDXL [61] and SDXL Turbo [7] on one NVIDIA A100 GPU.\n0.0 0.1 0.2 0.3 0.4\nAverage Time Per Output Token (s)\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPareto frontier\nA100 H100\n(a) Llama 3.1 8B [73]\n0 5 10 15 20 25\nGeneration latency (s)\n0\n100\n200\n300\n400\n500Energy consumption (J)\nPareto frontier\nA100 H100 (b) Stable Diffusion 2.1 [65]\nFigure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark.\nInference-time parameters and energy.Figure 6 shows the energy consumption of Stable Diffu-\nsion XL (SDXL) [61] and SDXL Turbo [7]. On the left, while SDXL and SDXL Turbo have identical\nmodel sizes and architectures, their energy consumption is significantly different. This is because\nSDXL Turbo is tuned to generate smaller resolution images (512×512) than SDXL (1024×1024),\nwhich leads to different latent sizes and amounts of computation. On the right, it can be seen that the\nnumber of denoising steps linearly increases energy consumption, as one denoising step requires one\nforward pass through the model. While simple in isolation, these inference-time parameters lead to\nnon-trivial design tradeoffs at the application-level. For instance, increasing the number of denoising\nsteps may improve final image quality, but beyond some point, it may be virtually indistinguishable\nto human users. Also, generating images in lower resolution and then upscaling them with a separate\nsuper-resolution model (e.g., DAT [19]) may consume less energy end-to-end.\n4.3 Automated Energy Optimization Recommendation\nFigure 7 shows the time–energy Pareto frontier constructed by the ML.ENERGY Benchmark mea-\nsurement results for Llama 3.1 8B and Stable Diffusion 2.1. In general, the Pareto frontier is convex,\nmeaning that by sacrificing some latency, one can achieve significant energy savings.\nConversational AI services like LLM-based chatbots achieve interactivity by streaming tokens to users\neither in written text or synthesized speech, making Time Per Output Token (TPOT) an important\nperformance metric that impacts user experience [44]. In this context, a chatbot provider can target\nan average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]),\nwhich is sufficient for most reading or listening speeds. This will land on the Pareto frontier at the\npoint where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared\nto the configuration that simply minimizes latency.\nHere, we note that for Llama 3.1 8B [73], the Pareto frontier is a mixture of configurations from both\nA100 and H100 GPUs. This is because LLM decoding does not fully exert the GPU’s compute units\nand are rather bound by memory, so going from A100 to H100 GPUs neither provides significantly\nhigher performance nor significantly increases power draw (See Appendix C for details). These two –\npower and time – multiplied, energy consumption is comparable across the two GPUs.\n9", "sentences": [{"text": "0.0 2.5 5.0 7.5 10.0 12.5 15.0\nBatch size\n0\n1000\n2000\n3000Energy consumption (J)\nSDXL (1024x1024)\nSDXL Turbo (512x512)\n(a) Different Resolutions\n0 10 20 30 40 50\nNumber of denoising steps\n0\n1000\n2000\n3000Energy consumption (J)\nSXDL (1024x1024)\nSDXL Turbo (512x512) (b) Varying Denoising Steps\nFigure 6: Energy consumption of SDXL [61] and SDXL Turbo [7] on one NVIDIA A100 GPU.", "metadata": {}}, {"text": "0.0 0.1 0.2 0.3 0.4\nAverage Time Per Output Token (s)\n0\n20\n40\n60\n80\n100Energy consumption (J)\nPareto frontier\nA100 H100\n(a) Llama 3.1 8B [73]\n0 5 10 15 20 25\nGeneration latency (s)\n0\n100\n200\n300\n400\n500Energy consumption (J)\nPareto frontier\nA100 H100 (b) Stable Diffusion 2.1 [65]\nFigure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "Inference-time parameters and energy.Figure 6 shows the energy consumption of Stable Diffu-\nsion XL (SDXL) [61] and SDXL Turbo [7].", "metadata": {}}, {"text": "On the left, while SDXL and SDXL Turbo have identical\nmodel sizes and architectures, their energy consumption is significantly different.", "metadata": {}}, {"text": "This is because\nSDXL Turbo is tuned to generate smaller resolution images (512×512) than SDXL (1024×1024),\nwhich leads to different latent sizes and amounts of computation.", "metadata": {}}, {"text": "On the right, it can be seen that the\nnumber of denoising steps linearly increases energy consumption, as one denoising step requires one\nforward pass through the model.", "metadata": {}}, {"text": "While simple in isolation, these inference-time parameters lead to\nnon-trivial design tradeoffs at the application-level.", "metadata": {}}, {"text": "For instance, increasing the number of denoising\nsteps may improve final image quality, but beyond some point, it may be virtually indistinguishable\nto human users.", "metadata": {}}, {"text": "Also, generating images in lower resolution and then upscaling them with a separate\nsuper-resolution model (e.g., DAT [19]) may consume less energy end-to-end.", "metadata": {}}, {"text": "4.3 Automated Energy Optimization Recommendation\nFigure 7 shows the time–energy Pareto frontier constructed by the ML.ENERGY Benchmark mea-\nsurement results for Llama 3.1 8B and Stable Diffusion 2.1.", "metadata": {}}, {"text": "In general, the Pareto frontier is convex,\nmeaning that by sacrificing some latency, one can achieve significant energy savings.", "metadata": {}}, {"text": "Conversational AI services like LLM-based chatbots achieve interactivity by streaming tokens to users\neither in written text or synthesized speech, making Time Per Output Token (TPOT) an important\nperformance metric that impacts user experience [44].", "metadata": {}}, {"text": "In this context, a chatbot provider can target\nan average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]),\nwhich is sufficient for most reading or listening speeds.", "metadata": {}}, {"text": "This will land on the Pareto frontier at the\npoint where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared\nto the configuration that simply minimizes latency.", "metadata": {}}, {"text": "Here, we note that for Llama 3.1 8B [73], the Pareto frontier is a mixture of configurations from both\nA100 and H100 GPUs.", "metadata": {}}, {"text": "This is because LLM decoding does not fully exert the GPU’s compute units\nand are rather bound by memory, so going from A100 to H100 GPUs neither provides significantly\nhigher performance nor significantly increases power draw (See Appendix C for details).", "metadata": {}}, {"text": "These two –\npower and time – multiplied, energy consumption is comparable across the two GPUs.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "On the other hand, for Stable Diffusion 2.1 [65], the Pareto frontier is dominated by configurations\non the H100 GPU. Diffusion models consume power close to the GPU’s TDP (See Appendix C\nfor details), which increases power draw significantly when going from A100 to H100. However,\nsince computation latency was reduced even more, configurations on H100 Pareto-dominate those on\nA100. If an application has a generation latency target of, for instance, 5 seconds, the energy-optimal\nconfiguration will lie on the Pareto frontier where latency is 3.63 seconds, which is 21% less energy\nthan the configuration that minimizes latency.\n5 Related Work\nML energy measurement.The Hugging Face LLM-Perf leaderboard [33] is specific to LLMs\nand reports theper-tokenenergy consumption of LLM text generation, which fails to capture the\nverbosity and task-specific output token length distribution difference of LLMs (Section 2.3). MLPerf\nPower [75] provides measurements for ML training and inference, but crucially, requires direct access\nto the system under test to physically install the power analyzer, which significantly limits who can\nrun the benchmarks (Section 2.1). Furthermore, it benchmarks at most a few model architectures for\neach task (sometimes only one), failing to provide insights on how ML design choices impact energy\nconsumption. The Hugging Face AI Energy Score leaderboard [27] provides measurement data for\nbroader AI tasks. However, it fixes the inference batch size to 1 for all models, failing to reflect how\nservices are deployed in the real world and thus their energy consumption (Section 2.2). Google\ndisclosed the median energy consumption of their AI service [24]. It provides a comprehensive\nscope of measurement, even including the energy consumption of idle machines provisioned for\nstable service operation. However, measurements and reports are based on internal Google systems,\nworkloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the\ngeneralizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the\nfirst inference energy benchmark for modern generative AI models, and empowers users to not only\nmeasure but also optimize the energy consumption of their models. See Appendix D for more details.\nML energy optimization.The ML.ENERGY Benchmark provides automated energy optimization\nrecommendations based on energy measurements (Section 3.3). There are several other efforts that\nalso provided automated energy optimizations – while preserving mathematical equivalence and/or\nmodel quality – for ML training and inference. Zeus [81], EnvPipe [20], and Perseus [21] optimizes\nthe energy consumption of ML training by adjusting GPU-level and training job-level configurations,\neither statically after profiling or dynamically during training. µ-Serve [63] and DynamoLLM [68]\nare also similar, but optimize energy consumption for ML inference clusters. Optimization recom-\nmendations by the ML.ENERGY Benchmark are complementary to the techniques proposed by these\nworks. Further, our results support the need for automatedcross-layerenergy optimizations that span\nall model, software, and hardware layers [22], as opposed to efforts siloed within a single layer.\n6 Conclusion\nIn this work, we described the ML.ENERGY Benchmark, a comprehensive energy benchmark for\ngenerative AI models that not only provides realistic energy measurements, but also automatically\nsuggests energy-optimal configurations based on user- and app-specific performance constraints.\nMeasurement results show that energy consumption is a metric that is impacted by design choices\nacross the whole AI stack, including application, model, software, and hardware, demonstrating the\nimportance of automatedcross-layerenergy optimizations instead of siloed optimizations within\na single layer. We are confident that the ML.ENERGY Benchmark will democratize the art of\nmeasuring, understanding, and optimizing ML energy consumption for the community.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Yunseok Jang and SymbioticLab members for helpful comments and sugges-\ntions on the paper. This work and its authors were in part supported by NSF grants CNS-2104243,\nCNS-2106184, and CNS-2450085, grants from VMware, the Mozilla Foundation, Cisco, Ford, and\nGitHub, and gifts from Salesforce and Google. Jae-Won Chung is additionally supported by the\nKwanjeong Educational Foundation.\n10", "sentences": [{"text": "On the other hand, for Stable Diffusion 2.1 [65], the Pareto frontier is dominated by configurations\non the H100 GPU.", "metadata": {}}, {"text": "Diffusion models consume power close to the GPU’s TDP (See Appendix C\nfor details), which increases power draw significantly when going from A100 to H100.", "metadata": {}}, {"text": "However,\nsince computation latency was reduced even more, configurations on H100 Pareto-dominate those on\nA100.", "metadata": {}}, {"text": "If an application has a generation latency target of, for instance, 5 seconds, the energy-optimal\nconfiguration will lie on the Pareto frontier where latency is 3.63 seconds, which is 21% less energy\nthan the configuration that minimizes latency.", "metadata": {}}, {"text": "5 Related Work\nML energy measurement.The Hugging Face LLM-Perf leaderboard [33] is specific to LLMs\nand reports theper-tokenenergy consumption of LLM text generation, which fails to capture the\nverbosity and task-specific output token length distribution difference of LLMs (Section 2.3).", "metadata": {}}, {"text": "MLPerf\nPower [75] provides measurements for ML training and inference, but crucially, requires direct access\nto the system under test to physically install the power analyzer, which significantly limits who can\nrun the benchmarks (Section 2.1).", "metadata": {}}, {"text": "Furthermore, it benchmarks at most a few model architectures for\neach task (sometimes only one), failing to provide insights on how ML design choices impact energy\nconsumption.", "metadata": {}}, {"text": "The Hugging Face AI Energy Score leaderboard [27] provides measurement data for\nbroader AI tasks.", "metadata": {}}, {"text": "However, it fixes the inference batch size to 1 for all models, failing to reflect how\nservices are deployed in the real world and thus their energy consumption (Section 2.2).", "metadata": {}}, {"text": "Google\ndisclosed the median energy consumption of their AI service [24].", "metadata": {}}, {"text": "It provides a comprehensive\nscope of measurement, even including the energy consumption of idle machines provisioned for\nstable service operation.", "metadata": {}}, {"text": "However, measurements and reports are based on internal Google systems,\nworkloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the\ngeneralizability and reproducibility of the results (Section 2.1).", "metadata": {}}, {"text": "The ML.ENERGY Benchmark is the\nfirst inference energy benchmark for modern generative AI models, and empowers users to not only\nmeasure but also optimize the energy consumption of their models.", "metadata": {}}, {"text": "See Appendix D for more details.", "metadata": {}}, {"text": "ML energy optimization.The ML.ENERGY Benchmark provides automated energy optimization\nrecommendations based on energy measurements (Section 3.3).", "metadata": {}}, {"text": "There are several other efforts that\nalso provided automated energy optimizations – while preserving mathematical equivalence and/or\nmodel quality – for ML training and inference.", "metadata": {}}, {"text": "Zeus [81], EnvPipe [20], and Perseus [21] optimizes\nthe energy consumption of ML training by adjusting GPU-level and training job-level configurations,\neither statically after profiling or dynamically during training.", "metadata": {}}, {"text": "µ-Serve [63] and DynamoLLM [68]\nare also similar, but optimize energy consumption for ML inference clusters.", "metadata": {}}, {"text": "Optimization recom-\nmendations by the ML.ENERGY Benchmark are complementary to the techniques proposed by these\nworks.", "metadata": {}}, {"text": "Further, our results support the need for automatedcross-layerenergy optimizations that span\nall model, software, and hardware layers [22], as opposed to efforts siloed within a single layer.", "metadata": {}}, {"text": "6 Conclusion\nIn this work, we described the ML.ENERGY Benchmark, a comprehensive energy benchmark for\ngenerative AI models that not only provides realistic energy measurements, but also automatically\nsuggests energy-optimal configurations based on user- and app-specific performance constraints.", "metadata": {}}, {"text": "Measurement results show that energy consumption is a metric that is impacted by design choices\nacross the whole AI stack, including application, model, software, and hardware, demonstrating the\nimportance of automatedcross-layerenergy optimizations instead of siloed optimizations within\na single layer.", "metadata": {}}, {"text": "We are confident that the ML.ENERGY Benchmark will democratize the art of\nmeasuring, understanding, and optimizing ML energy consumption for the community.", "metadata": {}}, {"text": "Acknowledgments and Disclosure of Funding\nWe would like to thank Yunseok Jang and SymbioticLab members for helpful comments and sugges-\ntions on the paper.", "metadata": {}}, {"text": "This work and its authors were in part supported by NSF grants CNS-2104243,\nCNS-2106184, and CNS-2450085, grants from VMware, the Mozilla Foundation, Cisco, Ford, and\nGitHub, and gifts from Salesforce and Google.", "metadata": {}}, {"text": "Jae-Won Chung is additionally supported by the\nKwanjeong Educational Foundation.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "References\n[1] NVIDIA Management Library (NVML). https://developer.nvidia.com/\nnvidia-management-library-nvml.\n[2] Zeus: Deep learning energy measurement and optimization. https://github.com/\nml-energy/zeus.\n[3] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl,\nLingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero\nKauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dim-\nitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue\nWu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report.arXiv preprint\narXiv:2504.21318, 2025.\n[4] International Energy Agency. Electricity 2025, 2025.\n[5] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-\nvani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in\nLLM inference with Sarathi-Serve. InOSDI, 2024.\n[6] Character AI. Character ai.https://character.ai, 2023.\n[7] Stability AI. Introducing SDXL turbo: A real-time text-to-image generation model, 2023.\n[8] AI@Meta. Llama 3 model card. 2024.\n[9] AI@Meta. Llama 4 model card. 2024.\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and\nSumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head\ncheckpoints.Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\n[11] Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly, Ayatelrahman Elsayed,\nAbdel-Hameed Badawy, Gopinath Chennupati, Stephan Eidenbenz, and Nandakishore Santhi.\nVerified instruction-level energy consumption measurement for NVIDIA GPUs.Proceedings of\nthe 17th ACM International Conference on Computing Frontiers, 2020.\n[12] Jeff Bar. Amazon EC2 update – inf1 instances with AWS inferentia chips for high performance\ncost-effective inferencing, 2019.\n[13] Nathan Beniach and Air Street Capital. State of AI report compute index. https://www.\nstateof.ai/compute.\n[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\nminik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, Varun Jampani, and Robin\nRombach. Stable video diffusion: Scaling latent video diffusion models to large datasets.arXiv\npreprint arXiv:2311.15127, 2023.\n[15] CBRE. Global data center trends 2023. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2023, 2023.\n[16] CBRE. Global data center trends 2024. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2024, 2024.\n[17] CBRE. Global data center trends 2025. https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2025, 2025.\n[18] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong\nDuan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.\nSharegpt4video: Improving video understanding and generation with better captions.Advances\nin Neural Information Processing Systems Datasets and Benchmarks, 2024.\n[19] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual\naggregation transformer for image super-resolution.Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2023.\n[20] Sangjin Choi, Inhoe Koo, Jeongseob Ahn, Myeongjae Jeon, and Youngjin Kwon. EnvPipe:\nPerformance-preserving DNN training framework for saving energy.Proceedings of the 2023\nUSENIX Annual Technical Conference, 2023.\n11", "sentences": [{"text": "References\n[1] NVIDIA Management Library (NVML).", "metadata": {}}, {"text": "https://developer.nvidia.com/\nnvidia-management-library-nvml.", "metadata": {}}, {"text": "[2] Zeus: Deep learning energy measurement and optimization.", "metadata": {}}, {"text": "https://github.com/\nml-energy/zeus.", "metadata": {}}, {"text": "[3] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl,\nLingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero\nKauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dim-\nitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue\nWu, Safoora Yousefi, and Guoqing Zheng.", "metadata": {}}, {"text": "Phi-4-reasoning technical report.arXiv preprint\narXiv:2504.21318, 2025.", "metadata": {}}, {"text": "[4] International Energy Agency.", "metadata": {}}, {"text": "Electricity 2025, 2025.", "metadata": {}}, {"text": "[5] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-\nvani, Alexey Tumanov, and Ramachandran Ramjee.", "metadata": {}}, {"text": "Taming Throughput-Latency tradeoff in\nLLM inference with Sarathi-Serve.", "metadata": {}}, {"text": "InOSDI, 2024.", "metadata": {}}, {"text": "[6] Character AI.", "metadata": {}}, {"text": "Character ai.https://character.ai, 2023.", "metadata": {}}, {"text": "[7] Stability AI.", "metadata": {}}, {"text": "Introducing SDXL turbo: A real-time text-to-image generation model, 2023.", "metadata": {}}, {"text": "[8] AI@Meta.", "metadata": {}}, {"text": "Llama 3 model card.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "[9] AI@Meta.", "metadata": {}}, {"text": "Llama 4 model card.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and\nSumit Sanghai.", "metadata": {}}, {"text": "GQA: Training generalized multi-query transformer models from multi-head\ncheckpoints.Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.", "metadata": {}}, {"text": "[11] Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly, Ayatelrahman Elsayed,\nAbdel-Hameed Badawy, Gopinath Chennupati, Stephan Eidenbenz, and Nandakishore Santhi.", "metadata": {}}, {"text": "Verified instruction-level energy consumption measurement for NVIDIA GPUs.Proceedings of\nthe 17th ACM International Conference on Computing Frontiers, 2020.", "metadata": {}}, {"text": "[12] Jeff Bar.", "metadata": {}}, {"text": "Amazon EC2 update – inf1 instances with AWS inferentia chips for high performance\ncost-effective inferencing, 2019.", "metadata": {}}, {"text": "[13] Nathan Beniach and Air Street Capital.", "metadata": {}}, {"text": "State of AI report compute index.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "stateof.ai/compute.", "metadata": {}}, {"text": "[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\nminik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, Varun Jampani, and Robin\nRombach.", "metadata": {}}, {"text": "Stable video diffusion: Scaling latent video diffusion models to large datasets.arXiv\npreprint arXiv:2311.15127, 2023.", "metadata": {}}, {"text": "[15] CBRE.", "metadata": {}}, {"text": "Global data center trends 2023.", "metadata": {}}, {"text": "https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2023, 2023.", "metadata": {}}, {"text": "[16] CBRE.", "metadata": {}}, {"text": "Global data center trends 2024.", "metadata": {}}, {"text": "https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2024, 2024.", "metadata": {}}, {"text": "[17] CBRE.", "metadata": {}}, {"text": "Global data center trends 2025.", "metadata": {}}, {"text": "https://www.cbre.com/insights/reports/\nglobal-data-center-trends-2025, 2025.", "metadata": {}}, {"text": "[18] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong\nDuan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.", "metadata": {}}, {"text": "Sharegpt4video: Improving video understanding and generation with better captions.Advances\nin Neural Information Processing Systems Datasets and Benchmarks, 2024.", "metadata": {}}, {"text": "[19] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu.", "metadata": {}}, {"text": "Dual\naggregation transformer for image super-resolution.Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2023.", "metadata": {}}, {"text": "[20] Sangjin Choi, Inhoe Koo, Jeongseob Ahn, Myeongjae Jeon, and Youngjin Kwon.", "metadata": {}}, {"text": "EnvPipe:\nPerformance-preserving DNN training framework for saving energy.Proceedings of the 2023\nUSENIX Annual Technical Conference, 2023.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "[21] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury.\nReducing energy bloat in large model training.Proceedings of the 30th ACM Symposium on\nOperating Systems Principles, 2024.\n[22] Jae-Won Chung, Nishil Talati, and Mosharaf Chowdhury. Toward cross-layer energy opti-\nmizations in AI systems.DOE ASCR Energy-Efficient Computing for Science Workshop,\n2024.\n[23] DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement\nlearning.arXiv preprint arXiv:2501.12948, 2025.\n[24] Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah\nGoodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes,\nand James Manyika. Measuring the environmental impact of delivering AI at google scale.\narXiv preprint arXiv:2508.15734, 2025.\n[25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion\nEnglish, and Robin Rombach. Scaling rectified flow transformers for high-resolution image\nsynthesis. InICML, 2024.\n[26] Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your\nphone.arXiv preprint arXiv:2404.14219, 2024.\n[27] Hugging Face. Ai energy score. https://huggingface.github.io/AIEnergyScore,\n2025.\n[28] Meta GenAI. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint, 2023.\n[29] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh\nAgrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image\ndiffusion models without specific tuning. InICLR, 2024.\n[30] Yatharth Gupta, Vishnu V . Jaddipal, Harish Prabhala, Sayak Paul, and Patrick V on Platen.\nProgressive knowledge distillation of stable diffusion xl using layer level loss.arXiv preprint\narXiv:2401.02677, 2024.\n[31] The White House. Fact sheet: President donald j. trump establishes the national energy\ndominance council, 2025.\n[32] HPCwire. AWS to offer NVIDIA’s T4 GPUs for AI inferencing, 2019.\n[33] Régis Pierrard Ilyas Moutawwakil. LLM-Perf leaderboard. https://huggingface.co/\nspaces/optimum/llm-perf-leaderboard, 2023.\n[34] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b.arXiv preprint\narXiv:2310.06825, 2023.\n[35] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier,\nMarie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,\nTeven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\nWilliam El Sayed. Mixtral of experts.arXiv preprint arXiv:2401.04088, 2024.\n[36] Heehoon Kim, Junyeol Ryu, and Jaejin Lee. TCCL: Discovering better communication paths\nfor PCIe GPU clusters.Proceedings of the 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume 3, 2024.\n[37] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan\nGenc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir\nGholami. Full stack optimization of transformer inference.Architecture and System Support\nfor Transformer Models, 2023.\n[38] Helen Kou. Power for AI: Easier said than built. https://about.bnef.com/insights/\ncommodities/power-for-ai-easier-said-than-built/, 2025.\n12", "sentences": [{"text": "[21] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury.", "metadata": {}}, {"text": "Reducing energy bloat in large model training.Proceedings of the 30th ACM Symposium on\nOperating Systems Principles, 2024.", "metadata": {}}, {"text": "[22] Jae-Won Chung, Nishil Talati, and Mosharaf Chowdhury.", "metadata": {}}, {"text": "Toward cross-layer energy opti-\nmizations in AI systems.DOE ASCR Energy-Efficient Computing for Science Workshop,\n2024.", "metadata": {}}, {"text": "[23] DeepSeek-AI.", "metadata": {}}, {"text": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement\nlearning.arXiv preprint arXiv:2501.12948, 2025.", "metadata": {}}, {"text": "[24] Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah\nGoodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes,\nand James Manyika.", "metadata": {}}, {"text": "Measuring the environmental impact of delivering AI at google scale.", "metadata": {}}, {"text": "arXiv preprint arXiv:2508.15734, 2025.", "metadata": {}}, {"text": "[25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion\nEnglish, and Robin Rombach.", "metadata": {}}, {"text": "Scaling rectified flow transformers for high-resolution image\nsynthesis.", "metadata": {}}, {"text": "InICML, 2024.", "metadata": {}}, {"text": "[26] Marah Abdin et al.", "metadata": {}}, {"text": "Phi-3 technical report: A highly capable language model locally on your\nphone.arXiv preprint arXiv:2404.14219, 2024.", "metadata": {}}, {"text": "[27] Hugging Face.", "metadata": {}}, {"text": "Ai energy score.", "metadata": {}}, {"text": "https://huggingface.github.io/AIEnergyScore,\n2025.", "metadata": {}}, {"text": "[28] Meta GenAI.", "metadata": {}}, {"text": "Llama 2: Open foundation and fine-tuned chat models.arXiv preprint, 2023.", "metadata": {}}, {"text": "[29] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh\nAgrawala, Dahua Lin, and Bo Dai.", "metadata": {}}, {"text": "AnimateDiff: Animate your personalized text-to-image\ndiffusion models without specific tuning.", "metadata": {}}, {"text": "InICLR, 2024.", "metadata": {}}, {"text": "[30] Yatharth Gupta, Vishnu V .", "metadata": {}}, {"text": "Jaddipal, Harish Prabhala, Sayak Paul, and Patrick V on Platen.", "metadata": {}}, {"text": "Progressive knowledge distillation of stable diffusion xl using layer level loss.arXiv preprint\narXiv:2401.02677, 2024.", "metadata": {}}, {"text": "[31] The White House.", "metadata": {}}, {"text": "Fact sheet: President donald j.", "metadata": {}}, {"text": "trump establishes the national energy\ndominance council, 2025.", "metadata": {}}, {"text": "[32] HPCwire.", "metadata": {}}, {"text": "AWS to offer NVIDIA’s T4 GPUs for AI inferencing, 2019.", "metadata": {}}, {"text": "[33] Régis Pierrard Ilyas Moutawwakil.", "metadata": {}}, {"text": "LLM-Perf leaderboard.", "metadata": {}}, {"text": "https://huggingface.co/\nspaces/optimum/llm-perf-leaderboard, 2023.", "metadata": {}}, {"text": "[34] Albert Q.", "metadata": {}}, {"text": "Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed.", "metadata": {}}, {"text": "Mistral 7b.arXiv preprint\narXiv:2310.06825, 2023.", "metadata": {}}, {"text": "[35] Albert Q.", "metadata": {}}, {"text": "Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier,\nMarie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,\nTeven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\nWilliam El Sayed.", "metadata": {}}, {"text": "Mixtral of experts.arXiv preprint arXiv:2401.04088, 2024.", "metadata": {}}, {"text": "[36] Heehoon Kim, Junyeol Ryu, and Jaejin Lee.", "metadata": {}}, {"text": "TCCL: Discovering better communication paths\nfor PCIe GPU clusters.Proceedings of the 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume 3, 2024.", "metadata": {}}, {"text": "[37] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan\nGenc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W.", "metadata": {}}, {"text": "Mahoney, Sophia Shao, and Amir\nGholami.", "metadata": {}}, {"text": "Full stack optimization of transformer inference.Architecture and System Support\nfor Transformer Models, 2023.", "metadata": {}}, {"text": "[38] Helen Kou.", "metadata": {}}, {"text": "Power for AI: Easier said than built.", "metadata": {}}, {"text": "https://about.bnef.com/insights/\ncommodities/power-for-ai-easier-said-than-built/, 2025.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language\nmodel serving with PagedAttention.Proceedings of the 29th Symposium on Operating Systems\nPrinciples, 2023.\n[40] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying\nthe carbon emissions of machine learning.arXiv preprint, 2019.\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. InCVPR, 2024.\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.\nLLaV A-NeXT: Improved reasoning, ocr, and world knowledge, 2024.\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.Advances\nin Neural Information Processing Systems, 2023.\n[44] Jiachen Liu, Jae-Won Chung, Zhiyu Wu, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.\nAndes: Defining and enhancing quality-of-experience in LLM-based text streaming services.\narXiv preprint arXiv:2404.16283, 2024.\n[45] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nchatGPT really correct? rigorous evaluation of large language models for code generation. In\nNeurIPS, 2023.\n[46] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noua-\nmane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack\nv2: The next generation.arXiv preprint arXiv:2402.19173, 2024.\n[47] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon\nfootprint of bloom, a 176b parameter language model.Journal of Machine Learning Research,\n2024.\n[48] McKinsey & Company. Investing in the rising data center economy. https:\n//www.mckinsey.com/industries/technology-media-and-telecommunications/\nour-insights/investing-in-the-rising-data-center-economy, 2023.\n[49] McKinsey & Company. How data centers and the energy sector can sate AI’s hunger for\npower. https://www.mckinsey.com/industries/private-capital/our-insights/\nhow-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power ,\n2024.\n[50] Midjourney. Midjourney.https://midjourney.com, 2022.\n[51] Sebastian Moss. Meta’s mark zuckerberg says energy constraints are holding back AI data\ncenter buildout, 2024.\n[52] NVIDIA. NVIDIA DGX A100 datasheet. https://www.nvidia.com/content/dam/\nen-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020.\n[53] NVIDIA. NVIDIA DGX H200 datasheet. https://resources.nvidia.com/\nen-us-dgx-systems/dgx-h200-datasheet, 2024.\n[54] NVIDIA. NVIDIA DGX B200 datasheet. https://resources.nvidia.com/\nen-us-dgx-systems/dgx-b200-datasheet, 2025.\n[55] OpenAI. What are tokens and how to count them? https://help.openai.com/en/\narticles/4936856-what-are-tokens-and-how-to-count-them.\n[56] OpenAI. ChatGPT.https://chatgpt.com, 2022.\n[57] OpenAI. Sora.https://openai.com/index/sora, 2024.\n[58] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam,\nand Ricardo Bianchini. Characterizing power management opportunities for llms in the cloud.\nASPLOS, 2024.\n[59] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and\nRicardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. InISCA,\n2024.\n13", "sentences": [{"text": "[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph Gonzalez, Hao Zhang, and Ion Stoica.", "metadata": {}}, {"text": "Efficient memory management for large language\nmodel serving with PagedAttention.Proceedings of the 29th Symposium on Operating Systems\nPrinciples, 2023.", "metadata": {}}, {"text": "[40] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.", "metadata": {}}, {"text": "Quantifying\nthe carbon emissions of machine learning.arXiv preprint, 2019.", "metadata": {}}, {"text": "[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.", "metadata": {}}, {"text": "Improved baselines with visual\ninstruction tuning.", "metadata": {}}, {"text": "InCVPR, 2024.", "metadata": {}}, {"text": "[42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.", "metadata": {}}, {"text": "LLaV A-NeXT: Improved reasoning, ocr, and world knowledge, 2024.", "metadata": {}}, {"text": "[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.", "metadata": {}}, {"text": "Visual instruction tuning.Advances\nin Neural Information Processing Systems, 2023.", "metadata": {}}, {"text": "[44] Jiachen Liu, Jae-Won Chung, Zhiyu Wu, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.", "metadata": {}}, {"text": "Andes: Defining and enhancing quality-of-experience in LLM-based text streaming services.", "metadata": {}}, {"text": "arXiv preprint arXiv:2404.16283, 2024.", "metadata": {}}, {"text": "[45] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang.", "metadata": {}}, {"text": "Is your code generated by\nchatGPT really correct?", "metadata": {}}, {"text": "rigorous evaluation of large language models for code generation.", "metadata": {}}, {"text": "In\nNeurIPS, 2023.", "metadata": {}}, {"text": "[46] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noua-\nmane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al.", "metadata": {}}, {"text": "Starcoder 2 and the stack\nv2: The next generation.arXiv preprint arXiv:2402.19173, 2024.", "metadata": {}}, {"text": "[47] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "Estimating the carbon\nfootprint of bloom, a 176b parameter language model.Journal of Machine Learning Research,\n2024.", "metadata": {}}, {"text": "[48] McKinsey & Company.", "metadata": {}}, {"text": "Investing in the rising data center economy.", "metadata": {}}, {"text": "https:\n//www.mckinsey.com/industries/technology-media-and-telecommunications/\nour-insights/investing-in-the-rising-data-center-economy, 2023.", "metadata": {}}, {"text": "[49] McKinsey & Company.", "metadata": {}}, {"text": "How data centers and the energy sector can sate AI’s hunger for\npower.", "metadata": {}}, {"text": "https://www.mckinsey.com/industries/private-capital/our-insights/\nhow-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power ,\n2024.", "metadata": {}}, {"text": "[50] Midjourney.", "metadata": {}}, {"text": "Midjourney.https://midjourney.com, 2022.", "metadata": {}}, {"text": "[51] Sebastian Moss.", "metadata": {}}, {"text": "Meta’s mark zuckerberg says energy constraints are holding back AI data\ncenter buildout, 2024.", "metadata": {}}, {"text": "[52] NVIDIA.", "metadata": {}}, {"text": "NVIDIA DGX A100 datasheet.", "metadata": {}}, {"text": "https://www.nvidia.com/content/dam/\nen-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020.", "metadata": {}}, {"text": "[53] NVIDIA.", "metadata": {}}, {"text": "NVIDIA DGX H200 datasheet.", "metadata": {}}, {"text": "https://resources.nvidia.com/\nen-us-dgx-systems/dgx-h200-datasheet, 2024.", "metadata": {}}, {"text": "[54] NVIDIA.", "metadata": {}}, {"text": "NVIDIA DGX B200 datasheet.", "metadata": {}}, {"text": "https://resources.nvidia.com/\nen-us-dgx-systems/dgx-b200-datasheet, 2025.", "metadata": {}}, {"text": "[55] OpenAI.", "metadata": {}}, {"text": "What are tokens and how to count them?", "metadata": {}}, {"text": "https://help.openai.com/en/\narticles/4936856-what-are-tokens-and-how-to-count-them.", "metadata": {}}, {"text": "[56] OpenAI.", "metadata": {}}, {"text": "ChatGPT.https://chatgpt.com, 2022.", "metadata": {}}, {"text": "[57] OpenAI.", "metadata": {}}, {"text": "Sora.https://openai.com/index/sora, 2024.", "metadata": {}}, {"text": "[58] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam,\nand Ricardo Bianchini.", "metadata": {}}, {"text": "Characterizing power management opportunities for llms in the cloud.", "metadata": {}}, {"text": "ASPLOS, 2024.", "metadata": {}}, {"text": "[59] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and\nRicardo Bianchini.", "metadata": {}}, {"text": "Splitwise: Efficient generative llm inference using phase splitting.", "metadata": {}}, {"text": "InISCA,\n2024.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "[60] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network\ntraining.arXiv preprint, 2021.\n[61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller,\nJoe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution\nimage synthesis. InICLR, 2024.\n[62] PromptHero. OpenJourney v4, 2023.\n[63] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus\nFranke, Zbigniew Kalbarczyk, Tamer Ba¸ sar, and Ravishankar K. Iyer. Power-aware deep\nlearning model serving with u-Serve. InATC, 2024.\n[64] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a\nbenchmark. InCoLM, 2024.\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer.\nHigh-resolution image synthesis with latent diffusion models. InCVPR, 2022.\n[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\nXiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for\ncode.arXiv preprint arXiv:2308.12950, 2024.\n[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training multi-billion parameter language models using model\nparallelism.arXiv preprint, 2019.\n[68] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM:\nDesigning llm inference clusters for performance and energy efficiency. InHPCA, 2025.\n[69] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models.arXiv preprint\narXiv:2405.09818, 2024.\n[70] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea\nHu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis,\nMateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham\nAgrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec,\nKelly Schaefer, and Scott Huffman. CodeGemma: Open code models based on gemma.arXiv\npreprint arXiv:2406.11409, 2024.\n[71] Gemma Team. Gemma 2: Improving open language models at a practical size.arXiv preprint\narXiv:2408.00118, 2024.\n[72] ShareGPT Team. ShareGPT.https://sharegpt.com/.\n[73] Llama team at Meta. The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open\nand efficient foundation language models.arXiv preprint, 2023.\n[75] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holle-\nman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver\nCurtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao\nPan, Manu Prasad Manmathan, Liz Raymond, Tom St. John, Arjun Suresh, Rowan Taubitz, Sean\nZhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi. MLPerf power: Benchmarking the\nenergy efficiency of machine learning systems from uWatts to MWatts for sustainable ai. In\nHPCA, 2025.\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in Neural Information\nProcessing Systems, 2017.\n[77] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas\n14", "sentences": [{"text": "[60] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "Carbon emissions and large neural network\ntraining.arXiv preprint, 2021.", "metadata": {}}, {"text": "[61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller,\nJoe Penna, and Robin Rombach.", "metadata": {}}, {"text": "SDXL: Improving latent diffusion models for high-resolution\nimage synthesis.", "metadata": {}}, {"text": "InICLR, 2024.", "metadata": {}}, {"text": "[62] PromptHero.", "metadata": {}}, {"text": "OpenJourney v4, 2023.", "metadata": {}}, {"text": "[63] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus\nFranke, Zbigniew Kalbarczyk, Tamer Ba¸ sar, and Ravishankar K.", "metadata": {}}, {"text": "Iyer.", "metadata": {}}, {"text": "Power-aware deep\nlearning model serving with u-Serve.", "metadata": {}}, {"text": "InATC, 2024.", "metadata": {}}, {"text": "[64] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R.", "metadata": {}}, {"text": "Bowman.", "metadata": {}}, {"text": "GPQA: A graduate-level google-proof q&a\nbenchmark.", "metadata": {}}, {"text": "InCoLM, 2024.", "metadata": {}}, {"text": "[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer.", "metadata": {}}, {"text": "High-resolution image synthesis with latent diffusion models.", "metadata": {}}, {"text": "InCVPR, 2022.", "metadata": {}}, {"text": "[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,\nIvan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan\nXiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve.", "metadata": {}}, {"text": "Code llama: Open foundation models for\ncode.arXiv preprint arXiv:2308.12950, 2024.", "metadata": {}}, {"text": "[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro.", "metadata": {}}, {"text": "Megatron-LM: Training multi-billion parameter language models using model\nparallelism.arXiv preprint, 2019.", "metadata": {}}, {"text": "[68] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse.", "metadata": {}}, {"text": "DynamoLLM:\nDesigning llm inference clusters for performance and energy efficiency.", "metadata": {}}, {"text": "InHPCA, 2025.", "metadata": {}}, {"text": "[69] Chameleon Team.", "metadata": {}}, {"text": "Chameleon: Mixed-modal early-fusion foundation models.arXiv preprint\narXiv:2405.09818, 2024.", "metadata": {}}, {"text": "[70] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea\nHu, Christopher A.", "metadata": {}}, {"text": "Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis,\nMateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham\nAgrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec,\nKelly Schaefer, and Scott Huffman.", "metadata": {}}, {"text": "CodeGemma: Open code models based on gemma.arXiv\npreprint arXiv:2406.11409, 2024.", "metadata": {}}, {"text": "[71] Gemma Team.", "metadata": {}}, {"text": "Gemma 2: Improving open language models at a practical size.arXiv preprint\narXiv:2408.00118, 2024.", "metadata": {}}, {"text": "[72] ShareGPT Team.", "metadata": {}}, {"text": "ShareGPT.https://sharegpt.com/.", "metadata": {}}, {"text": "[73] Llama team at Meta.", "metadata": {}}, {"text": "The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.", "metadata": {}}, {"text": "[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.", "metadata": {}}, {"text": "LLaMA: Open\nand efficient foundation language models.arXiv preprint, 2023.", "metadata": {}}, {"text": "[75] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holle-\nman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver\nCurtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao\nPan, Manu Prasad Manmathan, Liz Raymond, Tom St.", "metadata": {}}, {"text": "John, Arjun Suresh, Rowan Taubitz, Sean\nZhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi.", "metadata": {}}, {"text": "MLPerf power: Benchmarking the\nenergy efficiency of machine learning systems from uWatts to MWatts for sustainable ai.", "metadata": {}}, {"text": "In\nHPCA, 2025.", "metadata": {}}, {"text": "[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.", "metadata": {}}, {"text": "Gomez,\nŁukasz Kaiser, and Illia Polosukhin.", "metadata": {}}, {"text": "Attention is all you need.Advances in Neural Information\nProcessing Systems, 2017.", "metadata": {}}, {"text": "[77] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas\n14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/\ndiffusers.\n[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.\nModelScope text-to-video technical report.arXiv preprint arXiv:2308.06571, 2023.\n[79] Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, and Xin Jin. ServeGen: Workload\ncharacterization and generation of large language model serving in production.arXiv preprint\narXiv:2505.09999, 2025.\n[80] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang,\nFeng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin\nYang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin\nZhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,\nXingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng\nZhou, and Zihan Qiu. Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.\n[81] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU\nenergy consumption of DNN training.NSDI, 2023.\n[82] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca:\nA distributed serving system for Transformer-Based generative models. InOSDI, 2022.\n[83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive\nmodels for content-rich text-to-image generation.Transactions on Machine Learning Research,\n2022.\n[84] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang,\nDeli Zhao, and Jingren Zhou. I2VGen-XL: High-quality image-to-video synthesis via cascaded\ndiffusion models.arXiv preprint arXiv:2311.04145, 2023.\n[85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena. InNeurIPS, 2023.\n[86] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang. DistServe: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving. InOSDI, 2024.\n15", "sentences": [{"text": "Wolf.", "metadata": {}}, {"text": "Diffusers: State-of-the-art diffusion models.", "metadata": {}}, {"text": "https://github.com/huggingface/\ndiffusers.", "metadata": {}}, {"text": "[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.", "metadata": {}}, {"text": "ModelScope text-to-video technical report.arXiv preprint arXiv:2308.06571, 2023.", "metadata": {}}, {"text": "[79] Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, and Xin Jin.", "metadata": {}}, {"text": "ServeGen: Workload\ncharacterization and generation of large language model serving in production.arXiv preprint\narXiv:2505.09999, 2025.", "metadata": {}}, {"text": "[80] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang,\nFeng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin\nYang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin\nZhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,\nXingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng\nZhou, and Zihan Qiu.", "metadata": {}}, {"text": "Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.", "metadata": {}}, {"text": "[81] Jie You, Jae-Won Chung, and Mosharaf Chowdhury.", "metadata": {}}, {"text": "Zeus: Understanding and optimizing GPU\nenergy consumption of DNN training.NSDI, 2023.", "metadata": {}}, {"text": "[82] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.", "metadata": {}}, {"text": "Orca:\nA distributed serving system for Transformer-Based generative models.", "metadata": {}}, {"text": "InOSDI, 2022.", "metadata": {}}, {"text": "[83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.", "metadata": {}}, {"text": "Scaling autoregressive\nmodels for content-rich text-to-image generation.Transactions on Machine Learning Research,\n2022.", "metadata": {}}, {"text": "[84] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang,\nDeli Zhao, and Jingren Zhou.", "metadata": {}}, {"text": "I2VGen-XL: High-quality image-to-video synthesis via cascaded\ndiffusion models.arXiv preprint arXiv:2311.04145, 2023.", "metadata": {}}, {"text": "[85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P.", "metadata": {}}, {"text": "Xing, Hao Zhang, Joseph E.", "metadata": {}}, {"text": "Gonzalez, and Ion Stoica.", "metadata": {}}, {"text": "Judging llm-as-a-judge with mt-bench and chatbot arena.", "metadata": {}}, {"text": "InNeurIPS, 2023.", "metadata": {}}, {"text": "[86] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang.", "metadata": {}}, {"text": "DistServe: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving.", "metadata": {}}, {"text": "InOSDI, 2024.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "NeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction reflect the paper’s contributions and scope.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Limitations are discussed in Appendix E.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\n16", "sentences": [{"text": "NeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: The abstract and introduction reflect the paper’s contributions and scope.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.", "metadata": {}}, {"text": "• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations.", "metadata": {}}, {"text": "A No or\nNA answer to this question will not be perceived well by the reviewers.", "metadata": {}}, {"text": "• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.", "metadata": {}}, {"text": "• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.", "metadata": {}}, {"text": "2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: Limitations are discussed in Appendix E.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.", "metadata": {}}, {"text": "• The authors are encouraged to create a separate \"Limitations\" section in their paper.", "metadata": {}}, {"text": "• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally).", "metadata": {}}, {"text": "The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.", "metadata": {}}, {"text": "• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs.", "metadata": {}}, {"text": "In general, empirical results often\ndepend on implicit assumptions, which should be articulated.", "metadata": {}}, {"text": "• The authors should reflect on the factors that influence the performance of the approach.", "metadata": {}}, {"text": "For example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting.", "metadata": {}}, {"text": "Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.", "metadata": {}}, {"text": "• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.", "metadata": {}}, {"text": "• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.", "metadata": {}}, {"text": "• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper.", "metadata": {}}, {"text": "The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community.", "metadata": {}}, {"text": "Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.", "metadata": {}}, {"text": "3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?", "metadata": {}}, {"text": "Answer: [NA]\n16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "Justification: This paper does not include theoretical results.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.\nThe full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.\nenergy/leaderboard.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5.Open access to data and code\n17", "sentences": [{"text": "Justification: This paper does not include theoretical results.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not include theoretical results.", "metadata": {}}, {"text": "• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.", "metadata": {}}, {"text": "• All assumptions should be clearly stated or referenced in the statement of any theorems.", "metadata": {}}, {"text": "• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.", "metadata": {}}, {"text": "• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.", "metadata": {}}, {"text": "• Theorems and Lemmas that the proof relies upon should be properly referenced.", "metadata": {}}, {"text": "4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.", "metadata": {}}, {"text": "The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.", "metadata": {}}, {"text": "energy/leaderboard.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not include experiments.", "metadata": {}}, {"text": "• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.", "metadata": {}}, {"text": "• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.", "metadata": {}}, {"text": "• Depending on the contribution, reproducibility can be accomplished in various ways.", "metadata": {}}, {"text": "For example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model.", "metadata": {}}, {"text": "In general.", "metadata": {}}, {"text": "releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.", "metadata": {}}, {"text": "• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution.", "metadata": {}}, {"text": "For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.", "metadata": {}}, {"text": "(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.", "metadata": {}}, {"text": "(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).", "metadata": {}}, {"text": "(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.", "metadata": {}}, {"text": "In the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.", "metadata": {}}, {"text": "5.Open access to data and code\n17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.\nThe full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.\nenergy/leaderboard.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We mention important details in the main paper and provide more details in\nthe Appendix. Full details are available in the code repository.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: We were not able to run the benchmark multiple times due to the high monetary\ncost of even a single run.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n18", "sentences": [{"text": "Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: The benchmark code and the result data that supply the leaderboard are avail-\nable open-source and documented at https://github.com/ml-energy/leaderboard.", "metadata": {}}, {"text": "The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.", "metadata": {}}, {"text": "energy/leaderboard.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that paper does not include experiments requiring code.", "metadata": {}}, {"text": "• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.", "metadata": {}}, {"text": "• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer.", "metadata": {}}, {"text": "Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).", "metadata": {}}, {"text": "• The instructions should contain the exact command and environment needed to run to\nreproduce the results.", "metadata": {}}, {"text": "See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.", "metadata": {}}, {"text": "• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.", "metadata": {}}, {"text": "• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines.", "metadata": {}}, {"text": "If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.", "metadata": {}}, {"text": "• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).", "metadata": {}}, {"text": "• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.", "metadata": {}}, {"text": "6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: We mention important details in the main paper and provide more details in\nthe Appendix.", "metadata": {}}, {"text": "Full details are available in the code repository.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not include experiments.", "metadata": {}}, {"text": "• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.", "metadata": {}}, {"text": "• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.", "metadata": {}}, {"text": "7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?", "metadata": {}}, {"text": "Answer: [No]\nJustification: We were not able to run the benchmark multiple times due to the high monetary\ncost of even a single run.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not include experiments.", "metadata": {}}, {"text": "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.", "metadata": {}}, {"text": "18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We mention compute resources in the beginning of Section 4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We confirm that we reviewed the NeurIPS Code of Ethics and that our research\nconforms to it.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss this in Appendix F.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n19", "sentences": [{"text": "• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).", "metadata": {}}, {"text": "• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).", "metadata": {}}, {"text": "• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.", "metadata": {}}, {"text": "• It is OK to report 1-sigma error bars, but one should state it.", "metadata": {}}, {"text": "The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.", "metadata": {}}, {"text": "• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g.", "metadata": {}}, {"text": "negative\nerror rates).", "metadata": {}}, {"text": "• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.", "metadata": {}}, {"text": "8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: We mention compute resources in the beginning of Section 4.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not include experiments.", "metadata": {}}, {"text": "• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.", "metadata": {}}, {"text": "• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.", "metadata": {}}, {"text": "• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).", "metadata": {}}, {"text": "9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: We confirm that we reviewed the NeurIPS Code of Ethics and that our research\nconforms to it.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.", "metadata": {}}, {"text": "• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.", "metadata": {}}, {"text": "• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).", "metadata": {}}, {"text": "10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: We discuss this in Appendix F.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that there is no societal impact of the work performed.", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: We do not believe safeguards are necessary for our work.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We extensively use models and datasets created by others in our benchmark.\nWe credit the authors through citation. Appendix A contains a comprehensive table. The\nbenchmark has default request datasets that we recommend, but does not come packaged\nwith any specific model or dataset.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n20", "sentences": [{"text": "• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.", "metadata": {}}, {"text": "• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.", "metadata": {}}, {"text": "• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments.", "metadata": {}}, {"text": "However, if there is a direct path to\nany negative applications, the authors should point it out.", "metadata": {}}, {"text": "For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation.", "metadata": {}}, {"text": "On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.", "metadata": {}}, {"text": "• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.", "metadata": {}}, {"text": "• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).", "metadata": {}}, {"text": "11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?", "metadata": {}}, {"text": "Answer: [NA]\nJustification: We do not believe safeguards are necessary for our work.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper poses no such risks.", "metadata": {}}, {"text": "• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.", "metadata": {}}, {"text": "• Datasets that have been scraped from the Internet could pose safety risks.", "metadata": {}}, {"text": "The authors\nshould describe how they avoided releasing unsafe images.", "metadata": {}}, {"text": "• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.", "metadata": {}}, {"text": "12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: We extensively use models and datasets created by others in our benchmark.", "metadata": {}}, {"text": "We credit the authors through citation.", "metadata": {}}, {"text": "Appendix A contains a comprehensive table.", "metadata": {}}, {"text": "The\nbenchmark has default request datasets that we recommend, but does not come packaged\nwith any specific model or dataset.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not use existing assets.", "metadata": {}}, {"text": "• The authors should cite the original paper that produced the code package or dataset.", "metadata": {}}, {"text": "• The authors should state which version of the asset is used and, if possible, include a\nURL.", "metadata": {}}, {"text": "• The name of the license (e.g., CC-BY 4.0) should be included for each asset.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}, {"title": "Page 21", "paragraphs": [{"text": "• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: The benchmark code is available open-source and documented at https:\n//github.com/ml-energy/leaderboardunder the Apache-2.0 license.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n21", "sentences": [{"text": "• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.", "metadata": {}}, {"text": "• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided.", "metadata": {}}, {"text": "For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets.", "metadata": {}}, {"text": "Their licensing guide can help determine the\nlicense of a dataset.", "metadata": {}}, {"text": "• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.", "metadata": {}}, {"text": "• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.", "metadata": {}}, {"text": "13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?", "metadata": {}}, {"text": "Answer: [Yes]\nJustification: The benchmark code is available open-source and documented at https:\n//github.com/ml-energy/leaderboardunder the Apache-2.0 license.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not release new assets.", "metadata": {}}, {"text": "• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates.", "metadata": {}}, {"text": "This includes details about training, license,\nlimitations, etc.", "metadata": {}}, {"text": "• The paper should discuss whether and how consent was obtained from people whose\nasset is used.", "metadata": {}}, {"text": "• At submission time, remember to anonymize your assets (if applicable).", "metadata": {}}, {"text": "You can either\ncreate an anonymized URL or include an anonymized zip file.", "metadata": {}}, {"text": "14.Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?", "metadata": {}}, {"text": "Answer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.", "metadata": {}}, {"text": "• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.", "metadata": {}}, {"text": "• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.", "metadata": {}}, {"text": "15.", "metadata": {}}, {"text": "Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?", "metadata": {}}, {"text": "Answer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.", "metadata": {}}, {"text": "21", "metadata": {}}], "metadata": {"page": 21}}], "metadata": {"page": 21}}, {"title": "Page 22", "paragraphs": [{"text": "• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16.Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: We have used LLMs to assist in editing the paper, generating figures, and writ-\ning code snippets, and its use does not impact the core methodology, scientific rigorousness,\nor originality of the research.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22", "sentences": [{"text": "• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research.", "metadata": {}}, {"text": "If you obtained IRB approval, you\nshould clearly state this in the paper.", "metadata": {}}, {"text": "• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.", "metadata": {}}, {"text": "• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.", "metadata": {}}, {"text": "16.Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research?", "metadata": {}}, {"text": "Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.", "metadata": {}}, {"text": "Answer: [NA]\nJustification: We have used LLMs to assist in editing the paper, generating figures, and writ-\ning code snippets, and its use does not impact the core methodology, scientific rigorousness,\nor originality of the research.", "metadata": {}}, {"text": "Guidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.", "metadata": {}}, {"text": "• Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.", "metadata": {}}, {"text": "22", "metadata": {}}], "metadata": {"page": 22}}], "metadata": {"page": 22}}, {"title": "Page 23", "paragraphs": [{"text": "Table 2: Model type, task, and default request dataset used in the ML.ENERGY Benchmark.\nModel architecture Task Request dataset\nLarge Language Model Chat ShareGPT [72]\nCode EvalPlus [45]\nVision Language Model Visual chat LLaV A instruction dataset [43]\nDiffusion Model\nText-to-image PartiPrompts [83]\nText-to-video Captions in ShareGPT4Video [18]\nImage-to-video Captions and first frames in ShareGPT4Video [18]\nTable 3: Model architectures supported by the ML.ENERGY Benchmark for each task.\nTask Model Architectures\nChat Gemma 2 2B/9B/27B [71], Llama 3.1 8B/70B/405B [73],\nPhi 3 Mini/Small/Medium [26], Mistral 7B/Nemo/Large [34],\nMixtral 8x7B/8x22B [35]\nCode CodeLlama 7B/13B/34B/70B [66], StarCoder 2 3B/7B/15B [46],\nCodeGemma 2B/7B [70]\nVisual chat LLaV A 1.5 7B/13B [41], LLaV A NeXT 8B [42], Phi 3 Vision [26],\nChameleon 7B/30B [69]\nText-to-image Stable Diffusion 2.1/XL/XL Turbo/3 Medium [7, 25, 61, 65],\nOpenJourney 4 [62], SSD 1B [30]\nText-to-video ModelScope T2V [78], AnimateDiff [29]\nImage-to-video I2VGen XL [84], Stable Video Diffusion and Stable Video Diffusion XT [14]\nA Tasks, Model Architectures, and Default Request Datasets\nTables 2 and 3 list the model architectures and tasks supported by current iteration of the ML.ENERGY\nBenchmark, along with the default request datasets for each task. We note that models that were\nfine-tuned based on the supported models are also supported as is, and the benchmark is designed to\nbe extensible (Section 3.4).\nThe ML.ENERGY Benchmark cannot avoid being outdated given the rapid pace of development\nin the generative AI field. As such, we have been updating the benchmark (and the accompanying\nLeaderboard) with new tasks, models, datasets, hardware, runtimes, and more, and we intend to\ncontinue doing so as long as resources allow.\nB Energy Implication of System Parameters\nThis section discusses the energy implication of different system-level configurations. System-level\nconfigurations are those that do not changewhatis computed but ratherhowit is computed by the\nunderlying software system.\nB.1 Request Preemption Mechanism\nEven with the model and inference parameters fixed, the software system used to serve inference\nrequests, which determines how model computations are executed on a given hardware, significantly\nimpacts energy consumption. As a concrete example, we will examine the effect of “preemption\nmechanism,” a configuration parameter for LLM inference servers. When a server is overloaded with\nmore requests than its capacity, it needs to temporarily remove (or, preempt) some requests from the\nsystem and then later bring them back (or, restore). For LLM inference, there are two widely-used\nmechanisms for preemption: Recomputation and Swapping [39]. Recomputation simply drops all\ntemporary request data or state on preemption and recomputes everything from scratch on restoration.\nOn the other hand, Swapping moves the request state to the CPU’s memory, and then returns it to\nthe GPU on restoration. The best preemption mechanism depends on the computing hardware and\nsoftware configuration and the LLM being served.\n23", "sentences": [{"text": "Table 2: Model type, task, and default request dataset used in the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "Model architecture Task Request dataset\nLarge Language Model Chat ShareGPT [72]\nCode EvalPlus [45]\nVision Language Model Visual chat LLaV A instruction dataset [43]\nDiffusion Model\nText-to-image PartiPrompts [83]\nText-to-video Captions in ShareGPT4Video [18]\nImage-to-video Captions and first frames in ShareGPT4Video [18]\nTable 3: Model architectures supported by the ML.ENERGY Benchmark for each task.", "metadata": {}}, {"text": "Task Model Architectures\nChat Gemma 2 2B/9B/27B [71], Llama 3.1 8B/70B/405B [73],\nPhi 3 Mini/Small/Medium [26], Mistral 7B/Nemo/Large [34],\nMixtral 8x7B/8x22B [35]\nCode CodeLlama 7B/13B/34B/70B [66], StarCoder 2 3B/7B/15B [46],\nCodeGemma 2B/7B [70]\nVisual chat LLaV A 1.5 7B/13B [41], LLaV A NeXT 8B [42], Phi 3 Vision [26],\nChameleon 7B/30B [69]\nText-to-image Stable Diffusion 2.1/XL/XL Turbo/3 Medium [7, 25, 61, 65],\nOpenJourney 4 [62], SSD 1B [30]\nText-to-video ModelScope T2V [78], AnimateDiff [29]\nImage-to-video I2VGen XL [84], Stable Video Diffusion and Stable Video Diffusion XT [14]\nA Tasks, Model Architectures, and Default Request Datasets\nTables 2 and 3 list the model architectures and tasks supported by current iteration of the ML.ENERGY\nBenchmark, along with the default request datasets for each task.", "metadata": {}}, {"text": "We note that models that were\nfine-tuned based on the supported models are also supported as is, and the benchmark is designed to\nbe extensible (Section 3.4).", "metadata": {}}, {"text": "The ML.ENERGY Benchmark cannot avoid being outdated given the rapid pace of development\nin the generative AI field.", "metadata": {}}, {"text": "As such, we have been updating the benchmark (and the accompanying\nLeaderboard) with new tasks, models, datasets, hardware, runtimes, and more, and we intend to\ncontinue doing so as long as resources allow.", "metadata": {}}, {"text": "B Energy Implication of System Parameters\nThis section discusses the energy implication of different system-level configurations.", "metadata": {}}, {"text": "System-level\nconfigurations are those that do not changewhatis computed but ratherhowit is computed by the\nunderlying software system.", "metadata": {}}, {"text": "B.1 Request Preemption Mechanism\nEven with the model and inference parameters fixed, the software system used to serve inference\nrequests, which determines how model computations are executed on a given hardware, significantly\nimpacts energy consumption.", "metadata": {}}, {"text": "As a concrete example, we will examine the effect of “preemption\nmechanism,” a configuration parameter for LLM inference servers.", "metadata": {}}, {"text": "When a server is overloaded with\nmore requests than its capacity, it needs to temporarily remove (or, preempt) some requests from the\nsystem and then later bring them back (or, restore).", "metadata": {}}, {"text": "For LLM inference, there are two widely-used\nmechanisms for preemption: Recomputation and Swapping [39].", "metadata": {}}, {"text": "Recomputation simply drops all\ntemporary request data or state on preemption and recomputes everything from scratch on restoration.", "metadata": {}}, {"text": "On the other hand, Swapping moves the request state to the CPU’s memory, and then returns it to\nthe GPU on restoration.", "metadata": {}}, {"text": "The best preemption mechanism depends on the computing hardware and\nsoftware configuration and the LLM being served.", "metadata": {}}, {"text": "23", "metadata": {}}], "metadata": {"page": 23}}], "metadata": {"page": 23}}, {"title": "Page 24", "paragraphs": [{"text": "0 250 500 750 1000 1250 1500\nMaximum batch size configuration\n0\n20\n40\n60\n80\n100Energy consumption (J)\nServer is overloaded\nRecomputation\nSwapping\nFigure 8: Energy consumption per generation while varying the maximum batch size for Mistral\nNemo (12B). The LLM inference server’s preemption mechanism is compared.\n0 200 400 600 800 1000\nBatch size\n0\n100\n200\n300\n400Energy consumption (J)\n1 GPU\n2 GPUs\n4 GPUs\n8 GPUs\nFigure 9: Energy consumption per generation while varying batch size for Llama 3.1 8B. The number\nof NVIDIA A100 GPUs used to run the same model is scaled up.\nFigure 8, we compare the energy consumption per generation of the two preemption mechanisms with\nthe Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size\nconfiguration and causing preemption. It can be seen that when the server is overloaded, Swapping\nconsistently consumes less energy. This is because Recomputation performs extra computation\nwhen restoring requests whereas Swapping copies data without running computation, and the energy\nconsumption of computation is larger than memory operations (this will be further examined in\nthe next section). Furthermore, as the server gets more and more overloaded, energy consumption\ngenerally increases. This is because with higher overload, more preemptions – and thus more\nrecomputation or data movement – occur. Since preemptions do not directly contribute to the\ncompletion of the request, the extra energy consumption from preemptions increases the average\nenergy consumption of completing each request.\nB.2 Tensor Parallelism Scaling\nWe investigate the impact of communication overhead to energy consumption. This is important as\nmodern large models frequently do not fit within the memory capacity of a single GPU. This requires\nmultiple GPUs to execute inference for a single model, and GPUs must constantly communicate with\neach other to do so [67].\nIn order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary\nthe number of GPUs used (Figure 9). Because the amount of computation executed is the same\nregardless of the number of GPUs, energy consumption should ideally be constant. Indeed, energy\nconsumption barely changes when scaling from one GPU (no communication) to two, but when\nscaling further, energy consumption significantly increases. This is because, while the amount of\ncomputation decreases for each GPU, additional communication time between the GPUs offsets the\nreduction in computation time. Since communication time increases with the number of GPUs, using\ntoo many GPUs can lead to slowdowns in executing the same amount of computation and increase\nenergy consumption.\n24", "sentences": [{"text": "0 250 500 750 1000 1250 1500\nMaximum batch size configuration\n0\n20\n40\n60\n80\n100Energy consumption (J)\nServer is overloaded\nRecomputation\nSwapping\nFigure 8: Energy consumption per generation while varying the maximum batch size for Mistral\nNemo (12B).", "metadata": {}}, {"text": "The LLM inference server’s preemption mechanism is compared.", "metadata": {}}, {"text": "0 200 400 600 800 1000\nBatch size\n0\n100\n200\n300\n400Energy consumption (J)\n1 GPU\n2 GPUs\n4 GPUs\n8 GPUs\nFigure 9: Energy consumption per generation while varying batch size for Llama 3.1 8B.", "metadata": {}}, {"text": "The number\nof NVIDIA A100 GPUs used to run the same model is scaled up.", "metadata": {}}, {"text": "Figure 8, we compare the energy consumption per generation of the two preemption mechanisms with\nthe Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size\nconfiguration and causing preemption.", "metadata": {}}, {"text": "It can be seen that when the server is overloaded, Swapping\nconsistently consumes less energy.", "metadata": {}}, {"text": "This is because Recomputation performs extra computation\nwhen restoring requests whereas Swapping copies data without running computation, and the energy\nconsumption of computation is larger than memory operations (this will be further examined in\nthe next section).", "metadata": {}}, {"text": "Furthermore, as the server gets more and more overloaded, energy consumption\ngenerally increases.", "metadata": {}}, {"text": "This is because with higher overload, more preemptions – and thus more\nrecomputation or data movement – occur.", "metadata": {}}, {"text": "Since preemptions do not directly contribute to the\ncompletion of the request, the extra energy consumption from preemptions increases the average\nenergy consumption of completing each request.", "metadata": {}}, {"text": "B.2 Tensor Parallelism Scaling\nWe investigate the impact of communication overhead to energy consumption.", "metadata": {}}, {"text": "This is important as\nmodern large models frequently do not fit within the memory capacity of a single GPU.", "metadata": {}}, {"text": "This requires\nmultiple GPUs to execute inference for a single model, and GPUs must constantly communicate with\neach other to do so [67].", "metadata": {}}, {"text": "In order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary\nthe number of GPUs used (Figure 9).", "metadata": {}}, {"text": "Because the amount of computation executed is the same\nregardless of the number of GPUs, energy consumption should ideally be constant.", "metadata": {}}, {"text": "Indeed, energy\nconsumption barely changes when scaling from one GPU (no communication) to two, but when\nscaling further, energy consumption significantly increases.", "metadata": {}}, {"text": "This is because, while the amount of\ncomputation decreases for each GPU, additional communication time between the GPUs offsets the\nreduction in computation time.", "metadata": {}}, {"text": "Since communication time increases with the number of GPUs, using\ntoo many GPUs can lead to slowdowns in executing the same amount of computation and increase\nenergy consumption.", "metadata": {}}, {"text": "24", "metadata": {}}], "metadata": {"page": 24}}], "metadata": {"page": 24}}, {"title": "Page 25", "paragraphs": [{"text": "Model and deployment Request dataset\nInput mean 512\nOutput mean 512\nInput mean 512\nOutput mean 4096\nInput mean 4096\nOutput mean 512\nLlama 3.1 8B (TP=1, 1P3D) 37.71, 77.2% 665.77, 98.7% 208.34, 67.2%\nLlama 3.1 8B (TP=1, 2P2D) 36.22, 76.7% 706.27, 98.8% 151.75, 55.2%\nLlama 3.1 8B (TP=1, 3P1D) 37.26, 77.0% 748.45, 98.9% 158.85, 56.0%\nLlama 3.1 70B (TP=4, 1P1D) 276.93, 64.8% 907.60, 89.2% 1492.59, 50.0%\nTable 4: Energy per generation (Joules) and the percentage of decode energy consumption with\nPD disaggregation. Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean\nspecified in the table. TP means tensor parallelism degree, and xPyD means it was deployed with x\nprefill instances andydecode instances, each with TP-many GPUs.\nMax batch size Max batched tokens\n(sequences)32 64 128 256 512 1024 2048 4096 8192\n32 559.66 374.63 269.29 205.54 188.80 191.61 195.59 191.88 194.52\n64 362.49 266.98 200.43 168.27 165.52 170.17 168.78 169.58\n128 264.18 194.59 164.75 154.64 155.59 156.54 156.93\n256 194.39 161.87 153.97 155.11 157.13 159.25\n512 159.57 151.50 154.52 156.77 154.95\n1024 152.67 156.26 157.98 163.08\nTable 5: Energy per generation (Joules) of Llama 3.1 8B on a synthetic long context request dataset\nrunning on H100 GPUs. Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with mean 4,096 and alpha 2.5, and output lengths from an Exponential distribution with\nmean 512. Note that vLLM does not allow the max number of batched tokens to be smaller than the\nmax batch size, which is why the lower left triangle of the table is empty.\nFrom this scaling experiment, we can observe that the energy impact of communication overhead can\nbe large. This impact will be even more pronounced in hardware environments without sufficient or\nstate-of-the-art networking infrastructure, which is common in real world settings due to its cost [36].\nB.3 Prefill–Decode Disaggregation\nPrefill–decode (PD) disaggregation is a rising production deployment setting where prefill and decode\nphases are run on separate GPUs [59, 86]. This allows for independent scaling and optimization of\nprefill and decode phases based on workload characteristics, and leads to better latency deadline\nattainment. Table 4 shows energy measurements for different PD disaggregation configurations,\nwhere “xPyD” denotesxprefill instances andydecode instances.\nOverall, decode consumes the majority of energy, with some amount shifting to prefill when input\nlength is long. In our setup, PD disaggregation configurations did not have a large impact on absolute\nenergy consumption or the energy split as long as the throughput of prefill and decode instances are\nreasonably balanced.\nB.4 Chunked Prefill\nChunked prefill is a technique where long input prompts are split into chunks and processed alongside\ndecode iterations, improving GPU utilization and reducing the interference between long prefills and\ndecode iterations [5]. For chunked prefill, the max number of batched tokens is a key parameter that\ncontrols the chunk size. Table 5 shows the impact of this parameter on energy consumption.\nTable 5 shows that the more sequences or tokens you batch, the better the energy amortization you\nget and energy per generation decreases, and after a certain point, returns diminish.\n25", "sentences": [{"text": "Model and deployment Request dataset\nInput mean 512\nOutput mean 512\nInput mean 512\nOutput mean 4096\nInput mean 4096\nOutput mean 512\nLlama 3.1 8B (TP=1, 1P3D) 37.71, 77.2% 665.77, 98.7% 208.34, 67.2%\nLlama 3.1 8B (TP=1, 2P2D) 36.22, 76.7% 706.27, 98.8% 151.75, 55.2%\nLlama 3.1 8B (TP=1, 3P1D) 37.26, 77.0% 748.45, 98.9% 158.85, 56.0%\nLlama 3.1 70B (TP=4, 1P1D) 276.93, 64.8% 907.60, 89.2% 1492.59, 50.0%\nTable 4: Energy per generation (Joules) and the percentage of decode energy consumption with\nPD disaggregation.", "metadata": {}}, {"text": "Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean\nspecified in the table.", "metadata": {}}, {"text": "TP means tensor parallelism degree, and xPyD means it was deployed with x\nprefill instances andydecode instances, each with TP-many GPUs.", "metadata": {}}, {"text": "Max batch size Max batched tokens\n(sequences)32 64 128 256 512 1024 2048 4096 8192\n32 559.66 374.63 269.29 205.54 188.80 191.61 195.59 191.88 194.52\n64 362.49 266.98 200.43 168.27 165.52 170.17 168.78 169.58\n128 264.18 194.59 164.75 154.64 155.59 156.54 156.93\n256 194.39 161.87 153.97 155.11 157.13 159.25\n512 159.57 151.50 154.52 156.77 154.95\n1024 152.67 156.26 157.98 163.08\nTable 5: Energy per generation (Joules) of Llama 3.1 8B on a synthetic long context request dataset\nrunning on H100 GPUs.", "metadata": {}}, {"text": "Following recent trace analysis [79], we sampled input lengths from a Pareto\ndistribution with mean 4,096 and alpha 2.5, and output lengths from an Exponential distribution with\nmean 512.", "metadata": {}}, {"text": "Note that vLLM does not allow the max number of batched tokens to be smaller than the\nmax batch size, which is why the lower left triangle of the table is empty.", "metadata": {}}, {"text": "From this scaling experiment, we can observe that the energy impact of communication overhead can\nbe large.", "metadata": {}}, {"text": "This impact will be even more pronounced in hardware environments without sufficient or\nstate-of-the-art networking infrastructure, which is common in real world settings due to its cost [36].", "metadata": {}}, {"text": "B.3 Prefill–Decode Disaggregation\nPrefill–decode (PD) disaggregation is a rising production deployment setting where prefill and decode\nphases are run on separate GPUs [59, 86].", "metadata": {}}, {"text": "This allows for independent scaling and optimization of\nprefill and decode phases based on workload characteristics, and leads to better latency deadline\nattainment.", "metadata": {}}, {"text": "Table 4 shows energy measurements for different PD disaggregation configurations,\nwhere “xPyD” denotesxprefill instances andydecode instances.", "metadata": {}}, {"text": "Overall, decode consumes the majority of energy, with some amount shifting to prefill when input\nlength is long.", "metadata": {}}, {"text": "In our setup, PD disaggregation configurations did not have a large impact on absolute\nenergy consumption or the energy split as long as the throughput of prefill and decode instances are\nreasonably balanced.", "metadata": {}}, {"text": "B.4 Chunked Prefill\nChunked prefill is a technique where long input prompts are split into chunks and processed alongside\ndecode iterations, improving GPU utilization and reducing the interference between long prefills and\ndecode iterations [5].", "metadata": {}}, {"text": "For chunked prefill, the max number of batched tokens is a key parameter that\ncontrols the chunk size.", "metadata": {}}, {"text": "Table 5 shows the impact of this parameter on energy consumption.", "metadata": {}}, {"text": "Table 5 shows that the more sequences or tokens you batch, the better the energy amortization you\nget and energy per generation decreases, and after a certain point, returns diminish.", "metadata": {}}, {"text": "25", "metadata": {}}], "metadata": {"page": 25}}], "metadata": {"page": 25}}, {"title": "Page 26", "paragraphs": [{"text": "0 200 400 600 800 1000\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(a) Llama 3.1 8B\n0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100 (b) Llama 3.1 70B\n0 20 40 60 80 100 120\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(c) LLaV A 1.5 7B\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (d) Stable Diffusion 3 Medium\n0 10 20 30 40 50 60\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(e) Stable Diffusion 2.1\n1 2 3 4\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (f) Stable Video Diffusion\nFigure 10: Power consumption of various models on A100 and H100 GPUs.\nC Power Consumption Analysis\nFigure 10 shows the power consumption of various models on A100 and H100 GPUs. Figure 11\nfurther shows the ratio of a model’s power consumption to the maximum GPU power draw across all\nmodels. Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because\nLLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully\nutilize the GPU’s compute resources. VLMs show slightly higher power consumption than LLMs\ndue to its additional modality encoder, which is compute-intensive. Diffusion models, on the other\nhand, consume nearly the maximum power of the GPU when batch size is not small. This is because\nDiffusion models are significantly more compute-intensive compared to LLM decoding.\nFigure 12 shows the GPU power draw breakdown over time on one NVIDIA H100 GPU. The GPU’s\npower is measured (1) in whole and (2) only for the VRAM while the ML.ENERGY Benchmark\nis running. First, for Llama 3.1 8B, the timeline shows the effect of the two phases in LLM text\ngeneration: Prefill and Decode. Prefill happens once at the beginning of a request to digest the input\nprompt, which is then followed by hundreds to thousands of Decode phases, each of which generates\none output token. Importantly, Prefill has high compute-intensity (and also high power draw) because\nit needs to digest the whole input prompt, whereas Decode has low compute-intensity (and low power\ndraw) as it does not entail very much computation. With this, we can first understand the initial spike\n26", "sentences": [{"text": "0 200 400 600 800 1000\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(a) Llama 3.1 8B\n0 200 400 600 800 1000\nBatch size\n0\n1000\n2000\n3000Power draw (W)\n8 x A100 TDP (max power draw)\n4 x H100 TDP (max power draw)\n8 x A100 4 x H100 (b) Llama 3.1 70B\n0 20 40 60 80 100 120\nBatch size\n0\n200\n400\n600Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(c) LLaV A 1.5 7B\n0 5 10 15 20 25 30\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (d) Stable Diffusion 3 Medium\n0 10 20 30 40 50 60\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100\n(e) Stable Diffusion 2.1\n1 2 3 4\nBatch size\n0\n200\n400\n600\n800Power draw (W)\nA100 TDP (max power draw)\nH100 TDP (max power draw)\nA100 H100 (f) Stable Video Diffusion\nFigure 10: Power consumption of various models on A100 and H100 GPUs.", "metadata": {}}, {"text": "C Power Consumption Analysis\nFigure 10 shows the power consumption of various models on A100 and H100 GPUs.", "metadata": {}}, {"text": "Figure 11\nfurther shows the ratio of a model’s power consumption to the maximum GPU power draw across all\nmodels.", "metadata": {}}, {"text": "Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because\nLLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully\nutilize the GPU’s compute resources.", "metadata": {}}, {"text": "VLMs show slightly higher power consumption than LLMs\ndue to its additional modality encoder, which is compute-intensive.", "metadata": {}}, {"text": "Diffusion models, on the other\nhand, consume nearly the maximum power of the GPU when batch size is not small.", "metadata": {}}, {"text": "This is because\nDiffusion models are significantly more compute-intensive compared to LLM decoding.", "metadata": {}}, {"text": "Figure 12 shows the GPU power draw breakdown over time on one NVIDIA H100 GPU.", "metadata": {}}, {"text": "The GPU’s\npower is measured (1) in whole and (2) only for the VRAM while the ML.ENERGY Benchmark\nis running.", "metadata": {}}, {"text": "First, for Llama 3.1 8B, the timeline shows the effect of the two phases in LLM text\ngeneration: Prefill and Decode.", "metadata": {}}, {"text": "Prefill happens once at the beginning of a request to digest the input\nprompt, which is then followed by hundreds to thousands of Decode phases, each of which generates\none output token.", "metadata": {}}, {"text": "Importantly, Prefill has high compute-intensity (and also high power draw) because\nit needs to digest the whole input prompt, whereas Decode has low compute-intensity (and low power\ndraw) as it does not entail very much computation.", "metadata": {}}, {"text": "With this, we can first understand the initial spike\n26", "metadata": {}}], "metadata": {"page": 26}}], "metadata": {"page": 26}}, {"title": "Page 27", "paragraphs": [{"text": "Gemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (H100)\n(b) NVIDIA H100\nFigure 11: Ratio of power consumption to maximum GPU power draw across various models.\n0 20 40 60 80\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Llama 3.1 8B on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(a) Llama 3.1 8B\n0 50 100 150 200 250 300\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Stable Video Diffusion XT on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(b) Stable Video Diffusion XT\nFigure 12: GPU power draw breakdown over time on one NVIDIA H100 GPU. “Entire GPU”\nand “Only VRAM” (memory) were measured, and the two were subtracted to derive “Entire GPU\nexcluding VRAM.”\nin power draw – when the benchmark begins, the server begins admitting new requests, creating a\nshort period where numerous Prefills are executed back-to-back, leading to high power draw. After\nthe initial spike, power draw repeats a periodic fluctuation. This is because, before each Prefill or\nDecode, the server must make numerous control decisions, including determining which requests are\nnow finished and which ones should run next. Since these decisions are executed by the CPU, this\ncreates a periodic time gap where the GPU is not running any computation. This GPU idle time leads\nto the periodic drop in GPU power draw.\nOn the other hand, Stable Video Diffusion XT shows a different power draw pattern. Diffusion\nmodels generally have three phases: Encode, Denoise, and Decode. The Encode phase digests the\n27", "sentences": [{"text": "Gemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (A100)\n(a) NVIDIA A100\nGemma 2 2BGemma 2 9BGemma 2 27BLlama 3.1 8BLlama 3.1 70BLlama 3.1 405BPhi 3 mini (3.8B)Phi 3 small (7B)\nPhi 3 medium (14B)\nMistral 7B\nMistral Nemo (12B)Mistral Large (123B)Mixtral 8x7B (47B)Mixtral 8x22B (141B)\nCodeLlama 7BCodeLlama 13BCodeLlama 34BCodeLlama 70BStarCoder2 3BStarCoder2 7BStarCoder2 15BCodeGemma 2BCodeGemma 7B\nLLaVA 1.5 7BLLaVA 1.5 13BLLaVA NeXT 8B\nPhi 3 Vision\nChameleon 7BChameleon 30B\nSD2.1SDXL\nSDXL TurboSD3 medium\nSSD 1B\nOpenJourney 4ModelScope T2V\nAnimateDiffI2VGen XL\nSVD\nSVD XT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average power / max power\nMaximum power draw\nRatio of power draw to max GPU power (H100)\n(b) NVIDIA H100\nFigure 11: Ratio of power consumption to maximum GPU power draw across various models.", "metadata": {}}, {"text": "0 20 40 60 80\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Llama 3.1 8B on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(a) Llama 3.1 8B\n0 50 100 150 200 250 300\nTime (s)\n0\n100\n300\n500\n700Power draw (W)\nH100 TDP (max power draw)\nPower Draw Breakdown Over Time (Stable Video Diffusion XT on H100)\nEntire GPU Only VRAM Entire GPU excluding VRAM\n(b) Stable Video Diffusion XT\nFigure 12: GPU power draw breakdown over time on one NVIDIA H100 GPU.", "metadata": {}}, {"text": "“Entire GPU”\nand “Only VRAM” (memory) were measured, and the two were subtracted to derive “Entire GPU\nexcluding VRAM.”\nin power draw – when the benchmark begins, the server begins admitting new requests, creating a\nshort period where numerous Prefills are executed back-to-back, leading to high power draw.", "metadata": {}}, {"text": "After\nthe initial spike, power draw repeats a periodic fluctuation.", "metadata": {}}, {"text": "This is because, before each Prefill or\nDecode, the server must make numerous control decisions, including determining which requests are\nnow finished and which ones should run next.", "metadata": {}}, {"text": "Since these decisions are executed by the CPU, this\ncreates a periodic time gap where the GPU is not running any computation.", "metadata": {}}, {"text": "This GPU idle time leads\nto the periodic drop in GPU power draw.", "metadata": {}}, {"text": "On the other hand, Stable Video Diffusion XT shows a different power draw pattern.", "metadata": {}}, {"text": "Diffusion\nmodels generally have three phases: Encode, Denoise, and Decode.", "metadata": {}}, {"text": "The Encode phase digests the\n27", "metadata": {}}], "metadata": {"page": 27}}], "metadata": {"page": 27}}, {"title": "Page 28", "paragraphs": [{"text": "input prompt and passes it to the Denoise phase, which iteratively removes noise from a random\nvector. Finally, the Decode phase transforms the denoised vector into the final image or video.\nFrom the timeline, especially Denoise and Decode can be clearly distinguished. Denoise is the most\ncompute-intensive and consumes power close to the GPU’s TDP. For each batch, there are 25 local\npeaks that hit the GPU’s TDP, each of which corresponds to one denoising step in Denoise. During\nDecode, power draw generally decreases, with each local power peak corresponding to the two large\nlayers in the decoding module. On the other hand, VRAM power draw increases during Decode\nbecause it allocates a large chunk of memory and performs writes in order to generate the final video.\nFinally, as the final generated video is copied from the GPU’s memory to the CPU’s, the GPU does\nnot run any computation, resulting in a steep drop in power draw.\nFrom the power breakdown, we can observe that memory operations indeed draw significantly less\npower compared to computation, and thus computations with low compute-intensity should indeed\ndraw less power. Furthermore, we can observe that the power draw and energy consumption of a\nspecific hardware (GPU in this case) is not a function of just itself and the computations that it runs.\nRather, software and hardware components that are integrated in the same system stack impacts how\ncomputations are executed on each other, affecting their power draw and energy consumption.\nD The ML.ENERGY Leaderboard and Benchmark\nOn July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy\nleaderboard for modern generative AI models.6 Our goal was to measure and understand the energy\nconsumption of generative AI models, and we provided a web-based leaderboard to allow everyone\nto browse the results. The leaderboard started with only LLM chat with tens of different LLMs, but\ngradually expanded to include more tasks, models, and datasets. Our benchmarking suite to supply\ndata to the leaderboard is what we dub the ML.ENERGY Benchmark. This paper shares our design\nphilosophy and principles we have acquired over time by gradually maintaining and upgrading the\nML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained\nfrom the early 2025 iteration of the benchmark. Importantly, we plan to continuously update the\nbenchmark and the leaderboard as long as resources allow, and what is presented in this paper is only\na snapshot of the current state of the benchmark at the time of writing. We encourage readers to visit\nthe leaderboard website and benchmark repository for the latest results and updates.\nE Limitations\nThe ML.ENERGY Benchmark is not without limitations. First, we note that the benchmark is not\nexhaustive and does not cover all possible tasks, models, and datasets. This is particularly true as\ntime passes and new models and tasks are developed. We are aware of newer open-weight models\nand worthy tasks that were released after the early 2025 iteration of the benchmark was finalized.\nHowever, we cannot add each model or task one by one incrementally as they are released, due to the\nprohibitive monetary cost of running the benchmark on representative hardware; rather, we collect\nnew advances in a window of time and then mass-update the whole benchmark, accompanied by\nupgrades in hardware, software, and datasets. Second, the benchmark is not exhaustive in terms\nof hardware. We currently mainly support flagship NVIDIA GPUs, which arguably dominates the\nmarket especially when it comes to real-world generative AI services. Furthermore, we do not have\naccess to all possible hardware configurations, nor do they always provide a way for us to measure\nenergy consumption from software. Regardless, we are working to expand the benchmark to support\nmore hardware configurations.\nF Broader Impacts\nBy allowing everyone to accurate measure, understand, and optimize the energy consumption of\ngenerative AI models, we believe the ML.ENERGY Benchmark can enhance the understanding of\nenergy consumption of generative AI in the research community and the industry, and ultimately fuel\nworks that optimize energy consumption. Furthermore, energy is essentially throughput per watt,\n6https://github.com/ml-energy/leaderboard/releases/tag/2023-07-06\n28", "sentences": [{"text": "input prompt and passes it to the Denoise phase, which iteratively removes noise from a random\nvector.", "metadata": {}}, {"text": "Finally, the Decode phase transforms the denoised vector into the final image or video.", "metadata": {}}, {"text": "From the timeline, especially Denoise and Decode can be clearly distinguished.", "metadata": {}}, {"text": "Denoise is the most\ncompute-intensive and consumes power close to the GPU’s TDP.", "metadata": {}}, {"text": "For each batch, there are 25 local\npeaks that hit the GPU’s TDP, each of which corresponds to one denoising step in Denoise.", "metadata": {}}, {"text": "During\nDecode, power draw generally decreases, with each local power peak corresponding to the two large\nlayers in the decoding module.", "metadata": {}}, {"text": "On the other hand, VRAM power draw increases during Decode\nbecause it allocates a large chunk of memory and performs writes in order to generate the final video.", "metadata": {}}, {"text": "Finally, as the final generated video is copied from the GPU’s memory to the CPU’s, the GPU does\nnot run any computation, resulting in a steep drop in power draw.", "metadata": {}}, {"text": "From the power breakdown, we can observe that memory operations indeed draw significantly less\npower compared to computation, and thus computations with low compute-intensity should indeed\ndraw less power.", "metadata": {}}, {"text": "Furthermore, we can observe that the power draw and energy consumption of a\nspecific hardware (GPU in this case) is not a function of just itself and the computations that it runs.", "metadata": {}}, {"text": "Rather, software and hardware components that are integrated in the same system stack impacts how\ncomputations are executed on each other, affecting their power draw and energy consumption.", "metadata": {}}, {"text": "D The ML.ENERGY Leaderboard and Benchmark\nOn July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy\nleaderboard for modern generative AI models.6 Our goal was to measure and understand the energy\nconsumption of generative AI models, and we provided a web-based leaderboard to allow everyone\nto browse the results.", "metadata": {}}, {"text": "The leaderboard started with only LLM chat with tens of different LLMs, but\ngradually expanded to include more tasks, models, and datasets.", "metadata": {}}, {"text": "Our benchmarking suite to supply\ndata to the leaderboard is what we dub the ML.ENERGY Benchmark.", "metadata": {}}, {"text": "This paper shares our design\nphilosophy and principles we have acquired over time by gradually maintaining and upgrading the\nML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained\nfrom the early 2025 iteration of the benchmark.", "metadata": {}}, {"text": "Importantly, we plan to continuously update the\nbenchmark and the leaderboard as long as resources allow, and what is presented in this paper is only\na snapshot of the current state of the benchmark at the time of writing.", "metadata": {}}, {"text": "We encourage readers to visit\nthe leaderboard website and benchmark repository for the latest results and updates.", "metadata": {}}, {"text": "E Limitations\nThe ML.ENERGY Benchmark is not without limitations.", "metadata": {}}, {"text": "First, we note that the benchmark is not\nexhaustive and does not cover all possible tasks, models, and datasets.", "metadata": {}}, {"text": "This is particularly true as\ntime passes and new models and tasks are developed.", "metadata": {}}, {"text": "We are aware of newer open-weight models\nand worthy tasks that were released after the early 2025 iteration of the benchmark was finalized.", "metadata": {}}, {"text": "However, we cannot add each model or task one by one incrementally as they are released, due to the\nprohibitive monetary cost of running the benchmark on representative hardware;", "metadata": {}}, {"text": "rather, we collect\nnew advances in a window of time and then mass-update the whole benchmark, accompanied by\nupgrades in hardware, software, and datasets.", "metadata": {}}, {"text": "Second, the benchmark is not exhaustive in terms\nof hardware.", "metadata": {}}, {"text": "We currently mainly support flagship NVIDIA GPUs, which arguably dominates the\nmarket especially when it comes to real-world generative AI services.", "metadata": {}}, {"text": "Furthermore, we do not have\naccess to all possible hardware configurations, nor do they always provide a way for us to measure\nenergy consumption from software.", "metadata": {}}, {"text": "Regardless, we are working to expand the benchmark to support\nmore hardware configurations.", "metadata": {}}, {"text": "F Broader Impacts\nBy allowing everyone to accurate measure, understand, and optimize the energy consumption of\ngenerative AI models, we believe the ML.ENERGY Benchmark can enhance the understanding of\nenergy consumption of generative AI in the research community and the industry, and ultimately fuel\nworks that optimize energy consumption.", "metadata": {}}, {"text": "Furthermore, energy is essentially throughput per watt,\n6https://github.com/ml-energy/leaderboard/releases/tag/2023-07-06\n28", "metadata": {}}], "metadata": {"page": 28}}], "metadata": {"page": 28}}, {"title": "Page 29", "paragraphs": [{"text": "which is one factor that determines the cost of running generative AI services at the infrastructure\nlevel. By optimizing energy consumption, we can reduce the cost of running generative AI services,\nwhich can help democratize access to generative AI.\n29", "sentences": [{"text": "which is one factor that determines the cost of running generative AI services at the infrastructure\nlevel.", "metadata": {}}, {"text": "By optimizing energy consumption, we can reduce the cost of running generative AI services,\nwhich can help democratize access to generative AI.", "metadata": {}}, {"text": "29", "metadata": {}}], "metadata": {"page": 29}}], "metadata": {"page": 29}}]}