{"document_id": "kim2025", "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading", "text": "Cost-Efficient LLM Serving in the Cloud: VM\nSelection with KV Cache Offloading\nKihyun Kim 1, Jinwoo Kim 1, Hyunsun Chung 1, Myung-Hoon Cha 2, Hong-Yeon Kim 2, Youngjae Kim 1,†\n1Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea\n2ETRI, Daejeon, Republic of Korea\nAbstract—LLM inference is essential for applications like\ntext summarization, translation, and data analysis, but the\nhigh cost of GPU instances from Cloud Service Providers\n(CSPs) like A WS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud-\nbased LLM inference. InferSave optimizes KV cache offloading\nbased on Service Level Objectives (SLOs) and workload charac-\nteristics, estimating GPU memory needs, and recommending\ncost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection\naccuracy by adjusting for discrepancies between theoretical\nand actual GPU performance. Experiments on A WS GPU\ninstances show that selecting lower-cost instances without\nKV cache offloading improves cost efficiency by up to 73.7%\nfor online workloads, while KV cache offloading saves up to\n20.19% for offline workloads.\nIndex Terms —Cloud Computing, LLM Inference Tasks, Ser-\nvice Level Objective (SLO) Management, KV Cache Offloading\nI. Introduction\nLarge Language Models (LLMs) have become a core tech-\nnology in modern Natural Language Processing (NLP), demon-\nstrating outstanding performance in various applications such\nas text summarization, machine translation, and conversa-\ntional AI [ 1]. LLMs built on Transformer-based architectures,\nsuch as GPT [ 2] and LLaMA [ 3], leverage multi-layer self-\nattention mechanisms and large-scale pretraining to achieve\nnear-human-level language understanding and generation\ncapabilities. Thanks to their superior performance, LLMs are\nwidely used across industries, providing high accuracy and\nnatural responses in a wide range of tasks, including text\nsummarization, question answering, and document analysis.\nHowever, to efficiently design an LLM inference system, it\nis essential to consider task-specific Service Level Objectives\n(SLOs). For instance, in online inference tasks, such as real-\ntime conversational services or question answering, latency\nmust be minimized to ensure a seamless user experience.\nReducing inference latency is a key challenge in these\nscenarios.\nOn the other hand, in batch processing tasks [ 4, 5] such\nas text summarization for large datasets, log analysis, and\ndocument clustering, latency requirements are generally less\nstrict. Instead, maximizing throughput is critical, as these tasks\ninvolve processing large volumes of input data at once. In\nsuch batch processing environments, handling large batches\n†Y. Kim is the corresponding author.\ncan easily lead to GPU memory shortages. Due to the auto-\nregressive nature of LLM inference, the Key-Value (KV) cache,\nwhich stores past token information, continuously grows. As\na result, GPU memory usage increases sharply with sequence\nlength and batch size.\nA common technique to mitigate this issue is KV Cache\nOffloading, which offloads KV cache data exceeding GPU\nmemory limits to CPU memory or disk. This enables large-\nbatch processing without running out of memory [ 6, 7, 8, 9].\nHowever, if the additional latency introduced by offloading is\nnot properly managed, throughput can significantly degrade,\npotentially failing to meet the required SLOs.\nCost Efficiency of LLM Inference in Cloud Environ-\nments: Major cloud service providers such as AWS, GCP, and\nAzure offer a variety of GPU instance options with different\nperformance levels and cost structures, providing flexibility\nin resource utilization [ 10]. However, selecting a cost-efficient\nGPU instance in a cloud environment is a complex task\nthat is difficult for users to perform manually. The challenge\narises because GPU instances vary significantly in price and\nperformance (Refer to Table I), and workload characteristics\nrequire flexible KV cache offloading strategies, making optimal\nselection difficult.\nGiven this complexity, an optimized approach must inte-\ngrate the following two key factors:\n• GPU instance selection based on task characteristics\n• Efficient KV Cache Offloading strategy\nBalancing throughput targets and cost efficiency by combin-\ning these two factors remains a critical challenge that needs\nto be addressed.\nLimitations of Existing Research: Previous studies on\ncost efficiency in cloud environments [ 11, 12, 13, 14, 15] have\nfocused primarily on image processing or general machine\nlearning workloads. As a result, they do not capture the\nunique characteristics of large-scale LLM inference. Moreover,\nrecent research on cost-efficient LLM inference has largely\nconcentrated on real-time inference scenarios [ 16, 17, 18,\n19, 20], neglecting large-scale data processing environments\nwhere KV cache offloading could be leveraged effectively.\nFurthermore, these studies do not comprehensively analyze\ncost efficiency in relation to Service Level Objectives (SLOs).\nTo address these challenges, this paper proposes InferSave,\na software framework that automatically selects the optimal\nVM instance by considering both cost and performance based\non SLOs.\narXiv:2504.11816v1  [cs.LG]  16 Apr 2025\n\nThe InferSave framework operates as follows based on\nuser input: First, It calculates the required GPU memory\nbased on the specified SLO and workload size, analyzing\nthe feasibility of KV cache offloading to determine a set of\ncandidate instances. Next, using pre-collected performance\ndata, it performs a modeling step to predict the performance\nand cost of each instance. Finally, it evaluates these predictions\nto recommend the most cost-efficient instance that meets the\nuser’s SLO constraints.Through this process, the InferSave\nframework becomes the first solver system that automatically\nrecommends the most economical VM instance for LLM\nserving in cloud environments. By integrating KV cache\noffloading and GPU instance characteristics, it ensures SLO\ncompliance while optimizing costs.\nThe InferSave framework analyzes GPU instance perfor-\nmance based on user input and comprehensively considers\nthe feasibility of KV cache offloading to automatically recom-\nmend the optimal VM instance for LLM inference in cloud\nenvironments. By leveraging InferSave, users can easily find\nthe most cost-effective VM instance that meets their specified\nSLO while minimizing operational expenses.\nExperimental results show that applying InferSave\nachieves significant cost savings compared to traditional\nmaximum-performance-based policies, with reductions of up\nto 73.7% for online workloads and 20.19% for offline workloads.\nIn addition, it is designed to be flexible across various AWS\ninstances and cloud environments, providing a practical and\nefficient approach to operating LLM inference services.\nII. Background and Motivation\nA. LLM Architecutre and Inference\nLarge-scale language models (LLMs), such as OpenAI’s\nGPT [ 2] and Meta’s LLaMA [ 3], are built on the Trans-\nformer [1] architecture. These models consist of a multi-layer\nstructure incorporating Self-Attention mechanisms and Feed-\nForward Networks, enabling their broad applicability across\nvarious natural language processing (NLP) tasks.\nThe LLM inference process is divided into two stages: Prefill\nand Decode. In the Prefill stage, the input prompt is processed\nin parallel to generate the initial output tokens. During\nthis process, Query, Key, and Value vectors are computed\nfor each token in the input prompt, capturing contextual\ninformation through token-wise interactions. Simultaneously,\nthe computed Key and Value tensors are stored in the\nGPU memory as a Key-Value Cache (KV Cache) to alleviate\ncomputational overhead in subsequent operations.\nThe KV Cache is essential for preventing redundant\ncomputations in Self-Attention, thereby enhancing inference\nspeed and resource efficiency. For instance, if the Prefill stage\ncomputes and stores the Key and Value tensors for the input\n\"I am a, \" the Decode stage can reuse them to rapidly generate\nthe next token, \"man, \" without redundant computations.\nIn the Decode stage, new tokens are sequentially generated\nin an Auto-Regressive manner based on previously generated\noutput tokens. Here, the stored KV Cache is reused to\nreduce the computational burden of repeated Self-Attention\noperations and improve processing speed. However, the size\nof the KV Cache increases significantly with the input length\nand model size.\nFor example, as shown in Figure 1, in the OPT_2.7B model\nrunning on an AWS g4dn.xlarge instance with 1024 input\ntokens, the KV Cache consumes approximately 0.332GB at a\nbatch size of 2. When the batch size increases to 32, the\nKV Cache expands to 5.312GB, which can lead to GPU\nmemory exhaustion. This memory constraint may degrade\noverall system throughput and reduce resource utilization\nefficiency [1, 21].\nB. Memory Optimization for LLM Inference via KV Cache\nOffloading\nDuring LLM inference, the increasing size of the KV Cache\ncan lead to GPU memory exhaustion, resulting in an Out-of-\nMemory (OoM) issue. To address this, KV Cache Offloading\ntechniques have been proposed [ 6, 7, 8, 9]. These techniques\noperate by offloading KV Cache data that exceeds GPU\nmemory capacity to CPU memory or disk and retrieving\nit back to the GPU when needed for computation. This\napproach effectively alleviates the GPU memory pressure,\nenabling the processing of long sequences and large batch\nsizes. Additionally, it allows efficient inference on lower-end\nGPUs without requiring additional high-performance GPUs,\nthus reducing deployment costs.\nHowever, latency introduced by data transfer between the\nGPU and external storage (e.g., CPU memory or disk) is\na major limitation of KV Cache Offloading. If the transfer\nfrequency of KV Cache data is high, the increased latency\ncan lead to bandwidth bottlenecks, ultimately degrading\ninference performance. Therefore, for effective deployment of\nKV Cache Offloading, it is essential to optimize the process\nby considering LLM inference characteristics (e.g., sequence\nlength, batch size) and user-defined Service Level Objectives\n(SLOs), such as maximum allowable response time.\nC. Challenges of LLM Inference and KV Cache Offloading in\nthe Cloud\nCloud service providers (CSPs) such as Amazon AWS offer\na variety of GPU virtual machine (VM) instances. As shown\nin Table I, the price of these instances varies significantly,\nranging from $0.379 (g4ad.xlarge) to $40.96 (p4de.24xlarge),\ndepending on the type of GPU, the memory capacity, and the\nbandwidth of the network [22].\nMoreover, when applying KV Cache Offloading to LLM\ninference, the trade-off between inference performance and\nactual cost introduces a complex dilemma. To maximize cost-\nefficiency, users must carefully optimize their choice of VM\nand offloading strategy based on: (i) Model size, (ii) Sequence\nlength, and (iii) Service Level Objectives (SLOs), such as\nmaximum response time.\nHowever, a systematic framework for making these deci-\nsions is currently lacking. As a result, users must experiment\nwith multiple VM options and offloading policies manually to\n\nTABLE I\nV arious Types of instances provided by A WS.\nThis information was available on Feburary 4, 2025 in\nN.Virginia region.\nName GPU On- GPU FLOPS vCPU GPU Mem Mem Network\nType Demand ($) (#) (TFLOPS) (GiB) (GiB) (Gbps) (Gbps)\ng4dn.xlarge T4 0.526 1 8.141 4 16 16 - 25\ng4ad.xlargeV520 Pro 0.379 1 7.373 4 8 16 - 10\ng5.xlarge A10G 1.006 1 31.52 4 24 16 - 10\ng5g.xlarge T4G 0.42 1 8.141 4 16 8 - 10\ng6.xlarge L4 0.805 1 30.29 4 24 16 - 10\ng6.4xlarge L4 1.323 1 30.29 16 24 64 - 25\ng4dn.12xlarge T4 3.912 4 8.141 48 64 192 50\ng4dn.metal T4 7.824 8 8.141 96 128 384 100\ng4ad.16xlargeV520 Pro 3.468 4 7.373 64 32 256 25\ng5.12xlarge A10G 5.672 4 31.52 96 96 192 40\ng5g.16xlarge T4G 2.744 2 8.141 64 32 128 25\ng6.12xlarge L4 4.602 4 30.29 48 96 192 40\ng6.48xlarge L4 13.35 8 30.29 192 196 768 100\np4de.24xlargeA100 40.96 96 19.49 96 7680 640 400\ndetermine an optimal configuration, which adds significant\noverhead [6, 9].\nIn this paper, we outline the key dilemmas of KV Cache\nOffloading for LLM inference in the cloud as follows.\n• Dual Nature of KV Cache Offloading: KV Cache\nOffloading mitigates GPU memory shortage issues, allowing\nfor the processing of larger batch sizes (e.g., greater than\n16). However, it increases latency due to data transfer\nbetween CPU and GPU (e.g., up to 20% latency increase\nin FlexGen [ 6]). Specifically, when the sequence length\nexceeds 4096, the KV Cache size grows significantly\n(e.g., exceeding 3.2GB), making offloading essential. This,\nhowever, increases the likelihood of failing to meet Service\nLevel Objectives (SLOs) such as a 100ms response time.\n• Complexity of Cloud VM Selection: As shown in\nTable I, the performance and cost between instances like\ng4dn.xlarge ($0.526, 16 GiB GPU Memory) and p4de.24xlarge\n($40.96, 7680 GiB GPU Memory) vary significantly. The\noptimal VM selection depends on the model requirements\n(e.g., memory usage, computation speed). High-performance\nVMs reduce the need for KV Cache Offloading, while lower-\nend VMs increase reliance on offloading.\n• Difficulty of SLO-Based Optimization: High-\nperformance VMs (e.g., g6.48xlarge) solve the Out-\nof-Memory (OoM) problem but may lead to GPU\nutilization dropping below 50% when the inference load\nis low, resulting in wasted costs. On the other hand,\nlower-end VMs (e.g., g4ad.xlarge) have lower initial costs\nbut suffer from frequent KV Cache Offloading due to\nVRAM limitations, causing latency to increase by more\nthan double [ 9]. This results in a dilemma of (i) resource\nwastage with high-cost VM selection, and (ii) performance\ndegradation with low-cost VM selection.\n• Lack of Automated Optimization Systems: Currently,\nthere is a lack of guidelines for automating the selection of\nVMs and KV Cache Offloading in cloud environments. Users\nmust manually test various VMs (e.g., g5 vs. g6 series) and\noffloading settings, which increases time and cost burdens.\nThis study proposes the necessity of a framework that auto-\nmatically recommends optimal VM and KV Cache Offloading\nstrategies based on SLO, and introduces a model (Solver) that\ncan balance cost and performance.\n(b)\tg4dn.xlarge\twith\tOPT2.7B\tmodel\n(a)\tg4dn.xlarge\twith\tOPT1.3B\tmodel\nKV\tCache\tSize\nKV\tCache\tSize\t(GB)\n0\n1\n2\n3\n4\n5\n6\nBatch\tSize\n2\n4\n8\n16\n32\nKV\tCache\tSize\n0\n2\n4\n6\n8\nBatch\tSize\n2\n4\n8\n16\n32\nFig. 1. Analysis of KV Cache size growth across different models in response\nto increasing batch sizes.\nD. Existing Approaches and Their Limitations\nExisting research aiming to optimize LLM inference in\ncloud environments [ 16, 17, 18, 19] reveals limitations in\nachieving cost-efficient LLM serving as they do not consider\nan integrated approach to KV Cache Offloading and VM\nselection.\nMelange [16] proposes an allocation strategy that minimizes\ncost by mixing different GPU types (e.g., high-performance\nGPUs and low-cost GPUs) based on LLM service charac-\nteristics such as request size, frequency, and Service Level\nObjectives (SLOs, e.g., response time within 200ms). However,\nthis method relies on profiling GPU performance and the\nworkload pattern in advance, making it difficult to apply\nto new environments or models. Furthermore, it does not\naccount for KV Cache Offloading, failing to provide optimiza-\ntion solutions in memory-constrained scenarios (e.g., when\nsequence length exceeds 4096).\nAladdin [17] suggests a framework for jointly optimizing\nrequest batches and resource scaling to meet SLOs. For\ninstance, it adds additional GPUs to reduce latency at high\nrequest rates. However, it does not integrate the memory-\nsaving effects of KV Cache Offloading or the trade-offs\nbetween different GPU types, which limits the flexibility in\nVM configuration.\nSplitWise [ 19] and ThunderServe [ 18] utilize a Phase\nSplitting strategy, separating the Prefill (initial token gen-\neration) and Decode (subsequent token generation) stages.\nThese approaches allocate specialized GPUs to each stage\n(e.g., high-speed GPUs for Prefill and memory-centric GPUs\nfor Decode) to enhance efficiency. However, this method is\nonly effective in environments where the two stages can be\nphysically separated, making it difficult to apply to standard\nsingle-VM-based LLM serving. Additionally, transferring KV\nCache between stages requires high-speed interconnects (e.g.,\nNVLink, with GPU-to-GPU bandwidth above 100GB/s), which\nreduces practicality in cloud VMs without NVLink (e.g., AWS\ng4dn series).\nMeanwhile, DeepVM [ 12], which deals with deep learning\noptimization in cloud environments, focuses on optimizing\nVM clusters for checkpoint-based distributed CNN training.\nFor example, it reduces costs by leveraging saved states during\ntraining interruptions. However, this method is tailored for\n\ntraining and is not directly applicable to real-time inference\nor KV Cache management in LLM serving.\nIII. Problem Definition\nA. Definition of Service Level Objective (SLO) Metrics\nIn cloud environments, large language model (LLM) in-\nference involves a complex trade-off between memory con-\nstraints, cost, and service quality. Depending on the type\nof inference task, users may have different Service Level\nObjectives (SLOs).\nIn this paper, we define two types of inference tasks: Online\nInference and Offline Inference.\n• Online Inference (e.g., chatbots, voice assistants) pri-\noritizes low response latency (e.g., within 100ms) over\nquery throughput, as real-time responsiveness is crucial.\nThus, response time is used as the primary SLO metric.\n• Offline Inference (e.g., batch processing of large\ndatasets) prioritizes high query throughput over response\nlatency, making throughput the primary SLO metric.\nTo encompass both of these metrics under a unified\nframework, we define Tokens Per Second (TPS) as the SLO\nmetric. TPS represents the number of tokens processed per\nsecond, including both input tokens ( Lin) and output tokens\n(Lout).\nLLM inference is typically performed in batches, where a\nbatch consists of multiple queries ( BS). Given that the total\nprocessing time for a batch is denoted as TE2E, TPS is defined\nas follows:\nTPS = BS × (Lin + Lout)\nTE2E\n(1)\nB. Definition of Cost Efficiency\nIn this study, our primary objective is to minimize user\ncosts while ensuring that inference tasks meet their designated\nSLOs. To achieve this, we define a cost efficiency metric based\non the previously introduced Tokens Per Second (TPS) metric.\nLet TPSSLO denote the target TPS required by the user\nto meet the SLO, and let TPSactual represent the actual\nthroughput achieved during inference. Considering that the\neffective processing rate cannot exceed the user-defined\nSLO threshold, the effective TPS is defined as: TPSeffective =\nmin(TPSactual, TPSSLO)\nGiven this, the total time required to process a batch of\nqueries, denoted as Ttask, is calculated as:\nTtask = BS × (Lin + Lout)\nTPSeffective × 3600 (2)\nIn cloud environments, GPU usage is typically billed on an\nhourly basis. Therefore, we apply a ceiling function to Ttask\nto account for the actual billable time.\nBased on this, we define SLO-based cost efficiency (CE) as\na metric to evaluate the cost-effectiveness of a given inference\ntask while ensuring compliance with the SLO. Let VM Price\nrepresent the hourly cost of the virtual machine (in dollars\nper hour). The cost efficiency metric is then defined as:\nCEtask = TPSeffective × 3600\n⌈Ttask⌉ × VM Price (3)\nThis metric provides a quantitative measure of how effi-\nciently a system meets the required SLO while optimizing\ncosts in a cloud-based inference environment.\nC. Preliminary Results\nAs shown in Table I in Section II, cloud VM instances exhibit\nsignificant differences in both performance and cost. This\nvariability makes it challenging for users to select the most\ncost-efficient instance for LLM inference tasks. To validate\nthe complexity of this decision-making process, we evaluated\nthe Cost Efficiency (CE) of two representative VM instances\n(g4dn.xlarge and g5.xlarge) under different batch sizes and\nSLO requirements. The experiments were conducted for both\ncases: with and without KV Cache offloading, assessing its\nimpact on cost efficiency. The results of these experiments\nare quantitatively presented in Fig. 2.\nIn a strict SLO environment (100 TPS) , g5.xlarge\ndemonstrated higher cost efficiency than g4dn.xlarge even\nat small batch sizes (Batch Size < 16). This is because\ng5.xlarge delivers higher performance under high-throughput\nrequirements, allowing it to maintain superior cost efficiency\nover g4dn.xlarge even at smaller batch sizes. At Batch Size 16,\ng4dn.xlarge faced GPU memory constraints, necessitating KV\nCache offloading, which further reduced its cost efficiency. In\ncontrast, g5.xlarge had sufficient memory to operate without\noffloading, maintaining consistently high cost efficiency as\nthe batch size increased.\nIn a relaxed SLO environment (10 TPS) , g4dn.xlarge\nexhibited higher cost efficiency than g5.xlarge at smaller batch\nsizes (Batch Size < 16). This is because, under relaxed SLO\nconditions, instance cost became a more critical factor than\nraw performance. At Batch Size 16, despite g4dn.xlarge re-\nquiring KV Cache offloading due to GPU memory limitations,\nthe performance degradation caused by offloading was not a\nmajor issue under the relaxed SLO constraints. As a result,\ng4dn.xlarge, with its lower instance cost, achieved higher cost\nefficiency compared to g5.xlarge.\nTo sum up, cost efficiency varies significantly depending\non SLO settings and GPU memory utilization strategies,\ndemonstrating that using a high-performance GPU is not\nalways the optimal choice. Particularly in offline inference\ntasks, where response time constraints are less stringent, KV\nCache offloading techniques can enable cost-efficient inference\neven on lower-cost GPUs. These findings highlight that the\noptimal GPU instance selection depends on the user’s SLO\nrequirements and the characteristics of the inference task.\nIV. Design of InferSave\nA. InferSave: A Cost-Efficient VM Selection Framework\nSelecting a cost-efficient VM instance in a cloud environ-\nment is a challenging task for users. To address this issue,\nwe propose InferSave, a software tool designed to assist\n\nOffloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(100\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\n2\n2.5\nBatch\tSize\n1\n2\n4\n8\n16\nOffloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(10\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\nBatch\tSize\n1\n2\n4\n8\n16\nFig. 2. Comparison of cost efficiency per GPU instance based on SLO\nconstraints and batch size, based on experimental results using the OPT-2.7B\nmodel on an AWS g4dn.xlarge instance with an input length of 512 tokens\nand an output length of 128 tokens.\nTABLE II\nNotation and Formulas for Model and Memory Computation\nUser Input Parameters\nVariable Description and Formula\nBS Batch size\nLin Input token length\nLout Output token length\nPmax User max price willingness\nTPSSLO User SLO Requirement\nModel Parameters\nVariable Description and Formula\nh1 Hidden Size (model dimension)\nh2 Intermediate Size (projection)\nnh Number of Attention Heads\nL Transformer layers\nCoff KV cache offloading ratio\nPrecisionbytes Bytes per parameter (e.g., FP16 = 2B)\nMemmodel(Model Size) Number of Model Parameters·Precisionbytes\nMemKVcache(KV Cache Size) 2· BS· (Lin +Lout) · nh· Precisionbytes· L\nMemKVcache, per_layer KV Cache per layer:MemKVcache\nL\nMemactivation(Activation Size) 2· (Lin +Lout) · BS· h1\nInstance Specifications\nVariable Description and Formula\nFLOPSGPU GPU’s theoretical FLOPS\nBWgpu→cpu Bandwidth for GPU-to-CPU data transfer\nBWcpu→gpu Bandwidth for CPU-to-GPU data transfer\nusers in making cost-efficient VM selections. The InferSave\nframework operates in the following four stages:\n1) Stage 1 Requirement Analysis and Parameter Ex-\ntraction: The user provides input parameters, including\ncost constraints, model characteristics, and performance\nrequirements.\n2) Stage 2 Resource Suitability Assessment and Can-\ndidate Instance Identification : Based on the provided\nparameters, the framework calculates the required memory\ncapacity, analyzes the feasibility of KV Cache offloading,\nand identifies a set of suitable GPU instance candidates.\n3) Stage 3 Performance-Cost Prediction Modeling : Lever-\naging pre-profiled performance data, the framework pre-\ndicts the TPS of each candidate GPU instance and evaluates\nits cost efficiency.\n4) Stage 4 SLO-Based Optimization and Instance Selec-\ntion: The framework recommends the most cost-efficient\nGPU instance that satisfies the SLO constraints.\nB. Requirement Analysis and Parameter Extraction\nThis stage involves collecting key input parameters nec-\nessary for LLM inference tasks. The most critical parameter\nis the maximum willingness-to-pay price ( Pmax), which\nrepresents the maximum cost ($/hour) that the user is willing\nto pay. This value serves as a fundamental constraint in the\nsubsequent stages of the algorithm, determining the range of\nGPU instances that can be considered.\nAdditionally, the user specifies the target LLM model\n(e.g., OPT-2.7B, LLaMA-7B), and based on this selection, the\nsystem automatically extracts key model parameters such\nas model size, number of attention heads, head dimensions,\nfeed-forward network (FFN) dimensions, and activation size.\nOther essential input parameters include the average input\ntoken length, average output token length, batch size, and\nthe required SLO in terms of TPS (Tokens Per Second).\nThis stage plays a crucial role in transforming user\nrequirements into quantitative parameters, establishing the\nfoundation for resource suitability assessment and perfor-\nmance prediction. Ultimately, it is essential for selecting the\nmost cost-efficient GPU instance that meets both performance\nobjectives and budget constraints.\nC. Resource Suitability Assessment and Candidate Instance\nIdentification\nAt this stage, the system evaluates the memory require-\nments for inference based on the collected user parameters\nand assesses the feasibility of KV Cache Offloading to identify\nthe most suitable GPU instance candidates. First, the system\ncalculates the total memory requirement Memtotal for the\ngiven Transformer-based LLM model and its input-output\nparameters. This is defined as the sum of the following\nthree components: Memtotal = Memmodel + Memactivation +\nMemKVcache. Additionally, the base memory requirement is\ndefined as: Membase = Memmodel +Memactivation. These stages\nfollow three key criteria to evaluate GPU instance suitability\nand Algorithm 1.\nCase1) No Offloading Required: If the available GPU\nmemory is greater than or equal to the total memory re-\nquirement, i.e., GPUimemory ≥ Memtotal then the instance can\nfully accommodate the model without KV Cache Offloading.\nHere, i refers to the current particular running instance. In\nthis case, the offloading coefficient is set to Ci\noff = 0 and the\ninstance is added to the candidate pool.\nCase2) Offloading Not Feasible: An instance is deemed\nunsuitable if it meets any of the following conditions:\n• If the available GPU memory is smaller than the model\nweights: GPU imemory < Memmodel.\n• If the KV Cache size per layer exceeds the available\nmemory: Mem KVcache, per_layer > Memi\navail.\nThis condition arises because attention operations are per-\nformed on the GPU, requiring KV Cache to remain in GPU\nmemory. When the available memory is insufficient, an Out\nof Memory (OOM) error occurs, preventing execution.\nCase3) KV Cache Offloading Required: If an instance\ndoes not fall into either of the previous categories, KV Cache\nOffloading is required. In this case, the offloading coefficient\nis computed as: Ci\noff = 1 – Memi\navail\nMemKVcache\nFinally, the selected instances are sorted in ascending order\nbased on cost, and the results are used as input for the\n\nAlgorithm 1: Resource Suitability Evaluation and\nInstance Selection (Price Priority)\nInput: Memory Requirements:\nMemmodel — Model weight memory requirement\nMemactivation — Activation memory requirement\nMemKVcache — Total KV Cache memory requirement\nMemKVcache, per_layer — KV Cache memory per layer\nFor each GPU instance i:\nGPUi\nmemory — Total GPU memory\nGPUi\nprice — GPU price\nUser-defined maximum price: Pmax\nOutput: GPU candidates that satisfy both price and resource conditions\n1 Candidates ← ∅ // Initialize candidate set\n2 Memtotal ← Memmodel + Memactivation + MemKVcache ;\n3 Membase ← Memmodel + Memactivation ;\n4 for each GPU instance i do\n5 if GPUi\nprice ≤ Pmax then\n// Apply Price Filter\n6 Memi\navail ← GPUi\nmemory – Membase // Calculate Available\nMemory\n7 if GPUi\nmemory ≥ Memtotal then\n// Offloading Not Required\n8 Ci\noff ← 0 ;\n9 Add (i, Ci\noff) to Candidate Set ;\n10 else\n11 if GPUi\nmemory < Memmodel OR MemKVcache, per_layer > Memi\navail\nthen\n// Offloading Not Possible\n12 Mark GPU i as Unsuitable // Exclude from\ncandidates\n13 end\n14 else\n// KV Cache Offloading Required\n15 Ci\noff ← 1 –\nMemi\navail\nMemKVcache\n;\n16 if MemKVcache, per_layer ≤ Memi\navail then\n// Layer-Level Constraint Check\n17 Add (i, Ci\noff) to Candidate Set ;\n18 end\n19 else\n20 Mark GPU i as Unsuitable // Exclude from\ncandidates\n21 end\n22 end\n23 end\n24 end\n25 end\n26 Sort Candidate Set by GPU i\nprice in ascending order ;\n27 return Candidate Set Candidates ;\nperformance-cost prediction modeling stage. This systematic\napproach ensures that the most cost-efficient GPU instance is\nselected within the user’s budget while accurately evaluating\nthe feasibility and cost-efficiency of KV Cache Offloading.\nD. Instance Performance Prediction\nAt this stage, the system predicts Tokens Per Second (TPS)\nfor the candidate GPU instances identified in the previous\nstep. This is achieved through mathematical modeling that\nleverages model parameters, hardware profiling information\n(FLOPS, bandwidth, etc.) of each candidate instance, and\nthe offloading coefficient to quantitatively estimate the task\nprocessing time.\nThe total task processing time Ttask consists of the Prefill\nand Decode stages and is calculated as follows [6]:\n• Prefill Stage: This stage processes the entire input se-\nquence. The processing time per layer ( Tpre) is multiplied\nby the number of layers ( n).\n• Decode Stage: This stage generates each output token\nsequentially. The processing time per layer ( Tdec) is\nmultiplied by the number of layers ( n) and the number\nof generated tokens (Lout –1), since the first output token\nis already processed in the Prefill stage.\nThus, the total task processing time Ttask is expressed as\nfollows:\nTtask = Tpre · n| {z }\nPrefill Time\n+ Tdec · n · (Lout – 1)| {z }\nDecode Time\n(4)\na) Prefill Stage Processing Time: The Prefill stage pro-\ncessing time Tpre consists of computation time and KV Cache\nstorage time. Since GPU computation and KV Cache offloading\noccur in parallel, the total delay is determined by the process\nwith the longest execution time:\nTpre = max\n\u0010\nCTCF(Tp\ncompute), Tp\ntrans\n\u0011\n(5)\nThe computation time Tp\ncompute in the Prefill stage is divided\ninto Linear Layer computation and Self-Attention computation,\nboth of which are calculated by dividing the required floating-\npoint operations (FLOPs) by the GPU’s theoretical FLOPS\ncapacity:\nTp\ncompute = BS · (8Lin · h2\n1 + 4Lin · h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · L2\nin · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tp\ntrans in the Prefill stage\nrepresents the time required to offload the generated KV\nCache from GPU to CPU memory and is computed as:\nTp\ntrans = Coff · {2 · (Lin + 1) · h1 · Precisionbytes} · BS\nBWgpu→cpu\nb) Decode Stage Processing Time: The Decode stage\nprocessing time Tdec includes computation time and KV Cache\nretrieval time. If the KV Cache fully resides in the GPU, only\ncomputation time is considered. However, if offloading occurs,\nadditional latency is introduced due to data transfer from\nCPU to GPU. The Decode time is therefore expressed as:\nTdec = CTCF(Td\ncompute) + Td\ntrans (6)\nThe Decode computation time Tdcompute consists of Linear\nLayer and Self-Attention computation, and is computed as\nfollows:\nTd\ncompute = BS · (8h2\n1 + 4h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · (Lin + Lout\n2 ) · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tdtrans in the Decode stage refers\nto the time required to load KV Cache stored in CPU memory\nback into GPU memory and is computed as:\nTd\ntrans = (Coff · 2 · (Lin + 1) + Lout) · h1 · Precisionbytes · BS\nBWcpu→gpu\n\ng4dn.xlarge\tGiven\tTFLOPS\ng4dn.xlarge\tMeasured\tTFLOPS\ng5.2xlarge\tGiven\tTFLOPS\ng5.2xlarge\tMeasured\tTFLOPS\ng6.xlarge\tGiven\tTFLOPS\ng6.xlarge\tMeasured\tTFLOPS\nTFLOPS\n0\n20\n40\n60\n80\n100\nBatch\tSize\n2\n4\n8\n12\n16\n20\n24\n28\n32\nFig. 3. Comparison of FLOPS provided by the GPU manufacturer (NVIDIA)\nand the actual FLOPS utilized when calculating Prefill time on AWS GPU\nVMs. The results present TFLOPS measurements for three different GPU\nVMs using the OPT-2.7B model with an input size of 512 tokens and an\noutput size of 128 tokens as batch size grows.\nThis modeling approach accounts for both cases where of-\nfloading is necessary and unnecessary, effectively considering\nGPU memory constraints and computational performance.\nBy incorporating both computation latency and KV Cache\noffloading overhead, this approach enables a quantitative\nanalysis of the trade-off between computation and memory\naccess time in both Prefill and Decode stages.\nUsing this modeling framework, Tokens Per Second (TPS)\ncan be estimated, allowing for the selection of the most\noptimal GPU instance for a given inference task. While\nthis theoretical modeling provides a solid foundation, it\nis important to note that GPU manufacturers’ theoretical\nFLOPS values do not always accurately reflect real-world LLM\ninference workloads. The limitations of this approach, along\nwith the Compute Time Calibration Function (CTCF) designed\nto correct these discrepancies, are discussed in Section IV-F.\nE. Step 4: Final Instance Selection Based on SLO\nBased on the TPS (Tokens Per Second) values computed\nfor each GPU instance in the previous stage, this step selects\nthe most cost-efficient instance while ensuring that the user’s\nService Level Objective (SLO) is met. The selection process\nfollows these steps:\nFirst, instances that fail to satisfy the user-defined SLO\nconstraint (TPS ≥ TPSSLO) are eliminated from consideration.\nNext, the cost efficiency metric (Equation 3) is calculated for\neach remaining instance. Finally, the instance with the highest\ncost efficiency is selected. In the event of a tie, the instance\nwith the higher TPS is prioritized.\nThe final selection result is presented to the user along with\ncomprehensive details, including instance type, expected TPS,\ncost, and KV Cache offloading configuration. Additionally,\nthe system provides alternative options and a performance-\ncost trade-off analysis, enabling users to make an informed\ndecision that is optimized for their specific LLM inference\nworkload.\nF. Compute Time Calibration Function (CTCF)\nThe theoretical FLOPS values provided by GPU manufac-\nturers do not accurately reflect real-world performance in\nLLM inference workloads. Figure 3 illustrates the discrepancy\nbetween the FLOPS values advertised by the manufacturer and\nthose actually utilized in computation across three different\nGPU instances. This discrepancy arises from factors such as\nmemory bottlenecks, reduced GPU utilization, and variations\nin computation patterns, which manifest differently in the\nPrefill and Decode stages of LLM inference. As a result,\nselecting a GPU instance solely based on theoretical FLOPS\ncan lead to significant performance mismatches, causing users\nto incur unnecessary costs. To address this issue, it is essential\nto introduce a calibration method that aligns theoretical FLOPS\nvalues with actual computational performance.\n• CTCF Modeling : This study conducted preliminary\nexperiments across various batch sizes to analyze the\nrelationship between LLM inference time and batch size.\nThe results consistently showed a linear increase in\ninference time for both the Prefill and Decode stages.\nThis linear trend was observed across different GPU\narchitectures, including T4, A10G, L4, and L40s, leading\nto the introduction of a regression-based CTCF model.\nCTCF is a linear transformation function that adjusts\ntheoretical computation time to match actual execution time.\nIt is defined as follows:\nCTCF(Tcompute) = α · Tcompute + β (7)\nwhere α is a scaling factor that corrects overestimation or\nunderestimation of theoretical computation time, and β is a\nfixed offset that compensates for systematic delays caused\nby GPU execution bottlenecks, memory access latency, and\nother hardware constraints. These parameters are optimized\nusing the least squares method and are determined through\npre-profiling experiments.\nThrough extensive pre-profiling, α and β values were\ncomputed for all AWS GPU instances across various batch\nsizes and stored as reference data. As shown in Table III,\napplying these per-instance α and β values significantly\nreduces the prediction error, bringing the adjusted execution\ntime very close to the actual measurement. Based on this,\nInferSave profiles α and β values for all available AWS\nGPU instances, enabling precise FLOPS-based execution time\npredictions and recommending the optimal instance for users.\nTABLE III\nV alues ofα, β to calculate adjusted TPrefill (Model: OPT 2.7B)\nInstance Type (GPU Model) α β avg. error rate (%)\ng4dn.xlarge (T4) -0.185 24.35 4.47\ng5.2xlarge (A10G) -0.074 46.97 2.60\ng6.xlarge (L4) -0.1238 42.52 2.23\nThe CTCF-based correction method effectively compensates\nfor the inherent limitations of theoretical FLOPS values\nprovided by GPU manufacturers, leading to more accurate\nLLM inference performance predictions.\nV. Implementation\nWe developed InferSave using Python (3.10.14). For per-\nformance modeling and KV cache offloading optimization, we\nutilized NumPy (1.24.3) for efficient numerical computations\n\nand statistical analysis. Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities. A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations. Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations. The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.\nVI. Evaluation\nA. Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances. The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input. Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances. All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.\nWorkload Definition: For a holistic evaluation of\nInferSave’s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.\n• Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens. This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers. The workload evaluates a total\nof 3000 requests.\n• Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used. This takes into account tasks\nsuch as document summarization and data wrangling. To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.\nA WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025. To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f. For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.\nTABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.\nInstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite. To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.\nPolicy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing. Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.\n• Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible. However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.\n• Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance. However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.\nB. CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements. To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge. The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128. The model’s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 × 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time. For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).\nAfter applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate. As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements. Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate\n\n(d)\tg6.xlarge\tDecode\tStage\n(c)\tg6.xlarge\tPrefill\tStage\n(b)\tg4dn.xlarge\tDecode\tStage\n(a)\tg4dn.xlarge\tPrefill\tStage\n \tCTCF\tAdjusted\tTime\nPredicted\tTime\n Actual\tTime\nError\tRate\t(CTCF\tvs\tActual)\n0\n20\n40\n60\n80\n100\nT Prefil\n0\n2\n4\n6\n8\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\n0\n20\n40\n60\n80\n100\nT Decode\n0\n0.5\n1\n1.5\n2\n2.5\n3\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nFig. 4. CTCF accuracy analysis. The results illustrate the predicted time\n(blue), actual time (red), and CTCF-adjusted values (green) for Prefill and\nDecode times as batch size increases on two different GPU VMs. Additionally,\nthe Error Rate between the CTCF-adjusted time and actual time is presented.\nwas 2%. These results demonstrate that the CTCF-adjusted\ncomputation time aligns well with real-world measurements,\nthereby verifying that InferSave can accurately recommend\nthe most suitable GPU instance for users.\nC. Evaluation results\nTo evaluate the effectiveness of InferSave and our pro-\nposed methodologies, we conducted experiments on both\nonline and offline workloads. While we have performed\ncomprehensive experiments across various model sizes and\nbatch sizes, we have decided to focus on the analysis of\nrepresentative results using the OPT-2.7B model with a batch\nsize of 32. This configuration was chosen as it clearly shows\nthe performance variations of each GPU instance, and also\ndemonstrates a good middle ground of performance and\nresource utilization. We set the maximum cost per hour (Pmax)\nto $3.00/hr. This value was chosen as g6e.xlarge, the most\npowerful instance in our experiments, has an on-demand cost\nof $2.699/hr, and a slightly higher cost than this allows for a\nfair comparison across all instances.\nTABLE V\nComparison of Instance Selection Results by SLO Constraints\n(400 TPS and 600 TPS)\nSLO Evaluated PoliciesSelected InstancesTPS(avg.)Total Cost($)\n400 TPS\nInferSave-1st g4dn.xlarge 620.17 0.71\nInferSave-2nd g6.xlarge 802.19 1.167\nMax-Performance g6e.xlarge 1506.54 2.699\n600 TPS\nInferSave-1st g6.xlarge 800.15 1.167\nInferSave-2nd g5.xlarge 1206.12 1.466\nMax-Performance g6e.xlarge 1505.37 2.699\nInferSave-1st InferSave-2nd Max-Performance0\n200\n400\n600\n800\n1000\n1200\n1400\nSLO 400\nSLO 600\nAverage tokens per second (TPS)\nMinimum SLO\nInferSave-1st InferSave-2nd Max-Performance0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 400\nSLO 600\nFig. 5. Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor online inference workloads (Left: Average TPS, Right: Cost).\nTABLE VI\nComparison of Instance Selection Results by SLO Constraints\n(100 TPS and 200 TPS)\nSLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($)\n100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13InferSave-2nd g6.xlarge 60 415.04 2.344Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699\n200 TPS InferSave-1st g6.xlarge 60 414.28 2.334InferSave-2nd g5.xlarge 60 414.01 2.932Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1505.37 2.699\n1) Online inference workload results: Table V and Figure 5\nshows the instances selected by each policy based on the\nSLO requirements given for an online inference workload,\nas well as the performance and price comparisons. We\nanalyze the first and second selections of InferSave’s policy\nwithin two minimum TPS requirements (400 TPS, 600 TPS),\nand compare it with the selection of the Max-Performance\npolicy’s selection. Note that the results of InferSave without\nKV Cache offloading were the same as Max-Performance’s\nselection, and thus were excluded from Table V. This result\nwas observed as the workload size used in this experiment\nwas sufficiently small, allowing all KV Cache data to be\naccommodated within the GPU memory. Therefore, offloading\nhad no impact on performance, and consequently, there was\nno difference in the selected instances. Additionally, for these\nexperiments, the total runtime did not surpass an hour, leading\nto the hourly cost and the total cost to be the same.\nWith an SLO requirement of 400 TPS, InferSave selected\ng4dn.xlarge as its first choice, and this instance offered the\nlowest cost of $0.71 while providing 620.17 TPS. On the other\nhand, Max-Performance selected g6e.xlarge, which provides\nthe highest performance of 1506.54 TPS, but at a cost of $2.699,\nwhich is about 280% more expensive than InferSave’s top\nchoice. A similar pattern was observed with the 600 TPS SLO\nconstraint, with InferSave’s selection of g6.xlarge meeting\nthe SLO at a 56.75% lower cost than g6e.xlarge.\nThis shows that the instances chosen by the Max-\nPerformance policy overshoots the given SLO requirement\ngreatly, leading to wasted GPU utilization and higher running\ncosts. Meanwhile, InferSave demonstrates optimal instance\nselection by using accurate performance prediction to select\nthe most favorable instance for the given requirements.\n2) Offline inference workload results: Table VI and Figure 6\nshows the instance selection of each policy based on the\nSLO requirements given for an offline inference workload,\nand performance and price comparisons from the selection\n\n7300\n7800\nSLO 100\nSLO 200\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n100\n200\n500\nMinimum SLO\nAverage tokens per second (TPS)\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 100\nSLO 200\nFig. 6. Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor offline inference workloads (Left: Average TPS, Right: Cost).\nmade by each policy. As this workload uses a large input\ntoken size, all instances excluding g6e.xlarge make use of\nKV Cache offloading. Without considering offloading, only\none instance can be considered a top choice, and therefore,\nInferSave without offloading chose the same instance as the\nMax-Performance policy.\nGiven a SLO requirement of 100 TPS, InferSave selected\ng4dn.xlarge as its top choice, providing a throughput of about\n160 TPS with the lowest total processing cost of $2.13. On the\nother hand, both Max-Performance and InferSave without\noffloading selected g6e.xlarge, which delivers a very high\nthroughput of about 7600 TPS, but with a total cost of $2.699,\nan increase of about 26.7%. The selection of g6e.xlarge allows\nfor maximum throughput with the ability to store all KV\nCache in GPU memory without offloading. However, despite\nthe high throughput and meeting the SLO, the high cost of\nthe instance itself results in lower overall cost efficiency.\nWith a SLO requirement of 200 TPS, InferSave selected\ng5.xlarge as its top choice, as g4dn.xlarge not longer meet the\nperformance requirements. This instance provides about 400\nTPS while maintaining a total cost of $2.344. On the other\nhand, the Max-Performance policy still selected g6e.xlarge,\nproviding a performance of about 7600 TPS, but the total\ncost increased to $2.699, resulting in about a 15% higher cost.\nThis shows that without considering offloading, a needlessly\nhighly performant and expensive instance can be chosen,\nleading to excessively high costs, and thus reducing actual\ncost efficiency.\n3) Overall analysis and discussion: By evaluating experi-\nmental results that represent both online chatbot and batch\nprocessing workloads, we were able to derive key insights\nfor the efficient operation of LLM inference systems.\n(i) The impact of a workload’s I/O patterns on optimal\ninfrastructure selection: The requirements of online\nconversational chatbot inference and batch processing\ninference differ greatly in input and output token lengths,\nwhich act as key factors in determining optimal instance\nand offloading strategies.\n(ii) The significance of selectively applying KV Cache\noffloading: KV Cache offloading is not a universally\napplicable strategy for all workloads and achieves the\ngreatest cost reduction when selectively utilized ac-\ncording to workload characteristics. In particular, for\noffline batch processing workloads with long inputs,\ncost reductions up to 28% were possible with KV Cache\noffloading, while maintaining SLO requirements. On the\nother hand, in online conversational chatbot workloads,\nit was often more advantageous to apply KV Cache\noffloading when considering both cost and performance.\n(iii) Finding the optimal interface through InferSave:\nInferSave comprehensively considers the SLO require-\nments and workload characteristics to find the optimal\nbalance point in cost and performance. Instead of naively\nselecting the instance with the highest performance,\nInferSave finds the instance with the highest cost\nefficiency while still satisfying the SLO requirement.\nThese results reveal an opportunity for both cost and\nperformance optimization by flexibly adjusting the offloading\nstrategy and GPU instance choice to match workload patterns\nand SLO requirements. InferSave takes this opportunity and\nperforms said optimizations automatically with the selection\nof the optimal instance by considering information from\nprecise analysis of each workload’s characteristics. As a result,\nInferSave achieves optimal performance according to the\ngiven SLO while maintaining cost efficiency.\nHowever, this work currently has limitations in that it\nfocuses on a single GPU environment and does not address\noptimization strategies in multi-GPU or distributed inference\nenvironments. We plan to extend InferSave to multi-GPU\nand distributed cluster environments to develop optimization\nstrategies suitable for inference of more complex workloads\nand large-scale models.\nVII. Conclusion\nIn this study, we propose InferSave, which utilizes SLO-\nbased predictions to automatically select cost-efficient VM\ninstances in the cloud, and validate it across online and offline\nworkloads. We identify opportunities to enhance cost effi-\nciency and utilize cheaper, less powerful GPU instances, while\nmaintaining the specified SLO requirements by exploiting\ntechniques such as KV Cache offloading. Through extensive\nevaluation across both online and offline inference workloads,\nour results confirm that InferSave accurately exploits said\nopportunities, and achieves at most 73.7% lower running costs\nwhile maintaining SLO requirements.\nThis research suggests that LLM service providers can\noptimize cost and performance in a balanced way by selecting\noptimal instances based on SLO and effectively utilizing\noffloading strategies. InferSave offers these optimizations\nin a unified package in a LLM inferencing system that both\nlowers cost and maintains performance requirements.\nReferences\n[1] A. Vaswani, “Attention is all you need, ” Advances in\nNeural Information Processing Systems , 2017.\n[2] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n“Improving language understanding by generative pre-\ntraining, ”OpenAI Preprint , 2018.\n[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,\n\nF. Azhar, et al. , “Llama: Open and efficient foundation\nlanguage models, ”arXiv preprint arXiv:2302.13971 , 2023.\n[4] Anthropic, “Message batches api, ” 2024. https://www.\nanthropic.com/news/message-batches-api, Accessed: De-\ncember 30, 2024.\n[5] OpenAI, “Batch processing and rate limits, ” 2024. https:\n//platform.openai.com/docs/guides/batch#rate-limits, Ac-\ncessed: December 30, 2024.\n[6] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen,\nP. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single gpu, ” in International Conference\non Machine Learning , pp. 31094–31116, PMLR, 2023.\n[7] Y. Xiong, H. Wu, C. Shao, Z. Wang, R. Zhang, Y. Guo,\nJ. Zhao, K. Zhang, and Z. Pan, “Layerkv: Optimizing\nlarge language model serving with layer-wise kv cache\nmanagement, ” 2024.\n[8] X. Pan, E. Li, Q. Li, S. Liang, Y. Shan, K. Zhou, Y. Luo,\nX. Wang, and J. Zhang, “Instinfer: In-storage attention\noffloading for cost-effective long-context llm inference, ”\n2024.\n[9] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li,\nE. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, and\nY. He, “Deepspeed- inference: Enabling efficient inference\nof transformer models at unprecedented scale, ” in SC22:\nInternational Conference for High Performance Computing,\nNetworking, Storage and Analysis , pp. 1–15, 2022.\n[10] G. Cloud, “Compare aws and azure services to\ngoogle cloud. ” https://cloud.google.com/docs/get-started/\naws-azure-gcp-service-comparison?hl=ko, 2024. Ac-\ncessed: 2024-12-26.\n[11] A. Harlap, A. Tumanov, A. Chung, G. R. Ganger, and\nP. B. Gibbons, “Proteus: Agile ml elasticity through\ntiered reliability in dynamic resource markets, ” in 12nd\nEuropean Conference on Computer Systems , EuroSys ’17,\np. 589–604, 2017.\n[12] Y. Kim, K. Kim, Y. Cho, J. Kim, A. Khan, K.-D. Kang,\nB.-S. An, M.-H. Cha, H.-Y. Kim, and Y. Kim, “DeepVM:\nIntegrating Spot and On-Demand VMs for Cost-Efficient\nDeep Learning Clusters in the Cloud, ” in IEEE/ACM\nInternational Symposium on Cluster, Cloud and Grid\nComputing (CCGRID) , 2024.\n[13] G. Fragiadakis, V. Liagkou, E. Filiopoulou, D. Fragkakis,\nC. Michalakelis, and M. Nikolaidou, “Cloud services cost\ncomparison: a clustering analysis framework, ”Computing,\nvol. 105, pp. 1–28, 03 2023.\n[14] A. Andrzejak, D. Kondo, and S. Yi, “Decision model for\ncloud computing under sla constraints, ” in Proceedings of\nthe IEEE International Symposium on Modeling, Analysis\nand Simulation of Computer and Telecommunication\nSystems, MASCOTS ’10, pp. 257–266, IEEE, 2010.\n[15] P. Kokkinos, T. A. Varvarigou, A. Kretsis, P. Soumplis,\nand E. A. Varvarigos, “Cost and utilization optimization\nof amazon ec2 instances, ” in Proceedings of the 2013\nIEEE Sixth International Conference on Cloud Computing ,\npp. 518–525, IEEE, 2013.\n[16] T. Griggs, X. Liu, J. Yu, D. Kim, W.-L. Chiang, A. Cheung,\nand I. Stoica, “Mélange: Cost efficient large language\nmodel serving by exploiting gpu heterogeneity, ” 2024.\n[17] C. Nie, R. Fonseca, and Z. Liu, “Aladdin: Joint placement\nand scaling for slo-aware llm serving, ” arXiv preprint\narXiv:2405.06856, 2024.\n[18] Y. Jiang, F. Fu, X. Yao, T. Wang, B. Cui, A. Klimovic,\nand E. Yoneki, “Thunderserve: High-performance and\ncost-efficient llm serving in cloud environments, ” arXiv\npreprint arXiv:2502.09334 , 2025.\n[19] P. Patel, E. Choukse, C. Zhang, A. Shah, Í. Goiri, S. Maleki,\nand R. Bianchini, “Splitwise: Efficient generative llm\ninference using phase splitting, ” in 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architec-\nture (ISCA) , pp. 118–132, IEEE, 2024.\n[20] Z. Wang, S. Li, Y. Zhou, X. Li, R. Gu, N. Cam-Tu, C. Tian,\nand S. Zhong, “Revisiting slo and goodput metrics in\nllm serving, ”arXiv preprint arXiv:2410.14257 , 2024.\n[21] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury,\nJ. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently\nscaling transformer inference, ” inProceedings of Machine\nLearning and Systems 5 (MLSys 2023) , 2023.\n[22] AWS, “Aws amazon ec2 instance types-cloud computing\ninstances. ”", "metadata": {"url": "https://arxiv.org/pdf/2504.11816", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Cost-Efficient LLM Serving in the Cloud: VM\nSelection with KV Cache Offloading\nKihyun Kim 1, Jinwoo Kim 1, Hyunsun Chung 1, Myung-Hoon Cha 2, Hong-Yeon Kim 2, Youngjae Kim 1,†\n1Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea\n2ETRI, Daejeon, Republic of Korea\nAbstract—LLM inference is essential for applications like\ntext summarization, translation, and data analysis, but the\nhigh cost of GPU instances from Cloud Service Providers\n(CSPs) like A WS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud-\nbased LLM inference. InferSave optimizes KV cache offloading\nbased on Service Level Objectives (SLOs) and workload charac-\nteristics, estimating GPU memory needs, and recommending\ncost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection\naccuracy by adjusting for discrepancies between theoretical\nand actual GPU performance. Experiments on A WS GPU\ninstances show that selecting lower-cost instances without\nKV cache offloading improves cost efficiency by up to 73.7%\nfor online workloads, while KV cache offloading saves up to\n20.19% for offline workloads.\nIndex Terms —Cloud Computing, LLM Inference Tasks, Ser-\nvice Level Objective (SLO) Management, KV Cache Offloading\nI. Introduction\nLarge Language Models (LLMs) have become a core tech-\nnology in modern Natural Language Processing (NLP), demon-\nstrating outstanding performance in various applications such\nas text summarization, machine translation, and conversa-\ntional AI [ 1]. LLMs built on Transformer-based architectures,\nsuch as GPT [ 2] and LLaMA [ 3], leverage multi-layer self-\nattention mechanisms and large-scale pretraining to achieve\nnear-human-level language understanding and generation\ncapabilities. Thanks to their superior performance, LLMs are\nwidely used across industries, providing high accuracy and\nnatural responses in a wide range of tasks, including text\nsummarization, question answering, and document analysis.\nHowever, to efficiently design an LLM inference system, it\nis essential to consider task-specific Service Level Objectives\n(SLOs). For instance, in online inference tasks, such as real-\ntime conversational services or question answering, latency\nmust be minimized to ensure a seamless user experience.\nReducing inference latency is a key challenge in these\nscenarios.\nOn the other hand, in batch processing tasks [ 4, 5] such\nas text summarization for large datasets, log analysis, and\ndocument clustering, latency requirements are generally less\nstrict. Instead, maximizing throughput is critical, as these tasks\ninvolve processing large volumes of input data at once. In\nsuch batch processing environments, handling large batches\n†Y. Kim is the corresponding author.\ncan easily lead to GPU memory shortages. Due to the auto-\nregressive nature of LLM inference, the Key-Value (KV) cache,\nwhich stores past token information, continuously grows. As\na result, GPU memory usage increases sharply with sequence\nlength and batch size.\nA common technique to mitigate this issue is KV Cache\nOffloading, which offloads KV cache data exceeding GPU\nmemory limits to CPU memory or disk. This enables large-\nbatch processing without running out of memory [ 6, 7, 8, 9].\nHowever, if the additional latency introduced by offloading is\nnot properly managed, throughput can significantly degrade,\npotentially failing to meet the required SLOs.\nCost Efficiency of LLM Inference in Cloud Environ-\nments: Major cloud service providers such as AWS, GCP, and\nAzure offer a variety of GPU instance options with different\nperformance levels and cost structures, providing flexibility\nin resource utilization [ 10]. However, selecting a cost-efficient\nGPU instance in a cloud environment is a complex task\nthat is difficult for users to perform manually. The challenge\narises because GPU instances vary significantly in price and\nperformance (Refer to Table I), and workload characteristics\nrequire flexible KV cache offloading strategies, making optimal\nselection difficult.\nGiven this complexity, an optimized approach must inte-\ngrate the following two key factors:\n• GPU instance selection based on task characteristics\n• Efficient KV Cache Offloading strategy\nBalancing throughput targets and cost efficiency by combin-\ning these two factors remains a critical challenge that needs\nto be addressed.\nLimitations of Existing Research: Previous studies on\ncost efficiency in cloud environments [ 11, 12, 13, 14, 15] have\nfocused primarily on image processing or general machine\nlearning workloads. As a result, they do not capture the\nunique characteristics of large-scale LLM inference. Moreover,\nrecent research on cost-efficient LLM inference has largely\nconcentrated on real-time inference scenarios [ 16, 17, 18,\n19, 20], neglecting large-scale data processing environments\nwhere KV cache offloading could be leveraged effectively.\nFurthermore, these studies do not comprehensively analyze\ncost efficiency in relation to Service Level Objectives (SLOs).\nTo address these challenges, this paper proposes InferSave,\na software framework that automatically selects the optimal\nVM instance by considering both cost and performance based\non SLOs.\narXiv:2504.11816v1  [cs.LG]  16 Apr 2025", "sentences": [{"text": "Cost-Efficient LLM Serving in the Cloud: VM\nSelection with KV Cache Offloading\nKihyun Kim 1, Jinwoo Kim 1, Hyunsun Chung 1, Myung-Hoon Cha 2, Hong-Yeon Kim 2, Youngjae Kim 1,†\n1Dept.", "metadata": {}}, {"text": "of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea\n2ETRI, Daejeon, Republic of Korea\nAbstract—LLM inference is essential for applications like\ntext summarization, translation, and data analysis, but the\nhigh cost of GPU instances from Cloud Service Providers\n(CSPs) like A WS is a major burden.", "metadata": {}}, {"text": "This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud-\nbased LLM inference.", "metadata": {}}, {"text": "InferSave optimizes KV cache offloading\nbased on Service Level Objectives (SLOs) and workload charac-\nteristics, estimating GPU memory needs, and recommending\ncost-effective VM instances.", "metadata": {}}, {"text": "Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection\naccuracy by adjusting for discrepancies between theoretical\nand actual GPU performance.", "metadata": {}}, {"text": "Experiments on A WS GPU\ninstances show that selecting lower-cost instances without\nKV cache offloading improves cost efficiency by up to 73.7%\nfor online workloads, while KV cache offloading saves up to\n20.19% for offline workloads.", "metadata": {}}, {"text": "Index Terms —Cloud Computing, LLM Inference Tasks, Ser-\nvice Level Objective (SLO) Management, KV Cache Offloading\nI.", "metadata": {}}, {"text": "Introduction\nLarge Language Models (LLMs) have become a core tech-\nnology in modern Natural Language Processing (NLP), demon-\nstrating outstanding performance in various applications such\nas text summarization, machine translation, and conversa-\ntional AI [ 1].", "metadata": {}}, {"text": "LLMs built on Transformer-based architectures,\nsuch as GPT [ 2] and LLaMA [ 3], leverage multi-layer self-\nattention mechanisms and large-scale pretraining to achieve\nnear-human-level language understanding and generation\ncapabilities.", "metadata": {}}, {"text": "Thanks to their superior performance, LLMs are\nwidely used across industries, providing high accuracy and\nnatural responses in a wide range of tasks, including text\nsummarization, question answering, and document analysis.", "metadata": {}}, {"text": "However, to efficiently design an LLM inference system, it\nis essential to consider task-specific Service Level Objectives\n(SLOs).", "metadata": {}}, {"text": "For instance, in online inference tasks, such as real-\ntime conversational services or question answering, latency\nmust be minimized to ensure a seamless user experience.", "metadata": {}}, {"text": "Reducing inference latency is a key challenge in these\nscenarios.", "metadata": {}}, {"text": "On the other hand, in batch processing tasks [ 4, 5] such\nas text summarization for large datasets, log analysis, and\ndocument clustering, latency requirements are generally less\nstrict.", "metadata": {}}, {"text": "Instead, maximizing throughput is critical, as these tasks\ninvolve processing large volumes of input data at once.", "metadata": {}}, {"text": "In\nsuch batch processing environments, handling large batches\n†Y.", "metadata": {}}, {"text": "Kim is the corresponding author.", "metadata": {}}, {"text": "can easily lead to GPU memory shortages.", "metadata": {}}, {"text": "Due to the auto-\nregressive nature of LLM inference, the Key-Value (KV) cache,\nwhich stores past token information, continuously grows.", "metadata": {}}, {"text": "As\na result, GPU memory usage increases sharply with sequence\nlength and batch size.", "metadata": {}}, {"text": "A common technique to mitigate this issue is KV Cache\nOffloading, which offloads KV cache data exceeding GPU\nmemory limits to CPU memory or disk.", "metadata": {}}, {"text": "This enables large-\nbatch processing without running out of memory [ 6, 7, 8, 9].", "metadata": {}}, {"text": "However, if the additional latency introduced by offloading is\nnot properly managed, throughput can significantly degrade,\npotentially failing to meet the required SLOs.", "metadata": {}}, {"text": "Cost Efficiency of LLM Inference in Cloud Environ-\nments: Major cloud service providers such as AWS, GCP, and\nAzure offer a variety of GPU instance options with different\nperformance levels and cost structures, providing flexibility\nin resource utilization [ 10].", "metadata": {}}, {"text": "However, selecting a cost-efficient\nGPU instance in a cloud environment is a complex task\nthat is difficult for users to perform manually.", "metadata": {}}, {"text": "The challenge\narises because GPU instances vary significantly in price and\nperformance (Refer to Table I), and workload characteristics\nrequire flexible KV cache offloading strategies, making optimal\nselection difficult.", "metadata": {}}, {"text": "Given this complexity, an optimized approach must inte-\ngrate the following two key factors:\n• GPU instance selection based on task characteristics\n• Efficient KV Cache Offloading strategy\nBalancing throughput targets and cost efficiency by combin-\ning these two factors remains a critical challenge that needs\nto be addressed.", "metadata": {}}, {"text": "Limitations of Existing Research: Previous studies on\ncost efficiency in cloud environments [ 11, 12, 13, 14, 15] have\nfocused primarily on image processing or general machine\nlearning workloads.", "metadata": {}}, {"text": "As a result, they do not capture the\nunique characteristics of large-scale LLM inference.", "metadata": {}}, {"text": "Moreover,\nrecent research on cost-efficient LLM inference has largely\nconcentrated on real-time inference scenarios [ 16, 17, 18,\n19, 20], neglecting large-scale data processing environments\nwhere KV cache offloading could be leveraged effectively.", "metadata": {}}, {"text": "Furthermore, these studies do not comprehensively analyze\ncost efficiency in relation to Service Level Objectives (SLOs).", "metadata": {}}, {"text": "To address these challenges, this paper proposes InferSave,\na software framework that automatically selects the optimal\nVM instance by considering both cost and performance based\non SLOs.", "metadata": {}}, {"text": "arXiv:2504.11816v1  [cs.LG]  16 Apr 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "The InferSave framework operates as follows based on\nuser input: First, It calculates the required GPU memory\nbased on the specified SLO and workload size, analyzing\nthe feasibility of KV cache offloading to determine a set of\ncandidate instances. Next, using pre-collected performance\ndata, it performs a modeling step to predict the performance\nand cost of each instance. Finally, it evaluates these predictions\nto recommend the most cost-efficient instance that meets the\nuser’s SLO constraints.Through this process, the InferSave\nframework becomes the first solver system that automatically\nrecommends the most economical VM instance for LLM\nserving in cloud environments. By integrating KV cache\noffloading and GPU instance characteristics, it ensures SLO\ncompliance while optimizing costs.\nThe InferSave framework analyzes GPU instance perfor-\nmance based on user input and comprehensively considers\nthe feasibility of KV cache offloading to automatically recom-\nmend the optimal VM instance for LLM inference in cloud\nenvironments. By leveraging InferSave, users can easily find\nthe most cost-effective VM instance that meets their specified\nSLO while minimizing operational expenses.\nExperimental results show that applying InferSave\nachieves significant cost savings compared to traditional\nmaximum-performance-based policies, with reductions of up\nto 73.7% for online workloads and 20.19% for offline workloads.\nIn addition, it is designed to be flexible across various AWS\ninstances and cloud environments, providing a practical and\nefficient approach to operating LLM inference services.\nII. Background and Motivation\nA. LLM Architecutre and Inference\nLarge-scale language models (LLMs), such as OpenAI’s\nGPT [ 2] and Meta’s LLaMA [ 3], are built on the Trans-\nformer [1] architecture. These models consist of a multi-layer\nstructure incorporating Self-Attention mechanisms and Feed-\nForward Networks, enabling their broad applicability across\nvarious natural language processing (NLP) tasks.\nThe LLM inference process is divided into two stages: Prefill\nand Decode. In the Prefill stage, the input prompt is processed\nin parallel to generate the initial output tokens. During\nthis process, Query, Key, and Value vectors are computed\nfor each token in the input prompt, capturing contextual\ninformation through token-wise interactions. Simultaneously,\nthe computed Key and Value tensors are stored in the\nGPU memory as a Key-Value Cache (KV Cache) to alleviate\ncomputational overhead in subsequent operations.\nThe KV Cache is essential for preventing redundant\ncomputations in Self-Attention, thereby enhancing inference\nspeed and resource efficiency. For instance, if the Prefill stage\ncomputes and stores the Key and Value tensors for the input\n\"I am a, \" the Decode stage can reuse them to rapidly generate\nthe next token, \"man, \" without redundant computations.\nIn the Decode stage, new tokens are sequentially generated\nin an Auto-Regressive manner based on previously generated\noutput tokens. Here, the stored KV Cache is reused to\nreduce the computational burden of repeated Self-Attention\noperations and improve processing speed. However, the size\nof the KV Cache increases significantly with the input length\nand model size.\nFor example, as shown in Figure 1, in the OPT_2.7B model\nrunning on an AWS g4dn.xlarge instance with 1024 input\ntokens, the KV Cache consumes approximately 0.332GB at a\nbatch size of 2. When the batch size increases to 32, the\nKV Cache expands to 5.312GB, which can lead to GPU\nmemory exhaustion. This memory constraint may degrade\noverall system throughput and reduce resource utilization\nefficiency [1, 21].\nB. Memory Optimization for LLM Inference via KV Cache\nOffloading\nDuring LLM inference, the increasing size of the KV Cache\ncan lead to GPU memory exhaustion, resulting in an Out-of-\nMemory (OoM) issue. To address this, KV Cache Offloading\ntechniques have been proposed [ 6, 7, 8, 9]. These techniques\noperate by offloading KV Cache data that exceeds GPU\nmemory capacity to CPU memory or disk and retrieving\nit back to the GPU when needed for computation. This\napproach effectively alleviates the GPU memory pressure,\nenabling the processing of long sequences and large batch\nsizes. Additionally, it allows efficient inference on lower-end\nGPUs without requiring additional high-performance GPUs,\nthus reducing deployment costs.\nHowever, latency introduced by data transfer between the\nGPU and external storage (e.g., CPU memory or disk) is\na major limitation of KV Cache Offloading. If the transfer\nfrequency of KV Cache data is high, the increased latency\ncan lead to bandwidth bottlenecks, ultimately degrading\ninference performance. Therefore, for effective deployment of\nKV Cache Offloading, it is essential to optimize the process\nby considering LLM inference characteristics (e.g., sequence\nlength, batch size) and user-defined Service Level Objectives\n(SLOs), such as maximum allowable response time.\nC. Challenges of LLM Inference and KV Cache Offloading in\nthe Cloud\nCloud service providers (CSPs) such as Amazon AWS offer\na variety of GPU virtual machine (VM) instances. As shown\nin Table I, the price of these instances varies significantly,\nranging from $0.379 (g4ad.xlarge) to $40.96 (p4de.24xlarge),\ndepending on the type of GPU, the memory capacity, and the\nbandwidth of the network [22].\nMoreover, when applying KV Cache Offloading to LLM\ninference, the trade-off between inference performance and\nactual cost introduces a complex dilemma. To maximize cost-\nefficiency, users must carefully optimize their choice of VM\nand offloading strategy based on: (i) Model size, (ii) Sequence\nlength, and (iii) Service Level Objectives (SLOs), such as\nmaximum response time.\nHowever, a systematic framework for making these deci-\nsions is currently lacking. As a result, users must experiment\nwith multiple VM options and offloading policies manually to", "sentences": [{"text": "The InferSave framework operates as follows based on\nuser input: First, It calculates the required GPU memory\nbased on the specified SLO and workload size, analyzing\nthe feasibility of KV cache offloading to determine a set of\ncandidate instances.", "metadata": {}}, {"text": "Next, using pre-collected performance\ndata, it performs a modeling step to predict the performance\nand cost of each instance.", "metadata": {}}, {"text": "Finally, it evaluates these predictions\nto recommend the most cost-efficient instance that meets the\nuser’s SLO constraints.Through this process, the InferSave\nframework becomes the first solver system that automatically\nrecommends the most economical VM instance for LLM\nserving in cloud environments.", "metadata": {}}, {"text": "By integrating KV cache\noffloading and GPU instance characteristics, it ensures SLO\ncompliance while optimizing costs.", "metadata": {}}, {"text": "The InferSave framework analyzes GPU instance perfor-\nmance based on user input and comprehensively considers\nthe feasibility of KV cache offloading to automatically recom-\nmend the optimal VM instance for LLM inference in cloud\nenvironments.", "metadata": {}}, {"text": "By leveraging InferSave, users can easily find\nthe most cost-effective VM instance that meets their specified\nSLO while minimizing operational expenses.", "metadata": {}}, {"text": "Experimental results show that applying InferSave\nachieves significant cost savings compared to traditional\nmaximum-performance-based policies, with reductions of up\nto 73.7% for online workloads and 20.19% for offline workloads.", "metadata": {}}, {"text": "In addition, it is designed to be flexible across various AWS\ninstances and cloud environments, providing a practical and\nefficient approach to operating LLM inference services.", "metadata": {}}, {"text": "II.", "metadata": {}}, {"text": "Background and Motivation\nA.", "metadata": {}}, {"text": "LLM Architecutre and Inference\nLarge-scale language models (LLMs), such as OpenAI’s\nGPT [ 2] and Meta’s LLaMA [ 3], are built on the Trans-\nformer [1] architecture.", "metadata": {}}, {"text": "These models consist of a multi-layer\nstructure incorporating Self-Attention mechanisms and Feed-\nForward Networks, enabling their broad applicability across\nvarious natural language processing (NLP) tasks.", "metadata": {}}, {"text": "The LLM inference process is divided into two stages: Prefill\nand Decode.", "metadata": {}}, {"text": "In the Prefill stage, the input prompt is processed\nin parallel to generate the initial output tokens.", "metadata": {}}, {"text": "During\nthis process, Query, Key, and Value vectors are computed\nfor each token in the input prompt, capturing contextual\ninformation through token-wise interactions.", "metadata": {}}, {"text": "Simultaneously,\nthe computed Key and Value tensors are stored in the\nGPU memory as a Key-Value Cache (KV Cache) to alleviate\ncomputational overhead in subsequent operations.", "metadata": {}}, {"text": "The KV Cache is essential for preventing redundant\ncomputations in Self-Attention, thereby enhancing inference\nspeed and resource efficiency.", "metadata": {}}, {"text": "For instance, if the Prefill stage\ncomputes and stores the Key and Value tensors for the input\n\"I am a, \" the Decode stage can reuse them to rapidly generate\nthe next token, \"man, \" without redundant computations.", "metadata": {}}, {"text": "In the Decode stage, new tokens are sequentially generated\nin an Auto-Regressive manner based on previously generated\noutput tokens.", "metadata": {}}, {"text": "Here, the stored KV Cache is reused to\nreduce the computational burden of repeated Self-Attention\noperations and improve processing speed.", "metadata": {}}, {"text": "However, the size\nof the KV Cache increases significantly with the input length\nand model size.", "metadata": {}}, {"text": "For example, as shown in Figure 1, in the OPT_2.7B model\nrunning on an AWS g4dn.xlarge instance with 1024 input\ntokens, the KV Cache consumes approximately 0.332GB at a\nbatch size of 2.", "metadata": {}}, {"text": "When the batch size increases to 32, the\nKV Cache expands to 5.312GB, which can lead to GPU\nmemory exhaustion.", "metadata": {}}, {"text": "This memory constraint may degrade\noverall system throughput and reduce resource utilization\nefficiency [1, 21].", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Memory Optimization for LLM Inference via KV Cache\nOffloading\nDuring LLM inference, the increasing size of the KV Cache\ncan lead to GPU memory exhaustion, resulting in an Out-of-\nMemory (OoM) issue.", "metadata": {}}, {"text": "To address this, KV Cache Offloading\ntechniques have been proposed [ 6, 7, 8, 9].", "metadata": {}}, {"text": "These techniques\noperate by offloading KV Cache data that exceeds GPU\nmemory capacity to CPU memory or disk and retrieving\nit back to the GPU when needed for computation.", "metadata": {}}, {"text": "This\napproach effectively alleviates the GPU memory pressure,\nenabling the processing of long sequences and large batch\nsizes.", "metadata": {}}, {"text": "Additionally, it allows efficient inference on lower-end\nGPUs without requiring additional high-performance GPUs,\nthus reducing deployment costs.", "metadata": {}}, {"text": "However, latency introduced by data transfer between the\nGPU and external storage (e.g., CPU memory or disk) is\na major limitation of KV Cache Offloading.", "metadata": {}}, {"text": "If the transfer\nfrequency of KV Cache data is high, the increased latency\ncan lead to bandwidth bottlenecks, ultimately degrading\ninference performance.", "metadata": {}}, {"text": "Therefore, for effective deployment of\nKV Cache Offloading, it is essential to optimize the process\nby considering LLM inference characteristics (e.g., sequence\nlength, batch size) and user-defined Service Level Objectives\n(SLOs), such as maximum allowable response time.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Challenges of LLM Inference and KV Cache Offloading in\nthe Cloud\nCloud service providers (CSPs) such as Amazon AWS offer\na variety of GPU virtual machine (VM) instances.", "metadata": {}}, {"text": "As shown\nin Table I, the price of these instances varies significantly,\nranging from $0.379 (g4ad.xlarge) to $40.96 (p4de.24xlarge),\ndepending on the type of GPU, the memory capacity, and the\nbandwidth of the network [22].", "metadata": {}}, {"text": "Moreover, when applying KV Cache Offloading to LLM\ninference, the trade-off between inference performance and\nactual cost introduces a complex dilemma.", "metadata": {}}, {"text": "To maximize cost-\nefficiency, users must carefully optimize their choice of VM\nand offloading strategy based on: (i) Model size, (ii) Sequence\nlength, and (iii) Service Level Objectives (SLOs), such as\nmaximum response time.", "metadata": {}}, {"text": "However, a systematic framework for making these deci-\nsions is currently lacking.", "metadata": {}}, {"text": "As a result, users must experiment\nwith multiple VM options and offloading policies manually to", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "TABLE I\nV arious Types of instances provided by A WS.\nThis information was available on Feburary 4, 2025 in\nN.Virginia region.\nName GPU On- GPU FLOPS vCPU GPU Mem Mem Network\nType Demand ($) (#) (TFLOPS) (GiB) (GiB) (Gbps) (Gbps)\ng4dn.xlarge T4 0.526 1 8.141 4 16 16 - 25\ng4ad.xlargeV520 Pro 0.379 1 7.373 4 8 16 - 10\ng5.xlarge A10G 1.006 1 31.52 4 24 16 - 10\ng5g.xlarge T4G 0.42 1 8.141 4 16 8 - 10\ng6.xlarge L4 0.805 1 30.29 4 24 16 - 10\ng6.4xlarge L4 1.323 1 30.29 16 24 64 - 25\ng4dn.12xlarge T4 3.912 4 8.141 48 64 192 50\ng4dn.metal T4 7.824 8 8.141 96 128 384 100\ng4ad.16xlargeV520 Pro 3.468 4 7.373 64 32 256 25\ng5.12xlarge A10G 5.672 4 31.52 96 96 192 40\ng5g.16xlarge T4G 2.744 2 8.141 64 32 128 25\ng6.12xlarge L4 4.602 4 30.29 48 96 192 40\ng6.48xlarge L4 13.35 8 30.29 192 196 768 100\np4de.24xlargeA100 40.96 96 19.49 96 7680 640 400\ndetermine an optimal configuration, which adds significant\noverhead [6, 9].\nIn this paper, we outline the key dilemmas of KV Cache\nOffloading for LLM inference in the cloud as follows.\n• Dual Nature of KV Cache Offloading: KV Cache\nOffloading mitigates GPU memory shortage issues, allowing\nfor the processing of larger batch sizes (e.g., greater than\n16). However, it increases latency due to data transfer\nbetween CPU and GPU (e.g., up to 20% latency increase\nin FlexGen [ 6]). Specifically, when the sequence length\nexceeds 4096, the KV Cache size grows significantly\n(e.g., exceeding 3.2GB), making offloading essential. This,\nhowever, increases the likelihood of failing to meet Service\nLevel Objectives (SLOs) such as a 100ms response time.\n• Complexity of Cloud VM Selection: As shown in\nTable I, the performance and cost between instances like\ng4dn.xlarge ($0.526, 16 GiB GPU Memory) and p4de.24xlarge\n($40.96, 7680 GiB GPU Memory) vary significantly. The\noptimal VM selection depends on the model requirements\n(e.g., memory usage, computation speed). High-performance\nVMs reduce the need for KV Cache Offloading, while lower-\nend VMs increase reliance on offloading.\n• Difficulty of SLO-Based Optimization: High-\nperformance VMs (e.g., g6.48xlarge) solve the Out-\nof-Memory (OoM) problem but may lead to GPU\nutilization dropping below 50% when the inference load\nis low, resulting in wasted costs. On the other hand,\nlower-end VMs (e.g., g4ad.xlarge) have lower initial costs\nbut suffer from frequent KV Cache Offloading due to\nVRAM limitations, causing latency to increase by more\nthan double [ 9]. This results in a dilemma of (i) resource\nwastage with high-cost VM selection, and (ii) performance\ndegradation with low-cost VM selection.\n• Lack of Automated Optimization Systems: Currently,\nthere is a lack of guidelines for automating the selection of\nVMs and KV Cache Offloading in cloud environments. Users\nmust manually test various VMs (e.g., g5 vs. g6 series) and\noffloading settings, which increases time and cost burdens.\nThis study proposes the necessity of a framework that auto-\nmatically recommends optimal VM and KV Cache Offloading\nstrategies based on SLO, and introduces a model (Solver) that\ncan balance cost and performance.\n(b)\tg4dn.xlarge\twith\tOPT2.7B\tmodel\n(a)\tg4dn.xlarge\twith\tOPT1.3B\tmodel\nKV\tCache\tSize\nKV\tCache\tSize\t(GB)\n0\n1\n2\n3\n4\n5\n6\nBatch\tSize\n2\n4\n8\n16\n32\nKV\tCache\tSize\n0\n2\n4\n6\n8\nBatch\tSize\n2\n4\n8\n16\n32\nFig. 1. Analysis of KV Cache size growth across different models in response\nto increasing batch sizes.\nD. Existing Approaches and Their Limitations\nExisting research aiming to optimize LLM inference in\ncloud environments [ 16, 17, 18, 19] reveals limitations in\nachieving cost-efficient LLM serving as they do not consider\nan integrated approach to KV Cache Offloading and VM\nselection.\nMelange [16] proposes an allocation strategy that minimizes\ncost by mixing different GPU types (e.g., high-performance\nGPUs and low-cost GPUs) based on LLM service charac-\nteristics such as request size, frequency, and Service Level\nObjectives (SLOs, e.g., response time within 200ms). However,\nthis method relies on profiling GPU performance and the\nworkload pattern in advance, making it difficult to apply\nto new environments or models. Furthermore, it does not\naccount for KV Cache Offloading, failing to provide optimiza-\ntion solutions in memory-constrained scenarios (e.g., when\nsequence length exceeds 4096).\nAladdin [17] suggests a framework for jointly optimizing\nrequest batches and resource scaling to meet SLOs. For\ninstance, it adds additional GPUs to reduce latency at high\nrequest rates. However, it does not integrate the memory-\nsaving effects of KV Cache Offloading or the trade-offs\nbetween different GPU types, which limits the flexibility in\nVM configuration.\nSplitWise [ 19] and ThunderServe [ 18] utilize a Phase\nSplitting strategy, separating the Prefill (initial token gen-\neration) and Decode (subsequent token generation) stages.\nThese approaches allocate specialized GPUs to each stage\n(e.g., high-speed GPUs for Prefill and memory-centric GPUs\nfor Decode) to enhance efficiency. However, this method is\nonly effective in environments where the two stages can be\nphysically separated, making it difficult to apply to standard\nsingle-VM-based LLM serving. Additionally, transferring KV\nCache between stages requires high-speed interconnects (e.g.,\nNVLink, with GPU-to-GPU bandwidth above 100GB/s), which\nreduces practicality in cloud VMs without NVLink (e.g., AWS\ng4dn series).\nMeanwhile, DeepVM [ 12], which deals with deep learning\noptimization in cloud environments, focuses on optimizing\nVM clusters for checkpoint-based distributed CNN training.\nFor example, it reduces costs by leveraging saved states during\ntraining interruptions. However, this method is tailored for", "sentences": [{"text": "TABLE I\nV arious Types of instances provided by A WS.", "metadata": {}}, {"text": "This information was available on Feburary 4, 2025 in\nN.Virginia region.", "metadata": {}}, {"text": "Name GPU On- GPU FLOPS vCPU GPU Mem Mem Network\nType Demand ($) (#) (TFLOPS) (GiB) (GiB) (Gbps) (Gbps)\ng4dn.xlarge T4 0.526 1 8.141 4 16 16 - 25\ng4ad.xlargeV520 Pro 0.379 1 7.373 4 8 16 - 10\ng5.xlarge A10G 1.006 1 31.52 4 24 16 - 10\ng5g.xlarge T4G 0.42 1 8.141 4 16 8 - 10\ng6.xlarge L4 0.805 1 30.29 4 24 16 - 10\ng6.4xlarge L4 1.323 1 30.29 16 24 64 - 25\ng4dn.12xlarge T4 3.912 4 8.141 48 64 192 50\ng4dn.metal T4 7.824 8 8.141 96 128 384 100\ng4ad.16xlargeV520 Pro 3.468 4 7.373 64 32 256 25\ng5.12xlarge A10G 5.672 4 31.52 96 96 192 40\ng5g.16xlarge T4G 2.744 2 8.141 64 32 128 25\ng6.12xlarge L4 4.602 4 30.29 48 96 192 40\ng6.48xlarge L4 13.35 8 30.29 192 196 768 100\np4de.24xlargeA100 40.96 96 19.49 96 7680 640 400\ndetermine an optimal configuration, which adds significant\noverhead [6, 9].", "metadata": {}}, {"text": "In this paper, we outline the key dilemmas of KV Cache\nOffloading for LLM inference in the cloud as follows.", "metadata": {}}, {"text": "• Dual Nature of KV Cache Offloading: KV Cache\nOffloading mitigates GPU memory shortage issues, allowing\nfor the processing of larger batch sizes (e.g., greater than\n16).", "metadata": {}}, {"text": "However, it increases latency due to data transfer\nbetween CPU and GPU (e.g., up to 20% latency increase\nin FlexGen [ 6]).", "metadata": {}}, {"text": "Specifically, when the sequence length\nexceeds 4096, the KV Cache size grows significantly\n(e.g., exceeding 3.2GB), making offloading essential.", "metadata": {}}, {"text": "This,\nhowever, increases the likelihood of failing to meet Service\nLevel Objectives (SLOs) such as a 100ms response time.", "metadata": {}}, {"text": "• Complexity of Cloud VM Selection: As shown in\nTable I, the performance and cost between instances like\ng4dn.xlarge ($0.526, 16 GiB GPU Memory) and p4de.24xlarge\n($40.96, 7680 GiB GPU Memory) vary significantly.", "metadata": {}}, {"text": "The\noptimal VM selection depends on the model requirements\n(e.g., memory usage, computation speed).", "metadata": {}}, {"text": "High-performance\nVMs reduce the need for KV Cache Offloading, while lower-\nend VMs increase reliance on offloading.", "metadata": {}}, {"text": "• Difficulty of SLO-Based Optimization: High-\nperformance VMs (e.g., g6.48xlarge) solve the Out-\nof-Memory (OoM) problem but may lead to GPU\nutilization dropping below 50% when the inference load\nis low, resulting in wasted costs.", "metadata": {}}, {"text": "On the other hand,\nlower-end VMs (e.g., g4ad.xlarge) have lower initial costs\nbut suffer from frequent KV Cache Offloading due to\nVRAM limitations, causing latency to increase by more\nthan double [ 9].", "metadata": {}}, {"text": "This results in a dilemma of (i) resource\nwastage with high-cost VM selection, and (ii) performance\ndegradation with low-cost VM selection.", "metadata": {}}, {"text": "• Lack of Automated Optimization Systems: Currently,\nthere is a lack of guidelines for automating the selection of\nVMs and KV Cache Offloading in cloud environments.", "metadata": {}}, {"text": "Users\nmust manually test various VMs (e.g., g5 vs.", "metadata": {}}, {"text": "g6 series) and\noffloading settings, which increases time and cost burdens.", "metadata": {}}, {"text": "This study proposes the necessity of a framework that auto-\nmatically recommends optimal VM and KV Cache Offloading\nstrategies based on SLO, and introduces a model (Solver) that\ncan balance cost and performance.", "metadata": {}}, {"text": "(b)\tg4dn.xlarge\twith\tOPT2.7B\tmodel\n(a)\tg4dn.xlarge\twith\tOPT1.3B\tmodel\nKV\tCache\tSize\nKV\tCache\tSize\t(GB)\n0\n1\n2\n3\n4\n5\n6\nBatch\tSize\n2\n4\n8\n16\n32\nKV\tCache\tSize\n0\n2\n4\n6\n8\nBatch\tSize\n2\n4\n8\n16\n32\nFig.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "Analysis of KV Cache size growth across different models in response\nto increasing batch sizes.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Existing Approaches and Their Limitations\nExisting research aiming to optimize LLM inference in\ncloud environments [ 16, 17, 18, 19] reveals limitations in\nachieving cost-efficient LLM serving as they do not consider\nan integrated approach to KV Cache Offloading and VM\nselection.", "metadata": {}}, {"text": "Melange [16] proposes an allocation strategy that minimizes\ncost by mixing different GPU types (e.g., high-performance\nGPUs and low-cost GPUs) based on LLM service charac-\nteristics such as request size, frequency, and Service Level\nObjectives (SLOs, e.g., response time within 200ms).", "metadata": {}}, {"text": "However,\nthis method relies on profiling GPU performance and the\nworkload pattern in advance, making it difficult to apply\nto new environments or models.", "metadata": {}}, {"text": "Furthermore, it does not\naccount for KV Cache Offloading, failing to provide optimiza-\ntion solutions in memory-constrained scenarios (e.g., when\nsequence length exceeds 4096).", "metadata": {}}, {"text": "Aladdin [17] suggests a framework for jointly optimizing\nrequest batches and resource scaling to meet SLOs.", "metadata": {}}, {"text": "For\ninstance, it adds additional GPUs to reduce latency at high\nrequest rates.", "metadata": {}}, {"text": "However, it does not integrate the memory-\nsaving effects of KV Cache Offloading or the trade-offs\nbetween different GPU types, which limits the flexibility in\nVM configuration.", "metadata": {}}, {"text": "SplitWise [ 19] and ThunderServe [ 18] utilize a Phase\nSplitting strategy, separating the Prefill (initial token gen-\neration) and Decode (subsequent token generation) stages.", "metadata": {}}, {"text": "These approaches allocate specialized GPUs to each stage\n(e.g., high-speed GPUs for Prefill and memory-centric GPUs\nfor Decode) to enhance efficiency.", "metadata": {}}, {"text": "However, this method is\nonly effective in environments where the two stages can be\nphysically separated, making it difficult to apply to standard\nsingle-VM-based LLM serving.", "metadata": {}}, {"text": "Additionally, transferring KV\nCache between stages requires high-speed interconnects (e.g.,\nNVLink, with GPU-to-GPU bandwidth above 100GB/s), which\nreduces practicality in cloud VMs without NVLink (e.g., AWS\ng4dn series).", "metadata": {}}, {"text": "Meanwhile, DeepVM [ 12], which deals with deep learning\noptimization in cloud environments, focuses on optimizing\nVM clusters for checkpoint-based distributed CNN training.", "metadata": {}}, {"text": "For example, it reduces costs by leveraging saved states during\ntraining interruptions.", "metadata": {}}, {"text": "However, this method is tailored for", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "training and is not directly applicable to real-time inference\nor KV Cache management in LLM serving.\nIII. Problem Definition\nA. Definition of Service Level Objective (SLO) Metrics\nIn cloud environments, large language model (LLM) in-\nference involves a complex trade-off between memory con-\nstraints, cost, and service quality. Depending on the type\nof inference task, users may have different Service Level\nObjectives (SLOs).\nIn this paper, we define two types of inference tasks: Online\nInference and Offline Inference.\n• Online Inference (e.g., chatbots, voice assistants) pri-\noritizes low response latency (e.g., within 100ms) over\nquery throughput, as real-time responsiveness is crucial.\nThus, response time is used as the primary SLO metric.\n• Offline Inference (e.g., batch processing of large\ndatasets) prioritizes high query throughput over response\nlatency, making throughput the primary SLO metric.\nTo encompass both of these metrics under a unified\nframework, we define Tokens Per Second (TPS) as the SLO\nmetric. TPS represents the number of tokens processed per\nsecond, including both input tokens ( Lin) and output tokens\n(Lout).\nLLM inference is typically performed in batches, where a\nbatch consists of multiple queries ( BS). Given that the total\nprocessing time for a batch is denoted as TE2E, TPS is defined\nas follows:\nTPS = BS × (Lin + Lout)\nTE2E\n(1)\nB. Definition of Cost Efficiency\nIn this study, our primary objective is to minimize user\ncosts while ensuring that inference tasks meet their designated\nSLOs. To achieve this, we define a cost efficiency metric based\non the previously introduced Tokens Per Second (TPS) metric.\nLet TPSSLO denote the target TPS required by the user\nto meet the SLO, and let TPSactual represent the actual\nthroughput achieved during inference. Considering that the\neffective processing rate cannot exceed the user-defined\nSLO threshold, the effective TPS is defined as: TPSeffective =\nmin(TPSactual, TPSSLO)\nGiven this, the total time required to process a batch of\nqueries, denoted as Ttask, is calculated as:\nTtask = BS × (Lin + Lout)\nTPSeffective × 3600 (2)\nIn cloud environments, GPU usage is typically billed on an\nhourly basis. Therefore, we apply a ceiling function to Ttask\nto account for the actual billable time.\nBased on this, we define SLO-based cost efficiency (CE) as\na metric to evaluate the cost-effectiveness of a given inference\ntask while ensuring compliance with the SLO. Let VM Price\nrepresent the hourly cost of the virtual machine (in dollars\nper hour). The cost efficiency metric is then defined as:\nCEtask = TPSeffective × 3600\n⌈Ttask⌉ × VM Price (3)\nThis metric provides a quantitative measure of how effi-\nciently a system meets the required SLO while optimizing\ncosts in a cloud-based inference environment.\nC. Preliminary Results\nAs shown in Table I in Section II, cloud VM instances exhibit\nsignificant differences in both performance and cost. This\nvariability makes it challenging for users to select the most\ncost-efficient instance for LLM inference tasks. To validate\nthe complexity of this decision-making process, we evaluated\nthe Cost Efficiency (CE) of two representative VM instances\n(g4dn.xlarge and g5.xlarge) under different batch sizes and\nSLO requirements. The experiments were conducted for both\ncases: with and without KV Cache offloading, assessing its\nimpact on cost efficiency. The results of these experiments\nare quantitatively presented in Fig. 2.\nIn a strict SLO environment (100 TPS) , g5.xlarge\ndemonstrated higher cost efficiency than g4dn.xlarge even\nat small batch sizes (Batch Size < 16). This is because\ng5.xlarge delivers higher performance under high-throughput\nrequirements, allowing it to maintain superior cost efficiency\nover g4dn.xlarge even at smaller batch sizes. At Batch Size 16,\ng4dn.xlarge faced GPU memory constraints, necessitating KV\nCache offloading, which further reduced its cost efficiency. In\ncontrast, g5.xlarge had sufficient memory to operate without\noffloading, maintaining consistently high cost efficiency as\nthe batch size increased.\nIn a relaxed SLO environment (10 TPS) , g4dn.xlarge\nexhibited higher cost efficiency than g5.xlarge at smaller batch\nsizes (Batch Size < 16). This is because, under relaxed SLO\nconditions, instance cost became a more critical factor than\nraw performance. At Batch Size 16, despite g4dn.xlarge re-\nquiring KV Cache offloading due to GPU memory limitations,\nthe performance degradation caused by offloading was not a\nmajor issue under the relaxed SLO constraints. As a result,\ng4dn.xlarge, with its lower instance cost, achieved higher cost\nefficiency compared to g5.xlarge.\nTo sum up, cost efficiency varies significantly depending\non SLO settings and GPU memory utilization strategies,\ndemonstrating that using a high-performance GPU is not\nalways the optimal choice. Particularly in offline inference\ntasks, where response time constraints are less stringent, KV\nCache offloading techniques can enable cost-efficient inference\neven on lower-cost GPUs. These findings highlight that the\noptimal GPU instance selection depends on the user’s SLO\nrequirements and the characteristics of the inference task.\nIV. Design of InferSave\nA. InferSave: A Cost-Efficient VM Selection Framework\nSelecting a cost-efficient VM instance in a cloud environ-\nment is a challenging task for users. To address this issue,\nwe propose InferSave, a software tool designed to assist", "sentences": [{"text": "training and is not directly applicable to real-time inference\nor KV Cache management in LLM serving.", "metadata": {}}, {"text": "III.", "metadata": {}}, {"text": "Problem Definition\nA.", "metadata": {}}, {"text": "Definition of Service Level Objective (SLO) Metrics\nIn cloud environments, large language model (LLM) in-\nference involves a complex trade-off between memory con-\nstraints, cost, and service quality.", "metadata": {}}, {"text": "Depending on the type\nof inference task, users may have different Service Level\nObjectives (SLOs).", "metadata": {}}, {"text": "In this paper, we define two types of inference tasks: Online\nInference and Offline Inference.", "metadata": {}}, {"text": "• Online Inference (e.g., chatbots, voice assistants) pri-\noritizes low response latency (e.g., within 100ms) over\nquery throughput, as real-time responsiveness is crucial.", "metadata": {}}, {"text": "Thus, response time is used as the primary SLO metric.", "metadata": {}}, {"text": "• Offline Inference (e.g., batch processing of large\ndatasets) prioritizes high query throughput over response\nlatency, making throughput the primary SLO metric.", "metadata": {}}, {"text": "To encompass both of these metrics under a unified\nframework, we define Tokens Per Second (TPS) as the SLO\nmetric.", "metadata": {}}, {"text": "TPS represents the number of tokens processed per\nsecond, including both input tokens ( Lin) and output tokens\n(Lout).", "metadata": {}}, {"text": "LLM inference is typically performed in batches, where a\nbatch consists of multiple queries ( BS).", "metadata": {}}, {"text": "Given that the total\nprocessing time for a batch is denoted as TE2E, TPS is defined\nas follows:\nTPS = BS × (Lin + Lout)\nTE2E\n(1)\nB.", "metadata": {}}, {"text": "Definition of Cost Efficiency\nIn this study, our primary objective is to minimize user\ncosts while ensuring that inference tasks meet their designated\nSLOs.", "metadata": {}}, {"text": "To achieve this, we define a cost efficiency metric based\non the previously introduced Tokens Per Second (TPS) metric.", "metadata": {}}, {"text": "Let TPSSLO denote the target TPS required by the user\nto meet the SLO, and let TPSactual represent the actual\nthroughput achieved during inference.", "metadata": {}}, {"text": "Considering that the\neffective processing rate cannot exceed the user-defined\nSLO threshold, the effective TPS is defined as: TPSeffective =\nmin(TPSactual, TPSSLO)\nGiven this, the total time required to process a batch of\nqueries, denoted as Ttask, is calculated as:\nTtask = BS × (Lin + Lout)\nTPSeffective × 3600 (2)\nIn cloud environments, GPU usage is typically billed on an\nhourly basis.", "metadata": {}}, {"text": "Therefore, we apply a ceiling function to Ttask\nto account for the actual billable time.", "metadata": {}}, {"text": "Based on this, we define SLO-based cost efficiency (CE) as\na metric to evaluate the cost-effectiveness of a given inference\ntask while ensuring compliance with the SLO.", "metadata": {}}, {"text": "Let VM Price\nrepresent the hourly cost of the virtual machine (in dollars\nper hour).", "metadata": {}}, {"text": "The cost efficiency metric is then defined as:\nCEtask = TPSeffective × 3600\n⌈Ttask⌉ × VM Price (3)\nThis metric provides a quantitative measure of how effi-\nciently a system meets the required SLO while optimizing\ncosts in a cloud-based inference environment.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Preliminary Results\nAs shown in Table I in Section II, cloud VM instances exhibit\nsignificant differences in both performance and cost.", "metadata": {}}, {"text": "This\nvariability makes it challenging for users to select the most\ncost-efficient instance for LLM inference tasks.", "metadata": {}}, {"text": "To validate\nthe complexity of this decision-making process, we evaluated\nthe Cost Efficiency (CE) of two representative VM instances\n(g4dn.xlarge and g5.xlarge) under different batch sizes and\nSLO requirements.", "metadata": {}}, {"text": "The experiments were conducted for both\ncases: with and without KV Cache offloading, assessing its\nimpact on cost efficiency.", "metadata": {}}, {"text": "The results of these experiments\nare quantitatively presented in Fig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "In a strict SLO environment (100 TPS) , g5.xlarge\ndemonstrated higher cost efficiency than g4dn.xlarge even\nat small batch sizes (Batch Size < 16).", "metadata": {}}, {"text": "This is because\ng5.xlarge delivers higher performance under high-throughput\nrequirements, allowing it to maintain superior cost efficiency\nover g4dn.xlarge even at smaller batch sizes.", "metadata": {}}, {"text": "At Batch Size 16,\ng4dn.xlarge faced GPU memory constraints, necessitating KV\nCache offloading, which further reduced its cost efficiency.", "metadata": {}}, {"text": "In\ncontrast, g5.xlarge had sufficient memory to operate without\noffloading, maintaining consistently high cost efficiency as\nthe batch size increased.", "metadata": {}}, {"text": "In a relaxed SLO environment (10 TPS) , g4dn.xlarge\nexhibited higher cost efficiency than g5.xlarge at smaller batch\nsizes (Batch Size < 16).", "metadata": {}}, {"text": "This is because, under relaxed SLO\nconditions, instance cost became a more critical factor than\nraw performance.", "metadata": {}}, {"text": "At Batch Size 16, despite g4dn.xlarge re-\nquiring KV Cache offloading due to GPU memory limitations,\nthe performance degradation caused by offloading was not a\nmajor issue under the relaxed SLO constraints.", "metadata": {}}, {"text": "As a result,\ng4dn.xlarge, with its lower instance cost, achieved higher cost\nefficiency compared to g5.xlarge.", "metadata": {}}, {"text": "To sum up, cost efficiency varies significantly depending\non SLO settings and GPU memory utilization strategies,\ndemonstrating that using a high-performance GPU is not\nalways the optimal choice.", "metadata": {}}, {"text": "Particularly in offline inference\ntasks, where response time constraints are less stringent, KV\nCache offloading techniques can enable cost-efficient inference\neven on lower-cost GPUs.", "metadata": {}}, {"text": "These findings highlight that the\noptimal GPU instance selection depends on the user’s SLO\nrequirements and the characteristics of the inference task.", "metadata": {}}, {"text": "IV.", "metadata": {}}, {"text": "Design of InferSave\nA.", "metadata": {}}, {"text": "InferSave: A Cost-Efficient VM Selection Framework\nSelecting a cost-efficient VM instance in a cloud environ-\nment is a challenging task for users.", "metadata": {}}, {"text": "To address this issue,\nwe propose InferSave, a software tool designed to assist", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Offloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(100\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\n2\n2.5\nBatch\tSize\n1\n2\n4\n8\n16\nOffloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(10\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\nBatch\tSize\n1\n2\n4\n8\n16\nFig. 2. Comparison of cost efficiency per GPU instance based on SLO\nconstraints and batch size, based on experimental results using the OPT-2.7B\nmodel on an AWS g4dn.xlarge instance with an input length of 512 tokens\nand an output length of 128 tokens.\nTABLE II\nNotation and Formulas for Model and Memory Computation\nUser Input Parameters\nVariable Description and Formula\nBS Batch size\nLin Input token length\nLout Output token length\nPmax User max price willingness\nTPSSLO User SLO Requirement\nModel Parameters\nVariable Description and Formula\nh1 Hidden Size (model dimension)\nh2 Intermediate Size (projection)\nnh Number of Attention Heads\nL Transformer layers\nCoff KV cache offloading ratio\nPrecisionbytes Bytes per parameter (e.g., FP16 = 2B)\nMemmodel(Model Size) Number of Model Parameters·Precisionbytes\nMemKVcache(KV Cache Size) 2· BS· (Lin +Lout) · nh· Precisionbytes· L\nMemKVcache, per_layer KV Cache per layer:MemKVcache\nL\nMemactivation(Activation Size) 2· (Lin +Lout) · BS· h1\nInstance Specifications\nVariable Description and Formula\nFLOPSGPU GPU’s theoretical FLOPS\nBWgpu→cpu Bandwidth for GPU-to-CPU data transfer\nBWcpu→gpu Bandwidth for CPU-to-GPU data transfer\nusers in making cost-efficient VM selections. The InferSave\nframework operates in the following four stages:\n1) Stage 1 Requirement Analysis and Parameter Ex-\ntraction: The user provides input parameters, including\ncost constraints, model characteristics, and performance\nrequirements.\n2) Stage 2 Resource Suitability Assessment and Can-\ndidate Instance Identification : Based on the provided\nparameters, the framework calculates the required memory\ncapacity, analyzes the feasibility of KV Cache offloading,\nand identifies a set of suitable GPU instance candidates.\n3) Stage 3 Performance-Cost Prediction Modeling : Lever-\naging pre-profiled performance data, the framework pre-\ndicts the TPS of each candidate GPU instance and evaluates\nits cost efficiency.\n4) Stage 4 SLO-Based Optimization and Instance Selec-\ntion: The framework recommends the most cost-efficient\nGPU instance that satisfies the SLO constraints.\nB. Requirement Analysis and Parameter Extraction\nThis stage involves collecting key input parameters nec-\nessary for LLM inference tasks. The most critical parameter\nis the maximum willingness-to-pay price ( Pmax), which\nrepresents the maximum cost ($/hour) that the user is willing\nto pay. This value serves as a fundamental constraint in the\nsubsequent stages of the algorithm, determining the range of\nGPU instances that can be considered.\nAdditionally, the user specifies the target LLM model\n(e.g., OPT-2.7B, LLaMA-7B), and based on this selection, the\nsystem automatically extracts key model parameters such\nas model size, number of attention heads, head dimensions,\nfeed-forward network (FFN) dimensions, and activation size.\nOther essential input parameters include the average input\ntoken length, average output token length, batch size, and\nthe required SLO in terms of TPS (Tokens Per Second).\nThis stage plays a crucial role in transforming user\nrequirements into quantitative parameters, establishing the\nfoundation for resource suitability assessment and perfor-\nmance prediction. Ultimately, it is essential for selecting the\nmost cost-efficient GPU instance that meets both performance\nobjectives and budget constraints.\nC. Resource Suitability Assessment and Candidate Instance\nIdentification\nAt this stage, the system evaluates the memory require-\nments for inference based on the collected user parameters\nand assesses the feasibility of KV Cache Offloading to identify\nthe most suitable GPU instance candidates. First, the system\ncalculates the total memory requirement Memtotal for the\ngiven Transformer-based LLM model and its input-output\nparameters. This is defined as the sum of the following\nthree components: Memtotal = Memmodel + Memactivation +\nMemKVcache. Additionally, the base memory requirement is\ndefined as: Membase = Memmodel +Memactivation. These stages\nfollow three key criteria to evaluate GPU instance suitability\nand Algorithm 1.\nCase1) No Offloading Required: If the available GPU\nmemory is greater than or equal to the total memory re-\nquirement, i.e., GPUimemory ≥ Memtotal then the instance can\nfully accommodate the model without KV Cache Offloading.\nHere, i refers to the current particular running instance. In\nthis case, the offloading coefficient is set to Ci\noff = 0 and the\ninstance is added to the candidate pool.\nCase2) Offloading Not Feasible: An instance is deemed\nunsuitable if it meets any of the following conditions:\n• If the available GPU memory is smaller than the model\nweights: GPU imemory < Memmodel.\n• If the KV Cache size per layer exceeds the available\nmemory: Mem KVcache, per_layer > Memi\navail.\nThis condition arises because attention operations are per-\nformed on the GPU, requiring KV Cache to remain in GPU\nmemory. When the available memory is insufficient, an Out\nof Memory (OOM) error occurs, preventing execution.\nCase3) KV Cache Offloading Required: If an instance\ndoes not fall into either of the previous categories, KV Cache\nOffloading is required. In this case, the offloading coefficient\nis computed as: Ci\noff = 1 – Memi\navail\nMemKVcache\nFinally, the selected instances are sorted in ascending order\nbased on cost, and the results are used as input for the", "sentences": [{"text": "Offloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(100\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\n2\n2.5\nBatch\tSize\n1\n2\n4\n8\n16\nOffloading\nKV\tCache\nNormalized\tCost\tEfficiency\t(10\tTPS)\ng4dn.xlarge\ng5.xlarge\nNormalized\tCost\tEfficienty\t(Token/$)\n0\n0.5\n1\n1.5\nBatch\tSize\n1\n2\n4\n8\n16\nFig.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Comparison of cost efficiency per GPU instance based on SLO\nconstraints and batch size, based on experimental results using the OPT-2.7B\nmodel on an AWS g4dn.xlarge instance with an input length of 512 tokens\nand an output length of 128 tokens.", "metadata": {}}, {"text": "TABLE II\nNotation and Formulas for Model and Memory Computation\nUser Input Parameters\nVariable Description and Formula\nBS Batch size\nLin Input token length\nLout Output token length\nPmax User max price willingness\nTPSSLO User SLO Requirement\nModel Parameters\nVariable Description and Formula\nh1 Hidden Size (model dimension)\nh2 Intermediate Size (projection)\nnh Number of Attention Heads\nL Transformer layers\nCoff KV cache offloading ratio\nPrecisionbytes Bytes per parameter (e.g., FP16 = 2B)\nMemmodel(Model Size) Number of Model Parameters·Precisionbytes\nMemKVcache(KV Cache Size) 2· BS· (Lin +Lout) · nh· Precisionbytes· L\nMemKVcache, per_layer KV Cache per layer:MemKVcache\nL\nMemactivation(Activation Size) 2· (Lin +Lout) · BS· h1\nInstance Specifications\nVariable Description and Formula\nFLOPSGPU GPU’s theoretical FLOPS\nBWgpu→cpu Bandwidth for GPU-to-CPU data transfer\nBWcpu→gpu Bandwidth for CPU-to-GPU data transfer\nusers in making cost-efficient VM selections.", "metadata": {}}, {"text": "The InferSave\nframework operates in the following four stages:\n1) Stage 1 Requirement Analysis and Parameter Ex-\ntraction: The user provides input parameters, including\ncost constraints, model characteristics, and performance\nrequirements.", "metadata": {}}, {"text": "2) Stage 2 Resource Suitability Assessment and Can-\ndidate Instance Identification : Based on the provided\nparameters, the framework calculates the required memory\ncapacity, analyzes the feasibility of KV Cache offloading,\nand identifies a set of suitable GPU instance candidates.", "metadata": {}}, {"text": "3) Stage 3 Performance-Cost Prediction Modeling : Lever-\naging pre-profiled performance data, the framework pre-\ndicts the TPS of each candidate GPU instance and evaluates\nits cost efficiency.", "metadata": {}}, {"text": "4) Stage 4 SLO-Based Optimization and Instance Selec-\ntion: The framework recommends the most cost-efficient\nGPU instance that satisfies the SLO constraints.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Requirement Analysis and Parameter Extraction\nThis stage involves collecting key input parameters nec-\nessary for LLM inference tasks.", "metadata": {}}, {"text": "The most critical parameter\nis the maximum willingness-to-pay price ( Pmax), which\nrepresents the maximum cost ($/hour) that the user is willing\nto pay.", "metadata": {}}, {"text": "This value serves as a fundamental constraint in the\nsubsequent stages of the algorithm, determining the range of\nGPU instances that can be considered.", "metadata": {}}, {"text": "Additionally, the user specifies the target LLM model\n(e.g., OPT-2.7B, LLaMA-7B), and based on this selection, the\nsystem automatically extracts key model parameters such\nas model size, number of attention heads, head dimensions,\nfeed-forward network (FFN) dimensions, and activation size.", "metadata": {}}, {"text": "Other essential input parameters include the average input\ntoken length, average output token length, batch size, and\nthe required SLO in terms of TPS (Tokens Per Second).", "metadata": {}}, {"text": "This stage plays a crucial role in transforming user\nrequirements into quantitative parameters, establishing the\nfoundation for resource suitability assessment and perfor-\nmance prediction.", "metadata": {}}, {"text": "Ultimately, it is essential for selecting the\nmost cost-efficient GPU instance that meets both performance\nobjectives and budget constraints.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Resource Suitability Assessment and Candidate Instance\nIdentification\nAt this stage, the system evaluates the memory require-\nments for inference based on the collected user parameters\nand assesses the feasibility of KV Cache Offloading to identify\nthe most suitable GPU instance candidates.", "metadata": {}}, {"text": "First, the system\ncalculates the total memory requirement Memtotal for the\ngiven Transformer-based LLM model and its input-output\nparameters.", "metadata": {}}, {"text": "This is defined as the sum of the following\nthree components: Memtotal = Memmodel + Memactivation +\nMemKVcache.", "metadata": {}}, {"text": "Additionally, the base memory requirement is\ndefined as: Membase = Memmodel +Memactivation.", "metadata": {}}, {"text": "These stages\nfollow three key criteria to evaluate GPU instance suitability\nand Algorithm 1.", "metadata": {}}, {"text": "Case1) No Offloading Required: If the available GPU\nmemory is greater than or equal to the total memory re-\nquirement, i.e., GPUimemory ≥ Memtotal then the instance can\nfully accommodate the model without KV Cache Offloading.", "metadata": {}}, {"text": "Here, i refers to the current particular running instance.", "metadata": {}}, {"text": "In\nthis case, the offloading coefficient is set to Ci\noff = 0 and the\ninstance is added to the candidate pool.", "metadata": {}}, {"text": "Case2) Offloading Not Feasible: An instance is deemed\nunsuitable if it meets any of the following conditions:\n• If the available GPU memory is smaller than the model\nweights: GPU imemory < Memmodel.", "metadata": {}}, {"text": "• If the KV Cache size per layer exceeds the available\nmemory: Mem KVcache, per_layer > Memi\navail.", "metadata": {}}, {"text": "This condition arises because attention operations are per-\nformed on the GPU, requiring KV Cache to remain in GPU\nmemory.", "metadata": {}}, {"text": "When the available memory is insufficient, an Out\nof Memory (OOM) error occurs, preventing execution.", "metadata": {}}, {"text": "Case3) KV Cache Offloading Required: If an instance\ndoes not fall into either of the previous categories, KV Cache\nOffloading is required.", "metadata": {}}, {"text": "In this case, the offloading coefficient\nis computed as: Ci\noff = 1 – Memi\navail\nMemKVcache\nFinally, the selected instances are sorted in ascending order\nbased on cost, and the results are used as input for the", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Algorithm 1: Resource Suitability Evaluation and\nInstance Selection (Price Priority)\nInput: Memory Requirements:\nMemmodel — Model weight memory requirement\nMemactivation — Activation memory requirement\nMemKVcache — Total KV Cache memory requirement\nMemKVcache, per_layer — KV Cache memory per layer\nFor each GPU instance i:\nGPUi\nmemory — Total GPU memory\nGPUi\nprice — GPU price\nUser-defined maximum price: Pmax\nOutput: GPU candidates that satisfy both price and resource conditions\n1 Candidates ← ∅ // Initialize candidate set\n2 Memtotal ← Memmodel + Memactivation + MemKVcache ;\n3 Membase ← Memmodel + Memactivation ;\n4 for each GPU instance i do\n5 if GPUi\nprice ≤ Pmax then\n// Apply Price Filter\n6 Memi\navail ← GPUi\nmemory – Membase // Calculate Available\nMemory\n7 if GPUi\nmemory ≥ Memtotal then\n// Offloading Not Required\n8 Ci\noff ← 0 ;\n9 Add (i, Ci\noff) to Candidate Set ;\n10 else\n11 if GPUi\nmemory < Memmodel OR MemKVcache, per_layer > Memi\navail\nthen\n// Offloading Not Possible\n12 Mark GPU i as Unsuitable // Exclude from\ncandidates\n13 end\n14 else\n// KV Cache Offloading Required\n15 Ci\noff ← 1 –\nMemi\navail\nMemKVcache\n;\n16 if MemKVcache, per_layer ≤ Memi\navail then\n// Layer-Level Constraint Check\n17 Add (i, Ci\noff) to Candidate Set ;\n18 end\n19 else\n20 Mark GPU i as Unsuitable // Exclude from\ncandidates\n21 end\n22 end\n23 end\n24 end\n25 end\n26 Sort Candidate Set by GPU i\nprice in ascending order ;\n27 return Candidate Set Candidates ;\nperformance-cost prediction modeling stage. This systematic\napproach ensures that the most cost-efficient GPU instance is\nselected within the user’s budget while accurately evaluating\nthe feasibility and cost-efficiency of KV Cache Offloading.\nD. Instance Performance Prediction\nAt this stage, the system predicts Tokens Per Second (TPS)\nfor the candidate GPU instances identified in the previous\nstep. This is achieved through mathematical modeling that\nleverages model parameters, hardware profiling information\n(FLOPS, bandwidth, etc.) of each candidate instance, and\nthe offloading coefficient to quantitatively estimate the task\nprocessing time.\nThe total task processing time Ttask consists of the Prefill\nand Decode stages and is calculated as follows [6]:\n• Prefill Stage: This stage processes the entire input se-\nquence. The processing time per layer ( Tpre) is multiplied\nby the number of layers ( n).\n• Decode Stage: This stage generates each output token\nsequentially. The processing time per layer ( Tdec) is\nmultiplied by the number of layers ( n) and the number\nof generated tokens (Lout –1), since the first output token\nis already processed in the Prefill stage.\nThus, the total task processing time Ttask is expressed as\nfollows:\nTtask = Tpre · n| {z }\nPrefill Time\n+ Tdec · n · (Lout – 1)| {z }\nDecode Time\n(4)\na) Prefill Stage Processing Time: The Prefill stage pro-\ncessing time Tpre consists of computation time and KV Cache\nstorage time. Since GPU computation and KV Cache offloading\noccur in parallel, the total delay is determined by the process\nwith the longest execution time:\nTpre = max\n\u0010\nCTCF(Tp\ncompute), Tp\ntrans\n\u0011\n(5)\nThe computation time Tp\ncompute in the Prefill stage is divided\ninto Linear Layer computation and Self-Attention computation,\nboth of which are calculated by dividing the required floating-\npoint operations (FLOPs) by the GPU’s theoretical FLOPS\ncapacity:\nTp\ncompute = BS · (8Lin · h2\n1 + 4Lin · h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · L2\nin · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tp\ntrans in the Prefill stage\nrepresents the time required to offload the generated KV\nCache from GPU to CPU memory and is computed as:\nTp\ntrans = Coff · {2 · (Lin + 1) · h1 · Precisionbytes} · BS\nBWgpu→cpu\nb) Decode Stage Processing Time: The Decode stage\nprocessing time Tdec includes computation time and KV Cache\nretrieval time. If the KV Cache fully resides in the GPU, only\ncomputation time is considered. However, if offloading occurs,\nadditional latency is introduced due to data transfer from\nCPU to GPU. The Decode time is therefore expressed as:\nTdec = CTCF(Td\ncompute) + Td\ntrans (6)\nThe Decode computation time Tdcompute consists of Linear\nLayer and Self-Attention computation, and is computed as\nfollows:\nTd\ncompute = BS · (8h2\n1 + 4h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · (Lin + Lout\n2 ) · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tdtrans in the Decode stage refers\nto the time required to load KV Cache stored in CPU memory\nback into GPU memory and is computed as:\nTd\ntrans = (Coff · 2 · (Lin + 1) + Lout) · h1 · Precisionbytes · BS\nBWcpu→gpu", "sentences": [{"text": "Algorithm 1: Resource Suitability Evaluation and\nInstance Selection (Price Priority)\nInput: Memory Requirements:\nMemmodel — Model weight memory requirement\nMemactivation — Activation memory requirement\nMemKVcache — Total KV Cache memory requirement\nMemKVcache, per_layer — KV Cache memory per layer\nFor each GPU instance i:\nGPUi\nmemory — Total GPU memory\nGPUi\nprice — GPU price\nUser-defined maximum price: Pmax\nOutput: GPU candidates that satisfy both price and resource conditions\n1 Candidates ← ∅ // Initialize candidate set\n2 Memtotal ← Memmodel + Memactivation + MemKVcache ;", "metadata": {}}, {"text": "3 Membase ← Memmodel + Memactivation ;", "metadata": {}}, {"text": "4 for each GPU instance i do\n5 if GPUi\nprice ≤ Pmax then\n// Apply Price Filter\n6 Memi\navail ← GPUi\nmemory – Membase // Calculate Available\nMemory\n7 if GPUi\nmemory ≥ Memtotal then\n// Offloading Not Required\n8 Ci\noff ← 0 ;", "metadata": {}}, {"text": "9 Add (i, Ci\noff) to Candidate Set ;", "metadata": {}}, {"text": "10 else\n11 if GPUi\nmemory < Memmodel OR MemKVcache, per_layer > Memi\navail\nthen\n// Offloading Not Possible\n12 Mark GPU i as Unsuitable // Exclude from\ncandidates\n13 end\n14 else\n// KV Cache Offloading Required\n15 Ci\noff ← 1 –\nMemi\navail\nMemKVcache\n;", "metadata": {}}, {"text": "16 if MemKVcache, per_layer ≤ Memi\navail then\n// Layer-Level Constraint Check\n17 Add (i, Ci\noff) to Candidate Set ;", "metadata": {}}, {"text": "18 end\n19 else\n20 Mark GPU i as Unsuitable // Exclude from\ncandidates\n21 end\n22 end\n23 end\n24 end\n25 end\n26 Sort Candidate Set by GPU i\nprice in ascending order ;", "metadata": {}}, {"text": "27 return Candidate Set Candidates ;", "metadata": {}}, {"text": "performance-cost prediction modeling stage.", "metadata": {}}, {"text": "This systematic\napproach ensures that the most cost-efficient GPU instance is\nselected within the user’s budget while accurately evaluating\nthe feasibility and cost-efficiency of KV Cache Offloading.", "metadata": {}}, {"text": "D.", "metadata": {}}, {"text": "Instance Performance Prediction\nAt this stage, the system predicts Tokens Per Second (TPS)\nfor the candidate GPU instances identified in the previous\nstep.", "metadata": {}}, {"text": "This is achieved through mathematical modeling that\nleverages model parameters, hardware profiling information\n(FLOPS, bandwidth, etc.) of each candidate instance, and\nthe offloading coefficient to quantitatively estimate the task\nprocessing time.", "metadata": {}}, {"text": "The total task processing time Ttask consists of the Prefill\nand Decode stages and is calculated as follows [6]:\n• Prefill Stage: This stage processes the entire input se-\nquence.", "metadata": {}}, {"text": "The processing time per layer ( Tpre) is multiplied\nby the number of layers ( n).", "metadata": {}}, {"text": "• Decode Stage: This stage generates each output token\nsequentially.", "metadata": {}}, {"text": "The processing time per layer ( Tdec) is\nmultiplied by the number of layers ( n) and the number\nof generated tokens (Lout –1), since the first output token\nis already processed in the Prefill stage.", "metadata": {}}, {"text": "Thus, the total task processing time Ttask is expressed as\nfollows:\nTtask = Tpre · n| {z }\nPrefill Time\n+ Tdec · n · (Lout – 1)| {z }\nDecode Time\n(4)\na) Prefill Stage Processing Time: The Prefill stage pro-\ncessing time Tpre consists of computation time and KV Cache\nstorage time.", "metadata": {}}, {"text": "Since GPU computation and KV Cache offloading\noccur in parallel, the total delay is determined by the process\nwith the longest execution time:\nTpre = max\n\u0010\nCTCF(Tp\ncompute), Tp\ntrans\n\u0011\n(5)\nThe computation time Tp\ncompute in the Prefill stage is divided\ninto Linear Layer computation and Self-Attention computation,\nboth of which are calculated by dividing the required floating-\npoint operations (FLOPs) by the GPU’s theoretical FLOPS\ncapacity:\nTp\ncompute = BS · (8Lin · h2\n1 + 4Lin · h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · L2\nin · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tp\ntrans in the Prefill stage\nrepresents the time required to offload the generated KV\nCache from GPU to CPU memory and is computed as:\nTp\ntrans = Coff · {2 · (Lin + 1) · h1 · Precisionbytes} · BS\nBWgpu→cpu\nb) Decode Stage Processing Time: The Decode stage\nprocessing time Tdec includes computation time and KV Cache\nretrieval time.", "metadata": {}}, {"text": "If the KV Cache fully resides in the GPU, only\ncomputation time is considered.", "metadata": {}}, {"text": "However, if offloading occurs,\nadditional latency is introduced due to data transfer from\nCPU to GPU.", "metadata": {}}, {"text": "The Decode time is therefore expressed as:\nTdec = CTCF(Td\ncompute) + Td\ntrans (6)\nThe Decode computation time Tdcompute consists of Linear\nLayer and Self-Attention computation, and is computed as\nfollows:\nTd\ncompute = BS · (8h2\n1 + 4h1 · h2)\nFLOPSGPU| {z }\nLinear Layer compute time\n+ 4 · BS · (Lin + Lout\n2 ) · h1\nFLOPSGPU| {z }\nSelf-Attention compute time\nThe KV Cache transfer time Tdtrans in the Decode stage refers\nto the time required to load KV Cache stored in CPU memory\nback into GPU memory and is computed as:\nTd\ntrans = (Coff · 2 · (Lin + 1) + Lout) · h1 · Precisionbytes · BS\nBWcpu→gpu", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "g4dn.xlarge\tGiven\tTFLOPS\ng4dn.xlarge\tMeasured\tTFLOPS\ng5.2xlarge\tGiven\tTFLOPS\ng5.2xlarge\tMeasured\tTFLOPS\ng6.xlarge\tGiven\tTFLOPS\ng6.xlarge\tMeasured\tTFLOPS\nTFLOPS\n0\n20\n40\n60\n80\n100\nBatch\tSize\n2\n4\n8\n12\n16\n20\n24\n28\n32\nFig. 3. Comparison of FLOPS provided by the GPU manufacturer (NVIDIA)\nand the actual FLOPS utilized when calculating Prefill time on AWS GPU\nVMs. The results present TFLOPS measurements for three different GPU\nVMs using the OPT-2.7B model with an input size of 512 tokens and an\noutput size of 128 tokens as batch size grows.\nThis modeling approach accounts for both cases where of-\nfloading is necessary and unnecessary, effectively considering\nGPU memory constraints and computational performance.\nBy incorporating both computation latency and KV Cache\noffloading overhead, this approach enables a quantitative\nanalysis of the trade-off between computation and memory\naccess time in both Prefill and Decode stages.\nUsing this modeling framework, Tokens Per Second (TPS)\ncan be estimated, allowing for the selection of the most\noptimal GPU instance for a given inference task. While\nthis theoretical modeling provides a solid foundation, it\nis important to note that GPU manufacturers’ theoretical\nFLOPS values do not always accurately reflect real-world LLM\ninference workloads. The limitations of this approach, along\nwith the Compute Time Calibration Function (CTCF) designed\nto correct these discrepancies, are discussed in Section IV-F.\nE. Step 4: Final Instance Selection Based on SLO\nBased on the TPS (Tokens Per Second) values computed\nfor each GPU instance in the previous stage, this step selects\nthe most cost-efficient instance while ensuring that the user’s\nService Level Objective (SLO) is met. The selection process\nfollows these steps:\nFirst, instances that fail to satisfy the user-defined SLO\nconstraint (TPS ≥ TPSSLO) are eliminated from consideration.\nNext, the cost efficiency metric (Equation 3) is calculated for\neach remaining instance. Finally, the instance with the highest\ncost efficiency is selected. In the event of a tie, the instance\nwith the higher TPS is prioritized.\nThe final selection result is presented to the user along with\ncomprehensive details, including instance type, expected TPS,\ncost, and KV Cache offloading configuration. Additionally,\nthe system provides alternative options and a performance-\ncost trade-off analysis, enabling users to make an informed\ndecision that is optimized for their specific LLM inference\nworkload.\nF. Compute Time Calibration Function (CTCF)\nThe theoretical FLOPS values provided by GPU manufac-\nturers do not accurately reflect real-world performance in\nLLM inference workloads. Figure 3 illustrates the discrepancy\nbetween the FLOPS values advertised by the manufacturer and\nthose actually utilized in computation across three different\nGPU instances. This discrepancy arises from factors such as\nmemory bottlenecks, reduced GPU utilization, and variations\nin computation patterns, which manifest differently in the\nPrefill and Decode stages of LLM inference. As a result,\nselecting a GPU instance solely based on theoretical FLOPS\ncan lead to significant performance mismatches, causing users\nto incur unnecessary costs. To address this issue, it is essential\nto introduce a calibration method that aligns theoretical FLOPS\nvalues with actual computational performance.\n• CTCF Modeling : This study conducted preliminary\nexperiments across various batch sizes to analyze the\nrelationship between LLM inference time and batch size.\nThe results consistently showed a linear increase in\ninference time for both the Prefill and Decode stages.\nThis linear trend was observed across different GPU\narchitectures, including T4, A10G, L4, and L40s, leading\nto the introduction of a regression-based CTCF model.\nCTCF is a linear transformation function that adjusts\ntheoretical computation time to match actual execution time.\nIt is defined as follows:\nCTCF(Tcompute) = α · Tcompute + β (7)\nwhere α is a scaling factor that corrects overestimation or\nunderestimation of theoretical computation time, and β is a\nfixed offset that compensates for systematic delays caused\nby GPU execution bottlenecks, memory access latency, and\nother hardware constraints. These parameters are optimized\nusing the least squares method and are determined through\npre-profiling experiments.\nThrough extensive pre-profiling, α and β values were\ncomputed for all AWS GPU instances across various batch\nsizes and stored as reference data. As shown in Table III,\napplying these per-instance α and β values significantly\nreduces the prediction error, bringing the adjusted execution\ntime very close to the actual measurement. Based on this,\nInferSave profiles α and β values for all available AWS\nGPU instances, enabling precise FLOPS-based execution time\npredictions and recommending the optimal instance for users.\nTABLE III\nV alues ofα, β to calculate adjusted TPrefill (Model: OPT 2.7B)\nInstance Type (GPU Model) α β avg. error rate (%)\ng4dn.xlarge (T4) -0.185 24.35 4.47\ng5.2xlarge (A10G) -0.074 46.97 2.60\ng6.xlarge (L4) -0.1238 42.52 2.23\nThe CTCF-based correction method effectively compensates\nfor the inherent limitations of theoretical FLOPS values\nprovided by GPU manufacturers, leading to more accurate\nLLM inference performance predictions.\nV. Implementation\nWe developed InferSave using Python (3.10.14). For per-\nformance modeling and KV cache offloading optimization, we\nutilized NumPy (1.24.3) for efficient numerical computations", "sentences": [{"text": "g4dn.xlarge\tGiven\tTFLOPS\ng4dn.xlarge\tMeasured\tTFLOPS\ng5.2xlarge\tGiven\tTFLOPS\ng5.2xlarge\tMeasured\tTFLOPS\ng6.xlarge\tGiven\tTFLOPS\ng6.xlarge\tMeasured\tTFLOPS\nTFLOPS\n0\n20\n40\n60\n80\n100\nBatch\tSize\n2\n4\n8\n12\n16\n20\n24\n28\n32\nFig.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Comparison of FLOPS provided by the GPU manufacturer (NVIDIA)\nand the actual FLOPS utilized when calculating Prefill time on AWS GPU\nVMs.", "metadata": {}}, {"text": "The results present TFLOPS measurements for three different GPU\nVMs using the OPT-2.7B model with an input size of 512 tokens and an\noutput size of 128 tokens as batch size grows.", "metadata": {}}, {"text": "This modeling approach accounts for both cases where of-\nfloading is necessary and unnecessary, effectively considering\nGPU memory constraints and computational performance.", "metadata": {}}, {"text": "By incorporating both computation latency and KV Cache\noffloading overhead, this approach enables a quantitative\nanalysis of the trade-off between computation and memory\naccess time in both Prefill and Decode stages.", "metadata": {}}, {"text": "Using this modeling framework, Tokens Per Second (TPS)\ncan be estimated, allowing for the selection of the most\noptimal GPU instance for a given inference task.", "metadata": {}}, {"text": "While\nthis theoretical modeling provides a solid foundation, it\nis important to note that GPU manufacturers’ theoretical\nFLOPS values do not always accurately reflect real-world LLM\ninference workloads.", "metadata": {}}, {"text": "The limitations of this approach, along\nwith the Compute Time Calibration Function (CTCF) designed\nto correct these discrepancies, are discussed in Section IV-F.", "metadata": {}}, {"text": "E.", "metadata": {}}, {"text": "Step 4: Final Instance Selection Based on SLO\nBased on the TPS (Tokens Per Second) values computed\nfor each GPU instance in the previous stage, this step selects\nthe most cost-efficient instance while ensuring that the user’s\nService Level Objective (SLO) is met.", "metadata": {}}, {"text": "The selection process\nfollows these steps:\nFirst, instances that fail to satisfy the user-defined SLO\nconstraint (TPS ≥ TPSSLO) are eliminated from consideration.", "metadata": {}}, {"text": "Next, the cost efficiency metric (Equation 3) is calculated for\neach remaining instance.", "metadata": {}}, {"text": "Finally, the instance with the highest\ncost efficiency is selected.", "metadata": {}}, {"text": "In the event of a tie, the instance\nwith the higher TPS is prioritized.", "metadata": {}}, {"text": "The final selection result is presented to the user along with\ncomprehensive details, including instance type, expected TPS,\ncost, and KV Cache offloading configuration.", "metadata": {}}, {"text": "Additionally,\nthe system provides alternative options and a performance-\ncost trade-off analysis, enabling users to make an informed\ndecision that is optimized for their specific LLM inference\nworkload.", "metadata": {}}, {"text": "F.", "metadata": {}}, {"text": "Compute Time Calibration Function (CTCF)\nThe theoretical FLOPS values provided by GPU manufac-\nturers do not accurately reflect real-world performance in\nLLM inference workloads.", "metadata": {}}, {"text": "Figure 3 illustrates the discrepancy\nbetween the FLOPS values advertised by the manufacturer and\nthose actually utilized in computation across three different\nGPU instances.", "metadata": {}}, {"text": "This discrepancy arises from factors such as\nmemory bottlenecks, reduced GPU utilization, and variations\nin computation patterns, which manifest differently in the\nPrefill and Decode stages of LLM inference.", "metadata": {}}, {"text": "As a result,\nselecting a GPU instance solely based on theoretical FLOPS\ncan lead to significant performance mismatches, causing users\nto incur unnecessary costs.", "metadata": {}}, {"text": "To address this issue, it is essential\nto introduce a calibration method that aligns theoretical FLOPS\nvalues with actual computational performance.", "metadata": {}}, {"text": "• CTCF Modeling : This study conducted preliminary\nexperiments across various batch sizes to analyze the\nrelationship between LLM inference time and batch size.", "metadata": {}}, {"text": "The results consistently showed a linear increase in\ninference time for both the Prefill and Decode stages.", "metadata": {}}, {"text": "This linear trend was observed across different GPU\narchitectures, including T4, A10G, L4, and L40s, leading\nto the introduction of a regression-based CTCF model.", "metadata": {}}, {"text": "CTCF is a linear transformation function that adjusts\ntheoretical computation time to match actual execution time.", "metadata": {}}, {"text": "It is defined as follows:\nCTCF(Tcompute) = α · Tcompute + β (7)\nwhere α is a scaling factor that corrects overestimation or\nunderestimation of theoretical computation time, and β is a\nfixed offset that compensates for systematic delays caused\nby GPU execution bottlenecks, memory access latency, and\nother hardware constraints.", "metadata": {}}, {"text": "These parameters are optimized\nusing the least squares method and are determined through\npre-profiling experiments.", "metadata": {}}, {"text": "Through extensive pre-profiling, α and β values were\ncomputed for all AWS GPU instances across various batch\nsizes and stored as reference data.", "metadata": {}}, {"text": "As shown in Table III,\napplying these per-instance α and β values significantly\nreduces the prediction error, bringing the adjusted execution\ntime very close to the actual measurement.", "metadata": {}}, {"text": "Based on this,\nInferSave profiles α and β values for all available AWS\nGPU instances, enabling precise FLOPS-based execution time\npredictions and recommending the optimal instance for users.", "metadata": {}}, {"text": "TABLE III\nV alues ofα, β to calculate adjusted TPrefill (Model: OPT 2.7B)\nInstance Type (GPU Model) α β avg.", "metadata": {}}, {"text": "error rate (%)\ng4dn.xlarge (T4) -0.185 24.35 4.47\ng5.2xlarge (A10G) -0.074 46.97 2.60\ng6.xlarge (L4) -0.1238 42.52 2.23\nThe CTCF-based correction method effectively compensates\nfor the inherent limitations of theoretical FLOPS values\nprovided by GPU manufacturers, leading to more accurate\nLLM inference performance predictions.", "metadata": {}}, {"text": "V.", "metadata": {}}, {"text": "Implementation\nWe developed InferSave using Python (3.10.14).", "metadata": {}}, {"text": "For per-\nformance modeling and KV cache offloading optimization, we\nutilized NumPy (1.24.3) for efficient numerical computations", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "and statistical analysis. Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities. A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations. Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations. The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.\nVI. Evaluation\nA. Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances. The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input. Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances. All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.\nWorkload Definition: For a holistic evaluation of\nInferSave’s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.\n• Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens. This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers. The workload evaluates a total\nof 3000 requests.\n• Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used. This takes into account tasks\nsuch as document summarization and data wrangling. To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.\nA WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025. To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f. For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.\nTABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.\nInstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite. To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.\nPolicy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing. Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.\n• Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible. However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.\n• Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance. However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.\nB. CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements. To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge. The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128. The model’s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 × 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time. For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).\nAfter applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate. As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements. Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate", "sentences": [{"text": "and statistical analysis.", "metadata": {}}, {"text": "Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities.", "metadata": {}}, {"text": "A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations.", "metadata": {}}, {"text": "Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations.", "metadata": {}}, {"text": "The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.", "metadata": {}}, {"text": "VI.", "metadata": {}}, {"text": "Evaluation\nA.", "metadata": {}}, {"text": "Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances.", "metadata": {}}, {"text": "The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input.", "metadata": {}}, {"text": "Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances.", "metadata": {}}, {"text": "All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.", "metadata": {}}, {"text": "Workload Definition: For a holistic evaluation of\nInferSave’s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.", "metadata": {}}, {"text": "• Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens.", "metadata": {}}, {"text": "This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers.", "metadata": {}}, {"text": "The workload evaluates a total\nof 3000 requests.", "metadata": {}}, {"text": "• Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used.", "metadata": {}}, {"text": "This takes into account tasks\nsuch as document summarization and data wrangling.", "metadata": {}}, {"text": "To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.", "metadata": {}}, {"text": "A WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025.", "metadata": {}}, {"text": "To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f.", "metadata": {}}, {"text": "For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.", "metadata": {}}, {"text": "TABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.", "metadata": {}}, {"text": "InstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite.", "metadata": {}}, {"text": "To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.", "metadata": {}}, {"text": "Policy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing.", "metadata": {}}, {"text": "Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.", "metadata": {}}, {"text": "• Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible.", "metadata": {}}, {"text": "However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.", "metadata": {}}, {"text": "• Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance.", "metadata": {}}, {"text": "However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements.", "metadata": {}}, {"text": "To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge.", "metadata": {}}, {"text": "The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128.", "metadata": {}}, {"text": "The model’s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 × 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time.", "metadata": {}}, {"text": "For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).", "metadata": {}}, {"text": "After applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate.", "metadata": {}}, {"text": "As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements.", "metadata": {}}, {"text": "Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "(d)\tg6.xlarge\tDecode\tStage\n(c)\tg6.xlarge\tPrefill\tStage\n(b)\tg4dn.xlarge\tDecode\tStage\n(a)\tg4dn.xlarge\tPrefill\tStage\n \tCTCF\tAdjusted\tTime\nPredicted\tTime\n Actual\tTime\nError\tRate\t(CTCF\tvs\tActual)\n0\n20\n40\n60\n80\n100\nT Prefil\n0\n2\n4\n6\n8\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\n0\n20\n40\n60\n80\n100\nT Decode\n0\n0.5\n1\n1.5\n2\n2.5\n3\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nFig. 4. CTCF accuracy analysis. The results illustrate the predicted time\n(blue), actual time (red), and CTCF-adjusted values (green) for Prefill and\nDecode times as batch size increases on two different GPU VMs. Additionally,\nthe Error Rate between the CTCF-adjusted time and actual time is presented.\nwas 2%. These results demonstrate that the CTCF-adjusted\ncomputation time aligns well with real-world measurements,\nthereby verifying that InferSave can accurately recommend\nthe most suitable GPU instance for users.\nC. Evaluation results\nTo evaluate the effectiveness of InferSave and our pro-\nposed methodologies, we conducted experiments on both\nonline and offline workloads. While we have performed\ncomprehensive experiments across various model sizes and\nbatch sizes, we have decided to focus on the analysis of\nrepresentative results using the OPT-2.7B model with a batch\nsize of 32. This configuration was chosen as it clearly shows\nthe performance variations of each GPU instance, and also\ndemonstrates a good middle ground of performance and\nresource utilization. We set the maximum cost per hour (Pmax)\nto $3.00/hr. This value was chosen as g6e.xlarge, the most\npowerful instance in our experiments, has an on-demand cost\nof $2.699/hr, and a slightly higher cost than this allows for a\nfair comparison across all instances.\nTABLE V\nComparison of Instance Selection Results by SLO Constraints\n(400 TPS and 600 TPS)\nSLO Evaluated PoliciesSelected InstancesTPS(avg.)Total Cost($)\n400 TPS\nInferSave-1st g4dn.xlarge 620.17 0.71\nInferSave-2nd g6.xlarge 802.19 1.167\nMax-Performance g6e.xlarge 1506.54 2.699\n600 TPS\nInferSave-1st g6.xlarge 800.15 1.167\nInferSave-2nd g5.xlarge 1206.12 1.466\nMax-Performance g6e.xlarge 1505.37 2.699\nInferSave-1st InferSave-2nd Max-Performance0\n200\n400\n600\n800\n1000\n1200\n1400\nSLO 400\nSLO 600\nAverage tokens per second (TPS)\nMinimum SLO\nInferSave-1st InferSave-2nd Max-Performance0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 400\nSLO 600\nFig. 5. Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor online inference workloads (Left: Average TPS, Right: Cost).\nTABLE VI\nComparison of Instance Selection Results by SLO Constraints\n(100 TPS and 200 TPS)\nSLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($)\n100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13InferSave-2nd g6.xlarge 60 415.04 2.344Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699\n200 TPS InferSave-1st g6.xlarge 60 414.28 2.334InferSave-2nd g5.xlarge 60 414.01 2.932Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1505.37 2.699\n1) Online inference workload results: Table V and Figure 5\nshows the instances selected by each policy based on the\nSLO requirements given for an online inference workload,\nas well as the performance and price comparisons. We\nanalyze the first and second selections of InferSave’s policy\nwithin two minimum TPS requirements (400 TPS, 600 TPS),\nand compare it with the selection of the Max-Performance\npolicy’s selection. Note that the results of InferSave without\nKV Cache offloading were the same as Max-Performance’s\nselection, and thus were excluded from Table V. This result\nwas observed as the workload size used in this experiment\nwas sufficiently small, allowing all KV Cache data to be\naccommodated within the GPU memory. Therefore, offloading\nhad no impact on performance, and consequently, there was\nno difference in the selected instances. Additionally, for these\nexperiments, the total runtime did not surpass an hour, leading\nto the hourly cost and the total cost to be the same.\nWith an SLO requirement of 400 TPS, InferSave selected\ng4dn.xlarge as its first choice, and this instance offered the\nlowest cost of $0.71 while providing 620.17 TPS. On the other\nhand, Max-Performance selected g6e.xlarge, which provides\nthe highest performance of 1506.54 TPS, but at a cost of $2.699,\nwhich is about 280% more expensive than InferSave’s top\nchoice. A similar pattern was observed with the 600 TPS SLO\nconstraint, with InferSave’s selection of g6.xlarge meeting\nthe SLO at a 56.75% lower cost than g6e.xlarge.\nThis shows that the instances chosen by the Max-\nPerformance policy overshoots the given SLO requirement\ngreatly, leading to wasted GPU utilization and higher running\ncosts. Meanwhile, InferSave demonstrates optimal instance\nselection by using accurate performance prediction to select\nthe most favorable instance for the given requirements.\n2) Offline inference workload results: Table VI and Figure 6\nshows the instance selection of each policy based on the\nSLO requirements given for an offline inference workload,\nand performance and price comparisons from the selection", "sentences": [{"text": "(d)\tg6.xlarge\tDecode\tStage\n(c)\tg6.xlarge\tPrefill\tStage\n(b)\tg4dn.xlarge\tDecode\tStage\n(a)\tg4dn.xlarge\tPrefill\tStage\n \tCTCF\tAdjusted\tTime\nPredicted\tTime\n Actual\tTime\nError\tRate\t(CTCF\tvs\tActual)\n0\n20\n40\n60\n80\n100\nT Prefil\n0\n2\n4\n6\n8\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\n0\n20\n40\n60\n80\n100\nT Decode\n0\n0.5\n1\n1.5\n2\n2.5\n3\nBatch\tSize\n5\n10\n15\n20\n25\n30\nError\tRate(%)\n0\n20\n40\n60\n80\n100\n−2.5\n0\n2.5\n5\n7.5\n10\nBatch\tSize\n5\n10\n15\n20\n25\n30\nFig.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "CTCF accuracy analysis.", "metadata": {}}, {"text": "The results illustrate the predicted time\n(blue), actual time (red), and CTCF-adjusted values (green) for Prefill and\nDecode times as batch size increases on two different GPU VMs.", "metadata": {}}, {"text": "Additionally,\nthe Error Rate between the CTCF-adjusted time and actual time is presented.", "metadata": {}}, {"text": "was 2%.", "metadata": {}}, {"text": "These results demonstrate that the CTCF-adjusted\ncomputation time aligns well with real-world measurements,\nthereby verifying that InferSave can accurately recommend\nthe most suitable GPU instance for users.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "Evaluation results\nTo evaluate the effectiveness of InferSave and our pro-\nposed methodologies, we conducted experiments on both\nonline and offline workloads.", "metadata": {}}, {"text": "While we have performed\ncomprehensive experiments across various model sizes and\nbatch sizes, we have decided to focus on the analysis of\nrepresentative results using the OPT-2.7B model with a batch\nsize of 32.", "metadata": {}}, {"text": "This configuration was chosen as it clearly shows\nthe performance variations of each GPU instance, and also\ndemonstrates a good middle ground of performance and\nresource utilization.", "metadata": {}}, {"text": "We set the maximum cost per hour (Pmax)\nto $3.00/hr.", "metadata": {}}, {"text": "This value was chosen as g6e.xlarge, the most\npowerful instance in our experiments, has an on-demand cost\nof $2.699/hr, and a slightly higher cost than this allows for a\nfair comparison across all instances.", "metadata": {}}, {"text": "TABLE V\nComparison of Instance Selection Results by SLO Constraints\n(400 TPS and 600 TPS)\nSLO Evaluated PoliciesSelected InstancesTPS(avg.)Total Cost($)\n400 TPS\nInferSave-1st g4dn.xlarge 620.17 0.71\nInferSave-2nd g6.xlarge 802.19 1.167\nMax-Performance g6e.xlarge 1506.54 2.699\n600 TPS\nInferSave-1st g6.xlarge 800.15 1.167\nInferSave-2nd g5.xlarge 1206.12 1.466\nMax-Performance g6e.xlarge 1505.37 2.699\nInferSave-1st InferSave-2nd Max-Performance0\n200\n400\n600\n800\n1000\n1200\n1400\nSLO 400\nSLO 600\nAverage tokens per second (TPS)\nMinimum SLO\nInferSave-1st InferSave-2nd Max-Performance0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 400\nSLO 600\nFig.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor online inference workloads (Left: Average TPS, Right: Cost).", "metadata": {}}, {"text": "TABLE VI\nComparison of Instance Selection Results by SLO Constraints\n(100 TPS and 200 TPS)\nSLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($)\n100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13InferSave-2nd g6.xlarge 60 415.04 2.344Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699\n200 TPS InferSave-1st g6.xlarge 60 414.28 2.334InferSave-2nd g5.xlarge 60 414.01 2.932Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1505.37 2.699\n1) Online inference workload results: Table V and Figure 5\nshows the instances selected by each policy based on the\nSLO requirements given for an online inference workload,\nas well as the performance and price comparisons.", "metadata": {}}, {"text": "We\nanalyze the first and second selections of InferSave’s policy\nwithin two minimum TPS requirements (400 TPS, 600 TPS),\nand compare it with the selection of the Max-Performance\npolicy’s selection.", "metadata": {}}, {"text": "Note that the results of InferSave without\nKV Cache offloading were the same as Max-Performance’s\nselection, and thus were excluded from Table V.", "metadata": {}}, {"text": "This result\nwas observed as the workload size used in this experiment\nwas sufficiently small, allowing all KV Cache data to be\naccommodated within the GPU memory.", "metadata": {}}, {"text": "Therefore, offloading\nhad no impact on performance, and consequently, there was\nno difference in the selected instances.", "metadata": {}}, {"text": "Additionally, for these\nexperiments, the total runtime did not surpass an hour, leading\nto the hourly cost and the total cost to be the same.", "metadata": {}}, {"text": "With an SLO requirement of 400 TPS, InferSave selected\ng4dn.xlarge as its first choice, and this instance offered the\nlowest cost of $0.71 while providing 620.17 TPS.", "metadata": {}}, {"text": "On the other\nhand, Max-Performance selected g6e.xlarge, which provides\nthe highest performance of 1506.54 TPS, but at a cost of $2.699,\nwhich is about 280% more expensive than InferSave’s top\nchoice.", "metadata": {}}, {"text": "A similar pattern was observed with the 600 TPS SLO\nconstraint, with InferSave’s selection of g6.xlarge meeting\nthe SLO at a 56.75% lower cost than g6e.xlarge.", "metadata": {}}, {"text": "This shows that the instances chosen by the Max-\nPerformance policy overshoots the given SLO requirement\ngreatly, leading to wasted GPU utilization and higher running\ncosts.", "metadata": {}}, {"text": "Meanwhile, InferSave demonstrates optimal instance\nselection by using accurate performance prediction to select\nthe most favorable instance for the given requirements.", "metadata": {}}, {"text": "2) Offline inference workload results: Table VI and Figure 6\nshows the instance selection of each policy based on the\nSLO requirements given for an offline inference workload,\nand performance and price comparisons from the selection", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "7300\n7800\nSLO 100\nSLO 200\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n100\n200\n500\nMinimum SLO\nAverage tokens per second (TPS)\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 100\nSLO 200\nFig. 6. Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor offline inference workloads (Left: Average TPS, Right: Cost).\nmade by each policy. As this workload uses a large input\ntoken size, all instances excluding g6e.xlarge make use of\nKV Cache offloading. Without considering offloading, only\none instance can be considered a top choice, and therefore,\nInferSave without offloading chose the same instance as the\nMax-Performance policy.\nGiven a SLO requirement of 100 TPS, InferSave selected\ng4dn.xlarge as its top choice, providing a throughput of about\n160 TPS with the lowest total processing cost of $2.13. On the\nother hand, both Max-Performance and InferSave without\noffloading selected g6e.xlarge, which delivers a very high\nthroughput of about 7600 TPS, but with a total cost of $2.699,\nan increase of about 26.7%. The selection of g6e.xlarge allows\nfor maximum throughput with the ability to store all KV\nCache in GPU memory without offloading. However, despite\nthe high throughput and meeting the SLO, the high cost of\nthe instance itself results in lower overall cost efficiency.\nWith a SLO requirement of 200 TPS, InferSave selected\ng5.xlarge as its top choice, as g4dn.xlarge not longer meet the\nperformance requirements. This instance provides about 400\nTPS while maintaining a total cost of $2.344. On the other\nhand, the Max-Performance policy still selected g6e.xlarge,\nproviding a performance of about 7600 TPS, but the total\ncost increased to $2.699, resulting in about a 15% higher cost.\nThis shows that without considering offloading, a needlessly\nhighly performant and expensive instance can be chosen,\nleading to excessively high costs, and thus reducing actual\ncost efficiency.\n3) Overall analysis and discussion: By evaluating experi-\nmental results that represent both online chatbot and batch\nprocessing workloads, we were able to derive key insights\nfor the efficient operation of LLM inference systems.\n(i) The impact of a workload’s I/O patterns on optimal\ninfrastructure selection: The requirements of online\nconversational chatbot inference and batch processing\ninference differ greatly in input and output token lengths,\nwhich act as key factors in determining optimal instance\nand offloading strategies.\n(ii) The significance of selectively applying KV Cache\noffloading: KV Cache offloading is not a universally\napplicable strategy for all workloads and achieves the\ngreatest cost reduction when selectively utilized ac-\ncording to workload characteristics. In particular, for\noffline batch processing workloads with long inputs,\ncost reductions up to 28% were possible with KV Cache\noffloading, while maintaining SLO requirements. On the\nother hand, in online conversational chatbot workloads,\nit was often more advantageous to apply KV Cache\noffloading when considering both cost and performance.\n(iii) Finding the optimal interface through InferSave:\nInferSave comprehensively considers the SLO require-\nments and workload characteristics to find the optimal\nbalance point in cost and performance. Instead of naively\nselecting the instance with the highest performance,\nInferSave finds the instance with the highest cost\nefficiency while still satisfying the SLO requirement.\nThese results reveal an opportunity for both cost and\nperformance optimization by flexibly adjusting the offloading\nstrategy and GPU instance choice to match workload patterns\nand SLO requirements. InferSave takes this opportunity and\nperforms said optimizations automatically with the selection\nof the optimal instance by considering information from\nprecise analysis of each workload’s characteristics. As a result,\nInferSave achieves optimal performance according to the\ngiven SLO while maintaining cost efficiency.\nHowever, this work currently has limitations in that it\nfocuses on a single GPU environment and does not address\noptimization strategies in multi-GPU or distributed inference\nenvironments. We plan to extend InferSave to multi-GPU\nand distributed cluster environments to develop optimization\nstrategies suitable for inference of more complex workloads\nand large-scale models.\nVII. Conclusion\nIn this study, we propose InferSave, which utilizes SLO-\nbased predictions to automatically select cost-efficient VM\ninstances in the cloud, and validate it across online and offline\nworkloads. We identify opportunities to enhance cost effi-\nciency and utilize cheaper, less powerful GPU instances, while\nmaintaining the specified SLO requirements by exploiting\ntechniques such as KV Cache offloading. Through extensive\nevaluation across both online and offline inference workloads,\nour results confirm that InferSave accurately exploits said\nopportunities, and achieves at most 73.7% lower running costs\nwhile maintaining SLO requirements.\nThis research suggests that LLM service providers can\noptimize cost and performance in a balanced way by selecting\noptimal instances based on SLO and effectively utilizing\noffloading strategies. InferSave offers these optimizations\nin a unified package in a LLM inferencing system that both\nlowers cost and maintains performance requirements.\nReferences\n[1] A. Vaswani, “Attention is all you need, ” Advances in\nNeural Information Processing Systems , 2017.\n[2] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n“Improving language understanding by generative pre-\ntraining, ”OpenAI Preprint , 2018.\n[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,", "sentences": [{"text": "7300\n7800\nSLO 100\nSLO 200\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n100\n200\n500\nMinimum SLO\nAverage tokens per second (TPS)\nInferSave-1st InferSave-2nd Max-Performance &\nInferSave(w/o ofﬂoading)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nCost (USD)\nTotal Cost (USD)\nHourly Cost (USD/hour)\nSLO 100\nSLO 200\nFig.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "Comparison of average TPS and cost for different InferSave\nconfigurations and the baseline configuration under varying SLO constraints\nfor offline inference workloads (Left: Average TPS, Right: Cost).", "metadata": {}}, {"text": "made by each policy.", "metadata": {}}, {"text": "As this workload uses a large input\ntoken size, all instances excluding g6e.xlarge make use of\nKV Cache offloading.", "metadata": {}}, {"text": "Without considering offloading, only\none instance can be considered a top choice, and therefore,\nInferSave without offloading chose the same instance as the\nMax-Performance policy.", "metadata": {}}, {"text": "Given a SLO requirement of 100 TPS, InferSave selected\ng4dn.xlarge as its top choice, providing a throughput of about\n160 TPS with the lowest total processing cost of $2.13.", "metadata": {}}, {"text": "On the\nother hand, both Max-Performance and InferSave without\noffloading selected g6e.xlarge, which delivers a very high\nthroughput of about 7600 TPS, but with a total cost of $2.699,\nan increase of about 26.7%.", "metadata": {}}, {"text": "The selection of g6e.xlarge allows\nfor maximum throughput with the ability to store all KV\nCache in GPU memory without offloading.", "metadata": {}}, {"text": "However, despite\nthe high throughput and meeting the SLO, the high cost of\nthe instance itself results in lower overall cost efficiency.", "metadata": {}}, {"text": "With a SLO requirement of 200 TPS, InferSave selected\ng5.xlarge as its top choice, as g4dn.xlarge not longer meet the\nperformance requirements.", "metadata": {}}, {"text": "This instance provides about 400\nTPS while maintaining a total cost of $2.344.", "metadata": {}}, {"text": "On the other\nhand, the Max-Performance policy still selected g6e.xlarge,\nproviding a performance of about 7600 TPS, but the total\ncost increased to $2.699, resulting in about a 15% higher cost.", "metadata": {}}, {"text": "This shows that without considering offloading, a needlessly\nhighly performant and expensive instance can be chosen,\nleading to excessively high costs, and thus reducing actual\ncost efficiency.", "metadata": {}}, {"text": "3) Overall analysis and discussion: By evaluating experi-\nmental results that represent both online chatbot and batch\nprocessing workloads, we were able to derive key insights\nfor the efficient operation of LLM inference systems.", "metadata": {}}, {"text": "(i) The impact of a workload’s I/O patterns on optimal\ninfrastructure selection: The requirements of online\nconversational chatbot inference and batch processing\ninference differ greatly in input and output token lengths,\nwhich act as key factors in determining optimal instance\nand offloading strategies.", "metadata": {}}, {"text": "(ii) The significance of selectively applying KV Cache\noffloading: KV Cache offloading is not a universally\napplicable strategy for all workloads and achieves the\ngreatest cost reduction when selectively utilized ac-\ncording to workload characteristics.", "metadata": {}}, {"text": "In particular, for\noffline batch processing workloads with long inputs,\ncost reductions up to 28% were possible with KV Cache\noffloading, while maintaining SLO requirements.", "metadata": {}}, {"text": "On the\nother hand, in online conversational chatbot workloads,\nit was often more advantageous to apply KV Cache\noffloading when considering both cost and performance.", "metadata": {}}, {"text": "(iii) Finding the optimal interface through InferSave:\nInferSave comprehensively considers the SLO require-\nments and workload characteristics to find the optimal\nbalance point in cost and performance.", "metadata": {}}, {"text": "Instead of naively\nselecting the instance with the highest performance,\nInferSave finds the instance with the highest cost\nefficiency while still satisfying the SLO requirement.", "metadata": {}}, {"text": "These results reveal an opportunity for both cost and\nperformance optimization by flexibly adjusting the offloading\nstrategy and GPU instance choice to match workload patterns\nand SLO requirements.", "metadata": {}}, {"text": "InferSave takes this opportunity and\nperforms said optimizations automatically with the selection\nof the optimal instance by considering information from\nprecise analysis of each workload’s characteristics.", "metadata": {}}, {"text": "As a result,\nInferSave achieves optimal performance according to the\ngiven SLO while maintaining cost efficiency.", "metadata": {}}, {"text": "However, this work currently has limitations in that it\nfocuses on a single GPU environment and does not address\noptimization strategies in multi-GPU or distributed inference\nenvironments.", "metadata": {}}, {"text": "We plan to extend InferSave to multi-GPU\nand distributed cluster environments to develop optimization\nstrategies suitable for inference of more complex workloads\nand large-scale models.", "metadata": {}}, {"text": "VII.", "metadata": {}}, {"text": "Conclusion\nIn this study, we propose InferSave, which utilizes SLO-\nbased predictions to automatically select cost-efficient VM\ninstances in the cloud, and validate it across online and offline\nworkloads.", "metadata": {}}, {"text": "We identify opportunities to enhance cost effi-\nciency and utilize cheaper, less powerful GPU instances, while\nmaintaining the specified SLO requirements by exploiting\ntechniques such as KV Cache offloading.", "metadata": {}}, {"text": "Through extensive\nevaluation across both online and offline inference workloads,\nour results confirm that InferSave accurately exploits said\nopportunities, and achieves at most 73.7% lower running costs\nwhile maintaining SLO requirements.", "metadata": {}}, {"text": "This research suggests that LLM service providers can\noptimize cost and performance in a balanced way by selecting\noptimal instances based on SLO and effectively utilizing\noffloading strategies.", "metadata": {}}, {"text": "InferSave offers these optimizations\nin a unified package in a LLM inferencing system that both\nlowers cost and maintains performance requirements.", "metadata": {}}, {"text": "References\n[1] A.", "metadata": {}}, {"text": "Vaswani, “Attention is all you need, ” Advances in\nNeural Information Processing Systems , 2017.", "metadata": {}}, {"text": "[2] A.", "metadata": {}}, {"text": "Radford, K.", "metadata": {}}, {"text": "Narasimhan, T.", "metadata": {}}, {"text": "Salimans, and I.", "metadata": {}}, {"text": "Sutskever,\n“Improving language understanding by generative pre-\ntraining, ”OpenAI Preprint , 2018.", "metadata": {}}, {"text": "[3] H.", "metadata": {}}, {"text": "Touvron, T.", "metadata": {}}, {"text": "Lavril, G.", "metadata": {}}, {"text": "Izacard, X.", "metadata": {}}, {"text": "Martinet, M.-A.", "metadata": {}}, {"text": "Lachaux, T.", "metadata": {}}, {"text": "Lacroix, B.", "metadata": {}}, {"text": "Rozière, N.", "metadata": {}}, {"text": "Goyal, E.", "metadata": {}}, {"text": "Hambro,", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "F. Azhar, et al. , “Llama: Open and efficient foundation\nlanguage models, ”arXiv preprint arXiv:2302.13971 , 2023.\n[4] Anthropic, “Message batches api, ” 2024. https://www.\nanthropic.com/news/message-batches-api, Accessed: De-\ncember 30, 2024.\n[5] OpenAI, “Batch processing and rate limits, ” 2024. https:\n//platform.openai.com/docs/guides/batch#rate-limits, Ac-\ncessed: December 30, 2024.\n[6] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen,\nP. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single gpu, ” in International Conference\non Machine Learning , pp. 31094–31116, PMLR, 2023.\n[7] Y. Xiong, H. Wu, C. Shao, Z. Wang, R. Zhang, Y. Guo,\nJ. Zhao, K. Zhang, and Z. Pan, “Layerkv: Optimizing\nlarge language model serving with layer-wise kv cache\nmanagement, ” 2024.\n[8] X. Pan, E. Li, Q. Li, S. Liang, Y. Shan, K. Zhou, Y. Luo,\nX. Wang, and J. Zhang, “Instinfer: In-storage attention\noffloading for cost-effective long-context llm inference, ”\n2024.\n[9] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li,\nE. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, and\nY. He, “Deepspeed- inference: Enabling efficient inference\nof transformer models at unprecedented scale, ” in SC22:\nInternational Conference for High Performance Computing,\nNetworking, Storage and Analysis , pp. 1–15, 2022.\n[10] G. Cloud, “Compare aws and azure services to\ngoogle cloud. ” https://cloud.google.com/docs/get-started/\naws-azure-gcp-service-comparison?hl=ko, 2024. Ac-\ncessed: 2024-12-26.\n[11] A. Harlap, A. Tumanov, A. Chung, G. R. Ganger, and\nP. B. Gibbons, “Proteus: Agile ml elasticity through\ntiered reliability in dynamic resource markets, ” in 12nd\nEuropean Conference on Computer Systems , EuroSys ’17,\np. 589–604, 2017.\n[12] Y. Kim, K. Kim, Y. Cho, J. Kim, A. Khan, K.-D. Kang,\nB.-S. An, M.-H. Cha, H.-Y. Kim, and Y. Kim, “DeepVM:\nIntegrating Spot and On-Demand VMs for Cost-Efficient\nDeep Learning Clusters in the Cloud, ” in IEEE/ACM\nInternational Symposium on Cluster, Cloud and Grid\nComputing (CCGRID) , 2024.\n[13] G. Fragiadakis, V. Liagkou, E. Filiopoulou, D. Fragkakis,\nC. Michalakelis, and M. Nikolaidou, “Cloud services cost\ncomparison: a clustering analysis framework, ”Computing,\nvol. 105, pp. 1–28, 03 2023.\n[14] A. Andrzejak, D. Kondo, and S. Yi, “Decision model for\ncloud computing under sla constraints, ” in Proceedings of\nthe IEEE International Symposium on Modeling, Analysis\nand Simulation of Computer and Telecommunication\nSystems, MASCOTS ’10, pp. 257–266, IEEE, 2010.\n[15] P. Kokkinos, T. A. Varvarigou, A. Kretsis, P. Soumplis,\nand E. A. Varvarigos, “Cost and utilization optimization\nof amazon ec2 instances, ” in Proceedings of the 2013\nIEEE Sixth International Conference on Cloud Computing ,\npp. 518–525, IEEE, 2013.\n[16] T. Griggs, X. Liu, J. Yu, D. Kim, W.-L. Chiang, A. Cheung,\nand I. Stoica, “Mélange: Cost efficient large language\nmodel serving by exploiting gpu heterogeneity, ” 2024.\n[17] C. Nie, R. Fonseca, and Z. Liu, “Aladdin: Joint placement\nand scaling for slo-aware llm serving, ” arXiv preprint\narXiv:2405.06856, 2024.\n[18] Y. Jiang, F. Fu, X. Yao, T. Wang, B. Cui, A. Klimovic,\nand E. Yoneki, “Thunderserve: High-performance and\ncost-efficient llm serving in cloud environments, ” arXiv\npreprint arXiv:2502.09334 , 2025.\n[19] P. Patel, E. Choukse, C. Zhang, A. Shah, Í. Goiri, S. Maleki,\nand R. Bianchini, “Splitwise: Efficient generative llm\ninference using phase splitting, ” in 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architec-\nture (ISCA) , pp. 118–132, IEEE, 2024.\n[20] Z. Wang, S. Li, Y. Zhou, X. Li, R. Gu, N. Cam-Tu, C. Tian,\nand S. Zhong, “Revisiting slo and goodput metrics in\nllm serving, ”arXiv preprint arXiv:2410.14257 , 2024.\n[21] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury,\nJ. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently\nscaling transformer inference, ” inProceedings of Machine\nLearning and Systems 5 (MLSys 2023) , 2023.\n[22] AWS, “Aws amazon ec2 instance types-cloud computing\ninstances. ”", "sentences": [{"text": "F.", "metadata": {}}, {"text": "Azhar, et al.", "metadata": {}}, {"text": ", “Llama: Open and efficient foundation\nlanguage models, ”arXiv preprint arXiv:2302.13971 , 2023.", "metadata": {}}, {"text": "[4] Anthropic, “Message batches api, ” 2024.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "anthropic.com/news/message-batches-api, Accessed: De-\ncember 30, 2024.", "metadata": {}}, {"text": "[5] OpenAI, “Batch processing and rate limits, ” 2024.", "metadata": {}}, {"text": "https:\n//platform.openai.com/docs/guides/batch#rate-limits, Ac-\ncessed: December 30, 2024.", "metadata": {}}, {"text": "[6] Y.", "metadata": {}}, {"text": "Sheng, L.", "metadata": {}}, {"text": "Zheng, B.", "metadata": {}}, {"text": "Yuan, Z.", "metadata": {}}, {"text": "Li, M.", "metadata": {}}, {"text": "Ryabinin, B.", "metadata": {}}, {"text": "Chen,\nP.", "metadata": {}}, {"text": "Liang, C.", "metadata": {}}, {"text": "Ré, I.", "metadata": {}}, {"text": "Stoica, and C.", "metadata": {}}, {"text": "Zhang, “Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single gpu, ” in International Conference\non Machine Learning , pp.", "metadata": {}}, {"text": "31094–31116, PMLR, 2023.", "metadata": {}}, {"text": "[7] Y.", "metadata": {}}, {"text": "Xiong, H.", "metadata": {}}, {"text": "Wu, C.", "metadata": {}}, {"text": "Shao, Z.", "metadata": {}}, {"text": "Wang, R.", "metadata": {}}, {"text": "Zhang, Y.", "metadata": {}}, {"text": "Guo,\nJ.", "metadata": {}}, {"text": "Zhao, K.", "metadata": {}}, {"text": "Zhang, and Z.", "metadata": {}}, {"text": "Pan, “Layerkv: Optimizing\nlarge language model serving with layer-wise kv cache\nmanagement, ” 2024.", "metadata": {}}, {"text": "[8] X.", "metadata": {}}, {"text": "Pan, E.", "metadata": {}}, {"text": "Li, Q.", "metadata": {}}, {"text": "Li, S.", "metadata": {}}, {"text": "Liang, Y.", "metadata": {}}, {"text": "Shan, K.", "metadata": {}}, {"text": "Zhou, Y.", "metadata": {}}, {"text": "Luo,\nX.", "metadata": {}}, {"text": "Wang, and J.", "metadata": {}}, {"text": "Zhang, “Instinfer: In-storage attention\noffloading for cost-effective long-context llm inference, ”\n2024.", "metadata": {}}, {"text": "[9] R.", "metadata": {}}, {"text": "Y.", "metadata": {}}, {"text": "Aminabadi, S.", "metadata": {}}, {"text": "Rajbhandari, A.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Awan, C.", "metadata": {}}, {"text": "Li, D.", "metadata": {}}, {"text": "Li,\nE.", "metadata": {}}, {"text": "Zheng, O.", "metadata": {}}, {"text": "Ruwase, S.", "metadata": {}}, {"text": "Smith, M.", "metadata": {}}, {"text": "Zhang, J.", "metadata": {}}, {"text": "Rasley, and\nY.", "metadata": {}}, {"text": "He, “Deepspeed- inference: Enabling efficient inference\nof transformer models at unprecedented scale, ” in SC22:\nInternational Conference for High Performance Computing,\nNetworking, Storage and Analysis , pp.", "metadata": {}}, {"text": "1–15, 2022.", "metadata": {}}, {"text": "[10] G.", "metadata": {}}, {"text": "Cloud, “Compare aws and azure services to\ngoogle cloud.", "metadata": {}}, {"text": "” https://cloud.google.com/docs/get-started/\naws-azure-gcp-service-comparison?hl=ko, 2024.", "metadata": {}}, {"text": "Ac-\ncessed: 2024-12-26.", "metadata": {}}, {"text": "[11] A.", "metadata": {}}, {"text": "Harlap, A.", "metadata": {}}, {"text": "Tumanov, A.", "metadata": {}}, {"text": "Chung, G.", "metadata": {}}, {"text": "R.", "metadata": {}}, {"text": "Ganger, and\nP.", "metadata": {}}, {"text": "B.", "metadata": {}}, {"text": "Gibbons, “Proteus: Agile ml elasticity through\ntiered reliability in dynamic resource markets, ” in 12nd\nEuropean Conference on Computer Systems , EuroSys ’17,\np.", "metadata": {}}, {"text": "589–604, 2017.", "metadata": {}}, {"text": "[12] Y.", "metadata": {}}, {"text": "Kim, K.", "metadata": {}}, {"text": "Kim, Y.", "metadata": {}}, {"text": "Cho, J.", "metadata": {}}, {"text": "Kim, A.", "metadata": {}}, {"text": "Khan, K.-D.", "metadata": {}}, {"text": "Kang,\nB.-S.", "metadata": {}}, {"text": "An, M.-H.", "metadata": {}}, {"text": "Cha, H.-Y.", "metadata": {}}, {"text": "Kim, and Y.", "metadata": {}}, {"text": "Kim, “DeepVM:\nIntegrating Spot and On-Demand VMs for Cost-Efficient\nDeep Learning Clusters in the Cloud, ” in IEEE/ACM\nInternational Symposium on Cluster, Cloud and Grid\nComputing (CCGRID) , 2024.", "metadata": {}}, {"text": "[13] G.", "metadata": {}}, {"text": "Fragiadakis, V.", "metadata": {}}, {"text": "Liagkou, E.", "metadata": {}}, {"text": "Filiopoulou, D.", "metadata": {}}, {"text": "Fragkakis,\nC.", "metadata": {}}, {"text": "Michalakelis, and M.", "metadata": {}}, {"text": "Nikolaidou, “Cloud services cost\ncomparison: a clustering analysis framework, ”Computing,\nvol.", "metadata": {}}, {"text": "105, pp.", "metadata": {}}, {"text": "1–28, 03 2023.", "metadata": {}}, {"text": "[14] A.", "metadata": {}}, {"text": "Andrzejak, D.", "metadata": {}}, {"text": "Kondo, and S.", "metadata": {}}, {"text": "Yi, “Decision model for\ncloud computing under sla constraints, ” in Proceedings of\nthe IEEE International Symposium on Modeling, Analysis\nand Simulation of Computer and Telecommunication\nSystems, MASCOTS ’10, pp.", "metadata": {}}, {"text": "257–266, IEEE, 2010.", "metadata": {}}, {"text": "[15] P.", "metadata": {}}, {"text": "Kokkinos, T.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Varvarigou, A.", "metadata": {}}, {"text": "Kretsis, P.", "metadata": {}}, {"text": "Soumplis,\nand E.", "metadata": {}}, {"text": "A.", "metadata": {}}, {"text": "Varvarigos, “Cost and utilization optimization\nof amazon ec2 instances, ” in Proceedings of the 2013\nIEEE Sixth International Conference on Cloud Computing ,\npp.", "metadata": {}}, {"text": "518–525, IEEE, 2013.", "metadata": {}}, {"text": "[16] T.", "metadata": {}}, {"text": "Griggs, X.", "metadata": {}}, {"text": "Liu, J.", "metadata": {}}, {"text": "Yu, D.", "metadata": {}}, {"text": "Kim, W.-L.", "metadata": {}}, {"text": "Chiang, A.", "metadata": {}}, {"text": "Cheung,\nand I.", "metadata": {}}, {"text": "Stoica, “Mélange: Cost efficient large language\nmodel serving by exploiting gpu heterogeneity, ” 2024.", "metadata": {}}, {"text": "[17] C.", "metadata": {}}, {"text": "Nie, R.", "metadata": {}}, {"text": "Fonseca, and Z.", "metadata": {}}, {"text": "Liu, “Aladdin: Joint placement\nand scaling for slo-aware llm serving, ” arXiv preprint\narXiv:2405.06856, 2024.", "metadata": {}}, {"text": "[18] Y.", "metadata": {}}, {"text": "Jiang, F.", "metadata": {}}, {"text": "Fu, X.", "metadata": {}}, {"text": "Yao, T.", "metadata": {}}, {"text": "Wang, B.", "metadata": {}}, {"text": "Cui, A.", "metadata": {}}, {"text": "Klimovic,\nand E.", "metadata": {}}, {"text": "Yoneki, “Thunderserve: High-performance and\ncost-efficient llm serving in cloud environments, ” arXiv\npreprint arXiv:2502.09334 , 2025.", "metadata": {}}, {"text": "[19] P.", "metadata": {}}, {"text": "Patel, E.", "metadata": {}}, {"text": "Choukse, C.", "metadata": {}}, {"text": "Zhang, A.", "metadata": {}}, {"text": "Shah, Í.", "metadata": {}}, {"text": "Goiri, S.", "metadata": {}}, {"text": "Maleki,\nand R.", "metadata": {}}, {"text": "Bianchini, “Splitwise: Efficient generative llm\ninference using phase splitting, ” in 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architec-\nture (ISCA) , pp.", "metadata": {}}, {"text": "118–132, IEEE, 2024.", "metadata": {}}, {"text": "[20] Z.", "metadata": {}}, {"text": "Wang, S.", "metadata": {}}, {"text": "Li, Y.", "metadata": {}}, {"text": "Zhou, X.", "metadata": {}}, {"text": "Li, R.", "metadata": {}}, {"text": "Gu, N.", "metadata": {}}, {"text": "Cam-Tu, C.", "metadata": {}}, {"text": "Tian,\nand S.", "metadata": {}}, {"text": "Zhong, “Revisiting slo and goodput metrics in\nllm serving, ”arXiv preprint arXiv:2410.14257 , 2024.", "metadata": {}}, {"text": "[21] R.", "metadata": {}}, {"text": "Pope, S.", "metadata": {}}, {"text": "Douglas, A.", "metadata": {}}, {"text": "Chowdhery, J.", "metadata": {}}, {"text": "Devlin, J.", "metadata": {}}, {"text": "Bradbury,\nJ.", "metadata": {}}, {"text": "Heek, K.", "metadata": {}}, {"text": "Xiao, S.", "metadata": {}}, {"text": "Agrawal, and J.", "metadata": {}}, {"text": "Dean, “Efficiently\nscaling transformer inference, ” inProceedings of Machine\nLearning and Systems 5 (MLSys 2023) , 2023.", "metadata": {}}, {"text": "[22] AWS, “Aws amazon ec2 instance types-cloud computing\ninstances.", "metadata": {}}, {"text": "”", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}]}