{"document_id": "fernandez2025", "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "text": "Energy Considerations of Large Language Model Inference and Efficiency\nOptimizations\nJared Fernandez*1, Clara Na*1, Vashisth Tiwari*1,\nYonatan Bisk1, Sasha Luccioni2, Emma Strubell1\n1Carnegie Mellon University, 2Hugging Face,\nCorrespondence: {jaredfern, clarana, vashisthtiwari}@cmu.edu\nAbstract\nAs large language models (LLMs) scale in size\nand adoption, their computational and environ-\nmental costs continue to rise. Prior benchmark-\ning efforts have primarily focused on latency\nreduction in idealized settings, often overlook-\ning the diverse real-world inference workloads\nthat shape energy use. In this work, we system-\natically analyze the energy implications of com-\nmon inference efficiency optimizations across\ndiverse Natural Language Processing (NLP)\nand generative Artificial Intelligence (AI) work-\nloads, including conversational AI and code\ngeneration. We introduce a modeling approach\nthat approximates real-world LLM workflows\nthrough a binning strategy for input-output to-\nken distributions and batch size variations. Our\nempirical analysis spans software frameworks,\ndecoding strategies, GPU architectures, online\nand offline serving settings, and model paral-\nlelism configurations. We show that the effec-\ntiveness of inference optimizations is highly\nsensitive to workload geometry, software stack,\nand hardware accelerators, demonstrating that\nnaive energy estimates based on FLOPs or the-\noretical GPU utilization significantly underes-\ntimate real-world energy consumption. Our\nfindings reveal that the proper application of\nrelevant inference efficiency optimizations can\nreduce total energy use by up to 73% from un-\noptimized baselines. These insights provide a\nfoundation for sustainable LLM deployment\nand inform energy-efficient design strategies\nfor future AI infrastructure.\n1 Introduction\nImprovements in task performance by large lan-\nguage models (LLMs) have prompted large-scale\ninvestments in computing hardware and energy in-\nfrastructure to support the development and de-\nployment of LLM and related machine learning\nmodels (Isaac, 2025; Smith, 2025; Cai and Sophia,\n*Equal contribution\nBurstGPT Azure Code Azure Conv.\nTask\n101\n102\n103\nEnergy (kWh)\nTheoretical\nPyTorch\nvLLM\nFigure 1: Proper application of efficiency methods with\noptimized vLLM (orange) approaches the ideal energy\nconsumption (green) as compared with an unoptimized\nbaseline PyTorch (purple) implementation.\n2025). However, the growing prevalence of LLMs\nyields commensurate increases in the energy de-\nmand, water use, and carbon emissions associated\nwith their development and deployment (Morrison\net al., 2025; Li et al., 2025; Strubell et al., 2020;\nLuccioni et al., 2024b). Primarily motivated by the\nincreased demands from LLM and AI workloads,\nprojections estimate that that data centers consume\nbetween 9.1% and 11.7% of the total US energy de-\nmand by 2030 (Aljbour et al., 2024; Shehabi et al.,\n2024; Green et al., 2024). However, such projec-\ntions of energy use primarily rely upon sector-wide\nestimates of demand or substantial simplifications\nof the of the energy demands of individual models.\nIn order to develop effective energy policy for\nthis growing demand, it is necessary to characterize\nthe underlying computational workloads of devel-\nopment (i.e. model training) and deployment (i.e.\ninference). In particular, the cost and efficiency of\ninference is especially crucial due to the scale and\nincreased frequency at which models are served for\nrepeated use. Concretely, Meta reports that infer-\nence workloads constitute up to 70% of their AI\npower consumption (Wu et al., 2022) while Google\nattributes 60% of their ML energy (Patterson et al.,\n2022) and between 80 to 90% of ML AWS cloud\ncomputing demand (Barr, 2019; Leopold, 2019).\n1\narXiv:2504.17674v1  [cs.CL]  24 Apr 2025\n\nTo address the problem of inference efficiency,\nthe NLP and machine learning research commu-\nnities have developed various optimizations span-\nning: algorithms, software frameworks, and hard-\nware accelerators. Such optimizations have pri-\nmarily targeted improvements in model speed (e.g.\nlatency and throughput; (Leviathan et al., 2023;\nKwon et al., 2023)). Moreover, these methods are\nfrequently assessed in constrained settings or on\nsimplified datasets that fail to capture the broad\ndiversity of real-world tasks. These tasks range\nfrom traditional NLP applications like sequence\ntagging and summarization to more computation-\nally demanding workloads such as synthetic data\ngeneration and chain-of-thought reasoning. There\nremains a critical gap in understanding of the en-\nergy costs of language model inference, especially\nwhen efficiency interventions are applied jointly in\nreal-world settings.\nIn this work, we examine the energy costs of\nLLM inference and present a comprehensive anal-\nysis of the impact of: data dimensionality, decod-\ning strategies, serving frameworks, compilation\ntechniques, GPU hardware platforms, model par-\nallelism, and architectural variants on total energy\nuse during inference. Based on our energy profiling\nacross these optimizations, we approximate offline\ninference with LLMs based on real-world workload\nwith variable sequence lengths and batching, con-\nsidering both an upper bound of naive unoptimized\ninference and a lower bound of theoretical opti-\nmized inference. Our analysis reveals that while\nidealized estimations of hardware utilization sub-\nstantially underestimate the energy use of language\nmodel inference, proper application of inference\nefficiency optimizations can substantially reduce\nthe energy requirements of inference by up to\n73% from unoptimized baselines with vanilla\nPyTorch and Huggingface Transformersand to\nwithin 26.6% of theoretical ideal performance on\nsimulated offline workloads (see Table 4).\n2 Methodology\nIn the following section, we describe our experi-\nmental setup for evaluating inference efficiency.\nModel Architectures. We focus our experiments\non language models ranging from 1B to 32B pa-\nrameters, primarily evaluating Llama-3.1-8B-Ba\nse and Llama-3.1-8B-Instruct models as repre-\nsentative decoder-only transformers (Dubey et al.,\n2024). To investigate effects of scaling model archi-\ntecture, we include the Qwen-1.5-32B model (Bai\net al., 2023). For architectural comparisons, we\nanalyze the sparse OLMoE mixture-of-expert (MoE)\nmodel alongside its dense counterparts – the 1B and\n7B OLMo architectures – which maintain compara-\nble active and total parameter counts, respectively\n(Muennighoff et al., 2024; Groeneveld et al., 2024).\nData Dimensionality We investigate the impact\nof data dimensionality across three key dimensions:\ninput sequence lengths, output generation lengths,\nand batch sizes.\nInference with large language models is com-\nmonly decomposed into two stages: prefilling and\ntoken generation, each with a different energy pro-\nfile (Patel et al., 2024). The prefill stage processes\nprompts in parallel and is typically compute-bound,\nachieving high GPU utilization. In contrast, the\nautoregressive decoding stage is typically memory-\nbound and leads to GPU under-utilization. These\nbottlenecks and their resulting energy profiles shift\nwith input and output lengths.\nTo address GPU under-utilization during gener-\nation, serving systems employ batching strategies.\nHowever, the effectiveness of batching varies with\ninput-output characteristics (Agrawal et al., 2024;\nLi et al., 2024). Long input sequences limit maxi-\nmum batch sizes due to memory constraints, while\nvariable output lengths can lead to inefficient batch\nutilization as some sequences complete before oth-\ners.\nOur analysis spans batch sizes from 1 (single-\nexample inference) to task-dependent maximums\n(up to 1024), ensuring coverage of a broad range\nof maximally realistic settings.\nWe ground analysis in NLP workloads spanning\ntext classification, summarization, translation, and\nopen-ended text generation. Different tasks exhibit\ndifferent data dimensionalities: classification in-\nvolves minimal generation (often a single token),\nsummarization pairs long contexts with medium-\nlength outputs, and translation typically assumes\nbalanced input-output lengths. Input length statis-\ntics in considered datasets are shown in Table 3.\nIn a controlled sweep, we explore scenarios with\nup to 32k input tokens and 4k output tokens, vary-\ning sequence lengths by powers of two. We fix\ngeneration to 64 or 8 tokens when varying context\nlengths, and assume 512 or 64 token input context\nwhen varying output lengths. Input context length\nis enforced via truncation of longer sequences from\nPG19 (Rae et al., 2019).\n2\n\nFigure 2: Controlled sweeps of input and output sequence lengths on A6000 GPUs, on vLLM backend, described\nin §3.1. We decompose inference costs into prefill and decode energy. At small batch sizes and input sequence\nlengths, energy intensity of a workload scales sub-linearly with increasing sequence length input sequence lengths.\nDecoding is more energy intensive per token than prefill, but energy intensity begins scaling linearly even for short\ngenerations and small batch sizes with the vLLM framework.\nDecoding Strategies. Different decoding strate-\ngies used for generation have different computa-\ntional profiles and can have a substantial impact on\nthe generation efficiency (Kwon et al., 2023). In\norder to study the impact of sampling methods and\nauto-regressive decoding strategies, we investigate\ngreedy decoding, beam search decoding, temper-\nature sampling, top-p decoding affect the energy\nrequirements and end-to-end latency (Holtzman\net al., 2020).\nIn addition to auto-regressive decoding, we study\nthe impact of speculative decoding. Speculative de-\ncoding is commonly used as a latency minimization\ninference optimization (Kwon et al., 2023). In spec-\nulative decoding, a lightweight draft model is used\nto predict multiple tokens (γ) which are then veri-\nfied by the target model in parallel (Leviathan et al.,\n2023; Chen et al., 2023). Speculative decoding\nprovides latency improvement by better utilizing\nGPUs over autoregressive decoding.\nIn our experiments, we use the following target-\ndraft model pairs with a look-ahead value γ = 4\nacross various batch sizes: DeepSeek-R1-Distil\nl-Qwen-32B with mobiuslabsgmbh/DeepSeek-R\n1-ReDistill-Qwen-1.5B-v1.1 (Guo et al., 2025;\nYang et al., 2024); Llama-3.1-8B-Base with Lla\nma-3.2-1B (Dubey et al., 2024).\nSoftware Optimizations. Choice in the software\nframeworks used for inference significantly im-\npacts both latency and energy efficiency through op-\ntimized kernel implementations and computational\ngraph management (Georgiou et al., 2022; Fernan-\ndez et al., 2023). We evaluate two widely-adopted\nlibraries used in LLM inference: native PyTorch\nwith HuggingFace transformers (Wolf et al., 2020),\nand vLLM, an optimized framework for LLM infer-\nence that achieves improved compute and memory\nutilization (Paszke et al., 2019; Kwon et al., 2023);\nexperiments are conducted in bfloat16 precision.\nWithin these frameworks, we compare with a\nnative PyTorch baselines with Just-in-Time com-\npilation via TorchInductor (i.e. torch.compile)\nand CUDA Graphs kernel serialization. Further-\nmore, for vLLM, we evaluate continuous batching\nwhich efficiently handles variable output lengths\nin batch processing by overlaying sequences (Yu\net al., 2022).\nHardware Platforms. Our experiments are con-\nducted using an on-premise heterogeneous server\nwith multiple GPU types and node configurations.\nSpecifically, we conduct experiments on multiple\ngenerations of consumer workstation and datacen-\nter GPU accelerators from the Ampere (A6000,\nA100 80GB PCIe), and Ada Lovelace (A6000 Ada)\nmicroarchitecture.\nAll experiments run on 8-GPU nodes with stan-\ndardized node- and job-level CPU and RAM con-\nfigurations for each GPU type. For multi-GPU ex-\nperiments, we utilize up to 4 GPUs simultaneously,\ninvestigating tensor parallel inference with group\nsizes of 2 and 4 devices. 1. We examine both stan-\ndard and speculative decoding approaches using\nthe Llama-3.1-8B and Qwen-32B models. Addi-\ntional details on computing hardware are provided\nin Appendix A.\n1This configuration leaves 4-7 GPUs available for other\nusers. While the Slurm scheduler does not enforce complete\nisolation in network, memory, and CPU infrastructure across\njobs, concurrent workloads in practice were not CPU- or\nmemory-intensive enough to impact ours significantly – for\nexample, in the vast majority of cases (98%), an ambient\nmeasurement of the RAM utilization in a node our jobs were\nrunning on was less than 20% of the total available\n3\n\n20 21 22 23 24 25 26 27\nBatch Size\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nSpeculative Decoding\nGreedy Decoding\nBeam Search\nTemperature Sampling\nTop-p Sampling\nFigure 3: At small batch sizes, speculative decoding\nprovides reduced latency and energy savings. At larger\nbatch size speculative decoding increases energy.\nPerformance Measures. We evaluate the ef-\nficiency of inference by measuring the latency,\nthroughput, GPU energy, and GPU power required\nfor the inference of 1,024 examples 2. Total en-\nergy use and GPU power metrics are measured\nusing Nvidia Management Library (NVML) via\nthe CodeCarbon library (Courty et al., 2024). Prior\nto evaluation, we conduct a warmup on up to 20\nbatches to allow for memory allocation, required\nCUDA graph capture, and JiT compilation 3. Re-\nsults are reported as the mean values energy use,\nlatency, or power usage of three runs.\n3 Results\nIn the following section, we examine the effects of\nvariations of data dimensionality, model architec-\nture, decoding strategies, and software optimiza-\ntions on inference energy use.\n3.1 Effects of Dataset and Sequence Length\nWe present results from our controlled sweep of\nsequence lengths and batch sizes in Figure 2. Pre-\nfill costs increase as a function of input sequence\nlength, at the same rate regardless of batch sizes\nwhen scaling sequences larger than 128 tokens. At\nshorter sequence lengths and smaller batch sizes,\nthe energy costs of prefill are constant regardless\nof the computational workload due to significant\nundersaturation of the accelerator. Although we\nfix output generation tokens to 64, we verify that\nat this convergence in rate of energy intensity in-\ncrease occurs at the same point when instead fixing\ngeneration length to 8 tokens; see Figure 11 in\nAppendix E.\n2For experiments with batch sizes larger than 256, metrics\nare computed over 4096 examples and then normalized.\n3Due to size, warmup is limited to 4 batches for inference\nwith the Qwen-32B.\n21 23 25 27 29\nBatch Size\n2−10\n2−9\n2−8\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nOLMo 1B\nOLMo 7B\nOLMoE 1B-7B\nFigure 4: Mixture-of-Experts LLMs require more en-\nergy than dense models with comparable active parame-\nters; differences are pronounced at larger batch sizes.\nIn Figure 2, the energy intensity of the decode\nlikewise scales with input context length only at\nlarger input sequence lengths.\nHowever, the energy intensity of decoding scales\nlinearly with sequence length regardless of se-\nquence length or batch sizes due to the autoregres-\nsive, sequential nature of decoding.\nGenerally, decoding energy dominates the over-\nall workload in all settings but those with the short-\nest generation lengths, such as those seen in classi-\nfication workloads and short form summarization.\nNote the log-log scale and the parallel linear trends,\nwhere the differences in intercepts are proportion-\nate with the differences in batch size 4. In the\nfollowing sections, we discuss a variety of algorith-\nmic and software interventions that are appropriate\nfor different types of workload geometries.\n3.2 Effects of Algorithmic Optimizations\nSpeculative Decoding Only Reduces Energy at\nLow Batch Sizes. Speculative decoding is com-\nmonly used to achieve inference speedups in low-\nbatch inference in which autoregressive decoding\nfails to achieve high GPU VRAM utilization. How-\never, for large batch sizes where GPU is already\nsaturated, draft model speculation and excess verifi-\ncations introduce additional overhead. In the large\nbatch case, for short to medium contexts, LLM\ninference is typically compute bound, making spec-\nulative decoding slower than autoregressive decod-\ning with the target model (Chen et al., 2025; Liu\net al., 2024).\nCompared to variations in energy use from al-\nternate decoding strategies and sampling methods,\nspeculative decoding has the greatest effect on the\n4See Fig 10 in Appendix E for additional results on vanilla\nPyTorch backend, and Figure 12 for comparison with real\nenergy intensity measurements for a sample of classical NLP\ntasks\n4\n\n21 23 25 27 29\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (c) A6000\nFigure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends\nof 1024 samples for 64 output tokens. For each GPU, we compare PyTorch with and without compilation, and\nvLLM with and without CUDA Graph serialization. The line in black represents the maximum allowable batch size\nfor PyTorch. Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations\ncan serve a larger batch size.\nenergy use and latency of language model infer-\nence. At smaller batch sizes ( ≤ 16) speculative\ndecoding is effective in reducing the total energy\ncost of inference with up to +29.14% compared to\nsingle-example inference (Figure 3). However, au-\ntoregressive decoding methods are more efficient\nat larger batch sizes, with speculative decoding\nrequiring 25.65% more energy when performing\ninference at a batch size of 128.\nMixture of Experts Incurs Higher Inference En-\nergy Costs. Sparse mixture-of-experts are often\nutilized as an alternative architecture due to their\nincreased sample efficiency during training and\nincreased performance relative to dense neural net-\nworks with the same number of active parameters.\nAlthough both dense OLMo-1B and the OLMoE1B-7\nB mixture-of-experts models use substantially less\nenergy than the dense OLMo-7B model, the OLMoE\narchitecture utilizes up to 54.24% more energy\nthan the base OLMo 1B model, despite having a\nsimilar number of active parameters.\nWe identify that the increased energy and latency\nof MoE’s can be attributed to the fused kernel used\nin the expert layers which is substantially slower\nthan the corresponding GEMM operation in linear\nlayers in the dense model; 19.70% slower at batch\nsize 1 and 63% slower at batch size 8. Notably,\nwe observe that the additional routing operations\nin the MoE model introduce minimal latency; and\nthat the increased overhead of more CUDA graph\nand kernel launch operations are largely mitigated\nthrough kernel serialization and graph compilation\noptimizations (i.e. vLLM with CUDA Graphs).\n3.3 Effects of Software Optimizations\nPagedAttention with vLLM Improves Efficiency.\nCompared to native PyTorch, the vLLM inference\nserving engine improves both the throughput and\nthe energy efficiency. The vLLM framework uses\nPagedAttention to implement non-contiguous KV\ncache blocks which reduces memory fragmentation\nand allocation of redundant memory in the case of\nsparse sequences (Kwon et al., 2023).\nThese optimizations allow for improved memory\nefficiency and the vLLM framework to support\nlarger batch sizes on fixed memory GPUs.\nCompilation and Kernel Serialization Improves\nEfficiency. The graph compilation and kernel\nserialization increase hardware utilization by re-\nmoving redundant operations in the computational\ngraph and reducing the kernel launch overhead\n(Fernandez et al., 2023), respectively. We observe\nthat both torch.compile and CUDA graph serial-\nization (eager=False) improve throughput at no\nadditional energy cost in Figure 5. However, we\nnote that the benefits of CUDA graphs are more\napparent at lower batch sizes, as the relative cost of\nkernel launch is larger for smaller computational\nworkloads.\nContinuous Batching Reduces Energy Use.\nLLM inference is inherently autoregressive, requir-\ning many sequential operations. Static batching\nmaintains a fixed batch size throughout inference,\nwhich leads to GPU under-utilization when gener-\nation lengths vary and idle compute accumulates\nafter early terminations. Continuous batching miti-\ngates this by dynamically replacing completed re-\nquests with new ones, improving GPU utilization\nand reducing idle time (Yu et al., 2022). This ap-\n5\n\n21 23 25 27 29\nBatch Size\n2−7\n2−6\n2−5\n2−4\n2−3\nEnergy (kWh)\nLlama 3.1 8B\nSingle GPU\nTensor Parallel: 2\nTensor Parallel: 4\n21 23 25 27 29\nBatch Size\n2−4\n2−3\n2−2\nEnergy (kWh)\nDeepSeek Distill Qwen 32B\nTensor Parallel: 2\nTensor Parallel: 4\nFigure 6: Energy Use of Llama-3.1 8B and Qwen 32B\nwith varying degrees of Tensor Parallelism.\nproach is particularly effective when generation\nlengths have high variance, yielding significant\nspeedups at larger batch sizes.\nWe observe that at smaller batch sizes the over-\nhead of online scheduling outweighs its benefits but\nat larger batch sizes, online serving with continuous\nbatching requires less energy; details in Appendix\nD. We note that the numbers under-represent the\nimpact of continuous batching given the samples\nare drawn from the same dataset, thereby reducing\nthe variance in input and output lengths.\n3.4 Effects of Hardware Design Choices\nMulti-GPU Tensor Parallelism Reduces Latency\nfor Increased Power Use Model parallelism\ntechniques such as tensor and pipeline parallelism\nare frequently used to alleviate the memory pres-\nsure of large sets of model parameters and batch\nsizes, as well as to leverage multiple hardware ac-\ncelerators in order to speed up workload execu-\ntion (Narayanan et al., 2021). Additionally, for\nfixed workloads, tensor parallelism reduces both\nthe per-device computational intensity and per-\ndevice power utilization as the workload is sharded\nacross accelerator. However, the speedups from\nadditional accelerators are insufficient to offset the\nenergy cost of utilizing more devices (i.e. utilizing\ntwice the GPUs fails to yield a two-fold speedup).\nIn Figure 6, we observe that utilizing tensor par-\nallelism to scale from inference with a single GPU\nto four GPUs reduces latency and per-device power\nutilization for the Llama-3.1 8B model. However,\nincreasing parallelism yields higher total energy\nuse due to the larger number of accelerators. Con-\ncretely, parallelizing a fixed workload over two\nand four GPUs decreases latency by 40.16% and\n61.34% but increases total energy use by 29.3%\nand 55.23% at single batch inference due to the\nintroduction of additional devices.\nEffects of Hardware Speed The effectiveness of\noptimization techniques varies significantly across\nhardware platforms, with faster accelerators show-\ning greater benefits from optimizations that target\ncomputational efficiency. Our results demonstrate\nthat graph compilation, kernel serialization, and\nspeculative decoding achieve their maximum im-\npact on the A100 GPU.\nSpecifically, PyTorch compilation yields a\n29.90% improvement on the A100, which drops\nto 13.28% on the RTX 6000 Ada and further to\n1.96% on the A6000. Similarly, vLLM’s eager\nmode optimization shows a 25.47% improvement\non the A100 versus 2.97% on the A6000. This\npattern suggests that as hardware computational ca-\npabilities increase, the relative impact of software\noptimizations targeting kernel efficiency becomes\nmore pronounced.\n4 The Impact of Optimizations on\nInference Energy Use\nIn this section, we outline our approach to model-\ning the energy consumption of an LLM under both\nsynthetic and realistic workload distributions. We\nleverage classical NLP tasks and datasets of infer-\nence requests to estimate energy usage across dif-\nferent execution environments, including PyTorch-\nnative and vLLM backends with software optimiza-\ntions on a single A6000 GPU.\n4.1 Modeling Energy Requirements Using\nOffline Serving\nWe consider the energy required to process a\ndataset D = {R1, R2, . . . , RN } in an offline set-\nting in which all requests can be batch processed\nfreely, and where each request Rk consists of a\ntuple (ik, ok), representing the input token length\nik and the output generation length ok:\nRk = (ik, ok), ∀k ∈ {1, . . . , N}.\n6\n\n21 23 25 27 29 211 213\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Input Tokens - CDF\nReal Distribution\nBinned Approximation\n21 23 25 27 29 211\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Output Tokens - CDF\nReal Distribution\nBinned Approximation\nFigure 7: Comparison of the real token length distributions (blue) with the binned approximation (orange) for\nAzure conversation input (left) and output (right) token lengths. The CDF plots illustrate how our binning strategy\napproximates the empirical distribution while ensuring computational efficiency for energy estimation.\nSince ik and ok vary significantly across requests,\nwe utilize dataset statistics—including the median\nand 99th percentile of input and output lengths\n(discussed in §4.3) to inform our binning strategy.\nBinning Strategy. To effectively handle the\nbroad range of (ik, ok) values, we define discrete\nbin sets for input and output lengths:\nIbins = {2m | m ∈ N, 4 ≤ m ≤ 13}\n= {32, 128, 256, 512, 1024, 2048, 4096, 8192},\nObins = {2n | n ∈ N, 3 ≤ m ≤ 9}\n= {8, 16, 32, 64, 128, 256, 512}.\nThese bin choices ensure sufficient coverage across\nrealistic request distributions. Notably, we exclude\nextremely long input requests ( > 8k tokens) and\ngeneration outputs beyond 512 tokens.\nMapping Requests to Bins. Given a request\nR = (i, o), we map it to the closest ceiling bin:\nI ∗ = min{I ∈ Ibins | I ≥ i},\nO∗ = min{O ∈ Obins | O ≥ o}.\nWe group requests within the same (I ∗, O∗) bin\ninto batches of size B(I ∗, O∗), the maximum al-\nlowable batch size for the given hardware and back-\nend configuration. Each batch processes B(I ∗, O∗)\nrequests in parallel, allowing for more efficient en-\nergy utilization, which is more representative of\nreal-world inference setups.\nGiven our hardware configuration and backend,\nwe collect the estimates of Ebatch(I ∗, O∗), which\ncorresponds to the energy used to serve a request\nof batch size B with input prompts of length I∗\nand output lengths O∗.\nWe collect real energy measurements\nEreal\nbatch(I∗, O∗), representing the observed en-\nergy usage when processing a full batch of size\nB(I ∗, O∗) with input lengths I ∗ and output lengths\nO∗. Thus, the total estimated energy consumption\nacross the workload to serve N requests that fall in\nthe bin is given by:\nbEtotal =\nX\n(I ∗,O∗)\n\u0012 N real(I ∗, O∗)\nB(I ∗, O∗)\n\u0013\nEreal\nbatch(I ∗, O∗),\nwhere N real(I ∗, O∗) is the total number of ob-\nserved requests mapped to bin (I ∗, O∗), and\nN real(I ∗,O∗)\nB(I ∗,O∗) represents the number of batches re-\nquired to process them.\n4.2 Idealized Baseline\nAs a naive baseline, we estimate an upper bound\nof the energy efficiency of these workloads with a\nbaseline derived from the manufacturer-rated hard-\nware speeds ( F LOP SHW ), power draw (TDP)\n,and floating point operations (FLOPs) required for\ninference F LOP s 5. This approximation assumes\nhardware is being utilized as maximum efficiency\nboth in through idealized floating point operation\nthroughput and maximum power draw.\nbEOptimal =\n\u0012 TDP\nF LOP SHW\n\u0013\n×\nX\n(I ∗,O∗)\nN real(I ∗, O∗) × F LOP s(I ∗, O∗)\n7\n\nDataset Mean ± Std Median 99th\nBurstGPT 256.80 ± 242.27 215 1038\nAzure Chat 1631.58 ± 1529.64 928 6683\nAzure Code 2511.28 ± 2133.54 1930 7685\nTable 1: Input Sequence Length Statistics Across Real-\nWorld LLM Workloads\nDataset Mean ± Std Median 99th\nBurstGPT 35.10 ± 108.59 7 478\nAzure Chat 105.51 ± 158.25 41 694\nAzure Code 22.69 ± 74.78 8 271\nTable 2: Output Sequence Length Statistics Across Real-\nWorld LLM Workloads\n4.3 Evaluations\nWe examine a suite of classical NLP tasks and\nLLM inference workloads, each characterized by\na range of different input context and output gen-\neration sequences; with dataset statistics provided\nin Tables 3, 1, 2. We simulate a large-scale offline\nprocessing setting on the RTX A6000 GPUs, in\nwhich examples are binned by sequence lengths\n(as described in §4 and processed in parallel in the\nlargest possible batches that fit in GPU memory.\nUtilizing the simulated workloads described in\nSec 4.1, we estimate the effectiveness of the infer-\nence efficiency optimizations evaluated in Section\n4.1. Based on these results, we select an inference\nframework with efficiency optimizations targeting\nlarge batch inference. Concretely, we consider in-\nference with a dense model utilizing vLLM with\nCUDA graph serialization (eager mode off) on a\nsingle GPU and compare it to unoptimized infer-\nence native PyTorch as a lower bound on energy\nefficiency. In addition, we also model the idealized\nenergy baseline based on the model and hardware\nconfigurations.\nClassical NLP Tasks. We benchmark the energy\nuse in a set of classical natural language process-\ning tasks in the English language: text classifica-\ntion (IMDB, Maas et al., 2011), machine transla-\ntion (WMT-14, Bojar et al., 2014), summarization\n(CNN-DailyMail, Nallapati et al., 2016), and text\ngeneration (Wikitext-2 (Merity et al., 2016)).\nFor each of these tasks, we sample a subset of\n1024 examples with statistics of each dataset for\nthe input and the output tokens provided in Table 3.\n5Based on the Nvidia datasheet for the RTX A6000 GPU,\nwe utilize consider F LOP SHW of 309.7 TFLOPS and a\n300W TDP power draw; and estimate theoretical inference\nFLOPs with the DeepSpeed profiler (Rasley et al., 2020).\nTask Mean ± Std Max Output\nTranslation 49.96 ± 39.39 550 64\nGeneration 136.89 ± 93.13 547 64\nClassification 292.48 ± 239.94 3112 1\nSummarization 838.49 ± 400.70 2386 64\nTable 3: Tokenized Input and Output Length Statistics\nAcross NLP Tasks used for Energy Benchmarking\nwiki imdb CNN wmt\nTask\n0.01\n0.02\n0.03\n0.04\n0.05\nEnergy (kWh)\nPyTorch\nvLLM\nFigure 8: Energy Comparison in doing inference over 1024\nsamples between PyTorch with Compilation off and vLLM\nwith eager model off.\nWe note that the input sequences were padded to\nthe maximum sequence length. The energy profiles\nfor the best run, characterized by the least energy\nare summarized in Figure 8, with consistent reduc-\ntions in energy use provided by inference efficiency\noptimizations.\nReal-World LLM Workloads Additionally, we\nestimate the energy intensity and effectiveness of\nefficiency optimizations on real-world LLM work-\nloads. We simulate the offline processing of LLM\ninference requests as used in applications for short-\nform conversations with the Burst-GPT dataset\n(Wang et al., 2024) and long context conversations\nand code completion with the Azure LLM Infer-\nence chat and code traces (Stojkovic et al., 2024b).\nEach dataset provides a traces of LLM inference\nrequests with their corresponding input context and\noutput generation lengths. As compared with the\nclassical NLP tasks, modern LLM workloads tend\nto be longer in both input context and output gener-\nation token lengths, with code-assist applications\nhaving longer contexts, whereas conversational set-\ntings resulting in longer generations.\nDue to the larger number of requests and in-\ncreased sequence lengths, we observe that these\nworkloads require substantially larger amounts of\nenergy. However, we find that proper applications\nof inference efficiency optimizations can substan-\ntially reduce energy costs with savings of 73.00%,\n8\n\nDataset PyTorch % ∆ vLLM % ∆\nBurstGPT 506.52% 63.75%\nAzure Code 102.79% 26.59%\nAzure Conversation 490.23% 64.22%\nTable 4: Percentage differences of energy consumption\nrelative to theoretical values for Various Tasks with\nOffline Inference.\n37.58%, and 72.18% on BurstGPT, Azure Code\nand Conversation, respectively.\n5 Related Work\nEfficient Methods for LLM Inference To meet\nthe service-level-objective (SLO) serving require-\nments of real deployment settings, efficiency opti-\nmizations for LLM inference are often designed to\noptimize model serving speed, as measured by la-\ntency and time-to-first-token. A variety of methods\nhave been developed to meet these latency con-\nstraints, including: continuous batching (Yu et al.,\n2022), model parallelism (Narayanan et al., 2021;\nHuang et al., 2019; Li et al., 2020), speculative\ndecoding (Liu et al., 2024; Leviathan et al., 2023;\nChen et al., 2023, 2025), and disaggregated serving\n(Zhong et al., 2024).\nSolely optimizing system performance for speed\nis insufficient in characterizing and does not pro-\nvide insight into the model energy use and result-\ning carbon emissions of LLM inference; as such\nmethods may require additional computation or\nexhibit low correlation between efficiency cost in-\ndicators (Dehghani et al., 2022). Recent work has\nexplored methods for explicitly reducing energy re-\nquirements and carbon emissions for LLM serving\nvia disaggregated serving over heterogeneous hard-\nware (Shi et al., 2024), system-wide scheduling\nand request routing to energy-optimized instances\n(Stojkovic et al., 2024b), and prompt directives\nto induce shorter sequence generations (Li et al.,\n2024). However, the exact impact or improvements\nin energy requirements for latency-optimized meth-\nods remains not fully characterized.\nEstimations and Measurement of of Energy Use\nin NLP The energy and carbon emissions of ma-\nchine learning models have been a growing con-\ncern in the research community and industry as the\nscale of models and prevalence of deployment has\nincreased (Schwartz et al., 2020; Wu et al., 2022).\nEstimations of the energy requirements and envi-\nronmental impact of LLMs has largely focused on\nestimation of costs for pretraining and finetuning\ndue to the large singular costs of model develop-\nments (Strubell et al., 2020; Wang et al., 2023;\nLuccioni et al., 2023; Faiz et al., 2023); with large\nindustrial developers similarly reporting the energy\nrequired for pretraining (OLMo et al., 2024; Morri-\nson et al., 2025; Dubey et al., 2024).\nIn contrast to training, inference workloads are\nhigher in variability with variation in request fre-\nquencies, batching, input and output sequence\nlengths executed over diverse hardware platforms\nat scale; and more complex energy use profiles\ndue to variations in power draw during prefill and\ndecoding stages of generation (Patel et al., 2024).\nPrevious work has investigated the comparative\nenergy cost of machine learning models across var-\nious tasks (Luccioni et al., 2024b,a), the energy\ncosts of LMs of various sizes (Samsi et al., 2023;\nWu et al., 2025), the effects of hardware config-\nurations (i.e. GPU power capping and frequency\nscaling; (Samsi et al., 2023)), and the impact of\nsequence length variability and batching strategies\n(Patel et al., 2024; Stojkovic et al., 2024a; Wilkins\net al., 2024). However, such evaluations of infer-\nence energy use often rely on simplified deploy-\nment settings with limited sets of model architec-\ntures and serving frameworks.\n6 Conclusion\nIn this work, we evaluate the impact of common\ninference efficiency optimizations on the energy\nrequirements of large language model serving. We\nexamine a variety of optimization techniques and\nevaluate on representative data corresponding to\nclassical NLP tasks as well as modern LLM de-\nployment settings. We conclude that the effective-\nness of latency optimizations in reducing energy\nuse is highly sensitive to the shape of the input\ndata, underlying hardware architecture, and soft-\nware framework implementations; and that opti-\nmizations cannot be applied uniformly.\nAdditionally, we conduct a case study of classi-\ncal NLP tasks and real-world LLM inference work-\nloads and find that proper application of the studied\ninference optimizations can reduce total energy use\nby up to 73% on the BurstGPT chat dataset.\nLimitations and Risks\nIn this work, we evaluate the energy efficiency and\ncarbon emissions of LLM inference as approxi-\nmated by total GPU power usage. Although GPUs\nthe majority of arithmetic operations required for\n9\n\ninference and operate at a higher TDP than other\ncomponents, we do not account for the energy use\nby other other components of the hardware system\nsuch as power use from CPU, memory, or disk stor-\nage (McAllister et al., 2024; Patel et al., 2024); or\nestimate the energy requirements of other hardware\naccelerator architectures (e.g. TPUs, NPUs, etc.).\nLikewise, we conduct an investigation of com-\nmonly used inference software frameworks and\nstandard efficiency optimizations. However, there\nremain other settings and computational optimiza-\ntions that can be applied to LLM inference, such\nas utilizing: reduced or mixed precision, adaptive\nadjustment of GPU frequency, additional forms of\nmodel parallelism, or other forms of load manage-\nment and workload scheduling; which remain out\nof the scope of this work (Stojkovic et al., 2024b).\nIn this work, we primarily focus on the operation\nenergy use of machine learning inference. Estima-\ntion of the embodied costs of inference; and the\ncosts of machine learning training remain out of\nthe scope of this work.\nAlthough improved characterization of the en-\nergy use of LLM inference can be used to design\nmore efficient serving settings and reduce the en-\nergy needs of inference, it is possible that reduc-\ntions in the cost of pretraining may then lead more\nindividuals and organizations to pursue large model\npretraining (i.e. Jevons Paradox).\nReferences\nAmey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree\nMohan, Nipun Kwatra, Bhargav S Gulavani, Alexey\nTumanov, and Ramachandran Ramjee. 2024. Tam-\ning throughput-latency tradeoff in llm inference with\nsarathi-serve. Proceedings of 18th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion, 2024, Santa Clara.\nJordan Aljbour, Tom Wilson, and P Patel. 2024. Power-\ning intelligence: Analyzing artificial intelligence and\ndata center energy consumption. EPRI White Paper\nno. 3002028905.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nJeff Barr. 2019. Amazon ec2 update-infl instances\nwith aws inferentia chips for high performance cost-\neffective inferencing.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Ales Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nKendrick Cai and Deborah Mary Sophia. 2025. Alpha-\nbet plans massive capex hike, reports cloud revenue\ngrowth slowed. Reuters.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nJian Chen, Vashisth Tiwari, Ranajoy Sadhukhan,\nZhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and\nBeidi Chen. 2025. Magicdec: Breaking the latency-\nthroughput tradeoff for long context generation with\nspeculative decoding. In The Thirteenth Interna-\ntional Conference on Learning Representations.\nBenoit Courty, Victor Schmidt, Sasha Luccioni, Goyal-\nKamal, MarionCoutarel, Boris Feld, Jérémy Lecourt,\nLiamConnell, Amine Saboni, Inimaz, supatomic,\nMathilde Léval, Luis Blanche, Alexis Cruveiller,\nouminasara, Franklin Zhao, Aditya Joshi, Alexis\nBogroff, Hugues de Lavoreille, Niko Laskaris,\nEdoardo Abati, Douglas Blank, Ziyao Wang, Armin\nCatovic, Marc Alencon, Michał St˛ echły, Christian\nBauer, Lucas Otávio N. de Araújo, JPW, and Minerv-\naBooks. 2024. mlco2/codecarbon: v2.4.1.\nMostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer,\nand Ashish Vaswani. 2022. The efficiency misnomer.\nIn International Conference on Learning Representa-\ntions.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nAhmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi,\nPrateek Sharma, Fan Chen, and Lei Jiang. 2023.\nLlmcarbon: Modeling the end-to-end carbon foot-\nprint of large language models. arXiv preprint\narXiv:2309.14393.\nJared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk,\nand Emma Strubell. 2023. The framework tax: Dis-\nparities between inference efficiency in nlp research\nand deployment. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1588–1600.\nStefanos Georgiou, Maria Kechagia, Tushar Sharma,\nFederica Sarro, and Ying Zou. 2022. Green ai: Do\ndeep learning frameworks have different costs? In\nProceedings of the 44th International Conference on\nSoftware Engineering, pages 1082–1094.\n10\n\nAlistair Green, Humayun Tai, Jesse Noffsinger, and\nPankaj Sachdeva. 2024. How data centers and the en-\nergy sector can sate ai’s hunger for power.McKinsey\nand Company.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-\ngia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,\net al. 2024. Olmo: Accelerating the science of lan-\nguage models. arXiv preprint arXiv:2402.00838.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji-\nquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\nGpipe: Efficient training of giant neural networks\nusing pipeline parallelism. Advances in neural infor-\nmation processing systems, 32.\nMike Isaac. 2025. Meta to increase spending to $65\nbillion this year in a.i. push. New York Times.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nGeorge Leopold. 2019. Aws to offer nvidia’s t4\ngpus for ai inferencing. URL: https://web. archive.\norg/web/20220309000921/https://www. hpcwire.\ncom/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/(visited on 2022-04-19).\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, pages 19274–19286. PMLR.\nBaolin Li, Yankai Jiang, Vijay Gadepally, and Devesh\nTiwari. 2024. Sprout: Green generative ai with\ncarbon-efficient llm inference. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 21799–21813.\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and\nShaolei Ren. 2025. Making AI Less \"Thirsty\": Un-\ncovering and Addressing the Secret Water Footprint\nof AI Models. arXiv preprint. ArXiv:2304.03271\n[cs].\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,\nPieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith,\nBrian Vaughan, Pritam Damania, et al. 2020. Pytorch\ndistributed: Experiences on accelerating data parallel\ntraining. arXiv preprint arXiv:2006.15704.\nXiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk\nKwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung,\nZhijie Deng, Ion Stoica, and Hao Zhang. 2024.\nOptimizing speculative decoding for serving large\nlanguage models using goodput. arXiv preprint\narXiv:2406.14066.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat. 2023. Estimating the carbon footprint\nof bloom, a 176b parameter language model. Journal\nof Machine Learning Research, 24(253):1–15.\nSasha Luccioni, Boris Gamazaychikov, Sara Hooker,\nRégis Pierrard, Emma Strubell, Yacine Jernite, and\nCarole-Jean Wu. 2024a. Light bulbs have en-\nergy ratings—so why can’t ai chatbots? Nature,\n632(8026):736–738.\nSasha Luccioni, Yacine Jernite, and Emma Strubell.\n2024b. Power hungry processing: Watts driving the\ncost of ai deployment? In Proceedings of the 2024\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’24, page 85–99, New York,\nNY , USA. Association for Computing Machinery.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nSara McAllister, Fiodar Kazhamiaka, Daniel S Berger,\nRodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh\nSah, Ricardo Bianchini, George Amvrosiadis, Nathan\nBeckmann, et al. 2024. A call for research on storage\nemissions. In Proceedings of the 3rd Workshop on\nSustainable Computer Systems (HotCarbon).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. Preprint, arXiv:1609.07843.\nJacob Morrison, Clara Na, Jared Fernandez, Tim\nDettmers, Emma Strubell, and Jesse Dodge. 2025.\nHolistically evaluating the environmental impact of\ncreating language models. In The Thirteenth Interna-\ntional Conference on Learning Representations.\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld,\nKyle Lo, Jacob Morrison, Sewon Min, Weijia Shi,\nPete Walsh, Oyvind Tafjord, Nathan Lambert, et al.\n2024. Olmoe: Open mixture-of-experts language\nmodels. arXiv preprint arXiv:2409.02060.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\n11\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n15.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groen-\neveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling\nGu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo\n2 furious. arXiv preprint arXiv:2501.00656.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nPratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo\nGoiri, Brijesh Warrier, Nithish Mahalingam, and Ri-\ncardo Bianchini. 2024. Characterizing power man-\nagement opportunities for llms in the cloud. In\nProceedings of the 29th ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3, pages\n207–222.\nDavid Patterson, Joseph Gonzalez, Urs Hölzle, Quoc\nLe, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean.\n2022. The carbon footprint of machine learn-\ning training will plateau, then shrink. Preprint,\narXiv:2204.05149.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap. 2019. Com-\npressive transformers for long-range sequence mod-\nelling. arXiv preprint.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505–3506.\nSiddharth Samsi, Dan Zhao, Joseph McDonald, Baolin\nLi, Adam Michaleas, Michael Jones, William Berg-\neron, Jeremy Kepner, Devesh Tiwari, and Vijay Gade-\npally. 2023. From words to watts: Benchmarking the\nenergy costs of large language model inference. In\n2023 IEEE High Performance Extreme Computing\nConference (HPEC), pages 1–9. IEEE.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54–63.\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa\nLei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan\nKoomey, Eric Masanet, Dale Sartor, et al. 2024. 2024\nunited states data center energy usage report.\nTianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding. 2024.\nGreenllm: Disaggregating large language model serv-\ning on heterogeneous gpus for lower carbon emis-\nsions. arXiv preprint arXiv:2412.20322.\nBrad Smith. 2025. The golden opportunity for american\nai.\nJovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo\nGoiri, and Josep Torrellas. 2024a. Towards greener\nllms: Bringing energy-efficiency to the forefront of\nllm inference. arXiv preprint arXiv:2403.20306.\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Tor-\nrellas, and Esha Choukse. 2024b. Dynamollm: De-\nsigning llm inference clusters for performance and\nenergy efficiency. arXiv preprint arXiv:2408.00741.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2020. Energy and policy considerations for\nmodern deep learning research. In Proceedings of\nthe AAAI conference on artificial intelligence , vol-\nume 34, pages 13693–13696.\nXiaorong Wang, Clara Na, Emma Strubell, Sorelle\nFriedler, and Sasha Luccioni. 2023. Energy and car-\nbon considerations of fine-tuning BERT. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023 , pages 9058–9069, Singapore.\nAssociation for Computational Linguistics.\nYuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhen-\nheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang,\nAmelie Chi Zhou, and Xiaowen Chu. 2024. Burst-\ngpt: A real-world workload dataset to optimize llm\nserving systems. Preprint, arXiv:2401.17644.\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier.\n2024. Offline energy-optimal llm serving: Workload-\nbased energy models for llm inference on heteroge-\nneous systems. ACM SigEnergy newletter.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems, 4:795–813.\nYanran Wu, Inez Hua, and Yi Ding. 2025. Unveil-\ning environmental impacts of large language model\nserving: A functional unit view. arXiv preprint\narXiv:2502.11256.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\n12\n\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun. 2022. Orca: A\ndistributed serving system for {Transformer-Based}\ngenerative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI 22), pages 521–538.\nYinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.\n2024. {DistServe}: Disaggregating prefill and de-\ncoding for goodput-optimized large language model\nserving. In 18th USENIX Symposium on Operat-\ning Systems Design and Implementation (OSDI 24),\npages 193–210.\nA Hardware Details\nIn Table 5, we provide additional details on the\nhardware configurations of the nodes used in our\nbenchmarking experiments.\nB Dataset Licenses\nThe CNN-DailyMail dataset used for summariza-\ntion is released under the Apache-2.0 License. The\ndataset Wikitext-2 dataset for text generation is\navailable under the Creative Commons Attribution-\nShareAlike License. The WMT-14 translation\ndatasets are released for non-commercial use. The\nBurstGPT and Azure trace datasets are released\nunder CC-BY-4.0 licenses.\nC Acknowledgment of AI Assistance\nArtificial intelligence assistance was used to as-\nsist in literature review and for code completion\nassistance, specifically during the creation of visu-\nalizations.\nD Additional Optimzations: Continuous\nBatching\nIn Figure 9, we present additional results on the\nimpact of vLLM’s continuous batching for online\ninference in which we observe that at large batch\nsizes continuous batching yields reductions in en-\nergy use.\nE Additional Sequence Length Results\nIn Figure 10, we present additional results on the\neffects of scaling input and output sequence lengths\nwith the PyTorch framework.\n13\n\nCPU RAM GPU GPU TDP FP32 TFLOPS Bfloat16 TFLOPS\n256xAMD EPYC 7763 1TB Nvidia RTX A6000 300W 38.7 –\n128xAMD EPYC 7513 500GB Nvidia RTX A6000 Ada 300W 91.1 –\n128xAMD EPYC 7763 1TB Nvidia RTX A100-80 GB 300W 156 312\nTable 5: Node Hardware Specifications\n21 23 25 27 29\nBatch size\n−12\n−10\n−8\n−6\n−4\n−2\n0\n2\nEnergy Reduction (%)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n−12.5\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (c) A6000\nFigure 9: Energy reduction comparison between online and offline serving modes across different GPUs\n(Eof f line − Eonline) ∗ 100/Eof f line). The optimizations employed for online serving save up to 5% energy at\nlarger batch sizes\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nPreﬁll energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput or output sequence length (tokens)\n10−2\n10−1\n100\nEnergy for 1024 examples (kWh)\nEnergy scaling for input and output context lengths\nbs=1\nbs=8\nbs=64\nInput\nOutput\nFigure 10: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vanilla PyTorch backend.\n14\n\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nPreﬁll energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. output sequence length\nbs=1\nbs=8\nbs=64\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (s)\nGeneration duration vs. output sequence length\nbs=1\nbs=8\nbs=64\n64 prompt toks\n512 prompt toks\nFigure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference.\nHere, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other\ndimension of sequence length.\n15\n\nclassiﬁcation summarization textgen translation\n0.02\n0.04\n0.06\n0.08\n0.10\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.010\n0.015\n0.020\n0.025\n0.030\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.005\n0.010\n0.015\n0.020\n0.025\nEnergy for 1024 examples (kWh)\nFigure 12: Classical NLP tasks and their energy inten-\nsities with vLLM backends. From top to bottom, the\nbatch size varies from 1, 8, to 128\n16", "metadata": {"url": "https://arxiv.org/pdf/2504.17674", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Energy Considerations of Large Language Model Inference and Efficiency\nOptimizations\nJared Fernandez*1, Clara Na*1, Vashisth Tiwari*1,\nYonatan Bisk1, Sasha Luccioni2, Emma Strubell1\n1Carnegie Mellon University, 2Hugging Face,\nCorrespondence: {jaredfern, clarana, vashisthtiwari}@cmu.edu\nAbstract\nAs large language models (LLMs) scale in size\nand adoption, their computational and environ-\nmental costs continue to rise. Prior benchmark-\ning efforts have primarily focused on latency\nreduction in idealized settings, often overlook-\ning the diverse real-world inference workloads\nthat shape energy use. In this work, we system-\natically analyze the energy implications of com-\nmon inference efficiency optimizations across\ndiverse Natural Language Processing (NLP)\nand generative Artificial Intelligence (AI) work-\nloads, including conversational AI and code\ngeneration. We introduce a modeling approach\nthat approximates real-world LLM workflows\nthrough a binning strategy for input-output to-\nken distributions and batch size variations. Our\nempirical analysis spans software frameworks,\ndecoding strategies, GPU architectures, online\nand offline serving settings, and model paral-\nlelism configurations. We show that the effec-\ntiveness of inference optimizations is highly\nsensitive to workload geometry, software stack,\nand hardware accelerators, demonstrating that\nnaive energy estimates based on FLOPs or the-\noretical GPU utilization significantly underes-\ntimate real-world energy consumption. Our\nfindings reveal that the proper application of\nrelevant inference efficiency optimizations can\nreduce total energy use by up to 73% from un-\noptimized baselines. These insights provide a\nfoundation for sustainable LLM deployment\nand inform energy-efficient design strategies\nfor future AI infrastructure.\n1 Introduction\nImprovements in task performance by large lan-\nguage models (LLMs) have prompted large-scale\ninvestments in computing hardware and energy in-\nfrastructure to support the development and de-\nployment of LLM and related machine learning\nmodels (Isaac, 2025; Smith, 2025; Cai and Sophia,\n*Equal contribution\nBurstGPT Azure Code Azure Conv.\nTask\n101\n102\n103\nEnergy (kWh)\nTheoretical\nPyTorch\nvLLM\nFigure 1: Proper application of efficiency methods with\noptimized vLLM (orange) approaches the ideal energy\nconsumption (green) as compared with an unoptimized\nbaseline PyTorch (purple) implementation.\n2025). However, the growing prevalence of LLMs\nyields commensurate increases in the energy de-\nmand, water use, and carbon emissions associated\nwith their development and deployment (Morrison\net al., 2025; Li et al., 2025; Strubell et al., 2020;\nLuccioni et al., 2024b). Primarily motivated by the\nincreased demands from LLM and AI workloads,\nprojections estimate that that data centers consume\nbetween 9.1% and 11.7% of the total US energy de-\nmand by 2030 (Aljbour et al., 2024; Shehabi et al.,\n2024; Green et al., 2024). However, such projec-\ntions of energy use primarily rely upon sector-wide\nestimates of demand or substantial simplifications\nof the of the energy demands of individual models.\nIn order to develop effective energy policy for\nthis growing demand, it is necessary to characterize\nthe underlying computational workloads of devel-\nopment (i.e. model training) and deployment (i.e.\ninference). In particular, the cost and efficiency of\ninference is especially crucial due to the scale and\nincreased frequency at which models are served for\nrepeated use. Concretely, Meta reports that infer-\nence workloads constitute up to 70% of their AI\npower consumption (Wu et al., 2022) while Google\nattributes 60% of their ML energy (Patterson et al.,\n2022) and between 80 to 90% of ML AWS cloud\ncomputing demand (Barr, 2019; Leopold, 2019).\n1\narXiv:2504.17674v1  [cs.CL]  24 Apr 2025", "sentences": [{"text": "Energy Considerations of Large Language Model Inference and Efficiency\nOptimizations\nJared Fernandez*1, Clara Na*1, Vashisth Tiwari*1,\nYonatan Bisk1, Sasha Luccioni2, Emma Strubell1\n1Carnegie Mellon University, 2Hugging Face,\nCorrespondence: {jaredfern, clarana, vashisthtiwari}@cmu.edu\nAbstract\nAs large language models (LLMs) scale in size\nand adoption, their computational and environ-\nmental costs continue to rise.", "metadata": {}}, {"text": "Prior benchmark-\ning efforts have primarily focused on latency\nreduction in idealized settings, often overlook-\ning the diverse real-world inference workloads\nthat shape energy use.", "metadata": {}}, {"text": "In this work, we system-\natically analyze the energy implications of com-\nmon inference efficiency optimizations across\ndiverse Natural Language Processing (NLP)\nand generative Artificial Intelligence (AI) work-\nloads, including conversational AI and code\ngeneration.", "metadata": {}}, {"text": "We introduce a modeling approach\nthat approximates real-world LLM workflows\nthrough a binning strategy for input-output to-\nken distributions and batch size variations.", "metadata": {}}, {"text": "Our\nempirical analysis spans software frameworks,\ndecoding strategies, GPU architectures, online\nand offline serving settings, and model paral-\nlelism configurations.", "metadata": {}}, {"text": "We show that the effec-\ntiveness of inference optimizations is highly\nsensitive to workload geometry, software stack,\nand hardware accelerators, demonstrating that\nnaive energy estimates based on FLOPs or the-\noretical GPU utilization significantly underes-\ntimate real-world energy consumption.", "metadata": {}}, {"text": "Our\nfindings reveal that the proper application of\nrelevant inference efficiency optimizations can\nreduce total energy use by up to 73% from un-\noptimized baselines.", "metadata": {}}, {"text": "These insights provide a\nfoundation for sustainable LLM deployment\nand inform energy-efficient design strategies\nfor future AI infrastructure.", "metadata": {}}, {"text": "1 Introduction\nImprovements in task performance by large lan-\nguage models (LLMs) have prompted large-scale\ninvestments in computing hardware and energy in-\nfrastructure to support the development and de-\nployment of LLM and related machine learning\nmodels (Isaac, 2025;", "metadata": {}}, {"text": "Smith, 2025;", "metadata": {}}, {"text": "Cai and Sophia,\n*Equal contribution\nBurstGPT Azure Code Azure Conv.", "metadata": {}}, {"text": "Task\n101\n102\n103\nEnergy (kWh)\nTheoretical\nPyTorch\nvLLM\nFigure 1: Proper application of efficiency methods with\noptimized vLLM (orange) approaches the ideal energy\nconsumption (green) as compared with an unoptimized\nbaseline PyTorch (purple) implementation.", "metadata": {}}, {"text": "2025).", "metadata": {}}, {"text": "However, the growing prevalence of LLMs\nyields commensurate increases in the energy de-\nmand, water use, and carbon emissions associated\nwith their development and deployment (Morrison\net al., 2025;", "metadata": {}}, {"text": "Li et al., 2025;", "metadata": {}}, {"text": "Strubell et al., 2020;", "metadata": {}}, {"text": "Luccioni et al., 2024b).", "metadata": {}}, {"text": "Primarily motivated by the\nincreased demands from LLM and AI workloads,\nprojections estimate that that data centers consume\nbetween 9.1% and 11.7% of the total US energy de-\nmand by 2030 (Aljbour et al., 2024;", "metadata": {}}, {"text": "Shehabi et al.,\n2024;", "metadata": {}}, {"text": "Green et al., 2024).", "metadata": {}}, {"text": "However, such projec-\ntions of energy use primarily rely upon sector-wide\nestimates of demand or substantial simplifications\nof the of the energy demands of individual models.", "metadata": {}}, {"text": "In order to develop effective energy policy for\nthis growing demand, it is necessary to characterize\nthe underlying computational workloads of devel-\nopment (i.e.", "metadata": {}}, {"text": "model training) and deployment (i.e.", "metadata": {}}, {"text": "inference).", "metadata": {}}, {"text": "In particular, the cost and efficiency of\ninference is especially crucial due to the scale and\nincreased frequency at which models are served for\nrepeated use.", "metadata": {}}, {"text": "Concretely, Meta reports that infer-\nence workloads constitute up to 70% of their AI\npower consumption (Wu et al., 2022) while Google\nattributes 60% of their ML energy (Patterson et al.,\n2022) and between 80 to 90% of ML AWS cloud\ncomputing demand (Barr, 2019;", "metadata": {}}, {"text": "Leopold, 2019).", "metadata": {}}, {"text": "1\narXiv:2504.17674v1  [cs.CL]  24 Apr 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "To address the problem of inference efficiency,\nthe NLP and machine learning research commu-\nnities have developed various optimizations span-\nning: algorithms, software frameworks, and hard-\nware accelerators. Such optimizations have pri-\nmarily targeted improvements in model speed (e.g.\nlatency and throughput; (Leviathan et al., 2023;\nKwon et al., 2023)). Moreover, these methods are\nfrequently assessed in constrained settings or on\nsimplified datasets that fail to capture the broad\ndiversity of real-world tasks. These tasks range\nfrom traditional NLP applications like sequence\ntagging and summarization to more computation-\nally demanding workloads such as synthetic data\ngeneration and chain-of-thought reasoning. There\nremains a critical gap in understanding of the en-\nergy costs of language model inference, especially\nwhen efficiency interventions are applied jointly in\nreal-world settings.\nIn this work, we examine the energy costs of\nLLM inference and present a comprehensive anal-\nysis of the impact of: data dimensionality, decod-\ning strategies, serving frameworks, compilation\ntechniques, GPU hardware platforms, model par-\nallelism, and architectural variants on total energy\nuse during inference. Based on our energy profiling\nacross these optimizations, we approximate offline\ninference with LLMs based on real-world workload\nwith variable sequence lengths and batching, con-\nsidering both an upper bound of naive unoptimized\ninference and a lower bound of theoretical opti-\nmized inference. Our analysis reveals that while\nidealized estimations of hardware utilization sub-\nstantially underestimate the energy use of language\nmodel inference, proper application of inference\nefficiency optimizations can substantially reduce\nthe energy requirements of inference by up to\n73% from unoptimized baselines with vanilla\nPyTorch and Huggingface Transformersand to\nwithin 26.6% of theoretical ideal performance on\nsimulated offline workloads (see Table 4).\n2 Methodology\nIn the following section, we describe our experi-\nmental setup for evaluating inference efficiency.\nModel Architectures. We focus our experiments\non language models ranging from 1B to 32B pa-\nrameters, primarily evaluating Llama-3.1-8B-Ba\nse and Llama-3.1-8B-Instruct models as repre-\nsentative decoder-only transformers (Dubey et al.,\n2024). To investigate effects of scaling model archi-\ntecture, we include the Qwen-1.5-32B model (Bai\net al., 2023). For architectural comparisons, we\nanalyze the sparse OLMoE mixture-of-expert (MoE)\nmodel alongside its dense counterparts – the 1B and\n7B OLMo architectures – which maintain compara-\nble active and total parameter counts, respectively\n(Muennighoff et al., 2024; Groeneveld et al., 2024).\nData Dimensionality We investigate the impact\nof data dimensionality across three key dimensions:\ninput sequence lengths, output generation lengths,\nand batch sizes.\nInference with large language models is com-\nmonly decomposed into two stages: prefilling and\ntoken generation, each with a different energy pro-\nfile (Patel et al., 2024). The prefill stage processes\nprompts in parallel and is typically compute-bound,\nachieving high GPU utilization. In contrast, the\nautoregressive decoding stage is typically memory-\nbound and leads to GPU under-utilization. These\nbottlenecks and their resulting energy profiles shift\nwith input and output lengths.\nTo address GPU under-utilization during gener-\nation, serving systems employ batching strategies.\nHowever, the effectiveness of batching varies with\ninput-output characteristics (Agrawal et al., 2024;\nLi et al., 2024). Long input sequences limit maxi-\nmum batch sizes due to memory constraints, while\nvariable output lengths can lead to inefficient batch\nutilization as some sequences complete before oth-\ners.\nOur analysis spans batch sizes from 1 (single-\nexample inference) to task-dependent maximums\n(up to 1024), ensuring coverage of a broad range\nof maximally realistic settings.\nWe ground analysis in NLP workloads spanning\ntext classification, summarization, translation, and\nopen-ended text generation. Different tasks exhibit\ndifferent data dimensionalities: classification in-\nvolves minimal generation (often a single token),\nsummarization pairs long contexts with medium-\nlength outputs, and translation typically assumes\nbalanced input-output lengths. Input length statis-\ntics in considered datasets are shown in Table 3.\nIn a controlled sweep, we explore scenarios with\nup to 32k input tokens and 4k output tokens, vary-\ning sequence lengths by powers of two. We fix\ngeneration to 64 or 8 tokens when varying context\nlengths, and assume 512 or 64 token input context\nwhen varying output lengths. Input context length\nis enforced via truncation of longer sequences from\nPG19 (Rae et al., 2019).\n2", "sentences": [{"text": "To address the problem of inference efficiency,\nthe NLP and machine learning research commu-\nnities have developed various optimizations span-\nning: algorithms, software frameworks, and hard-\nware accelerators.", "metadata": {}}, {"text": "Such optimizations have pri-\nmarily targeted improvements in model speed (e.g.", "metadata": {}}, {"text": "latency and throughput;", "metadata": {}}, {"text": "(Leviathan et al., 2023;", "metadata": {}}, {"text": "Kwon et al., 2023)).", "metadata": {}}, {"text": "Moreover, these methods are\nfrequently assessed in constrained settings or on\nsimplified datasets that fail to capture the broad\ndiversity of real-world tasks.", "metadata": {}}, {"text": "These tasks range\nfrom traditional NLP applications like sequence\ntagging and summarization to more computation-\nally demanding workloads such as synthetic data\ngeneration and chain-of-thought reasoning.", "metadata": {}}, {"text": "There\nremains a critical gap in understanding of the en-\nergy costs of language model inference, especially\nwhen efficiency interventions are applied jointly in\nreal-world settings.", "metadata": {}}, {"text": "In this work, we examine the energy costs of\nLLM inference and present a comprehensive anal-\nysis of the impact of: data dimensionality, decod-\ning strategies, serving frameworks, compilation\ntechniques, GPU hardware platforms, model par-\nallelism, and architectural variants on total energy\nuse during inference.", "metadata": {}}, {"text": "Based on our energy profiling\nacross these optimizations, we approximate offline\ninference with LLMs based on real-world workload\nwith variable sequence lengths and batching, con-\nsidering both an upper bound of naive unoptimized\ninference and a lower bound of theoretical opti-\nmized inference.", "metadata": {}}, {"text": "Our analysis reveals that while\nidealized estimations of hardware utilization sub-\nstantially underestimate the energy use of language\nmodel inference, proper application of inference\nefficiency optimizations can substantially reduce\nthe energy requirements of inference by up to\n73% from unoptimized baselines with vanilla\nPyTorch and Huggingface Transformersand to\nwithin 26.6% of theoretical ideal performance on\nsimulated offline workloads (see Table 4).", "metadata": {}}, {"text": "2 Methodology\nIn the following section, we describe our experi-\nmental setup for evaluating inference efficiency.", "metadata": {}}, {"text": "Model Architectures.", "metadata": {}}, {"text": "We focus our experiments\non language models ranging from 1B to 32B pa-\nrameters, primarily evaluating Llama-3.1-8B-Ba\nse and Llama-3.1-8B-Instruct models as repre-\nsentative decoder-only transformers (Dubey et al.,\n2024).", "metadata": {}}, {"text": "To investigate effects of scaling model archi-\ntecture, we include the Qwen-1.5-32B model (Bai\net al., 2023).", "metadata": {}}, {"text": "For architectural comparisons, we\nanalyze the sparse OLMoE mixture-of-expert (MoE)\nmodel alongside its dense counterparts – the 1B and\n7B OLMo architectures – which maintain compara-\nble active and total parameter counts, respectively\n(Muennighoff et al., 2024;", "metadata": {}}, {"text": "Groeneveld et al., 2024).", "metadata": {}}, {"text": "Data Dimensionality We investigate the impact\nof data dimensionality across three key dimensions:\ninput sequence lengths, output generation lengths,\nand batch sizes.", "metadata": {}}, {"text": "Inference with large language models is com-\nmonly decomposed into two stages: prefilling and\ntoken generation, each with a different energy pro-\nfile (Patel et al., 2024).", "metadata": {}}, {"text": "The prefill stage processes\nprompts in parallel and is typically compute-bound,\nachieving high GPU utilization.", "metadata": {}}, {"text": "In contrast, the\nautoregressive decoding stage is typically memory-\nbound and leads to GPU under-utilization.", "metadata": {}}, {"text": "These\nbottlenecks and their resulting energy profiles shift\nwith input and output lengths.", "metadata": {}}, {"text": "To address GPU under-utilization during gener-\nation, serving systems employ batching strategies.", "metadata": {}}, {"text": "However, the effectiveness of batching varies with\ninput-output characteristics (Agrawal et al., 2024;", "metadata": {}}, {"text": "Li et al., 2024).", "metadata": {}}, {"text": "Long input sequences limit maxi-\nmum batch sizes due to memory constraints, while\nvariable output lengths can lead to inefficient batch\nutilization as some sequences complete before oth-\ners.", "metadata": {}}, {"text": "Our analysis spans batch sizes from 1 (single-\nexample inference) to task-dependent maximums\n(up to 1024), ensuring coverage of a broad range\nof maximally realistic settings.", "metadata": {}}, {"text": "We ground analysis in NLP workloads spanning\ntext classification, summarization, translation, and\nopen-ended text generation.", "metadata": {}}, {"text": "Different tasks exhibit\ndifferent data dimensionalities: classification in-\nvolves minimal generation (often a single token),\nsummarization pairs long contexts with medium-\nlength outputs, and translation typically assumes\nbalanced input-output lengths.", "metadata": {}}, {"text": "Input length statis-\ntics in considered datasets are shown in Table 3.", "metadata": {}}, {"text": "In a controlled sweep, we explore scenarios with\nup to 32k input tokens and 4k output tokens, vary-\ning sequence lengths by powers of two.", "metadata": {}}, {"text": "We fix\ngeneration to 64 or 8 tokens when varying context\nlengths, and assume 512 or 64 token input context\nwhen varying output lengths.", "metadata": {}}, {"text": "Input context length\nis enforced via truncation of longer sequences from\nPG19 (Rae et al., 2019).", "metadata": {}}, {"text": "2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Figure 2: Controlled sweeps of input and output sequence lengths on A6000 GPUs, on vLLM backend, described\nin §3.1. We decompose inference costs into prefill and decode energy. At small batch sizes and input sequence\nlengths, energy intensity of a workload scales sub-linearly with increasing sequence length input sequence lengths.\nDecoding is more energy intensive per token than prefill, but energy intensity begins scaling linearly even for short\ngenerations and small batch sizes with the vLLM framework.\nDecoding Strategies. Different decoding strate-\ngies used for generation have different computa-\ntional profiles and can have a substantial impact on\nthe generation efficiency (Kwon et al., 2023). In\norder to study the impact of sampling methods and\nauto-regressive decoding strategies, we investigate\ngreedy decoding, beam search decoding, temper-\nature sampling, top-p decoding affect the energy\nrequirements and end-to-end latency (Holtzman\net al., 2020).\nIn addition to auto-regressive decoding, we study\nthe impact of speculative decoding. Speculative de-\ncoding is commonly used as a latency minimization\ninference optimization (Kwon et al., 2023). In spec-\nulative decoding, a lightweight draft model is used\nto predict multiple tokens (γ) which are then veri-\nfied by the target model in parallel (Leviathan et al.,\n2023; Chen et al., 2023). Speculative decoding\nprovides latency improvement by better utilizing\nGPUs over autoregressive decoding.\nIn our experiments, we use the following target-\ndraft model pairs with a look-ahead value γ = 4\nacross various batch sizes: DeepSeek-R1-Distil\nl-Qwen-32B with mobiuslabsgmbh/DeepSeek-R\n1-ReDistill-Qwen-1.5B-v1.1 (Guo et al., 2025;\nYang et al., 2024); Llama-3.1-8B-Base with Lla\nma-3.2-1B (Dubey et al., 2024).\nSoftware Optimizations. Choice in the software\nframeworks used for inference significantly im-\npacts both latency and energy efficiency through op-\ntimized kernel implementations and computational\ngraph management (Georgiou et al., 2022; Fernan-\ndez et al., 2023). We evaluate two widely-adopted\nlibraries used in LLM inference: native PyTorch\nwith HuggingFace transformers (Wolf et al., 2020),\nand vLLM, an optimized framework for LLM infer-\nence that achieves improved compute and memory\nutilization (Paszke et al., 2019; Kwon et al., 2023);\nexperiments are conducted in bfloat16 precision.\nWithin these frameworks, we compare with a\nnative PyTorch baselines with Just-in-Time com-\npilation via TorchInductor (i.e. torch.compile)\nand CUDA Graphs kernel serialization. Further-\nmore, for vLLM, we evaluate continuous batching\nwhich efficiently handles variable output lengths\nin batch processing by overlaying sequences (Yu\net al., 2022).\nHardware Platforms. Our experiments are con-\nducted using an on-premise heterogeneous server\nwith multiple GPU types and node configurations.\nSpecifically, we conduct experiments on multiple\ngenerations of consumer workstation and datacen-\nter GPU accelerators from the Ampere (A6000,\nA100 80GB PCIe), and Ada Lovelace (A6000 Ada)\nmicroarchitecture.\nAll experiments run on 8-GPU nodes with stan-\ndardized node- and job-level CPU and RAM con-\nfigurations for each GPU type. For multi-GPU ex-\nperiments, we utilize up to 4 GPUs simultaneously,\ninvestigating tensor parallel inference with group\nsizes of 2 and 4 devices. 1. We examine both stan-\ndard and speculative decoding approaches using\nthe Llama-3.1-8B and Qwen-32B models. Addi-\ntional details on computing hardware are provided\nin Appendix A.\n1This configuration leaves 4-7 GPUs available for other\nusers. While the Slurm scheduler does not enforce complete\nisolation in network, memory, and CPU infrastructure across\njobs, concurrent workloads in practice were not CPU- or\nmemory-intensive enough to impact ours significantly – for\nexample, in the vast majority of cases (98%), an ambient\nmeasurement of the RAM utilization in a node our jobs were\nrunning on was less than 20% of the total available\n3", "sentences": [{"text": "Figure 2: Controlled sweeps of input and output sequence lengths on A6000 GPUs, on vLLM backend, described\nin §3.1.", "metadata": {}}, {"text": "We decompose inference costs into prefill and decode energy.", "metadata": {}}, {"text": "At small batch sizes and input sequence\nlengths, energy intensity of a workload scales sub-linearly with increasing sequence length input sequence lengths.", "metadata": {}}, {"text": "Decoding is more energy intensive per token than prefill, but energy intensity begins scaling linearly even for short\ngenerations and small batch sizes with the vLLM framework.", "metadata": {}}, {"text": "Decoding Strategies.", "metadata": {}}, {"text": "Different decoding strate-\ngies used for generation have different computa-\ntional profiles and can have a substantial impact on\nthe generation efficiency (Kwon et al., 2023).", "metadata": {}}, {"text": "In\norder to study the impact of sampling methods and\nauto-regressive decoding strategies, we investigate\ngreedy decoding, beam search decoding, temper-\nature sampling, top-p decoding affect the energy\nrequirements and end-to-end latency (Holtzman\net al., 2020).", "metadata": {}}, {"text": "In addition to auto-regressive decoding, we study\nthe impact of speculative decoding.", "metadata": {}}, {"text": "Speculative de-\ncoding is commonly used as a latency minimization\ninference optimization (Kwon et al., 2023).", "metadata": {}}, {"text": "In spec-\nulative decoding, a lightweight draft model is used\nto predict multiple tokens (γ) which are then veri-\nfied by the target model in parallel (Leviathan et al.,\n2023;", "metadata": {}}, {"text": "Chen et al., 2023).", "metadata": {}}, {"text": "Speculative decoding\nprovides latency improvement by better utilizing\nGPUs over autoregressive decoding.", "metadata": {}}, {"text": "In our experiments, we use the following target-\ndraft model pairs with a look-ahead value γ = 4\nacross various batch sizes: DeepSeek-R1-Distil\nl-Qwen-32B with mobiuslabsgmbh/DeepSeek-R\n1-ReDistill-Qwen-1.5B-v1.1 (Guo et al., 2025;", "metadata": {}}, {"text": "Yang et al., 2024);", "metadata": {}}, {"text": "Llama-3.1-8B-Base with Lla\nma-3.2-1B (Dubey et al., 2024).", "metadata": {}}, {"text": "Software Optimizations.", "metadata": {}}, {"text": "Choice in the software\nframeworks used for inference significantly im-\npacts both latency and energy efficiency through op-\ntimized kernel implementations and computational\ngraph management (Georgiou et al., 2022;", "metadata": {}}, {"text": "Fernan-\ndez et al., 2023).", "metadata": {}}, {"text": "We evaluate two widely-adopted\nlibraries used in LLM inference: native PyTorch\nwith HuggingFace transformers (Wolf et al., 2020),\nand vLLM, an optimized framework for LLM infer-\nence that achieves improved compute and memory\nutilization (Paszke et al., 2019;", "metadata": {}}, {"text": "Kwon et al., 2023);", "metadata": {}}, {"text": "experiments are conducted in bfloat16 precision.", "metadata": {}}, {"text": "Within these frameworks, we compare with a\nnative PyTorch baselines with Just-in-Time com-\npilation via TorchInductor (i.e.", "metadata": {}}, {"text": "torch.compile)\nand CUDA Graphs kernel serialization.", "metadata": {}}, {"text": "Further-\nmore, for vLLM, we evaluate continuous batching\nwhich efficiently handles variable output lengths\nin batch processing by overlaying sequences (Yu\net al., 2022).", "metadata": {}}, {"text": "Hardware Platforms.", "metadata": {}}, {"text": "Our experiments are con-\nducted using an on-premise heterogeneous server\nwith multiple GPU types and node configurations.", "metadata": {}}, {"text": "Specifically, we conduct experiments on multiple\ngenerations of consumer workstation and datacen-\nter GPU accelerators from the Ampere (A6000,\nA100 80GB PCIe), and Ada Lovelace (A6000 Ada)\nmicroarchitecture.", "metadata": {}}, {"text": "All experiments run on 8-GPU nodes with stan-\ndardized node- and job-level CPU and RAM con-\nfigurations for each GPU type.", "metadata": {}}, {"text": "For multi-GPU ex-\nperiments, we utilize up to 4 GPUs simultaneously,\ninvestigating tensor parallel inference with group\nsizes of 2 and 4 devices.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "We examine both stan-\ndard and speculative decoding approaches using\nthe Llama-3.1-8B and Qwen-32B models.", "metadata": {}}, {"text": "Addi-\ntional details on computing hardware are provided\nin Appendix A.", "metadata": {}}, {"text": "1This configuration leaves 4-7 GPUs available for other\nusers.", "metadata": {}}, {"text": "While the Slurm scheduler does not enforce complete\nisolation in network, memory, and CPU infrastructure across\njobs, concurrent workloads in practice were not CPU- or\nmemory-intensive enough to impact ours significantly – for\nexample, in the vast majority of cases (98%), an ambient\nmeasurement of the RAM utilization in a node our jobs were\nrunning on was less than 20% of the total available\n3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "20 21 22 23 24 25 26 27\nBatch Size\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nSpeculative Decoding\nGreedy Decoding\nBeam Search\nTemperature Sampling\nTop-p Sampling\nFigure 3: At small batch sizes, speculative decoding\nprovides reduced latency and energy savings. At larger\nbatch size speculative decoding increases energy.\nPerformance Measures. We evaluate the ef-\nficiency of inference by measuring the latency,\nthroughput, GPU energy, and GPU power required\nfor the inference of 1,024 examples 2. Total en-\nergy use and GPU power metrics are measured\nusing Nvidia Management Library (NVML) via\nthe CodeCarbon library (Courty et al., 2024). Prior\nto evaluation, we conduct a warmup on up to 20\nbatches to allow for memory allocation, required\nCUDA graph capture, and JiT compilation 3. Re-\nsults are reported as the mean values energy use,\nlatency, or power usage of three runs.\n3 Results\nIn the following section, we examine the effects of\nvariations of data dimensionality, model architec-\nture, decoding strategies, and software optimiza-\ntions on inference energy use.\n3.1 Effects of Dataset and Sequence Length\nWe present results from our controlled sweep of\nsequence lengths and batch sizes in Figure 2. Pre-\nfill costs increase as a function of input sequence\nlength, at the same rate regardless of batch sizes\nwhen scaling sequences larger than 128 tokens. At\nshorter sequence lengths and smaller batch sizes,\nthe energy costs of prefill are constant regardless\nof the computational workload due to significant\nundersaturation of the accelerator. Although we\nfix output generation tokens to 64, we verify that\nat this convergence in rate of energy intensity in-\ncrease occurs at the same point when instead fixing\ngeneration length to 8 tokens; see Figure 11 in\nAppendix E.\n2For experiments with batch sizes larger than 256, metrics\nare computed over 4096 examples and then normalized.\n3Due to size, warmup is limited to 4 batches for inference\nwith the Qwen-32B.\n21 23 25 27 29\nBatch Size\n2−10\n2−9\n2−8\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nOLMo 1B\nOLMo 7B\nOLMoE 1B-7B\nFigure 4: Mixture-of-Experts LLMs require more en-\nergy than dense models with comparable active parame-\nters; differences are pronounced at larger batch sizes.\nIn Figure 2, the energy intensity of the decode\nlikewise scales with input context length only at\nlarger input sequence lengths.\nHowever, the energy intensity of decoding scales\nlinearly with sequence length regardless of se-\nquence length or batch sizes due to the autoregres-\nsive, sequential nature of decoding.\nGenerally, decoding energy dominates the over-\nall workload in all settings but those with the short-\nest generation lengths, such as those seen in classi-\nfication workloads and short form summarization.\nNote the log-log scale and the parallel linear trends,\nwhere the differences in intercepts are proportion-\nate with the differences in batch size 4. In the\nfollowing sections, we discuss a variety of algorith-\nmic and software interventions that are appropriate\nfor different types of workload geometries.\n3.2 Effects of Algorithmic Optimizations\nSpeculative Decoding Only Reduces Energy at\nLow Batch Sizes. Speculative decoding is com-\nmonly used to achieve inference speedups in low-\nbatch inference in which autoregressive decoding\nfails to achieve high GPU VRAM utilization. How-\never, for large batch sizes where GPU is already\nsaturated, draft model speculation and excess verifi-\ncations introduce additional overhead. In the large\nbatch case, for short to medium contexts, LLM\ninference is typically compute bound, making spec-\nulative decoding slower than autoregressive decod-\ning with the target model (Chen et al., 2025; Liu\net al., 2024).\nCompared to variations in energy use from al-\nternate decoding strategies and sampling methods,\nspeculative decoding has the greatest effect on the\n4See Fig 10 in Appendix E for additional results on vanilla\nPyTorch backend, and Figure 12 for comparison with real\nenergy intensity measurements for a sample of classical NLP\ntasks\n4", "sentences": [{"text": "20 21 22 23 24 25 26 27\nBatch Size\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nSpeculative Decoding\nGreedy Decoding\nBeam Search\nTemperature Sampling\nTop-p Sampling\nFigure 3: At small batch sizes, speculative decoding\nprovides reduced latency and energy savings.", "metadata": {}}, {"text": "At larger\nbatch size speculative decoding increases energy.", "metadata": {}}, {"text": "Performance Measures.", "metadata": {}}, {"text": "We evaluate the ef-\nficiency of inference by measuring the latency,\nthroughput, GPU energy, and GPU power required\nfor the inference of 1,024 examples 2.", "metadata": {}}, {"text": "Total en-\nergy use and GPU power metrics are measured\nusing Nvidia Management Library (NVML) via\nthe CodeCarbon library (Courty et al., 2024).", "metadata": {}}, {"text": "Prior\nto evaluation, we conduct a warmup on up to 20\nbatches to allow for memory allocation, required\nCUDA graph capture, and JiT compilation 3.", "metadata": {}}, {"text": "Re-\nsults are reported as the mean values energy use,\nlatency, or power usage of three runs.", "metadata": {}}, {"text": "3 Results\nIn the following section, we examine the effects of\nvariations of data dimensionality, model architec-\nture, decoding strategies, and software optimiza-\ntions on inference energy use.", "metadata": {}}, {"text": "3.1 Effects of Dataset and Sequence Length\nWe present results from our controlled sweep of\nsequence lengths and batch sizes in Figure 2.", "metadata": {}}, {"text": "Pre-\nfill costs increase as a function of input sequence\nlength, at the same rate regardless of batch sizes\nwhen scaling sequences larger than 128 tokens.", "metadata": {}}, {"text": "At\nshorter sequence lengths and smaller batch sizes,\nthe energy costs of prefill are constant regardless\nof the computational workload due to significant\nundersaturation of the accelerator.", "metadata": {}}, {"text": "Although we\nfix output generation tokens to 64, we verify that\nat this convergence in rate of energy intensity in-\ncrease occurs at the same point when instead fixing\ngeneration length to 8 tokens;", "metadata": {}}, {"text": "see Figure 11 in\nAppendix E.", "metadata": {}}, {"text": "2For experiments with batch sizes larger than 256, metrics\nare computed over 4096 examples and then normalized.", "metadata": {}}, {"text": "3Due to size, warmup is limited to 4 batches for inference\nwith the Qwen-32B.", "metadata": {}}, {"text": "21 23 25 27 29\nBatch Size\n2−10\n2−9\n2−8\n2−7\n2−6\n2−5\n2−4\nEnergy (kWh)\nOLMo 1B\nOLMo 7B\nOLMoE 1B-7B\nFigure 4: Mixture-of-Experts LLMs require more en-\nergy than dense models with comparable active parame-\nters;", "metadata": {}}, {"text": "differences are pronounced at larger batch sizes.", "metadata": {}}, {"text": "In Figure 2, the energy intensity of the decode\nlikewise scales with input context length only at\nlarger input sequence lengths.", "metadata": {}}, {"text": "However, the energy intensity of decoding scales\nlinearly with sequence length regardless of se-\nquence length or batch sizes due to the autoregres-\nsive, sequential nature of decoding.", "metadata": {}}, {"text": "Generally, decoding energy dominates the over-\nall workload in all settings but those with the short-\nest generation lengths, such as those seen in classi-\nfication workloads and short form summarization.", "metadata": {}}, {"text": "Note the log-log scale and the parallel linear trends,\nwhere the differences in intercepts are proportion-\nate with the differences in batch size 4.", "metadata": {}}, {"text": "In the\nfollowing sections, we discuss a variety of algorith-\nmic and software interventions that are appropriate\nfor different types of workload geometries.", "metadata": {}}, {"text": "3.2 Effects of Algorithmic Optimizations\nSpeculative Decoding Only Reduces Energy at\nLow Batch Sizes.", "metadata": {}}, {"text": "Speculative decoding is com-\nmonly used to achieve inference speedups in low-\nbatch inference in which autoregressive decoding\nfails to achieve high GPU VRAM utilization.", "metadata": {}}, {"text": "How-\never, for large batch sizes where GPU is already\nsaturated, draft model speculation and excess verifi-\ncations introduce additional overhead.", "metadata": {}}, {"text": "In the large\nbatch case, for short to medium contexts, LLM\ninference is typically compute bound, making spec-\nulative decoding slower than autoregressive decod-\ning with the target model (Chen et al., 2025;", "metadata": {}}, {"text": "Liu\net al., 2024).", "metadata": {}}, {"text": "Compared to variations in energy use from al-\nternate decoding strategies and sampling methods,\nspeculative decoding has the greatest effect on the\n4See Fig 10 in Appendix E for additional results on vanilla\nPyTorch backend, and Figure 12 for comparison with real\nenergy intensity measurements for a sample of classical NLP\ntasks\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "21 23 25 27 29\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (c) A6000\nFigure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends\nof 1024 samples for 64 output tokens. For each GPU, we compare PyTorch with and without compilation, and\nvLLM with and without CUDA Graph serialization. The line in black represents the maximum allowable batch size\nfor PyTorch. Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations\ncan serve a larger batch size.\nenergy use and latency of language model infer-\nence. At smaller batch sizes ( ≤ 16) speculative\ndecoding is effective in reducing the total energy\ncost of inference with up to +29.14% compared to\nsingle-example inference (Figure 3). However, au-\ntoregressive decoding methods are more efficient\nat larger batch sizes, with speculative decoding\nrequiring 25.65% more energy when performing\ninference at a batch size of 128.\nMixture of Experts Incurs Higher Inference En-\nergy Costs. Sparse mixture-of-experts are often\nutilized as an alternative architecture due to their\nincreased sample efficiency during training and\nincreased performance relative to dense neural net-\nworks with the same number of active parameters.\nAlthough both dense OLMo-1B and the OLMoE1B-7\nB mixture-of-experts models use substantially less\nenergy than the dense OLMo-7B model, the OLMoE\narchitecture utilizes up to 54.24% more energy\nthan the base OLMo 1B model, despite having a\nsimilar number of active parameters.\nWe identify that the increased energy and latency\nof MoE’s can be attributed to the fused kernel used\nin the expert layers which is substantially slower\nthan the corresponding GEMM operation in linear\nlayers in the dense model; 19.70% slower at batch\nsize 1 and 63% slower at batch size 8. Notably,\nwe observe that the additional routing operations\nin the MoE model introduce minimal latency; and\nthat the increased overhead of more CUDA graph\nand kernel launch operations are largely mitigated\nthrough kernel serialization and graph compilation\noptimizations (i.e. vLLM with CUDA Graphs).\n3.3 Effects of Software Optimizations\nPagedAttention with vLLM Improves Efficiency.\nCompared to native PyTorch, the vLLM inference\nserving engine improves both the throughput and\nthe energy efficiency. The vLLM framework uses\nPagedAttention to implement non-contiguous KV\ncache blocks which reduces memory fragmentation\nand allocation of redundant memory in the case of\nsparse sequences (Kwon et al., 2023).\nThese optimizations allow for improved memory\nefficiency and the vLLM framework to support\nlarger batch sizes on fixed memory GPUs.\nCompilation and Kernel Serialization Improves\nEfficiency. The graph compilation and kernel\nserialization increase hardware utilization by re-\nmoving redundant operations in the computational\ngraph and reducing the kernel launch overhead\n(Fernandez et al., 2023), respectively. We observe\nthat both torch.compile and CUDA graph serial-\nization (eager=False) improve throughput at no\nadditional energy cost in Figure 5. However, we\nnote that the benefits of CUDA graphs are more\napparent at lower batch sizes, as the relative cost of\nkernel launch is larger for smaller computational\nworkloads.\nContinuous Batching Reduces Energy Use.\nLLM inference is inherently autoregressive, requir-\ning many sequential operations. Static batching\nmaintains a fixed batch size throughout inference,\nwhich leads to GPU under-utilization when gener-\nation lengths vary and idle compute accumulates\nafter early terminations. Continuous batching miti-\ngates this by dynamically replacing completed re-\nquests with new ones, improving GPU utilization\nand reducing idle time (Yu et al., 2022). This ap-\n5", "sentences": [{"text": "21 23 25 27 29\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n10−2\n10−1\nEnergy (kWh)\nPyTorch (compile=False)\nPyTorch (compile=True)\nvLLM (eager=False)\nvLLM (eager=True) (c) A6000\nFigure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends\nof 1024 samples for 64 output tokens.", "metadata": {}}, {"text": "For each GPU, we compare PyTorch with and without compilation, and\nvLLM with and without CUDA Graph serialization.", "metadata": {}}, {"text": "The line in black represents the maximum allowable batch size\nfor PyTorch.", "metadata": {}}, {"text": "Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations\ncan serve a larger batch size.", "metadata": {}}, {"text": "energy use and latency of language model infer-\nence.", "metadata": {}}, {"text": "At smaller batch sizes ( ≤ 16) speculative\ndecoding is effective in reducing the total energy\ncost of inference with up to +29.14% compared to\nsingle-example inference (Figure 3).", "metadata": {}}, {"text": "However, au-\ntoregressive decoding methods are more efficient\nat larger batch sizes, with speculative decoding\nrequiring 25.65% more energy when performing\ninference at a batch size of 128.", "metadata": {}}, {"text": "Mixture of Experts Incurs Higher Inference En-\nergy Costs.", "metadata": {}}, {"text": "Sparse mixture-of-experts are often\nutilized as an alternative architecture due to their\nincreased sample efficiency during training and\nincreased performance relative to dense neural net-\nworks with the same number of active parameters.", "metadata": {}}, {"text": "Although both dense OLMo-1B and the OLMoE1B-7\nB mixture-of-experts models use substantially less\nenergy than the dense OLMo-7B model, the OLMoE\narchitecture utilizes up to 54.24% more energy\nthan the base OLMo 1B model, despite having a\nsimilar number of active parameters.", "metadata": {}}, {"text": "We identify that the increased energy and latency\nof MoE’s can be attributed to the fused kernel used\nin the expert layers which is substantially slower\nthan the corresponding GEMM operation in linear\nlayers in the dense model;", "metadata": {}}, {"text": "19.70% slower at batch\nsize 1 and 63% slower at batch size 8.", "metadata": {}}, {"text": "Notably,\nwe observe that the additional routing operations\nin the MoE model introduce minimal latency;", "metadata": {}}, {"text": "and\nthat the increased overhead of more CUDA graph\nand kernel launch operations are largely mitigated\nthrough kernel serialization and graph compilation\noptimizations (i.e.", "metadata": {}}, {"text": "vLLM with CUDA Graphs).", "metadata": {}}, {"text": "3.3 Effects of Software Optimizations\nPagedAttention with vLLM Improves Efficiency.", "metadata": {}}, {"text": "Compared to native PyTorch, the vLLM inference\nserving engine improves both the throughput and\nthe energy efficiency.", "metadata": {}}, {"text": "The vLLM framework uses\nPagedAttention to implement non-contiguous KV\ncache blocks which reduces memory fragmentation\nand allocation of redundant memory in the case of\nsparse sequences (Kwon et al., 2023).", "metadata": {}}, {"text": "These optimizations allow for improved memory\nefficiency and the vLLM framework to support\nlarger batch sizes on fixed memory GPUs.", "metadata": {}}, {"text": "Compilation and Kernel Serialization Improves\nEfficiency.", "metadata": {}}, {"text": "The graph compilation and kernel\nserialization increase hardware utilization by re-\nmoving redundant operations in the computational\ngraph and reducing the kernel launch overhead\n(Fernandez et al., 2023), respectively.", "metadata": {}}, {"text": "We observe\nthat both torch.compile and CUDA graph serial-\nization (eager=False) improve throughput at no\nadditional energy cost in Figure 5.", "metadata": {}}, {"text": "However, we\nnote that the benefits of CUDA graphs are more\napparent at lower batch sizes, as the relative cost of\nkernel launch is larger for smaller computational\nworkloads.", "metadata": {}}, {"text": "Continuous Batching Reduces Energy Use.", "metadata": {}}, {"text": "LLM inference is inherently autoregressive, requir-\ning many sequential operations.", "metadata": {}}, {"text": "Static batching\nmaintains a fixed batch size throughout inference,\nwhich leads to GPU under-utilization when gener-\nation lengths vary and idle compute accumulates\nafter early terminations.", "metadata": {}}, {"text": "Continuous batching miti-\ngates this by dynamically replacing completed re-\nquests with new ones, improving GPU utilization\nand reducing idle time (Yu et al., 2022).", "metadata": {}}, {"text": "This ap-\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "21 23 25 27 29\nBatch Size\n2−7\n2−6\n2−5\n2−4\n2−3\nEnergy (kWh)\nLlama 3.1 8B\nSingle GPU\nTensor Parallel: 2\nTensor Parallel: 4\n21 23 25 27 29\nBatch Size\n2−4\n2−3\n2−2\nEnergy (kWh)\nDeepSeek Distill Qwen 32B\nTensor Parallel: 2\nTensor Parallel: 4\nFigure 6: Energy Use of Llama-3.1 8B and Qwen 32B\nwith varying degrees of Tensor Parallelism.\nproach is particularly effective when generation\nlengths have high variance, yielding significant\nspeedups at larger batch sizes.\nWe observe that at smaller batch sizes the over-\nhead of online scheduling outweighs its benefits but\nat larger batch sizes, online serving with continuous\nbatching requires less energy; details in Appendix\nD. We note that the numbers under-represent the\nimpact of continuous batching given the samples\nare drawn from the same dataset, thereby reducing\nthe variance in input and output lengths.\n3.4 Effects of Hardware Design Choices\nMulti-GPU Tensor Parallelism Reduces Latency\nfor Increased Power Use Model parallelism\ntechniques such as tensor and pipeline parallelism\nare frequently used to alleviate the memory pres-\nsure of large sets of model parameters and batch\nsizes, as well as to leverage multiple hardware ac-\ncelerators in order to speed up workload execu-\ntion (Narayanan et al., 2021). Additionally, for\nfixed workloads, tensor parallelism reduces both\nthe per-device computational intensity and per-\ndevice power utilization as the workload is sharded\nacross accelerator. However, the speedups from\nadditional accelerators are insufficient to offset the\nenergy cost of utilizing more devices (i.e. utilizing\ntwice the GPUs fails to yield a two-fold speedup).\nIn Figure 6, we observe that utilizing tensor par-\nallelism to scale from inference with a single GPU\nto four GPUs reduces latency and per-device power\nutilization for the Llama-3.1 8B model. However,\nincreasing parallelism yields higher total energy\nuse due to the larger number of accelerators. Con-\ncretely, parallelizing a fixed workload over two\nand four GPUs decreases latency by 40.16% and\n61.34% but increases total energy use by 29.3%\nand 55.23% at single batch inference due to the\nintroduction of additional devices.\nEffects of Hardware Speed The effectiveness of\noptimization techniques varies significantly across\nhardware platforms, with faster accelerators show-\ning greater benefits from optimizations that target\ncomputational efficiency. Our results demonstrate\nthat graph compilation, kernel serialization, and\nspeculative decoding achieve their maximum im-\npact on the A100 GPU.\nSpecifically, PyTorch compilation yields a\n29.90% improvement on the A100, which drops\nto 13.28% on the RTX 6000 Ada and further to\n1.96% on the A6000. Similarly, vLLM’s eager\nmode optimization shows a 25.47% improvement\non the A100 versus 2.97% on the A6000. This\npattern suggests that as hardware computational ca-\npabilities increase, the relative impact of software\noptimizations targeting kernel efficiency becomes\nmore pronounced.\n4 The Impact of Optimizations on\nInference Energy Use\nIn this section, we outline our approach to model-\ning the energy consumption of an LLM under both\nsynthetic and realistic workload distributions. We\nleverage classical NLP tasks and datasets of infer-\nence requests to estimate energy usage across dif-\nferent execution environments, including PyTorch-\nnative and vLLM backends with software optimiza-\ntions on a single A6000 GPU.\n4.1 Modeling Energy Requirements Using\nOffline Serving\nWe consider the energy required to process a\ndataset D = {R1, R2, . . . , RN } in an offline set-\nting in which all requests can be batch processed\nfreely, and where each request Rk consists of a\ntuple (ik, ok), representing the input token length\nik and the output generation length ok:\nRk = (ik, ok), ∀k ∈ {1, . . . , N}.\n6", "sentences": [{"text": "21 23 25 27 29\nBatch Size\n2−7\n2−6\n2−5\n2−4\n2−3\nEnergy (kWh)\nLlama 3.1 8B\nSingle GPU\nTensor Parallel: 2\nTensor Parallel: 4\n21 23 25 27 29\nBatch Size\n2−4\n2−3\n2−2\nEnergy (kWh)\nDeepSeek Distill Qwen 32B\nTensor Parallel: 2\nTensor Parallel: 4\nFigure 6: Energy Use of Llama-3.1 8B and Qwen 32B\nwith varying degrees of Tensor Parallelism.", "metadata": {}}, {"text": "proach is particularly effective when generation\nlengths have high variance, yielding significant\nspeedups at larger batch sizes.", "metadata": {}}, {"text": "We observe that at smaller batch sizes the over-\nhead of online scheduling outweighs its benefits but\nat larger batch sizes, online serving with continuous\nbatching requires less energy;", "metadata": {}}, {"text": "details in Appendix\nD.", "metadata": {}}, {"text": "We note that the numbers under-represent the\nimpact of continuous batching given the samples\nare drawn from the same dataset, thereby reducing\nthe variance in input and output lengths.", "metadata": {}}, {"text": "3.4 Effects of Hardware Design Choices\nMulti-GPU Tensor Parallelism Reduces Latency\nfor Increased Power Use Model parallelism\ntechniques such as tensor and pipeline parallelism\nare frequently used to alleviate the memory pres-\nsure of large sets of model parameters and batch\nsizes, as well as to leverage multiple hardware ac-\ncelerators in order to speed up workload execu-\ntion (Narayanan et al., 2021).", "metadata": {}}, {"text": "Additionally, for\nfixed workloads, tensor parallelism reduces both\nthe per-device computational intensity and per-\ndevice power utilization as the workload is sharded\nacross accelerator.", "metadata": {}}, {"text": "However, the speedups from\nadditional accelerators are insufficient to offset the\nenergy cost of utilizing more devices (i.e.", "metadata": {}}, {"text": "utilizing\ntwice the GPUs fails to yield a two-fold speedup).", "metadata": {}}, {"text": "In Figure 6, we observe that utilizing tensor par-\nallelism to scale from inference with a single GPU\nto four GPUs reduces latency and per-device power\nutilization for the Llama-3.1 8B model.", "metadata": {}}, {"text": "However,\nincreasing parallelism yields higher total energy\nuse due to the larger number of accelerators.", "metadata": {}}, {"text": "Con-\ncretely, parallelizing a fixed workload over two\nand four GPUs decreases latency by 40.16% and\n61.34% but increases total energy use by 29.3%\nand 55.23% at single batch inference due to the\nintroduction of additional devices.", "metadata": {}}, {"text": "Effects of Hardware Speed The effectiveness of\noptimization techniques varies significantly across\nhardware platforms, with faster accelerators show-\ning greater benefits from optimizations that target\ncomputational efficiency.", "metadata": {}}, {"text": "Our results demonstrate\nthat graph compilation, kernel serialization, and\nspeculative decoding achieve their maximum im-\npact on the A100 GPU.", "metadata": {}}, {"text": "Specifically, PyTorch compilation yields a\n29.90% improvement on the A100, which drops\nto 13.28% on the RTX 6000 Ada and further to\n1.96% on the A6000.", "metadata": {}}, {"text": "Similarly, vLLM’s eager\nmode optimization shows a 25.47% improvement\non the A100 versus 2.97% on the A6000.", "metadata": {}}, {"text": "This\npattern suggests that as hardware computational ca-\npabilities increase, the relative impact of software\noptimizations targeting kernel efficiency becomes\nmore pronounced.", "metadata": {}}, {"text": "4 The Impact of Optimizations on\nInference Energy Use\nIn this section, we outline our approach to model-\ning the energy consumption of an LLM under both\nsynthetic and realistic workload distributions.", "metadata": {}}, {"text": "We\nleverage classical NLP tasks and datasets of infer-\nence requests to estimate energy usage across dif-\nferent execution environments, including PyTorch-\nnative and vLLM backends with software optimiza-\ntions on a single A6000 GPU.", "metadata": {}}, {"text": "4.1 Modeling Energy Requirements Using\nOffline Serving\nWe consider the energy required to process a\ndataset D = {R1, R2, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ", RN } in an offline set-\nting in which all requests can be batch processed\nfreely, and where each request Rk consists of a\ntuple (ik, ok), representing the input token length\nik and the output generation length ok:\nRk = (ik, ok), ∀k ∈ {1, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ", N}.", "metadata": {}}, {"text": "6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "21 23 25 27 29 211 213\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Input Tokens - CDF\nReal Distribution\nBinned Approximation\n21 23 25 27 29 211\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Output Tokens - CDF\nReal Distribution\nBinned Approximation\nFigure 7: Comparison of the real token length distributions (blue) with the binned approximation (orange) for\nAzure conversation input (left) and output (right) token lengths. The CDF plots illustrate how our binning strategy\napproximates the empirical distribution while ensuring computational efficiency for energy estimation.\nSince ik and ok vary significantly across requests,\nwe utilize dataset statistics—including the median\nand 99th percentile of input and output lengths\n(discussed in §4.3) to inform our binning strategy.\nBinning Strategy. To effectively handle the\nbroad range of (ik, ok) values, we define discrete\nbin sets for input and output lengths:\nIbins = {2m | m ∈ N, 4 ≤ m ≤ 13}\n= {32, 128, 256, 512, 1024, 2048, 4096, 8192},\nObins = {2n | n ∈ N, 3 ≤ m ≤ 9}\n= {8, 16, 32, 64, 128, 256, 512}.\nThese bin choices ensure sufficient coverage across\nrealistic request distributions. Notably, we exclude\nextremely long input requests ( > 8k tokens) and\ngeneration outputs beyond 512 tokens.\nMapping Requests to Bins. Given a request\nR = (i, o), we map it to the closest ceiling bin:\nI ∗ = min{I ∈ Ibins | I ≥ i},\nO∗ = min{O ∈ Obins | O ≥ o}.\nWe group requests within the same (I ∗, O∗) bin\ninto batches of size B(I ∗, O∗), the maximum al-\nlowable batch size for the given hardware and back-\nend configuration. Each batch processes B(I ∗, O∗)\nrequests in parallel, allowing for more efficient en-\nergy utilization, which is more representative of\nreal-world inference setups.\nGiven our hardware configuration and backend,\nwe collect the estimates of Ebatch(I ∗, O∗), which\ncorresponds to the energy used to serve a request\nof batch size B with input prompts of length I∗\nand output lengths O∗.\nWe collect real energy measurements\nEreal\nbatch(I∗, O∗), representing the observed en-\nergy usage when processing a full batch of size\nB(I ∗, O∗) with input lengths I ∗ and output lengths\nO∗. Thus, the total estimated energy consumption\nacross the workload to serve N requests that fall in\nthe bin is given by:\nbEtotal =\nX\n(I ∗,O∗)\n\u0012 N real(I ∗, O∗)\nB(I ∗, O∗)\n\u0013\nEreal\nbatch(I ∗, O∗),\nwhere N real(I ∗, O∗) is the total number of ob-\nserved requests mapped to bin (I ∗, O∗), and\nN real(I ∗,O∗)\nB(I ∗,O∗) represents the number of batches re-\nquired to process them.\n4.2 Idealized Baseline\nAs a naive baseline, we estimate an upper bound\nof the energy efficiency of these workloads with a\nbaseline derived from the manufacturer-rated hard-\nware speeds ( F LOP SHW ), power draw (TDP)\n,and floating point operations (FLOPs) required for\ninference F LOP s 5. This approximation assumes\nhardware is being utilized as maximum efficiency\nboth in through idealized floating point operation\nthroughput and maximum power draw.\nbEOptimal =\n\u0012 TDP\nF LOP SHW\n\u0013\n×\nX\n(I ∗,O∗)\nN real(I ∗, O∗) × F LOP s(I ∗, O∗)\n7", "sentences": [{"text": "21 23 25 27 29 211 213\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Input Tokens - CDF\nReal Distribution\nBinned Approximation\n21 23 25 27 29 211\nToken Count\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nAzure Conversation Output Tokens - CDF\nReal Distribution\nBinned Approximation\nFigure 7: Comparison of the real token length distributions (blue) with the binned approximation (orange) for\nAzure conversation input (left) and output (right) token lengths.", "metadata": {}}, {"text": "The CDF plots illustrate how our binning strategy\napproximates the empirical distribution while ensuring computational efficiency for energy estimation.", "metadata": {}}, {"text": "Since ik and ok vary significantly across requests,\nwe utilize dataset statistics—including the median\nand 99th percentile of input and output lengths\n(discussed in §4.3) to inform our binning strategy.", "metadata": {}}, {"text": "Binning Strategy.", "metadata": {}}, {"text": "To effectively handle the\nbroad range of (ik, ok) values, we define discrete\nbin sets for input and output lengths:\nIbins = {2m | m ∈ N, 4 ≤ m ≤ 13}\n= {32, 128, 256, 512, 1024, 2048, 4096, 8192},\nObins = {2n | n ∈ N, 3 ≤ m ≤ 9}\n= {8, 16, 32, 64, 128, 256, 512}.", "metadata": {}}, {"text": "These bin choices ensure sufficient coverage across\nrealistic request distributions.", "metadata": {}}, {"text": "Notably, we exclude\nextremely long input requests ( > 8k tokens) and\ngeneration outputs beyond 512 tokens.", "metadata": {}}, {"text": "Mapping Requests to Bins.", "metadata": {}}, {"text": "Given a request\nR = (i, o), we map it to the closest ceiling bin:\nI ∗ = min{I ∈ Ibins | I ≥ i},\nO∗ = min{O ∈ Obins | O ≥ o}.", "metadata": {}}, {"text": "We group requests within the same (I ∗, O∗) bin\ninto batches of size B(I ∗, O∗), the maximum al-\nlowable batch size for the given hardware and back-\nend configuration.", "metadata": {}}, {"text": "Each batch processes B(I ∗, O∗)\nrequests in parallel, allowing for more efficient en-\nergy utilization, which is more representative of\nreal-world inference setups.", "metadata": {}}, {"text": "Given our hardware configuration and backend,\nwe collect the estimates of Ebatch(I ∗, O∗), which\ncorresponds to the energy used to serve a request\nof batch size B with input prompts of length I∗\nand output lengths O∗.", "metadata": {}}, {"text": "We collect real energy measurements\nEreal\nbatch(I∗, O∗), representing the observed en-\nergy usage when processing a full batch of size\nB(I ∗, O∗) with input lengths I ∗ and output lengths\nO∗.", "metadata": {}}, {"text": "Thus, the total estimated energy consumption\nacross the workload to serve N requests that fall in\nthe bin is given by:\nbEtotal =\nX\n(I ∗,O∗)\n\u0012 N real(I ∗, O∗)\nB(I ∗, O∗)\n\u0013\nEreal\nbatch(I ∗, O∗),\nwhere N real(I ∗, O∗) is the total number of ob-\nserved requests mapped to bin (I ∗, O∗), and\nN real(I ∗,O∗)\nB(I ∗,O∗) represents the number of batches re-\nquired to process them.", "metadata": {}}, {"text": "4.2 Idealized Baseline\nAs a naive baseline, we estimate an upper bound\nof the energy efficiency of these workloads with a\nbaseline derived from the manufacturer-rated hard-\nware speeds ( F LOP SHW ), power draw (TDP)\n,and floating point operations (FLOPs) required for\ninference F LOP s 5.", "metadata": {}}, {"text": "This approximation assumes\nhardware is being utilized as maximum efficiency\nboth in through idealized floating point operation\nthroughput and maximum power draw.", "metadata": {}}, {"text": "bEOptimal =\n\u0012 TDP\nF LOP SHW\n\u0013\n×\nX\n(I ∗,O∗)\nN real(I ∗, O∗) × F LOP s(I ∗, O∗)\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "Dataset Mean ± Std Median 99th\nBurstGPT 256.80 ± 242.27 215 1038\nAzure Chat 1631.58 ± 1529.64 928 6683\nAzure Code 2511.28 ± 2133.54 1930 7685\nTable 1: Input Sequence Length Statistics Across Real-\nWorld LLM Workloads\nDataset Mean ± Std Median 99th\nBurstGPT 35.10 ± 108.59 7 478\nAzure Chat 105.51 ± 158.25 41 694\nAzure Code 22.69 ± 74.78 8 271\nTable 2: Output Sequence Length Statistics Across Real-\nWorld LLM Workloads\n4.3 Evaluations\nWe examine a suite of classical NLP tasks and\nLLM inference workloads, each characterized by\na range of different input context and output gen-\neration sequences; with dataset statistics provided\nin Tables 3, 1, 2. We simulate a large-scale offline\nprocessing setting on the RTX A6000 GPUs, in\nwhich examples are binned by sequence lengths\n(as described in §4 and processed in parallel in the\nlargest possible batches that fit in GPU memory.\nUtilizing the simulated workloads described in\nSec 4.1, we estimate the effectiveness of the infer-\nence efficiency optimizations evaluated in Section\n4.1. Based on these results, we select an inference\nframework with efficiency optimizations targeting\nlarge batch inference. Concretely, we consider in-\nference with a dense model utilizing vLLM with\nCUDA graph serialization (eager mode off) on a\nsingle GPU and compare it to unoptimized infer-\nence native PyTorch as a lower bound on energy\nefficiency. In addition, we also model the idealized\nenergy baseline based on the model and hardware\nconfigurations.\nClassical NLP Tasks. We benchmark the energy\nuse in a set of classical natural language process-\ning tasks in the English language: text classifica-\ntion (IMDB, Maas et al., 2011), machine transla-\ntion (WMT-14, Bojar et al., 2014), summarization\n(CNN-DailyMail, Nallapati et al., 2016), and text\ngeneration (Wikitext-2 (Merity et al., 2016)).\nFor each of these tasks, we sample a subset of\n1024 examples with statistics of each dataset for\nthe input and the output tokens provided in Table 3.\n5Based on the Nvidia datasheet for the RTX A6000 GPU,\nwe utilize consider F LOP SHW of 309.7 TFLOPS and a\n300W TDP power draw; and estimate theoretical inference\nFLOPs with the DeepSpeed profiler (Rasley et al., 2020).\nTask Mean ± Std Max Output\nTranslation 49.96 ± 39.39 550 64\nGeneration 136.89 ± 93.13 547 64\nClassification 292.48 ± 239.94 3112 1\nSummarization 838.49 ± 400.70 2386 64\nTable 3: Tokenized Input and Output Length Statistics\nAcross NLP Tasks used for Energy Benchmarking\nwiki imdb CNN wmt\nTask\n0.01\n0.02\n0.03\n0.04\n0.05\nEnergy (kWh)\nPyTorch\nvLLM\nFigure 8: Energy Comparison in doing inference over 1024\nsamples between PyTorch with Compilation off and vLLM\nwith eager model off.\nWe note that the input sequences were padded to\nthe maximum sequence length. The energy profiles\nfor the best run, characterized by the least energy\nare summarized in Figure 8, with consistent reduc-\ntions in energy use provided by inference efficiency\noptimizations.\nReal-World LLM Workloads Additionally, we\nestimate the energy intensity and effectiveness of\nefficiency optimizations on real-world LLM work-\nloads. We simulate the offline processing of LLM\ninference requests as used in applications for short-\nform conversations with the Burst-GPT dataset\n(Wang et al., 2024) and long context conversations\nand code completion with the Azure LLM Infer-\nence chat and code traces (Stojkovic et al., 2024b).\nEach dataset provides a traces of LLM inference\nrequests with their corresponding input context and\noutput generation lengths. As compared with the\nclassical NLP tasks, modern LLM workloads tend\nto be longer in both input context and output gener-\nation token lengths, with code-assist applications\nhaving longer contexts, whereas conversational set-\ntings resulting in longer generations.\nDue to the larger number of requests and in-\ncreased sequence lengths, we observe that these\nworkloads require substantially larger amounts of\nenergy. However, we find that proper applications\nof inference efficiency optimizations can substan-\ntially reduce energy costs with savings of 73.00%,\n8", "sentences": [{"text": "Dataset Mean ± Std Median 99th\nBurstGPT 256.80 ± 242.27 215 1038\nAzure Chat 1631.58 ± 1529.64 928 6683\nAzure Code 2511.28 ± 2133.54 1930 7685\nTable 1: Input Sequence Length Statistics Across Real-\nWorld LLM Workloads\nDataset Mean ± Std Median 99th\nBurstGPT 35.10 ± 108.59 7 478\nAzure Chat 105.51 ± 158.25 41 694\nAzure Code 22.69 ± 74.78 8 271\nTable 2: Output Sequence Length Statistics Across Real-\nWorld LLM Workloads\n4.3 Evaluations\nWe examine a suite of classical NLP tasks and\nLLM inference workloads, each characterized by\na range of different input context and output gen-\neration sequences;", "metadata": {}}, {"text": "with dataset statistics provided\nin Tables 3, 1, 2.", "metadata": {}}, {"text": "We simulate a large-scale offline\nprocessing setting on the RTX A6000 GPUs, in\nwhich examples are binned by sequence lengths\n(as described in §4 and processed in parallel in the\nlargest possible batches that fit in GPU memory.", "metadata": {}}, {"text": "Utilizing the simulated workloads described in\nSec 4.1, we estimate the effectiveness of the infer-\nence efficiency optimizations evaluated in Section\n4.1.", "metadata": {}}, {"text": "Based on these results, we select an inference\nframework with efficiency optimizations targeting\nlarge batch inference.", "metadata": {}}, {"text": "Concretely, we consider in-\nference with a dense model utilizing vLLM with\nCUDA graph serialization (eager mode off) on a\nsingle GPU and compare it to unoptimized infer-\nence native PyTorch as a lower bound on energy\nefficiency.", "metadata": {}}, {"text": "In addition, we also model the idealized\nenergy baseline based on the model and hardware\nconfigurations.", "metadata": {}}, {"text": "Classical NLP Tasks.", "metadata": {}}, {"text": "We benchmark the energy\nuse in a set of classical natural language process-\ning tasks in the English language: text classifica-\ntion (IMDB, Maas et al., 2011), machine transla-\ntion (WMT-14, Bojar et al., 2014), summarization\n(CNN-DailyMail, Nallapati et al., 2016), and text\ngeneration (Wikitext-2 (Merity et al., 2016)).", "metadata": {}}, {"text": "For each of these tasks, we sample a subset of\n1024 examples with statistics of each dataset for\nthe input and the output tokens provided in Table 3.", "metadata": {}}, {"text": "5Based on the Nvidia datasheet for the RTX A6000 GPU,\nwe utilize consider F LOP SHW of 309.7 TFLOPS and a\n300W TDP power draw;", "metadata": {}}, {"text": "and estimate theoretical inference\nFLOPs with the DeepSpeed profiler (Rasley et al., 2020).", "metadata": {}}, {"text": "Task Mean ± Std Max Output\nTranslation 49.96 ± 39.39 550 64\nGeneration 136.89 ± 93.13 547 64\nClassification 292.48 ± 239.94 3112 1\nSummarization 838.49 ± 400.70 2386 64\nTable 3: Tokenized Input and Output Length Statistics\nAcross NLP Tasks used for Energy Benchmarking\nwiki imdb CNN wmt\nTask\n0.01\n0.02\n0.03\n0.04\n0.05\nEnergy (kWh)\nPyTorch\nvLLM\nFigure 8: Energy Comparison in doing inference over 1024\nsamples between PyTorch with Compilation off and vLLM\nwith eager model off.", "metadata": {}}, {"text": "We note that the input sequences were padded to\nthe maximum sequence length.", "metadata": {}}, {"text": "The energy profiles\nfor the best run, characterized by the least energy\nare summarized in Figure 8, with consistent reduc-\ntions in energy use provided by inference efficiency\noptimizations.", "metadata": {}}, {"text": "Real-World LLM Workloads Additionally, we\nestimate the energy intensity and effectiveness of\nefficiency optimizations on real-world LLM work-\nloads.", "metadata": {}}, {"text": "We simulate the offline processing of LLM\ninference requests as used in applications for short-\nform conversations with the Burst-GPT dataset\n(Wang et al., 2024) and long context conversations\nand code completion with the Azure LLM Infer-\nence chat and code traces (Stojkovic et al., 2024b).", "metadata": {}}, {"text": "Each dataset provides a traces of LLM inference\nrequests with their corresponding input context and\noutput generation lengths.", "metadata": {}}, {"text": "As compared with the\nclassical NLP tasks, modern LLM workloads tend\nto be longer in both input context and output gener-\nation token lengths, with code-assist applications\nhaving longer contexts, whereas conversational set-\ntings resulting in longer generations.", "metadata": {}}, {"text": "Due to the larger number of requests and in-\ncreased sequence lengths, we observe that these\nworkloads require substantially larger amounts of\nenergy.", "metadata": {}}, {"text": "However, we find that proper applications\nof inference efficiency optimizations can substan-\ntially reduce energy costs with savings of 73.00%,\n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Dataset PyTorch % ∆ vLLM % ∆\nBurstGPT 506.52% 63.75%\nAzure Code 102.79% 26.59%\nAzure Conversation 490.23% 64.22%\nTable 4: Percentage differences of energy consumption\nrelative to theoretical values for Various Tasks with\nOffline Inference.\n37.58%, and 72.18% on BurstGPT, Azure Code\nand Conversation, respectively.\n5 Related Work\nEfficient Methods for LLM Inference To meet\nthe service-level-objective (SLO) serving require-\nments of real deployment settings, efficiency opti-\nmizations for LLM inference are often designed to\noptimize model serving speed, as measured by la-\ntency and time-to-first-token. A variety of methods\nhave been developed to meet these latency con-\nstraints, including: continuous batching (Yu et al.,\n2022), model parallelism (Narayanan et al., 2021;\nHuang et al., 2019; Li et al., 2020), speculative\ndecoding (Liu et al., 2024; Leviathan et al., 2023;\nChen et al., 2023, 2025), and disaggregated serving\n(Zhong et al., 2024).\nSolely optimizing system performance for speed\nis insufficient in characterizing and does not pro-\nvide insight into the model energy use and result-\ning carbon emissions of LLM inference; as such\nmethods may require additional computation or\nexhibit low correlation between efficiency cost in-\ndicators (Dehghani et al., 2022). Recent work has\nexplored methods for explicitly reducing energy re-\nquirements and carbon emissions for LLM serving\nvia disaggregated serving over heterogeneous hard-\nware (Shi et al., 2024), system-wide scheduling\nand request routing to energy-optimized instances\n(Stojkovic et al., 2024b), and prompt directives\nto induce shorter sequence generations (Li et al.,\n2024). However, the exact impact or improvements\nin energy requirements for latency-optimized meth-\nods remains not fully characterized.\nEstimations and Measurement of of Energy Use\nin NLP The energy and carbon emissions of ma-\nchine learning models have been a growing con-\ncern in the research community and industry as the\nscale of models and prevalence of deployment has\nincreased (Schwartz et al., 2020; Wu et al., 2022).\nEstimations of the energy requirements and envi-\nronmental impact of LLMs has largely focused on\nestimation of costs for pretraining and finetuning\ndue to the large singular costs of model develop-\nments (Strubell et al., 2020; Wang et al., 2023;\nLuccioni et al., 2023; Faiz et al., 2023); with large\nindustrial developers similarly reporting the energy\nrequired for pretraining (OLMo et al., 2024; Morri-\nson et al., 2025; Dubey et al., 2024).\nIn contrast to training, inference workloads are\nhigher in variability with variation in request fre-\nquencies, batching, input and output sequence\nlengths executed over diverse hardware platforms\nat scale; and more complex energy use profiles\ndue to variations in power draw during prefill and\ndecoding stages of generation (Patel et al., 2024).\nPrevious work has investigated the comparative\nenergy cost of machine learning models across var-\nious tasks (Luccioni et al., 2024b,a), the energy\ncosts of LMs of various sizes (Samsi et al., 2023;\nWu et al., 2025), the effects of hardware config-\nurations (i.e. GPU power capping and frequency\nscaling; (Samsi et al., 2023)), and the impact of\nsequence length variability and batching strategies\n(Patel et al., 2024; Stojkovic et al., 2024a; Wilkins\net al., 2024). However, such evaluations of infer-\nence energy use often rely on simplified deploy-\nment settings with limited sets of model architec-\ntures and serving frameworks.\n6 Conclusion\nIn this work, we evaluate the impact of common\ninference efficiency optimizations on the energy\nrequirements of large language model serving. We\nexamine a variety of optimization techniques and\nevaluate on representative data corresponding to\nclassical NLP tasks as well as modern LLM de-\nployment settings. We conclude that the effective-\nness of latency optimizations in reducing energy\nuse is highly sensitive to the shape of the input\ndata, underlying hardware architecture, and soft-\nware framework implementations; and that opti-\nmizations cannot be applied uniformly.\nAdditionally, we conduct a case study of classi-\ncal NLP tasks and real-world LLM inference work-\nloads and find that proper application of the studied\ninference optimizations can reduce total energy use\nby up to 73% on the BurstGPT chat dataset.\nLimitations and Risks\nIn this work, we evaluate the energy efficiency and\ncarbon emissions of LLM inference as approxi-\nmated by total GPU power usage. Although GPUs\nthe majority of arithmetic operations required for\n9", "sentences": [{"text": "Dataset PyTorch % ∆ vLLM % ∆\nBurstGPT 506.52% 63.75%\nAzure Code 102.79% 26.59%\nAzure Conversation 490.23% 64.22%\nTable 4: Percentage differences of energy consumption\nrelative to theoretical values for Various Tasks with\nOffline Inference.", "metadata": {}}, {"text": "37.58%, and 72.18% on BurstGPT, Azure Code\nand Conversation, respectively.", "metadata": {}}, {"text": "5 Related Work\nEfficient Methods for LLM Inference To meet\nthe service-level-objective (SLO) serving require-\nments of real deployment settings, efficiency opti-\nmizations for LLM inference are often designed to\noptimize model serving speed, as measured by la-\ntency and time-to-first-token.", "metadata": {}}, {"text": "A variety of methods\nhave been developed to meet these latency con-\nstraints, including: continuous batching (Yu et al.,\n2022), model parallelism (Narayanan et al., 2021;", "metadata": {}}, {"text": "Huang et al., 2019;", "metadata": {}}, {"text": "Li et al., 2020), speculative\ndecoding (Liu et al., 2024;", "metadata": {}}, {"text": "Leviathan et al., 2023;", "metadata": {}}, {"text": "Chen et al., 2023, 2025), and disaggregated serving\n(Zhong et al., 2024).", "metadata": {}}, {"text": "Solely optimizing system performance for speed\nis insufficient in characterizing and does not pro-\nvide insight into the model energy use and result-\ning carbon emissions of LLM inference;", "metadata": {}}, {"text": "as such\nmethods may require additional computation or\nexhibit low correlation between efficiency cost in-\ndicators (Dehghani et al., 2022).", "metadata": {}}, {"text": "Recent work has\nexplored methods for explicitly reducing energy re-\nquirements and carbon emissions for LLM serving\nvia disaggregated serving over heterogeneous hard-\nware (Shi et al., 2024), system-wide scheduling\nand request routing to energy-optimized instances\n(Stojkovic et al., 2024b), and prompt directives\nto induce shorter sequence generations (Li et al.,\n2024).", "metadata": {}}, {"text": "However, the exact impact or improvements\nin energy requirements for latency-optimized meth-\nods remains not fully characterized.", "metadata": {}}, {"text": "Estimations and Measurement of of Energy Use\nin NLP The energy and carbon emissions of ma-\nchine learning models have been a growing con-\ncern in the research community and industry as the\nscale of models and prevalence of deployment has\nincreased (Schwartz et al., 2020;", "metadata": {}}, {"text": "Wu et al., 2022).", "metadata": {}}, {"text": "Estimations of the energy requirements and envi-\nronmental impact of LLMs has largely focused on\nestimation of costs for pretraining and finetuning\ndue to the large singular costs of model develop-\nments (Strubell et al., 2020;", "metadata": {}}, {"text": "Wang et al., 2023;", "metadata": {}}, {"text": "Luccioni et al., 2023;", "metadata": {}}, {"text": "Faiz et al., 2023);", "metadata": {}}, {"text": "with large\nindustrial developers similarly reporting the energy\nrequired for pretraining (OLMo et al., 2024;", "metadata": {}}, {"text": "Morri-\nson et al., 2025;", "metadata": {}}, {"text": "Dubey et al., 2024).", "metadata": {}}, {"text": "In contrast to training, inference workloads are\nhigher in variability with variation in request fre-\nquencies, batching, input and output sequence\nlengths executed over diverse hardware platforms\nat scale;", "metadata": {}}, {"text": "and more complex energy use profiles\ndue to variations in power draw during prefill and\ndecoding stages of generation (Patel et al., 2024).", "metadata": {}}, {"text": "Previous work has investigated the comparative\nenergy cost of machine learning models across var-\nious tasks (Luccioni et al., 2024b,a), the energy\ncosts of LMs of various sizes (Samsi et al., 2023;", "metadata": {}}, {"text": "Wu et al., 2025), the effects of hardware config-\nurations (i.e.", "metadata": {}}, {"text": "GPU power capping and frequency\nscaling;", "metadata": {}}, {"text": "(Samsi et al., 2023)), and the impact of\nsequence length variability and batching strategies\n(Patel et al., 2024;", "metadata": {}}, {"text": "Stojkovic et al., 2024a;", "metadata": {}}, {"text": "Wilkins\net al., 2024).", "metadata": {}}, {"text": "However, such evaluations of infer-\nence energy use often rely on simplified deploy-\nment settings with limited sets of model architec-\ntures and serving frameworks.", "metadata": {}}, {"text": "6 Conclusion\nIn this work, we evaluate the impact of common\ninference efficiency optimizations on the energy\nrequirements of large language model serving.", "metadata": {}}, {"text": "We\nexamine a variety of optimization techniques and\nevaluate on representative data corresponding to\nclassical NLP tasks as well as modern LLM de-\nployment settings.", "metadata": {}}, {"text": "We conclude that the effective-\nness of latency optimizations in reducing energy\nuse is highly sensitive to the shape of the input\ndata, underlying hardware architecture, and soft-\nware framework implementations;", "metadata": {}}, {"text": "and that opti-\nmizations cannot be applied uniformly.", "metadata": {}}, {"text": "Additionally, we conduct a case study of classi-\ncal NLP tasks and real-world LLM inference work-\nloads and find that proper application of the studied\ninference optimizations can reduce total energy use\nby up to 73% on the BurstGPT chat dataset.", "metadata": {}}, {"text": "Limitations and Risks\nIn this work, we evaluate the energy efficiency and\ncarbon emissions of LLM inference as approxi-\nmated by total GPU power usage.", "metadata": {}}, {"text": "Although GPUs\nthe majority of arithmetic operations required for\n9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "inference and operate at a higher TDP than other\ncomponents, we do not account for the energy use\nby other other components of the hardware system\nsuch as power use from CPU, memory, or disk stor-\nage (McAllister et al., 2024; Patel et al., 2024); or\nestimate the energy requirements of other hardware\naccelerator architectures (e.g. TPUs, NPUs, etc.).\nLikewise, we conduct an investigation of com-\nmonly used inference software frameworks and\nstandard efficiency optimizations. However, there\nremain other settings and computational optimiza-\ntions that can be applied to LLM inference, such\nas utilizing: reduced or mixed precision, adaptive\nadjustment of GPU frequency, additional forms of\nmodel parallelism, or other forms of load manage-\nment and workload scheduling; which remain out\nof the scope of this work (Stojkovic et al., 2024b).\nIn this work, we primarily focus on the operation\nenergy use of machine learning inference. Estima-\ntion of the embodied costs of inference; and the\ncosts of machine learning training remain out of\nthe scope of this work.\nAlthough improved characterization of the en-\nergy use of LLM inference can be used to design\nmore efficient serving settings and reduce the en-\nergy needs of inference, it is possible that reduc-\ntions in the cost of pretraining may then lead more\nindividuals and organizations to pursue large model\npretraining (i.e. Jevons Paradox).\nReferences\nAmey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree\nMohan, Nipun Kwatra, Bhargav S Gulavani, Alexey\nTumanov, and Ramachandran Ramjee. 2024. Tam-\ning throughput-latency tradeoff in llm inference with\nsarathi-serve. Proceedings of 18th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion, 2024, Santa Clara.\nJordan Aljbour, Tom Wilson, and P Patel. 2024. Power-\ning intelligence: Analyzing artificial intelligence and\ndata center energy consumption. EPRI White Paper\nno. 3002028905.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nJeff Barr. 2019. Amazon ec2 update-infl instances\nwith aws inferentia chips for high performance cost-\neffective inferencing.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Ales Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nKendrick Cai and Deborah Mary Sophia. 2025. Alpha-\nbet plans massive capex hike, reports cloud revenue\ngrowth slowed. Reuters.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nJian Chen, Vashisth Tiwari, Ranajoy Sadhukhan,\nZhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and\nBeidi Chen. 2025. Magicdec: Breaking the latency-\nthroughput tradeoff for long context generation with\nspeculative decoding. In The Thirteenth Interna-\ntional Conference on Learning Representations.\nBenoit Courty, Victor Schmidt, Sasha Luccioni, Goyal-\nKamal, MarionCoutarel, Boris Feld, Jérémy Lecourt,\nLiamConnell, Amine Saboni, Inimaz, supatomic,\nMathilde Léval, Luis Blanche, Alexis Cruveiller,\nouminasara, Franklin Zhao, Aditya Joshi, Alexis\nBogroff, Hugues de Lavoreille, Niko Laskaris,\nEdoardo Abati, Douglas Blank, Ziyao Wang, Armin\nCatovic, Marc Alencon, Michał St˛ echły, Christian\nBauer, Lucas Otávio N. de Araújo, JPW, and Minerv-\naBooks. 2024. mlco2/codecarbon: v2.4.1.\nMostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer,\nand Ashish Vaswani. 2022. The efficiency misnomer.\nIn International Conference on Learning Representa-\ntions.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nAhmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi,\nPrateek Sharma, Fan Chen, and Lei Jiang. 2023.\nLlmcarbon: Modeling the end-to-end carbon foot-\nprint of large language models. arXiv preprint\narXiv:2309.14393.\nJared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk,\nand Emma Strubell. 2023. The framework tax: Dis-\nparities between inference efficiency in nlp research\nand deployment. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1588–1600.\nStefanos Georgiou, Maria Kechagia, Tushar Sharma,\nFederica Sarro, and Ying Zou. 2022. Green ai: Do\ndeep learning frameworks have different costs? In\nProceedings of the 44th International Conference on\nSoftware Engineering, pages 1082–1094.\n10", "sentences": [{"text": "inference and operate at a higher TDP than other\ncomponents, we do not account for the energy use\nby other other components of the hardware system\nsuch as power use from CPU, memory, or disk stor-\nage (McAllister et al., 2024;", "metadata": {}}, {"text": "Patel et al., 2024);", "metadata": {}}, {"text": "or\nestimate the energy requirements of other hardware\naccelerator architectures (e.g.", "metadata": {}}, {"text": "TPUs, NPUs, etc.).", "metadata": {}}, {"text": "Likewise, we conduct an investigation of com-\nmonly used inference software frameworks and\nstandard efficiency optimizations.", "metadata": {}}, {"text": "However, there\nremain other settings and computational optimiza-\ntions that can be applied to LLM inference, such\nas utilizing: reduced or mixed precision, adaptive\nadjustment of GPU frequency, additional forms of\nmodel parallelism, or other forms of load manage-\nment and workload scheduling;", "metadata": {}}, {"text": "which remain out\nof the scope of this work (Stojkovic et al., 2024b).", "metadata": {}}, {"text": "In this work, we primarily focus on the operation\nenergy use of machine learning inference.", "metadata": {}}, {"text": "Estima-\ntion of the embodied costs of inference;", "metadata": {}}, {"text": "and the\ncosts of machine learning training remain out of\nthe scope of this work.", "metadata": {}}, {"text": "Although improved characterization of the en-\nergy use of LLM inference can be used to design\nmore efficient serving settings and reduce the en-\nergy needs of inference, it is possible that reduc-\ntions in the cost of pretraining may then lead more\nindividuals and organizations to pursue large model\npretraining (i.e.", "metadata": {}}, {"text": "Jevons Paradox).", "metadata": {}}, {"text": "References\nAmey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree\nMohan, Nipun Kwatra, Bhargav S Gulavani, Alexey\nTumanov, and Ramachandran Ramjee.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Tam-\ning throughput-latency tradeoff in llm inference with\nsarathi-serve.", "metadata": {}}, {"text": "Proceedings of 18th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion, 2024, Santa Clara.", "metadata": {}}, {"text": "Jordan Aljbour, Tom Wilson, and P Patel.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Power-\ning intelligence: Analyzing artificial intelligence and\ndata center energy consumption.", "metadata": {}}, {"text": "EPRI White Paper\nno.", "metadata": {}}, {"text": "3002028905.", "metadata": {}}, {"text": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Qwen technical report.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2309.16609.", "metadata": {}}, {"text": "Jeff Barr.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Amazon ec2 update-infl instances\nwith aws inferentia chips for high performance cost-\neffective inferencing.", "metadata": {}}, {"text": "Ondrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Ales Tam-\nchyna.", "metadata": {}}, {"text": "2014.", "metadata": {}}, {"text": "Findings of the 2014 workshop on\nstatistical machine translation.", "metadata": {}}, {"text": "In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA.", "metadata": {}}, {"text": "Associa-\ntion for Computational Linguistics.", "metadata": {}}, {"text": "Kendrick Cai and Deborah Mary Sophia.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Alpha-\nbet plans massive capex hike, reports cloud revenue\ngrowth slowed.", "metadata": {}}, {"text": "Reuters.", "metadata": {}}, {"text": "Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Accelerating large language model\ndecoding with speculative sampling.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2302.01318.", "metadata": {}}, {"text": "Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan,\nZhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and\nBeidi Chen.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Magicdec: Breaking the latency-\nthroughput tradeoff for long context generation with\nspeculative decoding.", "metadata": {}}, {"text": "In The Thirteenth Interna-\ntional Conference on Learning Representations.", "metadata": {}}, {"text": "Benoit Courty, Victor Schmidt, Sasha Luccioni, Goyal-\nKamal, MarionCoutarel, Boris Feld, Jérémy Lecourt,\nLiamConnell, Amine Saboni, Inimaz, supatomic,\nMathilde Léval, Luis Blanche, Alexis Cruveiller,\nouminasara, Franklin Zhao, Aditya Joshi, Alexis\nBogroff, Hugues de Lavoreille, Niko Laskaris,\nEdoardo Abati, Douglas Blank, Ziyao Wang, Armin\nCatovic, Marc Alencon, Michał St˛ echły, Christian\nBauer, Lucas Otávio N.", "metadata": {}}, {"text": "de Araújo, JPW, and Minerv-\naBooks.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "mlco2/codecarbon: v2.4.1.", "metadata": {}}, {"text": "Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer,\nand Ashish Vaswani.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The efficiency misnomer.", "metadata": {}}, {"text": "In International Conference on Learning Representa-\ntions.", "metadata": {}}, {"text": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "The llama 3 herd of models.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2407.21783.", "metadata": {}}, {"text": "Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi,\nPrateek Sharma, Fan Chen, and Lei Jiang.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Llmcarbon: Modeling the end-to-end carbon foot-\nprint of large language models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2309.14393.", "metadata": {}}, {"text": "Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk,\nand Emma Strubell.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "The framework tax: Dis-\nparities between inference efficiency in nlp research\nand deployment.", "metadata": {}}, {"text": "In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1588–1600.", "metadata": {}}, {"text": "Stefanos Georgiou, Maria Kechagia, Tushar Sharma,\nFederica Sarro, and Ying Zou.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Green ai: Do\ndeep learning frameworks have different costs?", "metadata": {}}, {"text": "In\nProceedings of the 44th International Conference on\nSoftware Engineering, pages 1082–1094.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Alistair Green, Humayun Tai, Jesse Noffsinger, and\nPankaj Sachdeva. 2024. How data centers and the en-\nergy sector can sate ai’s hunger for power.McKinsey\nand Company.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-\ngia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,\net al. 2024. Olmo: Accelerating the science of lan-\nguage models. arXiv preprint arXiv:2402.00838.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji-\nquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\nGpipe: Efficient training of giant neural networks\nusing pipeline parallelism. Advances in neural infor-\nmation processing systems, 32.\nMike Isaac. 2025. Meta to increase spending to $65\nbillion this year in a.i. push. New York Times.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nGeorge Leopold. 2019. Aws to offer nvidia’s t4\ngpus for ai inferencing. URL: https://web. archive.\norg/web/20220309000921/https://www. hpcwire.\ncom/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/(visited on 2022-04-19).\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, pages 19274–19286. PMLR.\nBaolin Li, Yankai Jiang, Vijay Gadepally, and Devesh\nTiwari. 2024. Sprout: Green generative ai with\ncarbon-efficient llm inference. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 21799–21813.\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and\nShaolei Ren. 2025. Making AI Less \"Thirsty\": Un-\ncovering and Addressing the Secret Water Footprint\nof AI Models. arXiv preprint. ArXiv:2304.03271\n[cs].\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,\nPieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith,\nBrian Vaughan, Pritam Damania, et al. 2020. Pytorch\ndistributed: Experiences on accelerating data parallel\ntraining. arXiv preprint arXiv:2006.15704.\nXiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk\nKwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung,\nZhijie Deng, Ion Stoica, and Hao Zhang. 2024.\nOptimizing speculative decoding for serving large\nlanguage models using goodput. arXiv preprint\narXiv:2406.14066.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat. 2023. Estimating the carbon footprint\nof bloom, a 176b parameter language model. Journal\nof Machine Learning Research, 24(253):1–15.\nSasha Luccioni, Boris Gamazaychikov, Sara Hooker,\nRégis Pierrard, Emma Strubell, Yacine Jernite, and\nCarole-Jean Wu. 2024a. Light bulbs have en-\nergy ratings—so why can’t ai chatbots? Nature,\n632(8026):736–738.\nSasha Luccioni, Yacine Jernite, and Emma Strubell.\n2024b. Power hungry processing: Watts driving the\ncost of ai deployment? In Proceedings of the 2024\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’24, page 85–99, New York,\nNY , USA. Association for Computing Machinery.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nSara McAllister, Fiodar Kazhamiaka, Daniel S Berger,\nRodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh\nSah, Ricardo Bianchini, George Amvrosiadis, Nathan\nBeckmann, et al. 2024. A call for research on storage\nemissions. In Proceedings of the 3rd Workshop on\nSustainable Computer Systems (HotCarbon).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. Preprint, arXiv:1609.07843.\nJacob Morrison, Clara Na, Jared Fernandez, Tim\nDettmers, Emma Strubell, and Jesse Dodge. 2025.\nHolistically evaluating the environmental impact of\ncreating language models. In The Thirteenth Interna-\ntional Conference on Learning Representations.\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld,\nKyle Lo, Jacob Morrison, Sewon Min, Weijia Shi,\nPete Walsh, Oyvind Tafjord, Nathan Lambert, et al.\n2024. Olmoe: Open mixture-of-experts language\nmodels. arXiv preprint arXiv:2409.02060.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\n11", "sentences": [{"text": "Alistair Green, Humayun Tai, Jesse Noffsinger, and\nPankaj Sachdeva.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "How data centers and the en-\nergy sector can sate ai’s hunger for power.McKinsey\nand Company.", "metadata": {}}, {"text": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-\ngia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,\net al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Olmo: Accelerating the science of lan-\nguage models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2402.00838.", "metadata": {}}, {"text": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2501.12948.", "metadata": {}}, {"text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "The curious case of neural text de-\ngeneration.", "metadata": {}}, {"text": "In International Conference on Learning\nRepresentations.", "metadata": {}}, {"text": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji-\nquan Ngiam, Quoc V Le, Yonghui Wu, et al.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Gpipe: Efficient training of giant neural networks\nusing pipeline parallelism.", "metadata": {}}, {"text": "Advances in neural infor-\nmation processing systems, 32.", "metadata": {}}, {"text": "Mike Isaac.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Meta to increase spending to $65\nbillion this year in a.i.", "metadata": {}}, {"text": "push.", "metadata": {}}, {"text": "New York Times.", "metadata": {}}, {"text": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.", "metadata": {}}, {"text": "Gonzalez, Hao Zhang, and Ion Stoica.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Effi-\ncient memory management for large language model\nserving with pagedattention.", "metadata": {}}, {"text": "In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.", "metadata": {}}, {"text": "George Leopold.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Aws to offer nvidia’s t4\ngpus for ai inferencing.", "metadata": {}}, {"text": "URL: https://web.", "metadata": {}}, {"text": "archive.", "metadata": {}}, {"text": "org/web/20220309000921/https://www.", "metadata": {}}, {"text": "hpcwire.", "metadata": {}}, {"text": "com/2019/03/19/aws-upgrades-its-gpu-backed-ai-\ninference-platform/(visited on 2022-04-19).", "metadata": {}}, {"text": "Yaniv Leviathan, Matan Kalman, and Yossi Matias.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Fast inference from transformers via spec-\nulative decoding.", "metadata": {}}, {"text": "In International Conference on\nMachine Learning, pages 19274–19286.", "metadata": {}}, {"text": "PMLR.", "metadata": {}}, {"text": "Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh\nTiwari.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Sprout: Green generative ai with\ncarbon-efficient llm inference.", "metadata": {}}, {"text": "In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 21799–21813.", "metadata": {}}, {"text": "Pengfei Li, Jianyi Yang, Mohammad A.", "metadata": {}}, {"text": "Islam, and\nShaolei Ren.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Making AI Less \"Thirsty\": Un-\ncovering and Addressing the Secret Water Footprint\nof AI Models.", "metadata": {}}, {"text": "arXiv preprint.", "metadata": {}}, {"text": "ArXiv:2304.03271\n[cs].", "metadata": {}}, {"text": "Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,\nPieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith,\nBrian Vaughan, Pritam Damania, et al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Pytorch\ndistributed: Experiences on accelerating data parallel\ntraining.", "metadata": {}}, {"text": "arXiv preprint arXiv:2006.15704.", "metadata": {}}, {"text": "Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk\nKwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung,\nZhijie Deng, Ion Stoica, and Hao Zhang.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Optimizing speculative decoding for serving large\nlanguage models using goodput.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2406.14066.", "metadata": {}}, {"text": "Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Estimating the carbon footprint\nof bloom, a 176b parameter language model.", "metadata": {}}, {"text": "Journal\nof Machine Learning Research, 24(253):1–15.", "metadata": {}}, {"text": "Sasha Luccioni, Boris Gamazaychikov, Sara Hooker,\nRégis Pierrard, Emma Strubell, Yacine Jernite, and\nCarole-Jean Wu.", "metadata": {}}, {"text": "2024a.", "metadata": {}}, {"text": "Light bulbs have en-\nergy ratings—so why can’t ai chatbots?", "metadata": {}}, {"text": "Nature,\n632(8026):736–738.", "metadata": {}}, {"text": "Sasha Luccioni, Yacine Jernite, and Emma Strubell.", "metadata": {}}, {"text": "2024b.", "metadata": {}}, {"text": "Power hungry processing: Watts driving the\ncost of ai deployment?", "metadata": {}}, {"text": "In Proceedings of the 2024\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’24, page 85–99, New York,\nNY , USA.", "metadata": {}}, {"text": "Association for Computing Machinery.", "metadata": {}}, {"text": "Andrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts.", "metadata": {}}, {"text": "2011.", "metadata": {}}, {"text": "Learning word vectors for sentiment analysis.", "metadata": {}}, {"text": "In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.", "metadata": {}}, {"text": "Sara McAllister, Fiodar Kazhamiaka, Daniel S Berger,\nRodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh\nSah, Ricardo Bianchini, George Amvrosiadis, Nathan\nBeckmann, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "A call for research on storage\nemissions.", "metadata": {}}, {"text": "In Proceedings of the 3rd Workshop on\nSustainable Computer Systems (HotCarbon).", "metadata": {}}, {"text": "Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Pointer sentinel mixture mod-\nels.", "metadata": {}}, {"text": "Preprint, arXiv:1609.07843.", "metadata": {}}, {"text": "Jacob Morrison, Clara Na, Jared Fernandez, Tim\nDettmers, Emma Strubell, and Jesse Dodge.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Holistically evaluating the environmental impact of\ncreating language models.", "metadata": {}}, {"text": "In The Thirteenth Interna-\ntional Conference on Learning Representations.", "metadata": {}}, {"text": "Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld,\nKyle Lo, Jacob Morrison, Sewon Min, Weijia Shi,\nPete Walsh, Oyvind Tafjord, Nathan Lambert, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Olmoe: Open mixture-of-experts language\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2409.02060.", "metadata": {}}, {"text": "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gu˙lçehre, and Bing Xiang.", "metadata": {}}, {"text": "2016.", "metadata": {}}, {"text": "Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond.", "metadata": {}}, {"text": "In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n15.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groen-\neveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling\nGu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo\n2 furious. arXiv preprint arXiv:2501.00656.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nPratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo\nGoiri, Brijesh Warrier, Nithish Mahalingam, and Ri-\ncardo Bianchini. 2024. Characterizing power man-\nagement opportunities for llms in the cloud. In\nProceedings of the 29th ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3, pages\n207–222.\nDavid Patterson, Joseph Gonzalez, Urs Hölzle, Quoc\nLe, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean.\n2022. The carbon footprint of machine learn-\ning training will plateau, then shrink. Preprint,\narXiv:2204.05149.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap. 2019. Com-\npressive transformers for long-range sequence mod-\nelling. arXiv preprint.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505–3506.\nSiddharth Samsi, Dan Zhao, Joseph McDonald, Baolin\nLi, Adam Michaleas, Michael Jones, William Berg-\neron, Jeremy Kepner, Devesh Tiwari, and Vijay Gade-\npally. 2023. From words to watts: Benchmarking the\nenergy costs of large language model inference. In\n2023 IEEE High Performance Extreme Computing\nConference (HPEC), pages 1–9. IEEE.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54–63.\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa\nLei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan\nKoomey, Eric Masanet, Dale Sartor, et al. 2024. 2024\nunited states data center energy usage report.\nTianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding. 2024.\nGreenllm: Disaggregating large language model serv-\ning on heterogeneous gpus for lower carbon emis-\nsions. arXiv preprint arXiv:2412.20322.\nBrad Smith. 2025. The golden opportunity for american\nai.\nJovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo\nGoiri, and Josep Torrellas. 2024a. Towards greener\nllms: Bringing energy-efficiency to the forefront of\nllm inference. arXiv preprint arXiv:2403.20306.\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Tor-\nrellas, and Esha Choukse. 2024b. Dynamollm: De-\nsigning llm inference clusters for performance and\nenergy efficiency. arXiv preprint arXiv:2408.00741.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2020. Energy and policy considerations for\nmodern deep learning research. In Proceedings of\nthe AAAI conference on artificial intelligence , vol-\nume 34, pages 13693–13696.\nXiaorong Wang, Clara Na, Emma Strubell, Sorelle\nFriedler, and Sasha Luccioni. 2023. Energy and car-\nbon considerations of fine-tuning BERT. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023 , pages 9058–9069, Singapore.\nAssociation for Computational Linguistics.\nYuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhen-\nheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang,\nAmelie Chi Zhou, and Xiaowen Chu. 2024. Burst-\ngpt: A real-world workload dataset to optimize llm\nserving systems. Preprint, arXiv:2401.17644.\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier.\n2024. Offline energy-optimal llm serving: Workload-\nbased energy models for llm inference on heteroge-\nneous systems. ACM SigEnergy newletter.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems, 4:795–813.\nYanran Wu, Inez Hua, and Yi Ding. 2025. Unveil-\ning environmental impacts of large language model\nserving: A functional unit view. arXiv preprint\narXiv:2502.11256.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\n12", "sentences": [{"text": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al.", "metadata": {}}, {"text": "2021.", "metadata": {}}, {"text": "Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm.", "metadata": {}}, {"text": "In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n15.", "metadata": {}}, {"text": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groen-\neveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling\nGu, Shengyi Huang, Matt Jordan, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "2 olmo\n2 furious.", "metadata": {}}, {"text": "arXiv preprint arXiv:2501.00656.", "metadata": {}}, {"text": "Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Pytorch: An imperative style,\nhigh-performance deep learning library.", "metadata": {}}, {"text": "Advances in\nneural information processing systems, 32.", "metadata": {}}, {"text": "Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo\nGoiri, Brijesh Warrier, Nithish Mahalingam, and Ri-\ncardo Bianchini.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Characterizing power man-\nagement opportunities for llms in the cloud.", "metadata": {}}, {"text": "In\nProceedings of the 29th ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3, pages\n207–222.", "metadata": {}}, {"text": "David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc\nLe, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "The carbon footprint of machine learn-\ning training will plateau, then shrink.", "metadata": {}}, {"text": "Preprint,\narXiv:2204.05149.", "metadata": {}}, {"text": "Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap.", "metadata": {}}, {"text": "2019.", "metadata": {}}, {"text": "Com-\npressive transformers for long-range sequence mod-\nelling.", "metadata": {}}, {"text": "arXiv preprint.", "metadata": {}}, {"text": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters.", "metadata": {}}, {"text": "In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505–3506.", "metadata": {}}, {"text": "Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin\nLi, Adam Michaleas, Michael Jones, William Berg-\neron, Jeremy Kepner, Devesh Tiwari, and Vijay Gade-\npally.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "From words to watts: Benchmarking the\nenergy costs of large language model inference.", "metadata": {}}, {"text": "In\n2023 IEEE High Performance Extreme Computing\nConference (HPEC), pages 1–9.", "metadata": {}}, {"text": "IEEE.", "metadata": {}}, {"text": "Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Green ai.", "metadata": {}}, {"text": "Communications of the\nACM, 63(12):54–63.", "metadata": {}}, {"text": "Arman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa\nLei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan\nKoomey, Eric Masanet, Dale Sartor, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "2024\nunited states data center energy usage report.", "metadata": {}}, {"text": "Tianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Greenllm: Disaggregating large language model serv-\ning on heterogeneous gpus for lower carbon emis-\nsions.", "metadata": {}}, {"text": "arXiv preprint arXiv:2412.20322.", "metadata": {}}, {"text": "Brad Smith.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "The golden opportunity for american\nai.", "metadata": {}}, {"text": "Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo\nGoiri, and Josep Torrellas.", "metadata": {}}, {"text": "2024a.", "metadata": {}}, {"text": "Towards greener\nllms: Bringing energy-efficiency to the forefront of\nllm inference.", "metadata": {}}, {"text": "arXiv preprint arXiv:2403.20306.", "metadata": {}}, {"text": "Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Tor-\nrellas, and Esha Choukse.", "metadata": {}}, {"text": "2024b.", "metadata": {}}, {"text": "Dynamollm: De-\nsigning llm inference clusters for performance and\nenergy efficiency.", "metadata": {}}, {"text": "arXiv preprint arXiv:2408.00741.", "metadata": {}}, {"text": "Emma Strubell, Ananya Ganesh, and Andrew McCal-\nlum.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Energy and policy considerations for\nmodern deep learning research.", "metadata": {}}, {"text": "In Proceedings of\nthe AAAI conference on artificial intelligence , vol-\nume 34, pages 13693–13696.", "metadata": {}}, {"text": "Xiaorong Wang, Clara Na, Emma Strubell, Sorelle\nFriedler, and Sasha Luccioni.", "metadata": {}}, {"text": "2023.", "metadata": {}}, {"text": "Energy and car-\nbon considerations of fine-tuning BERT.", "metadata": {}}, {"text": "In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023 , pages 9058–9069, Singapore.", "metadata": {}}, {"text": "Association for Computational Linguistics.", "metadata": {}}, {"text": "Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhen-\nheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang,\nAmelie Chi Zhou, and Xiaowen Chu.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Burst-\ngpt: A real-world workload dataset to optimize llm\nserving systems.", "metadata": {}}, {"text": "Preprint, arXiv:2401.17644.", "metadata": {}}, {"text": "Grant Wilkins, Srinivasan Keshav, and Richard Mortier.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Offline energy-optimal llm serving: Workload-\nbased energy models for llm inference on heteroge-\nneous systems.", "metadata": {}}, {"text": "ACM SigEnergy newletter.", "metadata": {}}, {"text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al.", "metadata": {}}, {"text": "2020.", "metadata": {}}, {"text": "Transformers: State-of-the-art natural\nlanguage processing.", "metadata": {}}, {"text": "In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.", "metadata": {}}, {"text": "Carole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Sustainable ai: Environmental implica-\ntions, challenges and opportunities.", "metadata": {}}, {"text": "Proceedings of\nMachine Learning and Systems, 4:795–813.", "metadata": {}}, {"text": "Yanran Wu, Inez Hua, and Yi Ding.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "Unveil-\ning environmental impacts of large language model\nserving: A functional unit view.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2502.11256.", "metadata": {}}, {"text": "An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "Qwen2.", "metadata": {}}, {"text": "5 tech-\nnical report.", "metadata": {}}, {"text": "arXiv preprint arXiv:2412.15115.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun. 2022. Orca: A\ndistributed serving system for {Transformer-Based}\ngenerative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI 22), pages 521–538.\nYinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.\n2024. {DistServe}: Disaggregating prefill and de-\ncoding for goodput-optimized large language model\nserving. In 18th USENIX Symposium on Operat-\ning Systems Design and Implementation (OSDI 24),\npages 193–210.\nA Hardware Details\nIn Table 5, we provide additional details on the\nhardware configurations of the nodes used in our\nbenchmarking experiments.\nB Dataset Licenses\nThe CNN-DailyMail dataset used for summariza-\ntion is released under the Apache-2.0 License. The\ndataset Wikitext-2 dataset for text generation is\navailable under the Creative Commons Attribution-\nShareAlike License. The WMT-14 translation\ndatasets are released for non-commercial use. The\nBurstGPT and Azure trace datasets are released\nunder CC-BY-4.0 licenses.\nC Acknowledgment of AI Assistance\nArtificial intelligence assistance was used to as-\nsist in literature review and for code completion\nassistance, specifically during the creation of visu-\nalizations.\nD Additional Optimzations: Continuous\nBatching\nIn Figure 9, we present additional results on the\nimpact of vLLM’s continuous batching for online\ninference in which we observe that at large batch\nsizes continuous batching yields reductions in en-\nergy use.\nE Additional Sequence Length Results\nIn Figure 10, we present additional results on the\neffects of scaling input and output sequence lengths\nwith the PyTorch framework.\n13", "sentences": [{"text": "Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun.", "metadata": {}}, {"text": "2022.", "metadata": {}}, {"text": "Orca: A\ndistributed serving system for {Transformer-Based}\ngenerative models.", "metadata": {}}, {"text": "In 16th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI 22), pages 521–538.", "metadata": {}}, {"text": "Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "{DistServe}: Disaggregating prefill and de-\ncoding for goodput-optimized large language model\nserving.", "metadata": {}}, {"text": "In 18th USENIX Symposium on Operat-\ning Systems Design and Implementation (OSDI 24),\npages 193–210.", "metadata": {}}, {"text": "A Hardware Details\nIn Table 5, we provide additional details on the\nhardware configurations of the nodes used in our\nbenchmarking experiments.", "metadata": {}}, {"text": "B Dataset Licenses\nThe CNN-DailyMail dataset used for summariza-\ntion is released under the Apache-2.0 License.", "metadata": {}}, {"text": "The\ndataset Wikitext-2 dataset for text generation is\navailable under the Creative Commons Attribution-\nShareAlike License.", "metadata": {}}, {"text": "The WMT-14 translation\ndatasets are released for non-commercial use.", "metadata": {}}, {"text": "The\nBurstGPT and Azure trace datasets are released\nunder CC-BY-4.0 licenses.", "metadata": {}}, {"text": "C Acknowledgment of AI Assistance\nArtificial intelligence assistance was used to as-\nsist in literature review and for code completion\nassistance, specifically during the creation of visu-\nalizations.", "metadata": {}}, {"text": "D Additional Optimzations: Continuous\nBatching\nIn Figure 9, we present additional results on the\nimpact of vLLM’s continuous batching for online\ninference in which we observe that at large batch\nsizes continuous batching yields reductions in en-\nergy use.", "metadata": {}}, {"text": "E Additional Sequence Length Results\nIn Figure 10, we present additional results on the\neffects of scaling input and output sequence lengths\nwith the PyTorch framework.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "CPU RAM GPU GPU TDP FP32 TFLOPS Bfloat16 TFLOPS\n256xAMD EPYC 7763 1TB Nvidia RTX A6000 300W 38.7 –\n128xAMD EPYC 7513 500GB Nvidia RTX A6000 Ada 300W 91.1 –\n128xAMD EPYC 7763 1TB Nvidia RTX A100-80 GB 300W 156 312\nTable 5: Node Hardware Specifications\n21 23 25 27 29\nBatch size\n−12\n−10\n−8\n−6\n−4\n−2\n0\n2\nEnergy Reduction (%)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n−12.5\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (c) A6000\nFigure 9: Energy reduction comparison between online and offline serving modes across different GPUs\n(Eof f line − Eonline) ∗ 100/Eof f line). The optimizations employed for online serving save up to 5% energy at\nlarger batch sizes\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nPreﬁll energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput or output sequence length (tokens)\n10−2\n10−1\n100\nEnergy for 1024 examples (kWh)\nEnergy scaling for input and output context lengths\nbs=1\nbs=8\nbs=64\nInput\nOutput\nFigure 10: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vanilla PyTorch backend.\n14", "sentences": [{"text": "CPU RAM GPU GPU TDP FP32 TFLOPS Bfloat16 TFLOPS\n256xAMD EPYC 7763 1TB Nvidia RTX A6000 300W 38.7 –\n128xAMD EPYC 7513 500GB Nvidia RTX A6000 Ada 300W 91.1 –\n128xAMD EPYC 7763 1TB Nvidia RTX A100-80 GB 300W 156 312\nTable 5: Node Hardware Specifications\n21 23 25 27 29\nBatch size\n−12\n−10\n−8\n−6\n−4\n−2\n0\n2\nEnergy Reduction (%)\n(a) A100 80GB PCIe\n20 21 22 23 24 25 26 27 28\nBatch size\n−12.5\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (b) A6000 Ada\n20 21 22 23 24 25 26 27 28\nBatch size\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\nEnergy Reduction (%) (c) A6000\nFigure 9: Energy reduction comparison between online and offline serving modes across different GPUs\n(Eof f line − Eonline) ∗ 100/Eof f line).", "metadata": {}}, {"text": "The optimizations employed for online serving save up to 5% energy at\nlarger batch sizes\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(kWh)\nPreﬁll energy vs.", "metadata": {}}, {"text": "input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(kWh)\nDecode energy vs.", "metadata": {}}, {"text": "input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput or output sequence length (tokens)\n10−2\n10−1\n100\nEnergy for 1024 examples (kWh)\nEnergy scaling for input and output context lengths\nbs=1\nbs=8\nbs=64\nInput\nOutput\nFigure 10: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vanilla PyTorch backend.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nPreﬁll energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (kWh)\nDecode energy vs. output sequence length\nbs=1\nbs=8\nbs=64\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex. (s)\nGeneration duration vs. output sequence length\nbs=1\nbs=8\nbs=64\n64 prompt toks\n512 prompt toks\nFigure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference.\nHere, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other\ndimension of sequence length.\n15", "sentences": [{"text": "100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(kWh)\nPreﬁll energy vs.", "metadata": {}}, {"text": "input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nInput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(kWh)\nDecode energy vs.", "metadata": {}}, {"text": "input sequence length\nbs=1\nbs=8\nbs=64\n8 output toks\n64 output toks\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(kWh)\nDecode energy vs.", "metadata": {}}, {"text": "output sequence length\nbs=1\nbs=8\nbs=64\n100 101 102 103 104\nOutput sequence length (tokens)\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\nEnergy for 1024 ex.", "metadata": {}}, {"text": "(s)\nGeneration duration vs.", "metadata": {}}, {"text": "output sequence length\nbs=1\nbs=8\nbs=64\n64 prompt toks\n512 prompt toks\nFigure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference.", "metadata": {}}, {"text": "Here, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other\ndimension of sequence length.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "classiﬁcation summarization textgen translation\n0.02\n0.04\n0.06\n0.08\n0.10\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.010\n0.015\n0.020\n0.025\n0.030\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.005\n0.010\n0.015\n0.020\n0.025\nEnergy for 1024 examples (kWh)\nFigure 12: Classical NLP tasks and their energy inten-\nsities with vLLM backends. From top to bottom, the\nbatch size varies from 1, 8, to 128\n16", "sentences": [{"text": "classiﬁcation summarization textgen translation\n0.02\n0.04\n0.06\n0.08\n0.10\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.010\n0.015\n0.020\n0.025\n0.030\nEnergy for 1024 examples (kWh)\nclassiﬁcation summarization textgen translation\n0.005\n0.010\n0.015\n0.020\n0.025\nEnergy for 1024 examples (kWh)\nFigure 12: Classical NLP tasks and their energy inten-\nsities with vLLM backends.", "metadata": {}}, {"text": "From top to bottom, the\nbatch size varies from 1, 8, to 128\n16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}]}