{"document_id": "chen2024", "title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation", "text": "Efficient Heterogeneous Large Language Model Decoding\nwith Model-Attention Disaggregation\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\nKang Chen1 Yongwei Wu1\n1Tsinghua University\n2ByteDance\nAbstract\nTransformer-based large language models (LLMs) exhibit\nimpressive performance in generative tasks but also intro-\nduce significant challenges in real-world serving due to in-\nefficient use of the expensive, computation-optimized accel-\nerators. Although disaggregated serving architectures have\nbeen proposed to split different phases of LLM inference, the\nefficiency of decoding phase is still low. This is caused by\nthe varying resource demands of different operators in the\ntransformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that\nclashes with the strengths of modern accelerators, especially\nfor long context requests.\nTo enhance the efficiency of LLM decoding, we introduce\nmodel-attention disaggregation. This approach leverages a\ncollection of cheap, memory-optimized devices for the atten-\ntion operator while still utilizing high-end accelerators for\nother parts of the model. This heterogeneous setup ensures\nthat each component is tailored to its specific workload, max-\nimizing overall performance and cost efficiency. Our com-\nprehensive analysis and experiments confirm the viability\nof splitting the attention computation over multiple devices.\nAlso, the communication bandwidth required between het-\nerogeneous devices proves to be manageable with prevalent\nnetworking technologies. To further validate our theory, we\ndevelop and deploy Lamina, an LLM inference system that\nincorporates model-attention disaggregation in a distributed\nheterogeneous cluster. Experimental results indicate that Lam-\nina can provide 16.1 ∼ 90.1% higher estimated throughput\nthan existing solutions with similar costs.\n1 Introduction\n1.1 Motivation\nDisaggregated serving architectures for large language mod-\nels (LLMs) [40, 41, 59] have recently emerged as efficient\nframeworks for handling generative inference requests. The\ncore concept of disaggregation involves allocating separate\nresources for different tasks to improve resource utilization.\nThis approach aligns perfectly with LLM processing, which\ncan be divided into two distinct phases. The first phase, known\nas the prefill phase, processes all input tokens from the prompt\nin parallel and is computation-bound. The second phase, i.e.,\nthe decode phase, generates the output tokens one after an-\nother, and is typically memory-bound.\nSplitting the two phases of inference reduces interference\nbetween different requests and allows for more flexible paral-\nlel configurations for the two phases. To better leverage the\ndiffering characteristics of each phase, several methods pro-\npose using heterogeneous hardware to reduce the cost of dis-\naggregated serving [12, 59]. Specifically, flagship all-rounder\nGPUs like NVIDIA H100 integrate high-performance com-\nputational units and high-bandwidth memory (HBM) within\na single package, delivering good performance for LLM infer-\nence. However, as shown in Table 1, specialized accelerators\noptimized for either computation or bandwidth can be sig-\nnificantly cheaper than the H100 in terms of TFLOPS per\ndollar/watt (e.g., TPU v6e) or bandwidth per dollar/watt (e.g.,\nNVIDIA H20), but not both. This cost disparity arises because\nall-rounder GPUs combine powerful computation units, HBM\ncontrollers, and high-bandwidth internal buses within a single\nchip. Such integration leads to larger die sizes and increased\ntransistor counts, posing additional challenges for chip de-\nsigning, packaging, and thermal management [21, 25, 55], all\nof which drive up the design and manufacturing cost.\nAccording to our analyses and experiments, while the sep-\naration of resources works well for the prefill nodes, we iden-\ntified significant inefficiencies in the decoding phase. For\ninstance, as analyzed in section 2, the computation resource\nutilization is often below 20% when serving the LLaMA3-\n70B model with H100. This is primarily due to the limited\nGPU memory size, which cannot accommodate the large ag-\ngregated KV cache for large batches, as well as the low arith-\nmetic intensity of the attention operators.\nA detailed examination reveals that the decoding phase\nmainly comprises two types of operators, each facing dis-\ntinct resource bottlenecks. Linear transformations, includ-\n1\narXiv:2405.01814v2  [cs.LG]  10 Apr 2025\n\nTable 1: H100, H20, and TPU v6e specifications.\nH100 H20 TPU v6e [7]\nBF16 TFLOPs 989 148 918\nMemory capacity 80 GB 96 GB 32 GB\nMemory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s\nPower rating 700 W 400 W unlisted\nInter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s\nNetwork bandwidth 400 Gbps 400 Gbps 200 Gbps\nPrice per chip [2] $11.06/hr $4.63/hr * $2.70/hr\n*: As H20 is not readily available on cloud service providers, the listed price\nis estimated using the relative complete system cost against H100.\ning QKVO projections and feedforward networks, are im-\nplemented with generalized matrix-matrix multiplications\n(GEMMs). Since all requests multiply with the same parame-\nter matrices in these operators, processing multiple requests in\nbatch can avoid repeated parameter loads from memory, mak-\ning these operators primarily computation-bound. In contrast,\nthe self-attention operator is memory-bound. This pivotal\noperator requires each request to read its own, distinct KV\ncache, resulting in a batched generalized matrix-vector mul-\ntiplication (BGEMV) pattern. Increasing batch sizes does\nnot improve the computation resource utilization but places\nadditional pressure on the already limited memory capacity.\n1.2 Our Contributions\nIn light of the above findings, we propose an innovative con-\ncept called model-attention disaggregation, as illustrated\nin Figure 1. This approach involves further disaggregating\nthe decoding phase by creating two pools of heterogeneous\naccelerators: one optimized for computational power and the\nother for memory resources. We use the memory-optimized\naccelerators to store the KV caches and process self-attention\noperators, while the computation-optimized devices handle\nall other operators. By choosing the most suitable devices\nfor each kind of operators, this architecture further increases\nhardware utilization and leads to better overall performance.\nMoreover, different LLMs and workloads present varying\ncomputation and memory resource requirements. Homoge-\nneous accelerator solutions, however, can only providea fixed\nratio of computation and memory resources , which can\nresult in resource wastage. For instance, as context lengths\nincrease, the memory capacity needed to store the KV cache\nexpands accordingly; with a fixed resource ratio, a substan-\ntial portion of computational resources remains underutilized\nwhen processing requests with long contexts. By pooling het-\nerogeneous accelerators, we can adjust the number of each\nkind of accelerators to better match the LLM and workload\nand hence improve resource utilization.\nThe primary challenge associated with attention offload-\ning arises from the substantial communication demands be-\nModel/Attention Disaggregation\nKV Cache①\n②\nPrefill/Decode\nDisaggreagtion\nModel\nWeights\nCompute-\nOptimized GPU\nKV Cache\nMemory-\nOptimized GPU\nPagedCache\nManager\nContinuous\nBatching\nRequest\nManager\nLocal Scheduler\nPrefill WorkersModel Workers Attention Workers\nModel\nWeights\nCompute-\nOptimized GPU\nGlobal \nScheduler\nQKV\nAttn Out\nFigure 1: The disaggregated architecture of LLM serving.\ntween heterogeneous accelerators when sending and receiving\nthe inputs and outputs of self-attention operators. Unlike the\noriginal prefill-decode disaggregation, where the KV cache\nis transferred only once between the prefill nodes and the\ndecode nodes, our model-attention disaggregation architec-\nture requires inter-GPU communication for every layer of the\nmodel. Even worse, communication between heterogeneous\nGPUs must rely on data center networks (DCNs), such as\nEthernet and InfiniBand, which provide only ~10% of the\nbandwidth of inter-chip interconnects (ICIs) like NVLink\nbetween homogeneous GPUs. If not handled properly, this\nfrequent communication would introduce high network round-\ntrip times (RTTs) to the token generation latency, worsening\nthe user experience.\nTo assess the practicality of our novel disaggregated ar-\nchitecture, we first conduct a detailed quantitative study in-\ndicating that these concerns are manageable in the context\nof LLM inference. In subsection 3.1, we provide profiling\nand analysis to determine the minimum bandwidth threshold\nbetween different accelerator pools. Our findings reveal that\n200/400Gbps DCNs, widely deployed in current AI-oriented\ndata centers, suffice for attention offloading. However, this\ncan only be achieved if the inter-GPU communication is care-\nfully implemented and optimized, which is not possible for\noff-the-shelf communication libraries such as NCCL or Gloo.\nTo realize the idea of model/attention disaggregation, we\nimplement two specific techniques to reduce the network-\ning overhead. First, we designed and deployed a fully host-\nbypassed network stack. Leveraging PCIe P2P capabilities,\nthis revamped network stack enables GPUs to directly talk\nwith network interface cards (NICs), eliminating the need\nfor host CPU synchronization and involvement for network\ntransmissions. The network data is also directly read from\nand written to GPU memory without passing through host\nmemory. Additionally, we developed an automated model\n2\n\n[Image page=2 idx=1 name=X11.png] Size: 512x512, Data: 28151 bytes\n\n[Image page=2 idx=2 name=X14.png] Size: 504x420, Data: 7905 bytes\n\nconverter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 ∼ 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39× on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 ∼ 1024\nl Sequence length. 128 ∼ 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n∑\nj=1\nsoftmax\n\u0012q⊤\ni k j√\nd\n\u0013\nv j, ⋆\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx′\ni = Wproj · fact (Wfc · yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by ⋆ in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3\n\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2Time (s)\n2 x NVIDIA H100\ntime\nMFU\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n4 x NVIDIA H100\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n8 x NVIDIA H100\n0\n1000\n2000\n0\n1000\n2000\n0\n1000\n2000\nMFU (TFlops)\nFigure 2: Measured time consumption and MFU of non-attention operators in LLaMA3-70B during one decode iteration. Results\nwith different tensor parallelisms are presented. The dotted lines indicate the projected values using the roofline model.\nGPU memory. The resulting arithmetic intensity, 2NB\ne(N+2Bd),\nincreases rapidly with larger batch sizes.\nFigure 2 shows the latency and memory throughput uti-\nlization (MFU) of non-attention operators in LLaMA3-70B,\nmeasured on an NVIDIA H100 GPU, alongside projections\nbased on the roofline model. For small batch sizes (less than\n100), the workload is bandwidth-bound, with latency predom-\ninantly caused by accessing model parameters from GPU\nmemory. In this regime, the MFU remains below 20%, indi-\ncating significant underutilization of computational resources.\nAs the batch size increases, the workload transitions to being\ncomputation-bound, with an increase in latency. To optimize\nGPU resource utilization, larger batch sizes are preferred. But,\nachieving this is often constrained by the limited VRAM ca-\npacity, which cannot accommodate the required KV cache\nsize, a limitation discussed in detail later.\n2.2.2 The Underutilization in Attention Operators\nDifferent from the weight matrix projection operators, the\nattention operator, when processing a batch of requestss still\nperforms a batched matrix-vector multiplication, where each\nquery accesses and processes its own KV cache. As a re-\nsult, the arithmetic intensity of the attention operator remains\nconstant, irrespective of the batch size. This behavior makes\nattention operations memory-bound, and increasing the batch\nsize does not improve resource utilization. More recent mod-\nels have adopt grouped-query attention (GQA), which splits\nqi into a group of G independent queries and reduce the size\nof ki and vi by a factor of G. Each query goes through the\nattention computation with the same ki and vi and the outputs\nare simply concatenated. With GQA, the arithmetic intensity\nof attention operators is increased G times, but is still quite\nlow compared with other operators.\nAs shown in Figure 3, the bandwidth utilization of attention\noperators remains above 70% even for small batch sizes, such\nas 20. This holds true even on memory-specialized acceler-\nators like H20, which delivers only 15% of the TFLOPs of\nthe H100. However, the batch size achievable for attention\noperations is constrained by GPU memory capacity, particu-\nlarly due to the high memory demand of KV caches for longer\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H100\ntime\nMBU\n10 20 30\nBatch size\nl=8192 @ NVIDIA H100\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H20\n10 20 30\nBatch size\nl=8192 @ NVIDIA H20\n0\n1000\n2000\n3000\nMBU (GB/s)\n(100%)\n0\n1000\n2000\n3000\n4000\nMBU (GB/s)\n(100%)\nFigure 3: Measured time consumption and model bandwidth\nutilization (MBU) of attention operators in LLaMA3-70B\nduring one decode iteration. Results with different sequence\nlengths and hardware configurations are presented.\ncontext lengths. For example, with a context length of 8192,\nthe full memory of an H100 can only hold KV caches for\nabout 30 requests, with the actual number being lower due to\nmemory used by model weights. Consequently, the limited\nbatch size for attention operations becomes a key bottleneck,\npreventing efficient utilization of computational resources for\nnon-attention operations during the decoding phase.\n3 Model-Attention Disaggregation\n3.1 Overview\nCurrent LLM serving systems often employ the same hard-\nware for both attention and non-attention operators during the\ndecode phase. However, our analysis reveals that this homo-\ngeneous approach leads to suboptimal resource utilization for\nboth types of operators, due to the following reasons:\n• Attention operators demonstrate low arithmetic inten-\nsity, as each value retrieved from the KV cache partici-\npates in only a limited number of computations. Given\n4\n\nthe disparity between memory bandwidth and computing\npower in modern high-performance accelerators, which\nfavor high arithmetic intensity for efficient resource uti-\nlization, these operators tend to underutilize the compu-\ntation resources of advanced GPUs.\n• For non-attention operators, while increasing the batch\nsize could potentially enhance hardware utilization, this\nalso results in a corresponding increase in the KV cache,\nwhich may exceed the available memory capacity. Con-\nsequently, to prevent memory overflow, the batch size is\noften kept small, which also leads to inefficient hardware\nutilization because of low arithmetic intensity.\nTo address the above limitations of homogeneous decoding\nsolutions, we propose the model-attention disaggregation\narchitecture, which uses memory-specialized accelerators to\nstore KV caches and compute the attention operators; the non-\nattention operators are still executed on original accelerators.\nA model-attention disaggregation system can use multiple de-\nvices of each kind to provide different degrees of parallelism\n(DOPs). If we use a GPUs for non-attention operators and b\nmemory-optimized GPUs for attention operators, we denote\nthe DOP as (a,b).\nBy leveraging the cheaper memory-optimized devices, we\ncan make larger batch sizes due to the extended memory\ncapacities to store the KV caches, hence increasing the arith-\nmetic intensity and promoting the hardware utilization of\nnon-attention operators. Moreover, as the attention compu-\ntation are moved to memory-optimized devices, we avoid\nwasting precious computation resources of high-end GPUs.\nOne potential obstacle in implementing attention offloading\nlies in the necessity of data transmission between heteroge-\nneous accelerators for each layer of the model, which could\nencounter the communication wall problem and increase the\nend-to-end decoding latency. We conduct a quantitative analy-\nsis to determine the required interconnect bandwidth for such\ntransfers. Say we run one iteration with batch size B, and\nwe can afford α× more latency for the networking overhead,\nthe minimum interconnect bandwidth required can thus be\ncalculated as\nminimum bandwidth =size of data to transmit\nα · computation time\n= (2 + 2/G)edBL\nα[MTIME(B) +ATIME(B,l)]\nwhere MTIME(B) and ATIME(B,l) is running time of non-\nattention and attention operators at batch size B and sequence\nlength l, respectively, and they can be measured experimen-\ntally. The estimated minimum bandwidths required for differ-\nent batch sizes, when α = 0.2, are calculated and presented\nin Figure 4.\nAs evident from the data presented, the required intercon-\nnect bandwidth does not exceed 30 GB/s, even when dealing\n0 50 100\nBatch size\n0\n5\n10\n15Bandwidth (GB/s)\nl=4096\nl=8192\n(a) DOP = (2,2)\n0 100 200\nBatch size\n0\n10\n20Bandwidth (GB/s)\nl=4096\nl=8192 (b) DOP = (2,4)\nFigure 4: The required network bandwidth for decoding\nLLaMA3-70B using attention offloading with H100 and H20,\nwith at most 20% latency slow-down for network overhead.\nwith batch sizes as high as 300. This bandwidth demand\ncan be easily met by networking technologies like 400Gbps\nEthernet. Indeed, contemporary data centers already fulfill\nthis requirement, where each GPU is typically equipped with\nan exclusive 400Gbps NIC to provide sufficient networking\nbandwidth for LLM training.\nFor memory devices, the identical interconnection band-\nwidth is also necessary to communicate with computational\ndevices. Since we employ a collection of more economical\nyet less powerful memory devices to collaboratively com-\npute attention, the communication bandwidth needed for each\nindividual device is significantly smaller. Consequently, we\ncan choose to either equip each device with a less powerful\nNIC or install a single shared 400Gbps NIC to serve multiple\nmemory devices.\n3.2 Practical Challenges\nWhile model-attention disaggregation promises potential ben-\nefits in improving LLM decoding efficiency, it also introduces\na set of formidable practical challenges. We discuss some of\nthese challenges below.\nFrequent network communications. By separating the\nattention operator from computation-optimized devices to\nmemory-optimized devices, we introduce cross-machine data\ncommunications within each model layer. Even though the\ninterconnect bandwidth in existing data centers is sufficient\nfor attention offloading, we found that networking latency\nmight still be a problem for efficient LLM decoding. With\nattention offloading, we have layer-wise data transfer be-\ntween GPUs on different nodes, which may be up to thou-\nsands round-trips per second. These frequent network trans-\nfers might significantly increase the decoding time due to the\naccumulated network latencies. Hence, we need a refurnished,\nlatency-optimized, GPU-aware networking stack for optimal\nperformance of model-attention disaggregation.\nSoftware engineering challenges. With model-attention\ndisaggregation, we are moving the execution of attention\noperator, an intermediate operation of the transformer block,\nto other devices. This requires complicated and destructive\n5\n\nmodifications to the existing LLM codebase. Specifically, we\nhave to dissect the models into separate slices that do not align\nwith the modular structure of the transformer-based LLMs.\nThis process is not only labor-intensive and error-prone but\nalso significantly increases maintenance complexity. Hence,\nautomated tools to help slice the models and perform relevant\noptimizations are highly desirable.\nDifficult execution overlapping. In a heterogeneous disag-\ngregated system, various devices such as compute-optimized\nGPUs, memory-optimized GPUs, and NICs can be utilized\nsimultaneously. Hence, we might achieve significant execu-\ntion time reduction if the execution of operations occupying\ndifferent devices could be overlapped. However, in the trans-\nformers architectures of current LLMs, attention operators\nand model operators are tightly interleaved in a sequential\nmanner, with the output of one operator being transmitted\nover the network for the input of the other. Consequently,\noperations that depend on distinct hardware resources cannot\nbe effectively overlapped in time, leading to considerable re-\nsource underutilization. Therefore, careful orchestration of\noperations on various devices and efficient design of task\npipelines are required to promote execution overlapping and\nincrease resource utilization.\n4 System Design\nWe build Lamina, a distributed heterogeneous LLM decoding\nsystem that implements model-attention disaggregation and\nsolves the related challenges. Lamina employs two kinds of\nacceleration devices: memory devices are used for storing KV\ncache and computing the attention operator, and computation\ndevices are used for storing model parameters and computing\nother parts of the model. These two kinds of devices are inter-\nconnected with high-speed DCN, e.g., Infiniband or Ethernet.\n4.1 Fully Host-Bypassed Network Stack\nThe communication between GPUs across different nodes,\noften utilizing RDMA technologies, is a complex process that\nrequires the coordination of multiple system agents, including\nthe CPU, GPU, and NIC. To reduce GPU-aware networking\noverhead, GPUDirect RDMA (GDR) [3] is developed to al-\nlow the RDMA-capable NIC (RNIC) to directly access GPU\nmemory. This eliminates the need for host memory as an in-\ntermediate buffer, thereby enhancing both network latency\nand bandwidth. However, the control path still requires CPU\ninvolvement and includes several steps, all of which lie on\nthe critical path and contribute to network latency. Specifi-\ncally, when transferring data using GPUDirect RDMA, the\nfollowing steps are performed:\n1. The local CPU waits for all prior GPU kernels to com-\nplete, ensuring the data to be transmitted is ready.\n2. The local CPU submits a send work request (WR) to the\nRNIC.\n3. The local RNIC processes the send WR, fetching the data\nfrom GPU memory and transmitting it over the physical\nnetwork link.\n4. The remote RNIC receives the data and writes it to the\nGPU memory.\n5. The remote CPU waits for the RDMA receive operation\nto complete.\n6. The remote CPU launches the subsequent GPU kernels.\nBased on our experimental results, steps 1 through 5 may\nincur a latency of 60–70 µs. Furthermore, because we have to\nlaunch the kernel after the received data is ready, the GPU ker-\nnel launch overhead, which might be up to20 µs, is also added\nto end-to-end latency. All these additional latencies pose a sig-\nnificant overhead for model-attention disaggregation, which\nmust rely on frequent network communications.\nTo reduce such networking overhead, we develop a fully\nhost-bypassed network (FHBN) stack, which completely\neliminates host CPU involvement in both control and data\npaths of GPU-aware networking. We describe how FHBN\nperforms send and recv operations below.\nFHBN recv. To implement the FHBN recv function, we\nemploy the device-side polling technique to await the comple-\ntion of the recv operation. Specifically, we allocate aseqno\nvariable on the receiver’s GPU memory. The sender incre-\nments the remote seqno with RDMA write after each send\noperation. The data send and seqno increment operations are\nbatched in a single WR post and hence would not increase\nthe end-to-end latency. When the receiver GPU is ready to\nreceive and process the incoming data, it actively polls the\nvalue of seqno with a specialized GPU kernel. This approach\nnot only eliminates the need for CPU involvement during\nthe recv process, but also allows asynchronous launch of the\npolling kernel and subsequent computation kernels to the\nGPU stream. Therefore, the GPU kernel launch overhead is\nalso removed from the critical path.\nFHBN send. The implementation of FHBN send, illustrated\nin Figure 5, is more involved as it necessitates the GPU to\ndirectly submit RDMA commands to RNIC. When the CPU\nsubmits a new RDMA WR to RNIC, it first enqueues the WR\nto the work queue (WQ) in the host memory. Then, it tells the\nRNIC that there is outstanding work by ringing the doorbell\n(DB), a special register in the user access region (UAR) of\nthe RNIC. The UAR is part of the RNIC’s mmio region and is\nmapped to the address space of unprivileged applications to\nallow kernel-bypass RDMA operations. All above steps are\nimplemented in the RDMA userspace library (libibverbs).\nTo enable direct RDMA command submission on GPUs,\nwe have to allow GPUs to directly access the UAR via PCIe\nP2P. Specifically, we use thecudaHostRegisterIoMemory\nAPI to map the UAR into the GPU’s address space. Then,\n6\n\nHost CPU\nCPU Core\nRNIC\nGPU PCIe Switch\ncudaDeviceSync\n(CUDA Driver)\nibv_post_send\n(libibverbs)\nHost DRAM\nWQ\nCQ\nRDMA Send Queue Regular GPU-\naware send\nFHBN send\nUser Access Region\nBlueFlameDB\nFigure 5: Diagram of WR submission with FHBN send and\nconventional GPU-aware send implementations.\nwe reimplement the RDMA command submission logic in\nCUDA device code. To further decrease latency, we leverage\nthe BlueFlame mechanism, a hardware feature provided by\nMellanox RNICs [4]. This approach allows the WR to be\ndirectly submitted to the RNIC with mmio write to UAR,\neliminating the need for the RNIC to fetch the WR from host\nmemory via an expensive PCIe DMA read. Note that the WR\nshould still be enqueued into the WQ, as the hardware may\noccasionally miss the BlueFlame WR and fall back to the\nregular workflow, particularly under heavy loads.\n4.2 Automated Model Converter\n4.2.1 Model Splitting\nIn the attention offloading architecture, different operators\nof the LLM might be executed on different hardware; hence,\nwe need to partition the model into slices, which is achieved\nby cutting at the attention operators. It often involves signifi-\ncant modifications to the existing codebase, primarily because\nthe desired cutting points do not align with the LLM’s inher-\nent modular structure. This misalignment complicates the\npartitioning process and increases the risk of errors and incon-\nsistencies within the heterogeneous system.\nTo facilitate model partitioning, we develop an automated\nmodel splitter capable of transforming the LLM into individ-\nually invokable slices, illustrated in Figure 6. Given the LLM\nsource code, the splitter uses symbolic execution to generate\na weighted computation graph. The weight of each edge de-\nnotes the size of the data passed between the operators, which\nis derived from the model’s shape specification.\nDue to the presence of residual connections and other in-\ntricate model constructs, directly removing the attention op-\nerator does not always result in a disconnected computation\ngraph. Therefore, we compute the minimum weighted cut of\nAttention\n10240\nMLP\n10240\n10240\n10240+\n10240\n30720\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN1\n10240\n10240\nLN2\nEmbd\n4\ninput\nmin cutSlice 1 Slice 2\n10240\nMLP\n10240\n+\n10240\nLN1\n10240\nAttention\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN2\n24576\n10240\n10240\nmin cut Slice 3\n...\nFigure 6: The partitioned computation graph of an LLM.\nthe remaining graph, from the input to the output of the atten-\ntion operator. The edges in this minimum cut, representing\nthe context that must be saved between slice invocations, are\nremoved from the computation graphs. This process is itera-\ntively applied to each attention operator, ultimately yielding\nn + 1 model slices, where n denotes the original number of\nthe attention operators.\n4.2.2 Resource Utilization Overlapping\nWhile the attention operators and other operators in a trans-\nformer block are executed sequentially, a closer examination\nof the attention computation reveals the potential for achiev-\ning partial overlapping of resource utilization. Given an at-\ntention query q and the set of token indices I, the attention\ncomputation can be carried out in a divide-and-conquer man-\nner. Assume that I can be written as the disjoint union of two\nsubsets I1 and I2, and let\nAq(I) = ∑\ni∈I\nsoftmax\n\u0012q⊤ki√\nd\n\u0013\nvi,\nSq(I) = ∑\ni∈I\nexp\n\u0012q⊤ki√\nd\n\u0013\n,\nwhere Aq(I) is the attention output and Sq(I) is the de-\nnominator of softmax, then Aq(I) can be easily obtained\nby combining the partial attention results on I1 and I2, i.e.,\n[Aq(I1),Sq(I1)] and [Aq(I2),Sq(I2)]:\nAq(I) = Aq(I1)Sq(I1) +Aq(I2)Sq(I2)\nSq(I1) +Sq(I2) .\nDuring LLM decoding, we may divide the current token set\ninto two partitions during attention computation: all previous\ntokens (prev) and the newly generated token (new). Note that\n[Aq(prev),Sq(prev)] can be computed as soon as qn is ready;\ntherefore, we may eagerly execute Q-Proj and transfer qn, and\nthen execute K-Proj, V-Proj and transfer kn,vn to the attention\n7\n\n[Image page=7 idx=1 name=X12.png] Size: 193x92, Data: 6377 bytes\n\n[Image page=7 idx=2 name=X8.png] Size: 512x512, Data: 28151 bytes\n\nworkers. As illustrated in Figure 7, this does not only improve\nthe GPU utilization on both kinds of workers, but also reduces\nthe end-to-end latency by hiding the communication behind\nthe computation.\nQ-Proj K-Proj V-Proj\nAttn\nOut-Proj\nQKV Attn Out\nModel\nWorkers\nAttention\nWorkers\n(a) Without resource utilization overlapping.\nQ-Proj K-Proj V-Proj\nPrev Attn\nOut-Proj\nQ Attn Out\nModel\nWorkers\nAttention\nWorkers\nNew Attn\n& Combine\nKV\n(b) With resource utilization overlapping.\nFigure 7: Illustration of resource utilization overlapping by\nsplitting the attention computation.\nThe above attention splitting optimization is integrated in\nour automated model converter. After dissecting the original\nmodel, the converter will generate a serial program of each\nmodel slice by computing a topological order of its compu-\ntation graph. During this topological sort, we always put the\nQ-Proj operator and all its dependencies as early as possible.\nThen, we insert the “send Q” instruction immediately after\nthe Q-Proj operator and “send KV” at the end of this slice.\n4.3 Execution Pipelining\nDue to the serial nature of transformer-based models, if there\nis only one batch under processing, the memory device is idle\nwhen the computation device is working, and vice versa. To\naddress this resource underutilization problem and increase\nsystem throughput, we may run multiple batches concurrently\nin a pipelined fashion. With properly designed pipelining,\nbetter hardware utilization can be achieved without sacrificing\nlatency. We propose the rotational staggered pipelining to\nsolve this problem.\nA2B2C2D2\nA1 D1\nB1\nB3C1\nC3\nA4B4C4\nModel 1\nModel 2\nModel 3\nAttention\nA3 D3\nD5\nC5\nB5\nD4A6\nA5\nB6C6D6\nFigure 8: Illustration of rotational staggered pipelining.\nAssume that we execute n batches concurrently. Let tm,ta\nrepresent the time required for executing one model slice and\none attention operator, respectively. As illustrated in Figure 8,\nwe deploy n − 1 model replicas, with each replica starting its\ntasks at a time of tm\nn−1 later than the previous one. All batches\nshare a common set of memory devices to maximize aggre-\ngated memory bandwidth and improve memory utilization.\nFor every batch, the KV cache is evenly partitioned across\nthese devices. All memory devices jointly compute the at-\ntention operator for a single batch. The number of memory\ndevices is selected to maketa = tm\nn−1. After the attention opera-\ntor, each batch transitions to the next model replica according\nto a rotational schedule; that is, the kth model slice of the jth\nbatch is executed on replica ( j + k) mod (n − 1) +1.\nThis rotational task scheduling, combined with the stag-\ngered execution intervals, guarantees seamless task transitions\nfor each batch and ensures a conflict- and bubble-free work-\nflow on each device. Furthermore, by increasing the num-\nber of concurrent batches, the overall inference latency can\nbe reduced due to the decreased attention computation time.\nHowever, the rotational scheduling requires migrating batch\nexecution contexts between computation devices. Note that\nwhen n = 2, the context migration is unnecessary because\nboth batches are executed within a single model replica.\n5 Implementation\nLamina is implemented with ~6000 lines of Python and C/C++\ncode, in addition to a few lines of CUDA code implementing\ncustom kernels. The fully host-bypassed network stack is built\non top of a modified version of rdma-core [6]. Lamina uses\nRay [5] to facilitate task scheduling and worker placement in\ndistributed heterogeneous environments.\nFault tolerance. With attention-offloading, we have two\ndifferent types of accelerators. Lamina addresses faults in\nthese two types of accelerators with different approaches.\nNote that all request states, i.e., the KV caches, are only stored\nin the attention devices. Consequently, should any model\nworker experience a failure, we can seamlessly replace that\nworker with a functioning one, without losing any progresses.\nIn case of an attention worker failure, we reconstruct the KV\ncache by using the prompt texts and already generated tokens,\nwhich are stored in the LLM service front-end.\nHandling the prefill-decode transition. During the prefill\nphase, the generated KV cache shall be transmitted to the\nattention workers for decoding. For each request, the global\nscheduler picks a set of model workers and attention workers\nto handle the decode phase. Like previous works [40, 59], the\nKV cache is asynchronously transferred in a layer-by-layer\nfashion to hide the communication latency behind computa-\ntion. Moreover, the data transfer is controlled by the attention\nworkers: the attention workers only reads the KV cache from\nprefill workers during the free periods between receiving\nQKV tensors from model workers. This approach minimizes\ninterference with ongoing decoding tasks.\nAttention parallelism. Given the limited capability of a\nsingle device, we may use multiple memory devices to jointly\n8\n\nRequest 1\nRequest 2\nRequest 3\nRequest 4\n(a) Request-level partition.\nRequest 1\nRequest 2\nRequest 3\nRequest 4\ntoken\nhead\ndevice 1\ndevice 2 (b) Head-level partition.\nFigure 9: Work partition methods of the attention operator.\nstore the KV caches and compute the attention operators. As\ndepicted in Figure 9, the attention operators can be paral-\nlelized among memory devices in various ways. One method\nis to distribute different requests across different devices; an\nalternative strategy is to partition and distribute the attention\nheads, which can also be computed independently, to differ-\nent devices. The head-level partitioning approach ensures\na balanced workload distribution, whereas the request-level\npartitioning may result in load imbalance due to the differ-\nences in sequence lengths and therefore the KV cache sizes\namong requests. However, head-level partitioning has limited\nflexibility, as it requires the number of memory devices to\nbe divisible by the number of attention heads. We opt for\nhead-level partitioning in Lamina, which offers optimal load\nbalancing.\n6 Evaluation\nTestbed. We deploy Lamina on a real heterogeneous cluster\nwith two kinds of GPU nodes. Each node consists of either\neight H100 or H20 GPUs, and each GPU is paired with a ded-\nicated ConnectX-7 NIC via PCIe switch. The GPU nodes are\ninterconnected with 400 Gbps RoCE network. We use H100\nas compute-optimized GPUs and H20 as memory-optimized\nGPUs for Lamina.\nModels. Lamina supports a wide variety of LLM architec-\ntures, including OPT [58], LLaMA [48], and LLaMA3 [9]. All\nthese architectures have similar outlines and workload charac-\nteristics and only have minor differences irrelevant to system\ndesigns. Hence, as listed in Table 3, we choose LLaMA-33B,\nLLaMA-65B, and LLaMA3-70B for evaluations. All model\nparameters and KV caches are stored in FP16 format.\nWorkloads To mirror the real-world LLM use cases, we\nuse four request traces collected from the production systems\nof two LLM service providers, Azure [1, 40] and Kimi [41].\nDue to data protection regulations, these traces only con-\ntain the sequence length of user requests but not the actual\ncontents. Hence, we use requests of dummy tokens with the\nsame sequence length for evaluation. The summaries of these\nTable 3: Large language models used for evaluation.\nModel Parameters L d G\nLLaMA-33B 64.7 GB 60 6656 1\nLLaMA-65B 130.1 GB 80 8192 1\nLLaMA3-70B 137.5 GB 80 8192 8\ntraces, including the average prompt tokens (lp) and average\ngenerated tokens (lg), are listed in Table 4.\nTable 4: Request traces used for evaluation.\nTrace # Requests lp lg\nAzure-Conv 19366 1154.7 211.1\nAzure-Code 8819 2047.8 27.9\nKimi-Conv 12031 12035.1 342.6\nKimi-TA 23608 8560.0 182.1\nBaseline system. We compare with vLLM [28], a state-of-\nthe-art LLM serving system optimized for high throughput.\nvLLM also integrates optimizations from other LLM infer-\nence systems, such as continuous batching from Orca [57].\nWe use vLLM with homogeneous H100 GPUs and use tensor\nparallel for multi-GPU inference. As Lamina only focuses\non the decode phase, we modify vLLM to remove the prefill\nphase during evaluation for a fair comparison.\n6.1 Serving Performance\nWe evaluate the serving performance of Lamina against\nvLLM using real-world request traces. We first use homo-\ngeneous and heterogeneous hardware settings of similar costs,\nlisted in Table 5, for vLLM and Lamina, respectively. Com-\npared with vLLM, Lamina replaces half of the H100 devices\nto H20, which is cheaper but provides more memory capacity\nand bandwidth. We measure the token generation throughput,\ntime between tokens (TBT), and average batch size.\nTable 5: Equal-cost hardware configurations for evaluation.\nModel Lamina vLLM\nLLaMA-33B DOP=(1,2) 2 ×H100\n($20.32/hr) ($22.12/hr)\nLLaMA-65B, LLaMA3-70B DOP=(2,4) 4 ×H100\n($40.64/hr) ($44.24/hr)\nAs illustrated in Figure 10, Lamina consistently achieves\n16.1 ∼ 90.1% higher throughput than vLLM among all mod-\nels and traces, given comparable hardware costs. This en-\nhancement is primarily attributed to the larger batch size at-\ntained by Lamina, which is2.39× of vLLM on average. These\n9\n\n0\n2000\n4000\nThroughput\n(token/s)\nAzure-Conv\nLamina\nvLLM\n0\n2000\n4000\nAzure-Code\n0\n500\n1000\nKimi-Conv\n0\n500\n1000\nKimi-TA\n0\n50\n100\n150TBT (ms)\n0\n50\n100\n0\n50\n100\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n250\n500\n750Mean batch size\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n200\n400\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nFigure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.\n0 50 100\nPrice ($/hr)\n0\n2000\n4000\nThroughput\n(token/s)\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n5000\n10000\n(2,1)\n(2,2)\n(2,4)(2,8)\n(4,8)\n(8,8)\n4 8\nLLaMA3-70B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n200\n400\n(2,1)(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Kimi-Conv\n0 50 100\nPrice ($/hr)\n0\n1000\n2000\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA3-70B, Kimi-Conv\nLamina\nvLLM\nFigure 11: Decoding throughput and hardware cost with various hardware configurations. The DOPs for Lamina and tensor\nparallelisms for vLLM are annotated in the plot. The configuration with best cost efficiency is bolded.\nresults demonstrate that Lamina effectively leverages the ex-\ntra memory capacity provided by memory-optimized devices\nto boost decoding throughput. Note that the throughput and\nbatch size of LLaMA3-70B is much larger than LLaMA-33B\nand LLaMA-65B; this is because LLaMA3-70B adopts GQA\nwith a group size of 8, which results in a much smaller KV\ncache size per request.\nLamina experiences an increased token generation latency\nthan vLLM. This can be attributed by two factors. First, Lam-\nina adopts a larger batch size, which results in longer exe-\ncution time on both model and attention workers. Second,\nthe disaggregation of model and attention in Lamina may\nincur additional scheduling and networking overhead. Never-\ntheless, the end-to-end latency of Lamina can still meet the\nSLO requirements of interactive online LLM services in most\ncases.\nWe also explore the decoding throughput of Lamina and\nvLLM under various hardware configurations. Specifically,\nwe adjust the DOPs for Lamina and the number of devices\ninvolving tensor parallelism for vLLM. As the results in Fig-\nure 11 shows, the throughput for Lamina rapidly increases\nwith more attention workers added, which enables larger batch\nsizes. The addition of expensive model worker can only mildly\nimprove the throughput by reducing the model-part execution\nlatency. An exception is the LLaMA3-70B model, where the\nattainable batch size reaches 800 for DOP = (2,4), which al-\nready saturates the computation resources on model workers;\nhence, adding more memory devices will not dramatically\nimprove the throughput. This indicates that the optimal ratio\nbetween model and attention workers varies for different mod-\nels and workloads. In practice, we may conduct a performance\nprofiling and select the best hardware configuration.\n10\n\n2 4 6 8 10 12 14\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=4096\n4 8 12 16 20 24 28 32\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=4096\n16 32 48 64 80 96 112 128\nBatch size\n0\n25\n50\n75Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=4096\n1 2 3 4 5 6 7\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=8192\n2 4 6 8 10 12 14 16\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=8192\n8 16 24 32 40 48 56 64\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=8192\nTBT Model Attention Network\nFigure 12: Token generation latency breakdown.\n6.2 Latency Breakdown\nLatency is a crucial indicator of the service quality offered by\nLLM applications. In this subsection, we measure the time\nbetween tokens (TBT) across various system configurations,\nas well as the execution time for model and attention workers\nand the networking overhead. We use requests with fixed\nsequence lengths (4096 or 8192) as the workload and disable\nrotational pipelining to better reveal the time breakdown.\nAs we can see from Figure 12, for smaller batch sizes, the\nmodel execution time dominates the token generation latency.\nThe attention and networking latency rapidly increases for\nlarger batch sizes, while the model execution time remains\nalmost constant. This indicates that the computation resource\nutilization gets improved as batch size increases. Note that the\nobserved TBT might be less than the sum of model worker\ntime, attention worker time, and network time. This is due to\nthe automated resource utilization overlapping optimization,\nwhich will be further profiled in subsection 6.4.\n6.3 Network Stack Optimizations\nWe evaluate the effectiveness of our fully host-bypassed net-\nwork (FHBN) stack with a microbenchmark. Specifically, we\nconduct a ping-pong test between two GPUs located on dis-\ntinct nodes, using NCCL, NCCL without GPUDirect RDMA,\nGloo, and FHBN as the networking engine. The initiator GPU\nsends a varying amount of data to the remote GPU. Upon re-\nceiving the complete data, the remote GPU immediately sends\nit back to the initiator. We measure the round-trip time from\nthe initiator GPU’s perspective, which encompasses the time\ninterval from the completion of the kernel that generates the\ndata for transmission to the start of the kernel that consumes\nthe received data.\nGloo NCCL (wo/ GDR) NCCL FHBN\n102 104 106 108\nPayload size (byte)\n102\n103\nRound-trip time (µs)\n(a) Round-trip time.\n102 104 106 108\nPayload size (byte)\n0\n20\n40Bandwidth (GB/s) (b) Bandwidth utilization.\nFigure 13: Network ping-pong test between two GPUs on\ndifferent nodes, interconnected with 400Gbps RoCE.\nAs illustrated in Figure 13, for smaller data sizes, the round-\ntrip time is primarily determined by network latency. In this\ncase, FHBN achieves an end-to-end latency of 33.0 µs, rep-\nresenting a 50.5% reduction compared to NCCL’s 66.6 µs\nlatency. This improvement is attributed to the removal of host\nCPU involvement in data transmission, eliminating expensive\nhost-device synchronization and PCIe transactions. This im-\nprovement justifies the efficacy of our fully host-bypassed\nnetwork stack design.\nFor larger payload sizes, the primary factor influencing\nnetworking time is the utilization of network bandwidth. In\nthis scenario, FHBN reaches a peak network bandwidth of\n45.7 GB/s, which corresponds to 91.4% of the line rate. Con-\nversely, NCCL only attains a bandwidth of 35.5 GB/s. As a\nresult, FHBN can also serve as a superior alternative to exist-\ning communication libraries for point-to-point transmission\nof large GPU memory blocks within DCNs.\n11\n\n6.4 Resource Utilization Overlapping\nTo assess the efficacy of resource utilization overlapping (sub-\nsubsection 4.2.2) implemented in our automated model con-\nverter, we conducted a series of experiments on the LLaMA-\n65B and LLaMA3-70B models, with the optimization either\nenabled or disabled. We use request batches of varying sizes\nand the context length of each request is fixed at 4096.\n10 20\nBatch size\n50\n55\n60TBT (ms)\nEnabled\nDisabled\n(a) LLaMA-65B, DOP=(2,2).\n25 50 75 100\nBatch size\n55\n60\n65TBT (ms)\n (b) LLaMA3-70B, DOP=(2,4).\nFigure 14: Time between tokens (TBT) results with automatic\nresource utilization overlapping enabled and disabled.\nAs illustrated in Figure 14, the LLaMA-65B model experi-\nences a significant improvement in performance, achieving up\nto a 13.2% with through automated resource utilization over-\nlapping. The speedup is particularly notable for larger batch\nsizes, which produce larger KV tensors and result in greater\nlatency reduction. The effectiveness is less pronounced for the\nLLaMA3-70B model, where the maximum latency reduction\nis only 3.5%. This is because LLaMA3-70B adopts GQA,\nwhose KV size is 8× smaller. Consequently, there is less\nroom for resource utilization overlapping in LLaMA3-70B.\n7 Discussion\nGenerality of our techniques. Although Lamina is built\nfor model-attention disaggregation, relevant techniques can\nalso be used to enable a wider range of fine-grained LLM\ndisaggregation techniques in distributed heterogeneous envi-\nronments. For example, LoRA [24] and Mixture-of-Experts\n(MoE) [18, 53] all add less computation-intensive operators\nto existing LLM architectures. Like Lamina, we may also of-\nfload the LoRA and MoE operators to less powerful but more\neconomic remote accelerators to reduce the inference cost.\nSuch operator-level disaggregations, unlike prefill-decode dis-\naggregation, require frequent layer-wise communications and\nare considered not feasible unless an optimized networking\nstack like the one in Lamina is used.\nAlternative heterogeneous devices. In Lamina, we may\nuse more specialized accelerating devices for optimal perfor-\nmance and cost. For example, we anticipate that Processing-\nin-Memory (PIM) devices [13,22,26,29,30,32,47,54] will be\na more suitable candidate for memory-optimized devices as\nthey demonstrate even greater cost advantages alongside their\nlarger capacity and higher bandwidth. Besides, we can also\nuse CPU and DRAM for attention computation and KV cache\nstorage. However, due to the relatively smaller bandwidth of\nhost DRAM, it is preferable to also adopt sparse attention\nmechanisms [14, 52] to reduce the size of data read during\nattention computation.\n8 Related Work\nSystem optimizations for LLM Inference. Splitwise [40]\nand DistServe [59] proposes prefill-decode disaggregation,\nwhich improves hardware utilization and minimizes the inter-\nference between the prefill and decode phases. Orca [57] pro-\nposes continuous batching, that batches incoming requests in\niteration granularity. Compared with whole-request batching,\ncontinuous batching greatly reduces resource waste caused\nby early termination during the decode phase. PagedAtten-\ntion [28] focuses on memory management optimizations, us-\ning fine-grained KV cache management to reduce memory\nwaste. PagedAttention can also be used to optimize various de-\ncoding scenarios, like beam search and shared prefixes. These\noptimizations can all be used in our system. FlexGen [44]\nis a heterogeneous LLM inference system employing layer-\nand token-level task partitioning and scheduling. However, it\ndoes not account for the varying characteristics of different\noperators within a layer. LLM-tailored inference systems, like\nDeepSeed [11], Megatron-LM [45], and TensorRT-LLM [39],\nuse optimizations of various aspects including kernel opti-\nmization [17, 23], advanced scheduling [8, 19, 33, 51], and\nefficient memory management [19].\nSpeculative Decoding The speculative decoding technol-\nogy [31,36,38] enables parallel generation of multiple tokens\nfor a single request during the decoding phase. This is done\nby guessing the next few tokens using a smaller auxiliary\nmodel. These predicted tokens are then validated by the pri-\nmary LLM. This validation of the predicted tokens can be\nexecuted in parallel, thereby enhancing the arithmetic inten-\nsity and reducing latency. However, speculative decoding can\nlead to a trade-off in throughput due to the auxiliary model’s\noverhead and the potential need for re-execution in case of\nmisprediction.\nVariations of the Attention Operator. Researchers have\ndeveloped many variations of the attention operator for large\nlanguage models to mitigate the memory bottleneck. GQA\n[10] and MLA [34] are two recent attention mechanisms\ntargeted for memory efficiency. Model quantization uses\nreduced-precision formats (e.g., FP8) to store KV caches. Var-\nious sparse attention mechanisms [14,15,27,35,37,42,43,56]\nhave been adopted, focusing on a subset of all history key-\nvalue pairs during attention computation. All these modifica-\ntions to the attention operator, however, might compromise\nthe model quality.\n12\n\n9 Conclusion\nIn this paper, we present model-attention disaggregation, an\ninnovative architectural approach to improve the efficiency of\nLLM decoding. This approach is motivated by the observation\nthat the LLM decoding phase can be divided into computation-\nintensive parts and memory-intensive parts (i.e., the attention\noperators). Hence, we may use computation- and memory-\noptimized devices for each part to improve the hardware re-\nsource utilization. Moreover, by adjusting the To realize this\nidea, we design a revamped latency-optimized networking\nstack that facilitate the frequent data transfer between remote\nGPUs. We also develop automated tools for transforming and\noptimizing existing LLMs for model-attention disaggrega-\ntion. We develop and deploy Lamina on a cluster comprising\nheterogeneous GPUs. Evaluation on traces collected from\nproduction systems show that Lamina provides16.1 ∼ 90.1%\nhigher throughput than heterogeneous solutions with similar\nhardware costs.\nReferences\n[1] Azure llm inference trace 2023. https:\n//github.com/Azure/AzurePublicDataset/blob/\nmaster/AzureLLMInferenceDataset2023.md.\n[2] Cloud Computing Services | Google Cloud. https:\n//cloud.google.com/.\n[3] GPUDirect RDMA. https://docs.nvidia.com/\ncuda/gpudirect-rdma/.\n[4] Mellanox adapters programmer’s reference manual.\nhttps://network.nvidia.com/files/doc-2020/\nethernet-adapters-programming-manual.pdf .\n[5] Ray. https://www.ray.io/.\n[6] RDMA core userspace libraries and daemons. https:\n//github.com/linux-rdma/rdma-core.\n[7] TPU v6e specification. https://cloud.google.com/\ntpu/docs/v6e.\n[8] Amey Agrawal, Ashish Panwar, Jayashree Mohan,\nNipun Kwatra, Bhargav S. Gulavani, and Ramachandran\nRamjee. Sarathi: Efficient llm inference by piggyback-\ning decodes with chunked prefills, 2023.\n[9] AI@Meta. Llama 3 model card. 2024.\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa:\nTraining generalized multi-query transformer models\nfrom multi-head checkpoints, 2023.\n[11] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\nZhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\nZheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and\nYuxiong He. Deepspeed inference: Enabling efficient\ninference of transformer models at unprecedented scale,\n2022.\n[12] Anonymous. Hexgen-2: Disaggregated generative in-\nference of LLMs in heterogeneous environment. In\nSubmitted to The Thirteenth International Conference\non Learning Representations, 2024. under review.\n[13] Kazi Asifuzzaman, Narasinga Rao Miniskar, Aaron R\nYoung, Frank Liu, and Jeffrey S Vetter. A survey on\nprocessing-in-memory techniques: Advances and chal-\nlenges. Memories-Materials, Devices, Circuits and Sys-\ntems, 4:100022, 2023.\n[14] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\n[15] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse trans-\nformers. arXiv preprint arXiv:1904.10509, 2019.\n[16] Y . Choi, Y . Kim, and M. Rhu. Lazy batching: An\nsla-aware batching system for cloud machine learn-\ning inference. In 2021 IEEE International Symposium\non High-Performance Computer Architecture (HPCA),\npages 493–506, Los Alamitos, CA, USA, mar 2021.\nIEEE Computer Society.\n[17] Tri Dao, Daniel Haziza, Francisco Massa, and Grig-\nory Sizov. Flash-decoding for long-context infer-\nence. https://crfm.stanford.edu/2023/10/12/\nflashdecoding.html.\n[18] Artyom Eliseev and Denis Mazur. Fast inference of\nmixture-of-experts language models with offloading,\n2023.\n[19] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.\nTurbotransformers: An efficient gpu serving system for\ntransformer models. In Proceedings of the 26th ACM\nSIGPLAN Symposium on Principles and Practice of\nParallel Programming, PPoPP ’21, page 389–402, New\nYork, NY , USA, 2021. Association for Computing Ma-\nchinery.\n[20] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. Low\nlatency rnn inference with cellular batching. In Proceed-\nings of the Thirteenth EuroSys Conference, EuroSys ’18,\nNew York, NY , USA, 2018. Association for Computing\nMachinery.\n[21] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman\nHooper, Michael W Mahoney, and Kurt Keutzer. Ai\nand memory wall. IEEE Micro, 2024.\n13\n\n[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok\nJeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N.\nVijaykumar. Newton: A dram-maker’s accelerator-in-\nmemory (aim) architecture for machine learning. In\n2020 53rd Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO), pages 372–385, 2020.\n[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong\nLi, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang.\nFlashdecoding++: Faster large language model infer-\nence on gpus, 2023.\n[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large lan-\nguage models, 2021.\n[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and\nKevin Skadron. Scaling with design constraints: Pre-\ndicting the future of big chips.IEEE Micro, 31(4):16–29,\n2011.\n[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu\nKim, Yuhwan Ro, Seungwon Lee, David Wang, Ji-\nhyun Choi, Jinin So, YeonGon Cho, JoonHo Song,\nJeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim.\nAquabolt-xl hbm2-pim, lpddr5-pim with in-memory pro-\ncessing, and axdimm with acceleration buffer. IEEE\nMicro, 42(3):20–30, 2022.\n[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efficient transformer. arXiv preprint\narXiv:2001.04451, 2020.\n[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,\nHao Zhang, and Ion Stoica. Efficient memory man-\nagement for large language model serving with page-\ndattention. In Proceedings of the 29th Symposium on\nOperating Systems Principles, SOSP ’23, page 611–626,\nNew York, NY , USA, 2023. Association for Computing\nMachinery.\n[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim,\nWoojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo,\nHaerang Choi, Guhyun Kim, Byeongju An, Jeongbin\nKim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook\nPark, Yosub Song, Byeongsu Yang, Hyungdeok Lee,\nSeho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim,\nSanghoon Oh, Joonhong Park, Gimoon Hong, Dongy-\noon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil\nKang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee,\nMinyoung Shin, Minhwan Shin, Jaekyung Cha, Chang-\nson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim,\nIl Park, Junhyun Chun, and Sk Hynix. System architec-\nture and software stack for gddr6-aim. In 2022 IEEE\nHot Chips 34 Symposium (HCS), pages 1–25, 2022.\n[30] Ann Franchesca Laguna, Arman Kazemi, Michael\nNiemier, and X. Sharon Hu. In-memory computing\nbased accelerator for transformer networks for long se-\nquences. In 2021 Design, Automation and Test in Europe\nConference & Exhibition (DATE) , pages 1839–1844,\n2021.\n[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast\ninference from transformers via speculative decoding,\n2023.\n[32] Wantong Li, Madison Manley, James Read, Ankit Kaul,\nMuhannad S. Bakir, and Shimeng Yu. H3datten: Het-\nerogeneous 3-d integrated hybrid analog and digital\ncompute-in-memory accelerator for vision transformer\nself-attention. IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(10):1592–1602, 2023.\n[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent\nLiu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,\nHao Zhang, Joseph E Gonzalez, et al. AlpaServe: Sta-\ntistical multiplexing with model parallelism for deep\nlearning serving. In 17th USENIX Symposium on Oper-\nating Systems Design and Implementation (OSDI 23),\npages 663–679, 2023.\n[34] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu,\nChenggang Zhao, Chengqi Dengr, Chong Ruan, Damai\nDai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Er-\nhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guant-\ning Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang,\nHaowei Zhang, Honghui Ding, Huajian Xin, Huazuo\nGao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong\nGuo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Jun-\njie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,\nLiang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J.\nChen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu,\nRuyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou,\nShanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong\nMa, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng\nZhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu\nSun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li,\nXiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu,\nXiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha\nChen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin\nLiu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou,\nXinyu Yang, Xuan Lu, Xuecheng Su, Y . Wu, Y . K. Li,\nY . X. Wei, Y . X. Zhu, Yanhong Xu, Yanping Huang, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,\nYi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao,\nYing He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan\n14\n\nTan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen\nZhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian\nMa, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen\nZhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu\nWen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan\nWang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts\nlanguage model, 2024.\n[35] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring at-\ntention with blockwise transformers for near-infinite\ncontext, 2023.\n[36] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica,\nZhijie Deng, Alvin Cheung, and Hao Zhang. Online\nspeculative decoding, 2023.\n[37] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher Re, et al. Deja vu: Con-\ntextual sparsity for efficient llms at inference time. In\nInternational Conference on Machine Learning, pages\n22137–22176. PMLR, 2023.\n[38] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-\nhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan\nZhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming\nChen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao\nJia. Specinfer: Accelerating generative large language\nmodel serving with speculative inference and token tree\nverification, 2023.\n[39] NVIDIA. Tensorrt-llm: A tensorrt toolbox for optimized\nlarge language model inference. https://github.\ncom/NVIDIA/TensorRT-LLM.\n[40] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka\nShah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini.\nSplitwise: Efficient generative llm inference using phase\nsplitting, 2024.\n[41] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang,\nYongwei Wu, Weimin Zheng, and Xinran Xu. Moon-\ncake: A kvcache-centric disaggregated architecture for\nllm serving. arXiv preprint arXiv:2407.00079, 2024.\n[42] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,\nSinong Wang, and Jie Tang. Blockwise self-attention\nfor long document understanding. arXiv preprint\narXiv:1911.02972, 2019.\n[43] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. Efficient content-based sparse attention\nwith routing transformers. Transactions of the Associa-\ntion for Computational Linguistics, 9:53–68, 2021.\n[44] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christopher\nRé, Ion Stoica, and Ce Zhang. Flexgen: High-throughput\ngenerative inference of large language models with a\nsingle gpu. In Proceedings of the 40th International\nConference on Machine Learning, ICML’23. JMLR.org,\n2023.\n[45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language\nmodels using model parallelism, 2020.\n[46] Franyell Silfa, Jose Maria Arnau, and Antonio González.\nE-batch: Energy-efficient and high-throughput rnn\nbatching. ACM Trans. Archit. Code Optim., 19(1), jan\n2022.\n[47] Shrihari Sridharan, Jacob R. Stevens, Kaushik Roy, and\nAnand Raghunathan. X-former: In-memory acceleration\nof transformers. IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(8):1223–1233, 2023.\n[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\ntiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foun-\ndation language models, 2023.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In\nProceedings of the 31st International Conference on\nNeural Information Processing Systems, NIPS’17, page\n6000–6010, Red Hook, NY , USA, 2017. Curran Asso-\nciates Inc.\n[50] Samuel Williams, Andrew Waterman, and David Pat-\nterson. Roofline: An insightful visual performance\nmodel for multicore architectures. Commun. ACM ,\n52(4):65–76, apr 2009.\n[51] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang,\nXuanzhe Liu, and Xin Jin. Fast distributed inference\nserving for large language models, 2023.\n[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. Efficient streaming language\nmodels with attention sinks, 2024.\n[53] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh\nMarina. Moe-infinity: Offloading-efficient moe model\nserving, 2024.\n[54] Xiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen. Re-\ntransformer: Reram-based processing-in-memory archi-\ntecture for transformer acceleration. In Proceedings of\n15\n\nthe 39th International Conference on Computer-Aided\nDesign, ICCAD ’20, New York, NY , USA, 2020. Asso-\nciation for Computing Machinery.\n[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming\nZhuang, Weifeng Zhang, Dharmesh Jani, and Peipei\nZhou. Challenges and opportunities to enable large-\nscale computing via heterogeneous chiplets. In 2024\n29th Asia and South Pacific Design Automation Confer-\nence (ASP-DAC), pages 765–770. IEEE, 2024.\n[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. Bp-transformer: Modelling long-\nrange context via binary partitioning. arXiv preprint\narXiv:1911.04070, 2019.\n[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun. Orca: A distributed\nserving system for Transformer-Based generative mod-\nels. In 16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22), pages 521–538,\n2022.\n[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\nformer language models, 2022.\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-\nServe: Disaggregating prefill and decoding for goodput-\noptimized large language model serving. In 18th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 24), pages 193–210, 2024.\n16", "metadata": {"url": "https://arxiv.org/pdf/2405.01814", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Efficient Heterogeneous Large Language Model Decoding\nwith Model-Attention Disaggregation\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\nKang Chen1 Yongwei Wu1\n1Tsinghua University\n2ByteDance\nAbstract\nTransformer-based large language models (LLMs) exhibit\nimpressive performance in generative tasks but also intro-\nduce significant challenges in real-world serving due to in-\nefficient use of the expensive, computation-optimized accel-\nerators. Although disaggregated serving architectures have\nbeen proposed to split different phases of LLM inference, the\nefficiency of decoding phase is still low. This is caused by\nthe varying resource demands of different operators in the\ntransformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that\nclashes with the strengths of modern accelerators, especially\nfor long context requests.\nTo enhance the efficiency of LLM decoding, we introduce\nmodel-attention disaggregation. This approach leverages a\ncollection of cheap, memory-optimized devices for the atten-\ntion operator while still utilizing high-end accelerators for\nother parts of the model. This heterogeneous setup ensures\nthat each component is tailored to its specific workload, max-\nimizing overall performance and cost efficiency. Our com-\nprehensive analysis and experiments confirm the viability\nof splitting the attention computation over multiple devices.\nAlso, the communication bandwidth required between het-\nerogeneous devices proves to be manageable with prevalent\nnetworking technologies. To further validate our theory, we\ndevelop and deploy Lamina, an LLM inference system that\nincorporates model-attention disaggregation in a distributed\nheterogeneous cluster. Experimental results indicate that Lam-\nina can provide 16.1 ∼ 90.1% higher estimated throughput\nthan existing solutions with similar costs.\n1 Introduction\n1.1 Motivation\nDisaggregated serving architectures for large language mod-\nels (LLMs) [40, 41, 59] have recently emerged as efficient\nframeworks for handling generative inference requests. The\ncore concept of disaggregation involves allocating separate\nresources for different tasks to improve resource utilization.\nThis approach aligns perfectly with LLM processing, which\ncan be divided into two distinct phases. The first phase, known\nas the prefill phase, processes all input tokens from the prompt\nin parallel and is computation-bound. The second phase, i.e.,\nthe decode phase, generates the output tokens one after an-\nother, and is typically memory-bound.\nSplitting the two phases of inference reduces interference\nbetween different requests and allows for more flexible paral-\nlel configurations for the two phases. To better leverage the\ndiffering characteristics of each phase, several methods pro-\npose using heterogeneous hardware to reduce the cost of dis-\naggregated serving [12, 59]. Specifically, flagship all-rounder\nGPUs like NVIDIA H100 integrate high-performance com-\nputational units and high-bandwidth memory (HBM) within\na single package, delivering good performance for LLM infer-\nence. However, as shown in Table 1, specialized accelerators\noptimized for either computation or bandwidth can be sig-\nnificantly cheaper than the H100 in terms of TFLOPS per\ndollar/watt (e.g., TPU v6e) or bandwidth per dollar/watt (e.g.,\nNVIDIA H20), but not both. This cost disparity arises because\nall-rounder GPUs combine powerful computation units, HBM\ncontrollers, and high-bandwidth internal buses within a single\nchip. Such integration leads to larger die sizes and increased\ntransistor counts, posing additional challenges for chip de-\nsigning, packaging, and thermal management [21, 25, 55], all\nof which drive up the design and manufacturing cost.\nAccording to our analyses and experiments, while the sep-\naration of resources works well for the prefill nodes, we iden-\ntified significant inefficiencies in the decoding phase. For\ninstance, as analyzed in section 2, the computation resource\nutilization is often below 20% when serving the LLaMA3-\n70B model with H100. This is primarily due to the limited\nGPU memory size, which cannot accommodate the large ag-\ngregated KV cache for large batches, as well as the low arith-\nmetic intensity of the attention operators.\nA detailed examination reveals that the decoding phase\nmainly comprises two types of operators, each facing dis-\ntinct resource bottlenecks. Linear transformations, includ-\n1\narXiv:2405.01814v2  [cs.LG]  10 Apr 2025", "sentences": [{"text": "Efficient Heterogeneous Large Language Model Decoding\nwith Model-Attention Disaggregation\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\nKang Chen1 Yongwei Wu1\n1Tsinghua University\n2ByteDance\nAbstract\nTransformer-based large language models (LLMs) exhibit\nimpressive performance in generative tasks but also intro-\nduce significant challenges in real-world serving due to in-\nefficient use of the expensive, computation-optimized accel-\nerators.", "metadata": {}}, {"text": "Although disaggregated serving architectures have\nbeen proposed to split different phases of LLM inference, the\nefficiency of decoding phase is still low.", "metadata": {}}, {"text": "This is caused by\nthe varying resource demands of different operators in the\ntransformer-based LLMs.", "metadata": {}}, {"text": "Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that\nclashes with the strengths of modern accelerators, especially\nfor long context requests.", "metadata": {}}, {"text": "To enhance the efficiency of LLM decoding, we introduce\nmodel-attention disaggregation.", "metadata": {}}, {"text": "This approach leverages a\ncollection of cheap, memory-optimized devices for the atten-\ntion operator while still utilizing high-end accelerators for\nother parts of the model.", "metadata": {}}, {"text": "This heterogeneous setup ensures\nthat each component is tailored to its specific workload, max-\nimizing overall performance and cost efficiency.", "metadata": {}}, {"text": "Our com-\nprehensive analysis and experiments confirm the viability\nof splitting the attention computation over multiple devices.", "metadata": {}}, {"text": "Also, the communication bandwidth required between het-\nerogeneous devices proves to be manageable with prevalent\nnetworking technologies.", "metadata": {}}, {"text": "To further validate our theory, we\ndevelop and deploy Lamina, an LLM inference system that\nincorporates model-attention disaggregation in a distributed\nheterogeneous cluster.", "metadata": {}}, {"text": "Experimental results indicate that Lam-\nina can provide 16.1 ∼ 90.1% higher estimated throughput\nthan existing solutions with similar costs.", "metadata": {}}, {"text": "1 Introduction\n1.1 Motivation\nDisaggregated serving architectures for large language mod-\nels (LLMs) [40, 41, 59] have recently emerged as efficient\nframeworks for handling generative inference requests.", "metadata": {}}, {"text": "The\ncore concept of disaggregation involves allocating separate\nresources for different tasks to improve resource utilization.", "metadata": {}}, {"text": "This approach aligns perfectly with LLM processing, which\ncan be divided into two distinct phases.", "metadata": {}}, {"text": "The first phase, known\nas the prefill phase, processes all input tokens from the prompt\nin parallel and is computation-bound.", "metadata": {}}, {"text": "The second phase, i.e.,\nthe decode phase, generates the output tokens one after an-\nother, and is typically memory-bound.", "metadata": {}}, {"text": "Splitting the two phases of inference reduces interference\nbetween different requests and allows for more flexible paral-\nlel configurations for the two phases.", "metadata": {}}, {"text": "To better leverage the\ndiffering characteristics of each phase, several methods pro-\npose using heterogeneous hardware to reduce the cost of dis-\naggregated serving [12, 59].", "metadata": {}}, {"text": "Specifically, flagship all-rounder\nGPUs like NVIDIA H100 integrate high-performance com-\nputational units and high-bandwidth memory (HBM) within\na single package, delivering good performance for LLM infer-\nence.", "metadata": {}}, {"text": "However, as shown in Table 1, specialized accelerators\noptimized for either computation or bandwidth can be sig-\nnificantly cheaper than the H100 in terms of TFLOPS per\ndollar/watt (e.g., TPU v6e) or bandwidth per dollar/watt (e.g.,\nNVIDIA H20), but not both.", "metadata": {}}, {"text": "This cost disparity arises because\nall-rounder GPUs combine powerful computation units, HBM\ncontrollers, and high-bandwidth internal buses within a single\nchip.", "metadata": {}}, {"text": "Such integration leads to larger die sizes and increased\ntransistor counts, posing additional challenges for chip de-\nsigning, packaging, and thermal management [21, 25, 55], all\nof which drive up the design and manufacturing cost.", "metadata": {}}, {"text": "According to our analyses and experiments, while the sep-\naration of resources works well for the prefill nodes, we iden-\ntified significant inefficiencies in the decoding phase.", "metadata": {}}, {"text": "For\ninstance, as analyzed in section 2, the computation resource\nutilization is often below 20% when serving the LLaMA3-\n70B model with H100.", "metadata": {}}, {"text": "This is primarily due to the limited\nGPU memory size, which cannot accommodate the large ag-\ngregated KV cache for large batches, as well as the low arith-\nmetic intensity of the attention operators.", "metadata": {}}, {"text": "A detailed examination reveals that the decoding phase\nmainly comprises two types of operators, each facing dis-\ntinct resource bottlenecks.", "metadata": {}}, {"text": "Linear transformations, includ-\n1\narXiv:2405.01814v2  [cs.LG]  10 Apr 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Table 1: H100, H20, and TPU v6e specifications.\nH100 H20 TPU v6e [7]\nBF16 TFLOPs 989 148 918\nMemory capacity 80 GB 96 GB 32 GB\nMemory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s\nPower rating 700 W 400 W unlisted\nInter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s\nNetwork bandwidth 400 Gbps 400 Gbps 200 Gbps\nPrice per chip [2] $11.06/hr $4.63/hr * $2.70/hr\n*: As H20 is not readily available on cloud service providers, the listed price\nis estimated using the relative complete system cost against H100.\ning QKVO projections and feedforward networks, are im-\nplemented with generalized matrix-matrix multiplications\n(GEMMs). Since all requests multiply with the same parame-\nter matrices in these operators, processing multiple requests in\nbatch can avoid repeated parameter loads from memory, mak-\ning these operators primarily computation-bound. In contrast,\nthe self-attention operator is memory-bound. This pivotal\noperator requires each request to read its own, distinct KV\ncache, resulting in a batched generalized matrix-vector mul-\ntiplication (BGEMV) pattern. Increasing batch sizes does\nnot improve the computation resource utilization but places\nadditional pressure on the already limited memory capacity.\n1.2 Our Contributions\nIn light of the above findings, we propose an innovative con-\ncept called model-attention disaggregation, as illustrated\nin Figure 1. This approach involves further disaggregating\nthe decoding phase by creating two pools of heterogeneous\naccelerators: one optimized for computational power and the\nother for memory resources. We use the memory-optimized\naccelerators to store the KV caches and process self-attention\noperators, while the computation-optimized devices handle\nall other operators. By choosing the most suitable devices\nfor each kind of operators, this architecture further increases\nhardware utilization and leads to better overall performance.\nMoreover, different LLMs and workloads present varying\ncomputation and memory resource requirements. Homoge-\nneous accelerator solutions, however, can only providea fixed\nratio of computation and memory resources , which can\nresult in resource wastage. For instance, as context lengths\nincrease, the memory capacity needed to store the KV cache\nexpands accordingly; with a fixed resource ratio, a substan-\ntial portion of computational resources remains underutilized\nwhen processing requests with long contexts. By pooling het-\nerogeneous accelerators, we can adjust the number of each\nkind of accelerators to better match the LLM and workload\nand hence improve resource utilization.\nThe primary challenge associated with attention offload-\ning arises from the substantial communication demands be-\nModel/Attention Disaggregation\nKV Cache①\n②\nPrefill/Decode\nDisaggreagtion\nModel\nWeights\nCompute-\nOptimized GPU\nKV Cache\nMemory-\nOptimized GPU\nPagedCache\nManager\nContinuous\nBatching\nRequest\nManager\nLocal Scheduler\nPrefill WorkersModel Workers Attention Workers\nModel\nWeights\nCompute-\nOptimized GPU\nGlobal \nScheduler\nQKV\nAttn Out\nFigure 1: The disaggregated architecture of LLM serving.\ntween heterogeneous accelerators when sending and receiving\nthe inputs and outputs of self-attention operators. Unlike the\noriginal prefill-decode disaggregation, where the KV cache\nis transferred only once between the prefill nodes and the\ndecode nodes, our model-attention disaggregation architec-\nture requires inter-GPU communication for every layer of the\nmodel. Even worse, communication between heterogeneous\nGPUs must rely on data center networks (DCNs), such as\nEthernet and InfiniBand, which provide only ~10% of the\nbandwidth of inter-chip interconnects (ICIs) like NVLink\nbetween homogeneous GPUs. If not handled properly, this\nfrequent communication would introduce high network round-\ntrip times (RTTs) to the token generation latency, worsening\nthe user experience.\nTo assess the practicality of our novel disaggregated ar-\nchitecture, we first conduct a detailed quantitative study in-\ndicating that these concerns are manageable in the context\nof LLM inference. In subsection 3.1, we provide profiling\nand analysis to determine the minimum bandwidth threshold\nbetween different accelerator pools. Our findings reveal that\n200/400Gbps DCNs, widely deployed in current AI-oriented\ndata centers, suffice for attention offloading. However, this\ncan only be achieved if the inter-GPU communication is care-\nfully implemented and optimized, which is not possible for\noff-the-shelf communication libraries such as NCCL or Gloo.\nTo realize the idea of model/attention disaggregation, we\nimplement two specific techniques to reduce the network-\ning overhead. First, we designed and deployed a fully host-\nbypassed network stack. Leveraging PCIe P2P capabilities,\nthis revamped network stack enables GPUs to directly talk\nwith network interface cards (NICs), eliminating the need\nfor host CPU synchronization and involvement for network\ntransmissions. The network data is also directly read from\nand written to GPU memory without passing through host\nmemory. Additionally, we developed an automated model\n2", "sentences": [{"text": "Table 1: H100, H20, and TPU v6e specifications.", "metadata": {}}, {"text": "H100 H20 TPU v6e [7]\nBF16 TFLOPs 989 148 918\nMemory capacity 80 GB 96 GB 32 GB\nMemory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s\nPower rating 700 W 400 W unlisted\nInter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s\nNetwork bandwidth 400 Gbps 400 Gbps 200 Gbps\nPrice per chip [2] $11.06/hr $4.63/hr * $2.70/hr\n*: As H20 is not readily available on cloud service providers, the listed price\nis estimated using the relative complete system cost against H100.", "metadata": {}}, {"text": "ing QKVO projections and feedforward networks, are im-\nplemented with generalized matrix-matrix multiplications\n(GEMMs).", "metadata": {}}, {"text": "Since all requests multiply with the same parame-\nter matrices in these operators, processing multiple requests in\nbatch can avoid repeated parameter loads from memory, mak-\ning these operators primarily computation-bound.", "metadata": {}}, {"text": "In contrast,\nthe self-attention operator is memory-bound.", "metadata": {}}, {"text": "This pivotal\noperator requires each request to read its own, distinct KV\ncache, resulting in a batched generalized matrix-vector mul-\ntiplication (BGEMV) pattern.", "metadata": {}}, {"text": "Increasing batch sizes does\nnot improve the computation resource utilization but places\nadditional pressure on the already limited memory capacity.", "metadata": {}}, {"text": "1.2 Our Contributions\nIn light of the above findings, we propose an innovative con-\ncept called model-attention disaggregation, as illustrated\nin Figure 1.", "metadata": {}}, {"text": "This approach involves further disaggregating\nthe decoding phase by creating two pools of heterogeneous\naccelerators: one optimized for computational power and the\nother for memory resources.", "metadata": {}}, {"text": "We use the memory-optimized\naccelerators to store the KV caches and process self-attention\noperators, while the computation-optimized devices handle\nall other operators.", "metadata": {}}, {"text": "By choosing the most suitable devices\nfor each kind of operators, this architecture further increases\nhardware utilization and leads to better overall performance.", "metadata": {}}, {"text": "Moreover, different LLMs and workloads present varying\ncomputation and memory resource requirements.", "metadata": {}}, {"text": "Homoge-\nneous accelerator solutions, however, can only providea fixed\nratio of computation and memory resources , which can\nresult in resource wastage.", "metadata": {}}, {"text": "For instance, as context lengths\nincrease, the memory capacity needed to store the KV cache\nexpands accordingly;", "metadata": {}}, {"text": "with a fixed resource ratio, a substan-\ntial portion of computational resources remains underutilized\nwhen processing requests with long contexts.", "metadata": {}}, {"text": "By pooling het-\nerogeneous accelerators, we can adjust the number of each\nkind of accelerators to better match the LLM and workload\nand hence improve resource utilization.", "metadata": {}}, {"text": "The primary challenge associated with attention offload-\ning arises from the substantial communication demands be-\nModel/Attention Disaggregation\nKV Cache①\n②\nPrefill/Decode\nDisaggreagtion\nModel\nWeights\nCompute-\nOptimized GPU\nKV Cache\nMemory-\nOptimized GPU\nPagedCache\nManager\nContinuous\nBatching\nRequest\nManager\nLocal Scheduler\nPrefill WorkersModel Workers Attention Workers\nModel\nWeights\nCompute-\nOptimized GPU\nGlobal \nScheduler\nQKV\nAttn Out\nFigure 1: The disaggregated architecture of LLM serving.", "metadata": {}}, {"text": "tween heterogeneous accelerators when sending and receiving\nthe inputs and outputs of self-attention operators.", "metadata": {}}, {"text": "Unlike the\noriginal prefill-decode disaggregation, where the KV cache\nis transferred only once between the prefill nodes and the\ndecode nodes, our model-attention disaggregation architec-\nture requires inter-GPU communication for every layer of the\nmodel.", "metadata": {}}, {"text": "Even worse, communication between heterogeneous\nGPUs must rely on data center networks (DCNs), such as\nEthernet and InfiniBand, which provide only ~10% of the\nbandwidth of inter-chip interconnects (ICIs) like NVLink\nbetween homogeneous GPUs.", "metadata": {}}, {"text": "If not handled properly, this\nfrequent communication would introduce high network round-\ntrip times (RTTs) to the token generation latency, worsening\nthe user experience.", "metadata": {}}, {"text": "To assess the practicality of our novel disaggregated ar-\nchitecture, we first conduct a detailed quantitative study in-\ndicating that these concerns are manageable in the context\nof LLM inference.", "metadata": {}}, {"text": "In subsection 3.1, we provide profiling\nand analysis to determine the minimum bandwidth threshold\nbetween different accelerator pools.", "metadata": {}}, {"text": "Our findings reveal that\n200/400Gbps DCNs, widely deployed in current AI-oriented\ndata centers, suffice for attention offloading.", "metadata": {}}, {"text": "However, this\ncan only be achieved if the inter-GPU communication is care-\nfully implemented and optimized, which is not possible for\noff-the-shelf communication libraries such as NCCL or Gloo.", "metadata": {}}, {"text": "To realize the idea of model/attention disaggregation, we\nimplement two specific techniques to reduce the network-\ning overhead.", "metadata": {}}, {"text": "First, we designed and deployed a fully host-\nbypassed network stack.", "metadata": {}}, {"text": "Leveraging PCIe P2P capabilities,\nthis revamped network stack enables GPUs to directly talk\nwith network interface cards (NICs), eliminating the need\nfor host CPU synchronization and involvement for network\ntransmissions.", "metadata": {}}, {"text": "The network data is also directly read from\nand written to GPU memory without passing through host\nmemory.", "metadata": {}}, {"text": "Additionally, we developed an automated model\n2", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=X11.png] Size: 512x512, Data: 28151 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=X11.png] Size: 512x512, Data: 28151 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "X11.png", "image_width": 512, "image_height": 512, "attachment_type": "image", "has_image_data": true, "image_data_size": 28151}}, {"text": "[Image page=2 idx=2 name=X14.png] Size: 504x420, Data: 7905 bytes", "sentences": [{"text": "[Image page=2 idx=2 name=X14.png] Size: 504x420, Data: 7905 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 2, "image_name": "X14.png", "image_width": 504, "image_height": 420, "attachment_type": "image", "has_image_data": true, "image_data_size": 7905}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "converter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 ∼ 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39× on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 ∼ 1024\nl Sequence length. 128 ∼ 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n∑\nj=1\nsoftmax\n\u0012q⊤\ni k j√\nd\n\u0013\nv j, ⋆\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx′\ni = Wproj · fact (Wfc · yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by ⋆ in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3", "sentences": [{"text": "converter.", "metadata": {}}, {"text": "This converter splits the model computation graph\ninto slices, interleaved with attention operators.", "metadata": {}}, {"text": "It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.", "metadata": {}}, {"text": "Moreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa.", "metadata": {}}, {"text": "To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization.", "metadata": {}}, {"text": "With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.", "metadata": {}}, {"text": "To validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation.", "metadata": {}}, {"text": "We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers.", "metadata": {}}, {"text": "Experi-\nmental results that our system can achieve up to16.1 ∼ 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions.", "metadata": {}}, {"text": "Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39× on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.", "metadata": {}}, {"text": "2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM.", "metadata": {}}, {"text": "The specific notations used\nin this analysis are explained in Table 2.", "metadata": {}}, {"text": "Table 2: Notations used in the performance analysis.", "metadata": {}}, {"text": "The\nvalues for LLaMA3-70B are also presented.", "metadata": {}}, {"text": "Parameter Description Typical Value\nN Number of parameters in LLM.", "metadata": {}}, {"text": "70 billion\nd Hidden dimension.", "metadata": {}}, {"text": "8192\nL Layers of the LLM.", "metadata": {}}, {"text": "80\nG GQA group size.", "metadata": {}}, {"text": "8\ne Bytes per element.", "metadata": {}}, {"text": "2\nB Batch size.", "metadata": {}}, {"text": "1 ∼ 1024\nl Sequence length.", "metadata": {}}, {"text": "128 ∼ 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49].", "metadata": {}}, {"text": "In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d.", "metadata": {}}, {"text": "These embeddings then pass through a series of\ntransformer blocks.", "metadata": {}}, {"text": "The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.", "metadata": {}}, {"text": "Within each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates.", "metadata": {}}, {"text": "These vectors are processed through an attention oper-\nator to compute attention scores.", "metadata": {}}, {"text": "The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.", "metadata": {}}, {"text": "qi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n∑\nj=1\nsoftmax\n\u0012q⊤\ni k j√\nd\n\u0013\nv j, ⋆\nyi = Woutai.", "metadata": {}}, {"text": "The output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx′\ni = Wproj · fact (Wfc · yi) .", "metadata": {}}, {"text": "Although the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by ⋆ in the equations) and the other matrix projection\nsteps.", "metadata": {}}, {"text": "Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators.", "metadata": {}}, {"text": "This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.", "metadata": {}}, {"text": "2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46].", "metadata": {}}, {"text": "By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive.", "metadata": {}}, {"text": "For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations.", "metadata": {}}, {"text": "Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "1 10 100 1000\nBatch size\n0.0\n0.1\n0.2Time (s)\n2 x NVIDIA H100\ntime\nMFU\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n4 x NVIDIA H100\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n8 x NVIDIA H100\n0\n1000\n2000\n0\n1000\n2000\n0\n1000\n2000\nMFU (TFlops)\nFigure 2: Measured time consumption and MFU of non-attention operators in LLaMA3-70B during one decode iteration. Results\nwith different tensor parallelisms are presented. The dotted lines indicate the projected values using the roofline model.\nGPU memory. The resulting arithmetic intensity, 2NB\ne(N+2Bd),\nincreases rapidly with larger batch sizes.\nFigure 2 shows the latency and memory throughput uti-\nlization (MFU) of non-attention operators in LLaMA3-70B,\nmeasured on an NVIDIA H100 GPU, alongside projections\nbased on the roofline model. For small batch sizes (less than\n100), the workload is bandwidth-bound, with latency predom-\ninantly caused by accessing model parameters from GPU\nmemory. In this regime, the MFU remains below 20%, indi-\ncating significant underutilization of computational resources.\nAs the batch size increases, the workload transitions to being\ncomputation-bound, with an increase in latency. To optimize\nGPU resource utilization, larger batch sizes are preferred. But,\nachieving this is often constrained by the limited VRAM ca-\npacity, which cannot accommodate the required KV cache\nsize, a limitation discussed in detail later.\n2.2.2 The Underutilization in Attention Operators\nDifferent from the weight matrix projection operators, the\nattention operator, when processing a batch of requestss still\nperforms a batched matrix-vector multiplication, where each\nquery accesses and processes its own KV cache. As a re-\nsult, the arithmetic intensity of the attention operator remains\nconstant, irrespective of the batch size. This behavior makes\nattention operations memory-bound, and increasing the batch\nsize does not improve resource utilization. More recent mod-\nels have adopt grouped-query attention (GQA), which splits\nqi into a group of G independent queries and reduce the size\nof ki and vi by a factor of G. Each query goes through the\nattention computation with the same ki and vi and the outputs\nare simply concatenated. With GQA, the arithmetic intensity\nof attention operators is increased G times, but is still quite\nlow compared with other operators.\nAs shown in Figure 3, the bandwidth utilization of attention\noperators remains above 70% even for small batch sizes, such\nas 20. This holds true even on memory-specialized acceler-\nators like H20, which delivers only 15% of the TFLOPs of\nthe H100. However, the batch size achievable for attention\noperations is constrained by GPU memory capacity, particu-\nlarly due to the high memory demand of KV caches for longer\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H100\ntime\nMBU\n10 20 30\nBatch size\nl=8192 @ NVIDIA H100\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H20\n10 20 30\nBatch size\nl=8192 @ NVIDIA H20\n0\n1000\n2000\n3000\nMBU (GB/s)\n(100%)\n0\n1000\n2000\n3000\n4000\nMBU (GB/s)\n(100%)\nFigure 3: Measured time consumption and model bandwidth\nutilization (MBU) of attention operators in LLaMA3-70B\nduring one decode iteration. Results with different sequence\nlengths and hardware configurations are presented.\ncontext lengths. For example, with a context length of 8192,\nthe full memory of an H100 can only hold KV caches for\nabout 30 requests, with the actual number being lower due to\nmemory used by model weights. Consequently, the limited\nbatch size for attention operations becomes a key bottleneck,\npreventing efficient utilization of computational resources for\nnon-attention operations during the decoding phase.\n3 Model-Attention Disaggregation\n3.1 Overview\nCurrent LLM serving systems often employ the same hard-\nware for both attention and non-attention operators during the\ndecode phase. However, our analysis reveals that this homo-\ngeneous approach leads to suboptimal resource utilization for\nboth types of operators, due to the following reasons:\n• Attention operators demonstrate low arithmetic inten-\nsity, as each value retrieved from the KV cache partici-\npates in only a limited number of computations. Given\n4", "sentences": [{"text": "1 10 100 1000\nBatch size\n0.0\n0.1\n0.2Time (s)\n2 x NVIDIA H100\ntime\nMFU\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n4 x NVIDIA H100\n1 10 100 1000\nBatch size\n0.0\n0.1\n0.2\n8 x NVIDIA H100\n0\n1000\n2000\n0\n1000\n2000\n0\n1000\n2000\nMFU (TFlops)\nFigure 2: Measured time consumption and MFU of non-attention operators in LLaMA3-70B during one decode iteration.", "metadata": {}}, {"text": "Results\nwith different tensor parallelisms are presented.", "metadata": {}}, {"text": "The dotted lines indicate the projected values using the roofline model.", "metadata": {}}, {"text": "GPU memory.", "metadata": {}}, {"text": "The resulting arithmetic intensity, 2NB\ne(N+2Bd),\nincreases rapidly with larger batch sizes.", "metadata": {}}, {"text": "Figure 2 shows the latency and memory throughput uti-\nlization (MFU) of non-attention operators in LLaMA3-70B,\nmeasured on an NVIDIA H100 GPU, alongside projections\nbased on the roofline model.", "metadata": {}}, {"text": "For small batch sizes (less than\n100), the workload is bandwidth-bound, with latency predom-\ninantly caused by accessing model parameters from GPU\nmemory.", "metadata": {}}, {"text": "In this regime, the MFU remains below 20%, indi-\ncating significant underutilization of computational resources.", "metadata": {}}, {"text": "As the batch size increases, the workload transitions to being\ncomputation-bound, with an increase in latency.", "metadata": {}}, {"text": "To optimize\nGPU resource utilization, larger batch sizes are preferred.", "metadata": {}}, {"text": "But,\nachieving this is often constrained by the limited VRAM ca-\npacity, which cannot accommodate the required KV cache\nsize, a limitation discussed in detail later.", "metadata": {}}, {"text": "2.2.2 The Underutilization in Attention Operators\nDifferent from the weight matrix projection operators, the\nattention operator, when processing a batch of requestss still\nperforms a batched matrix-vector multiplication, where each\nquery accesses and processes its own KV cache.", "metadata": {}}, {"text": "As a re-\nsult, the arithmetic intensity of the attention operator remains\nconstant, irrespective of the batch size.", "metadata": {}}, {"text": "This behavior makes\nattention operations memory-bound, and increasing the batch\nsize does not improve resource utilization.", "metadata": {}}, {"text": "More recent mod-\nels have adopt grouped-query attention (GQA), which splits\nqi into a group of G independent queries and reduce the size\nof ki and vi by a factor of G.", "metadata": {}}, {"text": "Each query goes through the\nattention computation with the same ki and vi and the outputs\nare simply concatenated.", "metadata": {}}, {"text": "With GQA, the arithmetic intensity\nof attention operators is increased G times, but is still quite\nlow compared with other operators.", "metadata": {}}, {"text": "As shown in Figure 3, the bandwidth utilization of attention\noperators remains above 70% even for small batch sizes, such\nas 20.", "metadata": {}}, {"text": "This holds true even on memory-specialized acceler-\nators like H20, which delivers only 15% of the TFLOPs of\nthe H100.", "metadata": {}}, {"text": "However, the batch size achievable for attention\noperations is constrained by GPU memory capacity, particu-\nlarly due to the high memory demand of KV caches for longer\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H100\ntime\nMBU\n10 20 30\nBatch size\nl=8192 @ NVIDIA H100\n20 40 60\nBatch size\n0.00\n0.01\n0.02\n0.03Time (s)\nl=4096 @ NVIDIA H20\n10 20 30\nBatch size\nl=8192 @ NVIDIA H20\n0\n1000\n2000\n3000\nMBU (GB/s)\n(100%)\n0\n1000\n2000\n3000\n4000\nMBU (GB/s)\n(100%)\nFigure 3: Measured time consumption and model bandwidth\nutilization (MBU) of attention operators in LLaMA3-70B\nduring one decode iteration.", "metadata": {}}, {"text": "Results with different sequence\nlengths and hardware configurations are presented.", "metadata": {}}, {"text": "context lengths.", "metadata": {}}, {"text": "For example, with a context length of 8192,\nthe full memory of an H100 can only hold KV caches for\nabout 30 requests, with the actual number being lower due to\nmemory used by model weights.", "metadata": {}}, {"text": "Consequently, the limited\nbatch size for attention operations becomes a key bottleneck,\npreventing efficient utilization of computational resources for\nnon-attention operations during the decoding phase.", "metadata": {}}, {"text": "3 Model-Attention Disaggregation\n3.1 Overview\nCurrent LLM serving systems often employ the same hard-\nware for both attention and non-attention operators during the\ndecode phase.", "metadata": {}}, {"text": "However, our analysis reveals that this homo-\ngeneous approach leads to suboptimal resource utilization for\nboth types of operators, due to the following reasons:\n• Attention operators demonstrate low arithmetic inten-\nsity, as each value retrieved from the KV cache partici-\npates in only a limited number of computations.", "metadata": {}}, {"text": "Given\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "the disparity between memory bandwidth and computing\npower in modern high-performance accelerators, which\nfavor high arithmetic intensity for efficient resource uti-\nlization, these operators tend to underutilize the compu-\ntation resources of advanced GPUs.\n• For non-attention operators, while increasing the batch\nsize could potentially enhance hardware utilization, this\nalso results in a corresponding increase in the KV cache,\nwhich may exceed the available memory capacity. Con-\nsequently, to prevent memory overflow, the batch size is\noften kept small, which also leads to inefficient hardware\nutilization because of low arithmetic intensity.\nTo address the above limitations of homogeneous decoding\nsolutions, we propose the model-attention disaggregation\narchitecture, which uses memory-specialized accelerators to\nstore KV caches and compute the attention operators; the non-\nattention operators are still executed on original accelerators.\nA model-attention disaggregation system can use multiple de-\nvices of each kind to provide different degrees of parallelism\n(DOPs). If we use a GPUs for non-attention operators and b\nmemory-optimized GPUs for attention operators, we denote\nthe DOP as (a,b).\nBy leveraging the cheaper memory-optimized devices, we\ncan make larger batch sizes due to the extended memory\ncapacities to store the KV caches, hence increasing the arith-\nmetic intensity and promoting the hardware utilization of\nnon-attention operators. Moreover, as the attention compu-\ntation are moved to memory-optimized devices, we avoid\nwasting precious computation resources of high-end GPUs.\nOne potential obstacle in implementing attention offloading\nlies in the necessity of data transmission between heteroge-\nneous accelerators for each layer of the model, which could\nencounter the communication wall problem and increase the\nend-to-end decoding latency. We conduct a quantitative analy-\nsis to determine the required interconnect bandwidth for such\ntransfers. Say we run one iteration with batch size B, and\nwe can afford α× more latency for the networking overhead,\nthe minimum interconnect bandwidth required can thus be\ncalculated as\nminimum bandwidth =size of data to transmit\nα · computation time\n= (2 + 2/G)edBL\nα[MTIME(B) +ATIME(B,l)]\nwhere MTIME(B) and ATIME(B,l) is running time of non-\nattention and attention operators at batch size B and sequence\nlength l, respectively, and they can be measured experimen-\ntally. The estimated minimum bandwidths required for differ-\nent batch sizes, when α = 0.2, are calculated and presented\nin Figure 4.\nAs evident from the data presented, the required intercon-\nnect bandwidth does not exceed 30 GB/s, even when dealing\n0 50 100\nBatch size\n0\n5\n10\n15Bandwidth (GB/s)\nl=4096\nl=8192\n(a) DOP = (2,2)\n0 100 200\nBatch size\n0\n10\n20Bandwidth (GB/s)\nl=4096\nl=8192 (b) DOP = (2,4)\nFigure 4: The required network bandwidth for decoding\nLLaMA3-70B using attention offloading with H100 and H20,\nwith at most 20% latency slow-down for network overhead.\nwith batch sizes as high as 300. This bandwidth demand\ncan be easily met by networking technologies like 400Gbps\nEthernet. Indeed, contemporary data centers already fulfill\nthis requirement, where each GPU is typically equipped with\nan exclusive 400Gbps NIC to provide sufficient networking\nbandwidth for LLM training.\nFor memory devices, the identical interconnection band-\nwidth is also necessary to communicate with computational\ndevices. Since we employ a collection of more economical\nyet less powerful memory devices to collaboratively com-\npute attention, the communication bandwidth needed for each\nindividual device is significantly smaller. Consequently, we\ncan choose to either equip each device with a less powerful\nNIC or install a single shared 400Gbps NIC to serve multiple\nmemory devices.\n3.2 Practical Challenges\nWhile model-attention disaggregation promises potential ben-\nefits in improving LLM decoding efficiency, it also introduces\na set of formidable practical challenges. We discuss some of\nthese challenges below.\nFrequent network communications. By separating the\nattention operator from computation-optimized devices to\nmemory-optimized devices, we introduce cross-machine data\ncommunications within each model layer. Even though the\ninterconnect bandwidth in existing data centers is sufficient\nfor attention offloading, we found that networking latency\nmight still be a problem for efficient LLM decoding. With\nattention offloading, we have layer-wise data transfer be-\ntween GPUs on different nodes, which may be up to thou-\nsands round-trips per second. These frequent network trans-\nfers might significantly increase the decoding time due to the\naccumulated network latencies. Hence, we need a refurnished,\nlatency-optimized, GPU-aware networking stack for optimal\nperformance of model-attention disaggregation.\nSoftware engineering challenges. With model-attention\ndisaggregation, we are moving the execution of attention\noperator, an intermediate operation of the transformer block,\nto other devices. This requires complicated and destructive\n5", "sentences": [{"text": "the disparity between memory bandwidth and computing\npower in modern high-performance accelerators, which\nfavor high arithmetic intensity for efficient resource uti-\nlization, these operators tend to underutilize the compu-\ntation resources of advanced GPUs.", "metadata": {}}, {"text": "• For non-attention operators, while increasing the batch\nsize could potentially enhance hardware utilization, this\nalso results in a corresponding increase in the KV cache,\nwhich may exceed the available memory capacity.", "metadata": {}}, {"text": "Con-\nsequently, to prevent memory overflow, the batch size is\noften kept small, which also leads to inefficient hardware\nutilization because of low arithmetic intensity.", "metadata": {}}, {"text": "To address the above limitations of homogeneous decoding\nsolutions, we propose the model-attention disaggregation\narchitecture, which uses memory-specialized accelerators to\nstore KV caches and compute the attention operators;", "metadata": {}}, {"text": "the non-\nattention operators are still executed on original accelerators.", "metadata": {}}, {"text": "A model-attention disaggregation system can use multiple de-\nvices of each kind to provide different degrees of parallelism\n(DOPs).", "metadata": {}}, {"text": "If we use a GPUs for non-attention operators and b\nmemory-optimized GPUs for attention operators, we denote\nthe DOP as (a,b).", "metadata": {}}, {"text": "By leveraging the cheaper memory-optimized devices, we\ncan make larger batch sizes due to the extended memory\ncapacities to store the KV caches, hence increasing the arith-\nmetic intensity and promoting the hardware utilization of\nnon-attention operators.", "metadata": {}}, {"text": "Moreover, as the attention compu-\ntation are moved to memory-optimized devices, we avoid\nwasting precious computation resources of high-end GPUs.", "metadata": {}}, {"text": "One potential obstacle in implementing attention offloading\nlies in the necessity of data transmission between heteroge-\nneous accelerators for each layer of the model, which could\nencounter the communication wall problem and increase the\nend-to-end decoding latency.", "metadata": {}}, {"text": "We conduct a quantitative analy-\nsis to determine the required interconnect bandwidth for such\ntransfers.", "metadata": {}}, {"text": "Say we run one iteration with batch size B, and\nwe can afford α× more latency for the networking overhead,\nthe minimum interconnect bandwidth required can thus be\ncalculated as\nminimum bandwidth =size of data to transmit\nα · computation time\n= (2 + 2/G)edBL\nα[MTIME(B) +ATIME(B,l)]\nwhere MTIME(B) and ATIME(B,l) is running time of non-\nattention and attention operators at batch size B and sequence\nlength l, respectively, and they can be measured experimen-\ntally.", "metadata": {}}, {"text": "The estimated minimum bandwidths required for differ-\nent batch sizes, when α = 0.2, are calculated and presented\nin Figure 4.", "metadata": {}}, {"text": "As evident from the data presented, the required intercon-\nnect bandwidth does not exceed 30 GB/s, even when dealing\n0 50 100\nBatch size\n0\n5\n10\n15Bandwidth (GB/s)\nl=4096\nl=8192\n(a) DOP = (2,2)\n0 100 200\nBatch size\n0\n10\n20Bandwidth (GB/s)\nl=4096\nl=8192 (b) DOP = (2,4)\nFigure 4: The required network bandwidth for decoding\nLLaMA3-70B using attention offloading with H100 and H20,\nwith at most 20% latency slow-down for network overhead.", "metadata": {}}, {"text": "with batch sizes as high as 300.", "metadata": {}}, {"text": "This bandwidth demand\ncan be easily met by networking technologies like 400Gbps\nEthernet.", "metadata": {}}, {"text": "Indeed, contemporary data centers already fulfill\nthis requirement, where each GPU is typically equipped with\nan exclusive 400Gbps NIC to provide sufficient networking\nbandwidth for LLM training.", "metadata": {}}, {"text": "For memory devices, the identical interconnection band-\nwidth is also necessary to communicate with computational\ndevices.", "metadata": {}}, {"text": "Since we employ a collection of more economical\nyet less powerful memory devices to collaboratively com-\npute attention, the communication bandwidth needed for each\nindividual device is significantly smaller.", "metadata": {}}, {"text": "Consequently, we\ncan choose to either equip each device with a less powerful\nNIC or install a single shared 400Gbps NIC to serve multiple\nmemory devices.", "metadata": {}}, {"text": "3.2 Practical Challenges\nWhile model-attention disaggregation promises potential ben-\nefits in improving LLM decoding efficiency, it also introduces\na set of formidable practical challenges.", "metadata": {}}, {"text": "We discuss some of\nthese challenges below.", "metadata": {}}, {"text": "Frequent network communications.", "metadata": {}}, {"text": "By separating the\nattention operator from computation-optimized devices to\nmemory-optimized devices, we introduce cross-machine data\ncommunications within each model layer.", "metadata": {}}, {"text": "Even though the\ninterconnect bandwidth in existing data centers is sufficient\nfor attention offloading, we found that networking latency\nmight still be a problem for efficient LLM decoding.", "metadata": {}}, {"text": "With\nattention offloading, we have layer-wise data transfer be-\ntween GPUs on different nodes, which may be up to thou-\nsands round-trips per second.", "metadata": {}}, {"text": "These frequent network trans-\nfers might significantly increase the decoding time due to the\naccumulated network latencies.", "metadata": {}}, {"text": "Hence, we need a refurnished,\nlatency-optimized, GPU-aware networking stack for optimal\nperformance of model-attention disaggregation.", "metadata": {}}, {"text": "Software engineering challenges.", "metadata": {}}, {"text": "With model-attention\ndisaggregation, we are moving the execution of attention\noperator, an intermediate operation of the transformer block,\nto other devices.", "metadata": {}}, {"text": "This requires complicated and destructive\n5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "modifications to the existing LLM codebase. Specifically, we\nhave to dissect the models into separate slices that do not align\nwith the modular structure of the transformer-based LLMs.\nThis process is not only labor-intensive and error-prone but\nalso significantly increases maintenance complexity. Hence,\nautomated tools to help slice the models and perform relevant\noptimizations are highly desirable.\nDifficult execution overlapping. In a heterogeneous disag-\ngregated system, various devices such as compute-optimized\nGPUs, memory-optimized GPUs, and NICs can be utilized\nsimultaneously. Hence, we might achieve significant execu-\ntion time reduction if the execution of operations occupying\ndifferent devices could be overlapped. However, in the trans-\nformers architectures of current LLMs, attention operators\nand model operators are tightly interleaved in a sequential\nmanner, with the output of one operator being transmitted\nover the network for the input of the other. Consequently,\noperations that depend on distinct hardware resources cannot\nbe effectively overlapped in time, leading to considerable re-\nsource underutilization. Therefore, careful orchestration of\noperations on various devices and efficient design of task\npipelines are required to promote execution overlapping and\nincrease resource utilization.\n4 System Design\nWe build Lamina, a distributed heterogeneous LLM decoding\nsystem that implements model-attention disaggregation and\nsolves the related challenges. Lamina employs two kinds of\nacceleration devices: memory devices are used for storing KV\ncache and computing the attention operator, and computation\ndevices are used for storing model parameters and computing\nother parts of the model. These two kinds of devices are inter-\nconnected with high-speed DCN, e.g., Infiniband or Ethernet.\n4.1 Fully Host-Bypassed Network Stack\nThe communication between GPUs across different nodes,\noften utilizing RDMA technologies, is a complex process that\nrequires the coordination of multiple system agents, including\nthe CPU, GPU, and NIC. To reduce GPU-aware networking\noverhead, GPUDirect RDMA (GDR) [3] is developed to al-\nlow the RDMA-capable NIC (RNIC) to directly access GPU\nmemory. This eliminates the need for host memory as an in-\ntermediate buffer, thereby enhancing both network latency\nand bandwidth. However, the control path still requires CPU\ninvolvement and includes several steps, all of which lie on\nthe critical path and contribute to network latency. Specifi-\ncally, when transferring data using GPUDirect RDMA, the\nfollowing steps are performed:\n1. The local CPU waits for all prior GPU kernels to com-\nplete, ensuring the data to be transmitted is ready.\n2. The local CPU submits a send work request (WR) to the\nRNIC.\n3. The local RNIC processes the send WR, fetching the data\nfrom GPU memory and transmitting it over the physical\nnetwork link.\n4. The remote RNIC receives the data and writes it to the\nGPU memory.\n5. The remote CPU waits for the RDMA receive operation\nto complete.\n6. The remote CPU launches the subsequent GPU kernels.\nBased on our experimental results, steps 1 through 5 may\nincur a latency of 60–70 µs. Furthermore, because we have to\nlaunch the kernel after the received data is ready, the GPU ker-\nnel launch overhead, which might be up to20 µs, is also added\nto end-to-end latency. All these additional latencies pose a sig-\nnificant overhead for model-attention disaggregation, which\nmust rely on frequent network communications.\nTo reduce such networking overhead, we develop a fully\nhost-bypassed network (FHBN) stack, which completely\neliminates host CPU involvement in both control and data\npaths of GPU-aware networking. We describe how FHBN\nperforms send and recv operations below.\nFHBN recv. To implement the FHBN recv function, we\nemploy the device-side polling technique to await the comple-\ntion of the recv operation. Specifically, we allocate aseqno\nvariable on the receiver’s GPU memory. The sender incre-\nments the remote seqno with RDMA write after each send\noperation. The data send and seqno increment operations are\nbatched in a single WR post and hence would not increase\nthe end-to-end latency. When the receiver GPU is ready to\nreceive and process the incoming data, it actively polls the\nvalue of seqno with a specialized GPU kernel. This approach\nnot only eliminates the need for CPU involvement during\nthe recv process, but also allows asynchronous launch of the\npolling kernel and subsequent computation kernels to the\nGPU stream. Therefore, the GPU kernel launch overhead is\nalso removed from the critical path.\nFHBN send. The implementation of FHBN send, illustrated\nin Figure 5, is more involved as it necessitates the GPU to\ndirectly submit RDMA commands to RNIC. When the CPU\nsubmits a new RDMA WR to RNIC, it first enqueues the WR\nto the work queue (WQ) in the host memory. Then, it tells the\nRNIC that there is outstanding work by ringing the doorbell\n(DB), a special register in the user access region (UAR) of\nthe RNIC. The UAR is part of the RNIC’s mmio region and is\nmapped to the address space of unprivileged applications to\nallow kernel-bypass RDMA operations. All above steps are\nimplemented in the RDMA userspace library (libibverbs).\nTo enable direct RDMA command submission on GPUs,\nwe have to allow GPUs to directly access the UAR via PCIe\nP2P. Specifically, we use thecudaHostRegisterIoMemory\nAPI to map the UAR into the GPU’s address space. Then,\n6", "sentences": [{"text": "modifications to the existing LLM codebase.", "metadata": {}}, {"text": "Specifically, we\nhave to dissect the models into separate slices that do not align\nwith the modular structure of the transformer-based LLMs.", "metadata": {}}, {"text": "This process is not only labor-intensive and error-prone but\nalso significantly increases maintenance complexity.", "metadata": {}}, {"text": "Hence,\nautomated tools to help slice the models and perform relevant\noptimizations are highly desirable.", "metadata": {}}, {"text": "Difficult execution overlapping.", "metadata": {}}, {"text": "In a heterogeneous disag-\ngregated system, various devices such as compute-optimized\nGPUs, memory-optimized GPUs, and NICs can be utilized\nsimultaneously.", "metadata": {}}, {"text": "Hence, we might achieve significant execu-\ntion time reduction if the execution of operations occupying\ndifferent devices could be overlapped.", "metadata": {}}, {"text": "However, in the trans-\nformers architectures of current LLMs, attention operators\nand model operators are tightly interleaved in a sequential\nmanner, with the output of one operator being transmitted\nover the network for the input of the other.", "metadata": {}}, {"text": "Consequently,\noperations that depend on distinct hardware resources cannot\nbe effectively overlapped in time, leading to considerable re-\nsource underutilization.", "metadata": {}}, {"text": "Therefore, careful orchestration of\noperations on various devices and efficient design of task\npipelines are required to promote execution overlapping and\nincrease resource utilization.", "metadata": {}}, {"text": "4 System Design\nWe build Lamina, a distributed heterogeneous LLM decoding\nsystem that implements model-attention disaggregation and\nsolves the related challenges.", "metadata": {}}, {"text": "Lamina employs two kinds of\nacceleration devices: memory devices are used for storing KV\ncache and computing the attention operator, and computation\ndevices are used for storing model parameters and computing\nother parts of the model.", "metadata": {}}, {"text": "These two kinds of devices are inter-\nconnected with high-speed DCN, e.g., Infiniband or Ethernet.", "metadata": {}}, {"text": "4.1 Fully Host-Bypassed Network Stack\nThe communication between GPUs across different nodes,\noften utilizing RDMA technologies, is a complex process that\nrequires the coordination of multiple system agents, including\nthe CPU, GPU, and NIC.", "metadata": {}}, {"text": "To reduce GPU-aware networking\noverhead, GPUDirect RDMA (GDR) [3] is developed to al-\nlow the RDMA-capable NIC (RNIC) to directly access GPU\nmemory.", "metadata": {}}, {"text": "This eliminates the need for host memory as an in-\ntermediate buffer, thereby enhancing both network latency\nand bandwidth.", "metadata": {}}, {"text": "However, the control path still requires CPU\ninvolvement and includes several steps, all of which lie on\nthe critical path and contribute to network latency.", "metadata": {}}, {"text": "Specifi-\ncally, when transferring data using GPUDirect RDMA, the\nfollowing steps are performed:\n1.", "metadata": {}}, {"text": "The local CPU waits for all prior GPU kernels to com-\nplete, ensuring the data to be transmitted is ready.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "The local CPU submits a send work request (WR) to the\nRNIC.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "The local RNIC processes the send WR, fetching the data\nfrom GPU memory and transmitting it over the physical\nnetwork link.", "metadata": {}}, {"text": "4.", "metadata": {}}, {"text": "The remote RNIC receives the data and writes it to the\nGPU memory.", "metadata": {}}, {"text": "5.", "metadata": {}}, {"text": "The remote CPU waits for the RDMA receive operation\nto complete.", "metadata": {}}, {"text": "6.", "metadata": {}}, {"text": "The remote CPU launches the subsequent GPU kernels.", "metadata": {}}, {"text": "Based on our experimental results, steps 1 through 5 may\nincur a latency of 60–70 µs.", "metadata": {}}, {"text": "Furthermore, because we have to\nlaunch the kernel after the received data is ready, the GPU ker-\nnel launch overhead, which might be up to20 µs, is also added\nto end-to-end latency.", "metadata": {}}, {"text": "All these additional latencies pose a sig-\nnificant overhead for model-attention disaggregation, which\nmust rely on frequent network communications.", "metadata": {}}, {"text": "To reduce such networking overhead, we develop a fully\nhost-bypassed network (FHBN) stack, which completely\neliminates host CPU involvement in both control and data\npaths of GPU-aware networking.", "metadata": {}}, {"text": "We describe how FHBN\nperforms send and recv operations below.", "metadata": {}}, {"text": "FHBN recv.", "metadata": {}}, {"text": "To implement the FHBN recv function, we\nemploy the device-side polling technique to await the comple-\ntion of the recv operation.", "metadata": {}}, {"text": "Specifically, we allocate aseqno\nvariable on the receiver’s GPU memory.", "metadata": {}}, {"text": "The sender incre-\nments the remote seqno with RDMA write after each send\noperation.", "metadata": {}}, {"text": "The data send and seqno increment operations are\nbatched in a single WR post and hence would not increase\nthe end-to-end latency.", "metadata": {}}, {"text": "When the receiver GPU is ready to\nreceive and process the incoming data, it actively polls the\nvalue of seqno with a specialized GPU kernel.", "metadata": {}}, {"text": "This approach\nnot only eliminates the need for CPU involvement during\nthe recv process, but also allows asynchronous launch of the\npolling kernel and subsequent computation kernels to the\nGPU stream.", "metadata": {}}, {"text": "Therefore, the GPU kernel launch overhead is\nalso removed from the critical path.", "metadata": {}}, {"text": "FHBN send.", "metadata": {}}, {"text": "The implementation of FHBN send, illustrated\nin Figure 5, is more involved as it necessitates the GPU to\ndirectly submit RDMA commands to RNIC.", "metadata": {}}, {"text": "When the CPU\nsubmits a new RDMA WR to RNIC, it first enqueues the WR\nto the work queue (WQ) in the host memory.", "metadata": {}}, {"text": "Then, it tells the\nRNIC that there is outstanding work by ringing the doorbell\n(DB), a special register in the user access region (UAR) of\nthe RNIC.", "metadata": {}}, {"text": "The UAR is part of the RNIC’s mmio region and is\nmapped to the address space of unprivileged applications to\nallow kernel-bypass RDMA operations.", "metadata": {}}, {"text": "All above steps are\nimplemented in the RDMA userspace library (libibverbs).", "metadata": {}}, {"text": "To enable direct RDMA command submission on GPUs,\nwe have to allow GPUs to directly access the UAR via PCIe\nP2P.", "metadata": {}}, {"text": "Specifically, we use thecudaHostRegisterIoMemory\nAPI to map the UAR into the GPU’s address space.", "metadata": {}}, {"text": "Then,\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "Host CPU\nCPU Core\nRNIC\nGPU PCIe Switch\ncudaDeviceSync\n(CUDA Driver)\nibv_post_send\n(libibverbs)\nHost DRAM\nWQ\nCQ\nRDMA Send Queue Regular GPU-\naware send\nFHBN send\nUser Access Region\nBlueFlameDB\nFigure 5: Diagram of WR submission with FHBN send and\nconventional GPU-aware send implementations.\nwe reimplement the RDMA command submission logic in\nCUDA device code. To further decrease latency, we leverage\nthe BlueFlame mechanism, a hardware feature provided by\nMellanox RNICs [4]. This approach allows the WR to be\ndirectly submitted to the RNIC with mmio write to UAR,\neliminating the need for the RNIC to fetch the WR from host\nmemory via an expensive PCIe DMA read. Note that the WR\nshould still be enqueued into the WQ, as the hardware may\noccasionally miss the BlueFlame WR and fall back to the\nregular workflow, particularly under heavy loads.\n4.2 Automated Model Converter\n4.2.1 Model Splitting\nIn the attention offloading architecture, different operators\nof the LLM might be executed on different hardware; hence,\nwe need to partition the model into slices, which is achieved\nby cutting at the attention operators. It often involves signifi-\ncant modifications to the existing codebase, primarily because\nthe desired cutting points do not align with the LLM’s inher-\nent modular structure. This misalignment complicates the\npartitioning process and increases the risk of errors and incon-\nsistencies within the heterogeneous system.\nTo facilitate model partitioning, we develop an automated\nmodel splitter capable of transforming the LLM into individ-\nually invokable slices, illustrated in Figure 6. Given the LLM\nsource code, the splitter uses symbolic execution to generate\na weighted computation graph. The weight of each edge de-\nnotes the size of the data passed between the operators, which\nis derived from the model’s shape specification.\nDue to the presence of residual connections and other in-\ntricate model constructs, directly removing the attention op-\nerator does not always result in a disconnected computation\ngraph. Therefore, we compute the minimum weighted cut of\nAttention\n10240\nMLP\n10240\n10240\n10240+\n10240\n30720\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN1\n10240\n10240\nLN2\nEmbd\n4\ninput\nmin cutSlice 1 Slice 2\n10240\nMLP\n10240\n+\n10240\nLN1\n10240\nAttention\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN2\n24576\n10240\n10240\nmin cut Slice 3\n...\nFigure 6: The partitioned computation graph of an LLM.\nthe remaining graph, from the input to the output of the atten-\ntion operator. The edges in this minimum cut, representing\nthe context that must be saved between slice invocations, are\nremoved from the computation graphs. This process is itera-\ntively applied to each attention operator, ultimately yielding\nn + 1 model slices, where n denotes the original number of\nthe attention operators.\n4.2.2 Resource Utilization Overlapping\nWhile the attention operators and other operators in a trans-\nformer block are executed sequentially, a closer examination\nof the attention computation reveals the potential for achiev-\ning partial overlapping of resource utilization. Given an at-\ntention query q and the set of token indices I, the attention\ncomputation can be carried out in a divide-and-conquer man-\nner. Assume that I can be written as the disjoint union of two\nsubsets I1 and I2, and let\nAq(I) = ∑\ni∈I\nsoftmax\n\u0012q⊤ki√\nd\n\u0013\nvi,\nSq(I) = ∑\ni∈I\nexp\n\u0012q⊤ki√\nd\n\u0013\n,\nwhere Aq(I) is the attention output and Sq(I) is the de-\nnominator of softmax, then Aq(I) can be easily obtained\nby combining the partial attention results on I1 and I2, i.e.,\n[Aq(I1),Sq(I1)] and [Aq(I2),Sq(I2)]:\nAq(I) = Aq(I1)Sq(I1) +Aq(I2)Sq(I2)\nSq(I1) +Sq(I2) .\nDuring LLM decoding, we may divide the current token set\ninto two partitions during attention computation: all previous\ntokens (prev) and the newly generated token (new). Note that\n[Aq(prev),Sq(prev)] can be computed as soon as qn is ready;\ntherefore, we may eagerly execute Q-Proj and transfer qn, and\nthen execute K-Proj, V-Proj and transfer kn,vn to the attention\n7", "sentences": [{"text": "Host CPU\nCPU Core\nRNIC\nGPU PCIe Switch\ncudaDeviceSync\n(CUDA Driver)\nibv_post_send\n(libibverbs)\nHost DRAM\nWQ\nCQ\nRDMA Send Queue Regular GPU-\naware send\nFHBN send\nUser Access Region\nBlueFlameDB\nFigure 5: Diagram of WR submission with FHBN send and\nconventional GPU-aware send implementations.", "metadata": {}}, {"text": "we reimplement the RDMA command submission logic in\nCUDA device code.", "metadata": {}}, {"text": "To further decrease latency, we leverage\nthe BlueFlame mechanism, a hardware feature provided by\nMellanox RNICs [4].", "metadata": {}}, {"text": "This approach allows the WR to be\ndirectly submitted to the RNIC with mmio write to UAR,\neliminating the need for the RNIC to fetch the WR from host\nmemory via an expensive PCIe DMA read.", "metadata": {}}, {"text": "Note that the WR\nshould still be enqueued into the WQ, as the hardware may\noccasionally miss the BlueFlame WR and fall back to the\nregular workflow, particularly under heavy loads.", "metadata": {}}, {"text": "4.2 Automated Model Converter\n4.2.1 Model Splitting\nIn the attention offloading architecture, different operators\nof the LLM might be executed on different hardware;", "metadata": {}}, {"text": "hence,\nwe need to partition the model into slices, which is achieved\nby cutting at the attention operators.", "metadata": {}}, {"text": "It often involves signifi-\ncant modifications to the existing codebase, primarily because\nthe desired cutting points do not align with the LLM’s inher-\nent modular structure.", "metadata": {}}, {"text": "This misalignment complicates the\npartitioning process and increases the risk of errors and incon-\nsistencies within the heterogeneous system.", "metadata": {}}, {"text": "To facilitate model partitioning, we develop an automated\nmodel splitter capable of transforming the LLM into individ-\nually invokable slices, illustrated in Figure 6.", "metadata": {}}, {"text": "Given the LLM\nsource code, the splitter uses symbolic execution to generate\na weighted computation graph.", "metadata": {}}, {"text": "The weight of each edge de-\nnotes the size of the data passed between the operators, which\nis derived from the model’s shape specification.", "metadata": {}}, {"text": "Due to the presence of residual connections and other in-\ntricate model constructs, directly removing the attention op-\nerator does not always result in a disconnected computation\ngraph.", "metadata": {}}, {"text": "Therefore, we compute the minimum weighted cut of\nAttention\n10240\nMLP\n10240\n10240\n10240+\n10240\n30720\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN1\n10240\n10240\nLN2\nEmbd\n4\ninput\nmin cutSlice 1 Slice 2\n10240\nMLP\n10240\n+\n10240\nLN1\n10240\nAttention\nLPqkv\n10240\nLPqkv\n10240+\n10240\nLN2\n24576\n10240\n10240\nmin cut Slice 3\n...", "metadata": {}}, {"text": "Figure 6: The partitioned computation graph of an LLM.", "metadata": {}}, {"text": "the remaining graph, from the input to the output of the atten-\ntion operator.", "metadata": {}}, {"text": "The edges in this minimum cut, representing\nthe context that must be saved between slice invocations, are\nremoved from the computation graphs.", "metadata": {}}, {"text": "This process is itera-\ntively applied to each attention operator, ultimately yielding\nn + 1 model slices, where n denotes the original number of\nthe attention operators.", "metadata": {}}, {"text": "4.2.2 Resource Utilization Overlapping\nWhile the attention operators and other operators in a trans-\nformer block are executed sequentially, a closer examination\nof the attention computation reveals the potential for achiev-\ning partial overlapping of resource utilization.", "metadata": {}}, {"text": "Given an at-\ntention query q and the set of token indices I, the attention\ncomputation can be carried out in a divide-and-conquer man-\nner.", "metadata": {}}, {"text": "Assume that I can be written as the disjoint union of two\nsubsets I1 and I2, and let\nAq(I) = ∑\ni∈I\nsoftmax\n\u0012q⊤ki√\nd\n\u0013\nvi,\nSq(I) = ∑\ni∈I\nexp\n\u0012q⊤ki√\nd\n\u0013\n,\nwhere Aq(I) is the attention output and Sq(I) is the de-\nnominator of softmax, then Aq(I) can be easily obtained\nby combining the partial attention results on I1 and I2, i.e.,\n[Aq(I1),Sq(I1)] and [Aq(I2),Sq(I2)]:\nAq(I) = Aq(I1)Sq(I1) +Aq(I2)Sq(I2)\nSq(I1) +Sq(I2) .", "metadata": {}}, {"text": "During LLM decoding, we may divide the current token set\ninto two partitions during attention computation: all previous\ntokens (prev) and the newly generated token (new).", "metadata": {}}, {"text": "Note that\n[Aq(prev),Sq(prev)] can be computed as soon as qn is ready;", "metadata": {}}, {"text": "therefore, we may eagerly execute Q-Proj and transfer qn, and\nthen execute K-Proj, V-Proj and transfer kn,vn to the attention\n7", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=X12.png] Size: 193x92, Data: 6377 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=X12.png] Size: 193x92, Data: 6377 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "X12.png", "image_width": 193, "image_height": 92, "attachment_type": "image", "has_image_data": true, "image_data_size": 6377}}, {"text": "[Image page=7 idx=2 name=X8.png] Size: 512x512, Data: 28151 bytes", "sentences": [{"text": "[Image page=7 idx=2 name=X8.png] Size: 512x512, Data: 28151 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 2, "image_name": "X8.png", "image_width": 512, "image_height": 512, "attachment_type": "image", "has_image_data": true, "image_data_size": 28151}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "workers. As illustrated in Figure 7, this does not only improve\nthe GPU utilization on both kinds of workers, but also reduces\nthe end-to-end latency by hiding the communication behind\nthe computation.\nQ-Proj K-Proj V-Proj\nAttn\nOut-Proj\nQKV Attn Out\nModel\nWorkers\nAttention\nWorkers\n(a) Without resource utilization overlapping.\nQ-Proj K-Proj V-Proj\nPrev Attn\nOut-Proj\nQ Attn Out\nModel\nWorkers\nAttention\nWorkers\nNew Attn\n& Combine\nKV\n(b) With resource utilization overlapping.\nFigure 7: Illustration of resource utilization overlapping by\nsplitting the attention computation.\nThe above attention splitting optimization is integrated in\nour automated model converter. After dissecting the original\nmodel, the converter will generate a serial program of each\nmodel slice by computing a topological order of its compu-\ntation graph. During this topological sort, we always put the\nQ-Proj operator and all its dependencies as early as possible.\nThen, we insert the “send Q” instruction immediately after\nthe Q-Proj operator and “send KV” at the end of this slice.\n4.3 Execution Pipelining\nDue to the serial nature of transformer-based models, if there\nis only one batch under processing, the memory device is idle\nwhen the computation device is working, and vice versa. To\naddress this resource underutilization problem and increase\nsystem throughput, we may run multiple batches concurrently\nin a pipelined fashion. With properly designed pipelining,\nbetter hardware utilization can be achieved without sacrificing\nlatency. We propose the rotational staggered pipelining to\nsolve this problem.\nA2B2C2D2\nA1 D1\nB1\nB3C1\nC3\nA4B4C4\nModel 1\nModel 2\nModel 3\nAttention\nA3 D3\nD5\nC5\nB5\nD4A6\nA5\nB6C6D6\nFigure 8: Illustration of rotational staggered pipelining.\nAssume that we execute n batches concurrently. Let tm,ta\nrepresent the time required for executing one model slice and\none attention operator, respectively. As illustrated in Figure 8,\nwe deploy n − 1 model replicas, with each replica starting its\ntasks at a time of tm\nn−1 later than the previous one. All batches\nshare a common set of memory devices to maximize aggre-\ngated memory bandwidth and improve memory utilization.\nFor every batch, the KV cache is evenly partitioned across\nthese devices. All memory devices jointly compute the at-\ntention operator for a single batch. The number of memory\ndevices is selected to maketa = tm\nn−1. After the attention opera-\ntor, each batch transitions to the next model replica according\nto a rotational schedule; that is, the kth model slice of the jth\nbatch is executed on replica ( j + k) mod (n − 1) +1.\nThis rotational task scheduling, combined with the stag-\ngered execution intervals, guarantees seamless task transitions\nfor each batch and ensures a conflict- and bubble-free work-\nflow on each device. Furthermore, by increasing the num-\nber of concurrent batches, the overall inference latency can\nbe reduced due to the decreased attention computation time.\nHowever, the rotational scheduling requires migrating batch\nexecution contexts between computation devices. Note that\nwhen n = 2, the context migration is unnecessary because\nboth batches are executed within a single model replica.\n5 Implementation\nLamina is implemented with ~6000 lines of Python and C/C++\ncode, in addition to a few lines of CUDA code implementing\ncustom kernels. The fully host-bypassed network stack is built\non top of a modified version of rdma-core [6]. Lamina uses\nRay [5] to facilitate task scheduling and worker placement in\ndistributed heterogeneous environments.\nFault tolerance. With attention-offloading, we have two\ndifferent types of accelerators. Lamina addresses faults in\nthese two types of accelerators with different approaches.\nNote that all request states, i.e., the KV caches, are only stored\nin the attention devices. Consequently, should any model\nworker experience a failure, we can seamlessly replace that\nworker with a functioning one, without losing any progresses.\nIn case of an attention worker failure, we reconstruct the KV\ncache by using the prompt texts and already generated tokens,\nwhich are stored in the LLM service front-end.\nHandling the prefill-decode transition. During the prefill\nphase, the generated KV cache shall be transmitted to the\nattention workers for decoding. For each request, the global\nscheduler picks a set of model workers and attention workers\nto handle the decode phase. Like previous works [40, 59], the\nKV cache is asynchronously transferred in a layer-by-layer\nfashion to hide the communication latency behind computa-\ntion. Moreover, the data transfer is controlled by the attention\nworkers: the attention workers only reads the KV cache from\nprefill workers during the free periods between receiving\nQKV tensors from model workers. This approach minimizes\ninterference with ongoing decoding tasks.\nAttention parallelism. Given the limited capability of a\nsingle device, we may use multiple memory devices to jointly\n8", "sentences": [{"text": "workers.", "metadata": {}}, {"text": "As illustrated in Figure 7, this does not only improve\nthe GPU utilization on both kinds of workers, but also reduces\nthe end-to-end latency by hiding the communication behind\nthe computation.", "metadata": {}}, {"text": "Q-Proj K-Proj V-Proj\nAttn\nOut-Proj\nQKV Attn Out\nModel\nWorkers\nAttention\nWorkers\n(a) Without resource utilization overlapping.", "metadata": {}}, {"text": "Q-Proj K-Proj V-Proj\nPrev Attn\nOut-Proj\nQ Attn Out\nModel\nWorkers\nAttention\nWorkers\nNew Attn\n& Combine\nKV\n(b) With resource utilization overlapping.", "metadata": {}}, {"text": "Figure 7: Illustration of resource utilization overlapping by\nsplitting the attention computation.", "metadata": {}}, {"text": "The above attention splitting optimization is integrated in\nour automated model converter.", "metadata": {}}, {"text": "After dissecting the original\nmodel, the converter will generate a serial program of each\nmodel slice by computing a topological order of its compu-\ntation graph.", "metadata": {}}, {"text": "During this topological sort, we always put the\nQ-Proj operator and all its dependencies as early as possible.", "metadata": {}}, {"text": "Then, we insert the “send Q” instruction immediately after\nthe Q-Proj operator and “send KV” at the end of this slice.", "metadata": {}}, {"text": "4.3 Execution Pipelining\nDue to the serial nature of transformer-based models, if there\nis only one batch under processing, the memory device is idle\nwhen the computation device is working, and vice versa.", "metadata": {}}, {"text": "To\naddress this resource underutilization problem and increase\nsystem throughput, we may run multiple batches concurrently\nin a pipelined fashion.", "metadata": {}}, {"text": "With properly designed pipelining,\nbetter hardware utilization can be achieved without sacrificing\nlatency.", "metadata": {}}, {"text": "We propose the rotational staggered pipelining to\nsolve this problem.", "metadata": {}}, {"text": "A2B2C2D2\nA1 D1\nB1\nB3C1\nC3\nA4B4C4\nModel 1\nModel 2\nModel 3\nAttention\nA3 D3\nD5\nC5\nB5\nD4A6\nA5\nB6C6D6\nFigure 8: Illustration of rotational staggered pipelining.", "metadata": {}}, {"text": "Assume that we execute n batches concurrently.", "metadata": {}}, {"text": "Let tm,ta\nrepresent the time required for executing one model slice and\none attention operator, respectively.", "metadata": {}}, {"text": "As illustrated in Figure 8,\nwe deploy n − 1 model replicas, with each replica starting its\ntasks at a time of tm\nn−1 later than the previous one.", "metadata": {}}, {"text": "All batches\nshare a common set of memory devices to maximize aggre-\ngated memory bandwidth and improve memory utilization.", "metadata": {}}, {"text": "For every batch, the KV cache is evenly partitioned across\nthese devices.", "metadata": {}}, {"text": "All memory devices jointly compute the at-\ntention operator for a single batch.", "metadata": {}}, {"text": "The number of memory\ndevices is selected to maketa = tm\nn−1.", "metadata": {}}, {"text": "After the attention opera-\ntor, each batch transitions to the next model replica according\nto a rotational schedule;", "metadata": {}}, {"text": "that is, the kth model slice of the jth\nbatch is executed on replica ( j + k) mod (n − 1) +1.", "metadata": {}}, {"text": "This rotational task scheduling, combined with the stag-\ngered execution intervals, guarantees seamless task transitions\nfor each batch and ensures a conflict- and bubble-free work-\nflow on each device.", "metadata": {}}, {"text": "Furthermore, by increasing the num-\nber of concurrent batches, the overall inference latency can\nbe reduced due to the decreased attention computation time.", "metadata": {}}, {"text": "However, the rotational scheduling requires migrating batch\nexecution contexts between computation devices.", "metadata": {}}, {"text": "Note that\nwhen n = 2, the context migration is unnecessary because\nboth batches are executed within a single model replica.", "metadata": {}}, {"text": "5 Implementation\nLamina is implemented with ~6000 lines of Python and C/C++\ncode, in addition to a few lines of CUDA code implementing\ncustom kernels.", "metadata": {}}, {"text": "The fully host-bypassed network stack is built\non top of a modified version of rdma-core [6].", "metadata": {}}, {"text": "Lamina uses\nRay [5] to facilitate task scheduling and worker placement in\ndistributed heterogeneous environments.", "metadata": {}}, {"text": "Fault tolerance.", "metadata": {}}, {"text": "With attention-offloading, we have two\ndifferent types of accelerators.", "metadata": {}}, {"text": "Lamina addresses faults in\nthese two types of accelerators with different approaches.", "metadata": {}}, {"text": "Note that all request states, i.e., the KV caches, are only stored\nin the attention devices.", "metadata": {}}, {"text": "Consequently, should any model\nworker experience a failure, we can seamlessly replace that\nworker with a functioning one, without losing any progresses.", "metadata": {}}, {"text": "In case of an attention worker failure, we reconstruct the KV\ncache by using the prompt texts and already generated tokens,\nwhich are stored in the LLM service front-end.", "metadata": {}}, {"text": "Handling the prefill-decode transition.", "metadata": {}}, {"text": "During the prefill\nphase, the generated KV cache shall be transmitted to the\nattention workers for decoding.", "metadata": {}}, {"text": "For each request, the global\nscheduler picks a set of model workers and attention workers\nto handle the decode phase.", "metadata": {}}, {"text": "Like previous works [40, 59], the\nKV cache is asynchronously transferred in a layer-by-layer\nfashion to hide the communication latency behind computa-\ntion.", "metadata": {}}, {"text": "Moreover, the data transfer is controlled by the attention\nworkers: the attention workers only reads the KV cache from\nprefill workers during the free periods between receiving\nQKV tensors from model workers.", "metadata": {}}, {"text": "This approach minimizes\ninterference with ongoing decoding tasks.", "metadata": {}}, {"text": "Attention parallelism.", "metadata": {}}, {"text": "Given the limited capability of a\nsingle device, we may use multiple memory devices to jointly\n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "Request 1\nRequest 2\nRequest 3\nRequest 4\n(a) Request-level partition.\nRequest 1\nRequest 2\nRequest 3\nRequest 4\ntoken\nhead\ndevice 1\ndevice 2 (b) Head-level partition.\nFigure 9: Work partition methods of the attention operator.\nstore the KV caches and compute the attention operators. As\ndepicted in Figure 9, the attention operators can be paral-\nlelized among memory devices in various ways. One method\nis to distribute different requests across different devices; an\nalternative strategy is to partition and distribute the attention\nheads, which can also be computed independently, to differ-\nent devices. The head-level partitioning approach ensures\na balanced workload distribution, whereas the request-level\npartitioning may result in load imbalance due to the differ-\nences in sequence lengths and therefore the KV cache sizes\namong requests. However, head-level partitioning has limited\nflexibility, as it requires the number of memory devices to\nbe divisible by the number of attention heads. We opt for\nhead-level partitioning in Lamina, which offers optimal load\nbalancing.\n6 Evaluation\nTestbed. We deploy Lamina on a real heterogeneous cluster\nwith two kinds of GPU nodes. Each node consists of either\neight H100 or H20 GPUs, and each GPU is paired with a ded-\nicated ConnectX-7 NIC via PCIe switch. The GPU nodes are\ninterconnected with 400 Gbps RoCE network. We use H100\nas compute-optimized GPUs and H20 as memory-optimized\nGPUs for Lamina.\nModels. Lamina supports a wide variety of LLM architec-\ntures, including OPT [58], LLaMA [48], and LLaMA3 [9]. All\nthese architectures have similar outlines and workload charac-\nteristics and only have minor differences irrelevant to system\ndesigns. Hence, as listed in Table 3, we choose LLaMA-33B,\nLLaMA-65B, and LLaMA3-70B for evaluations. All model\nparameters and KV caches are stored in FP16 format.\nWorkloads To mirror the real-world LLM use cases, we\nuse four request traces collected from the production systems\nof two LLM service providers, Azure [1, 40] and Kimi [41].\nDue to data protection regulations, these traces only con-\ntain the sequence length of user requests but not the actual\ncontents. Hence, we use requests of dummy tokens with the\nsame sequence length for evaluation. The summaries of these\nTable 3: Large language models used for evaluation.\nModel Parameters L d G\nLLaMA-33B 64.7 GB 60 6656 1\nLLaMA-65B 130.1 GB 80 8192 1\nLLaMA3-70B 137.5 GB 80 8192 8\ntraces, including the average prompt tokens (lp) and average\ngenerated tokens (lg), are listed in Table 4.\nTable 4: Request traces used for evaluation.\nTrace # Requests lp lg\nAzure-Conv 19366 1154.7 211.1\nAzure-Code 8819 2047.8 27.9\nKimi-Conv 12031 12035.1 342.6\nKimi-TA 23608 8560.0 182.1\nBaseline system. We compare with vLLM [28], a state-of-\nthe-art LLM serving system optimized for high throughput.\nvLLM also integrates optimizations from other LLM infer-\nence systems, such as continuous batching from Orca [57].\nWe use vLLM with homogeneous H100 GPUs and use tensor\nparallel for multi-GPU inference. As Lamina only focuses\non the decode phase, we modify vLLM to remove the prefill\nphase during evaluation for a fair comparison.\n6.1 Serving Performance\nWe evaluate the serving performance of Lamina against\nvLLM using real-world request traces. We first use homo-\ngeneous and heterogeneous hardware settings of similar costs,\nlisted in Table 5, for vLLM and Lamina, respectively. Com-\npared with vLLM, Lamina replaces half of the H100 devices\nto H20, which is cheaper but provides more memory capacity\nand bandwidth. We measure the token generation throughput,\ntime between tokens (TBT), and average batch size.\nTable 5: Equal-cost hardware configurations for evaluation.\nModel Lamina vLLM\nLLaMA-33B DOP=(1,2) 2 ×H100\n($20.32/hr) ($22.12/hr)\nLLaMA-65B, LLaMA3-70B DOP=(2,4) 4 ×H100\n($40.64/hr) ($44.24/hr)\nAs illustrated in Figure 10, Lamina consistently achieves\n16.1 ∼ 90.1% higher throughput than vLLM among all mod-\nels and traces, given comparable hardware costs. This en-\nhancement is primarily attributed to the larger batch size at-\ntained by Lamina, which is2.39× of vLLM on average. These\n9", "sentences": [{"text": "Request 1\nRequest 2\nRequest 3\nRequest 4\n(a) Request-level partition.", "metadata": {}}, {"text": "Request 1\nRequest 2\nRequest 3\nRequest 4\ntoken\nhead\ndevice 1\ndevice 2 (b) Head-level partition.", "metadata": {}}, {"text": "Figure 9: Work partition methods of the attention operator.", "metadata": {}}, {"text": "store the KV caches and compute the attention operators.", "metadata": {}}, {"text": "As\ndepicted in Figure 9, the attention operators can be paral-\nlelized among memory devices in various ways.", "metadata": {}}, {"text": "One method\nis to distribute different requests across different devices;", "metadata": {}}, {"text": "an\nalternative strategy is to partition and distribute the attention\nheads, which can also be computed independently, to differ-\nent devices.", "metadata": {}}, {"text": "The head-level partitioning approach ensures\na balanced workload distribution, whereas the request-level\npartitioning may result in load imbalance due to the differ-\nences in sequence lengths and therefore the KV cache sizes\namong requests.", "metadata": {}}, {"text": "However, head-level partitioning has limited\nflexibility, as it requires the number of memory devices to\nbe divisible by the number of attention heads.", "metadata": {}}, {"text": "We opt for\nhead-level partitioning in Lamina, which offers optimal load\nbalancing.", "metadata": {}}, {"text": "6 Evaluation\nTestbed.", "metadata": {}}, {"text": "We deploy Lamina on a real heterogeneous cluster\nwith two kinds of GPU nodes.", "metadata": {}}, {"text": "Each node consists of either\neight H100 or H20 GPUs, and each GPU is paired with a ded-\nicated ConnectX-7 NIC via PCIe switch.", "metadata": {}}, {"text": "The GPU nodes are\ninterconnected with 400 Gbps RoCE network.", "metadata": {}}, {"text": "We use H100\nas compute-optimized GPUs and H20 as memory-optimized\nGPUs for Lamina.", "metadata": {}}, {"text": "Models.", "metadata": {}}, {"text": "Lamina supports a wide variety of LLM architec-\ntures, including OPT [58], LLaMA [48], and LLaMA3 [9].", "metadata": {}}, {"text": "All\nthese architectures have similar outlines and workload charac-\nteristics and only have minor differences irrelevant to system\ndesigns.", "metadata": {}}, {"text": "Hence, as listed in Table 3, we choose LLaMA-33B,\nLLaMA-65B, and LLaMA3-70B for evaluations.", "metadata": {}}, {"text": "All model\nparameters and KV caches are stored in FP16 format.", "metadata": {}}, {"text": "Workloads To mirror the real-world LLM use cases, we\nuse four request traces collected from the production systems\nof two LLM service providers, Azure [1, 40] and Kimi [41].", "metadata": {}}, {"text": "Due to data protection regulations, these traces only con-\ntain the sequence length of user requests but not the actual\ncontents.", "metadata": {}}, {"text": "Hence, we use requests of dummy tokens with the\nsame sequence length for evaluation.", "metadata": {}}, {"text": "The summaries of these\nTable 3: Large language models used for evaluation.", "metadata": {}}, {"text": "Model Parameters L d G\nLLaMA-33B 64.7 GB 60 6656 1\nLLaMA-65B 130.1 GB 80 8192 1\nLLaMA3-70B 137.5 GB 80 8192 8\ntraces, including the average prompt tokens (lp) and average\ngenerated tokens (lg), are listed in Table 4.", "metadata": {}}, {"text": "Table 4: Request traces used for evaluation.", "metadata": {}}, {"text": "Trace # Requests lp lg\nAzure-Conv 19366 1154.7 211.1\nAzure-Code 8819 2047.8 27.9\nKimi-Conv 12031 12035.1 342.6\nKimi-TA 23608 8560.0 182.1\nBaseline system.", "metadata": {}}, {"text": "We compare with vLLM [28], a state-of-\nthe-art LLM serving system optimized for high throughput.", "metadata": {}}, {"text": "vLLM also integrates optimizations from other LLM infer-\nence systems, such as continuous batching from Orca [57].", "metadata": {}}, {"text": "We use vLLM with homogeneous H100 GPUs and use tensor\nparallel for multi-GPU inference.", "metadata": {}}, {"text": "As Lamina only focuses\non the decode phase, we modify vLLM to remove the prefill\nphase during evaluation for a fair comparison.", "metadata": {}}, {"text": "6.1 Serving Performance\nWe evaluate the serving performance of Lamina against\nvLLM using real-world request traces.", "metadata": {}}, {"text": "We first use homo-\ngeneous and heterogeneous hardware settings of similar costs,\nlisted in Table 5, for vLLM and Lamina, respectively.", "metadata": {}}, {"text": "Com-\npared with vLLM, Lamina replaces half of the H100 devices\nto H20, which is cheaper but provides more memory capacity\nand bandwidth.", "metadata": {}}, {"text": "We measure the token generation throughput,\ntime between tokens (TBT), and average batch size.", "metadata": {}}, {"text": "Table 5: Equal-cost hardware configurations for evaluation.", "metadata": {}}, {"text": "Model Lamina vLLM\nLLaMA-33B DOP=(1,2) 2 ×H100\n($20.32/hr) ($22.12/hr)\nLLaMA-65B, LLaMA3-70B DOP=(2,4) 4 ×H100\n($40.64/hr) ($44.24/hr)\nAs illustrated in Figure 10, Lamina consistently achieves\n16.1 ∼ 90.1% higher throughput than vLLM among all mod-\nels and traces, given comparable hardware costs.", "metadata": {}}, {"text": "This en-\nhancement is primarily attributed to the larger batch size at-\ntained by Lamina, which is2.39× of vLLM on average.", "metadata": {}}, {"text": "These\n9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "0\n2000\n4000\nThroughput\n(token/s)\nAzure-Conv\nLamina\nvLLM\n0\n2000\n4000\nAzure-Code\n0\n500\n1000\nKimi-Conv\n0\n500\n1000\nKimi-TA\n0\n50\n100\n150TBT (ms)\n0\n50\n100\n0\n50\n100\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n250\n500\n750Mean batch size\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n200\n400\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nFigure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.\n0 50 100\nPrice ($/hr)\n0\n2000\n4000\nThroughput\n(token/s)\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n5000\n10000\n(2,1)\n(2,2)\n(2,4)(2,8)\n(4,8)\n(8,8)\n4 8\nLLaMA3-70B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n200\n400\n(2,1)(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Kimi-Conv\n0 50 100\nPrice ($/hr)\n0\n1000\n2000\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA3-70B, Kimi-Conv\nLamina\nvLLM\nFigure 11: Decoding throughput and hardware cost with various hardware configurations. The DOPs for Lamina and tensor\nparallelisms for vLLM are annotated in the plot. The configuration with best cost efficiency is bolded.\nresults demonstrate that Lamina effectively leverages the ex-\ntra memory capacity provided by memory-optimized devices\nto boost decoding throughput. Note that the throughput and\nbatch size of LLaMA3-70B is much larger than LLaMA-33B\nand LLaMA-65B; this is because LLaMA3-70B adopts GQA\nwith a group size of 8, which results in a much smaller KV\ncache size per request.\nLamina experiences an increased token generation latency\nthan vLLM. This can be attributed by two factors. First, Lam-\nina adopts a larger batch size, which results in longer exe-\ncution time on both model and attention workers. Second,\nthe disaggregation of model and attention in Lamina may\nincur additional scheduling and networking overhead. Never-\ntheless, the end-to-end latency of Lamina can still meet the\nSLO requirements of interactive online LLM services in most\ncases.\nWe also explore the decoding throughput of Lamina and\nvLLM under various hardware configurations. Specifically,\nwe adjust the DOPs for Lamina and the number of devices\ninvolving tensor parallelism for vLLM. As the results in Fig-\nure 11 shows, the throughput for Lamina rapidly increases\nwith more attention workers added, which enables larger batch\nsizes. The addition of expensive model worker can only mildly\nimprove the throughput by reducing the model-part execution\nlatency. An exception is the LLaMA3-70B model, where the\nattainable batch size reaches 800 for DOP = (2,4), which al-\nready saturates the computation resources on model workers;\nhence, adding more memory devices will not dramatically\nimprove the throughput. This indicates that the optimal ratio\nbetween model and attention workers varies for different mod-\nels and workloads. In practice, we may conduct a performance\nprofiling and select the best hardware configuration.\n10", "sentences": [{"text": "0\n2000\n4000\nThroughput\n(token/s)\nAzure-Conv\nLamina\nvLLM\n0\n2000\n4000\nAzure-Code\n0\n500\n1000\nKimi-Conv\n0\n500\n1000\nKimi-TA\n0\n50\n100\n150TBT (ms)\n0\n50\n100\n0\n50\n100\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n250\n500\n750Mean batch size\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n200\n400\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nLLaMA-33BLLaMA-65BLLaMA3-70B\n0\n50\n100\nFigure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.", "metadata": {}}, {"text": "0 50 100\nPrice ($/hr)\n0\n2000\n4000\nThroughput\n(token/s)\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n5000\n10000\n(2,1)\n(2,2)\n(2,4)(2,8)\n(4,8)\n(8,8)\n4 8\nLLaMA3-70B, Azure-Conv\n0 50 100\nPrice ($/hr)\n0\n200\n400\n(2,1)(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA-65B, Kimi-Conv\n0 50 100\nPrice ($/hr)\n0\n1000\n2000\n(2,1)\n(2,2)\n(2,4)\n(2,8)\n(4,8) (8,8)\n4\n8\nLLaMA3-70B, Kimi-Conv\nLamina\nvLLM\nFigure 11: Decoding throughput and hardware cost with various hardware configurations.", "metadata": {}}, {"text": "The DOPs for Lamina and tensor\nparallelisms for vLLM are annotated in the plot.", "metadata": {}}, {"text": "The configuration with best cost efficiency is bolded.", "metadata": {}}, {"text": "results demonstrate that Lamina effectively leverages the ex-\ntra memory capacity provided by memory-optimized devices\nto boost decoding throughput.", "metadata": {}}, {"text": "Note that the throughput and\nbatch size of LLaMA3-70B is much larger than LLaMA-33B\nand LLaMA-65B;", "metadata": {}}, {"text": "this is because LLaMA3-70B adopts GQA\nwith a group size of 8, which results in a much smaller KV\ncache size per request.", "metadata": {}}, {"text": "Lamina experiences an increased token generation latency\nthan vLLM.", "metadata": {}}, {"text": "This can be attributed by two factors.", "metadata": {}}, {"text": "First, Lam-\nina adopts a larger batch size, which results in longer exe-\ncution time on both model and attention workers.", "metadata": {}}, {"text": "Second,\nthe disaggregation of model and attention in Lamina may\nincur additional scheduling and networking overhead.", "metadata": {}}, {"text": "Never-\ntheless, the end-to-end latency of Lamina can still meet the\nSLO requirements of interactive online LLM services in most\ncases.", "metadata": {}}, {"text": "We also explore the decoding throughput of Lamina and\nvLLM under various hardware configurations.", "metadata": {}}, {"text": "Specifically,\nwe adjust the DOPs for Lamina and the number of devices\ninvolving tensor parallelism for vLLM.", "metadata": {}}, {"text": "As the results in Fig-\nure 11 shows, the throughput for Lamina rapidly increases\nwith more attention workers added, which enables larger batch\nsizes.", "metadata": {}}, {"text": "The addition of expensive model worker can only mildly\nimprove the throughput by reducing the model-part execution\nlatency.", "metadata": {}}, {"text": "An exception is the LLaMA3-70B model, where the\nattainable batch size reaches 800 for DOP = (2,4), which al-\nready saturates the computation resources on model workers;", "metadata": {}}, {"text": "hence, adding more memory devices will not dramatically\nimprove the throughput.", "metadata": {}}, {"text": "This indicates that the optimal ratio\nbetween model and attention workers varies for different mod-\nels and workloads.", "metadata": {}}, {"text": "In practice, we may conduct a performance\nprofiling and select the best hardware configuration.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "2 4 6 8 10 12 14\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=4096\n4 8 12 16 20 24 28 32\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=4096\n16 32 48 64 80 96 112 128\nBatch size\n0\n25\n50\n75Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=4096\n1 2 3 4 5 6 7\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=8192\n2 4 6 8 10 12 14 16\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=8192\n8 16 24 32 40 48 56 64\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=8192\nTBT Model Attention Network\nFigure 12: Token generation latency breakdown.\n6.2 Latency Breakdown\nLatency is a crucial indicator of the service quality offered by\nLLM applications. In this subsection, we measure the time\nbetween tokens (TBT) across various system configurations,\nas well as the execution time for model and attention workers\nand the networking overhead. We use requests with fixed\nsequence lengths (4096 or 8192) as the workload and disable\nrotational pipelining to better reveal the time breakdown.\nAs we can see from Figure 12, for smaller batch sizes, the\nmodel execution time dominates the token generation latency.\nThe attention and networking latency rapidly increases for\nlarger batch sizes, while the model execution time remains\nalmost constant. This indicates that the computation resource\nutilization gets improved as batch size increases. Note that the\nobserved TBT might be less than the sum of model worker\ntime, attention worker time, and network time. This is due to\nthe automated resource utilization overlapping optimization,\nwhich will be further profiled in subsection 6.4.\n6.3 Network Stack Optimizations\nWe evaluate the effectiveness of our fully host-bypassed net-\nwork (FHBN) stack with a microbenchmark. Specifically, we\nconduct a ping-pong test between two GPUs located on dis-\ntinct nodes, using NCCL, NCCL without GPUDirect RDMA,\nGloo, and FHBN as the networking engine. The initiator GPU\nsends a varying amount of data to the remote GPU. Upon re-\nceiving the complete data, the remote GPU immediately sends\nit back to the initiator. We measure the round-trip time from\nthe initiator GPU’s perspective, which encompasses the time\ninterval from the completion of the kernel that generates the\ndata for transmission to the start of the kernel that consumes\nthe received data.\nGloo NCCL (wo/ GDR) NCCL FHBN\n102 104 106 108\nPayload size (byte)\n102\n103\nRound-trip time (µs)\n(a) Round-trip time.\n102 104 106 108\nPayload size (byte)\n0\n20\n40Bandwidth (GB/s) (b) Bandwidth utilization.\nFigure 13: Network ping-pong test between two GPUs on\ndifferent nodes, interconnected with 400Gbps RoCE.\nAs illustrated in Figure 13, for smaller data sizes, the round-\ntrip time is primarily determined by network latency. In this\ncase, FHBN achieves an end-to-end latency of 33.0 µs, rep-\nresenting a 50.5% reduction compared to NCCL’s 66.6 µs\nlatency. This improvement is attributed to the removal of host\nCPU involvement in data transmission, eliminating expensive\nhost-device synchronization and PCIe transactions. This im-\nprovement justifies the efficacy of our fully host-bypassed\nnetwork stack design.\nFor larger payload sizes, the primary factor influencing\nnetworking time is the utilization of network bandwidth. In\nthis scenario, FHBN reaches a peak network bandwidth of\n45.7 GB/s, which corresponds to 91.4% of the line rate. Con-\nversely, NCCL only attains a bandwidth of 35.5 GB/s. As a\nresult, FHBN can also serve as a superior alternative to exist-\ning communication libraries for point-to-point transmission\nof large GPU memory blocks within DCNs.\n11", "sentences": [{"text": "2 4 6 8 10 12 14\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=4096\n4 8 12 16 20 24 28 32\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=4096\n16 32 48 64 80 96 112 128\nBatch size\n0\n25\n50\n75Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=4096\n1 2 3 4 5 6 7\nBatch size\n0\n20\n40Latency (ms)\nLLaMA-33B, DOP=(1,2), l=8192\n2 4 6 8 10 12 14 16\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA-65B, DOP=(2,4), l=8192\n8 16 24 32 40 48 56 64\nBatch size\n0\n20\n40\n60Latency (ms)\nLLaMA3-70B, DOP=(2,2), l=8192\nTBT Model Attention Network\nFigure 12: Token generation latency breakdown.", "metadata": {}}, {"text": "6.2 Latency Breakdown\nLatency is a crucial indicator of the service quality offered by\nLLM applications.", "metadata": {}}, {"text": "In this subsection, we measure the time\nbetween tokens (TBT) across various system configurations,\nas well as the execution time for model and attention workers\nand the networking overhead.", "metadata": {}}, {"text": "We use requests with fixed\nsequence lengths (4096 or 8192) as the workload and disable\nrotational pipelining to better reveal the time breakdown.", "metadata": {}}, {"text": "As we can see from Figure 12, for smaller batch sizes, the\nmodel execution time dominates the token generation latency.", "metadata": {}}, {"text": "The attention and networking latency rapidly increases for\nlarger batch sizes, while the model execution time remains\nalmost constant.", "metadata": {}}, {"text": "This indicates that the computation resource\nutilization gets improved as batch size increases.", "metadata": {}}, {"text": "Note that the\nobserved TBT might be less than the sum of model worker\ntime, attention worker time, and network time.", "metadata": {}}, {"text": "This is due to\nthe automated resource utilization overlapping optimization,\nwhich will be further profiled in subsection 6.4.", "metadata": {}}, {"text": "6.3 Network Stack Optimizations\nWe evaluate the effectiveness of our fully host-bypassed net-\nwork (FHBN) stack with a microbenchmark.", "metadata": {}}, {"text": "Specifically, we\nconduct a ping-pong test between two GPUs located on dis-\ntinct nodes, using NCCL, NCCL without GPUDirect RDMA,\nGloo, and FHBN as the networking engine.", "metadata": {}}, {"text": "The initiator GPU\nsends a varying amount of data to the remote GPU.", "metadata": {}}, {"text": "Upon re-\nceiving the complete data, the remote GPU immediately sends\nit back to the initiator.", "metadata": {}}, {"text": "We measure the round-trip time from\nthe initiator GPU’s perspective, which encompasses the time\ninterval from the completion of the kernel that generates the\ndata for transmission to the start of the kernel that consumes\nthe received data.", "metadata": {}}, {"text": "Gloo NCCL (wo/ GDR) NCCL FHBN\n102 104 106 108\nPayload size (byte)\n102\n103\nRound-trip time (µs)\n(a) Round-trip time.", "metadata": {}}, {"text": "102 104 106 108\nPayload size (byte)\n0\n20\n40Bandwidth (GB/s) (b) Bandwidth utilization.", "metadata": {}}, {"text": "Figure 13: Network ping-pong test between two GPUs on\ndifferent nodes, interconnected with 400Gbps RoCE.", "metadata": {}}, {"text": "As illustrated in Figure 13, for smaller data sizes, the round-\ntrip time is primarily determined by network latency.", "metadata": {}}, {"text": "In this\ncase, FHBN achieves an end-to-end latency of 33.0 µs, rep-\nresenting a 50.5% reduction compared to NCCL’s 66.6 µs\nlatency.", "metadata": {}}, {"text": "This improvement is attributed to the removal of host\nCPU involvement in data transmission, eliminating expensive\nhost-device synchronization and PCIe transactions.", "metadata": {}}, {"text": "This im-\nprovement justifies the efficacy of our fully host-bypassed\nnetwork stack design.", "metadata": {}}, {"text": "For larger payload sizes, the primary factor influencing\nnetworking time is the utilization of network bandwidth.", "metadata": {}}, {"text": "In\nthis scenario, FHBN reaches a peak network bandwidth of\n45.7 GB/s, which corresponds to 91.4% of the line rate.", "metadata": {}}, {"text": "Con-\nversely, NCCL only attains a bandwidth of 35.5 GB/s.", "metadata": {}}, {"text": "As a\nresult, FHBN can also serve as a superior alternative to exist-\ning communication libraries for point-to-point transmission\nof large GPU memory blocks within DCNs.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "6.4 Resource Utilization Overlapping\nTo assess the efficacy of resource utilization overlapping (sub-\nsubsection 4.2.2) implemented in our automated model con-\nverter, we conducted a series of experiments on the LLaMA-\n65B and LLaMA3-70B models, with the optimization either\nenabled or disabled. We use request batches of varying sizes\nand the context length of each request is fixed at 4096.\n10 20\nBatch size\n50\n55\n60TBT (ms)\nEnabled\nDisabled\n(a) LLaMA-65B, DOP=(2,2).\n25 50 75 100\nBatch size\n55\n60\n65TBT (ms)\n (b) LLaMA3-70B, DOP=(2,4).\nFigure 14: Time between tokens (TBT) results with automatic\nresource utilization overlapping enabled and disabled.\nAs illustrated in Figure 14, the LLaMA-65B model experi-\nences a significant improvement in performance, achieving up\nto a 13.2% with through automated resource utilization over-\nlapping. The speedup is particularly notable for larger batch\nsizes, which produce larger KV tensors and result in greater\nlatency reduction. The effectiveness is less pronounced for the\nLLaMA3-70B model, where the maximum latency reduction\nis only 3.5%. This is because LLaMA3-70B adopts GQA,\nwhose KV size is 8× smaller. Consequently, there is less\nroom for resource utilization overlapping in LLaMA3-70B.\n7 Discussion\nGenerality of our techniques. Although Lamina is built\nfor model-attention disaggregation, relevant techniques can\nalso be used to enable a wider range of fine-grained LLM\ndisaggregation techniques in distributed heterogeneous envi-\nronments. For example, LoRA [24] and Mixture-of-Experts\n(MoE) [18, 53] all add less computation-intensive operators\nto existing LLM architectures. Like Lamina, we may also of-\nfload the LoRA and MoE operators to less powerful but more\neconomic remote accelerators to reduce the inference cost.\nSuch operator-level disaggregations, unlike prefill-decode dis-\naggregation, require frequent layer-wise communications and\nare considered not feasible unless an optimized networking\nstack like the one in Lamina is used.\nAlternative heterogeneous devices. In Lamina, we may\nuse more specialized accelerating devices for optimal perfor-\nmance and cost. For example, we anticipate that Processing-\nin-Memory (PIM) devices [13,22,26,29,30,32,47,54] will be\na more suitable candidate for memory-optimized devices as\nthey demonstrate even greater cost advantages alongside their\nlarger capacity and higher bandwidth. Besides, we can also\nuse CPU and DRAM for attention computation and KV cache\nstorage. However, due to the relatively smaller bandwidth of\nhost DRAM, it is preferable to also adopt sparse attention\nmechanisms [14, 52] to reduce the size of data read during\nattention computation.\n8 Related Work\nSystem optimizations for LLM Inference. Splitwise [40]\nand DistServe [59] proposes prefill-decode disaggregation,\nwhich improves hardware utilization and minimizes the inter-\nference between the prefill and decode phases. Orca [57] pro-\nposes continuous batching, that batches incoming requests in\niteration granularity. Compared with whole-request batching,\ncontinuous batching greatly reduces resource waste caused\nby early termination during the decode phase. PagedAtten-\ntion [28] focuses on memory management optimizations, us-\ning fine-grained KV cache management to reduce memory\nwaste. PagedAttention can also be used to optimize various de-\ncoding scenarios, like beam search and shared prefixes. These\noptimizations can all be used in our system. FlexGen [44]\nis a heterogeneous LLM inference system employing layer-\nand token-level task partitioning and scheduling. However, it\ndoes not account for the varying characteristics of different\noperators within a layer. LLM-tailored inference systems, like\nDeepSeed [11], Megatron-LM [45], and TensorRT-LLM [39],\nuse optimizations of various aspects including kernel opti-\nmization [17, 23], advanced scheduling [8, 19, 33, 51], and\nefficient memory management [19].\nSpeculative Decoding The speculative decoding technol-\nogy [31,36,38] enables parallel generation of multiple tokens\nfor a single request during the decoding phase. This is done\nby guessing the next few tokens using a smaller auxiliary\nmodel. These predicted tokens are then validated by the pri-\nmary LLM. This validation of the predicted tokens can be\nexecuted in parallel, thereby enhancing the arithmetic inten-\nsity and reducing latency. However, speculative decoding can\nlead to a trade-off in throughput due to the auxiliary model’s\noverhead and the potential need for re-execution in case of\nmisprediction.\nVariations of the Attention Operator. Researchers have\ndeveloped many variations of the attention operator for large\nlanguage models to mitigate the memory bottleneck. GQA\n[10] and MLA [34] are two recent attention mechanisms\ntargeted for memory efficiency. Model quantization uses\nreduced-precision formats (e.g., FP8) to store KV caches. Var-\nious sparse attention mechanisms [14,15,27,35,37,42,43,56]\nhave been adopted, focusing on a subset of all history key-\nvalue pairs during attention computation. All these modifica-\ntions to the attention operator, however, might compromise\nthe model quality.\n12", "sentences": [{"text": "6.4 Resource Utilization Overlapping\nTo assess the efficacy of resource utilization overlapping (sub-\nsubsection 4.2.2) implemented in our automated model con-\nverter, we conducted a series of experiments on the LLaMA-\n65B and LLaMA3-70B models, with the optimization either\nenabled or disabled.", "metadata": {}}, {"text": "We use request batches of varying sizes\nand the context length of each request is fixed at 4096.", "metadata": {}}, {"text": "10 20\nBatch size\n50\n55\n60TBT (ms)\nEnabled\nDisabled\n(a) LLaMA-65B, DOP=(2,2).", "metadata": {}}, {"text": "25 50 75 100\nBatch size\n55\n60\n65TBT (ms)\n (b) LLaMA3-70B, DOP=(2,4).", "metadata": {}}, {"text": "Figure 14: Time between tokens (TBT) results with automatic\nresource utilization overlapping enabled and disabled.", "metadata": {}}, {"text": "As illustrated in Figure 14, the LLaMA-65B model experi-\nences a significant improvement in performance, achieving up\nto a 13.2% with through automated resource utilization over-\nlapping.", "metadata": {}}, {"text": "The speedup is particularly notable for larger batch\nsizes, which produce larger KV tensors and result in greater\nlatency reduction.", "metadata": {}}, {"text": "The effectiveness is less pronounced for the\nLLaMA3-70B model, where the maximum latency reduction\nis only 3.5%.", "metadata": {}}, {"text": "This is because LLaMA3-70B adopts GQA,\nwhose KV size is 8× smaller.", "metadata": {}}, {"text": "Consequently, there is less\nroom for resource utilization overlapping in LLaMA3-70B.", "metadata": {}}, {"text": "7 Discussion\nGenerality of our techniques.", "metadata": {}}, {"text": "Although Lamina is built\nfor model-attention disaggregation, relevant techniques can\nalso be used to enable a wider range of fine-grained LLM\ndisaggregation techniques in distributed heterogeneous envi-\nronments.", "metadata": {}}, {"text": "For example, LoRA [24] and Mixture-of-Experts\n(MoE) [18, 53] all add less computation-intensive operators\nto existing LLM architectures.", "metadata": {}}, {"text": "Like Lamina, we may also of-\nfload the LoRA and MoE operators to less powerful but more\neconomic remote accelerators to reduce the inference cost.", "metadata": {}}, {"text": "Such operator-level disaggregations, unlike prefill-decode dis-\naggregation, require frequent layer-wise communications and\nare considered not feasible unless an optimized networking\nstack like the one in Lamina is used.", "metadata": {}}, {"text": "Alternative heterogeneous devices.", "metadata": {}}, {"text": "In Lamina, we may\nuse more specialized accelerating devices for optimal perfor-\nmance and cost.", "metadata": {}}, {"text": "For example, we anticipate that Processing-\nin-Memory (PIM) devices [13,22,26,29,30,32,47,54] will be\na more suitable candidate for memory-optimized devices as\nthey demonstrate even greater cost advantages alongside their\nlarger capacity and higher bandwidth.", "metadata": {}}, {"text": "Besides, we can also\nuse CPU and DRAM for attention computation and KV cache\nstorage.", "metadata": {}}, {"text": "However, due to the relatively smaller bandwidth of\nhost DRAM, it is preferable to also adopt sparse attention\nmechanisms [14, 52] to reduce the size of data read during\nattention computation.", "metadata": {}}, {"text": "8 Related Work\nSystem optimizations for LLM Inference.", "metadata": {}}, {"text": "Splitwise [40]\nand DistServe [59] proposes prefill-decode disaggregation,\nwhich improves hardware utilization and minimizes the inter-\nference between the prefill and decode phases.", "metadata": {}}, {"text": "Orca [57] pro-\nposes continuous batching, that batches incoming requests in\niteration granularity.", "metadata": {}}, {"text": "Compared with whole-request batching,\ncontinuous batching greatly reduces resource waste caused\nby early termination during the decode phase.", "metadata": {}}, {"text": "PagedAtten-\ntion [28] focuses on memory management optimizations, us-\ning fine-grained KV cache management to reduce memory\nwaste.", "metadata": {}}, {"text": "PagedAttention can also be used to optimize various de-\ncoding scenarios, like beam search and shared prefixes.", "metadata": {}}, {"text": "These\noptimizations can all be used in our system.", "metadata": {}}, {"text": "FlexGen [44]\nis a heterogeneous LLM inference system employing layer-\nand token-level task partitioning and scheduling.", "metadata": {}}, {"text": "However, it\ndoes not account for the varying characteristics of different\noperators within a layer.", "metadata": {}}, {"text": "LLM-tailored inference systems, like\nDeepSeed [11], Megatron-LM [45], and TensorRT-LLM [39],\nuse optimizations of various aspects including kernel opti-\nmization [17, 23], advanced scheduling [8, 19, 33, 51], and\nefficient memory management [19].", "metadata": {}}, {"text": "Speculative Decoding The speculative decoding technol-\nogy [31,36,38] enables parallel generation of multiple tokens\nfor a single request during the decoding phase.", "metadata": {}}, {"text": "This is done\nby guessing the next few tokens using a smaller auxiliary\nmodel.", "metadata": {}}, {"text": "These predicted tokens are then validated by the pri-\nmary LLM.", "metadata": {}}, {"text": "This validation of the predicted tokens can be\nexecuted in parallel, thereby enhancing the arithmetic inten-\nsity and reducing latency.", "metadata": {}}, {"text": "However, speculative decoding can\nlead to a trade-off in throughput due to the auxiliary model’s\noverhead and the potential need for re-execution in case of\nmisprediction.", "metadata": {}}, {"text": "Variations of the Attention Operator.", "metadata": {}}, {"text": "Researchers have\ndeveloped many variations of the attention operator for large\nlanguage models to mitigate the memory bottleneck.", "metadata": {}}, {"text": "GQA\n[10] and MLA [34] are two recent attention mechanisms\ntargeted for memory efficiency.", "metadata": {}}, {"text": "Model quantization uses\nreduced-precision formats (e.g., FP8) to store KV caches.", "metadata": {}}, {"text": "Var-\nious sparse attention mechanisms [14,15,27,35,37,42,43,56]\nhave been adopted, focusing on a subset of all history key-\nvalue pairs during attention computation.", "metadata": {}}, {"text": "All these modifica-\ntions to the attention operator, however, might compromise\nthe model quality.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "9 Conclusion\nIn this paper, we present model-attention disaggregation, an\ninnovative architectural approach to improve the efficiency of\nLLM decoding. This approach is motivated by the observation\nthat the LLM decoding phase can be divided into computation-\nintensive parts and memory-intensive parts (i.e., the attention\noperators). Hence, we may use computation- and memory-\noptimized devices for each part to improve the hardware re-\nsource utilization. Moreover, by adjusting the To realize this\nidea, we design a revamped latency-optimized networking\nstack that facilitate the frequent data transfer between remote\nGPUs. We also develop automated tools for transforming and\noptimizing existing LLMs for model-attention disaggrega-\ntion. We develop and deploy Lamina on a cluster comprising\nheterogeneous GPUs. Evaluation on traces collected from\nproduction systems show that Lamina provides16.1 ∼ 90.1%\nhigher throughput than heterogeneous solutions with similar\nhardware costs.\nReferences\n[1] Azure llm inference trace 2023. https:\n//github.com/Azure/AzurePublicDataset/blob/\nmaster/AzureLLMInferenceDataset2023.md.\n[2] Cloud Computing Services | Google Cloud. https:\n//cloud.google.com/.\n[3] GPUDirect RDMA. https://docs.nvidia.com/\ncuda/gpudirect-rdma/.\n[4] Mellanox adapters programmer’s reference manual.\nhttps://network.nvidia.com/files/doc-2020/\nethernet-adapters-programming-manual.pdf .\n[5] Ray. https://www.ray.io/.\n[6] RDMA core userspace libraries and daemons. https:\n//github.com/linux-rdma/rdma-core.\n[7] TPU v6e specification. https://cloud.google.com/\ntpu/docs/v6e.\n[8] Amey Agrawal, Ashish Panwar, Jayashree Mohan,\nNipun Kwatra, Bhargav S. Gulavani, and Ramachandran\nRamjee. Sarathi: Efficient llm inference by piggyback-\ning decodes with chunked prefills, 2023.\n[9] AI@Meta. Llama 3 model card. 2024.\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa:\nTraining generalized multi-query transformer models\nfrom multi-head checkpoints, 2023.\n[11] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\nZhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\nZheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and\nYuxiong He. Deepspeed inference: Enabling efficient\ninference of transformer models at unprecedented scale,\n2022.\n[12] Anonymous. Hexgen-2: Disaggregated generative in-\nference of LLMs in heterogeneous environment. In\nSubmitted to The Thirteenth International Conference\non Learning Representations, 2024. under review.\n[13] Kazi Asifuzzaman, Narasinga Rao Miniskar, Aaron R\nYoung, Frank Liu, and Jeffrey S Vetter. A survey on\nprocessing-in-memory techniques: Advances and chal-\nlenges. Memories-Materials, Devices, Circuits and Sys-\ntems, 4:100022, 2023.\n[14] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\n[15] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse trans-\nformers. arXiv preprint arXiv:1904.10509, 2019.\n[16] Y . Choi, Y . Kim, and M. Rhu. Lazy batching: An\nsla-aware batching system for cloud machine learn-\ning inference. In 2021 IEEE International Symposium\non High-Performance Computer Architecture (HPCA),\npages 493–506, Los Alamitos, CA, USA, mar 2021.\nIEEE Computer Society.\n[17] Tri Dao, Daniel Haziza, Francisco Massa, and Grig-\nory Sizov. Flash-decoding for long-context infer-\nence. https://crfm.stanford.edu/2023/10/12/\nflashdecoding.html.\n[18] Artyom Eliseev and Denis Mazur. Fast inference of\nmixture-of-experts language models with offloading,\n2023.\n[19] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.\nTurbotransformers: An efficient gpu serving system for\ntransformer models. In Proceedings of the 26th ACM\nSIGPLAN Symposium on Principles and Practice of\nParallel Programming, PPoPP ’21, page 389–402, New\nYork, NY , USA, 2021. Association for Computing Ma-\nchinery.\n[20] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. Low\nlatency rnn inference with cellular batching. In Proceed-\nings of the Thirteenth EuroSys Conference, EuroSys ’18,\nNew York, NY , USA, 2018. Association for Computing\nMachinery.\n[21] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman\nHooper, Michael W Mahoney, and Kurt Keutzer. Ai\nand memory wall. IEEE Micro, 2024.\n13", "sentences": [{"text": "9 Conclusion\nIn this paper, we present model-attention disaggregation, an\ninnovative architectural approach to improve the efficiency of\nLLM decoding.", "metadata": {}}, {"text": "This approach is motivated by the observation\nthat the LLM decoding phase can be divided into computation-\nintensive parts and memory-intensive parts (i.e., the attention\noperators).", "metadata": {}}, {"text": "Hence, we may use computation- and memory-\noptimized devices for each part to improve the hardware re-\nsource utilization.", "metadata": {}}, {"text": "Moreover, by adjusting the To realize this\nidea, we design a revamped latency-optimized networking\nstack that facilitate the frequent data transfer between remote\nGPUs.", "metadata": {}}, {"text": "We also develop automated tools for transforming and\noptimizing existing LLMs for model-attention disaggrega-\ntion.", "metadata": {}}, {"text": "We develop and deploy Lamina on a cluster comprising\nheterogeneous GPUs.", "metadata": {}}, {"text": "Evaluation on traces collected from\nproduction systems show that Lamina provides16.1 ∼ 90.1%\nhigher throughput than heterogeneous solutions with similar\nhardware costs.", "metadata": {}}, {"text": "References\n[1] Azure llm inference trace 2023.", "metadata": {}}, {"text": "https:\n//github.com/Azure/AzurePublicDataset/blob/\nmaster/AzureLLMInferenceDataset2023.md.", "metadata": {}}, {"text": "[2] Cloud Computing Services | Google Cloud.", "metadata": {}}, {"text": "https:\n//cloud.google.com/.", "metadata": {}}, {"text": "[3] GPUDirect RDMA.", "metadata": {}}, {"text": "https://docs.nvidia.com/\ncuda/gpudirect-rdma/.", "metadata": {}}, {"text": "[4] Mellanox adapters programmer’s reference manual.", "metadata": {}}, {"text": "https://network.nvidia.com/files/doc-2020/\nethernet-adapters-programming-manual.pdf .", "metadata": {}}, {"text": "[5] Ray.", "metadata": {}}, {"text": "https://www.ray.io/.", "metadata": {}}, {"text": "[6] RDMA core userspace libraries and daemons.", "metadata": {}}, {"text": "https:\n//github.com/linux-rdma/rdma-core.", "metadata": {}}, {"text": "[7] TPU v6e specification.", "metadata": {}}, {"text": "https://cloud.google.com/\ntpu/docs/v6e.", "metadata": {}}, {"text": "[8] Amey Agrawal, Ashish Panwar, Jayashree Mohan,\nNipun Kwatra, Bhargav S.", "metadata": {}}, {"text": "Gulavani, and Ramachandran\nRamjee.", "metadata": {}}, {"text": "Sarathi: Efficient llm inference by piggyback-\ning decodes with chunked prefills, 2023.", "metadata": {}}, {"text": "[9] AI@Meta.", "metadata": {}}, {"text": "Llama 3 model card.", "metadata": {}}, {"text": "2024.", "metadata": {}}, {"text": "[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebrón, and Sumit Sanghai.", "metadata": {}}, {"text": "Gqa:\nTraining generalized multi-query transformer models\nfrom multi-head checkpoints, 2023.", "metadata": {}}, {"text": "[11] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\nZhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\nZheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and\nYuxiong He.", "metadata": {}}, {"text": "Deepspeed inference: Enabling efficient\ninference of transformer models at unprecedented scale,\n2022.", "metadata": {}}, {"text": "[12] Anonymous.", "metadata": {}}, {"text": "Hexgen-2: Disaggregated generative in-\nference of LLMs in heterogeneous environment.", "metadata": {}}, {"text": "In\nSubmitted to The Thirteenth International Conference\non Learning Representations, 2024.", "metadata": {}}, {"text": "under review.", "metadata": {}}, {"text": "[13] Kazi Asifuzzaman, Narasinga Rao Miniskar, Aaron R\nYoung, Frank Liu, and Jeffrey S Vetter.", "metadata": {}}, {"text": "A survey on\nprocessing-in-memory techniques: Advances and chal-\nlenges.", "metadata": {}}, {"text": "Memories-Materials, Devices, Circuits and Sys-\ntems, 4:100022, 2023.", "metadata": {}}, {"text": "[14] Iz Beltagy, Matthew E Peters, and Arman Cohan.", "metadata": {}}, {"text": "Long-\nformer: The long-document transformer.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2004.05150, 2020.", "metadata": {}}, {"text": "[15] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever.", "metadata": {}}, {"text": "Generating long sequences with sparse trans-\nformers.", "metadata": {}}, {"text": "arXiv preprint arXiv:1904.10509, 2019.", "metadata": {}}, {"text": "[16] Y .", "metadata": {}}, {"text": "Choi, Y .", "metadata": {}}, {"text": "Kim, and M.", "metadata": {}}, {"text": "Rhu.", "metadata": {}}, {"text": "Lazy batching: An\nsla-aware batching system for cloud machine learn-\ning inference.", "metadata": {}}, {"text": "In 2021 IEEE International Symposium\non High-Performance Computer Architecture (HPCA),\npages 493–506, Los Alamitos, CA, USA, mar 2021.", "metadata": {}}, {"text": "IEEE Computer Society.", "metadata": {}}, {"text": "[17] Tri Dao, Daniel Haziza, Francisco Massa, and Grig-\nory Sizov.", "metadata": {}}, {"text": "Flash-decoding for long-context infer-\nence.", "metadata": {}}, {"text": "https://crfm.stanford.edu/2023/10/12/\nflashdecoding.html.", "metadata": {}}, {"text": "[18] Artyom Eliseev and Denis Mazur.", "metadata": {}}, {"text": "Fast inference of\nmixture-of-experts language models with offloading,\n2023.", "metadata": {}}, {"text": "[19] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.", "metadata": {}}, {"text": "Turbotransformers: An efficient gpu serving system for\ntransformer models.", "metadata": {}}, {"text": "In Proceedings of the 26th ACM\nSIGPLAN Symposium on Principles and Practice of\nParallel Programming, PPoPP ’21, page 389–402, New\nYork, NY , USA, 2021.", "metadata": {}}, {"text": "Association for Computing Ma-\nchinery.", "metadata": {}}, {"text": "[20] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li.", "metadata": {}}, {"text": "Low\nlatency rnn inference with cellular batching.", "metadata": {}}, {"text": "In Proceed-\nings of the Thirteenth EuroSys Conference, EuroSys ’18,\nNew York, NY , USA, 2018.", "metadata": {}}, {"text": "Association for Computing\nMachinery.", "metadata": {}}, {"text": "[21] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman\nHooper, Michael W Mahoney, and Kurt Keutzer.", "metadata": {}}, {"text": "Ai\nand memory wall.", "metadata": {}}, {"text": "IEEE Micro, 2024.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok\nJeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N.\nVijaykumar. Newton: A dram-maker’s accelerator-in-\nmemory (aim) architecture for machine learning. In\n2020 53rd Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO), pages 372–385, 2020.\n[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong\nLi, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang.\nFlashdecoding++: Faster large language model infer-\nence on gpus, 2023.\n[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large lan-\nguage models, 2021.\n[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and\nKevin Skadron. Scaling with design constraints: Pre-\ndicting the future of big chips.IEEE Micro, 31(4):16–29,\n2011.\n[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu\nKim, Yuhwan Ro, Seungwon Lee, David Wang, Ji-\nhyun Choi, Jinin So, YeonGon Cho, JoonHo Song,\nJeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim.\nAquabolt-xl hbm2-pim, lpddr5-pim with in-memory pro-\ncessing, and axdimm with acceleration buffer. IEEE\nMicro, 42(3):20–30, 2022.\n[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efficient transformer. arXiv preprint\narXiv:2001.04451, 2020.\n[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,\nHao Zhang, and Ion Stoica. Efficient memory man-\nagement for large language model serving with page-\ndattention. In Proceedings of the 29th Symposium on\nOperating Systems Principles, SOSP ’23, page 611–626,\nNew York, NY , USA, 2023. Association for Computing\nMachinery.\n[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim,\nWoojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo,\nHaerang Choi, Guhyun Kim, Byeongju An, Jeongbin\nKim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook\nPark, Yosub Song, Byeongsu Yang, Hyungdeok Lee,\nSeho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim,\nSanghoon Oh, Joonhong Park, Gimoon Hong, Dongy-\noon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil\nKang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee,\nMinyoung Shin, Minhwan Shin, Jaekyung Cha, Chang-\nson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim,\nIl Park, Junhyun Chun, and Sk Hynix. System architec-\nture and software stack for gddr6-aim. In 2022 IEEE\nHot Chips 34 Symposium (HCS), pages 1–25, 2022.\n[30] Ann Franchesca Laguna, Arman Kazemi, Michael\nNiemier, and X. Sharon Hu. In-memory computing\nbased accelerator for transformer networks for long se-\nquences. In 2021 Design, Automation and Test in Europe\nConference & Exhibition (DATE) , pages 1839–1844,\n2021.\n[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast\ninference from transformers via speculative decoding,\n2023.\n[32] Wantong Li, Madison Manley, James Read, Ankit Kaul,\nMuhannad S. Bakir, and Shimeng Yu. H3datten: Het-\nerogeneous 3-d integrated hybrid analog and digital\ncompute-in-memory accelerator for vision transformer\nself-attention. IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(10):1592–1602, 2023.\n[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent\nLiu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,\nHao Zhang, Joseph E Gonzalez, et al. AlpaServe: Sta-\ntistical multiplexing with model parallelism for deep\nlearning serving. In 17th USENIX Symposium on Oper-\nating Systems Design and Implementation (OSDI 23),\npages 663–679, 2023.\n[34] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu,\nChenggang Zhao, Chengqi Dengr, Chong Ruan, Damai\nDai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Er-\nhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guant-\ning Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang,\nHaowei Zhang, Honghui Ding, Huajian Xin, Huazuo\nGao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong\nGuo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Jun-\njie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,\nLiang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J.\nChen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu,\nRuyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou,\nShanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong\nMa, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng\nZhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu\nSun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li,\nXiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu,\nXiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha\nChen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin\nLiu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou,\nXinyu Yang, Xuan Lu, Xuecheng Su, Y . Wu, Y . K. Li,\nY . X. Wei, Y . X. Zhu, Yanhong Xu, Yanping Huang, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,\nYi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao,\nYing He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan\n14", "sentences": [{"text": "[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok\nJeong, Seho Kim, Il Park, Mithuna Thottethodi, and T.", "metadata": {}}, {"text": "N.", "metadata": {}}, {"text": "Vijaykumar.", "metadata": {}}, {"text": "Newton: A dram-maker’s accelerator-in-\nmemory (aim) architecture for machine learning.", "metadata": {}}, {"text": "In\n2020 53rd Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO), pages 372–385, 2020.", "metadata": {}}, {"text": "[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong\nLi, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang.", "metadata": {}}, {"text": "Flashdecoding++: Faster large language model infer-\nence on gpus, 2023.", "metadata": {}}, {"text": "[24] Edward J.", "metadata": {}}, {"text": "Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen.", "metadata": {}}, {"text": "Lora: Low-rank adaptation of large lan-\nguage models, 2021.", "metadata": {}}, {"text": "[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and\nKevin Skadron.", "metadata": {}}, {"text": "Scaling with design constraints: Pre-\ndicting the future of big chips.IEEE Micro, 31(4):16–29,\n2011.", "metadata": {}}, {"text": "[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu\nKim, Yuhwan Ro, Seungwon Lee, David Wang, Ji-\nhyun Choi, Jinin So, YeonGon Cho, JoonHo Song,\nJeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim.", "metadata": {}}, {"text": "Aquabolt-xl hbm2-pim, lpddr5-pim with in-memory pro-\ncessing, and axdimm with acceleration buffer.", "metadata": {}}, {"text": "IEEE\nMicro, 42(3):20–30, 2022.", "metadata": {}}, {"text": "[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.", "metadata": {}}, {"text": "Reformer: The efficient transformer.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2001.04451, 2020.", "metadata": {}}, {"text": "[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,\nHao Zhang, and Ion Stoica.", "metadata": {}}, {"text": "Efficient memory man-\nagement for large language model serving with page-\ndattention.", "metadata": {}}, {"text": "In Proceedings of the 29th Symposium on\nOperating Systems Principles, SOSP ’23, page 611–626,\nNew York, NY , USA, 2023.", "metadata": {}}, {"text": "Association for Computing\nMachinery.", "metadata": {}}, {"text": "[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim,\nWoojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo,\nHaerang Choi, Guhyun Kim, Byeongju An, Jeongbin\nKim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook\nPark, Yosub Song, Byeongsu Yang, Hyungdeok Lee,\nSeho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim,\nSanghoon Oh, Joonhong Park, Gimoon Hong, Dongy-\noon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil\nKang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee,\nMinyoung Shin, Minhwan Shin, Jaekyung Cha, Chang-\nson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim,\nIl Park, Junhyun Chun, and Sk Hynix.", "metadata": {}}, {"text": "System architec-\nture and software stack for gddr6-aim.", "metadata": {}}, {"text": "In 2022 IEEE\nHot Chips 34 Symposium (HCS), pages 1–25, 2022.", "metadata": {}}, {"text": "[30] Ann Franchesca Laguna, Arman Kazemi, Michael\nNiemier, and X.", "metadata": {}}, {"text": "Sharon Hu.", "metadata": {}}, {"text": "In-memory computing\nbased accelerator for transformer networks for long se-\nquences.", "metadata": {}}, {"text": "In 2021 Design, Automation and Test in Europe\nConference & Exhibition (DATE) , pages 1839–1844,\n2021.", "metadata": {}}, {"text": "[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias.", "metadata": {}}, {"text": "Fast\ninference from transformers via speculative decoding,\n2023.", "metadata": {}}, {"text": "[32] Wantong Li, Madison Manley, James Read, Ankit Kaul,\nMuhannad S.", "metadata": {}}, {"text": "Bakir, and Shimeng Yu.", "metadata": {}}, {"text": "H3datten: Het-\nerogeneous 3-d integrated hybrid analog and digital\ncompute-in-memory accelerator for vision transformer\nself-attention.", "metadata": {}}, {"text": "IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(10):1592–1602, 2023.", "metadata": {}}, {"text": "[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent\nLiu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,\nHao Zhang, Joseph E Gonzalez, et al.", "metadata": {}}, {"text": "AlpaServe: Sta-\ntistical multiplexing with model parallelism for deep\nlearning serving.", "metadata": {}}, {"text": "In 17th USENIX Symposium on Oper-\nating Systems Design and Implementation (OSDI 23),\npages 663–679, 2023.", "metadata": {}}, {"text": "[34] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu,\nChenggang Zhao, Chengqi Dengr, Chong Ruan, Damai\nDai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Er-\nhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guant-\ning Chen, Guowei Li, H.", "metadata": {}}, {"text": "Zhang, Hanwei Xu, Hao Yang,\nHaowei Zhang, Honghui Ding, Huajian Xin, Huazuo\nGao, Hui Li, Hui Qu, J.", "metadata": {}}, {"text": "L.", "metadata": {}}, {"text": "Cai, Jian Liang, Jianzhong\nGuo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Jun-\njie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,\nLiang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.", "metadata": {}}, {"text": "J.", "metadata": {}}, {"text": "Chen, R.", "metadata": {}}, {"text": "L.", "metadata": {}}, {"text": "Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu,\nRuyi Chen, S.", "metadata": {}}, {"text": "S.", "metadata": {}}, {"text": "Li, Shanghao Lu, Shangyan Zhou,\nShanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong\nMa, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng\nZhou, Size Zheng, T.", "metadata": {}}, {"text": "Wang, Tian Pei, Tian Yuan, Tianyu\nSun, W.", "metadata": {}}, {"text": "L.", "metadata": {}}, {"text": "Xiao, Wangding Zeng, Wei An, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wentao Zhang, X.", "metadata": {}}, {"text": "Q.", "metadata": {}}, {"text": "Li,\nXiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu,\nXiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha\nChen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin\nLiu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou,\nXinyu Yang, Xuan Lu, Xuecheng Su, Y .", "metadata": {}}, {"text": "Wu, Y .", "metadata": {}}, {"text": "K.", "metadata": {}}, {"text": "Li,\nY .", "metadata": {}}, {"text": "X.", "metadata": {}}, {"text": "Wei, Y .", "metadata": {}}, {"text": "X.", "metadata": {}}, {"text": "Zhu, Yanhong Xu, Yanping Huang, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,\nYi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao,\nYing He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan\n14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen\nZhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian\nMa, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen\nZhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu\nWen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan\nWang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts\nlanguage model, 2024.\n[35] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring at-\ntention with blockwise transformers for near-infinite\ncontext, 2023.\n[36] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica,\nZhijie Deng, Alvin Cheung, and Hao Zhang. Online\nspeculative decoding, 2023.\n[37] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher Re, et al. Deja vu: Con-\ntextual sparsity for efficient llms at inference time. In\nInternational Conference on Machine Learning, pages\n22137–22176. PMLR, 2023.\n[38] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-\nhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan\nZhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming\nChen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao\nJia. Specinfer: Accelerating generative large language\nmodel serving with speculative inference and token tree\nverification, 2023.\n[39] NVIDIA. Tensorrt-llm: A tensorrt toolbox for optimized\nlarge language model inference. https://github.\ncom/NVIDIA/TensorRT-LLM.\n[40] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka\nShah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini.\nSplitwise: Efficient generative llm inference using phase\nsplitting, 2024.\n[41] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang,\nYongwei Wu, Weimin Zheng, and Xinran Xu. Moon-\ncake: A kvcache-centric disaggregated architecture for\nllm serving. arXiv preprint arXiv:2407.00079, 2024.\n[42] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,\nSinong Wang, and Jie Tang. Blockwise self-attention\nfor long document understanding. arXiv preprint\narXiv:1911.02972, 2019.\n[43] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. Efficient content-based sparse attention\nwith routing transformers. Transactions of the Associa-\ntion for Computational Linguistics, 9:53–68, 2021.\n[44] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christopher\nRé, Ion Stoica, and Ce Zhang. Flexgen: High-throughput\ngenerative inference of large language models with a\nsingle gpu. In Proceedings of the 40th International\nConference on Machine Learning, ICML’23. JMLR.org,\n2023.\n[45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language\nmodels using model parallelism, 2020.\n[46] Franyell Silfa, Jose Maria Arnau, and Antonio González.\nE-batch: Energy-efficient and high-throughput rnn\nbatching. ACM Trans. Archit. Code Optim., 19(1), jan\n2022.\n[47] Shrihari Sridharan, Jacob R. Stevens, Kaushik Roy, and\nAnand Raghunathan. X-former: In-memory acceleration\nof transformers. IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(8):1223–1233, 2023.\n[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\ntiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foun-\ndation language models, 2023.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In\nProceedings of the 31st International Conference on\nNeural Information Processing Systems, NIPS’17, page\n6000–6010, Red Hook, NY , USA, 2017. Curran Asso-\nciates Inc.\n[50] Samuel Williams, Andrew Waterman, and David Pat-\nterson. Roofline: An insightful visual performance\nmodel for multicore architectures. Commun. ACM ,\n52(4):65–76, apr 2009.\n[51] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang,\nXuanzhe Liu, and Xin Jin. Fast distributed inference\nserving for large language models, 2023.\n[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. Efficient streaming language\nmodels with attention sinks, 2024.\n[53] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh\nMarina. Moe-infinity: Offloading-efficient moe model\nserving, 2024.\n[54] Xiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen. Re-\ntransformer: Reram-based processing-in-memory archi-\ntecture for transformer acceleration. In Proceedings of\n15", "sentences": [{"text": "Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen\nZhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian\nMa, Yuting Yan, Yuxiang You, Yuxuan Liu, Z.", "metadata": {}}, {"text": "Z.", "metadata": {}}, {"text": "Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen\nZhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu\nWen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan\nWang, Zihui Gu, Zilin Li, and Ziwei Xie.", "metadata": {}}, {"text": "Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts\nlanguage model, 2024.", "metadata": {}}, {"text": "[35] Hao Liu, Matei Zaharia, and Pieter Abbeel.", "metadata": {}}, {"text": "Ring at-\ntention with blockwise transformers for near-infinite\ncontext, 2023.", "metadata": {}}, {"text": "[36] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica,\nZhijie Deng, Alvin Cheung, and Hao Zhang.", "metadata": {}}, {"text": "Online\nspeculative decoding, 2023.", "metadata": {}}, {"text": "[37] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher Re, et al.", "metadata": {}}, {"text": "Deja vu: Con-\ntextual sparsity for efficient llms at inference time.", "metadata": {}}, {"text": "In\nInternational Conference on Machine Learning, pages\n22137–22176.", "metadata": {}}, {"text": "PMLR, 2023.", "metadata": {}}, {"text": "[38] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-\nhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan\nZhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming\nChen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao\nJia.", "metadata": {}}, {"text": "Specinfer: Accelerating generative large language\nmodel serving with speculative inference and token tree\nverification, 2023.", "metadata": {}}, {"text": "[39] NVIDIA.", "metadata": {}}, {"text": "Tensorrt-llm: A tensorrt toolbox for optimized\nlarge language model inference.", "metadata": {}}, {"text": "https://github.", "metadata": {}}, {"text": "com/NVIDIA/TensorRT-LLM.", "metadata": {}}, {"text": "[40] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka\nShah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini.", "metadata": {}}, {"text": "Splitwise: Efficient generative llm inference using phase\nsplitting, 2024.", "metadata": {}}, {"text": "[41] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang,\nYongwei Wu, Weimin Zheng, and Xinran Xu.", "metadata": {}}, {"text": "Moon-\ncake: A kvcache-centric disaggregated architecture for\nllm serving.", "metadata": {}}, {"text": "arXiv preprint arXiv:2407.00079, 2024.", "metadata": {}}, {"text": "[42] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,\nSinong Wang, and Jie Tang.", "metadata": {}}, {"text": "Blockwise self-attention\nfor long document understanding.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1911.02972, 2019.", "metadata": {}}, {"text": "[43] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier.", "metadata": {}}, {"text": "Efficient content-based sparse attention\nwith routing transformers.", "metadata": {}}, {"text": "Transactions of the Associa-\ntion for Computational Linguistics, 9:53–68, 2021.", "metadata": {}}, {"text": "[44] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christopher\nRé, Ion Stoica, and Ce Zhang.", "metadata": {}}, {"text": "Flexgen: High-throughput\ngenerative inference of large language models with a\nsingle gpu.", "metadata": {}}, {"text": "In Proceedings of the 40th International\nConference on Machine Learning, ICML’23.", "metadata": {}}, {"text": "JMLR.org,\n2023.", "metadata": {}}, {"text": "[45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.", "metadata": {}}, {"text": "Megatron-lm: Training multi-billion parameter language\nmodels using model parallelism, 2020.", "metadata": {}}, {"text": "[46] Franyell Silfa, Jose Maria Arnau, and Antonio González.", "metadata": {}}, {"text": "E-batch: Energy-efficient and high-throughput rnn\nbatching.", "metadata": {}}, {"text": "ACM Trans.", "metadata": {}}, {"text": "Archit.", "metadata": {}}, {"text": "Code Optim., 19(1), jan\n2022.", "metadata": {}}, {"text": "[47] Shrihari Sridharan, Jacob R.", "metadata": {}}, {"text": "Stevens, Kaushik Roy, and\nAnand Raghunathan.", "metadata": {}}, {"text": "X-former: In-memory acceleration\nof transformers.", "metadata": {}}, {"text": "IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, 31(8):1223–1233, 2023.", "metadata": {}}, {"text": "[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\ntiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample.", "metadata": {}}, {"text": "Llama: Open and efficient foun-\ndation language models, 2023.", "metadata": {}}, {"text": "[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N.", "metadata": {}}, {"text": "Gomez, Łukasz Kaiser,\nand Illia Polosukhin.", "metadata": {}}, {"text": "Attention is all you need.", "metadata": {}}, {"text": "In\nProceedings of the 31st International Conference on\nNeural Information Processing Systems, NIPS’17, page\n6000–6010, Red Hook, NY , USA, 2017.", "metadata": {}}, {"text": "Curran Asso-\nciates Inc.", "metadata": {}}, {"text": "[50] Samuel Williams, Andrew Waterman, and David Pat-\nterson.", "metadata": {}}, {"text": "Roofline: An insightful visual performance\nmodel for multicore architectures.", "metadata": {}}, {"text": "Commun.", "metadata": {}}, {"text": "ACM ,\n52(4):65–76, apr 2009.", "metadata": {}}, {"text": "[51] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang,\nXuanzhe Liu, and Xin Jin.", "metadata": {}}, {"text": "Fast distributed inference\nserving for large language models, 2023.", "metadata": {}}, {"text": "[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis.", "metadata": {}}, {"text": "Efficient streaming language\nmodels with attention sinks, 2024.", "metadata": {}}, {"text": "[53] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh\nMarina.", "metadata": {}}, {"text": "Moe-infinity: Offloading-efficient moe model\nserving, 2024.", "metadata": {}}, {"text": "[54] Xiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen.", "metadata": {}}, {"text": "Re-\ntransformer: Reram-based processing-in-memory archi-\ntecture for transformer acceleration.", "metadata": {}}, {"text": "In Proceedings of\n15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "the 39th International Conference on Computer-Aided\nDesign, ICCAD ’20, New York, NY , USA, 2020. Asso-\nciation for Computing Machinery.\n[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming\nZhuang, Weifeng Zhang, Dharmesh Jani, and Peipei\nZhou. Challenges and opportunities to enable large-\nscale computing via heterogeneous chiplets. In 2024\n29th Asia and South Pacific Design Automation Confer-\nence (ASP-DAC), pages 765–770. IEEE, 2024.\n[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. Bp-transformer: Modelling long-\nrange context via binary partitioning. arXiv preprint\narXiv:1911.04070, 2019.\n[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun. Orca: A distributed\nserving system for Transformer-Based generative mod-\nels. In 16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22), pages 521–538,\n2022.\n[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\nformer language models, 2022.\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-\nServe: Disaggregating prefill and decoding for goodput-\noptimized large language model serving. In 18th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 24), pages 193–210, 2024.\n16", "sentences": [{"text": "the 39th International Conference on Computer-Aided\nDesign, ICCAD ’20, New York, NY , USA, 2020.", "metadata": {}}, {"text": "Asso-\nciation for Computing Machinery.", "metadata": {}}, {"text": "[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming\nZhuang, Weifeng Zhang, Dharmesh Jani, and Peipei\nZhou.", "metadata": {}}, {"text": "Challenges and opportunities to enable large-\nscale computing via heterogeneous chiplets.", "metadata": {}}, {"text": "In 2024\n29th Asia and South Pacific Design Automation Confer-\nence (ASP-DAC), pages 765–770.", "metadata": {}}, {"text": "IEEE, 2024.", "metadata": {}}, {"text": "[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang.", "metadata": {}}, {"text": "Bp-transformer: Modelling long-\nrange context via binary partitioning.", "metadata": {}}, {"text": "arXiv preprint\narXiv:1911.04070, 2019.", "metadata": {}}, {"text": "[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun.", "metadata": {}}, {"text": "Orca: A distributed\nserving system for Transformer-Based generative mod-\nels.", "metadata": {}}, {"text": "In 16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22), pages 521–538,\n2022.", "metadata": {}}, {"text": "[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer.", "metadata": {}}, {"text": "Opt: Open pre-trained trans-\nformer language models, 2022.", "metadata": {}}, {"text": "[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.", "metadata": {}}, {"text": "Dist-\nServe: Disaggregating prefill and decoding for goodput-\noptimized large language model serving.", "metadata": {}}, {"text": "In 18th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 24), pages 193–210, 2024.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}]}