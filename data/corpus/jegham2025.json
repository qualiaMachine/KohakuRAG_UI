{"document_id": "jegham2025", "title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference", "text": "How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1∗\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 × the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel’s total lifecycle energy use [14, 15].\n∗Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025\n\nDespite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI’s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o’s\nannual environmental footprint and section 7 analyzes the impact of GPT-5’s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3’s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta’s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs’ TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2\n\nnot include LLMs, API-based deployments, or essential infrastructure considerations like PUE and\nWUE.\nComplementary studies, including Luccioni et al. [ 28], assess general-purpose and task-specific\nmodels in the A100 systems. While they provide valuable cross-model insights, they do not consider\nproprietary models, water usage, or carbon emissions. CodeCarbon [15] calculates carbon footprints\nbased on device-level data and regional carbon intensity, but it lacks the granularity needed for\nprompt-level analysis and does not work with API-based inferences. On a larger scale, Harding\net al. [ 29] connect AI adoption to national productivity, allowing for extrapolation of energy and\ncarbon effects. Though this provides a useful overarching view, it overlooks variability in per-prompt\ninference, the behavior of specific models, and the infrastructure used for deployment.\nMost efforts focus on training and local model evaluation, lacking standardized, scalable methods,\nignoring infrastructural overhead, and omitting resource categories such as water consumption and\ncarbon emissions. Our work addresses these gaps by integrating API-based performance metrics with\nGPU and system power specifications and environmental multipliers to estimate the environmental\nimpact of LLM inference at the prompt level in data centers. We infer deployment infrastructure\nthrough statistical analysis and apply DEA to contextualize environmental impact versus performance.\nAdditionally, we conduct two case studies estimating GPT-4o’s annual environmental footprint based\non scaled usage data and analyzing the impact of GPT-5’s adapative model routing, providing the\nfirst infrastructure-aware, prompt-level benchmark of inference sustainability at scale.\n3 Preliminaries\nTo capture infrastructure-level overhead in data center operations, we apply three standard environ-\nmental multipliers: Power Usage Effectiveness (PUE) [19, 20], Water Usage Effectiveness (WUE)\n[19, 20], and Carbon Intensity Factor (CIF) [21, 22].\nPUE accounts for non-computational energy overheads such as cooling, lighting, and power distribu-\ntion. Defined as the ratio of total data center energy consumption to IT-specific energy use.\nWUE captures the water used per kilowatt-hour of IT energy, encompassing on-site cooling (Scope\n1), off-site electricity generation (Scope 2), and embodied water from hardware manufacturing and\ntransport (Scope 3). WUE can be computed based on either water withdrawal (the total volume\ndrawn from natural or municipal sources) or water consumption (the portion of withdrawn water\npermanently lost, primarily through evaporation).\nCIF measures carbon emissions per kilowatt-hour of energy consumed, largely driven by the regional\nelectricity mix. Emissions are categorized as direct on-site combustion (Scope 1), off-site electricity\ngeneration (Scope 2), and embodied emissions from manufacturing and transport (Scope 3).\n4 Methodology\nThis section presents our novel methodology for estimating the environmental footprint of LLM\ninference. Our framework integrates model-specific performance metrics with infrastructure-level\nenvironmental multipliers to calculate operational energy consumption, water usage, and carbon\nemissions per query. We also evaluate eco-efficiency using DEA, mapping sustainability trade-offs\nagainst a composite performance benchmark, and develop an interactive dashboard for a more\nthorough analysis.\n4.1 Model Selection and Hardware Estimation\nWe analyze 30 large language models across OpenAI, Anthropic, Meta, and DeepSeek. Table 1\nsummarizes each model’s deployment context, including provider, cloud host, hardware type and\nspecifications, and company-specific environmental multipliers (PUE, WUE, CIF). All models are\nusually run on NVIDIA DGX systems using A100, H100, H200, or H800 GPUs [30, 45, 46, 47, 48].\nU.S.-based providers such as OpenAI and Anthropic have acquired large volumes of H200 and H100\nchips [31, 41, 42], making them the most probable choice for recent deployments. DeepSeek, which\noperates under U.S. export restrictions, uses the H800, NVIDIA’s export-compliant GPU for the\nChinese market [38, 49]. Both the H200 and H800 retain the same Hopper architecture and peak\n3\n\nTable 1: Deployment and infrastructure specifications of models.\nModel LaunchDate Company Host Hardware CriticalPower(kW) PUE WUE(on-site, L/kWh)WUE(off-site, L/kWh)CIF(kgCO2e/kWh)\nGPT-4.1 Apr, 2025\nOpenAI Microsoft Azure DGX H200/H100 [30, 31] 10.20 [32] 1.12 [33] 0.30 [34] 4.35 [35] 0.35 [36]\nGPT-4.1 mini Apr, 2025GPT-4.1 nano Apr, 2025o4-mini (high) Apr, 2025o3 Apr, 2025o3-mini (high) Jan, 2025o3-mini Jan, 2025o1 Dec, 2024o1-mini Sep, 2024GPT-4o (Mar ’25) May, 2024GPT-4o mini July, 2024OpenAI Microsoft AzureDGX A100* 6.50[37] 1.12 0.30 4.35 0.35GPT-4 Turbo Nov, 2023GPT-4 Mar, 2023DeepSeek-R1 Jan, 2025Deepseek Deepseek DGX H800 [8] 10.20 [38] 1.27 [39] 1.20 [39] 6.016 [35] 0.6 [40]DeepSeek-V3 Dec, 2024DeepSeek-R1 Jan, 2025Deepseek Microsoft Azure DGX H200/H100 10.20 1.12 0.30 4.35 0.35DeepSeek-V3 Dec, 2024Claude-3.7 Sonnet Feb, 2025\nAnthropic AWS DGX H200/H100 [41, 42] 10.20 1.14 [43] 0.18 [43] 5.11 [35] 0.287 [44]Claude-3.5 Sonnet Jun, 2024Claude-3.5 Haiku Nov, 2024LLaMA-3.3 70B Dec, 2024\nMeta AWS DGX H200/H100 10.20 1.14 0.18 5.11 0.287\nLLaMA-3.2-vision 90B Sep, 2024LLaMA-3.2-vision 11B Sep, 2024LLaMA-3.2 3B Sep, 2024LLaMA-3.2 1B Sep, 2024LLaMA-3.1-405B Jul, 2024LLaMA-3.1-70B Jul, 2024LLaMA-3.1-8B Jul, 2024LLaMA-3-70B Apr, 2024LLaMA-3-8B Apr, 2024\n*DGX A100 was estimated for GPT-4o mini, GPT-4 Turbo, and GPT-4. Justification and estimation details are provided in Section 4.3.1.\npower draw as the H100, with system-level energy characteristics that are nearly identical [50]. While\nthe H200 achieves greater energy efficiency due to faster memory and higher bandwidth, and the\nH800 may exhibit reduced performance due to export-related firmware limitations, both maintain\nthe same peak power draw, thermal design profile, and system-level utilization characteristics as the\nH100 [38, 50]. These architectural differences affect throughput and latency, resulting in higher or\nlower energy consumed per token, but do not impact total system power demand under load. We\ntherefore treat H100, H200, and H800 as equivalent in our power modeling, since our estimates are\nbased on power draw and utilization rather than task-level performance.\nEnvironmental multipliers such as PUE, WUE, and CIF are assigned according to each cloud\nprovider’s data center locations and corresponding regional grid characteristics. For OpenAI and\nDeepSeek models hosted on Microsoft Azure, we use Azure-reported PUE and site-level WUE values,\nwhile CIF and source-level WUE are derived from the specific geographic locations of Microsoft\ndata centers around the world. For AWS-hosted models, including those from Anthropic and Meta,\nwe apply AWS-reported PUE and site-level WUE, and compute CIF and source-level WUE based\non the regional distribution of AWS data centers used for inference. For DeepSeek models that are\ndeployed in Chinese datacenters, we adopt the average PUE and site-level WUE of the thirty most\nefficient data centers in China, while CIF and source-level WUE are determined using the regional\nlocations of its known or reported data center deployments.\n4.2 Per-Query Energy Consumption Estimation\nTo quantify the energy required for a single inference, we introduce a probabilistic framework that\ncaptures the stochastic nature of LLM workloads. The model integrates standardized performance\ndata [51], which report latency to first-token generation (L) and tokens-per-second (TPS, denoted R)\nacross empirical quantiles (5th, 25th, 50th, 75th, and 95th percentiles) and three representative prompt\nconfigurations: short-form (100 input, 300 output tokens), medium (1,000 input, 1,000 output), and\nlong-form (10,000 input, 1,500 output), reflecting variability across multiple test runs for each model\nand prompt configuration.\nTo model realistic runtime behavior, we construct a joint distribution ofL and R using a Gaussian\ncopula with correlation coefficient ρ = −0.3, capturing the negative dependence typically observed\nbetween latency and TPS. From this distribution, we draw 10,000 correlated samples (Li, Ri), each\nrepresenting one plausible inference scenario. The culmination of this infrastructure-aware framework\nis the introduction of our novel formula to precisely estimate the per-query energy consumption:\n4\n\nLet Li captures the initialization latency and Output Length\nRi\nrepresents the time it takes to generate the\nresponse. Also, let PGPU and Pnon-GPU denote the rated power draw (in kW) of the GPU subsystem\nand the non-GPU subsystem (e.g., CPUs, SSDs, network, and cooling control electronics), respec-\ntively. The parameters UGPU,min and UGPU,max represent the minimum and maximum GPU utilization\nfractions observed during inference, while Unon-GPU represents the average utilization fraction for\nnon-GPU components. PUE factor is also incorporated to account for datacenter-level overheads.\nWe compute energy consumption at the lower and upper utilization bounds as:\nEi,{min,max} =\n\nLi + Output Length\nRi\n3600\n!\n| {z }\nTotal inference time (Ti, hours)\n×\n\nPGPU × UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU × Unon-GPU| {z }\nNon-GPU power (kW)\n\n × PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes. The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 − wmax)Ei,min, ¯Equery = E[Ei,exp], σ Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.\n4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7–20B),\nSmall (20–40B), Medium (40–70B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.\nModels that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled “Nano” such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].\nAI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.\nBenchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios. A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100\nsystems, we adopt measurements from Patel et al. and Kakolyris et al.’s work [56, 57]. Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G × DGPU\nN × B , U non-GPU total = G × Dnon-GPU\nN × B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size. DGPU denotes the assigned GPUs’ power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes. Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.\n5\n\nTable 2: Estimated node-level GPU and non-GPU utilization by model class for H100 and A100.\nClass GPU\nCount\nDGPU\n(H100)\nDGPU\n(A100)\nUGPU total\n(H100)\nUGPU total\n(A100) Unon-GPU total\nNano 1 35–65% 80–90% 0.55–1.00% 1.25–1.5% 0.87%\nMicro 1 50–80% 90–100% 0.75–1.25% 1.5–1.6% 0.87%\nSmall 2 55–80% N/A 1.70–2.50% N/A 1.6%\nMedium 4 50–70% 100–110% 3.00–4.50% 6.25–7% 3.125%\nLarge 8 45–60% 100–120% 5.50–7.50% 12.5–15.0% 6.25%\nFigure 1: (Left) Mean energy consumption of GPT-4o and GPT-4o mini across providers and GPU\ntypes, measured by output size. (Right) Distribution of TPS (averaged across output sizes)\n4.3.1 GPT-4, GPT-4 Turbo, and GPT-4o mini Hardware Estimation\nIn our experiment, we observed a performance discrepancy: GPT-4o mini showed significantly\nlower throughput and higher latency on OpenAI’s API compared to Microsoft Azure under identical\nprompt settings, as shown in Figure 1. Both variants also underperformed relative to OpenAI’s\nGPT-4o, with 60% and 27% lower TPS, respectively. Given GPT-4o mini’s smaller size and H200’s\narchitectural advantages, its performance would be expected to match or exceed GPT-4o if served\non H200 infrastructure. The observed gap is inconsistent with H200 deployment and suggests that\nGPT-4o mini is running on A100 or H100 systems. Notably, Azure’s version outperforms OpenAI’s\nby 47% on average, further supporting the likelihood that Azure uses H100 and OpenAI retains\nA100. Therefore, to validate our hardware estimations, we tested this hypothesis using two-way\nANOV A and Tukey HSD (Table 3). At 300-token prompts, energy consumption was statistically\nsimilar across platforms, as expected given the small computational load. However, at larger output\nsizes, significant differences emerged: OpenAI’s presumed A100 deployment differed from Azure’s\nH100 deployment with p < 0.05, and Azure’s H100 also outperformed OpenAI’s assumed H100\nwith p < 0.05, reinforcing the likelihood that OpenAI’s GPT-4o mini is not served on H100. We\ntherefore consider GPT-4o mini to be running on A100. Additionally, with reports that GPT-4 was\ntrained and deployed on A100 systems [58], and given the architectural continuity between GPT-4\nand GPT-4 Turbo and their low throughput, high latency, and impending deprecation [59], we also\nconsider they are running on A100 architecture since it is unlikely that they have migrated to newer\nhardware.\nTable 3: Tukey HSD Adjusted p-values for energy consumption differences by provider, GPU system,\nand prompt size\nGroup 1 Group 2 300 tokens 1000 tokens 1500 tokens\nAzure (H100) OpenAI (A100) 0.979 0.0009 <0.0001\nAzure (H100) OpenAI (H100) 0.951 0.0001 <0.0001\n6\n\n[Image page=6 idx=1 name=Im1.png] Size: 2000x700, Data: 105459 bytes\n\n4.4 Per-Query Water Consumption and Carbon Emissions Estimation\nThis study focuses exclusively on operational emissions and resource consumption during the\ninference phase of the model. Accordingly, embodied emissions and water use from hardware\nmanufacturing and supply chains (Scope 3) are excluded due to their limited relevance to real-time\ndeployment and the risk of inflating per-query estimates when applied without deployment-specific\nattribution or when model lifecycles remain ongoing. For water usage, we focus solely on water\nconsumption (water permanently removed from the source). For carbon emissions, we exclude Scope\n1 emissions as they are generally negligible compared to Scope 2 emissions due to the infrequent\nuse of on-site fuel combustion for backup generators and facility heating in data centers [60]. For\nexample, Scope 1 emissions accounted for only 1.6% of Microsoft’s Scope 2 emissions in 2023 [36],\na figure that includes executive air travel, ground transportation, refrigerant leakage, and on-site fuel\nuse, further diminishing the share attributable to data center operations. Accordingly, our analysis\nfocuses exclusively on Scope 2 emissions, which capture the carbon intensity of electricity consumed\nduring inference. A more detailed discussion of these considerations is provided in Appendix B.\nWater consumption and carbon emissions per query are calculated as:\nWater (L) = Equery\nPUE · WUEsite\n| {z }\nOn-site cooling\n+ Equery · WUEsource\n| {z }\nOff-site electricity\n(4)\nCarbon (kgCO2e) = Equery · CIF (5)\n4.5 Eco-Efficiency via Data Envelopment Analysis (DEA)\nWe apply cross-efficiency DEA to evaluate the effectiveness of each model in converting environ-\nmental resources into functional intelligence. Inputs include per-query energy consumption, PUE,\nWUEsource, WUEsite, and CIF. The output is the Artificial Intelligence Index, a composite score\nweighted across multiple benchmark domains [ 51]. Specifically, reasoning and knowledge tasks\n(MMLU-Pro [61], HLE [62], GPQA [63]) collectively contribute 50% of the index (1/6 each); mathe-\nmatical proficiency (MATH-500 [64], AIME [65]) contributes 25% (1/8 each); and coding ability\n(SciCode [66], LiveCodeBench [67]) accounts for the remaining 25% (1/8 each).\nIn contrast to standard Charnes-Cooper-Rhodes (CCR) or Banker-Charnes-Cooper (BCC) models,\nwhich enable each model to choose its optimal weightings, sometimes inflating performance, cross-\nefficiency assesses each model based on its own and all peer weightings. This approach reduces\nself-evaluation bias and recognizes models that maintain strong performance from various efficiency\nviewpoints. The resulting scores offer a more robust and comparative measure of eco-efficiency. Full\nresults and additional discussion are provided in Appendix C.\n4.6 Power BI Dashboard\nTo democratize access to these novel assessments, we built and deployed an automated Power BI\ndashboard that runs our entire framework in real time, a first-of-its-kind tool for continuously tracking\nAI inference sustainability. The data are scraped daily from the Artificial Analysis website, cleaned\nautomatically, and then visualized on Power BI as seen in Figures 2a and 2b. The main dashboard\ndisplays the average and standard deviation of energy use, water consumption (site, source, and\ncombined), and carbon emissions for the three query sizes. It also visualizes latency and TPS\nfluctuations, benchmark results, and the total environmental impact when scaling up to 1, 50, or\n100 billion queries, compared with real-world equivalents such as household electricity use, annual\ndrinking needs, and transportation emissions. Users can filter by company, model size, query size, or\nsustainability metric, and download the full dataset. Additionally, the dashboard tracks day-to-day\nchanges in each model’s footprint, visualizing time-series trends and the average in energy, water,\nand carbon metrics across data centers and hardware setups. It includes an extended list of models\nbeyond those analyzed in this study and automatically incorporates new ones as they are released,\nallowing continuous monitoring of inference-phase sustainability and cross-model comparisons over\ntime.\n7\n\n(a) Overview of the main dashboard displaying the\nenergy consumption per model, latency, TPS, bench-\nmark scores, and equivalent environmental impacts\nfor an example model (GPT-5 minimal).\n(b) Overview of the timeseries dashboard displaying\naverage energy consumption per model, and the daily\nfluctuations of the selected model (Grok 4).\nFigure 2: Visual overview of the AI sustainability dashboard.\n5 Experimental Evaluation\nWe benchmark the environmental footprint of 30 LLMs across three modalities: Energy consumption,\nwater usage, and carbon emissions, based on equations 2, 4, and 5, respectively. For the long-form\nquery evaluation, GPT-4 and LLaMA-3 (8B and 70B) are excluded due to context window limitations.\n5.1 Energy Consumption\nFigure 3: Energy consumption per model across\nthree prompt sizes (Wh, log-scale).\nTable 4: Energy consumption (mean ± std\ndev) per model across three prompt sizes\n(Wh).\nModel Energy Consumption(100 input-300 output)(Wh)\nEnergy Consumption(1k input-1k output)(Wh)\nEnergy Consumption(10k input-1.5k output)(Wh)GPT-4.1 0.871 ± 0.302 3.161 ± 0515 4.833 ± 0.650GPT-4.1 mini 0.450 ± 0.081 1.545 ± 0.211 2.122 ± 0.348GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094o4-mini (high) 3.649 ± 1.468 7.380 ± 2.177 7.237 ± 1.674o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082o3-mini (high) 3.012 ± 0.991 6.865 ± 1.33 5.389 ± 1.183o3-mini 0.674 ± 0.015 2.423 ± 0.237 3.525 ± 0.168o1 2.268 ± 0.654 4.047 ± 0.497 6.181 ± 0.877o1-mini 0.535 ± 0.182 1.547 ± 0.405 2.317 ± 0.530GPT-4o (Mar ’25) 0.423 ± 0.085 1.215 ± 0.241 2.875 ± 0.421GPT-4o mini 0.577 ± 0.139 1.897 ± 0.570 3.098 ± 0.639GPT-4 Turbo 1.699 ± 0.355 5.940 ± 1.441 9.877 ± 1.304GPT-4 1.797 ± 0.259 6.925 ± 1.553 —DeepSeek-R1 (DS)* 19.251 ± 9.449 24.596 ± 9.4 29.078 ± 9.725DeepSeek-V3 (DS)* 2.777 ± 0.223 8.864 ± 0.724 13.162 ± 1.126DeepSeek-R1 (AZ)† 2.353 ± 1.129 4.331 ± 1.695 7.410 ± 2.159DeepSeek-V3 (AZ)† 0.742 ± 0.125 2.165 ± 0.578 3.696 ± 0.221Claude-3.7 Sonnet 0.950 ± 0.040 2.989 ± 0.201 5.671 ± 0.302Claude-3.5 Sonnet 0.973 ± 0.066 3.638 ± 0.256 7.772 ± 0.345Claude-3.5 Haiku 0.975 ± 0.063 4.464 ± 0.283 8.010 ± 0.338LLaMA-3-8B 0.108 ± 0.002 0.370 ± 0.005 —LLaMA-3-70B 0.861 ± 0.022 2.871 ± 0.094 —LLaMA-3.1-8B 0.052 ± 0.008 0.172 ± 0.015 0.443 ± 0.028LLaMA-3.1-70B 1.271 ± 0.020 4.525 ± 0.053 19.183 ± 0.560LLaMA-3.1-405B 2.226 ± 0.142 9.042 ± 0.385 25.202 ± 0.526LLaMA-3.2 1B 0.109 ± 0.013 0.342 ± 0.025 0.552 ± 0.059LLaMA-3.2 3B 0.143 ± 0.006 0.479 ± 0.017 0.707 ± 0.020LLaMA-3.2-vision 11B 0.078 ± 0.021 0.242 ± 0.071 1.087 ± 0.060LLaMA-3.2-vision 90B 1.235 ± 0.054 4.534 ± 0.448 6.852 ± 0.780LLaMA-3.3 70B 0.237 ± 0.023 0.760 ± 0.079 1.447 ± 0.188\n* DeepSeek Host\n† Microsoft Azure Host\nFigure 3 and Table 4 highlight how energy consumption scales with prompt length and model\narchitecture, revealing wide disparities across systems. LLaMA-3.1-8B is the most efficient, requiring\nonly 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output),\nfollowed by LLaMA-3.2 1B and LLaMA-3.2 3B at 0.552 Wh and 0.707 Wh, respectively. GPT-\n4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly\ntwice the energy of LLaMA-3.1-8B. In contrast, DeepSeek-R1 (DS) consumes 29.075 Wh, around\nsixty five times more than the most efficient model, underscoring the large overhead of reasoning\nmodels.\n8\n\n[Image page=8 idx=1 name=Im2.png] Size: 1496x829, Data: 280459 bytes\n\n[Image page=8 idx=2 name=Im3.png] Size: 1509x841, Data: 154154 bytes\n\n[Image page=8 idx=3 name=Im4.png] Size: 1545x1891, Data: 576342 bytes\n\nThe LLaMA family shows clear scaling effects: energy use rises from 0.443 Wh at 8B parameters\nto 25.202 Wh at 405B, illustrating steep power demands at high parameter counts. Additionally,\nthe DeepSeek models reveal striking infrastructure effects. DeepSeek-R1 and DeepSeek-V3 hosted\non DeepSeek’s own servers consume 29.078 Wh and 13.162 Wh, while the same models on Azure\nuse just 7.410 Wh and 3.696 Wh, over 70% less energy. This gap highlights that hardware and\ndata center efficiency, not model design alone, drives real-world energy use. For context, a single\nlong query to DeepSeek-R1 (DS) consumes about as much electricity as running a 65-inch LED\ntelevision (≈ 130 W) for roughly 13 minutes. GPT-4o and GPT-4o mini also show that infrastructure\ncan outweigh model size in determining energy efficiency. For instance GPT-4o consumes around\n2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on\nA100 hardware instead of H100s.\n5.2 Water and Carbon Emissions\n(a) Water consumption per model across three prompt\nsizes (ml, log-scale).\n(b) Carbon emissions per model across three prompt\nsizes (gCO2e, log-scale)\nFigure 4: Water consumption and carbon emissions per model.\nFigure 4 showcases the water consumption and carbon emissions of models across all prompt sizes.\nThe most resource-efficient systems, such as LLaMA-3.2 1B, LLaMA-3.2 3B, LLaMA-3.1-8B,\nLLaMA-3-8B, and GPT-4.1 nano, emit less than 0.3 gCO2e and consume under 4 mL of water even\nfor long-form prompts, demonstrating exceptional sustainability across scales.\nIn contrast, large-scale and reasoning models such as o3, DeepSeek-R1 (DS), and DeepSeek-V3 (DS)\nexhibit substantially higher footprints. DeepSeek-R1 (DS) consumes over 200 mL of water and emits\napproximately 17 gCO2e per long query, while the same model on Azure consumes only 34 mL and\nemits 2.5 gCO2e, a reduction of nearly 85%. These figures suggest that environmental impacts are\nshaped not only by model architecture but also by deployment strategies and regional infrastructure\nconditions. In particular, the elevated emissions and water usage observed in DeepSeek models likely\nreflect inefficiencies in their data centers, including higher PUE, suboptimal cooling technologies,\nand less efficient hardware.\nWhile these per-query values may seem modest when isolated, their impact becomes considerable\nat scale. A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit\nas much carbon as thousands of transatlantic flights and consume water equivalent to the annual\ndrinking needs of millions of people. We revisit this scaling analysis in greater detail in Section 6.\n9\n\n[Image page=9 idx=1 name=Im5.png] Size: 1545x1833, Data: 544218 bytes\n\n[Image page=9 idx=2 name=Im6.png] Size: 1545x1833, Data: 565339 bytes\n\nFigure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total\nannual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.\n5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference. Our framework estimates 0.42 Wh ( ±0.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman’s figure. Similarly, the results for Mistral Large 2\nalign closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e,\n±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation. Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.\n6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025. Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5. A single short GPT-4o query consumes 0.42\nWh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.\nScaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (±0.358 Wh). For\nmedium-length queries, this increases to 9.71 Wh (±1.106 Wh). These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI. While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.\n6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\nGiven GPT-4o’s status as the default model, we conservatively attribute 700 million daily queries to\n10\n\n[Image page=10 idx=1 name=Im7.png] Size: 2400x1200, Data: 390607 bytes\n\nGPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT’s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\nWithin these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns. Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of\n35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.\n6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks. Based on scaled inference volumes, GPT-4o’s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people. Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.\n6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o’s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5. Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London. In sequestration terms, offsetting GPT-4o’s\nannual emissions would require over 138,000 acres of average U.S. forest, an area roughly equivalent\nto the size of Chicago. These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden. This burden is only expected to grow\nas AI usage continues to scale.\n7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive “Thinking”\nmodel for complex reasoning tasks. This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.\nHowever, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6. For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase. Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels. For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o’s\n0.42 Wh per short prompt. Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.\nThese results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts. This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.\n11\n\nFigure 6: Energy consumption of GPT-5 across query lengths and reasoning modes\n8 Discussion and Policy Implications\n8.1 The Critical Role of Infrastructure in AI Sustainability\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\ngains can stem as much from datacenter design as from model optimization. These observations\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\nrenewable energy sources, and infrastructure-aware deployment strategies.\n8.2 Rebound Effects and the Jevons Paradox\nAlthough large language models consume significantly less energy, water, and carbon per task than\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\nAs per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource\nconsumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives\nsystemic demand. The acceleration and affordability of AI remove traditional human and resource\nconstraints, enabling unprecedented levels of usage. Consequently, the cumulative environmental\nburden threatens to overwhelm the sustainability baselines that AI efficiency improvements initially\nsought to mitigate. As such, sustainable AI deployment must focus on systemic frameworks that\nassess how well models balance capability with environmental cost. In response, we propose DEA as\na principled method for benchmarking model-level eco-efficiency.\n8.3 Policy Implications\nAs AI systems scale globally, ensuring environmental sustainability requires both model-level\noptimizations and systemic regulation of infrastructure. Government agencies should encourage\nthresholds on the permissible environmental footprint per inference regarding energy, water, and\ncarbon emissions that AI models must not exceed. These thresholds can be met through architectural\ninnovations, such as sparsity and quantization, or through infrastructure-level optimizations like more\nefficient hardware, cleaner energy sourcing, and improved cooling systems. Our methodology offers a\nstandardized, scalable framework to quantify these efforts. Incorporating technologies like dielectric\nliquid cooling offers a promising path to reduce or eliminate water use in data centers drastically [77].\nTransparency must also be elevated through system-level reporting of per-inference energy, water,\nand carbon metrics. Additionally, deployment strategies, such as batching, should be integrated into\nsustainability planning, as larger batch sizes can reduce per-query energy use by improving hardware\nutilization with only minimal impact on latency.\n12\n\n[Image page=12 idx=1 name=Im8.png] Size: 2400x800, Data: 104610 bytes\n\n9 Conclusion, Limitations, and Future Work\nThis paper introduces the first large-scale, infrastructure-aware framework for benchmarking the\nenvironmental footprint of LLM inference, integrating API performance, environmental multipliers,\nand statistical inference to assess energy, water, and carbon costs under real-world conditions.\nBy applying cross-efficiency DEA, we contextualize environmental impact in terms of functional\nperformance, revealing that eco-efficiency hinges not only on model design but also on infrastructure.\nOur GPT-4o case study emphasizes the Jevons Paradox: As AI becomes cheaper and faster, total\nusage expands, intensifying environmental strain despite gains in per-query efficiency. Additionally,\nour GPT-5 case study sheds lights on the importance of prompt-level efficiency and adaptive routing.\nWithout structural shifts in how LLMs are designed, deployed, and used, these invisible costs will\ncontinue to rise, threatening to offset the societal benefits that made these systems valuable in the first\nplace. This work establishes a standardized, scalable framework for benchmarking the environmental\nfootprint of LLM inference in real-world data center deployments, providing a basis for transparent,\ninfrastructure-aware sustainability assessment and future regulation.\nOur work inherits certain limitations that we acknowledge: we avoid overstating model-specific\nfootprints by conservatively including only the energy drawn by actively assigned GPUs. This\nis due to the lack of means to determine whether unused GPUs’ capacity is reassigned, load-\nbalanced, or left inactive. Isolating non-GPU power consumption was also difficult. We applied\na fixed utilization estimate from prior studies, acknowledging that their variation across inference\nworkloads is typically significantly lower than that of GPUs. Moreover, for proprietary models\nwithout disclosed size, we classified their scale based on observed API performance. Future work\nshould address these limitations as more detailed telemetry and facility-level reporting become\navailable. Additionally, future studies should also extend beyond text generation to evaluate image,\nvideo, and audio generation, which are likely to impose greater environmental costs due to higher\ncomputational intensity.\nReferences\n[1] Google Inc. How google is integrating generative ai into search. https://blog.google/\nproducts/search/generative-ai-search-update/ , 2023.\n[2] Chong Qin, Zheng Liu, Huisi Wang, Wanchuan Zhou, Xipeng Sun, and Xuanjing Qiu. Toolllm:\nFacilitating language models to master 160+ tools. arXiv preprint arXiv:2309.12288, 2023.\n[3] Erin Hannan and Shuguang Liu. Ai: new source of competitiveness in higher education.\nCompetitiveness Review: An International Business Journal, 33(2):265–279, 2023.\n[4] Pranav Rajpurkar, James Yang, Henry Hope, and Yongqun Yu. The ai-assisted doctor: The\nimpact of large language models on medicine. Nature Medicine, 29(4):592–600, 2023.\n[5] OpenAI. Gpt-4o: Openai’s multimodal flagship model. https://openai.com/index/gpt-\n4o, 2024.\n[6] Anthropic. Claude 3: Next-generation language models from anthropic. https://www.\nanthropic.com/news/claude-3-family, 2024.\n[7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama\n3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n[8] DeepSeek AI. Deepseek v3: Open-source llms for multilingual and multimodal tasks. https:\n//deepseek.com, 2024.\n[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[10] OpenAI. Gpt-o1 model card. https://openai.com/o1/, 2024.\n[11] OpenAI. Gpt-o3 and o3-mini: Multimodal instruction-tuned models by openai. https:\n//openai.com/index/openai-o3-mini/, 2025.\n13\n\n[12] David Patterson, Joseph Gonzalez, Quoc V . Le, Chen Liang, Xinlei Chen, and Andrew Ng.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\n[13] Shaolei Li. Making ai less “thirsty”: Uncovering and addressing the secret water footprint of ai\nmodels. arXiv preprint arXiv:2304.03271, 2023.\n[14] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. Trends in ai\ninference energy consumption: Beyond the performance-vs-parameter laws of deep learning.\nSustainable Computing: Informatics and Systems, 38:100857, 2023.\n[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Codecarbon:\nEstimate and track carbon emissions from machine learning training. https://github.com/\nmlco2/codecarbon, 2022.\n[16] Microsoft Corporation. 2024 environmental sustainability report. https://www.microsoft.\ncom/en-us/corporate-responsibility/sustainability/report, May 2024.\n[17] Google. 2024 environmental report. https://sustainability.google/reports/google-\n2024-environmental-report/, July 2024.\n[18] Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, and Sagar Sen. The price of prompting:\nProfiling energy use in large language models inference. arXiv preprint arXiv:2407.16893,\n2024.\n[19] The Green Grid. PUE™ : A Comprehensive Examination of the Metric. February 2012. White\nPaper 49.\n[20] International Organization for Standardization (ISO) and International Electrotechnical Com-\nmission (IEC). Information technology – Data centres – Key performance indicators – Part\n2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/\n63211.html.\n[21] U.S. Environmental Protection Agency (EPA). Emissions & Generation Resource Integrated\nDatabase (eGRID). https://www.epa.gov/egrid, 2025.\n[22] International Energy Agency (IEA). Emissions Factors. 2025.\n[23] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 13693–13696, 2020.\n[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[26] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to\nwatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High\nPerformance Extreme Computing Conference (HPEC), pages 1–9. IEEE, 2023.\n[27] Zeyu Yang, Karel Adamek, and Wesley Armour. Double-exponential increases in inference\nenergy: The cost of the race for accuracy. arXiv preprint arXiv:2412.09731, 2024.\n[28] Sasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the\ncost of ai deployment? In Proceedings of the 2024 ACM conference on fairness, accountability,\nand transparency, pages 85–99, 2024.\n[29] Anthony Harding and Juan Moreno-Cruz. Watts and bots: The energy implications of ai\nadoption. arXiv preprint arXiv:2409.06626, 2024.\n14\n\n[30] Dallin Grimm. Nvidia ceo hand-delivers world’s fastest ai system to openai. https://www.\ntomshardware.com/tech-industry/artificial-intelligence/, April 2024.\n[31] NVIDIA. NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows.\nhttps://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-\nas-demand-for-ai-grows , March 2023.\n[32] Imran Latif, Alex C. Newkirk, Matthew R. Carbone, Arslan Munir, Yuewei Lin, Jonathan\nKoomey, Xi Yu, and Zhihua Dong. Single-node power demand during ai training: Measurements\non an 8-gpu nvidia h100 system. IEEE Access, 13:61740–61747, 2025. doi: 10.1109/ACCESS.\n2025.3554728.\n[33] Noelle Walsh. How microsoft measures datacenter water and energy use to improve azure\ncloud sustainability. https://azure.microsoft.com/blog/how-microsoft-measures-\ndatacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/ ,\nApril 2022. Microsoft Azure Blog.\n[34] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for\ncooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/\nsustainable-by-design-next-generation-datacenters-consume-zero-water-\nfor-cooling/, December 2024. Microsoft Cloud Blog.\n[35] World Resources Institute. Guidance for calculating water use embedded in purchased electricity.\nTechnical report, World Resources Institute, 2024.\n[36] Microsoft Corporation. 2024 environmental sustainability report data fact sheet. https:\n//cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/\ndocuments/presentations/CSR/2024-Environmental-Sustainability-Report-\nData-Fact.pdf, May 2024. Comprehensive environmental metrics including greenhouse gas\nemissions, energy consumption, water usage, waste management, and land protection for fiscal\nyear 2023.\n[37] NVIDIA Corporation. Nvidia dgx a100: The universal system for ai infrastruc-\nture. https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-\na100-datasheet.pdf, 2020. Datasheet detailing specifications and features of the NVIDIA\nDGX A100 system.\n[38] NVIDIA Corporation. Nvidia dgx h800 system. https://viperatech.com/shop/nvidia-\ndgx-h800-systems/, 2024. High-performance AI system featuring 8x NVIDIA H800 GPUs,\n640 GB HBM3 memory, and up to 32 petaFLOPS FP8 performance.\n[39] Hequan Wu. Academician hequan wu: Green and low-carbon development of data centers\nrequires multi-dimensional coordination of “source, grid, load, and storage”. https://www.\ncace.org.cn/News/NContent?key=04e714e4e006d433617f5d7148df2eb0, April 2024.\nChina Communications Enterprise Association News.\n[40] Wenli Ni, Xiurong Hu, Hongyang Du, Yulin Kang, Yi Ju, and Qunwei Wang. Co2 emission-\nmitigation pathways for china’s data centers. Resources, Conservation and Recycling, 202:\n107383, 2024.\n[41] AWS News Blog. New amazon ec2 p5 instances powered by nvidia h100 tensor core gpus\nfor accelerating generative ai and hpc applications. https://aws.amazon.com/blogs/aws/\nnew-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-\nfor-accelerating-generative-ai-and-hpc-applications/ .\n[42] AWS News Blog. New amazon ec2 p5e instances with nvidia h200 tensor core gpus\nand efav3 networking. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-\ninstances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking , 2024.\n[43] Amazon.com, Inc. 2023 amazon sustainability report. Technical report, Amazon.com, Inc.,\n2024.\n[44] Electricity Maps. Electricity maps — live carbon intensity map. https://app.\nelectricitymaps.com/map/, 2025.\n15\n\n[45] NVIDIA Corporation. NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX\nH100 Systems – Electrical Specifications, October 2024.\n[46] Arman Shehabi, Sarah J. Smith, Nathaniel Horner, Inês Azevedo, Richard Brown, Jonathan\nKoomey, Eric Masanet, Dale Sartor, Magnus Herrlin, and William Lintner. 2024 united states\ndata center energy usage report. Technical report, Lawrence Berkeley National Laboratory,\nDecember 2024.\n[47] Rani Borkar. Microsoft and nvidia partnership continues to deliver on the\npromise of ai. https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-\npartnership-continues-to-deliver-on-the-promise-of-ai/ , March 2024. Mi-\ncrosoft Azure Blog.\n[48] NVIDIA. Project ceiba. https://resources.nvidia.com/en-us-dgx-cloud/project-\nceiba-video?ncid=so-twit-266831&ncid=no-ncid , 2023.\n[49] The New York Times. Nvidia’s h20 chip faces new u.s. export restrictions\nto china. https://www.nytimes.com/2025/04/15/technology/nvidia-h20-chip-\nchina-restrictions.html, April 2025.\n[50] NVIDIA Corporation. NVIDIA DGX H100/H200 System User Guide, 2025.\n[51] Artificial Analysis. Artificial analysis: Ai model & api providers analysis. https://\nartificialanalysis.ai, 2025.\n[52] NVIDIA. Triton inference server user guide: Dynamic batching. https:\n//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/\nuser_guide/batcher.html, 2024.\n[53] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya\nTanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-\ninference-bench: Inference benchmarking of large language models on ai accelerators. In\nSC24-W: Workshops of the International Conference for High Performance Computing, Net-\nworking, Storage and Analysis, pages 1362–1379. IEEE Computer Society, 2024.\n[54] Ankit V ora, Avik Chaudhuri, Deepak Narayanan, and Matei Zaharia. Splitwise: Efficient\ngenerative llm inference using phase-splitting. In Proceedings of the 51st Annual International\nSymposium on Computer Architecture (ISCA). IEEE, 2024.\n[55] Xing Chen, Daniel Lo, Sitao Xiang, Daniel Kang, and Kunle Olukotun. A latency processing\nunit: A latency-optimized and highly scalable processor for large language model inference.\nIn Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA).\nIEEE, 2024.\n[56] P. Patel et al. Characterizing power management opportunities for llms in the cloud. In\nProceedings of the 29th International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), 2024.\n[57] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Sotirios Xydis, and Dimitrios Soudris.\nSlo-aware gpu dvfs for energy-efficient llm inference serving. IEEE Computer Architecture\nLetters, 2024.\n[58] Dylan Patel and Gerald Wong. Gpt-4 architecture, infrastructure, training dataset,\ncosts, vision, moe. https://semianalysis.com/2023/07/10/gpt-4-architecture-\ninfrastructure/, July 2023.\n[59] OpenAI. Deprecations - openai api. https://platform.openai.com/docs/\ndeprecations, 2025.\n[60] Tu˘gana Aslan, Peter Holzapfel, Lutz Stobbe, Andreas Grimm, Nils F Nissen, and Matthias\nFinkbeiner. Toward climate neutral data centers: Greenhouse gas inventory, scenarios, and\nstrategies. iScience, 28(1), 2025.\n16\n\n[61] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark. Advances in Neural Information\nProcessing Systems, 37:95266–95290, 2025.\n[62] Dan Hendrycks et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. URL\nhttps://arxiv.org/abs/2501.14249.\n[63] David Rein et al. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint\narXiv:2311.12022, 2023.\n[64] HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/\nHuggingFaceH4/MATH-500, 2024.\n[65] Maxwell-Jia. Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/\nAIME_2024, 2024.\n[66] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas,\nPan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: A research coding benchmark curated by\nscientists. Advances in Neural Information Processing Systems, 37:30624–30650, 2024.\n[67] Fanjia Yan et al. Livecodebench: Holistic and contamination free evaluation of llms for code.\narXiv preprint arXiv:2403.07974, 2024.\n[68] Sam Altman. The gentle singularity. https://blog.samaltman.com/the-gentle-\nsingularity, 2025.\n[69] Mistral AI. Our contribution to a global environmental standard for AI, Jul 2025.\nURL https://mistral.ai/news/our-contribution-to-a-global-environmental-\nstandard-for-ai.\n[70] Reuters. Openai’s weekly active users surpass 400 million. https://www.reuters.com/\ntechnology/artificial-intelligence/openais-weekly-active-users-surpass-\n400-million-2025-02-20/ , February 2025.\n[71] Emma Roth. Chatgpt now has over 300 million weekly users. https://www.theverge.com/\n2024/12/4/24313097/chatgpt-300-million-weekly-users , December 2024.\n[72] Shubham Singh. Chatgpt statistics (2025): Dau & mau data worldwide. https://www.\ndemandsage.com/chatgpt-statistics/, April 2025.\n[73] Anthony Cardillo. How many google searches are there per day? (march 2025). https:\n//explodingtopics.com/blog/google-searches-per-day , April 2025.\n[74] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025.\n[75] Shaolei Ren, Bill Tomlinson, Rebecca W Black, and Andrew W Torrance. Reconciling the\ncontrasting narratives on the environmental impact of large language models. Scientific Reports,\n14(1):26310, 2024.\n[76] John M Polimeni and Raluca Iorgulescu Polimeni. Jevons’ paradox and the myth of technologi-\ncal liberation. Ecological Complexity, 3(4):344–353, 2006.\n[77] Aleksandar Ristic-Smith and Daniel J. Rogers. Compact two-phase immersion cooling with\ndielectric fluid for pcb-based power electronics. IEEE Open Journal of Power Electronics, 5:\n1107–1118, 2024. doi: 10.1109/OJPEL.2024.3432989.\n17\n\nTable 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.\nBatch Size DGPU UGPU total Unon-GPU total\n4 40-55% 10-13.5% 12.5%\n8 45-60% 5.5-7.5% 6.25%\n16 55-70% 3.5-4.5% 3.125%\nAppendices\nA Batch Size Sensitivity Analysis (GPT-4o)\nIn our main analysis, we adopt a batch size of 8 for all per-prompt energy estimations. This choice\nreflects a middle ground in real-world deployments, where AI providers typically batch requests in\nthe range of 4 to 16 to balance latency constraints with energy efficiency. However, the specific batch\nsize used during inference can significantly influence energy consumption due to changes in GPU\nand system utilization.\nTo assess this effect, we present a sensitivity analysis using GPT-4o as a representative model. The\nonly parameter varied is batch size, allowing us to examine how plausible batching configurations\ncan significantly shift energy outcomes. This variation underscores the rationale behind our use of\nbatch size 8 as a representative midpoint in real-world deployments.\nFigure 7: GPT-4o per-prompt energy consumption (Wh) across batch sizes and prompt lengths.\nTable 5 summarizes the utilization rates applied to each batch size, following the same method used\nin our methodology section 4, which drives the corresponding per-prompt energy estimates shown in\nFigure 7.\nThe results show substantial efficiency gains with higher batching: moving from batch size 4 to 8\nreduces energy per prompt by approximately 45%, while increasing from 8 to 16 yields a further 43%\nreduction. If we had used a batch size of 4 throughout our study, energy estimates would have been\nsignificantly higher, overstating the environmental footprint of LLM inference. Conversely, using a\nbatch size of 16 would have resulted in notably lower energy values, possibly underestimating the\nfootprint in more latency-constrained or low-traffic scenarios.\nThese differences highlight the critical role that batching decisions play in shaping the environmental\nfootprint of large-scale LLM deployments. As AI models utilize dynamic batching to address traffic\nand latency issues, adjusting the batch size can significantly impact the environmental footprint of\neach prompt. Large-scale providers like OpenAI have a significant advantage in this regard, as their\nhigh traffic volume allows them to rely on higher batch sizes without sacrificing latency to the same\nextent as smaller or less active deployments.\nB Scope 3 Considerations\nWhile this study focuses on operational emissions and resource consumption during inference (Scopes\n1 and 2), it is important to briefly discuss the Scope 3 impacts associated with the manufacturing,\ntransportation, and end-of-life disposal of the hardware used to power LLMs.\nScope 3 emissions are typically the most significant contributor to the lifecycle footprint of data center\ninfrastructure, encompassing embodied carbon from GPU fabrication, water usage in semiconductor\n18\n\n[Image page=18 idx=1 name=Im9.png] Size: 2000x450, Data: 67469 bytes\n\nFigure 8: Cross efficiency DEA scores. Bar labels show the AI Index (top) and cross-efficiency score\n(bottom).\nmanufacturing, emissions from global logistics, and hardware retirement. For instance, Microsoft’s\nScope 3 CO2e emissions in 2023 accounted for 66% of the total emissions [16]. Yet, these values\nare highly variable across vendors, manufacturing locations, and fabrication nodes, and they lack\ndeployment-specific attribution when applied to real-time inference tasks.\nMoreover, given that many large-scale models are continually updated and deployed across evolving\ninfrastructures, ascribing a fixed fraction of embodied emissions or water per query is both method-\nologically fragile and likely to result in overestimation. Applying complete hardware manufacturing\nfootprints to ongoing inference, without amortizing them over the expected hardware lifespan or\nquery volume, risks artificially inflating per-query environmental costs.\nIn light of this, we excluded Scope 3 from our prompt-level framework, as its inclusion would\nintroduce non-trivial uncertainty and potentially distort comparative eco-efficiency across models.\nNevertheless, the long-term sustainability of AI infrastructure will depend on extending lifecycle\naccountability beyond the inference phase; future work is encouraged to adopt comprehensive\nlifecycle analyses (LCA) that integrate Scope 3 considerations once transparent and standardized\ndata become available.\nC Cross-effficiency DEA Results\nBefore presenting the eco-efficiency results, it is worth noting that Claude 3.5 Sonnet, Claude 3.5\nHaiku, GPT-4, and GPT-4 Turbo were excluded due to the lack of benchmark results on certain\ntests. Since cross-efficiency requires complete inputs and outputs, these models could not be fairly\nevaluated.\nAs shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini\nachieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and An-\nthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively\nmodest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These\nresults suggest that downsizing reasoning models can yield meaningful sustainability gains without\ncompromising performance.\nAt the opposite end, DeepSeek-R1 (0.067) and DeepSeek-V3 (0.059) recorded the lowest efficiency\nscores. Despite their advanced reasoning capabilities, their high energy, water, and carbon costs\nindicate significant infrastructural inefficiencies. Their Azure-hosted variants performed better,\nDeepSeek-R1 (0.539) and DeepSeek-V3 (0.523), yet remained below most OpenAI and Anthropic\nsystems. Among OpenAI models, GPT-4.1 mini (0.580) and GPT-4.1 nano (0.508) balanced output\nquality and sustainability particularly well. LLaMA models clustered between 0.4 and 0.6, reflecting\nefficient power use but limited reasoning performance.\n19\n\n[Image page=19 idx=1 name=Im10.png] Size: 2500x1000, Data: 180715 bytes\n\nIn summary, eco-efficiency relies on both output quality and environmental cost. OpenAI’s smaller\nreasoning models and Claude 3.7 Sonnet strike that balance most effectively, while DeepSeek and\nLLaMA demonstrate the limitations of concentrating on capability or sustainability alone.\n20", "metadata": {"url": "https://arxiv.org/pdf/2505.09598", "type": "paper", "year": "2025"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1∗\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 × the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel’s total lifecycle energy use [14, 15].\n∗Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025", "sentences": [{"text": "How Hungry is AI?", "metadata": {}}, {"text": "Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1∗\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters.", "metadata": {}}, {"text": "The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations.", "metadata": {}}, {"text": "We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics.", "metadata": {}}, {"text": "Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 × the most efficient systems.", "metadata": {}}, {"text": "Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S.", "metadata": {}}, {"text": "homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset.", "metadata": {}}, {"text": "These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption.", "metadata": {}}, {"text": "Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.", "metadata": {}}, {"text": "1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4].", "metadata": {}}, {"text": "Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation.", "metadata": {}}, {"text": "On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.", "metadata": {}}, {"text": "However, the advancement of LLMs does involve shortcomings in environmental aspects.", "metadata": {}}, {"text": "Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.", "metadata": {}}, {"text": "Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs.", "metadata": {}}, {"text": "In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale.", "metadata": {}}, {"text": "Recent estimates suggest inference can account for up to 90% of a\nmodel’s total lifecycle energy use [14, 15].", "metadata": {}}, {"text": "∗Corresponding author.", "metadata": {}}, {"text": "arXiv:2505.09598v6  [cs.CY]  24 Nov 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI’s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o’s\nannual environmental footprint and section 7 analyzes the impact of GPT-5’s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3’s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta’s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs’ TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2", "sentences": [{"text": "Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent.", "metadata": {}}, {"text": "A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.", "metadata": {}}, {"text": "Existing environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17].", "metadata": {}}, {"text": "This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI’s\ntrue environmental cost.", "metadata": {}}, {"text": "To address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers.", "metadata": {}}, {"text": "Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications.", "metadata": {}}, {"text": "Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead.", "metadata": {}}, {"text": "This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research.", "metadata": {}}, {"text": "Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations.", "metadata": {}}, {"text": "To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers.", "metadata": {}}, {"text": "This novel dashboard incorporates new models as they get released.", "metadata": {}}, {"text": "Moreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance.", "metadata": {}}, {"text": "As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data.", "metadata": {}}, {"text": "We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning.", "metadata": {}}, {"text": "Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.", "metadata": {}}, {"text": "The remainder of the paper is organized as follows.", "metadata": {}}, {"text": "Section 2 reviews existing studies on the\nenvironmental impact of LLMs.", "metadata": {}}, {"text": "Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers.", "metadata": {}}, {"text": "Section 4 details our framework for estimating inference-phase cost.", "metadata": {}}, {"text": "Section 5 presents findings across 30 models.", "metadata": {}}, {"text": "Section 6 provides a focused analysis of GPT-4o’s\nannual environmental footprint and section 7 analyzes the impact of GPT-5’s adapative model routing.", "metadata": {}}, {"text": "Section 8 outlines key insights and implications.", "metadata": {}}, {"text": "Section 9 summarizes the main takeaways and\nlimitations and directions for future work.", "metadata": {}}, {"text": "2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.", "metadata": {}}, {"text": "Li et al.", "metadata": {}}, {"text": "[13] analyzed GPT-3’s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.", "metadata": {}}, {"text": "Although their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability.", "metadata": {}}, {"text": "In parallel, Strubell et al.", "metadata": {}}, {"text": "[ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.", "metadata": {}}, {"text": "However, their analysis excludes inference and infrastructural overhead.", "metadata": {}}, {"text": "Similar limitations appear\nin Meta’s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs’ TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.", "metadata": {}}, {"text": "Regarding inference, Husom et al.", "metadata": {}}, {"text": "[18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models.", "metadata": {}}, {"text": "Samsi et al.", "metadata": {}}, {"text": "[ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference.", "metadata": {}}, {"text": "Yang et al.", "metadata": {}}, {"text": "[ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score.", "metadata": {}}, {"text": "However, their analysis does\n2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "not include LLMs, API-based deployments, or essential infrastructure considerations like PUE and\nWUE.\nComplementary studies, including Luccioni et al. [ 28], assess general-purpose and task-specific\nmodels in the A100 systems. While they provide valuable cross-model insights, they do not consider\nproprietary models, water usage, or carbon emissions. CodeCarbon [15] calculates carbon footprints\nbased on device-level data and regional carbon intensity, but it lacks the granularity needed for\nprompt-level analysis and does not work with API-based inferences. On a larger scale, Harding\net al. [ 29] connect AI adoption to national productivity, allowing for extrapolation of energy and\ncarbon effects. Though this provides a useful overarching view, it overlooks variability in per-prompt\ninference, the behavior of specific models, and the infrastructure used for deployment.\nMost efforts focus on training and local model evaluation, lacking standardized, scalable methods,\nignoring infrastructural overhead, and omitting resource categories such as water consumption and\ncarbon emissions. Our work addresses these gaps by integrating API-based performance metrics with\nGPU and system power specifications and environmental multipliers to estimate the environmental\nimpact of LLM inference at the prompt level in data centers. We infer deployment infrastructure\nthrough statistical analysis and apply DEA to contextualize environmental impact versus performance.\nAdditionally, we conduct two case studies estimating GPT-4o’s annual environmental footprint based\non scaled usage data and analyzing the impact of GPT-5’s adapative model routing, providing the\nfirst infrastructure-aware, prompt-level benchmark of inference sustainability at scale.\n3 Preliminaries\nTo capture infrastructure-level overhead in data center operations, we apply three standard environ-\nmental multipliers: Power Usage Effectiveness (PUE) [19, 20], Water Usage Effectiveness (WUE)\n[19, 20], and Carbon Intensity Factor (CIF) [21, 22].\nPUE accounts for non-computational energy overheads such as cooling, lighting, and power distribu-\ntion. Defined as the ratio of total data center energy consumption to IT-specific energy use.\nWUE captures the water used per kilowatt-hour of IT energy, encompassing on-site cooling (Scope\n1), off-site electricity generation (Scope 2), and embodied water from hardware manufacturing and\ntransport (Scope 3). WUE can be computed based on either water withdrawal (the total volume\ndrawn from natural or municipal sources) or water consumption (the portion of withdrawn water\npermanently lost, primarily through evaporation).\nCIF measures carbon emissions per kilowatt-hour of energy consumed, largely driven by the regional\nelectricity mix. Emissions are categorized as direct on-site combustion (Scope 1), off-site electricity\ngeneration (Scope 2), and embodied emissions from manufacturing and transport (Scope 3).\n4 Methodology\nThis section presents our novel methodology for estimating the environmental footprint of LLM\ninference. Our framework integrates model-specific performance metrics with infrastructure-level\nenvironmental multipliers to calculate operational energy consumption, water usage, and carbon\nemissions per query. We also evaluate eco-efficiency using DEA, mapping sustainability trade-offs\nagainst a composite performance benchmark, and develop an interactive dashboard for a more\nthorough analysis.\n4.1 Model Selection and Hardware Estimation\nWe analyze 30 large language models across OpenAI, Anthropic, Meta, and DeepSeek. Table 1\nsummarizes each model’s deployment context, including provider, cloud host, hardware type and\nspecifications, and company-specific environmental multipliers (PUE, WUE, CIF). All models are\nusually run on NVIDIA DGX systems using A100, H100, H200, or H800 GPUs [30, 45, 46, 47, 48].\nU.S.-based providers such as OpenAI and Anthropic have acquired large volumes of H200 and H100\nchips [31, 41, 42], making them the most probable choice for recent deployments. DeepSeek, which\noperates under U.S. export restrictions, uses the H800, NVIDIA’s export-compliant GPU for the\nChinese market [38, 49]. Both the H200 and H800 retain the same Hopper architecture and peak\n3", "sentences": [{"text": "not include LLMs, API-based deployments, or essential infrastructure considerations like PUE and\nWUE.", "metadata": {}}, {"text": "Complementary studies, including Luccioni et al.", "metadata": {}}, {"text": "[ 28], assess general-purpose and task-specific\nmodels in the A100 systems.", "metadata": {}}, {"text": "While they provide valuable cross-model insights, they do not consider\nproprietary models, water usage, or carbon emissions.", "metadata": {}}, {"text": "CodeCarbon [15] calculates carbon footprints\nbased on device-level data and regional carbon intensity, but it lacks the granularity needed for\nprompt-level analysis and does not work with API-based inferences.", "metadata": {}}, {"text": "On a larger scale, Harding\net al.", "metadata": {}}, {"text": "[ 29] connect AI adoption to national productivity, allowing for extrapolation of energy and\ncarbon effects.", "metadata": {}}, {"text": "Though this provides a useful overarching view, it overlooks variability in per-prompt\ninference, the behavior of specific models, and the infrastructure used for deployment.", "metadata": {}}, {"text": "Most efforts focus on training and local model evaluation, lacking standardized, scalable methods,\nignoring infrastructural overhead, and omitting resource categories such as water consumption and\ncarbon emissions.", "metadata": {}}, {"text": "Our work addresses these gaps by integrating API-based performance metrics with\nGPU and system power specifications and environmental multipliers to estimate the environmental\nimpact of LLM inference at the prompt level in data centers.", "metadata": {}}, {"text": "We infer deployment infrastructure\nthrough statistical analysis and apply DEA to contextualize environmental impact versus performance.", "metadata": {}}, {"text": "Additionally, we conduct two case studies estimating GPT-4o’s annual environmental footprint based\non scaled usage data and analyzing the impact of GPT-5’s adapative model routing, providing the\nfirst infrastructure-aware, prompt-level benchmark of inference sustainability at scale.", "metadata": {}}, {"text": "3 Preliminaries\nTo capture infrastructure-level overhead in data center operations, we apply three standard environ-\nmental multipliers: Power Usage Effectiveness (PUE) [19, 20], Water Usage Effectiveness (WUE)\n[19, 20], and Carbon Intensity Factor (CIF) [21, 22].", "metadata": {}}, {"text": "PUE accounts for non-computational energy overheads such as cooling, lighting, and power distribu-\ntion.", "metadata": {}}, {"text": "Defined as the ratio of total data center energy consumption to IT-specific energy use.", "metadata": {}}, {"text": "WUE captures the water used per kilowatt-hour of IT energy, encompassing on-site cooling (Scope\n1), off-site electricity generation (Scope 2), and embodied water from hardware manufacturing and\ntransport (Scope 3).", "metadata": {}}, {"text": "WUE can be computed based on either water withdrawal (the total volume\ndrawn from natural or municipal sources) or water consumption (the portion of withdrawn water\npermanently lost, primarily through evaporation).", "metadata": {}}, {"text": "CIF measures carbon emissions per kilowatt-hour of energy consumed, largely driven by the regional\nelectricity mix.", "metadata": {}}, {"text": "Emissions are categorized as direct on-site combustion (Scope 1), off-site electricity\ngeneration (Scope 2), and embodied emissions from manufacturing and transport (Scope 3).", "metadata": {}}, {"text": "4 Methodology\nThis section presents our novel methodology for estimating the environmental footprint of LLM\ninference.", "metadata": {}}, {"text": "Our framework integrates model-specific performance metrics with infrastructure-level\nenvironmental multipliers to calculate operational energy consumption, water usage, and carbon\nemissions per query.", "metadata": {}}, {"text": "We also evaluate eco-efficiency using DEA, mapping sustainability trade-offs\nagainst a composite performance benchmark, and develop an interactive dashboard for a more\nthorough analysis.", "metadata": {}}, {"text": "4.1 Model Selection and Hardware Estimation\nWe analyze 30 large language models across OpenAI, Anthropic, Meta, and DeepSeek.", "metadata": {}}, {"text": "Table 1\nsummarizes each model’s deployment context, including provider, cloud host, hardware type and\nspecifications, and company-specific environmental multipliers (PUE, WUE, CIF).", "metadata": {}}, {"text": "All models are\nusually run on NVIDIA DGX systems using A100, H100, H200, or H800 GPUs [30, 45, 46, 47, 48].", "metadata": {}}, {"text": "U.S.-based providers such as OpenAI and Anthropic have acquired large volumes of H200 and H100\nchips [31, 41, 42], making them the most probable choice for recent deployments.", "metadata": {}}, {"text": "DeepSeek, which\noperates under U.S.", "metadata": {}}, {"text": "export restrictions, uses the H800, NVIDIA’s export-compliant GPU for the\nChinese market [38, 49].", "metadata": {}}, {"text": "Both the H200 and H800 retain the same Hopper architecture and peak\n3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "Table 1: Deployment and infrastructure specifications of models.\nModel LaunchDate Company Host Hardware CriticalPower(kW) PUE WUE(on-site, L/kWh)WUE(off-site, L/kWh)CIF(kgCO2e/kWh)\nGPT-4.1 Apr, 2025\nOpenAI Microsoft Azure DGX H200/H100 [30, 31] 10.20 [32] 1.12 [33] 0.30 [34] 4.35 [35] 0.35 [36]\nGPT-4.1 mini Apr, 2025GPT-4.1 nano Apr, 2025o4-mini (high) Apr, 2025o3 Apr, 2025o3-mini (high) Jan, 2025o3-mini Jan, 2025o1 Dec, 2024o1-mini Sep, 2024GPT-4o (Mar ’25) May, 2024GPT-4o mini July, 2024OpenAI Microsoft AzureDGX A100* 6.50[37] 1.12 0.30 4.35 0.35GPT-4 Turbo Nov, 2023GPT-4 Mar, 2023DeepSeek-R1 Jan, 2025Deepseek Deepseek DGX H800 [8] 10.20 [38] 1.27 [39] 1.20 [39] 6.016 [35] 0.6 [40]DeepSeek-V3 Dec, 2024DeepSeek-R1 Jan, 2025Deepseek Microsoft Azure DGX H200/H100 10.20 1.12 0.30 4.35 0.35DeepSeek-V3 Dec, 2024Claude-3.7 Sonnet Feb, 2025\nAnthropic AWS DGX H200/H100 [41, 42] 10.20 1.14 [43] 0.18 [43] 5.11 [35] 0.287 [44]Claude-3.5 Sonnet Jun, 2024Claude-3.5 Haiku Nov, 2024LLaMA-3.3 70B Dec, 2024\nMeta AWS DGX H200/H100 10.20 1.14 0.18 5.11 0.287\nLLaMA-3.2-vision 90B Sep, 2024LLaMA-3.2-vision 11B Sep, 2024LLaMA-3.2 3B Sep, 2024LLaMA-3.2 1B Sep, 2024LLaMA-3.1-405B Jul, 2024LLaMA-3.1-70B Jul, 2024LLaMA-3.1-8B Jul, 2024LLaMA-3-70B Apr, 2024LLaMA-3-8B Apr, 2024\n*DGX A100 was estimated for GPT-4o mini, GPT-4 Turbo, and GPT-4. Justification and estimation details are provided in Section 4.3.1.\npower draw as the H100, with system-level energy characteristics that are nearly identical [50]. While\nthe H200 achieves greater energy efficiency due to faster memory and higher bandwidth, and the\nH800 may exhibit reduced performance due to export-related firmware limitations, both maintain\nthe same peak power draw, thermal design profile, and system-level utilization characteristics as the\nH100 [38, 50]. These architectural differences affect throughput and latency, resulting in higher or\nlower energy consumed per token, but do not impact total system power demand under load. We\ntherefore treat H100, H200, and H800 as equivalent in our power modeling, since our estimates are\nbased on power draw and utilization rather than task-level performance.\nEnvironmental multipliers such as PUE, WUE, and CIF are assigned according to each cloud\nprovider’s data center locations and corresponding regional grid characteristics. For OpenAI and\nDeepSeek models hosted on Microsoft Azure, we use Azure-reported PUE and site-level WUE values,\nwhile CIF and source-level WUE are derived from the specific geographic locations of Microsoft\ndata centers around the world. For AWS-hosted models, including those from Anthropic and Meta,\nwe apply AWS-reported PUE and site-level WUE, and compute CIF and source-level WUE based\non the regional distribution of AWS data centers used for inference. For DeepSeek models that are\ndeployed in Chinese datacenters, we adopt the average PUE and site-level WUE of the thirty most\nefficient data centers in China, while CIF and source-level WUE are determined using the regional\nlocations of its known or reported data center deployments.\n4.2 Per-Query Energy Consumption Estimation\nTo quantify the energy required for a single inference, we introduce a probabilistic framework that\ncaptures the stochastic nature of LLM workloads. The model integrates standardized performance\ndata [51], which report latency to first-token generation (L) and tokens-per-second (TPS, denoted R)\nacross empirical quantiles (5th, 25th, 50th, 75th, and 95th percentiles) and three representative prompt\nconfigurations: short-form (100 input, 300 output tokens), medium (1,000 input, 1,000 output), and\nlong-form (10,000 input, 1,500 output), reflecting variability across multiple test runs for each model\nand prompt configuration.\nTo model realistic runtime behavior, we construct a joint distribution ofL and R using a Gaussian\ncopula with correlation coefficient ρ = −0.3, capturing the negative dependence typically observed\nbetween latency and TPS. From this distribution, we draw 10,000 correlated samples (Li, Ri), each\nrepresenting one plausible inference scenario. The culmination of this infrastructure-aware framework\nis the introduction of our novel formula to precisely estimate the per-query energy consumption:\n4", "sentences": [{"text": "Table 1: Deployment and infrastructure specifications of models.", "metadata": {}}, {"text": "Model LaunchDate Company Host Hardware CriticalPower(kW) PUE WUE(on-site, L/kWh)WUE(off-site, L/kWh)CIF(kgCO2e/kWh)\nGPT-4.1 Apr, 2025\nOpenAI Microsoft Azure DGX H200/H100 [30, 31] 10.20 [32] 1.12 [33] 0.30 [34] 4.35 [35] 0.35 [36]\nGPT-4.1 mini Apr, 2025GPT-4.1 nano Apr, 2025o4-mini (high) Apr, 2025o3 Apr, 2025o3-mini (high) Jan, 2025o3-mini Jan, 2025o1 Dec, 2024o1-mini Sep, 2024GPT-4o (Mar ’25) May, 2024GPT-4o mini July, 2024OpenAI Microsoft AzureDGX A100* 6.50[37] 1.12 0.30 4.35 0.35GPT-4 Turbo Nov, 2023GPT-4 Mar, 2023DeepSeek-R1 Jan, 2025Deepseek Deepseek DGX H800 [8] 10.20 [38] 1.27 [39] 1.20 [39] 6.016 [35] 0.6 [40]DeepSeek-V3 Dec, 2024DeepSeek-R1 Jan, 2025Deepseek Microsoft Azure DGX H200/H100 10.20 1.12 0.30 4.35 0.35DeepSeek-V3 Dec, 2024Claude-3.7 Sonnet Feb, 2025\nAnthropic AWS DGX H200/H100 [41, 42] 10.20 1.14 [43] 0.18 [43] 5.11 [35] 0.287 [44]Claude-3.5 Sonnet Jun, 2024Claude-3.5 Haiku Nov, 2024LLaMA-3.3 70B Dec, 2024\nMeta AWS DGX H200/H100 10.20 1.14 0.18 5.11 0.287\nLLaMA-3.2-vision 90B Sep, 2024LLaMA-3.2-vision 11B Sep, 2024LLaMA-3.2 3B Sep, 2024LLaMA-3.2 1B Sep, 2024LLaMA-3.1-405B Jul, 2024LLaMA-3.1-70B Jul, 2024LLaMA-3.1-8B Jul, 2024LLaMA-3-70B Apr, 2024LLaMA-3-8B Apr, 2024\n*DGX A100 was estimated for GPT-4o mini, GPT-4 Turbo, and GPT-4.", "metadata": {}}, {"text": "Justification and estimation details are provided in Section 4.3.1.", "metadata": {}}, {"text": "power draw as the H100, with system-level energy characteristics that are nearly identical [50].", "metadata": {}}, {"text": "While\nthe H200 achieves greater energy efficiency due to faster memory and higher bandwidth, and the\nH800 may exhibit reduced performance due to export-related firmware limitations, both maintain\nthe same peak power draw, thermal design profile, and system-level utilization characteristics as the\nH100 [38, 50].", "metadata": {}}, {"text": "These architectural differences affect throughput and latency, resulting in higher or\nlower energy consumed per token, but do not impact total system power demand under load.", "metadata": {}}, {"text": "We\ntherefore treat H100, H200, and H800 as equivalent in our power modeling, since our estimates are\nbased on power draw and utilization rather than task-level performance.", "metadata": {}}, {"text": "Environmental multipliers such as PUE, WUE, and CIF are assigned according to each cloud\nprovider’s data center locations and corresponding regional grid characteristics.", "metadata": {}}, {"text": "For OpenAI and\nDeepSeek models hosted on Microsoft Azure, we use Azure-reported PUE and site-level WUE values,\nwhile CIF and source-level WUE are derived from the specific geographic locations of Microsoft\ndata centers around the world.", "metadata": {}}, {"text": "For AWS-hosted models, including those from Anthropic and Meta,\nwe apply AWS-reported PUE and site-level WUE, and compute CIF and source-level WUE based\non the regional distribution of AWS data centers used for inference.", "metadata": {}}, {"text": "For DeepSeek models that are\ndeployed in Chinese datacenters, we adopt the average PUE and site-level WUE of the thirty most\nefficient data centers in China, while CIF and source-level WUE are determined using the regional\nlocations of its known or reported data center deployments.", "metadata": {}}, {"text": "4.2 Per-Query Energy Consumption Estimation\nTo quantify the energy required for a single inference, we introduce a probabilistic framework that\ncaptures the stochastic nature of LLM workloads.", "metadata": {}}, {"text": "The model integrates standardized performance\ndata [51], which report latency to first-token generation (L) and tokens-per-second (TPS, denoted R)\nacross empirical quantiles (5th, 25th, 50th, 75th, and 95th percentiles) and three representative prompt\nconfigurations: short-form (100 input, 300 output tokens), medium (1,000 input, 1,000 output), and\nlong-form (10,000 input, 1,500 output), reflecting variability across multiple test runs for each model\nand prompt configuration.", "metadata": {}}, {"text": "To model realistic runtime behavior, we construct a joint distribution ofL and R using a Gaussian\ncopula with correlation coefficient ρ = −0.3, capturing the negative dependence typically observed\nbetween latency and TPS.", "metadata": {}}, {"text": "From this distribution, we draw 10,000 correlated samples (Li, Ri), each\nrepresenting one plausible inference scenario.", "metadata": {}}, {"text": "The culmination of this infrastructure-aware framework\nis the introduction of our novel formula to precisely estimate the per-query energy consumption:\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Let Li captures the initialization latency and Output Length\nRi\nrepresents the time it takes to generate the\nresponse. Also, let PGPU and Pnon-GPU denote the rated power draw (in kW) of the GPU subsystem\nand the non-GPU subsystem (e.g., CPUs, SSDs, network, and cooling control electronics), respec-\ntively. The parameters UGPU,min and UGPU,max represent the minimum and maximum GPU utilization\nfractions observed during inference, while Unon-GPU represents the average utilization fraction for\nnon-GPU components. PUE factor is also incorporated to account for datacenter-level overheads.\nWe compute energy consumption at the lower and upper utilization bounds as:\nEi,{min,max} =", "sentences": [{"text": "Let Li captures the initialization latency and Output Length\nRi\nrepresents the time it takes to generate the\nresponse.", "metadata": {}}, {"text": "Also, let PGPU and Pnon-GPU denote the rated power draw (in kW) of the GPU subsystem\nand the non-GPU subsystem (e.g., CPUs, SSDs, network, and cooling control electronics), respec-\ntively.", "metadata": {}}, {"text": "The parameters UGPU,min and UGPU,max represent the minimum and maximum GPU utilization\nfractions observed during inference, while Unon-GPU represents the average utilization fraction for\nnon-GPU components.", "metadata": {}}, {"text": "PUE factor is also incorporated to account for datacenter-level overheads.", "metadata": {}}, {"text": "We compute energy consumption at the lower and upper utilization bounds as:\nEi,{min,max} =", "metadata": {}}], "metadata": {"page": 5}}, {"text": "Li + Output Length\nRi\n3600\n!\n| {z }\nTotal inference time (Ti, hours)\n×\n\nPGPU × UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU × Unon-GPU| {z }\nNon-GPU power (kW)\n\n × PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes. The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 − wmax)Ei,min, ¯Equery = E[Ei,exp], σ Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.\n4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7–20B),\nSmall (20–40B), Medium (40–70B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.\nModels that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled “Nano” such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].\nAI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.\nBenchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios. A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100\nsystems, we adopt measurements from Patel et al. and Kakolyris et al.’s work [56, 57]. Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G × DGPU\nN × B , U non-GPU total = G × Dnon-GPU\nN × B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size. DGPU denotes the assigned GPUs’ power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes. Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.\n5", "sentences": [{"text": "Li + Output Length\nRi\n3600\n!", "metadata": {}}, {"text": "| {z }\nTotal inference time (Ti, hours)\n×\n\nPGPU × UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU × Unon-GPU| {z }\nNon-GPU power (kW)\n\n × PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes.", "metadata": {}}, {"text": "The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 − wmax)Ei,min, ¯Equery = E[Ei,exp], σ Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.", "metadata": {}}, {"text": "4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7–20B),\nSmall (20–40B), Medium (40–70B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.", "metadata": {}}, {"text": "Models that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled “Nano” such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].", "metadata": {}}, {"text": "AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52].", "metadata": {}}, {"text": "Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.", "metadata": {}}, {"text": "Benchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second.", "metadata": {}}, {"text": "Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16.", "metadata": {}}, {"text": "This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput.", "metadata": {}}, {"text": "Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios.", "metadata": {}}, {"text": "A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A.", "metadata": {}}, {"text": "The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53].", "metadata": {}}, {"text": "For A100\nsystems, we adopt measurements from Patel et al.", "metadata": {}}, {"text": "and Kakolyris et al.’s work [56, 57].", "metadata": {}}, {"text": "Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G × DGPU\nN × B , U non-GPU total = G × Dnon-GPU\nN × B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size.", "metadata": {}}, {"text": "DGPU denotes the assigned GPUs’ power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32].", "metadata": {}}, {"text": "We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle.", "metadata": {}}, {"text": "Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes.", "metadata": {}}, {"text": "Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.", "metadata": {}}, {"text": "5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Table 2: Estimated node-level GPU and non-GPU utilization by model class for H100 and A100.\nClass GPU\nCount\nDGPU\n(H100)\nDGPU\n(A100)\nUGPU total\n(H100)\nUGPU total\n(A100) Unon-GPU total\nNano 1 35–65% 80–90% 0.55–1.00% 1.25–1.5% 0.87%\nMicro 1 50–80% 90–100% 0.75–1.25% 1.5–1.6% 0.87%\nSmall 2 55–80% N/A 1.70–2.50% N/A 1.6%\nMedium 4 50–70% 100–110% 3.00–4.50% 6.25–7% 3.125%\nLarge 8 45–60% 100–120% 5.50–7.50% 12.5–15.0% 6.25%\nFigure 1: (Left) Mean energy consumption of GPT-4o and GPT-4o mini across providers and GPU\ntypes, measured by output size. (Right) Distribution of TPS (averaged across output sizes)\n4.3.1 GPT-4, GPT-4 Turbo, and GPT-4o mini Hardware Estimation\nIn our experiment, we observed a performance discrepancy: GPT-4o mini showed significantly\nlower throughput and higher latency on OpenAI’s API compared to Microsoft Azure under identical\nprompt settings, as shown in Figure 1. Both variants also underperformed relative to OpenAI’s\nGPT-4o, with 60% and 27% lower TPS, respectively. Given GPT-4o mini’s smaller size and H200’s\narchitectural advantages, its performance would be expected to match or exceed GPT-4o if served\non H200 infrastructure. The observed gap is inconsistent with H200 deployment and suggests that\nGPT-4o mini is running on A100 or H100 systems. Notably, Azure’s version outperforms OpenAI’s\nby 47% on average, further supporting the likelihood that Azure uses H100 and OpenAI retains\nA100. Therefore, to validate our hardware estimations, we tested this hypothesis using two-way\nANOV A and Tukey HSD (Table 3). At 300-token prompts, energy consumption was statistically\nsimilar across platforms, as expected given the small computational load. However, at larger output\nsizes, significant differences emerged: OpenAI’s presumed A100 deployment differed from Azure’s\nH100 deployment with p < 0.05, and Azure’s H100 also outperformed OpenAI’s assumed H100\nwith p < 0.05, reinforcing the likelihood that OpenAI’s GPT-4o mini is not served on H100. We\ntherefore consider GPT-4o mini to be running on A100. Additionally, with reports that GPT-4 was\ntrained and deployed on A100 systems [58], and given the architectural continuity between GPT-4\nand GPT-4 Turbo and their low throughput, high latency, and impending deprecation [59], we also\nconsider they are running on A100 architecture since it is unlikely that they have migrated to newer\nhardware.\nTable 3: Tukey HSD Adjusted p-values for energy consumption differences by provider, GPU system,\nand prompt size\nGroup 1 Group 2 300 tokens 1000 tokens 1500 tokens\nAzure (H100) OpenAI (A100) 0.979 0.0009 <0.0001\nAzure (H100) OpenAI (H100) 0.951 0.0001 <0.0001\n6", "sentences": [{"text": "Table 2: Estimated node-level GPU and non-GPU utilization by model class for H100 and A100.", "metadata": {}}, {"text": "Class GPU\nCount\nDGPU\n(H100)\nDGPU\n(A100)\nUGPU total\n(H100)\nUGPU total\n(A100) Unon-GPU total\nNano 1 35–65% 80–90% 0.55–1.00% 1.25–1.5% 0.87%\nMicro 1 50–80% 90–100% 0.75–1.25% 1.5–1.6% 0.87%\nSmall 2 55–80% N/A 1.70–2.50% N/A 1.6%\nMedium 4 50–70% 100–110% 3.00–4.50% 6.25–7% 3.125%\nLarge 8 45–60% 100–120% 5.50–7.50% 12.5–15.0% 6.25%\nFigure 1: (Left) Mean energy consumption of GPT-4o and GPT-4o mini across providers and GPU\ntypes, measured by output size.", "metadata": {}}, {"text": "(Right) Distribution of TPS (averaged across output sizes)\n4.3.1 GPT-4, GPT-4 Turbo, and GPT-4o mini Hardware Estimation\nIn our experiment, we observed a performance discrepancy: GPT-4o mini showed significantly\nlower throughput and higher latency on OpenAI’s API compared to Microsoft Azure under identical\nprompt settings, as shown in Figure 1.", "metadata": {}}, {"text": "Both variants also underperformed relative to OpenAI’s\nGPT-4o, with 60% and 27% lower TPS, respectively.", "metadata": {}}, {"text": "Given GPT-4o mini’s smaller size and H200’s\narchitectural advantages, its performance would be expected to match or exceed GPT-4o if served\non H200 infrastructure.", "metadata": {}}, {"text": "The observed gap is inconsistent with H200 deployment and suggests that\nGPT-4o mini is running on A100 or H100 systems.", "metadata": {}}, {"text": "Notably, Azure’s version outperforms OpenAI’s\nby 47% on average, further supporting the likelihood that Azure uses H100 and OpenAI retains\nA100.", "metadata": {}}, {"text": "Therefore, to validate our hardware estimations, we tested this hypothesis using two-way\nANOV A and Tukey HSD (Table 3).", "metadata": {}}, {"text": "At 300-token prompts, energy consumption was statistically\nsimilar across platforms, as expected given the small computational load.", "metadata": {}}, {"text": "However, at larger output\nsizes, significant differences emerged: OpenAI’s presumed A100 deployment differed from Azure’s\nH100 deployment with p < 0.05, and Azure’s H100 also outperformed OpenAI’s assumed H100\nwith p < 0.05, reinforcing the likelihood that OpenAI’s GPT-4o mini is not served on H100.", "metadata": {}}, {"text": "We\ntherefore consider GPT-4o mini to be running on A100.", "metadata": {}}, {"text": "Additionally, with reports that GPT-4 was\ntrained and deployed on A100 systems [58], and given the architectural continuity between GPT-4\nand GPT-4 Turbo and their low throughput, high latency, and impending deprecation [59], we also\nconsider they are running on A100 architecture since it is unlikely that they have migrated to newer\nhardware.", "metadata": {}}, {"text": "Table 3: Tukey HSD Adjusted p-values for energy consumption differences by provider, GPU system,\nand prompt size\nGroup 1 Group 2 300 tokens 1000 tokens 1500 tokens\nAzure (H100) OpenAI (A100) 0.979 0.0009 <0.0001\nAzure (H100) OpenAI (H100) 0.951 0.0001 <0.0001\n6", "metadata": {}}], "metadata": {"page": 6}}, {"text": "[Image page=6 idx=1 name=Im1.png] Size: 2000x700, Data: 105459 bytes", "sentences": [{"text": "[Image page=6 idx=1 name=Im1.png] Size: 2000x700, Data: 105459 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 1, "image_name": "Im1.png", "image_width": 2000, "image_height": 700, "attachment_type": "image", "has_image_data": true, "image_data_size": 105459}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "4.4 Per-Query Water Consumption and Carbon Emissions Estimation\nThis study focuses exclusively on operational emissions and resource consumption during the\ninference phase of the model. Accordingly, embodied emissions and water use from hardware\nmanufacturing and supply chains (Scope 3) are excluded due to their limited relevance to real-time\ndeployment and the risk of inflating per-query estimates when applied without deployment-specific\nattribution or when model lifecycles remain ongoing. For water usage, we focus solely on water\nconsumption (water permanently removed from the source). For carbon emissions, we exclude Scope\n1 emissions as they are generally negligible compared to Scope 2 emissions due to the infrequent\nuse of on-site fuel combustion for backup generators and facility heating in data centers [60]. For\nexample, Scope 1 emissions accounted for only 1.6% of Microsoft’s Scope 2 emissions in 2023 [36],\na figure that includes executive air travel, ground transportation, refrigerant leakage, and on-site fuel\nuse, further diminishing the share attributable to data center operations. Accordingly, our analysis\nfocuses exclusively on Scope 2 emissions, which capture the carbon intensity of electricity consumed\nduring inference. A more detailed discussion of these considerations is provided in Appendix B.\nWater consumption and carbon emissions per query are calculated as:\nWater (L) = Equery\nPUE · WUEsite\n| {z }\nOn-site cooling\n+ Equery · WUEsource\n| {z }\nOff-site electricity\n(4)\nCarbon (kgCO2e) = Equery · CIF (5)\n4.5 Eco-Efficiency via Data Envelopment Analysis (DEA)\nWe apply cross-efficiency DEA to evaluate the effectiveness of each model in converting environ-\nmental resources into functional intelligence. Inputs include per-query energy consumption, PUE,\nWUEsource, WUEsite, and CIF. The output is the Artificial Intelligence Index, a composite score\nweighted across multiple benchmark domains [ 51]. Specifically, reasoning and knowledge tasks\n(MMLU-Pro [61], HLE [62], GPQA [63]) collectively contribute 50% of the index (1/6 each); mathe-\nmatical proficiency (MATH-500 [64], AIME [65]) contributes 25% (1/8 each); and coding ability\n(SciCode [66], LiveCodeBench [67]) accounts for the remaining 25% (1/8 each).\nIn contrast to standard Charnes-Cooper-Rhodes (CCR) or Banker-Charnes-Cooper (BCC) models,\nwhich enable each model to choose its optimal weightings, sometimes inflating performance, cross-\nefficiency assesses each model based on its own and all peer weightings. This approach reduces\nself-evaluation bias and recognizes models that maintain strong performance from various efficiency\nviewpoints. The resulting scores offer a more robust and comparative measure of eco-efficiency. Full\nresults and additional discussion are provided in Appendix C.\n4.6 Power BI Dashboard\nTo democratize access to these novel assessments, we built and deployed an automated Power BI\ndashboard that runs our entire framework in real time, a first-of-its-kind tool for continuously tracking\nAI inference sustainability. The data are scraped daily from the Artificial Analysis website, cleaned\nautomatically, and then visualized on Power BI as seen in Figures 2a and 2b. The main dashboard\ndisplays the average and standard deviation of energy use, water consumption (site, source, and\ncombined), and carbon emissions for the three query sizes. It also visualizes latency and TPS\nfluctuations, benchmark results, and the total environmental impact when scaling up to 1, 50, or\n100 billion queries, compared with real-world equivalents such as household electricity use, annual\ndrinking needs, and transportation emissions. Users can filter by company, model size, query size, or\nsustainability metric, and download the full dataset. Additionally, the dashboard tracks day-to-day\nchanges in each model’s footprint, visualizing time-series trends and the average in energy, water,\nand carbon metrics across data centers and hardware setups. It includes an extended list of models\nbeyond those analyzed in this study and automatically incorporates new ones as they are released,\nallowing continuous monitoring of inference-phase sustainability and cross-model comparisons over\ntime.\n7", "sentences": [{"text": "4.4 Per-Query Water Consumption and Carbon Emissions Estimation\nThis study focuses exclusively on operational emissions and resource consumption during the\ninference phase of the model.", "metadata": {}}, {"text": "Accordingly, embodied emissions and water use from hardware\nmanufacturing and supply chains (Scope 3) are excluded due to their limited relevance to real-time\ndeployment and the risk of inflating per-query estimates when applied without deployment-specific\nattribution or when model lifecycles remain ongoing.", "metadata": {}}, {"text": "For water usage, we focus solely on water\nconsumption (water permanently removed from the source).", "metadata": {}}, {"text": "For carbon emissions, we exclude Scope\n1 emissions as they are generally negligible compared to Scope 2 emissions due to the infrequent\nuse of on-site fuel combustion for backup generators and facility heating in data centers [60].", "metadata": {}}, {"text": "For\nexample, Scope 1 emissions accounted for only 1.6% of Microsoft’s Scope 2 emissions in 2023 [36],\na figure that includes executive air travel, ground transportation, refrigerant leakage, and on-site fuel\nuse, further diminishing the share attributable to data center operations.", "metadata": {}}, {"text": "Accordingly, our analysis\nfocuses exclusively on Scope 2 emissions, which capture the carbon intensity of electricity consumed\nduring inference.", "metadata": {}}, {"text": "A more detailed discussion of these considerations is provided in Appendix B.", "metadata": {}}, {"text": "Water consumption and carbon emissions per query are calculated as:\nWater (L) = Equery\nPUE · WUEsite\n| {z }\nOn-site cooling\n+ Equery · WUEsource\n| {z }\nOff-site electricity\n(4)\nCarbon (kgCO2e) = Equery · CIF (5)\n4.5 Eco-Efficiency via Data Envelopment Analysis (DEA)\nWe apply cross-efficiency DEA to evaluate the effectiveness of each model in converting environ-\nmental resources into functional intelligence.", "metadata": {}}, {"text": "Inputs include per-query energy consumption, PUE,\nWUEsource, WUEsite, and CIF.", "metadata": {}}, {"text": "The output is the Artificial Intelligence Index, a composite score\nweighted across multiple benchmark domains [ 51].", "metadata": {}}, {"text": "Specifically, reasoning and knowledge tasks\n(MMLU-Pro [61], HLE [62], GPQA [63]) collectively contribute 50% of the index (1/6 each);", "metadata": {}}, {"text": "mathe-\nmatical proficiency (MATH-500 [64], AIME [65]) contributes 25% (1/8 each);", "metadata": {}}, {"text": "and coding ability\n(SciCode [66], LiveCodeBench [67]) accounts for the remaining 25% (1/8 each).", "metadata": {}}, {"text": "In contrast to standard Charnes-Cooper-Rhodes (CCR) or Banker-Charnes-Cooper (BCC) models,\nwhich enable each model to choose its optimal weightings, sometimes inflating performance, cross-\nefficiency assesses each model based on its own and all peer weightings.", "metadata": {}}, {"text": "This approach reduces\nself-evaluation bias and recognizes models that maintain strong performance from various efficiency\nviewpoints.", "metadata": {}}, {"text": "The resulting scores offer a more robust and comparative measure of eco-efficiency.", "metadata": {}}, {"text": "Full\nresults and additional discussion are provided in Appendix C.", "metadata": {}}, {"text": "4.6 Power BI Dashboard\nTo democratize access to these novel assessments, we built and deployed an automated Power BI\ndashboard that runs our entire framework in real time, a first-of-its-kind tool for continuously tracking\nAI inference sustainability.", "metadata": {}}, {"text": "The data are scraped daily from the Artificial Analysis website, cleaned\nautomatically, and then visualized on Power BI as seen in Figures 2a and 2b.", "metadata": {}}, {"text": "The main dashboard\ndisplays the average and standard deviation of energy use, water consumption (site, source, and\ncombined), and carbon emissions for the three query sizes.", "metadata": {}}, {"text": "It also visualizes latency and TPS\nfluctuations, benchmark results, and the total environmental impact when scaling up to 1, 50, or\n100 billion queries, compared with real-world equivalents such as household electricity use, annual\ndrinking needs, and transportation emissions.", "metadata": {}}, {"text": "Users can filter by company, model size, query size, or\nsustainability metric, and download the full dataset.", "metadata": {}}, {"text": "Additionally, the dashboard tracks day-to-day\nchanges in each model’s footprint, visualizing time-series trends and the average in energy, water,\nand carbon metrics across data centers and hardware setups.", "metadata": {}}, {"text": "It includes an extended list of models\nbeyond those analyzed in this study and automatically incorporates new ones as they are released,\nallowing continuous monitoring of inference-phase sustainability and cross-model comparisons over\ntime.", "metadata": {}}, {"text": "7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "(a) Overview of the main dashboard displaying the\nenergy consumption per model, latency, TPS, bench-\nmark scores, and equivalent environmental impacts\nfor an example model (GPT-5 minimal).\n(b) Overview of the timeseries dashboard displaying\naverage energy consumption per model, and the daily\nfluctuations of the selected model (Grok 4).\nFigure 2: Visual overview of the AI sustainability dashboard.\n5 Experimental Evaluation\nWe benchmark the environmental footprint of 30 LLMs across three modalities: Energy consumption,\nwater usage, and carbon emissions, based on equations 2, 4, and 5, respectively. For the long-form\nquery evaluation, GPT-4 and LLaMA-3 (8B and 70B) are excluded due to context window limitations.\n5.1 Energy Consumption\nFigure 3: Energy consumption per model across\nthree prompt sizes (Wh, log-scale).\nTable 4: Energy consumption (mean ± std\ndev) per model across three prompt sizes\n(Wh).\nModel Energy Consumption(100 input-300 output)(Wh)\nEnergy Consumption(1k input-1k output)(Wh)\nEnergy Consumption(10k input-1.5k output)(Wh)GPT-4.1 0.871 ± 0.302 3.161 ± 0515 4.833 ± 0.650GPT-4.1 mini 0.450 ± 0.081 1.545 ± 0.211 2.122 ± 0.348GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094o4-mini (high) 3.649 ± 1.468 7.380 ± 2.177 7.237 ± 1.674o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082o3-mini (high) 3.012 ± 0.991 6.865 ± 1.33 5.389 ± 1.183o3-mini 0.674 ± 0.015 2.423 ± 0.237 3.525 ± 0.168o1 2.268 ± 0.654 4.047 ± 0.497 6.181 ± 0.877o1-mini 0.535 ± 0.182 1.547 ± 0.405 2.317 ± 0.530GPT-4o (Mar ’25) 0.423 ± 0.085 1.215 ± 0.241 2.875 ± 0.421GPT-4o mini 0.577 ± 0.139 1.897 ± 0.570 3.098 ± 0.639GPT-4 Turbo 1.699 ± 0.355 5.940 ± 1.441 9.877 ± 1.304GPT-4 1.797 ± 0.259 6.925 ± 1.553 —DeepSeek-R1 (DS)* 19.251 ± 9.449 24.596 ± 9.4 29.078 ± 9.725DeepSeek-V3 (DS)* 2.777 ± 0.223 8.864 ± 0.724 13.162 ± 1.126DeepSeek-R1 (AZ)† 2.353 ± 1.129 4.331 ± 1.695 7.410 ± 2.159DeepSeek-V3 (AZ)† 0.742 ± 0.125 2.165 ± 0.578 3.696 ± 0.221Claude-3.7 Sonnet 0.950 ± 0.040 2.989 ± 0.201 5.671 ± 0.302Claude-3.5 Sonnet 0.973 ± 0.066 3.638 ± 0.256 7.772 ± 0.345Claude-3.5 Haiku 0.975 ± 0.063 4.464 ± 0.283 8.010 ± 0.338LLaMA-3-8B 0.108 ± 0.002 0.370 ± 0.005 —LLaMA-3-70B 0.861 ± 0.022 2.871 ± 0.094 —LLaMA-3.1-8B 0.052 ± 0.008 0.172 ± 0.015 0.443 ± 0.028LLaMA-3.1-70B 1.271 ± 0.020 4.525 ± 0.053 19.183 ± 0.560LLaMA-3.1-405B 2.226 ± 0.142 9.042 ± 0.385 25.202 ± 0.526LLaMA-3.2 1B 0.109 ± 0.013 0.342 ± 0.025 0.552 ± 0.059LLaMA-3.2 3B 0.143 ± 0.006 0.479 ± 0.017 0.707 ± 0.020LLaMA-3.2-vision 11B 0.078 ± 0.021 0.242 ± 0.071 1.087 ± 0.060LLaMA-3.2-vision 90B 1.235 ± 0.054 4.534 ± 0.448 6.852 ± 0.780LLaMA-3.3 70B 0.237 ± 0.023 0.760 ± 0.079 1.447 ± 0.188\n* DeepSeek Host\n† Microsoft Azure Host\nFigure 3 and Table 4 highlight how energy consumption scales with prompt length and model\narchitecture, revealing wide disparities across systems. LLaMA-3.1-8B is the most efficient, requiring\nonly 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output),\nfollowed by LLaMA-3.2 1B and LLaMA-3.2 3B at 0.552 Wh and 0.707 Wh, respectively. GPT-\n4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly\ntwice the energy of LLaMA-3.1-8B. In contrast, DeepSeek-R1 (DS) consumes 29.075 Wh, around\nsixty five times more than the most efficient model, underscoring the large overhead of reasoning\nmodels.\n8", "sentences": [{"text": "(a) Overview of the main dashboard displaying the\nenergy consumption per model, latency, TPS, bench-\nmark scores, and equivalent environmental impacts\nfor an example model (GPT-5 minimal).", "metadata": {}}, {"text": "(b) Overview of the timeseries dashboard displaying\naverage energy consumption per model, and the daily\nfluctuations of the selected model (Grok 4).", "metadata": {}}, {"text": "Figure 2: Visual overview of the AI sustainability dashboard.", "metadata": {}}, {"text": "5 Experimental Evaluation\nWe benchmark the environmental footprint of 30 LLMs across three modalities: Energy consumption,\nwater usage, and carbon emissions, based on equations 2, 4, and 5, respectively.", "metadata": {}}, {"text": "For the long-form\nquery evaluation, GPT-4 and LLaMA-3 (8B and 70B) are excluded due to context window limitations.", "metadata": {}}, {"text": "5.1 Energy Consumption\nFigure 3: Energy consumption per model across\nthree prompt sizes (Wh, log-scale).", "metadata": {}}, {"text": "Table 4: Energy consumption (mean ± std\ndev) per model across three prompt sizes\n(Wh).", "metadata": {}}, {"text": "Model Energy Consumption(100 input-300 output)(Wh)\nEnergy Consumption(1k input-1k output)(Wh)\nEnergy Consumption(10k input-1.5k output)(Wh)GPT-4.1 0.871 ± 0.302 3.161 ± 0515 4.833 ± 0.650GPT-4.1 mini 0.450 ± 0.081 1.545 ± 0.211 2.122 ± 0.348GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094o4-mini (high) 3.649 ± 1.468 7.380 ± 2.177 7.237 ± 1.674o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082o3-mini (high) 3.012 ± 0.991 6.865 ± 1.33 5.389 ± 1.183o3-mini 0.674 ± 0.015 2.423 ± 0.237 3.525 ± 0.168o1 2.268 ± 0.654 4.047 ± 0.497 6.181 ± 0.877o1-mini 0.535 ± 0.182 1.547 ± 0.405 2.317 ± 0.530GPT-4o (Mar ’25) 0.423 ± 0.085 1.215 ± 0.241 2.875 ± 0.421GPT-4o mini 0.577 ± 0.139 1.897 ± 0.570 3.098 ± 0.639GPT-4 Turbo 1.699 ± 0.355 5.940 ± 1.441 9.877 ± 1.304GPT-4 1.797 ± 0.259 6.925 ± 1.553 —DeepSeek-R1 (DS)* 19.251 ± 9.449 24.596 ± 9.4 29.078 ± 9.725DeepSeek-V3 (DS)* 2.777 ± 0.223 8.864 ± 0.724 13.162 ± 1.126DeepSeek-R1 (AZ)† 2.353 ± 1.129 4.331 ± 1.695 7.410 ± 2.159DeepSeek-V3 (AZ)† 0.742 ± 0.125 2.165 ± 0.578 3.696 ± 0.221Claude-3.7 Sonnet 0.950 ± 0.040 2.989 ± 0.201 5.671 ± 0.302Claude-3.5 Sonnet 0.973 ± 0.066 3.638 ± 0.256 7.772 ± 0.345Claude-3.5 Haiku 0.975 ± 0.063 4.464 ± 0.283 8.010 ± 0.338LLaMA-3-8B 0.108 ± 0.002 0.370 ± 0.005 —LLaMA-3-70B 0.861 ± 0.022 2.871 ± 0.094 —LLaMA-3.1-8B 0.052 ± 0.008 0.172 ± 0.015 0.443 ± 0.028LLaMA-3.1-70B 1.271 ± 0.020 4.525 ± 0.053 19.183 ± 0.560LLaMA-3.1-405B 2.226 ± 0.142 9.042 ± 0.385 25.202 ± 0.526LLaMA-3.2 1B 0.109 ± 0.013 0.342 ± 0.025 0.552 ± 0.059LLaMA-3.2 3B 0.143 ± 0.006 0.479 ± 0.017 0.707 ± 0.020LLaMA-3.2-vision 11B 0.078 ± 0.021 0.242 ± 0.071 1.087 ± 0.060LLaMA-3.2-vision 90B 1.235 ± 0.054 4.534 ± 0.448 6.852 ± 0.780LLaMA-3.3 70B 0.237 ± 0.023 0.760 ± 0.079 1.447 ± 0.188\n* DeepSeek Host\n† Microsoft Azure Host\nFigure 3 and Table 4 highlight how energy consumption scales with prompt length and model\narchitecture, revealing wide disparities across systems.", "metadata": {}}, {"text": "LLaMA-3.1-8B is the most efficient, requiring\nonly 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output),\nfollowed by LLaMA-3.2 1B and LLaMA-3.2 3B at 0.552 Wh and 0.707 Wh, respectively.", "metadata": {}}, {"text": "GPT-\n4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly\ntwice the energy of LLaMA-3.1-8B.", "metadata": {}}, {"text": "In contrast, DeepSeek-R1 (DS) consumes 29.075 Wh, around\nsixty five times more than the most efficient model, underscoring the large overhead of reasoning\nmodels.", "metadata": {}}, {"text": "8", "metadata": {}}], "metadata": {"page": 8}}, {"text": "[Image page=8 idx=1 name=Im2.png] Size: 1496x829, Data: 280459 bytes", "sentences": [{"text": "[Image page=8 idx=1 name=Im2.png] Size: 1496x829, Data: 280459 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 1, "image_name": "Im2.png", "image_width": 1496, "image_height": 829, "attachment_type": "image", "has_image_data": true, "image_data_size": 280459}}, {"text": "[Image page=8 idx=2 name=Im3.png] Size: 1509x841, Data: 154154 bytes", "sentences": [{"text": "[Image page=8 idx=2 name=Im3.png] Size: 1509x841, Data: 154154 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 2, "image_name": "Im3.png", "image_width": 1509, "image_height": 841, "attachment_type": "image", "has_image_data": true, "image_data_size": 154154}}, {"text": "[Image page=8 idx=3 name=Im4.png] Size: 1545x1891, Data: 576342 bytes", "sentences": [{"text": "[Image page=8 idx=3 name=Im4.png] Size: 1545x1891, Data: 576342 bytes", "metadata": {}}], "metadata": {"page": 8, "image_index": 3, "image_name": "Im4.png", "image_width": 1545, "image_height": 1891, "attachment_type": "image", "has_image_data": true, "image_data_size": 576342}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "The LLaMA family shows clear scaling effects: energy use rises from 0.443 Wh at 8B parameters\nto 25.202 Wh at 405B, illustrating steep power demands at high parameter counts. Additionally,\nthe DeepSeek models reveal striking infrastructure effects. DeepSeek-R1 and DeepSeek-V3 hosted\non DeepSeek’s own servers consume 29.078 Wh and 13.162 Wh, while the same models on Azure\nuse just 7.410 Wh and 3.696 Wh, over 70% less energy. This gap highlights that hardware and\ndata center efficiency, not model design alone, drives real-world energy use. For context, a single\nlong query to DeepSeek-R1 (DS) consumes about as much electricity as running a 65-inch LED\ntelevision (≈ 130 W) for roughly 13 minutes. GPT-4o and GPT-4o mini also show that infrastructure\ncan outweigh model size in determining energy efficiency. For instance GPT-4o consumes around\n2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on\nA100 hardware instead of H100s.\n5.2 Water and Carbon Emissions\n(a) Water consumption per model across three prompt\nsizes (ml, log-scale).\n(b) Carbon emissions per model across three prompt\nsizes (gCO2e, log-scale)\nFigure 4: Water consumption and carbon emissions per model.\nFigure 4 showcases the water consumption and carbon emissions of models across all prompt sizes.\nThe most resource-efficient systems, such as LLaMA-3.2 1B, LLaMA-3.2 3B, LLaMA-3.1-8B,\nLLaMA-3-8B, and GPT-4.1 nano, emit less than 0.3 gCO2e and consume under 4 mL of water even\nfor long-form prompts, demonstrating exceptional sustainability across scales.\nIn contrast, large-scale and reasoning models such as o3, DeepSeek-R1 (DS), and DeepSeek-V3 (DS)\nexhibit substantially higher footprints. DeepSeek-R1 (DS) consumes over 200 mL of water and emits\napproximately 17 gCO2e per long query, while the same model on Azure consumes only 34 mL and\nemits 2.5 gCO2e, a reduction of nearly 85%. These figures suggest that environmental impacts are\nshaped not only by model architecture but also by deployment strategies and regional infrastructure\nconditions. In particular, the elevated emissions and water usage observed in DeepSeek models likely\nreflect inefficiencies in their data centers, including higher PUE, suboptimal cooling technologies,\nand less efficient hardware.\nWhile these per-query values may seem modest when isolated, their impact becomes considerable\nat scale. A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit\nas much carbon as thousands of transatlantic flights and consume water equivalent to the annual\ndrinking needs of millions of people. We revisit this scaling analysis in greater detail in Section 6.\n9", "sentences": [{"text": "The LLaMA family shows clear scaling effects: energy use rises from 0.443 Wh at 8B parameters\nto 25.202 Wh at 405B, illustrating steep power demands at high parameter counts.", "metadata": {}}, {"text": "Additionally,\nthe DeepSeek models reveal striking infrastructure effects.", "metadata": {}}, {"text": "DeepSeek-R1 and DeepSeek-V3 hosted\non DeepSeek’s own servers consume 29.078 Wh and 13.162 Wh, while the same models on Azure\nuse just 7.410 Wh and 3.696 Wh, over 70% less energy.", "metadata": {}}, {"text": "This gap highlights that hardware and\ndata center efficiency, not model design alone, drives real-world energy use.", "metadata": {}}, {"text": "For context, a single\nlong query to DeepSeek-R1 (DS) consumes about as much electricity as running a 65-inch LED\ntelevision (≈ 130 W) for roughly 13 minutes.", "metadata": {}}, {"text": "GPT-4o and GPT-4o mini also show that infrastructure\ncan outweigh model size in determining energy efficiency.", "metadata": {}}, {"text": "For instance GPT-4o consumes around\n2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on\nA100 hardware instead of H100s.", "metadata": {}}, {"text": "5.2 Water and Carbon Emissions\n(a) Water consumption per model across three prompt\nsizes (ml, log-scale).", "metadata": {}}, {"text": "(b) Carbon emissions per model across three prompt\nsizes (gCO2e, log-scale)\nFigure 4: Water consumption and carbon emissions per model.", "metadata": {}}, {"text": "Figure 4 showcases the water consumption and carbon emissions of models across all prompt sizes.", "metadata": {}}, {"text": "The most resource-efficient systems, such as LLaMA-3.2 1B, LLaMA-3.2 3B, LLaMA-3.1-8B,\nLLaMA-3-8B, and GPT-4.1 nano, emit less than 0.3 gCO2e and consume under 4 mL of water even\nfor long-form prompts, demonstrating exceptional sustainability across scales.", "metadata": {}}, {"text": "In contrast, large-scale and reasoning models such as o3, DeepSeek-R1 (DS), and DeepSeek-V3 (DS)\nexhibit substantially higher footprints.", "metadata": {}}, {"text": "DeepSeek-R1 (DS) consumes over 200 mL of water and emits\napproximately 17 gCO2e per long query, while the same model on Azure consumes only 34 mL and\nemits 2.5 gCO2e, a reduction of nearly 85%.", "metadata": {}}, {"text": "These figures suggest that environmental impacts are\nshaped not only by model architecture but also by deployment strategies and regional infrastructure\nconditions.", "metadata": {}}, {"text": "In particular, the elevated emissions and water usage observed in DeepSeek models likely\nreflect inefficiencies in their data centers, including higher PUE, suboptimal cooling technologies,\nand less efficient hardware.", "metadata": {}}, {"text": "While these per-query values may seem modest when isolated, their impact becomes considerable\nat scale.", "metadata": {}}, {"text": "A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit\nas much carbon as thousands of transatlantic flights and consume water equivalent to the annual\ndrinking needs of millions of people.", "metadata": {}}, {"text": "We revisit this scaling analysis in greater detail in Section 6.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}, {"text": "[Image page=9 idx=1 name=Im5.png] Size: 1545x1833, Data: 544218 bytes", "sentences": [{"text": "[Image page=9 idx=1 name=Im5.png] Size: 1545x1833, Data: 544218 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 1, "image_name": "Im5.png", "image_width": 1545, "image_height": 1833, "attachment_type": "image", "has_image_data": true, "image_data_size": 544218}}, {"text": "[Image page=9 idx=2 name=Im6.png] Size: 1545x1833, Data: 565339 bytes", "sentences": [{"text": "[Image page=9 idx=2 name=Im6.png] Size: 1545x1833, Data: 565339 bytes", "metadata": {}}], "metadata": {"page": 9, "image_index": 2, "image_name": "Im6.png", "image_width": 1545, "image_height": 1833, "attachment_type": "image", "has_image_data": true, "image_data_size": 565339}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total\nannual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.\n5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference. Our framework estimates 0.42 Wh ( ±0.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman’s figure. Similarly, the results for Mistral Large 2\nalign closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e,\n±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation. Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.\n6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025. Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5. A single short GPT-4o query consumes 0.42\nWh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.\nScaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (±0.358 Wh). For\nmedium-length queries, this increases to 9.71 Wh (±1.106 Wh). These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI. While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.\n6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\nGiven GPT-4o’s status as the default model, we conservatively attribute 700 million daily queries to\n10", "sentences": [{"text": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o.", "metadata": {}}, {"text": "(Top Right) Estimated total\nannual energy usage of GPT-4o in 2025.", "metadata": {}}, {"text": "(Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o.", "metadata": {}}, {"text": "(Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.", "metadata": {}}, {"text": "5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation.", "metadata": {}}, {"text": "In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68].", "metadata": {}}, {"text": "Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference.", "metadata": {}}, {"text": "Our framework estimates 0.42 Wh ( ±0.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman’s figure.", "metadata": {}}, {"text": "Similarly, the results for Mistral Large 2\nalign closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query.", "metadata": {}}, {"text": "Our corresponding estimate for 300 tokens (0.82 gCO2e,\n±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation.", "metadata": {}}, {"text": "Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.", "metadata": {}}, {"text": "6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025.", "metadata": {}}, {"text": "Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5.", "metadata": {}}, {"text": "A single short GPT-4o query consumes 0.42\nWh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.", "metadata": {}}, {"text": "Scaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (±0.358 Wh).", "metadata": {}}, {"text": "For\nmedium-length queries, this increases to 9.71 Wh (±1.106 Wh).", "metadata": {}}, {"text": "These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI.", "metadata": {}}, {"text": "While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.", "metadata": {}}, {"text": "6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].", "metadata": {}}, {"text": "Given GPT-4o’s status as the default model, we conservatively attribute 700 million daily queries to\n10", "metadata": {}}], "metadata": {"page": 10}}, {"text": "[Image page=10 idx=1 name=Im7.png] Size: 2400x1200, Data: 390607 bytes", "sentences": [{"text": "[Image page=10 idx=1 name=Im7.png] Size: 2400x1200, Data: 390607 bytes", "metadata": {}}], "metadata": {"page": 10, "image_index": 1, "image_name": "Im7.png", "image_width": 2400, "image_height": 1200, "attachment_type": "image", "has_image_data": true, "image_data_size": 390607}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "GPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT’s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\nWithin these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns. Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of\n35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.\n6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks. Based on scaled inference volumes, GPT-4o’s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people. Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.\n6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o’s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5. Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London. In sequestration terms, offsetting GPT-4o’s\nannual emissions would require over 138,000 acres of average U.S. forest, an area roughly equivalent\nto the size of Chicago. These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden. This burden is only expected to grow\nas AI usage continues to scale.\n7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive “Thinking”\nmodel for complex reasoning tasks. This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.\nHowever, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6. For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase. Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels. For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o’s\n0.42 Wh per short prompt. Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.\nThese results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts. This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.\n11", "sentences": [{"text": "GPT-4o.", "metadata": {}}, {"text": "To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT’s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72].", "metadata": {}}, {"text": "This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].", "metadata": {}}, {"text": "Within these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns.", "metadata": {}}, {"text": "Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5.", "metadata": {}}, {"text": "These values exceed the total electricity consumption of\n35,000 U.S.", "metadata": {}}, {"text": "residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.", "metadata": {}}, {"text": "6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks.", "metadata": {}}, {"text": "Based on scaled inference volumes, GPT-4o’s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL.", "metadata": {}}, {"text": "These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people.", "metadata": {}}, {"text": "Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled.", "metadata": {}}, {"text": "GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.", "metadata": {}}, {"text": "6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o’s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5.", "metadata": {}}, {"text": "Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum.", "metadata": {}}, {"text": "These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London.", "metadata": {}}, {"text": "In sequestration terms, offsetting GPT-4o’s\nannual emissions would require over 138,000 acres of average U.S.", "metadata": {}}, {"text": "forest, an area roughly equivalent\nto the size of Chicago.", "metadata": {}}, {"text": "These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden.", "metadata": {}}, {"text": "This burden is only expected to grow\nas AI usage continues to scale.", "metadata": {}}, {"text": "7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive “Thinking”\nmodel for complex reasoning tasks.", "metadata": {}}, {"text": "This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.", "metadata": {}}, {"text": "However, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6.", "metadata": {}}, {"text": "For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase.", "metadata": {}}, {"text": "Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels.", "metadata": {}}, {"text": "For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o’s\n0.42 Wh per short prompt.", "metadata": {}}, {"text": "Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.", "metadata": {}}, {"text": "These results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts.", "metadata": {}}, {"text": "This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "Figure 6: Energy consumption of GPT-5 across query lengths and reasoning modes\n8 Discussion and Policy Implications\n8.1 The Critical Role of Infrastructure in AI Sustainability\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\ngains can stem as much from datacenter design as from model optimization. These observations\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\nrenewable energy sources, and infrastructure-aware deployment strategies.\n8.2 Rebound Effects and the Jevons Paradox\nAlthough large language models consume significantly less energy, water, and carbon per task than\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\nAs per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource\nconsumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives\nsystemic demand. The acceleration and affordability of AI remove traditional human and resource\nconstraints, enabling unprecedented levels of usage. Consequently, the cumulative environmental\nburden threatens to overwhelm the sustainability baselines that AI efficiency improvements initially\nsought to mitigate. As such, sustainable AI deployment must focus on systemic frameworks that\nassess how well models balance capability with environmental cost. In response, we propose DEA as\na principled method for benchmarking model-level eco-efficiency.\n8.3 Policy Implications\nAs AI systems scale globally, ensuring environmental sustainability requires both model-level\noptimizations and systemic regulation of infrastructure. Government agencies should encourage\nthresholds on the permissible environmental footprint per inference regarding energy, water, and\ncarbon emissions that AI models must not exceed. These thresholds can be met through architectural\ninnovations, such as sparsity and quantization, or through infrastructure-level optimizations like more\nefficient hardware, cleaner energy sourcing, and improved cooling systems. Our methodology offers a\nstandardized, scalable framework to quantify these efforts. Incorporating technologies like dielectric\nliquid cooling offers a promising path to reduce or eliminate water use in data centers drastically [77].\nTransparency must also be elevated through system-level reporting of per-inference energy, water,\nand carbon metrics. Additionally, deployment strategies, such as batching, should be integrated into\nsustainability planning, as larger batch sizes can reduce per-query energy use by improving hardware\nutilization with only minimal impact on latency.\n12", "sentences": [{"text": "Figure 6: Energy consumption of GPT-5 across query lengths and reasoning modes\n8 Discussion and Policy Implications\n8.1 The Critical Role of Infrastructure in AI Sustainability\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability.", "metadata": {}}, {"text": "While\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\non deployment conditions and factors such as renewable energy usage and hardware efficiency.", "metadata": {}}, {"text": "For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes.", "metadata": {}}, {"text": "Similarly, DeepSeek\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\ntheir Azure-hosted counterparts.", "metadata": {}}, {"text": "The Azure deployments benefit from better hardware, more efficient\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\ngains can stem as much from datacenter design as from model optimization.", "metadata": {}}, {"text": "These observations\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\nrenewable energy sources, and infrastructure-aware deployment strategies.", "metadata": {}}, {"text": "8.2 Rebound Effects and the Jevons Paradox\nAlthough large language models consume significantly less energy, water, and carbon per task than\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.", "metadata": {}}, {"text": "As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource\nconsumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives\nsystemic demand.", "metadata": {}}, {"text": "The acceleration and affordability of AI remove traditional human and resource\nconstraints, enabling unprecedented levels of usage.", "metadata": {}}, {"text": "Consequently, the cumulative environmental\nburden threatens to overwhelm the sustainability baselines that AI efficiency improvements initially\nsought to mitigate.", "metadata": {}}, {"text": "As such, sustainable AI deployment must focus on systemic frameworks that\nassess how well models balance capability with environmental cost.", "metadata": {}}, {"text": "In response, we propose DEA as\na principled method for benchmarking model-level eco-efficiency.", "metadata": {}}, {"text": "8.3 Policy Implications\nAs AI systems scale globally, ensuring environmental sustainability requires both model-level\noptimizations and systemic regulation of infrastructure.", "metadata": {}}, {"text": "Government agencies should encourage\nthresholds on the permissible environmental footprint per inference regarding energy, water, and\ncarbon emissions that AI models must not exceed.", "metadata": {}}, {"text": "These thresholds can be met through architectural\ninnovations, such as sparsity and quantization, or through infrastructure-level optimizations like more\nefficient hardware, cleaner energy sourcing, and improved cooling systems.", "metadata": {}}, {"text": "Our methodology offers a\nstandardized, scalable framework to quantify these efforts.", "metadata": {}}, {"text": "Incorporating technologies like dielectric\nliquid cooling offers a promising path to reduce or eliminate water use in data centers drastically [77].", "metadata": {}}, {"text": "Transparency must also be elevated through system-level reporting of per-inference energy, water,\nand carbon metrics.", "metadata": {}}, {"text": "Additionally, deployment strategies, such as batching, should be integrated into\nsustainability planning, as larger batch sizes can reduce per-query energy use by improving hardware\nutilization with only minimal impact on latency.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}, {"text": "[Image page=12 idx=1 name=Im8.png] Size: 2400x800, Data: 104610 bytes", "sentences": [{"text": "[Image page=12 idx=1 name=Im8.png] Size: 2400x800, Data: 104610 bytes", "metadata": {}}], "metadata": {"page": 12, "image_index": 1, "image_name": "Im8.png", "image_width": 2400, "image_height": 800, "attachment_type": "image", "has_image_data": true, "image_data_size": 104610}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "9 Conclusion, Limitations, and Future Work\nThis paper introduces the first large-scale, infrastructure-aware framework for benchmarking the\nenvironmental footprint of LLM inference, integrating API performance, environmental multipliers,\nand statistical inference to assess energy, water, and carbon costs under real-world conditions.\nBy applying cross-efficiency DEA, we contextualize environmental impact in terms of functional\nperformance, revealing that eco-efficiency hinges not only on model design but also on infrastructure.\nOur GPT-4o case study emphasizes the Jevons Paradox: As AI becomes cheaper and faster, total\nusage expands, intensifying environmental strain despite gains in per-query efficiency. Additionally,\nour GPT-5 case study sheds lights on the importance of prompt-level efficiency and adaptive routing.\nWithout structural shifts in how LLMs are designed, deployed, and used, these invisible costs will\ncontinue to rise, threatening to offset the societal benefits that made these systems valuable in the first\nplace. This work establishes a standardized, scalable framework for benchmarking the environmental\nfootprint of LLM inference in real-world data center deployments, providing a basis for transparent,\ninfrastructure-aware sustainability assessment and future regulation.\nOur work inherits certain limitations that we acknowledge: we avoid overstating model-specific\nfootprints by conservatively including only the energy drawn by actively assigned GPUs. This\nis due to the lack of means to determine whether unused GPUs’ capacity is reassigned, load-\nbalanced, or left inactive. Isolating non-GPU power consumption was also difficult. We applied\na fixed utilization estimate from prior studies, acknowledging that their variation across inference\nworkloads is typically significantly lower than that of GPUs. Moreover, for proprietary models\nwithout disclosed size, we classified their scale based on observed API performance. Future work\nshould address these limitations as more detailed telemetry and facility-level reporting become\navailable. Additionally, future studies should also extend beyond text generation to evaluate image,\nvideo, and audio generation, which are likely to impose greater environmental costs due to higher\ncomputational intensity.\nReferences\n[1] Google Inc. How google is integrating generative ai into search. https://blog.google/\nproducts/search/generative-ai-search-update/ , 2023.\n[2] Chong Qin, Zheng Liu, Huisi Wang, Wanchuan Zhou, Xipeng Sun, and Xuanjing Qiu. Toolllm:\nFacilitating language models to master 160+ tools. arXiv preprint arXiv:2309.12288, 2023.\n[3] Erin Hannan and Shuguang Liu. Ai: new source of competitiveness in higher education.\nCompetitiveness Review: An International Business Journal, 33(2):265–279, 2023.\n[4] Pranav Rajpurkar, James Yang, Henry Hope, and Yongqun Yu. The ai-assisted doctor: The\nimpact of large language models on medicine. Nature Medicine, 29(4):592–600, 2023.\n[5] OpenAI. Gpt-4o: Openai’s multimodal flagship model. https://openai.com/index/gpt-\n4o, 2024.\n[6] Anthropic. Claude 3: Next-generation language models from anthropic. https://www.\nanthropic.com/news/claude-3-family, 2024.\n[7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama\n3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n[8] DeepSeek AI. Deepseek v3: Open-source llms for multilingual and multimodal tasks. https:\n//deepseek.com, 2024.\n[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[10] OpenAI. Gpt-o1 model card. https://openai.com/o1/, 2024.\n[11] OpenAI. Gpt-o3 and o3-mini: Multimodal instruction-tuned models by openai. https:\n//openai.com/index/openai-o3-mini/, 2025.\n13", "sentences": [{"text": "9 Conclusion, Limitations, and Future Work\nThis paper introduces the first large-scale, infrastructure-aware framework for benchmarking the\nenvironmental footprint of LLM inference, integrating API performance, environmental multipliers,\nand statistical inference to assess energy, water, and carbon costs under real-world conditions.", "metadata": {}}, {"text": "By applying cross-efficiency DEA, we contextualize environmental impact in terms of functional\nperformance, revealing that eco-efficiency hinges not only on model design but also on infrastructure.", "metadata": {}}, {"text": "Our GPT-4o case study emphasizes the Jevons Paradox: As AI becomes cheaper and faster, total\nusage expands, intensifying environmental strain despite gains in per-query efficiency.", "metadata": {}}, {"text": "Additionally,\nour GPT-5 case study sheds lights on the importance of prompt-level efficiency and adaptive routing.", "metadata": {}}, {"text": "Without structural shifts in how LLMs are designed, deployed, and used, these invisible costs will\ncontinue to rise, threatening to offset the societal benefits that made these systems valuable in the first\nplace.", "metadata": {}}, {"text": "This work establishes a standardized, scalable framework for benchmarking the environmental\nfootprint of LLM inference in real-world data center deployments, providing a basis for transparent,\ninfrastructure-aware sustainability assessment and future regulation.", "metadata": {}}, {"text": "Our work inherits certain limitations that we acknowledge: we avoid overstating model-specific\nfootprints by conservatively including only the energy drawn by actively assigned GPUs.", "metadata": {}}, {"text": "This\nis due to the lack of means to determine whether unused GPUs’ capacity is reassigned, load-\nbalanced, or left inactive.", "metadata": {}}, {"text": "Isolating non-GPU power consumption was also difficult.", "metadata": {}}, {"text": "We applied\na fixed utilization estimate from prior studies, acknowledging that their variation across inference\nworkloads is typically significantly lower than that of GPUs.", "metadata": {}}, {"text": "Moreover, for proprietary models\nwithout disclosed size, we classified their scale based on observed API performance.", "metadata": {}}, {"text": "Future work\nshould address these limitations as more detailed telemetry and facility-level reporting become\navailable.", "metadata": {}}, {"text": "Additionally, future studies should also extend beyond text generation to evaluate image,\nvideo, and audio generation, which are likely to impose greater environmental costs due to higher\ncomputational intensity.", "metadata": {}}, {"text": "References\n[1] Google Inc.", "metadata": {}}, {"text": "How google is integrating generative ai into search.", "metadata": {}}, {"text": "https://blog.google/\nproducts/search/generative-ai-search-update/ , 2023.", "metadata": {}}, {"text": "[2] Chong Qin, Zheng Liu, Huisi Wang, Wanchuan Zhou, Xipeng Sun, and Xuanjing Qiu.", "metadata": {}}, {"text": "Toolllm:\nFacilitating language models to master 160+ tools.", "metadata": {}}, {"text": "arXiv preprint arXiv:2309.12288, 2023.", "metadata": {}}, {"text": "[3] Erin Hannan and Shuguang Liu.", "metadata": {}}, {"text": "Ai: new source of competitiveness in higher education.", "metadata": {}}, {"text": "Competitiveness Review: An International Business Journal, 33(2):265–279, 2023.", "metadata": {}}, {"text": "[4] Pranav Rajpurkar, James Yang, Henry Hope, and Yongqun Yu.", "metadata": {}}, {"text": "The ai-assisted doctor: The\nimpact of large language models on medicine.", "metadata": {}}, {"text": "Nature Medicine, 29(4):592–600, 2023.", "metadata": {}}, {"text": "[5] OpenAI.", "metadata": {}}, {"text": "Gpt-4o: Openai’s multimodal flagship model.", "metadata": {}}, {"text": "https://openai.com/index/gpt-\n4o, 2024.", "metadata": {}}, {"text": "[6] Anthropic.", "metadata": {}}, {"text": "Claude 3: Next-generation language models from anthropic.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "anthropic.com/news/claude-3-family, 2024.", "metadata": {}}, {"text": "[7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al.", "metadata": {}}, {"text": "The llama\n3 herd of models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2407.21783, 2024.", "metadata": {}}, {"text": "[8] DeepSeek AI.", "metadata": {}}, {"text": "Deepseek v3: Open-source llms for multilingual and multimodal tasks.", "metadata": {}}, {"text": "https:\n//deepseek.com, 2024.", "metadata": {}}, {"text": "[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al.", "metadata": {}}, {"text": "Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.", "metadata": {}}, {"text": "arXiv preprint arXiv:2501.12948, 2025.", "metadata": {}}, {"text": "[10] OpenAI.", "metadata": {}}, {"text": "Gpt-o1 model card.", "metadata": {}}, {"text": "https://openai.com/o1/, 2024.", "metadata": {}}, {"text": "[11] OpenAI.", "metadata": {}}, {"text": "Gpt-o3 and o3-mini: Multimodal instruction-tuned models by openai.", "metadata": {}}, {"text": "https:\n//openai.com/index/openai-o3-mini/, 2025.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "[12] David Patterson, Joseph Gonzalez, Quoc V . Le, Chen Liang, Xinlei Chen, and Andrew Ng.\nCarbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\n[13] Shaolei Li. Making ai less “thirsty”: Uncovering and addressing the secret water footprint of ai\nmodels. arXiv preprint arXiv:2304.03271, 2023.\n[14] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. Trends in ai\ninference energy consumption: Beyond the performance-vs-parameter laws of deep learning.\nSustainable Computing: Informatics and Systems, 38:100857, 2023.\n[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Codecarbon:\nEstimate and track carbon emissions from machine learning training. https://github.com/\nmlco2/codecarbon, 2022.\n[16] Microsoft Corporation. 2024 environmental sustainability report. https://www.microsoft.\ncom/en-us/corporate-responsibility/sustainability/report, May 2024.\n[17] Google. 2024 environmental report. https://sustainability.google/reports/google-\n2024-environmental-report/, July 2024.\n[18] Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, and Sagar Sen. The price of prompting:\nProfiling energy use in large language models inference. arXiv preprint arXiv:2407.16893,\n2024.\n[19] The Green Grid. PUE™ : A Comprehensive Examination of the Metric. February 2012. White\nPaper 49.\n[20] International Organization for Standardization (ISO) and International Electrotechnical Com-\nmission (IEC). Information technology – Data centres – Key performance indicators – Part\n2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/\n63211.html.\n[21] U.S. Environmental Protection Agency (EPA). Emissions & Generation Resource Integrated\nDatabase (eGRID). https://www.epa.gov/egrid, 2025.\n[22] International Energy Agency (IEA). Emissions Factors. 2025.\n[23] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 13693–13696, 2020.\n[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[26] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to\nwatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High\nPerformance Extreme Computing Conference (HPEC), pages 1–9. IEEE, 2023.\n[27] Zeyu Yang, Karel Adamek, and Wesley Armour. Double-exponential increases in inference\nenergy: The cost of the race for accuracy. arXiv preprint arXiv:2412.09731, 2024.\n[28] Sasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the\ncost of ai deployment? In Proceedings of the 2024 ACM conference on fairness, accountability,\nand transparency, pages 85–99, 2024.\n[29] Anthony Harding and Juan Moreno-Cruz. Watts and bots: The energy implications of ai\nadoption. arXiv preprint arXiv:2409.06626, 2024.\n14", "sentences": [{"text": "[12] David Patterson, Joseph Gonzalez, Quoc V .", "metadata": {}}, {"text": "Le, Chen Liang, Xinlei Chen, and Andrew Ng.", "metadata": {}}, {"text": "Carbon emissions and large neural network training.", "metadata": {}}, {"text": "arXiv preprint arXiv:2104.10350, 2021.", "metadata": {}}, {"text": "[13] Shaolei Li.", "metadata": {}}, {"text": "Making ai less “thirsty”: Uncovering and addressing the secret water footprint of ai\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2304.03271, 2023.", "metadata": {}}, {"text": "[14] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo.", "metadata": {}}, {"text": "Trends in ai\ninference energy consumption: Beyond the performance-vs-parameter laws of deep learning.", "metadata": {}}, {"text": "Sustainable Computing: Informatics and Systems, 38:100857, 2023.", "metadata": {}}, {"text": "[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.", "metadata": {}}, {"text": "Codecarbon:\nEstimate and track carbon emissions from machine learning training.", "metadata": {}}, {"text": "https://github.com/\nmlco2/codecarbon, 2022.", "metadata": {}}, {"text": "[16] Microsoft Corporation.", "metadata": {}}, {"text": "2024 environmental sustainability report.", "metadata": {}}, {"text": "https://www.microsoft.", "metadata": {}}, {"text": "com/en-us/corporate-responsibility/sustainability/report, May 2024.", "metadata": {}}, {"text": "[17] Google.", "metadata": {}}, {"text": "2024 environmental report.", "metadata": {}}, {"text": "https://sustainability.google/reports/google-\n2024-environmental-report/, July 2024.", "metadata": {}}, {"text": "[18] Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, and Sagar Sen.", "metadata": {}}, {"text": "The price of prompting:\nProfiling energy use in large language models inference.", "metadata": {}}, {"text": "arXiv preprint arXiv:2407.16893,\n2024.", "metadata": {}}, {"text": "[19] The Green Grid.", "metadata": {}}, {"text": "PUE™ : A Comprehensive Examination of the Metric.", "metadata": {}}, {"text": "February 2012.", "metadata": {}}, {"text": "White\nPaper 49.", "metadata": {}}, {"text": "[20] International Organization for Standardization (ISO) and International Electrotechnical Com-\nmission (IEC).", "metadata": {}}, {"text": "Information technology – Data centres – Key performance indicators – Part\n2: Power usage effectiveness (PUE), April 2016.", "metadata": {}}, {"text": "URL https://www.iso.org/standard/\n63211.html.", "metadata": {}}, {"text": "[21] U.S.", "metadata": {}}, {"text": "Environmental Protection Agency (EPA).", "metadata": {}}, {"text": "Emissions & Generation Resource Integrated\nDatabase (eGRID).", "metadata": {}}, {"text": "https://www.epa.gov/egrid, 2025.", "metadata": {}}, {"text": "[22] International Energy Agency (IEA).", "metadata": {}}, {"text": "Emissions Factors.", "metadata": {}}, {"text": "2025.", "metadata": {}}, {"text": "[23] Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "Energy and policy considerations for\nmodern deep learning research.", "metadata": {}}, {"text": "In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 13693–13696, 2020.", "metadata": {}}, {"text": "[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.", "metadata": {}}, {"text": "Llama: Open\nand efficient foundation language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.13971, 2023.", "metadata": {}}, {"text": "[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.", "metadata": {}}, {"text": "Llama 2: Open\nfoundation and fine-tuned chat models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2307.09288, 2023.", "metadata": {}}, {"text": "[26] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally.", "metadata": {}}, {"text": "From words to\nwatts: Benchmarking the energy costs of large language model inference.", "metadata": {}}, {"text": "In 2023 IEEE High\nPerformance Extreme Computing Conference (HPEC), pages 1–9.", "metadata": {}}, {"text": "IEEE, 2023.", "metadata": {}}, {"text": "[27] Zeyu Yang, Karel Adamek, and Wesley Armour.", "metadata": {}}, {"text": "Double-exponential increases in inference\nenergy: The cost of the race for accuracy.", "metadata": {}}, {"text": "arXiv preprint arXiv:2412.09731, 2024.", "metadata": {}}, {"text": "[28] Sasha Luccioni, Yacine Jernite, and Emma Strubell.", "metadata": {}}, {"text": "Power hungry processing: Watts driving the\ncost of ai deployment?", "metadata": {}}, {"text": "In Proceedings of the 2024 ACM conference on fairness, accountability,\nand transparency, pages 85–99, 2024.", "metadata": {}}, {"text": "[29] Anthony Harding and Juan Moreno-Cruz.", "metadata": {}}, {"text": "Watts and bots: The energy implications of ai\nadoption.", "metadata": {}}, {"text": "arXiv preprint arXiv:2409.06626, 2024.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "[30] Dallin Grimm. Nvidia ceo hand-delivers world’s fastest ai system to openai. https://www.\ntomshardware.com/tech-industry/artificial-intelligence/, April 2024.\n[31] NVIDIA. NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows.\nhttps://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-\nas-demand-for-ai-grows , March 2023.\n[32] Imran Latif, Alex C. Newkirk, Matthew R. Carbone, Arslan Munir, Yuewei Lin, Jonathan\nKoomey, Xi Yu, and Zhihua Dong. Single-node power demand during ai training: Measurements\non an 8-gpu nvidia h100 system. IEEE Access, 13:61740–61747, 2025. doi: 10.1109/ACCESS.\n2025.3554728.\n[33] Noelle Walsh. How microsoft measures datacenter water and energy use to improve azure\ncloud sustainability. https://azure.microsoft.com/blog/how-microsoft-measures-\ndatacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/ ,\nApril 2022. Microsoft Azure Blog.\n[34] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for\ncooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/\nsustainable-by-design-next-generation-datacenters-consume-zero-water-\nfor-cooling/, December 2024. Microsoft Cloud Blog.\n[35] World Resources Institute. Guidance for calculating water use embedded in purchased electricity.\nTechnical report, World Resources Institute, 2024.\n[36] Microsoft Corporation. 2024 environmental sustainability report data fact sheet. https:\n//cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/\ndocuments/presentations/CSR/2024-Environmental-Sustainability-Report-\nData-Fact.pdf, May 2024. Comprehensive environmental metrics including greenhouse gas\nemissions, energy consumption, water usage, waste management, and land protection for fiscal\nyear 2023.\n[37] NVIDIA Corporation. Nvidia dgx a100: The universal system for ai infrastruc-\nture. https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-\na100-datasheet.pdf, 2020. Datasheet detailing specifications and features of the NVIDIA\nDGX A100 system.\n[38] NVIDIA Corporation. Nvidia dgx h800 system. https://viperatech.com/shop/nvidia-\ndgx-h800-systems/, 2024. High-performance AI system featuring 8x NVIDIA H800 GPUs,\n640 GB HBM3 memory, and up to 32 petaFLOPS FP8 performance.\n[39] Hequan Wu. Academician hequan wu: Green and low-carbon development of data centers\nrequires multi-dimensional coordination of “source, grid, load, and storage”. https://www.\ncace.org.cn/News/NContent?key=04e714e4e006d433617f5d7148df2eb0, April 2024.\nChina Communications Enterprise Association News.\n[40] Wenli Ni, Xiurong Hu, Hongyang Du, Yulin Kang, Yi Ju, and Qunwei Wang. Co2 emission-\nmitigation pathways for china’s data centers. Resources, Conservation and Recycling, 202:\n107383, 2024.\n[41] AWS News Blog. New amazon ec2 p5 instances powered by nvidia h100 tensor core gpus\nfor accelerating generative ai and hpc applications. https://aws.amazon.com/blogs/aws/\nnew-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-\nfor-accelerating-generative-ai-and-hpc-applications/ .\n[42] AWS News Blog. New amazon ec2 p5e instances with nvidia h200 tensor core gpus\nand efav3 networking. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-\ninstances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking , 2024.\n[43] Amazon.com, Inc. 2023 amazon sustainability report. Technical report, Amazon.com, Inc.,\n2024.\n[44] Electricity Maps. Electricity maps — live carbon intensity map. https://app.\nelectricitymaps.com/map/, 2025.\n15", "sentences": [{"text": "[30] Dallin Grimm.", "metadata": {}}, {"text": "Nvidia ceo hand-delivers world’s fastest ai system to openai.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "tomshardware.com/tech-industry/artificial-intelligence/, April 2024.", "metadata": {}}, {"text": "[31] NVIDIA.", "metadata": {}}, {"text": "NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows.", "metadata": {}}, {"text": "https://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-\nas-demand-for-ai-grows , March 2023.", "metadata": {}}, {"text": "[32] Imran Latif, Alex C.", "metadata": {}}, {"text": "Newkirk, Matthew R.", "metadata": {}}, {"text": "Carbone, Arslan Munir, Yuewei Lin, Jonathan\nKoomey, Xi Yu, and Zhihua Dong.", "metadata": {}}, {"text": "Single-node power demand during ai training: Measurements\non an 8-gpu nvidia h100 system.", "metadata": {}}, {"text": "IEEE Access, 13:61740–61747, 2025.", "metadata": {}}, {"text": "doi: 10.1109/ACCESS.", "metadata": {}}, {"text": "2025.3554728.", "metadata": {}}, {"text": "[33] Noelle Walsh.", "metadata": {}}, {"text": "How microsoft measures datacenter water and energy use to improve azure\ncloud sustainability.", "metadata": {}}, {"text": "https://azure.microsoft.com/blog/how-microsoft-measures-\ndatacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/ ,\nApril 2022.", "metadata": {}}, {"text": "Microsoft Azure Blog.", "metadata": {}}, {"text": "[34] Steve Solomon.", "metadata": {}}, {"text": "Sustainable by design: Next-generation datacenters consume zero water for\ncooling.", "metadata": {}}, {"text": "https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/\nsustainable-by-design-next-generation-datacenters-consume-zero-water-\nfor-cooling/, December 2024.", "metadata": {}}, {"text": "Microsoft Cloud Blog.", "metadata": {}}, {"text": "[35] World Resources Institute.", "metadata": {}}, {"text": "Guidance for calculating water use embedded in purchased electricity.", "metadata": {}}, {"text": "Technical report, World Resources Institute, 2024.", "metadata": {}}, {"text": "[36] Microsoft Corporation.", "metadata": {}}, {"text": "2024 environmental sustainability report data fact sheet.", "metadata": {}}, {"text": "https:\n//cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/\ndocuments/presentations/CSR/2024-Environmental-Sustainability-Report-\nData-Fact.pdf, May 2024.", "metadata": {}}, {"text": "Comprehensive environmental metrics including greenhouse gas\nemissions, energy consumption, water usage, waste management, and land protection for fiscal\nyear 2023.", "metadata": {}}, {"text": "[37] NVIDIA Corporation.", "metadata": {}}, {"text": "Nvidia dgx a100: The universal system for ai infrastruc-\nture.", "metadata": {}}, {"text": "https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-\na100-datasheet.pdf, 2020.", "metadata": {}}, {"text": "Datasheet detailing specifications and features of the NVIDIA\nDGX A100 system.", "metadata": {}}, {"text": "[38] NVIDIA Corporation.", "metadata": {}}, {"text": "Nvidia dgx h800 system.", "metadata": {}}, {"text": "https://viperatech.com/shop/nvidia-\ndgx-h800-systems/, 2024.", "metadata": {}}, {"text": "High-performance AI system featuring 8x NVIDIA H800 GPUs,\n640 GB HBM3 memory, and up to 32 petaFLOPS FP8 performance.", "metadata": {}}, {"text": "[39] Hequan Wu.", "metadata": {}}, {"text": "Academician hequan wu: Green and low-carbon development of data centers\nrequires multi-dimensional coordination of “source, grid, load, and storage”.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "cace.org.cn/News/NContent?key=04e714e4e006d433617f5d7148df2eb0, April 2024.", "metadata": {}}, {"text": "China Communications Enterprise Association News.", "metadata": {}}, {"text": "[40] Wenli Ni, Xiurong Hu, Hongyang Du, Yulin Kang, Yi Ju, and Qunwei Wang.", "metadata": {}}, {"text": "Co2 emission-\nmitigation pathways for china’s data centers.", "metadata": {}}, {"text": "Resources, Conservation and Recycling, 202:\n107383, 2024.", "metadata": {}}, {"text": "[41] AWS News Blog.", "metadata": {}}, {"text": "New amazon ec2 p5 instances powered by nvidia h100 tensor core gpus\nfor accelerating generative ai and hpc applications.", "metadata": {}}, {"text": "https://aws.amazon.com/blogs/aws/\nnew-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-\nfor-accelerating-generative-ai-and-hpc-applications/ .", "metadata": {}}, {"text": "[42] AWS News Blog.", "metadata": {}}, {"text": "New amazon ec2 p5e instances with nvidia h200 tensor core gpus\nand efav3 networking.", "metadata": {}}, {"text": "https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-\ninstances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking , 2024.", "metadata": {}}, {"text": "[43] Amazon.com, Inc.", "metadata": {}}, {"text": "2023 amazon sustainability report.", "metadata": {}}, {"text": "Technical report, Amazon.com, Inc.,\n2024.", "metadata": {}}, {"text": "[44] Electricity Maps.", "metadata": {}}, {"text": "Electricity maps — live carbon intensity map.", "metadata": {}}, {"text": "https://app.", "metadata": {}}, {"text": "electricitymaps.com/map/, 2025.", "metadata": {}}, {"text": "15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "[45] NVIDIA Corporation. NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX\nH100 Systems – Electrical Specifications, October 2024.\n[46] Arman Shehabi, Sarah J. Smith, Nathaniel Horner, Inês Azevedo, Richard Brown, Jonathan\nKoomey, Eric Masanet, Dale Sartor, Magnus Herrlin, and William Lintner. 2024 united states\ndata center energy usage report. Technical report, Lawrence Berkeley National Laboratory,\nDecember 2024.\n[47] Rani Borkar. Microsoft and nvidia partnership continues to deliver on the\npromise of ai. https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-\npartnership-continues-to-deliver-on-the-promise-of-ai/ , March 2024. Mi-\ncrosoft Azure Blog.\n[48] NVIDIA. Project ceiba. https://resources.nvidia.com/en-us-dgx-cloud/project-\nceiba-video?ncid=so-twit-266831&ncid=no-ncid , 2023.\n[49] The New York Times. Nvidia’s h20 chip faces new u.s. export restrictions\nto china. https://www.nytimes.com/2025/04/15/technology/nvidia-h20-chip-\nchina-restrictions.html, April 2025.\n[50] NVIDIA Corporation. NVIDIA DGX H100/H200 System User Guide, 2025.\n[51] Artificial Analysis. Artificial analysis: Ai model & api providers analysis. https://\nartificialanalysis.ai, 2025.\n[52] NVIDIA. Triton inference server user guide: Dynamic batching. https:\n//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/\nuser_guide/batcher.html, 2024.\n[53] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya\nTanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-\ninference-bench: Inference benchmarking of large language models on ai accelerators. In\nSC24-W: Workshops of the International Conference for High Performance Computing, Net-\nworking, Storage and Analysis, pages 1362–1379. IEEE Computer Society, 2024.\n[54] Ankit V ora, Avik Chaudhuri, Deepak Narayanan, and Matei Zaharia. Splitwise: Efficient\ngenerative llm inference using phase-splitting. In Proceedings of the 51st Annual International\nSymposium on Computer Architecture (ISCA). IEEE, 2024.\n[55] Xing Chen, Daniel Lo, Sitao Xiang, Daniel Kang, and Kunle Olukotun. A latency processing\nunit: A latency-optimized and highly scalable processor for large language model inference.\nIn Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA).\nIEEE, 2024.\n[56] P. Patel et al. Characterizing power management opportunities for llms in the cloud. In\nProceedings of the 29th International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), 2024.\n[57] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Sotirios Xydis, and Dimitrios Soudris.\nSlo-aware gpu dvfs for energy-efficient llm inference serving. IEEE Computer Architecture\nLetters, 2024.\n[58] Dylan Patel and Gerald Wong. Gpt-4 architecture, infrastructure, training dataset,\ncosts, vision, moe. https://semianalysis.com/2023/07/10/gpt-4-architecture-\ninfrastructure/, July 2023.\n[59] OpenAI. Deprecations - openai api. https://platform.openai.com/docs/\ndeprecations, 2025.\n[60] Tu˘gana Aslan, Peter Holzapfel, Lutz Stobbe, Andreas Grimm, Nils F Nissen, and Matthias\nFinkbeiner. Toward climate neutral data centers: Greenhouse gas inventory, scenarios, and\nstrategies. iScience, 28(1), 2025.\n16", "sentences": [{"text": "[45] NVIDIA Corporation.", "metadata": {}}, {"text": "NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX\nH100 Systems – Electrical Specifications, October 2024.", "metadata": {}}, {"text": "[46] Arman Shehabi, Sarah J.", "metadata": {}}, {"text": "Smith, Nathaniel Horner, Inês Azevedo, Richard Brown, Jonathan\nKoomey, Eric Masanet, Dale Sartor, Magnus Herrlin, and William Lintner.", "metadata": {}}, {"text": "2024 united states\ndata center energy usage report.", "metadata": {}}, {"text": "Technical report, Lawrence Berkeley National Laboratory,\nDecember 2024.", "metadata": {}}, {"text": "[47] Rani Borkar.", "metadata": {}}, {"text": "Microsoft and nvidia partnership continues to deliver on the\npromise of ai.", "metadata": {}}, {"text": "https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-\npartnership-continues-to-deliver-on-the-promise-of-ai/ , March 2024.", "metadata": {}}, {"text": "Mi-\ncrosoft Azure Blog.", "metadata": {}}, {"text": "[48] NVIDIA.", "metadata": {}}, {"text": "Project ceiba.", "metadata": {}}, {"text": "https://resources.nvidia.com/en-us-dgx-cloud/project-\nceiba-video?ncid=so-twit-266831&ncid=no-ncid , 2023.", "metadata": {}}, {"text": "[49] The New York Times.", "metadata": {}}, {"text": "Nvidia’s h20 chip faces new u.s.", "metadata": {}}, {"text": "export restrictions\nto china.", "metadata": {}}, {"text": "https://www.nytimes.com/2025/04/15/technology/nvidia-h20-chip-\nchina-restrictions.html, April 2025.", "metadata": {}}, {"text": "[50] NVIDIA Corporation.", "metadata": {}}, {"text": "NVIDIA DGX H100/H200 System User Guide, 2025.", "metadata": {}}, {"text": "[51] Artificial Analysis.", "metadata": {}}, {"text": "Artificial analysis: Ai model & api providers analysis.", "metadata": {}}, {"text": "https://\nartificialanalysis.ai, 2025.", "metadata": {}}, {"text": "[52] NVIDIA.", "metadata": {}}, {"text": "Triton inference server user guide: Dynamic batching.", "metadata": {}}, {"text": "https:\n//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/\nuser_guide/batcher.html, 2024.", "metadata": {}}, {"text": "[53] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya\nTanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath.", "metadata": {}}, {"text": "Llm-\ninference-bench: Inference benchmarking of large language models on ai accelerators.", "metadata": {}}, {"text": "In\nSC24-W: Workshops of the International Conference for High Performance Computing, Net-\nworking, Storage and Analysis, pages 1362–1379.", "metadata": {}}, {"text": "IEEE Computer Society, 2024.", "metadata": {}}, {"text": "[54] Ankit V ora, Avik Chaudhuri, Deepak Narayanan, and Matei Zaharia.", "metadata": {}}, {"text": "Splitwise: Efficient\ngenerative llm inference using phase-splitting.", "metadata": {}}, {"text": "In Proceedings of the 51st Annual International\nSymposium on Computer Architecture (ISCA).", "metadata": {}}, {"text": "IEEE, 2024.", "metadata": {}}, {"text": "[55] Xing Chen, Daniel Lo, Sitao Xiang, Daniel Kang, and Kunle Olukotun.", "metadata": {}}, {"text": "A latency processing\nunit: A latency-optimized and highly scalable processor for large language model inference.", "metadata": {}}, {"text": "In Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA).", "metadata": {}}, {"text": "IEEE, 2024.", "metadata": {}}, {"text": "[56] P.", "metadata": {}}, {"text": "Patel et al.", "metadata": {}}, {"text": "Characterizing power management opportunities for llms in the cloud.", "metadata": {}}, {"text": "In\nProceedings of the 29th International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), 2024.", "metadata": {}}, {"text": "[57] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Sotirios Xydis, and Dimitrios Soudris.", "metadata": {}}, {"text": "Slo-aware gpu dvfs for energy-efficient llm inference serving.", "metadata": {}}, {"text": "IEEE Computer Architecture\nLetters, 2024.", "metadata": {}}, {"text": "[58] Dylan Patel and Gerald Wong.", "metadata": {}}, {"text": "Gpt-4 architecture, infrastructure, training dataset,\ncosts, vision, moe.", "metadata": {}}, {"text": "https://semianalysis.com/2023/07/10/gpt-4-architecture-\ninfrastructure/, July 2023.", "metadata": {}}, {"text": "[59] OpenAI.", "metadata": {}}, {"text": "Deprecations - openai api.", "metadata": {}}, {"text": "https://platform.openai.com/docs/\ndeprecations, 2025.", "metadata": {}}, {"text": "[60] Tu˘gana Aslan, Peter Holzapfel, Lutz Stobbe, Andreas Grimm, Nils F Nissen, and Matthias\nFinkbeiner.", "metadata": {}}, {"text": "Toward climate neutral data centers: Greenhouse gas inventory, scenarios, and\nstrategies.", "metadata": {}}, {"text": "iScience, 28(1), 2025.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "[61] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark. Advances in Neural Information\nProcessing Systems, 37:95266–95290, 2025.\n[62] Dan Hendrycks et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. URL\nhttps://arxiv.org/abs/2501.14249.\n[63] David Rein et al. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint\narXiv:2311.12022, 2023.\n[64] HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/\nHuggingFaceH4/MATH-500, 2024.\n[65] Maxwell-Jia. Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/\nAIME_2024, 2024.\n[66] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas,\nPan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: A research coding benchmark curated by\nscientists. Advances in Neural Information Processing Systems, 37:30624–30650, 2024.\n[67] Fanjia Yan et al. Livecodebench: Holistic and contamination free evaluation of llms for code.\narXiv preprint arXiv:2403.07974, 2024.\n[68] Sam Altman. The gentle singularity. https://blog.samaltman.com/the-gentle-\nsingularity, 2025.\n[69] Mistral AI. Our contribution to a global environmental standard for AI, Jul 2025.\nURL https://mistral.ai/news/our-contribution-to-a-global-environmental-\nstandard-for-ai.\n[70] Reuters. Openai’s weekly active users surpass 400 million. https://www.reuters.com/\ntechnology/artificial-intelligence/openais-weekly-active-users-surpass-\n400-million-2025-02-20/ , February 2025.\n[71] Emma Roth. Chatgpt now has over 300 million weekly users. https://www.theverge.com/\n2024/12/4/24313097/chatgpt-300-million-weekly-users , December 2024.\n[72] Shubham Singh. Chatgpt statistics (2025): Dau & mau data worldwide. https://www.\ndemandsage.com/chatgpt-statistics/, April 2025.\n[73] Anthony Cardillo. How many google searches are there per day? (march 2025). https:\n//explodingtopics.com/blog/google-searches-per-day , April 2025.\n[74] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025.\n[75] Shaolei Ren, Bill Tomlinson, Rebecca W Black, and Andrew W Torrance. Reconciling the\ncontrasting narratives on the environmental impact of large language models. Scientific Reports,\n14(1):26310, 2024.\n[76] John M Polimeni and Raluca Iorgulescu Polimeni. Jevons’ paradox and the myth of technologi-\ncal liberation. Ecological Complexity, 3(4):344–353, 2006.\n[77] Aleksandar Ristic-Smith and Daniel J. Rogers. Compact two-phase immersion cooling with\ndielectric fluid for pcb-based power electronics. IEEE Open Journal of Power Electronics, 5:\n1107–1118, 2024. doi: 10.1109/OJPEL.2024.3432989.\n17", "sentences": [{"text": "[61] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al.", "metadata": {}}, {"text": "Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark.", "metadata": {}}, {"text": "Advances in Neural Information\nProcessing Systems, 37:95266–95290, 2025.", "metadata": {}}, {"text": "[62] Dan Hendrycks et al.", "metadata": {}}, {"text": "Humanity’s last exam.", "metadata": {}}, {"text": "arXiv preprint arXiv:2501.14249, 2025.", "metadata": {}}, {"text": "URL\nhttps://arxiv.org/abs/2501.14249.", "metadata": {}}, {"text": "[63] David Rein et al.", "metadata": {}}, {"text": "Gpqa: A graduate-level google-proof q&a benchmark.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2311.12022, 2023.", "metadata": {}}, {"text": "[64] HuggingFaceH4.", "metadata": {}}, {"text": "Math-500 dataset.", "metadata": {}}, {"text": "https://huggingface.co/datasets/\nHuggingFaceH4/MATH-500, 2024.", "metadata": {}}, {"text": "[65] Maxwell-Jia.", "metadata": {}}, {"text": "Aime 2024 dataset.", "metadata": {}}, {"text": "https://huggingface.co/datasets/Maxwell-Jia/\nAIME_2024, 2024.", "metadata": {}}, {"text": "[66] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas,\nPan Ji, Kittithat Krongchon, Yao Li, et al.", "metadata": {}}, {"text": "Scicode: A research coding benchmark curated by\nscientists.", "metadata": {}}, {"text": "Advances in Neural Information Processing Systems, 37:30624–30650, 2024.", "metadata": {}}, {"text": "[67] Fanjia Yan et al.", "metadata": {}}, {"text": "Livecodebench: Holistic and contamination free evaluation of llms for code.", "metadata": {}}, {"text": "arXiv preprint arXiv:2403.07974, 2024.", "metadata": {}}, {"text": "[68] Sam Altman.", "metadata": {}}, {"text": "The gentle singularity.", "metadata": {}}, {"text": "https://blog.samaltman.com/the-gentle-\nsingularity, 2025.", "metadata": {}}, {"text": "[69] Mistral AI.", "metadata": {}}, {"text": "Our contribution to a global environmental standard for AI, Jul 2025.", "metadata": {}}, {"text": "URL https://mistral.ai/news/our-contribution-to-a-global-environmental-\nstandard-for-ai.", "metadata": {}}, {"text": "[70] Reuters.", "metadata": {}}, {"text": "Openai’s weekly active users surpass 400 million.", "metadata": {}}, {"text": "https://www.reuters.com/\ntechnology/artificial-intelligence/openais-weekly-active-users-surpass-\n400-million-2025-02-20/ , February 2025.", "metadata": {}}, {"text": "[71] Emma Roth.", "metadata": {}}, {"text": "Chatgpt now has over 300 million weekly users.", "metadata": {}}, {"text": "https://www.theverge.com/\n2024/12/4/24313097/chatgpt-300-million-weekly-users , December 2024.", "metadata": {}}, {"text": "[72] Shubham Singh.", "metadata": {}}, {"text": "Chatgpt statistics (2025): Dau & mau data worldwide.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "demandsage.com/chatgpt-statistics/, April 2025.", "metadata": {}}, {"text": "[73] Anthony Cardillo.", "metadata": {}}, {"text": "How many google searches are there per day?", "metadata": {}}, {"text": "(march 2025).", "metadata": {}}, {"text": "https:\n//explodingtopics.com/blog/google-searches-per-day , April 2025.", "metadata": {}}, {"text": "[74] OpenAI.", "metadata": {}}, {"text": "Introducing gpt-5.", "metadata": {}}, {"text": "https://openai.com/index/introducing-gpt-5/, 2025.", "metadata": {}}, {"text": "[75] Shaolei Ren, Bill Tomlinson, Rebecca W Black, and Andrew W Torrance.", "metadata": {}}, {"text": "Reconciling the\ncontrasting narratives on the environmental impact of large language models.", "metadata": {}}, {"text": "Scientific Reports,\n14(1):26310, 2024.", "metadata": {}}, {"text": "[76] John M Polimeni and Raluca Iorgulescu Polimeni.", "metadata": {}}, {"text": "Jevons’ paradox and the myth of technologi-\ncal liberation.", "metadata": {}}, {"text": "Ecological Complexity, 3(4):344–353, 2006.", "metadata": {}}, {"text": "[77] Aleksandar Ristic-Smith and Daniel J.", "metadata": {}}, {"text": "Rogers.", "metadata": {}}, {"text": "Compact two-phase immersion cooling with\ndielectric fluid for pcb-based power electronics.", "metadata": {}}, {"text": "IEEE Open Journal of Power Electronics, 5:\n1107–1118, 2024.", "metadata": {}}, {"text": "doi: 10.1109/OJPEL.2024.3432989.", "metadata": {}}, {"text": "17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.\nBatch Size DGPU UGPU total Unon-GPU total\n4 40-55% 10-13.5% 12.5%\n8 45-60% 5.5-7.5% 6.25%\n16 55-70% 3.5-4.5% 3.125%\nAppendices\nA Batch Size Sensitivity Analysis (GPT-4o)\nIn our main analysis, we adopt a batch size of 8 for all per-prompt energy estimations. This choice\nreflects a middle ground in real-world deployments, where AI providers typically batch requests in\nthe range of 4 to 16 to balance latency constraints with energy efficiency. However, the specific batch\nsize used during inference can significantly influence energy consumption due to changes in GPU\nand system utilization.\nTo assess this effect, we present a sensitivity analysis using GPT-4o as a representative model. The\nonly parameter varied is batch size, allowing us to examine how plausible batching configurations\ncan significantly shift energy outcomes. This variation underscores the rationale behind our use of\nbatch size 8 as a representative midpoint in real-world deployments.\nFigure 7: GPT-4o per-prompt energy consumption (Wh) across batch sizes and prompt lengths.\nTable 5 summarizes the utilization rates applied to each batch size, following the same method used\nin our methodology section 4, which drives the corresponding per-prompt energy estimates shown in\nFigure 7.\nThe results show substantial efficiency gains with higher batching: moving from batch size 4 to 8\nreduces energy per prompt by approximately 45%, while increasing from 8 to 16 yields a further 43%\nreduction. If we had used a batch size of 4 throughout our study, energy estimates would have been\nsignificantly higher, overstating the environmental footprint of LLM inference. Conversely, using a\nbatch size of 16 would have resulted in notably lower energy values, possibly underestimating the\nfootprint in more latency-constrained or low-traffic scenarios.\nThese differences highlight the critical role that batching decisions play in shaping the environmental\nfootprint of large-scale LLM deployments. As AI models utilize dynamic batching to address traffic\nand latency issues, adjusting the batch size can significantly impact the environmental footprint of\neach prompt. Large-scale providers like OpenAI have a significant advantage in this regard, as their\nhigh traffic volume allows them to rely on higher batch sizes without sacrificing latency to the same\nextent as smaller or less active deployments.\nB Scope 3 Considerations\nWhile this study focuses on operational emissions and resource consumption during inference (Scopes\n1 and 2), it is important to briefly discuss the Scope 3 impacts associated with the manufacturing,\ntransportation, and end-of-life disposal of the hardware used to power LLMs.\nScope 3 emissions are typically the most significant contributor to the lifecycle footprint of data center\ninfrastructure, encompassing embodied carbon from GPU fabrication, water usage in semiconductor\n18", "sentences": [{"text": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.", "metadata": {}}, {"text": "Batch Size DGPU UGPU total Unon-GPU total\n4 40-55% 10-13.5% 12.5%\n8 45-60% 5.5-7.5% 6.25%\n16 55-70% 3.5-4.5% 3.125%\nAppendices\nA Batch Size Sensitivity Analysis (GPT-4o)\nIn our main analysis, we adopt a batch size of 8 for all per-prompt energy estimations.", "metadata": {}}, {"text": "This choice\nreflects a middle ground in real-world deployments, where AI providers typically batch requests in\nthe range of 4 to 16 to balance latency constraints with energy efficiency.", "metadata": {}}, {"text": "However, the specific batch\nsize used during inference can significantly influence energy consumption due to changes in GPU\nand system utilization.", "metadata": {}}, {"text": "To assess this effect, we present a sensitivity analysis using GPT-4o as a representative model.", "metadata": {}}, {"text": "The\nonly parameter varied is batch size, allowing us to examine how plausible batching configurations\ncan significantly shift energy outcomes.", "metadata": {}}, {"text": "This variation underscores the rationale behind our use of\nbatch size 8 as a representative midpoint in real-world deployments.", "metadata": {}}, {"text": "Figure 7: GPT-4o per-prompt energy consumption (Wh) across batch sizes and prompt lengths.", "metadata": {}}, {"text": "Table 5 summarizes the utilization rates applied to each batch size, following the same method used\nin our methodology section 4, which drives the corresponding per-prompt energy estimates shown in\nFigure 7.", "metadata": {}}, {"text": "The results show substantial efficiency gains with higher batching: moving from batch size 4 to 8\nreduces energy per prompt by approximately 45%, while increasing from 8 to 16 yields a further 43%\nreduction.", "metadata": {}}, {"text": "If we had used a batch size of 4 throughout our study, energy estimates would have been\nsignificantly higher, overstating the environmental footprint of LLM inference.", "metadata": {}}, {"text": "Conversely, using a\nbatch size of 16 would have resulted in notably lower energy values, possibly underestimating the\nfootprint in more latency-constrained or low-traffic scenarios.", "metadata": {}}, {"text": "These differences highlight the critical role that batching decisions play in shaping the environmental\nfootprint of large-scale LLM deployments.", "metadata": {}}, {"text": "As AI models utilize dynamic batching to address traffic\nand latency issues, adjusting the batch size can significantly impact the environmental footprint of\neach prompt.", "metadata": {}}, {"text": "Large-scale providers like OpenAI have a significant advantage in this regard, as their\nhigh traffic volume allows them to rely on higher batch sizes without sacrificing latency to the same\nextent as smaller or less active deployments.", "metadata": {}}, {"text": "B Scope 3 Considerations\nWhile this study focuses on operational emissions and resource consumption during inference (Scopes\n1 and 2), it is important to briefly discuss the Scope 3 impacts associated with the manufacturing,\ntransportation, and end-of-life disposal of the hardware used to power LLMs.", "metadata": {}}, {"text": "Scope 3 emissions are typically the most significant contributor to the lifecycle footprint of data center\ninfrastructure, encompassing embodied carbon from GPU fabrication, water usage in semiconductor\n18", "metadata": {}}], "metadata": {"page": 18}}, {"text": "[Image page=18 idx=1 name=Im9.png] Size: 2000x450, Data: 67469 bytes", "sentences": [{"text": "[Image page=18 idx=1 name=Im9.png] Size: 2000x450, Data: 67469 bytes", "metadata": {}}], "metadata": {"page": 18, "image_index": 1, "image_name": "Im9.png", "image_width": 2000, "image_height": 450, "attachment_type": "image", "has_image_data": true, "image_data_size": 67469}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Figure 8: Cross efficiency DEA scores. Bar labels show the AI Index (top) and cross-efficiency score\n(bottom).\nmanufacturing, emissions from global logistics, and hardware retirement. For instance, Microsoft’s\nScope 3 CO2e emissions in 2023 accounted for 66% of the total emissions [16]. Yet, these values\nare highly variable across vendors, manufacturing locations, and fabrication nodes, and they lack\ndeployment-specific attribution when applied to real-time inference tasks.\nMoreover, given that many large-scale models are continually updated and deployed across evolving\ninfrastructures, ascribing a fixed fraction of embodied emissions or water per query is both method-\nologically fragile and likely to result in overestimation. Applying complete hardware manufacturing\nfootprints to ongoing inference, without amortizing them over the expected hardware lifespan or\nquery volume, risks artificially inflating per-query environmental costs.\nIn light of this, we excluded Scope 3 from our prompt-level framework, as its inclusion would\nintroduce non-trivial uncertainty and potentially distort comparative eco-efficiency across models.\nNevertheless, the long-term sustainability of AI infrastructure will depend on extending lifecycle\naccountability beyond the inference phase; future work is encouraged to adopt comprehensive\nlifecycle analyses (LCA) that integrate Scope 3 considerations once transparent and standardized\ndata become available.\nC Cross-effficiency DEA Results\nBefore presenting the eco-efficiency results, it is worth noting that Claude 3.5 Sonnet, Claude 3.5\nHaiku, GPT-4, and GPT-4 Turbo were excluded due to the lack of benchmark results on certain\ntests. Since cross-efficiency requires complete inputs and outputs, these models could not be fairly\nevaluated.\nAs shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini\nachieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and An-\nthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively\nmodest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These\nresults suggest that downsizing reasoning models can yield meaningful sustainability gains without\ncompromising performance.\nAt the opposite end, DeepSeek-R1 (0.067) and DeepSeek-V3 (0.059) recorded the lowest efficiency\nscores. Despite their advanced reasoning capabilities, their high energy, water, and carbon costs\nindicate significant infrastructural inefficiencies. Their Azure-hosted variants performed better,\nDeepSeek-R1 (0.539) and DeepSeek-V3 (0.523), yet remained below most OpenAI and Anthropic\nsystems. Among OpenAI models, GPT-4.1 mini (0.580) and GPT-4.1 nano (0.508) balanced output\nquality and sustainability particularly well. LLaMA models clustered between 0.4 and 0.6, reflecting\nefficient power use but limited reasoning performance.\n19", "sentences": [{"text": "Figure 8: Cross efficiency DEA scores.", "metadata": {}}, {"text": "Bar labels show the AI Index (top) and cross-efficiency score\n(bottom).", "metadata": {}}, {"text": "manufacturing, emissions from global logistics, and hardware retirement.", "metadata": {}}, {"text": "For instance, Microsoft’s\nScope 3 CO2e emissions in 2023 accounted for 66% of the total emissions [16].", "metadata": {}}, {"text": "Yet, these values\nare highly variable across vendors, manufacturing locations, and fabrication nodes, and they lack\ndeployment-specific attribution when applied to real-time inference tasks.", "metadata": {}}, {"text": "Moreover, given that many large-scale models are continually updated and deployed across evolving\ninfrastructures, ascribing a fixed fraction of embodied emissions or water per query is both method-\nologically fragile and likely to result in overestimation.", "metadata": {}}, {"text": "Applying complete hardware manufacturing\nfootprints to ongoing inference, without amortizing them over the expected hardware lifespan or\nquery volume, risks artificially inflating per-query environmental costs.", "metadata": {}}, {"text": "In light of this, we excluded Scope 3 from our prompt-level framework, as its inclusion would\nintroduce non-trivial uncertainty and potentially distort comparative eco-efficiency across models.", "metadata": {}}, {"text": "Nevertheless, the long-term sustainability of AI infrastructure will depend on extending lifecycle\naccountability beyond the inference phase;", "metadata": {}}, {"text": "future work is encouraged to adopt comprehensive\nlifecycle analyses (LCA) that integrate Scope 3 considerations once transparent and standardized\ndata become available.", "metadata": {}}, {"text": "C Cross-effficiency DEA Results\nBefore presenting the eco-efficiency results, it is worth noting that Claude 3.5 Sonnet, Claude 3.5\nHaiku, GPT-4, and GPT-4 Turbo were excluded due to the lack of benchmark results on certain\ntests.", "metadata": {}}, {"text": "Since cross-efficiency requires complete inputs and outputs, these models could not be fairly\nevaluated.", "metadata": {}}, {"text": "As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier.", "metadata": {}}, {"text": "o3-mini\nachieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and An-\nthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively\nmodest environmental footprint.", "metadata": {}}, {"text": "GPT-4o (Mar) (0.789) and o3 (0.758) also performed well.", "metadata": {}}, {"text": "These\nresults suggest that downsizing reasoning models can yield meaningful sustainability gains without\ncompromising performance.", "metadata": {}}, {"text": "At the opposite end, DeepSeek-R1 (0.067) and DeepSeek-V3 (0.059) recorded the lowest efficiency\nscores.", "metadata": {}}, {"text": "Despite their advanced reasoning capabilities, their high energy, water, and carbon costs\nindicate significant infrastructural inefficiencies.", "metadata": {}}, {"text": "Their Azure-hosted variants performed better,\nDeepSeek-R1 (0.539) and DeepSeek-V3 (0.523), yet remained below most OpenAI and Anthropic\nsystems.", "metadata": {}}, {"text": "Among OpenAI models, GPT-4.1 mini (0.580) and GPT-4.1 nano (0.508) balanced output\nquality and sustainability particularly well.", "metadata": {}}, {"text": "LLaMA models clustered between 0.4 and 0.6, reflecting\nefficient power use but limited reasoning performance.", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}, {"text": "[Image page=19 idx=1 name=Im10.png] Size: 2500x1000, Data: 180715 bytes", "sentences": [{"text": "[Image page=19 idx=1 name=Im10.png] Size: 2500x1000, Data: 180715 bytes", "metadata": {}}], "metadata": {"page": 19, "image_index": 1, "image_name": "Im10.png", "image_width": 2500, "image_height": 1000, "attachment_type": "image", "has_image_data": true, "image_data_size": 180715}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "In summary, eco-efficiency relies on both output quality and environmental cost. OpenAI’s smaller\nreasoning models and Claude 3.7 Sonnet strike that balance most effectively, while DeepSeek and\nLLaMA demonstrate the limitations of concentrating on capability or sustainability alone.\n20", "sentences": [{"text": "In summary, eco-efficiency relies on both output quality and environmental cost.", "metadata": {}}, {"text": "OpenAI’s smaller\nreasoning models and Claude 3.7 Sonnet strike that balance most effectively, while DeepSeek and\nLLaMA demonstrate the limitations of concentrating on capability or sustainability alone.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}]}