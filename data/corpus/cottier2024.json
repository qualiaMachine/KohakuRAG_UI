{"document_id": "cottier2024", "title": "The Rising Costs of Training Frontier AI Models", "text": "THE RISING COSTS OF TRAINING FRONTIER AI MODELS\nBen Cottier1 Robi Rahman1,2\nLoredana Fattorini2 Nestor Maslej2 Tamay Besiroglu1 David Owen1\nABSTRACT\nThe costs of training frontier AI models have grown dramatically in recent years, but there is limited\npublic data on the magnitude and growth of these expenses. This paper develops a detailed cost\nmodel to address this gap, estimating training costs using three approaches that account for hardware,\nenergy, cloud rental, and staff expenses. The analysis reveals that the amortized cost to train the most\ncompute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI:\n2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses\nare AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs\ninclude server components (15-22%), cluster-level interconnect (9-13%), and energy consumption\n(2-6%). If the trend of growing development costs continues, the largest training runs will cost more\nthan a billion dollars by 2027, meaning that only the most well-funded organizations will be able to\nfinance frontier AI models.\n1 Introduction\nThe large and growing cost of training state-of-the-art AI models has become an important issue in the field of\nAI [1]. Improving AI capabilities demand exponential increases in computing power, as evidenced by both economic\nanalysis [2] and the discovery of empirical scaling laws, which show that model performance improves with more\nparameters and training data [3, 4]. Dario Amodei, CEO of the AI lab Anthropic, has stated that frontier AI developers\nare likely to spend close to a billion dollars on a single training run this year, and up to ten billion-dollar training runs\nin the next two years [5]. Given this trend, some innovations, particularly those requiring large-scale training, may\nbecome inaccessible to all but the most well-funded organizations.\nAlthough it is widely known that training the largest ML models is expensive, until recently there were few concrete\nestimates of training costs in the public domain. In collaboration with Epoch AI, the 2024 AI Index presented one of\nthe most comprehensive datasets to date, estimating the costs of training runs based on cloud rental prices [ 6]. We\nbuild on that work with a more in-depth account of hardware, energy and R&D staff costs for both training runs and\nexperiments, as well as a more detailed analysis of how costs are increasing over time. To our knowledge, our study is\nthe most thorough analysis of model development costs to date.\nOur methods are built upon a comprehensive database of notable machine learning models [ 7], and informed by\ninterviews with industry experts. We consider three complementary approaches to measuring the cost of frontier\nmodels. The first approach estimates the hardware capital expenses (CapEx) amortized over the final training run,\nalong with the cost of hardware energy consumption. By considering AI accelerator chips, other server hardware,\nnetworking hardware, and energy separately, this approach can provide more accurate training costs. We find that the\nmost expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at\n$30M. Among frontier models, defined as models within the top 10 most compute-intensive models when they are\nreleased, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).\nWe then compare this approach to the cloud-price approach that was first presented in the AI Index [ 6]. Instead of\nestimating hourly compute costs in detail, the cloud-price approach simply uses historical rental rates from cloud\nplatforms. The cloud-price approach shows a similar growth rate (2.5× per year with a 90% CI of 2.1× to 3.1×), but\n1Epoch AI. 2Stanford University.\narXiv:2405.21015v2  [cs.CY]  7 Feb 2025\n\nFigure 1: Amortized hardware cost plus energy cost for the final training run of frontier models. The selected models\nare among the top 10 most compute-intensive for their time. Amortized hardware costs are the product of training\nchip-hours and a depreciated hardware cost, with 23% overhead added for cluster-level networking. Open circles\nindicate costs which used an estimated production cost of Google TPU hardware. These costs are generally more\nuncertain than the others, which used actual price data rather than estimates.\nyields costs that are about twice as large on average. We expect the cloud-price approach to overestimate frontier model\ncosts, since model developers usually either own or have private rental agreements for their training hardware. Using\nboth approaches helps validate our estimate of cost growth, while also highlighting the uncertainty of individual costs.\nOur third and most in-depth approach breaks down hardware, energy, and R&D staff costs over the entire development\nof the model (i.e. both experiments and training). We select four especially notable models for this approach—GPT-3,\nOPT-175B, GPT-4, and Gemini Ultra. For these models, we find that R&D staff costs including equity are between\n29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%.\nHowever, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware\ncosts and energy rise to 61–76% and 2–7% respectively.\nBy taking into account hardware purchase costs, energy costs, and the more opaque costs of R&D labor, our analysis\nprovides a clearer picture of the true costs of AI development. This sheds light on not only current costs but also the\neconomic hurdles that lie ahead as AI continues to scale.\nAll of our results can be reproduced using the code and data available at https://github.com/epoch-research/\ntraining-cost-trends.\n2 Methodology\n2.1 Datasets and frontier model selection\nOur investigation draws upon the Notable AI Models database, which documents 796 notable models across the history\nof machine learning [7]. Key details captured for each model include training compute, dataset size, and parameter\ncount. To focus on the largest-scale models, we initially filtered the database to models that had training compute\nestimates and that were published on or after 1 October 2015 (the start of the large-scale ML era according to [ 8])\nand up to 31 December 2023. This resulted in 276 selected models. For these models, we recorded the training time,\nhardware type and quantity, and utilization rate sourced from each model’s original publication, where possible.\nFor our main results, we examined 41 models that were historically at the frontier of compute. Specifically, we filtered\nfor models that were in the top 10 of training compute as of their release.1 Appendix B.1 provides further details on this\nselection procedure and a comparison to three alternative methods.\n1We excluded models that are fine-tuned versions of a separately listed model, to avoid double-counting costs.\n2\n\nIn addition to the data on machine learning models, we compiled a dataset of historical hardware prices, allowing us to\nestimate training costs. This price dataset contained cloud rental prices and hardware purchase prices for 24 different\nhardware models (e.g. NVIDIA A100) between 2015 and 2023. In total there were 142 entries, 52 of which were\npurchase prices and 90 of which were cloud rental prices.\n2.2 Amortizing the cost of hardware for training\nTo estimate the cost of hardware for a training run, we first calculated the cost to acquire the necessary accelerator chips\n(GPUs/TPUs), servers, and networking hardware. This involved looking up historical prices for GPUs, or estimating\nproduction costs for TPUs. Further details are provided in Appendix A.1.\nHardware normally remains available for future use after a training run finishes, but its value depreciates over time due\nto hardware progress. We amortized the cost of a training run based on this depreciation. Specifically, we depreciated\nthe value of hardware at a rate of r = 0 .14 orders of magnitude per year, based on the growth rate of ML GPU\nprice-performance [9].2 To get the value of the hardware at the start of training, we used the following formula:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere the difference in dates is in units of years. For example, if the training run starts one year after hardware is\nacquired, the start value is approximately 72% of the acquisition cost. We neglected the impact of hardware failures on\ndepreciation, as the effect seemed small compared to hardware progress. We provide evidence for that in Appendix A.3.\nAfter finding the initial value of the hardware, the amortized cost of the training run is then the portion of value that is\nlost during the training run. However, the training time is often difficult to determine, due to a lack of public information.\nMore often, we were able to estimate the number of chip-hours: the product of the training time and the number of\nchips. So we substituted chip-hours for the training time and the number of chips, using a linear approximation. This\nled to our final formula for amortized training cost:\nAmortized training cost ≈ Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nUp until Section 3.5, our results only account for the chip-hours of the final training run. In Section 3.5, we scale up the\nchip-hours to account for all experiments towards developing an AI model. Although the amortized cost model involves\nseveral estimates and approximations, our results are robust to reasonable changes in the method (see Appendix A.3 for\nfurther methodological details and Appendix B.3 for the sensitivity analysis).\n2.3 Hardware energy consumption cost\nIn addition to the capital costs of hardware, we also considered the cost of energy consumed by hardware during model\ntraining. We estimated this using the following formula:\nTotal energy cost of training = Energy cost rate ($/kWh) × Hardware TDP (kW) ×\nAverage power to TDP ratio (%) × Data center PUE × Training chip-hours (h)\nwhere TDP is thermal design power and PUE is power usage effectiveness, which accounts for the overhead of data\ncenter power distribution and cooling. We selected the energy cost rate by year, hardware TDP by the hardware type,\naverage power to TDP ratio by the hardware manufacturer, and the data center PUE by the ML model developer. These\nwere set based on hardware manufacturers’ literature. However, some parameters such as average power to TDP\nratio could not be found in technical specifications and had to be estimated. For references and method details, see\nAppendix A.4.\n2.4 Cloud compute cost\nWhile the amortized hardware CapEx + energy approach is a bottom-up method that accounts for hardware and energy\ncosts, cloud rental prices offer a simpler method. Many AI labs rely on cloud computing services to train their models,\nand the associated costs are often more readily available and easier to estimate. By comparing our bottom-up estimates\n2This growth rate measures improvement at 32-bit precision. One-time improvements from lower-precision and tensor number\nformats would make the rate faster, but this was not estimated. This also assumes that hardware improves continuously. In reality,\nhardware improves in increments with each new release.\n3\n\nwith those derived from cloud rental prices, we can validate our approach and provide a more comprehensive picture\nof AI training costs. The cloud approach also allows estimates of model training cost that are not possible using the\namortized hardware CapEx + energy approach and our data. However, note that the cloud approach can overestimate\nthe cost of models whose developers used their own hardware rather than renting compute from a cloud provider.\nTo estimate training costs from cloud rental prices, we used the following formula:\nTotal cost = Price per chip-hour × Training chip-hours\nThe price per chip-hour was obtained from our hardware price database, which includes prices for various hardware\ntypes, cloud providers, and rental dates. We matched the hardware type and publication date of each ML model with the\nmost appropriate price, using the developer of the ML model to determine the most likely cloud provider (e.g., Google\nCloud for Google labs, Microsoft Azure for OpenAI). See Appendix A.5 for further details.\n2.5 Total amortized model development cost\nAlthough the final training run ultimately determines an AI model’s capabilities and impact, the research and develop-\nment surrounding it is crucial. We therefore used a third approach that considers all of the compute that went into model\ndevelopment, as well as the cost of R&D staff developing the model. Since this approach was more time-intensive, and\nrelied on having a list of contributors to estimate R&D staff cost, we applied it to just four models: GPT-3, OPT-175B,\nGPT-4, and Gemini Ultra.\nTo estimate the compute cost over model development—including experiments, failed attempts, evaluation and fine-\ntuning—we applied a multiplicative factor to the final training run compute. We estimated this factor based on\nevidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta.\nAppendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90%\nCI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.\n2.5.1 R&D staff costs\nResearch and development (R&D) staff costs are an often-neglected component of the total cost of developing ML\nmodels. These costs include the salaries and equity compensation of the researchers, engineers, and managers in the\nproject, but excludes operations staff and data center employees. We set out to better quantify these costs for a few\nselected models to see how significant they are relative to the hardware costs.\nWe estimated total annual compensation of R&D personnel by multiplying the estimated full-time equivalent workload\nper contributor by their compensation and by the total time spent on model development. Since these parameters were\nall quite uncertain, we sampled from log-normal distributions over each parameter.\nFor full-time equivalent workers, we were informed by the type and number of contributors listed on the research paper.\nFor all models except Gemini Ultra, we sampled full-time equivalent workloads from a 90% credible interval of 5% to\n80% FTE for each contributor, resulting in a median of 20%. For Gemini Ultra, we used different workloads for each\ntype of contributor listed [10, pp. 66–69].\nFor compensation, we were informed by company-specific data from https://www.levels.fyi/ and https:\n//aipaygrad.es/. From levels.fyi, we used data for Google Software Engineers from level 3 to level 8. From\naipaygrad.es, we used the overall statistics for all companies and all roles (researchers, engineers and managers). After\naveraging the two sources, base salaries were modeled with a 90% CI of $140K to $160K, and equity with a 90% CI of\n$35K to $490K. We applied an overhead factor of 1.25x to 1.4x to base salaries to account for taxes and benefits [11],\nresulting in total compensation with a 90% CI of $210K to $690K and a median of $330K.\nActual staff compensation may vary significantly between AI labs. The chosen estimate of compensation may be\nparticularly unreliable for small and early companies, such as OpenAI in its earlier years, where there are many\nuncertainties about how to value equity compensation. However, these numbers serve as a reasonable baseline, and our\nestimates provide a useful starting point to analyze R&D labor costs.\n4\n\n3 Results\n3.1 Amortized training costs of frontier models have grown by 2.4x per year since 2016\nThe amortized training costs of frontier models have increased by a factor of 2.4x per year since 2016. This is the result\nof the preferred amortized hardware CapEex + energy approach, shown in Figure 2. Table 1 compares this to the cloud\napproach, which yields a similar growth rate of 2.5× per year. The growth rate is also similar if we vary hardware\ndepreciation or training start date within reasonable limits (see Appendix B.4). However, the growth rate rises to 2.9x\nper year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.\nApproach N× increase per year OOMs/year Doubling Time (months) R-squared N\nAmortized hardware\nCapEx + energy\n2.4 [2.0, 2.9] 0.38 [0.29, 0.47] 9 [8, 12] 0.58 41\nAmortized hardware\nCapEx + energy—no TPUs\n3.0 [2.4, 3.7] 0.47 [0.37, 0.57] 8 [6, 10] 0.77 22\nRenting from the cloud 2.5 [2.1, 3.1] 0.40 [0.32, 0.48] 9 [7, 11] 0.66 36\nTable 1: Cost growth rates based on log-linear regression, for different cost estimation approaches. All approaches select\nthe top 10 most compute intensive models at the time of model release. N refers to the number of relevant observations.\nBased on a two-sided t-test adjusted for correlation of residuals, the growth rates for amortized hardware capex + energy\nand cloud are not significantly different (p = 0.13). However, when the costs of models trained with estimated TPU\nproduction costs are excluded, the growth rate rises significantly to 2.9x per year (p < 0.01). OOMs/year: orders of\nmagnitude per year. Square brackets: 90% confidence interval.\nFigure 2: (Reproduction of Figure 1 for convenience.) Amortized hardware cost plus energy cost for the final training\nrun of frontier models. The selected models are among the top 10 most compute-intensive for their time. Amortized\nhardware costs are the product of training chip-hours and a depreciated hardware cost, with 23% overhead added for\ncluster-level networking. Open circles indicate costs which used an estimated production cost of Google TPU hardware.\nThese costs are generally more uncertain than the others, which used actual price data rather than estimates.\nEstimating costs from cloud rental prices, although less representative of actual costs, has the advantage of simplicity.\nThe cloud cost approach also helps to check the robustness of the amortized hardware CapEx + energy approach.\nFigure 3 shows the trend of cloud compute cost to train models among the top 10 most compute-intensive as of their\nrelease. Note that some of these estimates previously appeared in the 2024 AI Index report [6]. We find that the cost of\ntraining models based on cloud rental prices has grown by 2.5× per year since 2016, with a 90% CI of 2.1× to 3.1×.\n5\n\nFigure 3: Estimated cloud compute costs for the final training run of frontier models. The selected models are among\nthe top 10 most compute-intensive for their time. The costs are the product of the number of training chip-hours and a\nhistorical cloud rental price.\nThis is consistent with the amortized hardware CapEx + energy approach, as shown in Table 1. This shows that our\ntrend estimates are robust to two different ways of estimating prices per chip-hour.\nOverall, these results suggest that the cloud approach is valid for estimating growth rates in compute costs, and has the\nadvantage of simplicity. However, public cloud rental prices are less reliable for individual model costs when the model\ndeveloper owns the hardware or has a special partnership with a cloud provider.\n3.2 The trend suggests that the most expensive publicly announced model will cost one billion dollars to train\nby the start of 2027\nThe growth rate in training cost indicates how rapidly AI investment is scaling. We can use this growth rate to extrapolate\nthe cost of the largest training run. Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.\nGPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive\npublicly announced model by the start of 2027 will cost about $1 billion.\nWhether this cost is justified hinges on how profitable the resulting AI model is—but parts of the AI industry believe it\nis worthwhile. The CEO of the AI lab Anthropic has claimed that close to a billion dollars will already be spent on a\nsingle training run in 2024 (implying an amortized cost), which is even sooner than the historical trend suggests [5].\n3.3 Hardware acquisition costs are one to two orders of magnitude higher than amortized costs\nIt’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime\nof the hardware, and the acquisition cost of purchasing that hardware outright. The choice of which cost to consider\ndepends on the purpose of the analysis. Amortized costs are more relevant for understanding the economics of training\nand deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and\nfinancial risks involved in developing such models.\nTo illustrate the difference between amortized hardware cost and acquisition cost, Figure 4 shows the acquisition costs\nwe were able to estimate using hardware purchase prices and training hardware quantities. Since this is the up-front\ncost of acquiring the hardware, the costs are one to two orders of magnitude larger than amortized hardware costs.\nFor example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the\namortized hardware CapEx + energy cost. The ratio between the two depends on when and for how long the model is\ntrained.\nBased on 40 estimates of acquisition cost, we find a growth rate of2.5× per year (90% CI: 2.1×, 3.0×). This is slightly\nfaster than the rate of 2× per year suggested by amortized costs (2.4× per year) divided by training times (1.2× per\n6\n\nyear) [13]. The discrepancy is due to different AI models appearing in each analysis, highlighting a source of sensitivity\nin our results.\nFigure 4: Estimated hardware acquisition costs to train frontier models. The selected models are among the top 10 most\ncompute-intensive for their time. The costs are the product of the number of servers and the earliest available server\nprice, with about 23% overhead added for cluster-level networking hardware.\n3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips\nBreaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes\ntoward AI accelerator chips. The rest of the server (including markup) makes up 29% of the cost, while cluster level\ninterconnect makes up 17%.\nEnergy makes up the remainder of costs, averaging 9% but varying across models. Although this is a small fraction,\nit corresponds to rapid growth in energy use and power requirements over time. The trend in power requirements is\nprovided in Appendix C.\nNote that this breakdown does not include all costs associated with an AI supercomputer. Other costs include the data\ncenter infrastructure besides servers and networking, as well as data center personnel and maintenance.\n3.5 R&D staff are a significant fraction of costs over the whole model development process\nWe now use our third cost estimation approach to examine how the cost of labor from researchers and engineers\ncompares to the amortized cost of compute. Unlike the previous approaches, which only measured the cost of the final\ntraining run, this approach counts compute usage throughout model development including experiments, fine-tuning\nand evaluation. Figure 6 shows the cost breakdown for GPT-3, OPT-175B (notable as a GPT-3 replication attempt by a\nteam at Meta AI), the original GPT-4 model by OpenAI, and the original Gemini 1.0 Ultra model by Google DeepMind.\nWe find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model\ndevelopment costs, depending on the model. Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5\nfor additional plots). Notably, this fraction does not change much from GPT-3 to GPT-4, which spans three and a half\nyears of AI progress. The number of reported contributors increased from 25 for GPT-3 [14] to 284 for GPT-4 [12],\nwhile the amortized hardware cost over the whole model development increased from $4M to $90M. However, due to\nthe limited data, we caution against extrapolating the fraction of R&D staff costs to future frontier models.\nGemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier\nmodels. Firstly, Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators,\nand this makes the hardware cost relatively low. Secondly, our methodology is limited by deriving the number of\nfull-time equivalent staff from the reported number of contributors, for which Gemini had 941—much higher than\n7\n\nB r e a k d o w n  o f  a m o rt i z e d  h a r d w a r e  a n d  e n e r g y  c o s t s  f o r  f r o n t i e r  A I  m o d e l s\nA I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\nG e m i n i  1 . 0  U l t r a\nI n f l e c t i o n - 2\nF a l c o n - 1 8 0 B\nP a L M  2\nG P T - 4\nG P T - 3 . 5  ( t e x t - d a v i n c i - 0 0 3 )\nB L O O M - 1 7 6 B\nG L M - 1 3 0 B\nP a r t i\nO P T - 1 7 5 B\nP a L M  ( 5 4 0 B )\nL a M D A\nG L a M\nG o p h e r  ( 2 8 0 B )\nM e g a t r o n - T u r i n g  N L G  5 3 0 B\nH y p e r C L O V A  8 2 B\nG O A T\nB y T 5 - X X L\nP r o t T 5 - X X L\nM e t a  P s e u d o  L a b e l s\nS w i t c h\nD A L L - E\ni G P T - X L\nG P T - 3  1 7 5 B  ( d a v i n c i )\nT u r i n g - N L G\nM e e n a\nA l p h a S t a r\nT 5 - 1 1 B\nM e g a t r o n - L M  ( 8 . 3 B )\nM e g a t r o n - B E R T\nR o B E R T a  L a r g e\nB i g G A N - d e e p  5 1 2 x 5 1 2\nA l p h a Z e r o\nA l p h a G o  Z e r o\nJ F T\nM o E\nA l p h a G o  M a s t e r\nP o l y N e t\nX c e p t i o n\nG N M T\nD e e p S p e e c h 2  ( E n g l i s h )\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nFigure 5: The percentage of the amortized hardware CapEx + energy estimates made up by different hardware and\nenergy costs. Note that the breakdown across models is approximate. Cluster-level interconnect is assumed to be a\nconstant 19% fraction of the cluster CapEx, and the proportion of server components is based on only three comparisons\nbetween NVIDIA DGX server prices and single GPU prices (see Appendix A.1 for details). The energy costs are more\nspecific, varying with the number of training chip-hours and the hardware (see Appendix A.4).\n8\n\nA m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 . 0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 1 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 . 0  U l t r a\n3 3 % 3 1 % 2 1 % 1 2 % 3 %\n4 3 % 2 7 % 1 8 % 1 1 %\n2 9 % 3 2 % 1 2 % 6 %\n4 9 % 2 3 % 1 5 % 9 % 5 %\n(b)\nFigure 6: (a) Breakdown of total amortized model development costs for selected models. Hardware costs are amortized\nto the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of\ndevelopment from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar\nvalues are medians. (b) Costs components as a percentage of the total, based on median estimates.\nGPT-4 at 284 contributors. Though we assumed a very small contribution from the 428 people under the “Contributors”\nrole—a median full-time equivalent of about 1%—the estimate may still be too high.\nOn the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while\nenergy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise\nto 61–76% and 2–7% respectively. Note that while energy consumption is a small fraction of total cost, this doesn’t\nentail that power requirements are not a challenge in frontier AI development. Regulatory and logistical hurdles to\nsecure power supplies may cause bottlenecks in the coming years, but we leave that topic to future work.\n4 Discussion\n4.1 Implications\nThe rapid growth in AI training costs will have a major impact on the future of AI development. Our findings suggest\nthat if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed\none billion dollars by 2027. Given the potential bias in our estimates’ absolute values, this may happen even sooner—as\nsuggested by cloud-price costs, and news reporting on training costs [5]. If realized, this level of investment is likely to\ndrive rapid advances in AI capabilities, given the track record of scaling up AI models.\n9\n\nHowever, only a handful of large companies and government institutions have the financial resources to operate at\nthis frontier. This concentration of AI development could limit the range of perspectives and approaches considered,\nespecially from academia and broader society. Both AI developers and policymakers must grapple with the rapid AI\nadvances brought on by increasing investment, as well as the tradeoffs involved in the concentration of AI development.\nOn one hand, having few key players at the frontier could make it easier for them to coordinate on responsible AI\ndevelopment. On the other hand, this raises concerns about a lack of public oversight for such a powerful technology.\n4.2 How to estimate training costs\nWe used two approaches to estimate the cost of final training runs: the amortized hardware CapEx + energy approach,\nand the cloud rental price approach. These two approaches produced consistent estimates of the growth rate in training\ncost over time. However, the approaches diverged on individual costs: the cloud costs were twice as large on average.\nWe recommend using the amortized hardware CapEx + energy approach for frontier models wherever it’s feasible,\nbecause it accounts for the lower costs in practice for large training runs, and can be broken down into components.\nOur third approach adds the cost of R&D staff, as well as the compute cost of experiments, evaluations, and fine-tuning\ninvolved in model development. To our knowledge, we present the first detailed estimates of these costs for GPT-3,\nOPT-175B and Gemini Ultra. Moreover, our results suggest that R&D staff costs were a major component of total costs\nfor these frontier models. Although this is the most comprehensive of the three approaches, further data collection and\nevidence on the AI development process are needed before we can recommend it as the default.\n4.3 Limitations\nWhile our study provides valuable insights into the growth of AI training costs, there are important limitations. The\nanalysis relies on publicly available information, which may lead to biases or gaps in the dataset. Cost estimation\nmethods are subject to uncertainties due to factors such as hardware depreciation rates and pricing dynamics. Moreover,\nour methods neglect several costs that are potentially significant, including the data center infrastructure apart from the\ntraining cluster, and the acquisition of data for model training.\nOur results may also have limited generality. The trends observed for the selected frontier models may not generalize to\nthe broader AI landscape, or specific AI domains such as language modeling. Rapid innovation could also lead to large\ngains in hardware and software efficiency that are difficult to predict from historical data. Further research on all of\nthese unknowns would help refine our insights, and inform evidence-based strategies to respond to growing financial\nbarriers in ML.\n5 Conclusion\nIn this paper we used three approaches to analyze the cost of training ML models at the frontier. The first two\napproaches—one based on hardware purchase prices and energy costs, the other based on cloud rental prices—indicate\nthat the amortized cost of compute for these training runs has grown by around 2.4x per year (90% CI: 2.0x to 2.9x)\nsince 2016. This shows the large role of investment in driving AI progress.\nBreaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and\nGemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. This motivates further\nresearch on the scaling of R&D labor with computing power.\nThe rapid exponential growth of costs over eight years suggests that growth is unlikely to stall in the next few years.\nHowever, frontier AI labs appear to face non-trivial challenges to scaling further. One such challenge is securing enough\npower capacity for increasingly large computing clusters. Analyzing potential bottlenecks such as this is an important\ntopic for future work.\nThe rapid increase in AI investment is likely to drive major advances in AI capabilities. Given that total model\ndevelopment costs at the frontier are already over $100 million, these advances may only be accessible to the largest\ncompanies and government institutions. The concentration of such a powerful technology among a few key players\nraises questions about responsible development and deployment. Both AI developers and policymakers must engage\nwith these issues and consider the tradeoffs involved. The stakes are high—decisions made now about the governance\nand trajectory of AI could have profound consequences for society.\n10\n\nAcknowledgements\nWe thank Bartosz Podkanowicz for assisting with data collection, Luke Frymire for assisting with ground truth cost\nverification, and Josh You for copyediting. We thank Igor Molybog, Yafah Edelman and Horace He for helpful\nconversations about the training process and requirements for creating large ML models. We thank Konstantin Pilz,\nYafah Edelman, Tom Davidson, Isabel Juniewicz, Carl Shulman, Jaime Sevilla, Aleksandar Kostovic, Tim Fist, Haydn\nBelfield, Alan Chan, David Patterson, Mauricio Baker, Erich Grunewald, and Cullen O’Keefe for feedback on drafts.\nThis study was supported by a grant from the AI Index based out of the Stanford Institute for Human-Centered Artificial\nIntelligence. The cloud compute cost estimates shown here previously appeared in the 2024 Stanford AI Index Report.\nReferences\n[1] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al. Artificial Intelligence Index Report 2023. arXiv\npreprint arXiv:2310.03715, 2023.\n[2] Neil C Thompson, Shuning Ge, and Gabriel F Manso. The importance of (exponentially more) computing power.\narXiv preprint arXiv:2206.14007, 2022.\n[3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022.\n[5] Ezra Klein and Dario Amodei. What if Dario Amodei Is Right about A.I.? https://www.nytimes.com/\n2024/04/12/opinion/ezra-klein-podcast-dario-amodei.html?showTranscript=1 , 2024. Accessed:\n2024-05-30.\n[6] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al. Artificial Intelligence Index Report 2024, 2024.\n[7] Epoch AI. Parameter, Compute and Data Trends in Machine Learning.https://epochai.org/data/epochdb/\nvisualization, 2022. Accessed: 2024-05-30.\n[8] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute\ntrends across three eras of machine learning. In 2022 International Joint Conference on Neural Networks (IJCNN) ,\npages 1–8. IEEE, 2022.\n[9] Marius Hobbhahn, Lennart Heim, and Gökçe Aydos. Trends in Machine Learning Hardware. https://epochai.\norg/blog/trends-in-machine-learning-hardware , 2023. Accessed: 2024-05-30.\n[10] Gemini Team. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2024.\n[11] Barbara Weltman. How Much Does an Employee Cost You? . https://www.sba.gov/blog/\nhow-much-does-employee-cost-you , 2019. Accessed: 2024-05-30.\n[12] OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2024.\n[13] Epoch AI. The length of time spent training notable models is growing. https://epoch.ai/data/\nnotable-ai-models#training-time-growth , 2024. Accessed: 2024-12-09.\n[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877–1901, 2020.\n[15] NVIDIA Corporation. NVIDIA DGX SuperPOD Reference Architecture, 2023.\n[16] Tim Fist. Personal communication, 2024.\n[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates,\nSuresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In\nProceedings of the 44th annual international symposium on computer architecture , pages 1–12, 2017.\n[18] The Information. Google discussed dropping Broadcom as their AI chips supplier, 2023. Accessed: 2024-05-30.\n[19] Dylan Patel and Gerald Wong. AI server cost analysis – memory is the biggest loser, 2023. Accessed: 2024-05-30.\n11\n\n[20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/\n1691877797487165443, 2024. Accessed: 2024-05-30.\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs , 2024.\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs , 2024.\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,\nRoman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei\nZhou, and David Patterson. Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) , pages 1–14, 2021.\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet , 2023.\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint\narXiv:2104.10350, 2021.\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems) , 4 2023.\nVersion 01.\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a\nComputer: Designing Warehouse-scale Machines, 2018.\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Ac-\ncessed: 2024-05-30.\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI\nDatacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race , 2024.\nAccessed: 2024-05-30.\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=\nepmt_5_6_a, 2024. Accessed: 2024-01-15.\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and\nopportunities. Proceedings of Machine Learning and Systems , 4:795–813, 2022.\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/\nwhy-gpus-are-great-for-ai/ , 2023. Accessed: 2024-05-30.\n[38] Electricity generation, capacity, and sales in the United States. https://web.archive.\norg/web/20240407085026/https://www.eia.gov/energyexplained/electricity/\nelectricity-in-the-us-top-10.php , 2022. Accessed: 2024-05-29.\n12\n\nA Training cost estimation\nA.1 Hardware acquisition cost\nThe frontier AI models in our dataset were trained on clusters of many GPU or TPU chips (or “chips” for short). We\nset out to estimate the total cost of the chips, servers, and networking equipment in these clusters, which we call the\nhardware acquisition cost. This cost is calculated as follows:\nHardware acquisition cost = Acquisition cost per chip × Number of chips\nWhere “Acquisition cost per chip” accounts for the GPU or TPU chip itself, other server costs (CPUs, memory,\nchip-to-chip networking, and markup), and the cost of server-to-server networking equipment. Table 2 shows how we\ncalculated this quantity depending on what was known about the training hardware.\nKnown information Formula for “Acquisition cost per chip”\nSingle GPU price GPU chip price × Chip-to-server factor × Server-to-cluster factor\nGPU server price GPU server price / GPUs per server × Server-to-cluster factor\nChips are TPUs TPU chip cost × Chip-to-server factor × Server-to-cluster factor\nTable 2: The formula to estimate “Acquisition cost per chip”, depending on what is known about the training hardware.\nIf the training hardware was a GPU, we looked up the earliest known price linked to that GPU. If the price we found\nwas for a single GPU, we multiplied that price by a “chip-to-server” cost factor (detailed later). If the price we found\nwas instead for a DGX server, we divided that price by the number of GPUs per server.3 Finally, if the training hardware\nwas a Google TPU, we used the “Geometric mean” production cost estimate from Table 3. This represents the cost of\nthe TPU chip itself, which we then multiplied by a chip-to-server cost factor.\nWe calculated chip-to-server cost factors based on known DGX and single-GPU prices near release, using the formula\n(DGX cost)/(8×GPU cost). We were able to estimate this for the NVIDIA P100 (1.54×), V100 (1.69×), and A100\n(1.66×). For other NVIDIA chips, and for TPUs, we used the mean of these three known factors (1.64×). We assumed\nthat the DGX server prices included the cost of chip-to-chip interconnect switches and transceivers.4 We did not account\nfor financing, i.e. the interest paid on a loan to purchase the hardware up-front.\nOnce we had the cost per chip for a single server, we added the cost of server-to-server networking equipment. We\nused an estimate by Kostovic (forthcoming), based on the reference architecture of the NVIDIA H100 SuperPOD [15].\nAccording to this estimate, approximately 19% of the total cost of the SuperPOD goes towards cluster-level interconnect\nfor configurations with less than 4096 GPUs, and 20% for 4096 GPUs and above, due to an additional third layer of\nswitches. Consistent with these figures, another expert in AI hardware estimated a range of 10% to 20% for A100-based\nclusters with an Infiniband network [16].\nFor simplicity, we assumed that 19% of the hardware acquisition cost was for server-to-server networking equipment. We\ntherefore multiplied the cost per chip for a single server by a “server-to-cluster” factor of100%/(100%−19%) ≈ 1.23×,\nresulting in the final “Acquisition cost per chip”. We assumed that the overhead factor is accurate for TPU servers as\nwell as GPU servers, though we have substantial uncertainty about this. In reality, the proportion of costs varies with\nthe cluster architecture and size.\nA.2 Cost of Google TPUs\nTensor Processing Units are a class of proprietary AI accelerator hardware developed by Google, and used in their\ninternal computing projects and employed in Google Cloud datacenters [17]. These chips are not available for sale, but\nsome of them can be rented on the cloud. Since they have never been sold, there are no available purchase prices, which\nmakes it more difficult to estimate the amortized capital expenses for Google Brain, DeepMind, and other Google labs.\n3HGX servers are more suited to large-scale, customized AI supercomputers, but we found very little information on their pricing,\nso we used DGX pricing.\n4Some of the NVIDIA product pages where we found hardware prices listed NVSwitches as components, but it was unclear\nwhether NVLink cables for chip-to-chip links were included.\n13\n\nTo estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU\nmanufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end\nestimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead\nof co-designing TPUs with Broadcom [18]. The second approach models the equivalent purchase prices of Google\nTPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We\nconsider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We\ninterpolated hardware costs based on price-performance:\nTPU effective cost = GPU cost × TPU performance\nGPU performance × date adjustment factor\nwhere the date adjustment factor adjusts costs compared on different dates to make them comparable, based on the\ntrend that GPU performance per unit cost improves at a rate of 0.14 orders of magnitude per year.\nFor the manufacturing cost approach, we estimated the manufacturing cost for the NVIDIA DGX SuperPOD at $8,665\nper GPU. This estimate was informed by [19] and [20]—calculations are available in ‘h100_manufacturing_cost.ipynb‘.\nAfter converting this to the TPU cost using the above formula, we divided by the average server-to-chip cost ratio\nof 1.64 that we estimated from NVIDIA GPU prices (see Appendix A.1). The results are listed in Table 3. For the\nequivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google\nML GPUs, listed in Table 4 [21, 22, 23, 24].\nH100 TPU v1 TPU v2 TPU v3 TPU v4\nRelease date 2022-09-21 2015-05-20 2017 2018 2021\nPerformance ratio to H100 100% 5% 9% 12% 28%\nDate adjustment factor 1.00 10.66 5.38 3.90 1.48\nServer manufacturing cost (per chip) $8,665 $4,295 $4,244 $4,200 $3,570\nChip manufacturing cost (estimate) $5,346 $2,650 $2,619 $2,591 $2,203\nPrice of equivalent GPU (estimate) - $11,263 $9,752 $10,742 $12,119\nGeometric mean - $5,463 $5,054 $5,276 $5,176\nTable 3: Cost and performance comparison between Google TPUs and the NVIDIA H100. Performance ratios to the\nNVIDIA H100 use the same number format and are without sparsity. Our overall estimate of TPU costs is the geometric\nmean of estimates for the chip manufacturing cost and the price of an equivalent-performance GPU.\nK80 P100 PCIe 16GB V100 SXM2 32GB A100 SXM4 40GB\nRelease date 2014-11-17 2016-06-20 2018-03-27 2020-05-14\nPerformance in TFLOPS 8 (FP32) 19 (FP16) 125 (FP16) 312 (FP16)\nMemory 24 GB 16 GB 32 GB 40 GB\nSale price $5,000 (at release) $5,699 (at release) $11,458 (2018-05-08) $15,000 (2020)\n$3,700 (2016) $12,500 (2022)\nTable 4: Comparison of GPU specifications. By interpolation between GPUs, and their price-performance data, we\nestimate performance-equivalent prices for TPU versions.\nAs explained above, we consider the manufacturing costs to be low estimates and the equivalent GPU prices to be\nhigh-end estimates of the full production cost. To aggregate the two approaches into a final estimate, we took the\ngeometric mean, as shown in Table 3. Each TPU version has an estimated cost (for Google) of about $5,000.\n14\n\nA.3 Amortization model\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware\navailability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware\nannouncement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU\nactually being shipped to the buyer. Our results are robust to variations in this buffer time—see Appendix B.4.\nFor the training start date, there were a few known cases—for example, GPT-4 finished training in August 2022 [ 12].\nOtherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account\nfor time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the\ntraining time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\nThe precise way to amortize the training cost through exponential depreciation is:\nAmortized training cost = Start value per chip × Number of chips × Depreciation during training\n= Start value per chip × Number of chips ×\n\u0010\n1 − exp\n\u0002\n− Training time × r ln 10\n\u0003\u0011\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training\ntime or the number of chips separately. This is because chip-hours can also be estimated from training compute in\nFLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these\nchip-hour estimates:\nAmortized training cost = Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nThis approximation is valid if (Training time) × r ln 10 is small, and this is the case for the training times in our data and\nour choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 × 0.14 ln(10) ∼= 32% deprecation\ncompared to 1 − exp(−1 × 0.14 ln(10)) ∼= 28% depreciation. This is not a large difference relative to other sources\nof uncertainty.\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a\nsignificant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per\nweek occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all\ncatastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs\ndefective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.\nA.4 Energy cost estimation\nTo model the cost of energy consumed by hardware during a training run, we started with the thermal design power\n(TDP) of the GPU or TPU used for training. We then scaled this up to estimate the TDP of one server. For TPUs, the\nserver scale-up was based on data from Table 1 of [26]. For NVIDIA GPUs, we used specifications such as [27].\nNext, we converted TDP to the average power actually consumed during training. For TPUs we used an average value\nof 43% using data on TDP and average power in [28, Table 4], as well as data on TDP in [26, Table 1]. For GPUs we\nalso aggregated multiple sources ([29],[28], and [30, p. 133]), arriving at an all-things-considered estimate of 75%.\nTo account for power consumed by data center power distribution and cooling, we multiplied average server power by\nthe power usage effectiveness. We used a PUE of 1.1 for data centers owned by hyperscalers such as Alphabet and\nMicrosoft, based on [28, Table 4] and a statement by Meta [31]. Otherwise, we used 1.25 [32].\nTo get the total energy cost of the training run, we multiplied the energy consumption by the average industrial electricity\nprice in the model publication year [33].\nA.5 Cloud price selection\nTo estimate training costs from cloud rental prices, we matched the hardware type and publication date of each ML\nmodel with a price from the hardware price database. To do this, we first filtered the database for prices which matched\n15\n\nthe hardware type and the most likely cloud provider that would be used, with the latter based on the developer of\nthe ML model, e.g. using Google Cloud for any Google lab, and using Microsoft Azure for OpenAI based on their\npartnership with Microsoft. We then estimated the date of hardware procurement as the publication date minus the\ntraining time, and minus a further 2 months to account for preparation before the training run.5 If the training time was\nunavailable, we used the median value of approximately 33 days.\nFinally, we searched the price database for the price per chip-hour that was dated nearest to the estimated procurement\ndate. We defaulted to the price for a 3-year rental commitment. Based on a few custom quotes we requested from cloud\nproviders, we found that actual cloud computing prices are negotiable and can be substantially lower than publicly\nlisted prices. We concluded that a 3-year commitment price is the closest on average to what developers would be\nquoted, even if they make a shorter commitment.\nPrices were not available for every specific combination of hardware, cloud provider, and rental date, so we used\nseveral fallbacks to select the most closely applicable cloud rental price, for example using nearest prices in time, using\nprices for similar hardware models, etc. The full procedure is provided at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/prices.py#L210-L294 .\nA.6 Accounting for compute used throughout model development\nIt is important to consider compute used throughout model development. The cost of experiments, evaluations, and\nfine-tuning reflects actual costs for developers to research and possibly deploy a useful ML model. This compute is not\nonly important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges\nfrom 1.2x to 4x, with a median of 2.2x.\nOne source of evidence on the allocation of compute is the training of smaller model sizes for a given architecture. For\nexample, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6 × parameters × tokens) [14, Table\n2.1]. This shows at least 14% of compute was spent outside the main training run. Similar reasoning for BLOOM\nreveals about 63% of compute was used on smaller models [25, Table 5].\nAnother source of evidence is reports of how compute budgets are allocated. For example, the OPT-175B developers\nestimated total cost at “roughly 2x higher” than the largest training run [34]. Meanwhile, across Meta’s AI infrastructure,\none estimate in the literature suggested a 1:2 ratio between experimentation and training, where training includes\nadditional hyper-parameter tuning and retraining [35].\nFor GPT-3, the true ratio is almost certainly higher than 1.14x due to failures and other experiments. We believe the\nMeta, BLOOM and OPT-175B cases are the more central examples as they account better for all experiments. So\na factor close to 2x seems like a reasonable median estimate. On the high end, it’s plausible that several large-scale\nexperiments are necessary before training—say, 4x. We sampled from the range of plausible values using a log-normal\ndistribution. The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.\nA.7 Cost uncertainty analysis\nOur cost estimation methods have many sources of uncertainty, making it important to measure overall uncertainty in\nthe estimates. To do this, we first made a rough estimate of the relative uncertainty in each input variable, based on\nempirical data. For example, for the overhead of per-GPU server cost relative to single GPU cost we assigned a 90%\ncredible interval of 1.3x to 2.1x, which is wider than the range of values in our data and from industry sources.6\nWe then used a simulation to sample from distributions over each input variable. The simulation, along\nwith details of the bounds for each input variable, are available at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/uncertainty.ipynb. The simulation used log-normal distributions for all\nvariables except depreciation rate and utilization rate, which used normal distributions. The sampled variables were\ncombined into a sample of final costs, using the relevant formula. The cost sample was then normalized to have a\nmedian value of 1. The 90% CI of this normalized sample represents the relative uncertainty in cost.\nThe relative uncertainties in cost are listed in Table 5. Hardware acquisition cost involves fewer variables with less\nuncertainty, so estimates are generally accurate within a factor of two for models trained on GPUs, and within a factor of\n4 for models trained on TPUs. Meanwhile, amortized hardware CapEx + energy is generally accurate within a factor of\nthree or four for models trained on GPUs, and a factor of five for models trained on TPUs. The cost estimates are most\n5The choice of 2 months was an educated guess. The matching of models to cloud prices was not very sensitive to this choice\nbecause the price data was sparse and stable over time.\n6The three actual values we calculated ranged from 1.54 (P100) to 1.69 (V100). A pre-existing cost breakdown of a DGX H100\nimplies a ratio of approximately 1.4 (total cost divided by \"8 GPU + 4 NVSwitch Baseboard\" cost) [19].\n16\n\nsensitive to the GPU and TPU unit cost (accurate within factors of 2 and 4 respectively) and the training chip-hours\n(factor of 3).\nCost quantity 90% CI\nHardware acquisition (GPUs) 0.5x to 2x\nHardware acquisition (TPUs) 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) 0.2x to 5x\nTable 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger\nuncertainty due to the additional uncertainty in estimating their equivalent costs.\nA.8 Ground truth cost comparison\nIn order to verify that our results are reasonable, we sought to compare our cost estimates with true costs reported by\ndevelopers and other sources. However, there are very few models where the developers report both the computing\nresource usage and the total cost. Training costs and compute resources are independently known for BLOOM-176B\nand OPT-175B, so we compare our estimates with these.\nBLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs\nutilization and a cost of $3 million (including experiments) [25]. We estimated a cloud compute cost of $1.99M or an\namortized cost of $0.8M for BLOOM-176B. The accuracy of this estimate depends on how much of the grant was spent\non experiments versus the final training run. According to BLOOM’s model page on Hugging Face, the “Estimated\ncost of training” is the “Equivalent of $2–5M in cloud computing (including preliminary experiments)”. Preliminary\nexperiments included training smaller BLOOM models. The final training run for the 176B model used 37.24% of the\nenergy of the BLOOM project [36]; if the total cost of the project was C3M as in the grant description, this implies that\nBLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized\ncost approach ($900K) than the cloud cost approach ($2M).\nOPT-175B was trained for 793.5 hours, at a cost of $2500/hour as reported in the training logbook [ 34], for a total\ncost of $1.98 million. We estimated a cloud compute cost of $1.5M for the final training run of OPT-175B, which is\noff by 25%, and an amortized hardware and energy cost of $700K, off by 65%. OPT’s cluster cost rate per hour was\nlikely greater than what we estimate from the quantity of GPUs, or less than the approximate figure mentioned by the\ndevelopers in the training log.\nB Sensitivity analysis\nB.1 Selection of historic frontier models\nIn order to analyse trends in frontier ML models, we must define what counts as a frontier model at any point in time.7\nOur preferred approach is to select models from the database that were in the top 10 most compute-intensive models as\nof their release date, although we considered others as shown in Figure 7.\nFor the most part, different selection approaches gave similar results. The exception was selecting frontier models based\non distance from the compute trend. This approach imposes an artificially flat floor on the eligible models. Due to this,\nit leaves out many earlier models, and produces a flatter cost trend than the other methods.\nOur preferred approach has an advantage over alternatives: the selection is more robust to the sampling of our dataset.\nApproaches based on quantiles, or distance from the historic trend, are influenced by data collected on models outside\nthe frontier. Selecting the top-ranked models, in comparison, is merely influenced by whether the dataset contains those\nfrontier models.\n7Models in the database meet one or more of the following criteria: (i) advanced the state of the art on a qualifying benchmark,\n(ii) at least 1000 citations, (iii) at least one million monthly active users, or (iv) equivalent historical significance [7]. However, this\nmeans the database includes many models that were far from the frontier of compute.\n17\n\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop-N=10 Top 20% of models in year before/after\nTop 15% of models in year before Top 20% of residuals from trend\n2.4x per year 2.3x per year\n2.2x per year 1.6x per year\nFigure 7: Comparison of hardware capex + energy cost regression using different frontier model selection methods.\nResults are fairly similar across methods, although taking the top 20% of residuals leads to a flatter trend.\nB.2 Varying N in top-N model selection\nWhen selecting frontier models by the top- N method, there is a question of how to choose N. We chose N = 10\nto produce a large enough sample size while still focusing on models near the frontier. The estimated growth rate is\nmoderately robust to the choice of N, as it is similar for N = 3, N = 5 and N = 20 (see Figure 8).\nB.3 Varying the depreciation of hardware value\nThe growth in price-performance for ML GPUs running at FP32 precision has been estimated at 0.14 OOMs/year with\na 90% CI of 0.10 to 0.18 OOMs/year [ 9]. Substituting the lower and upper bounds of that CI for the depreciation\nrate did not significantly change the growth rate of amortized hardware CapEx + energy. For the lower bound of 0.10\nOOMs/year, cost estimates decreased by 15% on average, while for the upper bound of 0.18 OOMs/year, cost estimates\nincreased by 10% on average. Note that increasing the depreciation rate has two effects that partially cancel out: 1. the\nvalue of hardware at the start of training is smaller, 2. the proportion of value used up by training is larger.\nWe also tested 0.3 OOMs/year as an extreme case, based on a claim that single-GPU inference performance has\nimproved by 1000× in the last decade [37]. This did not significantly change the growth rate either, but it increased\ncosts by an average of 30%.\nB.4 Varying the time between hardware acquisition and the start of training\nWe tested different estimates of the hardware acquisition date relative to the release date, as well as the training start\ndate relative to the model publication date. These dates affect the time over which hardware value depreciates. To\nmake the depreciation times long, we removed the minimum buffer of 90 days between hardware release and hardware\nacquisition, and pushed the default training start date back by 15 days relative to the publication date. This decreased\nthe estimated costs by 4% on average, and did not change the growth rate. Similarly, we tested a short depreciation time\nby extending the hardware acquisition buffer time to 180 days and bringing the default training start date forward by 60\ndays. This increased costs by 10% on average and did not change the growth rate.\n18\n\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop 3 Top 5\nTop 10 Top 20\n2.3x per year 2.3x per year\n2.4x per year 2.8x per year\nFigure 8: Comparison of amortized hardware capex + energy regression for varying top-N selection.\nB.5 Excluding equity from R&D staff costs\nTo measure the impact of equity on the total amortized model development cost, Figure 9a shows the cost breakdown\nwith equity excluded from the R&D staff cost. The proportion of cost on R&D staff decreases from 29–49% with equity\nincluded, to 19–33% with equity excluded.\nC Power capacity for model training\nFigure 10 shows the trend in the power capacity of the compute cluster needed for training frontier models. This was\nbased on the following formula:\nPower capacity (kW) = Hardware quantity × Hardware TDP (kW) × Data center PUE\nwhere “Hardware TDP” includes all server hardware. We find a growth rate of 2.2x per year (90% CI: 1.9x to 2.6x).\nGemini Ultra has the largest estimated power capacity, at around 35 MW. Projecting the trend forward from Gemini\nUltra, the most power-intensive training run would draw 1 GW at some point in 2028. To put this in context, the top ten\nlargest power plants in the United States have a capacity ranging from 3 GW to 7 GW [38].\n19\n\nA m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 . 0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( e x c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 4 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 . 0  U l t r a\n2 1 % 3 6 % 2 5 % 1 4 % 3 %\n3 2 % 3 2 % 2 1 % 1 3 %\n1 9 % 3 6 % 1 4 % 7 %\n3 3 % 3 0 % 1 9 % 1 1 % 6 %\n(b)\nFigure 9: (a) Breakdown of total amortized model development costs for selected models, with equity excluded from\nthe R&D staff cost. Hardware costs are amortized to the total number of chip-hours spent on experiments and training,\nwhile R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate\n90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based\non median estimates.\nFigure 10: The trend in AI compute cluster power (in kilowatts) required to train frontier models over time. Power is\ncalculated as the product of the number of servers, server TDP, and power usage effectiveness.\n20", "metadata": {"url": "https://arxiv.org/pdf/2405.21015", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "THE RISING COSTS OF TRAINING FRONTIER AI MODELS\nBen Cottier1 Robi Rahman1,2\nLoredana Fattorini2 Nestor Maslej2 Tamay Besiroglu1 David Owen1\nABSTRACT\nThe costs of training frontier AI models have grown dramatically in recent years, but there is limited\npublic data on the magnitude and growth of these expenses. This paper develops a detailed cost\nmodel to address this gap, estimating training costs using three approaches that account for hardware,\nenergy, cloud rental, and staff expenses. The analysis reveals that the amortized cost to train the most\ncompute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI:\n2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses\nare AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs\ninclude server components (15-22%), cluster-level interconnect (9-13%), and energy consumption\n(2-6%). If the trend of growing development costs continues, the largest training runs will cost more\nthan a billion dollars by 2027, meaning that only the most well-funded organizations will be able to\nfinance frontier AI models.\n1 Introduction\nThe large and growing cost of training state-of-the-art AI models has become an important issue in the field of\nAI [1]. Improving AI capabilities demand exponential increases in computing power, as evidenced by both economic\nanalysis [2] and the discovery of empirical scaling laws, which show that model performance improves with more\nparameters and training data [3, 4]. Dario Amodei, CEO of the AI lab Anthropic, has stated that frontier AI developers\nare likely to spend close to a billion dollars on a single training run this year, and up to ten billion-dollar training runs\nin the next two years [5]. Given this trend, some innovations, particularly those requiring large-scale training, may\nbecome inaccessible to all but the most well-funded organizations.\nAlthough it is widely known that training the largest ML models is expensive, until recently there were few concrete\nestimates of training costs in the public domain. In collaboration with Epoch AI, the 2024 AI Index presented one of\nthe most comprehensive datasets to date, estimating the costs of training runs based on cloud rental prices [ 6]. We\nbuild on that work with a more in-depth account of hardware, energy and R&D staff costs for both training runs and\nexperiments, as well as a more detailed analysis of how costs are increasing over time. To our knowledge, our study is\nthe most thorough analysis of model development costs to date.\nOur methods are built upon a comprehensive database of notable machine learning models [ 7], and informed by\ninterviews with industry experts. We consider three complementary approaches to measuring the cost of frontier\nmodels. The first approach estimates the hardware capital expenses (CapEx) amortized over the final training run,\nalong with the cost of hardware energy consumption. By considering AI accelerator chips, other server hardware,\nnetworking hardware, and energy separately, this approach can provide more accurate training costs. We find that the\nmost expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at\n$30M. Among frontier models, defined as models within the top 10 most compute-intensive models when they are\nreleased, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).\nWe then compare this approach to the cloud-price approach that was first presented in the AI Index [ 6]. Instead of\nestimating hourly compute costs in detail, the cloud-price approach simply uses historical rental rates from cloud\nplatforms. The cloud-price approach shows a similar growth rate (2.5× per year with a 90% CI of 2.1× to 3.1×), but\n1Epoch AI. 2Stanford University.\narXiv:2405.21015v2  [cs.CY]  7 Feb 2025", "sentences": [{"text": "THE RISING COSTS OF TRAINING FRONTIER AI MODELS\nBen Cottier1 Robi Rahman1,2\nLoredana Fattorini2 Nestor Maslej2 Tamay Besiroglu1 David Owen1\nABSTRACT\nThe costs of training frontier AI models have grown dramatically in recent years, but there is limited\npublic data on the magnitude and growth of these expenses.", "metadata": {}}, {"text": "This paper develops a detailed cost\nmodel to address this gap, estimating training costs using three approaches that account for hardware,\nenergy, cloud rental, and staff expenses.", "metadata": {}}, {"text": "The analysis reveals that the amortized cost to train the most\ncompute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI:\n2.0× to 2.9×).", "metadata": {}}, {"text": "For key frontier models, such as GPT-4 and Gemini, the most significant expenses\nare AI accelerator chips and staff costs, each costing tens of millions of dollars.", "metadata": {}}, {"text": "Other notable costs\ninclude server components (15-22%), cluster-level interconnect (9-13%), and energy consumption\n(2-6%).", "metadata": {}}, {"text": "If the trend of growing development costs continues, the largest training runs will cost more\nthan a billion dollars by 2027, meaning that only the most well-funded organizations will be able to\nfinance frontier AI models.", "metadata": {}}, {"text": "1 Introduction\nThe large and growing cost of training state-of-the-art AI models has become an important issue in the field of\nAI [1].", "metadata": {}}, {"text": "Improving AI capabilities demand exponential increases in computing power, as evidenced by both economic\nanalysis [2] and the discovery of empirical scaling laws, which show that model performance improves with more\nparameters and training data [3, 4].", "metadata": {}}, {"text": "Dario Amodei, CEO of the AI lab Anthropic, has stated that frontier AI developers\nare likely to spend close to a billion dollars on a single training run this year, and up to ten billion-dollar training runs\nin the next two years [5].", "metadata": {}}, {"text": "Given this trend, some innovations, particularly those requiring large-scale training, may\nbecome inaccessible to all but the most well-funded organizations.", "metadata": {}}, {"text": "Although it is widely known that training the largest ML models is expensive, until recently there were few concrete\nestimates of training costs in the public domain.", "metadata": {}}, {"text": "In collaboration with Epoch AI, the 2024 AI Index presented one of\nthe most comprehensive datasets to date, estimating the costs of training runs based on cloud rental prices [ 6].", "metadata": {}}, {"text": "We\nbuild on that work with a more in-depth account of hardware, energy and R&D staff costs for both training runs and\nexperiments, as well as a more detailed analysis of how costs are increasing over time.", "metadata": {}}, {"text": "To our knowledge, our study is\nthe most thorough analysis of model development costs to date.", "metadata": {}}, {"text": "Our methods are built upon a comprehensive database of notable machine learning models [ 7], and informed by\ninterviews with industry experts.", "metadata": {}}, {"text": "We consider three complementary approaches to measuring the cost of frontier\nmodels.", "metadata": {}}, {"text": "The first approach estimates the hardware capital expenses (CapEx) amortized over the final training run,\nalong with the cost of hardware energy consumption.", "metadata": {}}, {"text": "By considering AI accelerator chips, other server hardware,\nnetworking hardware, and energy separately, this approach can provide more accurate training costs.", "metadata": {}}, {"text": "We find that the\nmost expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at\n$30M.", "metadata": {}}, {"text": "Among frontier models, defined as models within the top 10 most compute-intensive models when they are\nreleased, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).", "metadata": {}}, {"text": "We then compare this approach to the cloud-price approach that was first presented in the AI Index [ 6].", "metadata": {}}, {"text": "Instead of\nestimating hourly compute costs in detail, the cloud-price approach simply uses historical rental rates from cloud\nplatforms.", "metadata": {}}, {"text": "The cloud-price approach shows a similar growth rate (2.5× per year with a 90% CI of 2.1× to 3.1×), but\n1Epoch AI.", "metadata": {}}, {"text": "2Stanford University.", "metadata": {}}, {"text": "arXiv:2405.21015v2  [cs.CY]  7 Feb 2025", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Figure 1: Amortized hardware cost plus energy cost for the final training run of frontier models. The selected models\nare among the top 10 most compute-intensive for their time. Amortized hardware costs are the product of training\nchip-hours and a depreciated hardware cost, with 23% overhead added for cluster-level networking. Open circles\nindicate costs which used an estimated production cost of Google TPU hardware. These costs are generally more\nuncertain than the others, which used actual price data rather than estimates.\nyields costs that are about twice as large on average. We expect the cloud-price approach to overestimate frontier model\ncosts, since model developers usually either own or have private rental agreements for their training hardware. Using\nboth approaches helps validate our estimate of cost growth, while also highlighting the uncertainty of individual costs.\nOur third and most in-depth approach breaks down hardware, energy, and R&D staff costs over the entire development\nof the model (i.e. both experiments and training). We select four especially notable models for this approach—GPT-3,\nOPT-175B, GPT-4, and Gemini Ultra. For these models, we find that R&D staff costs including equity are between\n29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%.\nHowever, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware\ncosts and energy rise to 61–76% and 2–7% respectively.\nBy taking into account hardware purchase costs, energy costs, and the more opaque costs of R&D labor, our analysis\nprovides a clearer picture of the true costs of AI development. This sheds light on not only current costs but also the\neconomic hurdles that lie ahead as AI continues to scale.\nAll of our results can be reproduced using the code and data available at https://github.com/epoch-research/\ntraining-cost-trends.\n2 Methodology\n2.1 Datasets and frontier model selection\nOur investigation draws upon the Notable AI Models database, which documents 796 notable models across the history\nof machine learning [7]. Key details captured for each model include training compute, dataset size, and parameter\ncount. To focus on the largest-scale models, we initially filtered the database to models that had training compute\nestimates and that were published on or after 1 October 2015 (the start of the large-scale ML era according to [ 8])\nand up to 31 December 2023. This resulted in 276 selected models. For these models, we recorded the training time,\nhardware type and quantity, and utilization rate sourced from each model’s original publication, where possible.\nFor our main results, we examined 41 models that were historically at the frontier of compute. Specifically, we filtered\nfor models that were in the top 10 of training compute as of their release.1 Appendix B.1 provides further details on this\nselection procedure and a comparison to three alternative methods.\n1We excluded models that are fine-tuned versions of a separately listed model, to avoid double-counting costs.\n2", "sentences": [{"text": "Figure 1: Amortized hardware cost plus energy cost for the final training run of frontier models.", "metadata": {}}, {"text": "The selected models\nare among the top 10 most compute-intensive for their time.", "metadata": {}}, {"text": "Amortized hardware costs are the product of training\nchip-hours and a depreciated hardware cost, with 23% overhead added for cluster-level networking.", "metadata": {}}, {"text": "Open circles\nindicate costs which used an estimated production cost of Google TPU hardware.", "metadata": {}}, {"text": "These costs are generally more\nuncertain than the others, which used actual price data rather than estimates.", "metadata": {}}, {"text": "yields costs that are about twice as large on average.", "metadata": {}}, {"text": "We expect the cloud-price approach to overestimate frontier model\ncosts, since model developers usually either own or have private rental agreements for their training hardware.", "metadata": {}}, {"text": "Using\nboth approaches helps validate our estimate of cost growth, while also highlighting the uncertainty of individual costs.", "metadata": {}}, {"text": "Our third and most in-depth approach breaks down hardware, energy, and R&D staff costs over the entire development\nof the model (i.e.", "metadata": {}}, {"text": "both experiments and training).", "metadata": {}}, {"text": "We select four especially notable models for this approach—GPT-3,\nOPT-175B, GPT-4, and Gemini Ultra.", "metadata": {}}, {"text": "For these models, we find that R&D staff costs including equity are between\n29% and 49% of the total amortized cost.", "metadata": {}}, {"text": "Computing hardware makes up 47–64%, while energy comprises only 2–6%.", "metadata": {}}, {"text": "However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware\ncosts and energy rise to 61–76% and 2–7% respectively.", "metadata": {}}, {"text": "By taking into account hardware purchase costs, energy costs, and the more opaque costs of R&D labor, our analysis\nprovides a clearer picture of the true costs of AI development.", "metadata": {}}, {"text": "This sheds light on not only current costs but also the\neconomic hurdles that lie ahead as AI continues to scale.", "metadata": {}}, {"text": "All of our results can be reproduced using the code and data available at https://github.com/epoch-research/\ntraining-cost-trends.", "metadata": {}}, {"text": "2 Methodology\n2.1 Datasets and frontier model selection\nOur investigation draws upon the Notable AI Models database, which documents 796 notable models across the history\nof machine learning [7].", "metadata": {}}, {"text": "Key details captured for each model include training compute, dataset size, and parameter\ncount.", "metadata": {}}, {"text": "To focus on the largest-scale models, we initially filtered the database to models that had training compute\nestimates and that were published on or after 1 October 2015 (the start of the large-scale ML era according to [ 8])\nand up to 31 December 2023.", "metadata": {}}, {"text": "This resulted in 276 selected models.", "metadata": {}}, {"text": "For these models, we recorded the training time,\nhardware type and quantity, and utilization rate sourced from each model’s original publication, where possible.", "metadata": {}}, {"text": "For our main results, we examined 41 models that were historically at the frontier of compute.", "metadata": {}}, {"text": "Specifically, we filtered\nfor models that were in the top 10 of training compute as of their release.1 Appendix B.1 provides further details on this\nselection procedure and a comparison to three alternative methods.", "metadata": {}}, {"text": "1We excluded models that are fine-tuned versions of a separately listed model, to avoid double-counting costs.", "metadata": {}}, {"text": "2", "metadata": {}}], "metadata": {"page": 2}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "In addition to the data on machine learning models, we compiled a dataset of historical hardware prices, allowing us to\nestimate training costs. This price dataset contained cloud rental prices and hardware purchase prices for 24 different\nhardware models (e.g. NVIDIA A100) between 2015 and 2023. In total there were 142 entries, 52 of which were\npurchase prices and 90 of which were cloud rental prices.\n2.2 Amortizing the cost of hardware for training\nTo estimate the cost of hardware for a training run, we first calculated the cost to acquire the necessary accelerator chips\n(GPUs/TPUs), servers, and networking hardware. This involved looking up historical prices for GPUs, or estimating\nproduction costs for TPUs. Further details are provided in Appendix A.1.\nHardware normally remains available for future use after a training run finishes, but its value depreciates over time due\nto hardware progress. We amortized the cost of a training run based on this depreciation. Specifically, we depreciated\nthe value of hardware at a rate of r = 0 .14 orders of magnitude per year, based on the growth rate of ML GPU\nprice-performance [9].2 To get the value of the hardware at the start of training, we used the following formula:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere the difference in dates is in units of years. For example, if the training run starts one year after hardware is\nacquired, the start value is approximately 72% of the acquisition cost. We neglected the impact of hardware failures on\ndepreciation, as the effect seemed small compared to hardware progress. We provide evidence for that in Appendix A.3.\nAfter finding the initial value of the hardware, the amortized cost of the training run is then the portion of value that is\nlost during the training run. However, the training time is often difficult to determine, due to a lack of public information.\nMore often, we were able to estimate the number of chip-hours: the product of the training time and the number of\nchips. So we substituted chip-hours for the training time and the number of chips, using a linear approximation. This\nled to our final formula for amortized training cost:\nAmortized training cost ≈ Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nUp until Section 3.5, our results only account for the chip-hours of the final training run. In Section 3.5, we scale up the\nchip-hours to account for all experiments towards developing an AI model. Although the amortized cost model involves\nseveral estimates and approximations, our results are robust to reasonable changes in the method (see Appendix A.3 for\nfurther methodological details and Appendix B.3 for the sensitivity analysis).\n2.3 Hardware energy consumption cost\nIn addition to the capital costs of hardware, we also considered the cost of energy consumed by hardware during model\ntraining. We estimated this using the following formula:\nTotal energy cost of training = Energy cost rate ($/kWh) × Hardware TDP (kW) ×\nAverage power to TDP ratio (%) × Data center PUE × Training chip-hours (h)\nwhere TDP is thermal design power and PUE is power usage effectiveness, which accounts for the overhead of data\ncenter power distribution and cooling. We selected the energy cost rate by year, hardware TDP by the hardware type,\naverage power to TDP ratio by the hardware manufacturer, and the data center PUE by the ML model developer. These\nwere set based on hardware manufacturers’ literature. However, some parameters such as average power to TDP\nratio could not be found in technical specifications and had to be estimated. For references and method details, see\nAppendix A.4.\n2.4 Cloud compute cost\nWhile the amortized hardware CapEx + energy approach is a bottom-up method that accounts for hardware and energy\ncosts, cloud rental prices offer a simpler method. Many AI labs rely on cloud computing services to train their models,\nand the associated costs are often more readily available and easier to estimate. By comparing our bottom-up estimates\n2This growth rate measures improvement at 32-bit precision. One-time improvements from lower-precision and tensor number\nformats would make the rate faster, but this was not estimated. This also assumes that hardware improves continuously. In reality,\nhardware improves in increments with each new release.\n3", "sentences": [{"text": "In addition to the data on machine learning models, we compiled a dataset of historical hardware prices, allowing us to\nestimate training costs.", "metadata": {}}, {"text": "This price dataset contained cloud rental prices and hardware purchase prices for 24 different\nhardware models (e.g.", "metadata": {}}, {"text": "NVIDIA A100) between 2015 and 2023.", "metadata": {}}, {"text": "In total there were 142 entries, 52 of which were\npurchase prices and 90 of which were cloud rental prices.", "metadata": {}}, {"text": "2.2 Amortizing the cost of hardware for training\nTo estimate the cost of hardware for a training run, we first calculated the cost to acquire the necessary accelerator chips\n(GPUs/TPUs), servers, and networking hardware.", "metadata": {}}, {"text": "This involved looking up historical prices for GPUs, or estimating\nproduction costs for TPUs.", "metadata": {}}, {"text": "Further details are provided in Appendix A.1.", "metadata": {}}, {"text": "Hardware normally remains available for future use after a training run finishes, but its value depreciates over time due\nto hardware progress.", "metadata": {}}, {"text": "We amortized the cost of a training run based on this depreciation.", "metadata": {}}, {"text": "Specifically, we depreciated\nthe value of hardware at a rate of r = 0 .14 orders of magnitude per year, based on the growth rate of ML GPU\nprice-performance [9].2 To get the value of the hardware at the start of training, we used the following formula:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere the difference in dates is in units of years.", "metadata": {}}, {"text": "For example, if the training run starts one year after hardware is\nacquired, the start value is approximately 72% of the acquisition cost.", "metadata": {}}, {"text": "We neglected the impact of hardware failures on\ndepreciation, as the effect seemed small compared to hardware progress.", "metadata": {}}, {"text": "We provide evidence for that in Appendix A.3.", "metadata": {}}, {"text": "After finding the initial value of the hardware, the amortized cost of the training run is then the portion of value that is\nlost during the training run.", "metadata": {}}, {"text": "However, the training time is often difficult to determine, due to a lack of public information.", "metadata": {}}, {"text": "More often, we were able to estimate the number of chip-hours: the product of the training time and the number of\nchips.", "metadata": {}}, {"text": "So we substituted chip-hours for the training time and the number of chips, using a linear approximation.", "metadata": {}}, {"text": "This\nled to our final formula for amortized training cost:\nAmortized training cost ≈ Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nUp until Section 3.5, our results only account for the chip-hours of the final training run.", "metadata": {}}, {"text": "In Section 3.5, we scale up the\nchip-hours to account for all experiments towards developing an AI model.", "metadata": {}}, {"text": "Although the amortized cost model involves\nseveral estimates and approximations, our results are robust to reasonable changes in the method (see Appendix A.3 for\nfurther methodological details and Appendix B.3 for the sensitivity analysis).", "metadata": {}}, {"text": "2.3 Hardware energy consumption cost\nIn addition to the capital costs of hardware, we also considered the cost of energy consumed by hardware during model\ntraining.", "metadata": {}}, {"text": "We estimated this using the following formula:\nTotal energy cost of training = Energy cost rate ($/kWh) × Hardware TDP (kW) ×\nAverage power to TDP ratio (%) × Data center PUE × Training chip-hours (h)\nwhere TDP is thermal design power and PUE is power usage effectiveness, which accounts for the overhead of data\ncenter power distribution and cooling.", "metadata": {}}, {"text": "We selected the energy cost rate by year, hardware TDP by the hardware type,\naverage power to TDP ratio by the hardware manufacturer, and the data center PUE by the ML model developer.", "metadata": {}}, {"text": "These\nwere set based on hardware manufacturers’ literature.", "metadata": {}}, {"text": "However, some parameters such as average power to TDP\nratio could not be found in technical specifications and had to be estimated.", "metadata": {}}, {"text": "For references and method details, see\nAppendix A.4.", "metadata": {}}, {"text": "2.4 Cloud compute cost\nWhile the amortized hardware CapEx + energy approach is a bottom-up method that accounts for hardware and energy\ncosts, cloud rental prices offer a simpler method.", "metadata": {}}, {"text": "Many AI labs rely on cloud computing services to train their models,\nand the associated costs are often more readily available and easier to estimate.", "metadata": {}}, {"text": "By comparing our bottom-up estimates\n2This growth rate measures improvement at 32-bit precision.", "metadata": {}}, {"text": "One-time improvements from lower-precision and tensor number\nformats would make the rate faster, but this was not estimated.", "metadata": {}}, {"text": "This also assumes that hardware improves continuously.", "metadata": {}}, {"text": "In reality,\nhardware improves in increments with each new release.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "with those derived from cloud rental prices, we can validate our approach and provide a more comprehensive picture\nof AI training costs. The cloud approach also allows estimates of model training cost that are not possible using the\namortized hardware CapEx + energy approach and our data. However, note that the cloud approach can overestimate\nthe cost of models whose developers used their own hardware rather than renting compute from a cloud provider.\nTo estimate training costs from cloud rental prices, we used the following formula:\nTotal cost = Price per chip-hour × Training chip-hours\nThe price per chip-hour was obtained from our hardware price database, which includes prices for various hardware\ntypes, cloud providers, and rental dates. We matched the hardware type and publication date of each ML model with the\nmost appropriate price, using the developer of the ML model to determine the most likely cloud provider (e.g., Google\nCloud for Google labs, Microsoft Azure for OpenAI). See Appendix A.5 for further details.\n2.5 Total amortized model development cost\nAlthough the final training run ultimately determines an AI model’s capabilities and impact, the research and develop-\nment surrounding it is crucial. We therefore used a third approach that considers all of the compute that went into model\ndevelopment, as well as the cost of R&D staff developing the model. Since this approach was more time-intensive, and\nrelied on having a list of contributors to estimate R&D staff cost, we applied it to just four models: GPT-3, OPT-175B,\nGPT-4, and Gemini Ultra.\nTo estimate the compute cost over model development—including experiments, failed attempts, evaluation and fine-\ntuning—we applied a multiplicative factor to the final training run compute. We estimated this factor based on\nevidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta.\nAppendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90%\nCI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.\n2.5.1 R&D staff costs\nResearch and development (R&D) staff costs are an often-neglected component of the total cost of developing ML\nmodels. These costs include the salaries and equity compensation of the researchers, engineers, and managers in the\nproject, but excludes operations staff and data center employees. We set out to better quantify these costs for a few\nselected models to see how significant they are relative to the hardware costs.\nWe estimated total annual compensation of R&D personnel by multiplying the estimated full-time equivalent workload\nper contributor by their compensation and by the total time spent on model development. Since these parameters were\nall quite uncertain, we sampled from log-normal distributions over each parameter.\nFor full-time equivalent workers, we were informed by the type and number of contributors listed on the research paper.\nFor all models except Gemini Ultra, we sampled full-time equivalent workloads from a 90% credible interval of 5% to\n80% FTE for each contributor, resulting in a median of 20%. For Gemini Ultra, we used different workloads for each\ntype of contributor listed [10, pp. 66–69].\nFor compensation, we were informed by company-specific data from https://www.levels.fyi/ and https:\n//aipaygrad.es/. From levels.fyi, we used data for Google Software Engineers from level 3 to level 8. From\naipaygrad.es, we used the overall statistics for all companies and all roles (researchers, engineers and managers). After\naveraging the two sources, base salaries were modeled with a 90% CI of $140K to $160K, and equity with a 90% CI of\n$35K to $490K. We applied an overhead factor of 1.25x to 1.4x to base salaries to account for taxes and benefits [11],\nresulting in total compensation with a 90% CI of $210K to $690K and a median of $330K.\nActual staff compensation may vary significantly between AI labs. The chosen estimate of compensation may be\nparticularly unreliable for small and early companies, such as OpenAI in its earlier years, where there are many\nuncertainties about how to value equity compensation. However, these numbers serve as a reasonable baseline, and our\nestimates provide a useful starting point to analyze R&D labor costs.\n4", "sentences": [{"text": "with those derived from cloud rental prices, we can validate our approach and provide a more comprehensive picture\nof AI training costs.", "metadata": {}}, {"text": "The cloud approach also allows estimates of model training cost that are not possible using the\namortized hardware CapEx + energy approach and our data.", "metadata": {}}, {"text": "However, note that the cloud approach can overestimate\nthe cost of models whose developers used their own hardware rather than renting compute from a cloud provider.", "metadata": {}}, {"text": "To estimate training costs from cloud rental prices, we used the following formula:\nTotal cost = Price per chip-hour × Training chip-hours\nThe price per chip-hour was obtained from our hardware price database, which includes prices for various hardware\ntypes, cloud providers, and rental dates.", "metadata": {}}, {"text": "We matched the hardware type and publication date of each ML model with the\nmost appropriate price, using the developer of the ML model to determine the most likely cloud provider (e.g., Google\nCloud for Google labs, Microsoft Azure for OpenAI).", "metadata": {}}, {"text": "See Appendix A.5 for further details.", "metadata": {}}, {"text": "2.5 Total amortized model development cost\nAlthough the final training run ultimately determines an AI model’s capabilities and impact, the research and develop-\nment surrounding it is crucial.", "metadata": {}}, {"text": "We therefore used a third approach that considers all of the compute that went into model\ndevelopment, as well as the cost of R&D staff developing the model.", "metadata": {}}, {"text": "Since this approach was more time-intensive, and\nrelied on having a list of contributors to estimate R&D staff cost, we applied it to just four models: GPT-3, OPT-175B,\nGPT-4, and Gemini Ultra.", "metadata": {}}, {"text": "To estimate the compute cost over model development—including experiments, failed attempts, evaluation and fine-\ntuning—we applied a multiplicative factor to the final training run compute.", "metadata": {}}, {"text": "We estimated this factor based on\nevidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta.", "metadata": {}}, {"text": "Appendix A.6 provides further details.", "metadata": {}}, {"text": "Based on this, we sampled the factor from a log-normal distribution with a 90%\nCI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.", "metadata": {}}, {"text": "2.5.1 R&D staff costs\nResearch and development (R&D) staff costs are an often-neglected component of the total cost of developing ML\nmodels.", "metadata": {}}, {"text": "These costs include the salaries and equity compensation of the researchers, engineers, and managers in the\nproject, but excludes operations staff and data center employees.", "metadata": {}}, {"text": "We set out to better quantify these costs for a few\nselected models to see how significant they are relative to the hardware costs.", "metadata": {}}, {"text": "We estimated total annual compensation of R&D personnel by multiplying the estimated full-time equivalent workload\nper contributor by their compensation and by the total time spent on model development.", "metadata": {}}, {"text": "Since these parameters were\nall quite uncertain, we sampled from log-normal distributions over each parameter.", "metadata": {}}, {"text": "For full-time equivalent workers, we were informed by the type and number of contributors listed on the research paper.", "metadata": {}}, {"text": "For all models except Gemini Ultra, we sampled full-time equivalent workloads from a 90% credible interval of 5% to\n80% FTE for each contributor, resulting in a median of 20%.", "metadata": {}}, {"text": "For Gemini Ultra, we used different workloads for each\ntype of contributor listed [10, pp.", "metadata": {}}, {"text": "66–69].", "metadata": {}}, {"text": "For compensation, we were informed by company-specific data from https://www.levels.fyi/ and https:\n//aipaygrad.es/.", "metadata": {}}, {"text": "From levels.fyi, we used data for Google Software Engineers from level 3 to level 8.", "metadata": {}}, {"text": "From\naipaygrad.es, we used the overall statistics for all companies and all roles (researchers, engineers and managers).", "metadata": {}}, {"text": "After\naveraging the two sources, base salaries were modeled with a 90% CI of $140K to $160K, and equity with a 90% CI of\n$35K to $490K.", "metadata": {}}, {"text": "We applied an overhead factor of 1.25x to 1.4x to base salaries to account for taxes and benefits [11],\nresulting in total compensation with a 90% CI of $210K to $690K and a median of $330K.", "metadata": {}}, {"text": "Actual staff compensation may vary significantly between AI labs.", "metadata": {}}, {"text": "The chosen estimate of compensation may be\nparticularly unreliable for small and early companies, such as OpenAI in its earlier years, where there are many\nuncertainties about how to value equity compensation.", "metadata": {}}, {"text": "However, these numbers serve as a reasonable baseline, and our\nestimates provide a useful starting point to analyze R&D labor costs.", "metadata": {}}, {"text": "4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "3 Results\n3.1 Amortized training costs of frontier models have grown by 2.4x per year since 2016\nThe amortized training costs of frontier models have increased by a factor of 2.4x per year since 2016. This is the result\nof the preferred amortized hardware CapEex + energy approach, shown in Figure 2. Table 1 compares this to the cloud\napproach, which yields a similar growth rate of 2.5× per year. The growth rate is also similar if we vary hardware\ndepreciation or training start date within reasonable limits (see Appendix B.4). However, the growth rate rises to 2.9x\nper year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.\nApproach N× increase per year OOMs/year Doubling Time (months) R-squared N\nAmortized hardware\nCapEx + energy\n2.4 [2.0, 2.9] 0.38 [0.29, 0.47] 9 [8, 12] 0.58 41\nAmortized hardware\nCapEx + energy—no TPUs\n3.0 [2.4, 3.7] 0.47 [0.37, 0.57] 8 [6, 10] 0.77 22\nRenting from the cloud 2.5 [2.1, 3.1] 0.40 [0.32, 0.48] 9 [7, 11] 0.66 36\nTable 1: Cost growth rates based on log-linear regression, for different cost estimation approaches. All approaches select\nthe top 10 most compute intensive models at the time of model release. N refers to the number of relevant observations.\nBased on a two-sided t-test adjusted for correlation of residuals, the growth rates for amortized hardware capex + energy\nand cloud are not significantly different (p = 0.13). However, when the costs of models trained with estimated TPU\nproduction costs are excluded, the growth rate rises significantly to 2.9x per year (p < 0.01). OOMs/year: orders of\nmagnitude per year. Square brackets: 90% confidence interval.\nFigure 2: (Reproduction of Figure 1 for convenience.) Amortized hardware cost plus energy cost for the final training\nrun of frontier models. The selected models are among the top 10 most compute-intensive for their time. Amortized\nhardware costs are the product of training chip-hours and a depreciated hardware cost, with 23% overhead added for\ncluster-level networking. Open circles indicate costs which used an estimated production cost of Google TPU hardware.\nThese costs are generally more uncertain than the others, which used actual price data rather than estimates.\nEstimating costs from cloud rental prices, although less representative of actual costs, has the advantage of simplicity.\nThe cloud cost approach also helps to check the robustness of the amortized hardware CapEx + energy approach.\nFigure 3 shows the trend of cloud compute cost to train models among the top 10 most compute-intensive as of their\nrelease. Note that some of these estimates previously appeared in the 2024 AI Index report [6]. We find that the cost of\ntraining models based on cloud rental prices has grown by 2.5× per year since 2016, with a 90% CI of 2.1× to 3.1×.\n5", "sentences": [{"text": "3 Results\n3.1 Amortized training costs of frontier models have grown by 2.4x per year since 2016\nThe amortized training costs of frontier models have increased by a factor of 2.4x per year since 2016.", "metadata": {}}, {"text": "This is the result\nof the preferred amortized hardware CapEex + energy approach, shown in Figure 2.", "metadata": {}}, {"text": "Table 1 compares this to the cloud\napproach, which yields a similar growth rate of 2.5× per year.", "metadata": {}}, {"text": "The growth rate is also similar if we vary hardware\ndepreciation or training start date within reasonable limits (see Appendix B.4).", "metadata": {}}, {"text": "However, the growth rate rises to 2.9x\nper year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.", "metadata": {}}, {"text": "Approach N× increase per year OOMs/year Doubling Time (months) R-squared N\nAmortized hardware\nCapEx + energy\n2.4 [2.0, 2.9] 0.38 [0.29, 0.47] 9 [8, 12] 0.58 41\nAmortized hardware\nCapEx + energy—no TPUs\n3.0 [2.4, 3.7] 0.47 [0.37, 0.57] 8 [6, 10] 0.77 22\nRenting from the cloud 2.5 [2.1, 3.1] 0.40 [0.32, 0.48] 9 [7, 11] 0.66 36\nTable 1: Cost growth rates based on log-linear regression, for different cost estimation approaches.", "metadata": {}}, {"text": "All approaches select\nthe top 10 most compute intensive models at the time of model release.", "metadata": {}}, {"text": "N refers to the number of relevant observations.", "metadata": {}}, {"text": "Based on a two-sided t-test adjusted for correlation of residuals, the growth rates for amortized hardware capex + energy\nand cloud are not significantly different (p = 0.13).", "metadata": {}}, {"text": "However, when the costs of models trained with estimated TPU\nproduction costs are excluded, the growth rate rises significantly to 2.9x per year (p < 0.01).", "metadata": {}}, {"text": "OOMs/year: orders of\nmagnitude per year.", "metadata": {}}, {"text": "Square brackets: 90% confidence interval.", "metadata": {}}, {"text": "Figure 2: (Reproduction of Figure 1 for convenience.) Amortized hardware cost plus energy cost for the final training\nrun of frontier models.", "metadata": {}}, {"text": "The selected models are among the top 10 most compute-intensive for their time.", "metadata": {}}, {"text": "Amortized\nhardware costs are the product of training chip-hours and a depreciated hardware cost, with 23% overhead added for\ncluster-level networking.", "metadata": {}}, {"text": "Open circles indicate costs which used an estimated production cost of Google TPU hardware.", "metadata": {}}, {"text": "These costs are generally more uncertain than the others, which used actual price data rather than estimates.", "metadata": {}}, {"text": "Estimating costs from cloud rental prices, although less representative of actual costs, has the advantage of simplicity.", "metadata": {}}, {"text": "The cloud cost approach also helps to check the robustness of the amortized hardware CapEx + energy approach.", "metadata": {}}, {"text": "Figure 3 shows the trend of cloud compute cost to train models among the top 10 most compute-intensive as of their\nrelease.", "metadata": {}}, {"text": "Note that some of these estimates previously appeared in the 2024 AI Index report [6].", "metadata": {}}, {"text": "We find that the cost of\ntraining models based on cloud rental prices has grown by 2.5× per year since 2016, with a 90% CI of 2.1× to 3.1×.", "metadata": {}}, {"text": "5", "metadata": {}}], "metadata": {"page": 5}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "Figure 3: Estimated cloud compute costs for the final training run of frontier models. The selected models are among\nthe top 10 most compute-intensive for their time. The costs are the product of the number of training chip-hours and a\nhistorical cloud rental price.\nThis is consistent with the amortized hardware CapEx + energy approach, as shown in Table 1. This shows that our\ntrend estimates are robust to two different ways of estimating prices per chip-hour.\nOverall, these results suggest that the cloud approach is valid for estimating growth rates in compute costs, and has the\nadvantage of simplicity. However, public cloud rental prices are less reliable for individual model costs when the model\ndeveloper owns the hardware or has a special partnership with a cloud provider.\n3.2 The trend suggests that the most expensive publicly announced model will cost one billion dollars to train\nby the start of 2027\nThe growth rate in training cost indicates how rapidly AI investment is scaling. We can use this growth rate to extrapolate\nthe cost of the largest training run. Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.\nGPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive\npublicly announced model by the start of 2027 will cost about $1 billion.\nWhether this cost is justified hinges on how profitable the resulting AI model is—but parts of the AI industry believe it\nis worthwhile. The CEO of the AI lab Anthropic has claimed that close to a billion dollars will already be spent on a\nsingle training run in 2024 (implying an amortized cost), which is even sooner than the historical trend suggests [5].\n3.3 Hardware acquisition costs are one to two orders of magnitude higher than amortized costs\nIt’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime\nof the hardware, and the acquisition cost of purchasing that hardware outright. The choice of which cost to consider\ndepends on the purpose of the analysis. Amortized costs are more relevant for understanding the economics of training\nand deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and\nfinancial risks involved in developing such models.\nTo illustrate the difference between amortized hardware cost and acquisition cost, Figure 4 shows the acquisition costs\nwe were able to estimate using hardware purchase prices and training hardware quantities. Since this is the up-front\ncost of acquiring the hardware, the costs are one to two orders of magnitude larger than amortized hardware costs.\nFor example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the\namortized hardware CapEx + energy cost. The ratio between the two depends on when and for how long the model is\ntrained.\nBased on 40 estimates of acquisition cost, we find a growth rate of2.5× per year (90% CI: 2.1×, 3.0×). This is slightly\nfaster than the rate of 2× per year suggested by amortized costs (2.4× per year) divided by training times (1.2× per\n6", "sentences": [{"text": "Figure 3: Estimated cloud compute costs for the final training run of frontier models.", "metadata": {}}, {"text": "The selected models are among\nthe top 10 most compute-intensive for their time.", "metadata": {}}, {"text": "The costs are the product of the number of training chip-hours and a\nhistorical cloud rental price.", "metadata": {}}, {"text": "This is consistent with the amortized hardware CapEx + energy approach, as shown in Table 1.", "metadata": {}}, {"text": "This shows that our\ntrend estimates are robust to two different ways of estimating prices per chip-hour.", "metadata": {}}, {"text": "Overall, these results suggest that the cloud approach is valid for estimating growth rates in compute costs, and has the\nadvantage of simplicity.", "metadata": {}}, {"text": "However, public cloud rental prices are less reliable for individual model costs when the model\ndeveloper owns the hardware or has a special partnership with a cloud provider.", "metadata": {}}, {"text": "3.2 The trend suggests that the most expensive publicly announced model will cost one billion dollars to train\nby the start of 2027\nThe growth rate in training cost indicates how rapidly AI investment is scaling.", "metadata": {}}, {"text": "We can use this growth rate to extrapolate\nthe cost of the largest training run.", "metadata": {}}, {"text": "Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.", "metadata": {}}, {"text": "GPT-4 was published in March of 2023 [12].", "metadata": {}}, {"text": "This implies that, at a growth rate of 2.4× per year, the most expensive\npublicly announced model by the start of 2027 will cost about $1 billion.", "metadata": {}}, {"text": "Whether this cost is justified hinges on how profitable the resulting AI model is—but parts of the AI industry believe it\nis worthwhile.", "metadata": {}}, {"text": "The CEO of the AI lab Anthropic has claimed that close to a billion dollars will already be spent on a\nsingle training run in 2024 (implying an amortized cost), which is even sooner than the historical trend suggests [5].", "metadata": {}}, {"text": "3.3 Hardware acquisition costs are one to two orders of magnitude higher than amortized costs\nIt’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime\nof the hardware, and the acquisition cost of purchasing that hardware outright.", "metadata": {}}, {"text": "The choice of which cost to consider\ndepends on the purpose of the analysis.", "metadata": {}}, {"text": "Amortized costs are more relevant for understanding the economics of training\nand deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and\nfinancial risks involved in developing such models.", "metadata": {}}, {"text": "To illustrate the difference between amortized hardware cost and acquisition cost, Figure 4 shows the acquisition costs\nwe were able to estimate using hardware purchase prices and training hardware quantities.", "metadata": {}}, {"text": "Since this is the up-front\ncost of acquiring the hardware, the costs are one to two orders of magnitude larger than amortized hardware costs.", "metadata": {}}, {"text": "For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the\namortized hardware CapEx + energy cost.", "metadata": {}}, {"text": "The ratio between the two depends on when and for how long the model is\ntrained.", "metadata": {}}, {"text": "Based on 40 estimates of acquisition cost, we find a growth rate of2.5× per year (90% CI: 2.1×, 3.0×).", "metadata": {}}, {"text": "This is slightly\nfaster than the rate of 2× per year suggested by amortized costs (2.4× per year) divided by training times (1.2× per\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "year) [13]. The discrepancy is due to different AI models appearing in each analysis, highlighting a source of sensitivity\nin our results.\nFigure 4: Estimated hardware acquisition costs to train frontier models. The selected models are among the top 10 most\ncompute-intensive for their time. The costs are the product of the number of servers and the earliest available server\nprice, with about 23% overhead added for cluster-level networking hardware.\n3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips\nBreaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes\ntoward AI accelerator chips. The rest of the server (including markup) makes up 29% of the cost, while cluster level\ninterconnect makes up 17%.\nEnergy makes up the remainder of costs, averaging 9% but varying across models. Although this is a small fraction,\nit corresponds to rapid growth in energy use and power requirements over time. The trend in power requirements is\nprovided in Appendix C.\nNote that this breakdown does not include all costs associated with an AI supercomputer. Other costs include the data\ncenter infrastructure besides servers and networking, as well as data center personnel and maintenance.\n3.5 R&D staff are a significant fraction of costs over the whole model development process\nWe now use our third cost estimation approach to examine how the cost of labor from researchers and engineers\ncompares to the amortized cost of compute. Unlike the previous approaches, which only measured the cost of the final\ntraining run, this approach counts compute usage throughout model development including experiments, fine-tuning\nand evaluation. Figure 6 shows the cost breakdown for GPT-3, OPT-175B (notable as a GPT-3 replication attempt by a\nteam at Meta AI), the original GPT-4 model by OpenAI, and the original Gemini 1.0 Ultra model by Google DeepMind.\nWe find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model\ndevelopment costs, depending on the model. Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5\nfor additional plots). Notably, this fraction does not change much from GPT-3 to GPT-4, which spans three and a half\nyears of AI progress. The number of reported contributors increased from 25 for GPT-3 [14] to 284 for GPT-4 [12],\nwhile the amortized hardware cost over the whole model development increased from $4M to $90M. However, due to\nthe limited data, we caution against extrapolating the fraction of R&D staff costs to future frontier models.\nGemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier\nmodels. Firstly, Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators,\nand this makes the hardware cost relatively low. Secondly, our methodology is limited by deriving the number of\nfull-time equivalent staff from the reported number of contributors, for which Gemini had 941—much higher than\n7", "sentences": [{"text": "year) [13].", "metadata": {}}, {"text": "The discrepancy is due to different AI models appearing in each analysis, highlighting a source of sensitivity\nin our results.", "metadata": {}}, {"text": "Figure 4: Estimated hardware acquisition costs to train frontier models.", "metadata": {}}, {"text": "The selected models are among the top 10 most\ncompute-intensive for their time.", "metadata": {}}, {"text": "The costs are the product of the number of servers and the earliest available server\nprice, with about 23% overhead added for cluster-level networking hardware.", "metadata": {}}, {"text": "3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips\nBreaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes\ntoward AI accelerator chips.", "metadata": {}}, {"text": "The rest of the server (including markup) makes up 29% of the cost, while cluster level\ninterconnect makes up 17%.", "metadata": {}}, {"text": "Energy makes up the remainder of costs, averaging 9% but varying across models.", "metadata": {}}, {"text": "Although this is a small fraction,\nit corresponds to rapid growth in energy use and power requirements over time.", "metadata": {}}, {"text": "The trend in power requirements is\nprovided in Appendix C.", "metadata": {}}, {"text": "Note that this breakdown does not include all costs associated with an AI supercomputer.", "metadata": {}}, {"text": "Other costs include the data\ncenter infrastructure besides servers and networking, as well as data center personnel and maintenance.", "metadata": {}}, {"text": "3.5 R&D staff are a significant fraction of costs over the whole model development process\nWe now use our third cost estimation approach to examine how the cost of labor from researchers and engineers\ncompares to the amortized cost of compute.", "metadata": {}}, {"text": "Unlike the previous approaches, which only measured the cost of the final\ntraining run, this approach counts compute usage throughout model development including experiments, fine-tuning\nand evaluation.", "metadata": {}}, {"text": "Figure 6 shows the cost breakdown for GPT-3, OPT-175B (notable as a GPT-3 replication attempt by a\nteam at Meta AI), the original GPT-4 model by OpenAI, and the original Gemini 1.0 Ultra model by Google DeepMind.", "metadata": {}}, {"text": "We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model\ndevelopment costs, depending on the model.", "metadata": {}}, {"text": "Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5\nfor additional plots).", "metadata": {}}, {"text": "Notably, this fraction does not change much from GPT-3 to GPT-4, which spans three and a half\nyears of AI progress.", "metadata": {}}, {"text": "The number of reported contributors increased from 25 for GPT-3 [14] to 284 for GPT-4 [12],\nwhile the amortized hardware cost over the whole model development increased from $4M to $90M.", "metadata": {}}, {"text": "However, due to\nthe limited data, we caution against extrapolating the fraction of R&D staff costs to future frontier models.", "metadata": {}}, {"text": "Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier\nmodels.", "metadata": {}}, {"text": "Firstly, Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators,\nand this makes the hardware cost relatively low.", "metadata": {}}, {"text": "Secondly, our methodology is limited by deriving the number of\nfull-time equivalent staff from the reported number of contributors, for which Gemini had 941—much higher than\n7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "B r e a k d o w n  o f  a m o rt i z e d  h a r d w a r e  a n d  e n e r g y  c o s t s  f o r  f r o n t i e r  A I  m o d e l s\nA I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\nG e m i n i  1 . 0  U l t r a\nI n f l e c t i o n - 2\nF a l c o n - 1 8 0 B\nP a L M  2\nG P T - 4\nG P T - 3 . 5  ( t e x t - d a v i n c i - 0 0 3 )\nB L O O M - 1 7 6 B\nG L M - 1 3 0 B\nP a r t i\nO P T - 1 7 5 B\nP a L M  ( 5 4 0 B )\nL a M D A\nG L a M\nG o p h e r  ( 2 8 0 B )\nM e g a t r o n - T u r i n g  N L G  5 3 0 B\nH y p e r C L O V A  8 2 B\nG O A T\nB y T 5 - X X L\nP r o t T 5 - X X L\nM e t a  P s e u d o  L a b e l s\nS w i t c h\nD A L L - E\ni G P T - X L\nG P T - 3  1 7 5 B  ( d a v i n c i )\nT u r i n g - N L G\nM e e n a\nA l p h a S t a r\nT 5 - 1 1 B\nM e g a t r o n - L M  ( 8 . 3 B )\nM e g a t r o n - B E R T\nR o B E R T a  L a r g e\nB i g G A N - d e e p  5 1 2 x 5 1 2\nA l p h a Z e r o\nA l p h a G o  Z e r o\nJ F T\nM o E\nA l p h a G o  M a s t e r\nP o l y N e t\nX c e p t i o n\nG N M T\nD e e p S p e e c h 2  ( E n g l i s h )\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nFigure 5: The percentage of the amortized hardware CapEx + energy estimates made up by different hardware and\nenergy costs. Note that the breakdown across models is approximate. Cluster-level interconnect is assumed to be a\nconstant 19% fraction of the cluster CapEx, and the proportion of server components is based on only three comparisons\nbetween NVIDIA DGX server prices and single GPU prices (see Appendix A.1 for details). The energy costs are more\nspecific, varying with the number of training chip-hours and the hardware (see Appendix A.4).\n8", "sentences": [{"text": "B r e a k d o w n  o f  a m o rt i z e d  h a r d w a r e  a n d  e n e r g y  c o s t s  f o r  f r o n t i e r  A I  m o d e l s\nA I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\nG e m i n i  1 .", "metadata": {}}, {"text": "0  U l t r a\nI n f l e c t i o n - 2\nF a l c o n - 1 8 0 B\nP a L M  2\nG P T - 4\nG P T - 3 .", "metadata": {}}, {"text": "5  ( t e x t - d a v i n c i - 0 0 3 )\nB L O O M - 1 7 6 B\nG L M - 1 3 0 B\nP a r t i\nO P T - 1 7 5 B\nP a L M  ( 5 4 0 B )\nL a M D A\nG L a M\nG o p h e r  ( 2 8 0 B )\nM e g a t r o n - T u r i n g  N L G  5 3 0 B\nH y p e r C L O V A  8 2 B\nG O A T\nB y T 5 - X X L\nP r o t T 5 - X X L\nM e t a  P s e u d o  L a b e l s\nS w i t c h\nD A L L - E\ni G P T - X L\nG P T - 3  1 7 5 B  ( d a v i n c i )\nT u r i n g - N L G\nM e e n a\nA l p h a S t a r\nT 5 - 1 1 B\nM e g a t r o n - L M  ( 8 .", "metadata": {}}, {"text": "3 B )\nM e g a t r o n - B E R T\nR o B E R T a  L a r g e\nB i g G A N - d e e p  5 1 2 x 5 1 2\nA l p h a Z e r o\nA l p h a G o  Z e r o\nJ F T\nM o E\nA l p h a G o  M a s t e r\nP o l y N e t\nX c e p t i o n\nG N M T\nD e e p S p e e c h 2  ( E n g l i s h )\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nFigure 5: The percentage of the amortized hardware CapEx + energy estimates made up by different hardware and\nenergy costs.", "metadata": {}}, {"text": "Note that the breakdown across models is approximate.", "metadata": {}}, {"text": "Cluster-level interconnect is assumed to be a\nconstant 19% fraction of the cluster CapEx, and the proportion of server components is based on only three comparisons\nbetween NVIDIA DGX server prices and single GPU prices (see Appendix A.1 for details).", "metadata": {}}, {"text": "The energy costs are more\nspecific, varying with the number of training chip-hours and the hardware (see Appendix A.4).", "metadata": {}}, {"text": "8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "A m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 . 0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 1 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 . 0  U l t r a\n3 3 % 3 1 % 2 1 % 1 2 % 3 %\n4 3 % 2 7 % 1 8 % 1 1 %\n2 9 % 3 2 % 1 2 % 6 %\n4 9 % 2 3 % 1 5 % 9 % 5 %\n(b)\nFigure 6: (a) Breakdown of total amortized model development costs for selected models. Hardware costs are amortized\nto the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of\ndevelopment from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar\nvalues are medians. (b) Costs components as a percentage of the total, based on median estimates.\nGPT-4 at 284 contributors. Though we assumed a very small contribution from the 428 people under the “Contributors”\nrole—a median full-time equivalent of about 1%—the estimate may still be too high.\nOn the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while\nenergy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise\nto 61–76% and 2–7% respectively. Note that while energy consumption is a small fraction of total cost, this doesn’t\nentail that power requirements are not a challenge in frontier AI development. Regulatory and logistical hurdles to\nsecure power supplies may cause bottlenecks in the coming years, but we leave that topic to future work.\n4 Discussion\n4.1 Implications\nThe rapid growth in AI training costs will have a major impact on the future of AI development. Our findings suggest\nthat if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed\none billion dollars by 2027. Given the potential bias in our estimates’ absolute values, this may happen even sooner—as\nsuggested by cloud-price costs, and news reporting on training costs [5]. If realized, this level of investment is likely to\ndrive rapid advances in AI capabilities, given the track record of scaling up AI models.\n9", "sentences": [{"text": "A m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 .", "metadata": {}}, {"text": "0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 1 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 .", "metadata": {}}, {"text": "0  U l t r a\n3 3 % 3 1 % 2 1 % 1 2 % 3 %\n4 3 % 2 7 % 1 8 % 1 1 %\n2 9 % 3 2 % 1 2 % 6 %\n4 9 % 2 3 % 1 5 % 9 % 5 %\n(b)\nFigure 6: (a) Breakdown of total amortized model development costs for selected models.", "metadata": {}}, {"text": "Hardware costs are amortized\nto the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of\ndevelopment from initial experiments to publication.", "metadata": {}}, {"text": "Error bars indicate 90% credible intervals, while the main bar\nvalues are medians.", "metadata": {}}, {"text": "(b) Costs components as a percentage of the total, based on median estimates.", "metadata": {}}, {"text": "GPT-4 at 284 contributors.", "metadata": {}}, {"text": "Though we assumed a very small contribution from the 428 people under the “Contributors”\nrole—a median full-time equivalent of about 1%—the estimate may still be too high.", "metadata": {}}, {"text": "On the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while\nenergy comprises only 2–6%.", "metadata": {}}, {"text": "With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise\nto 61–76% and 2–7% respectively.", "metadata": {}}, {"text": "Note that while energy consumption is a small fraction of total cost, this doesn’t\nentail that power requirements are not a challenge in frontier AI development.", "metadata": {}}, {"text": "Regulatory and logistical hurdles to\nsecure power supplies may cause bottlenecks in the coming years, but we leave that topic to future work.", "metadata": {}}, {"text": "4 Discussion\n4.1 Implications\nThe rapid growth in AI training costs will have a major impact on the future of AI development.", "metadata": {}}, {"text": "Our findings suggest\nthat if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed\none billion dollars by 2027.", "metadata": {}}, {"text": "Given the potential bias in our estimates’ absolute values, this may happen even sooner—as\nsuggested by cloud-price costs, and news reporting on training costs [5].", "metadata": {}}, {"text": "If realized, this level of investment is likely to\ndrive rapid advances in AI capabilities, given the track record of scaling up AI models.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "However, only a handful of large companies and government institutions have the financial resources to operate at\nthis frontier. This concentration of AI development could limit the range of perspectives and approaches considered,\nespecially from academia and broader society. Both AI developers and policymakers must grapple with the rapid AI\nadvances brought on by increasing investment, as well as the tradeoffs involved in the concentration of AI development.\nOn one hand, having few key players at the frontier could make it easier for them to coordinate on responsible AI\ndevelopment. On the other hand, this raises concerns about a lack of public oversight for such a powerful technology.\n4.2 How to estimate training costs\nWe used two approaches to estimate the cost of final training runs: the amortized hardware CapEx + energy approach,\nand the cloud rental price approach. These two approaches produced consistent estimates of the growth rate in training\ncost over time. However, the approaches diverged on individual costs: the cloud costs were twice as large on average.\nWe recommend using the amortized hardware CapEx + energy approach for frontier models wherever it’s feasible,\nbecause it accounts for the lower costs in practice for large training runs, and can be broken down into components.\nOur third approach adds the cost of R&D staff, as well as the compute cost of experiments, evaluations, and fine-tuning\ninvolved in model development. To our knowledge, we present the first detailed estimates of these costs for GPT-3,\nOPT-175B and Gemini Ultra. Moreover, our results suggest that R&D staff costs were a major component of total costs\nfor these frontier models. Although this is the most comprehensive of the three approaches, further data collection and\nevidence on the AI development process are needed before we can recommend it as the default.\n4.3 Limitations\nWhile our study provides valuable insights into the growth of AI training costs, there are important limitations. The\nanalysis relies on publicly available information, which may lead to biases or gaps in the dataset. Cost estimation\nmethods are subject to uncertainties due to factors such as hardware depreciation rates and pricing dynamics. Moreover,\nour methods neglect several costs that are potentially significant, including the data center infrastructure apart from the\ntraining cluster, and the acquisition of data for model training.\nOur results may also have limited generality. The trends observed for the selected frontier models may not generalize to\nthe broader AI landscape, or specific AI domains such as language modeling. Rapid innovation could also lead to large\ngains in hardware and software efficiency that are difficult to predict from historical data. Further research on all of\nthese unknowns would help refine our insights, and inform evidence-based strategies to respond to growing financial\nbarriers in ML.\n5 Conclusion\nIn this paper we used three approaches to analyze the cost of training ML models at the frontier. The first two\napproaches—one based on hardware purchase prices and energy costs, the other based on cloud rental prices—indicate\nthat the amortized cost of compute for these training runs has grown by around 2.4x per year (90% CI: 2.0x to 2.9x)\nsince 2016. This shows the large role of investment in driving AI progress.\nBreaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and\nGemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. This motivates further\nresearch on the scaling of R&D labor with computing power.\nThe rapid exponential growth of costs over eight years suggests that growth is unlikely to stall in the next few years.\nHowever, frontier AI labs appear to face non-trivial challenges to scaling further. One such challenge is securing enough\npower capacity for increasingly large computing clusters. Analyzing potential bottlenecks such as this is an important\ntopic for future work.\nThe rapid increase in AI investment is likely to drive major advances in AI capabilities. Given that total model\ndevelopment costs at the frontier are already over $100 million, these advances may only be accessible to the largest\ncompanies and government institutions. The concentration of such a powerful technology among a few key players\nraises questions about responsible development and deployment. Both AI developers and policymakers must engage\nwith these issues and consider the tradeoffs involved. The stakes are high—decisions made now about the governance\nand trajectory of AI could have profound consequences for society.\n10", "sentences": [{"text": "However, only a handful of large companies and government institutions have the financial resources to operate at\nthis frontier.", "metadata": {}}, {"text": "This concentration of AI development could limit the range of perspectives and approaches considered,\nespecially from academia and broader society.", "metadata": {}}, {"text": "Both AI developers and policymakers must grapple with the rapid AI\nadvances brought on by increasing investment, as well as the tradeoffs involved in the concentration of AI development.", "metadata": {}}, {"text": "On one hand, having few key players at the frontier could make it easier for them to coordinate on responsible AI\ndevelopment.", "metadata": {}}, {"text": "On the other hand, this raises concerns about a lack of public oversight for such a powerful technology.", "metadata": {}}, {"text": "4.2 How to estimate training costs\nWe used two approaches to estimate the cost of final training runs: the amortized hardware CapEx + energy approach,\nand the cloud rental price approach.", "metadata": {}}, {"text": "These two approaches produced consistent estimates of the growth rate in training\ncost over time.", "metadata": {}}, {"text": "However, the approaches diverged on individual costs: the cloud costs were twice as large on average.", "metadata": {}}, {"text": "We recommend using the amortized hardware CapEx + energy approach for frontier models wherever it’s feasible,\nbecause it accounts for the lower costs in practice for large training runs, and can be broken down into components.", "metadata": {}}, {"text": "Our third approach adds the cost of R&D staff, as well as the compute cost of experiments, evaluations, and fine-tuning\ninvolved in model development.", "metadata": {}}, {"text": "To our knowledge, we present the first detailed estimates of these costs for GPT-3,\nOPT-175B and Gemini Ultra.", "metadata": {}}, {"text": "Moreover, our results suggest that R&D staff costs were a major component of total costs\nfor these frontier models.", "metadata": {}}, {"text": "Although this is the most comprehensive of the three approaches, further data collection and\nevidence on the AI development process are needed before we can recommend it as the default.", "metadata": {}}, {"text": "4.3 Limitations\nWhile our study provides valuable insights into the growth of AI training costs, there are important limitations.", "metadata": {}}, {"text": "The\nanalysis relies on publicly available information, which may lead to biases or gaps in the dataset.", "metadata": {}}, {"text": "Cost estimation\nmethods are subject to uncertainties due to factors such as hardware depreciation rates and pricing dynamics.", "metadata": {}}, {"text": "Moreover,\nour methods neglect several costs that are potentially significant, including the data center infrastructure apart from the\ntraining cluster, and the acquisition of data for model training.", "metadata": {}}, {"text": "Our results may also have limited generality.", "metadata": {}}, {"text": "The trends observed for the selected frontier models may not generalize to\nthe broader AI landscape, or specific AI domains such as language modeling.", "metadata": {}}, {"text": "Rapid innovation could also lead to large\ngains in hardware and software efficiency that are difficult to predict from historical data.", "metadata": {}}, {"text": "Further research on all of\nthese unknowns would help refine our insights, and inform evidence-based strategies to respond to growing financial\nbarriers in ML.", "metadata": {}}, {"text": "5 Conclusion\nIn this paper we used three approaches to analyze the cost of training ML models at the frontier.", "metadata": {}}, {"text": "The first two\napproaches—one based on hardware purchase prices and energy costs, the other based on cloud rental prices—indicate\nthat the amortized cost of compute for these training runs has grown by around 2.4x per year (90% CI: 2.0x to 2.9x)\nsince 2016.", "metadata": {}}, {"text": "This shows the large role of investment in driving AI progress.", "metadata": {}}, {"text": "Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and\nGemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total.", "metadata": {}}, {"text": "This motivates further\nresearch on the scaling of R&D labor with computing power.", "metadata": {}}, {"text": "The rapid exponential growth of costs over eight years suggests that growth is unlikely to stall in the next few years.", "metadata": {}}, {"text": "However, frontier AI labs appear to face non-trivial challenges to scaling further.", "metadata": {}}, {"text": "One such challenge is securing enough\npower capacity for increasingly large computing clusters.", "metadata": {}}, {"text": "Analyzing potential bottlenecks such as this is an important\ntopic for future work.", "metadata": {}}, {"text": "The rapid increase in AI investment is likely to drive major advances in AI capabilities.", "metadata": {}}, {"text": "Given that total model\ndevelopment costs at the frontier are already over $100 million, these advances may only be accessible to the largest\ncompanies and government institutions.", "metadata": {}}, {"text": "The concentration of such a powerful technology among a few key players\nraises questions about responsible development and deployment.", "metadata": {}}, {"text": "Both AI developers and policymakers must engage\nwith these issues and consider the tradeoffs involved.", "metadata": {}}, {"text": "The stakes are high—decisions made now about the governance\nand trajectory of AI could have profound consequences for society.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "Acknowledgements\nWe thank Bartosz Podkanowicz for assisting with data collection, Luke Frymire for assisting with ground truth cost\nverification, and Josh You for copyediting. We thank Igor Molybog, Yafah Edelman and Horace He for helpful\nconversations about the training process and requirements for creating large ML models. We thank Konstantin Pilz,\nYafah Edelman, Tom Davidson, Isabel Juniewicz, Carl Shulman, Jaime Sevilla, Aleksandar Kostovic, Tim Fist, Haydn\nBelfield, Alan Chan, David Patterson, Mauricio Baker, Erich Grunewald, and Cullen O’Keefe for feedback on drafts.\nThis study was supported by a grant from the AI Index based out of the Stanford Institute for Human-Centered Artificial\nIntelligence. The cloud compute cost estimates shown here previously appeared in the 2024 Stanford AI Index Report.\nReferences\n[1] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al. Artificial Intelligence Index Report 2023. arXiv\npreprint arXiv:2310.03715, 2023.\n[2] Neil C Thompson, Shuning Ge, and Gabriel F Manso. The importance of (exponentially more) computing power.\narXiv preprint arXiv:2206.14007, 2022.\n[3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022.\n[5] Ezra Klein and Dario Amodei. What if Dario Amodei Is Right about A.I.? https://www.nytimes.com/\n2024/04/12/opinion/ezra-klein-podcast-dario-amodei.html?showTranscript=1 , 2024. Accessed:\n2024-05-30.\n[6] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al. Artificial Intelligence Index Report 2024, 2024.\n[7] Epoch AI. Parameter, Compute and Data Trends in Machine Learning.https://epochai.org/data/epochdb/\nvisualization, 2022. Accessed: 2024-05-30.\n[8] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute\ntrends across three eras of machine learning. In 2022 International Joint Conference on Neural Networks (IJCNN) ,\npages 1–8. IEEE, 2022.\n[9] Marius Hobbhahn, Lennart Heim, and Gökçe Aydos. Trends in Machine Learning Hardware. https://epochai.\norg/blog/trends-in-machine-learning-hardware , 2023. Accessed: 2024-05-30.\n[10] Gemini Team. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2024.\n[11] Barbara Weltman. How Much Does an Employee Cost You? . https://www.sba.gov/blog/\nhow-much-does-employee-cost-you , 2019. Accessed: 2024-05-30.\n[12] OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2024.\n[13] Epoch AI. The length of time spent training notable models is growing. https://epoch.ai/data/\nnotable-ai-models#training-time-growth , 2024. Accessed: 2024-12-09.\n[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877–1901, 2020.\n[15] NVIDIA Corporation. NVIDIA DGX SuperPOD Reference Architecture, 2023.\n[16] Tim Fist. Personal communication, 2024.\n[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates,\nSuresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In\nProceedings of the 44th annual international symposium on computer architecture , pages 1–12, 2017.\n[18] The Information. Google discussed dropping Broadcom as their AI chips supplier, 2023. Accessed: 2024-05-30.\n[19] Dylan Patel and Gerald Wong. AI server cost analysis – memory is the biggest loser, 2023. Accessed: 2024-05-30.\n11", "sentences": [{"text": "Acknowledgements\nWe thank Bartosz Podkanowicz for assisting with data collection, Luke Frymire for assisting with ground truth cost\nverification, and Josh You for copyediting.", "metadata": {}}, {"text": "We thank Igor Molybog, Yafah Edelman and Horace He for helpful\nconversations about the training process and requirements for creating large ML models.", "metadata": {}}, {"text": "We thank Konstantin Pilz,\nYafah Edelman, Tom Davidson, Isabel Juniewicz, Carl Shulman, Jaime Sevilla, Aleksandar Kostovic, Tim Fist, Haydn\nBelfield, Alan Chan, David Patterson, Mauricio Baker, Erich Grunewald, and Cullen O’Keefe for feedback on drafts.", "metadata": {}}, {"text": "This study was supported by a grant from the AI Index based out of the Stanford Institute for Human-Centered Artificial\nIntelligence.", "metadata": {}}, {"text": "The cloud compute cost estimates shown here previously appeared in the 2024 Stanford AI Index Report.", "metadata": {}}, {"text": "References\n[1] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al.", "metadata": {}}, {"text": "Artificial Intelligence Index Report 2023.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2310.03715, 2023.", "metadata": {}}, {"text": "[2] Neil C Thompson, Shuning Ge, and Gabriel F Manso.", "metadata": {}}, {"text": "The importance of (exponentially more) computing power.", "metadata": {}}, {"text": "arXiv preprint arXiv:2206.14007, 2022.", "metadata": {}}, {"text": "[3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei.", "metadata": {}}, {"text": "Scaling laws for neural language models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2001.08361, 2020.", "metadata": {}}, {"text": "[4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.", "metadata": {}}, {"text": "Training compute-optimal large language\nmodels.", "metadata": {}}, {"text": "arXiv preprint arXiv:2203.15556, 2022.", "metadata": {}}, {"text": "[5] Ezra Klein and Dario Amodei.", "metadata": {}}, {"text": "What if Dario Amodei Is Right about A.I.?", "metadata": {}}, {"text": "https://www.nytimes.com/\n2024/04/12/opinion/ezra-klein-podcast-dario-amodei.html?showTranscript=1 , 2024.", "metadata": {}}, {"text": "Accessed:\n2024-05-30.", "metadata": {}}, {"text": "[6] Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, et al.", "metadata": {}}, {"text": "Artificial Intelligence Index Report 2024, 2024.", "metadata": {}}, {"text": "[7] Epoch AI.", "metadata": {}}, {"text": "Parameter, Compute and Data Trends in Machine Learning.https://epochai.org/data/epochdb/\nvisualization, 2022.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[8] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos.", "metadata": {}}, {"text": "Compute\ntrends across three eras of machine learning.", "metadata": {}}, {"text": "In 2022 International Joint Conference on Neural Networks (IJCNN) ,\npages 1–8.", "metadata": {}}, {"text": "IEEE, 2022.", "metadata": {}}, {"text": "[9] Marius Hobbhahn, Lennart Heim, and Gökçe Aydos.", "metadata": {}}, {"text": "Trends in Machine Learning Hardware.", "metadata": {}}, {"text": "https://epochai.", "metadata": {}}, {"text": "org/blog/trends-in-machine-learning-hardware , 2023.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[10] Gemini Team.", "metadata": {}}, {"text": "Gemini: A Family of Highly Capable Multimodal Models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2312.11805, 2024.", "metadata": {}}, {"text": "[11] Barbara Weltman.", "metadata": {}}, {"text": "How Much Does an Employee Cost You?", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "https://www.sba.gov/blog/\nhow-much-does-employee-cost-you , 2019.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[12] OpenAI.", "metadata": {}}, {"text": "GPT-4 Technical Report.", "metadata": {}}, {"text": "arXiv preprint arXiv:2303.08774, 2024.", "metadata": {}}, {"text": "[13] Epoch AI.", "metadata": {}}, {"text": "The length of time spent training notable models is growing.", "metadata": {}}, {"text": "https://epoch.ai/data/\nnotable-ai-models#training-time-growth , 2024.", "metadata": {}}, {"text": "Accessed: 2024-12-09.", "metadata": {}}, {"text": "[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.", "metadata": {}}, {"text": "Language models are few-shot learners.", "metadata": {}}, {"text": "Advances\nin neural information processing systems , 33:1877–1901, 2020.", "metadata": {}}, {"text": "[15] NVIDIA Corporation.", "metadata": {}}, {"text": "NVIDIA DGX SuperPOD Reference Architecture, 2023.", "metadata": {}}, {"text": "[16] Tim Fist.", "metadata": {}}, {"text": "Personal communication, 2024.", "metadata": {}}, {"text": "[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates,\nSuresh Bhatia, Nan Boden, Al Borchers, et al.", "metadata": {}}, {"text": "In-datacenter performance analysis of a tensor processing unit.", "metadata": {}}, {"text": "In\nProceedings of the 44th annual international symposium on computer architecture , pages 1–12, 2017.", "metadata": {}}, {"text": "[18] The Information.", "metadata": {}}, {"text": "Google discussed dropping Broadcom as their AI chips supplier, 2023.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[19] Dylan Patel and Gerald Wong.", "metadata": {}}, {"text": "AI server cost analysis – memory is the biggest loser, 2023.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "[20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/\n1691877797487165443, 2024. Accessed: 2024-05-30.\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs , 2024.\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs , 2024.\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,\nRoman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei\nZhou, and David Patterson. Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) , pages 1–14, 2021.\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet , 2023.\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint\narXiv:2104.10350, 2021.\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems) , 4 2023.\nVersion 01.\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a\nComputer: Designing Warehouse-scale Machines, 2018.\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Ac-\ncessed: 2024-05-30.\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI\nDatacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race , 2024.\nAccessed: 2024-05-30.\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=\nepmt_5_6_a, 2024. Accessed: 2024-01-15.\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and\nopportunities. Proceedings of Machine Learning and Systems , 4:795–813, 2022.\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/\nwhy-gpus-are-great-for-ai/ , 2023. Accessed: 2024-05-30.\n[38] Electricity generation, capacity, and sales in the United States. https://web.archive.\norg/web/20240407085026/https://www.eia.gov/energyexplained/electricity/\nelectricity-in-the-us-top-10.php , 2022. Accessed: 2024-05-29.\n12", "sentences": [{"text": "[20] Tae Kim.", "metadata": {}}, {"text": "Raymond James estimates it costs Nvidia $3,320.", "metadata": {}}, {"text": "https://x.com/firstadopter/status/\n1691877797487165443, 2024.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[21] TechPowerUp.", "metadata": {}}, {"text": "NVIDIA Tesla K80 Specs, 2024.", "metadata": {}}, {"text": "[22] TechPowerUp.", "metadata": {}}, {"text": "NVIDIA Tesla P100 PCIe Specs, 2024.", "metadata": {}}, {"text": "[23] TechPowerUp.", "metadata": {}}, {"text": "NVIDIA Tesla V100 SXM2 32 GB Specs , 2024.", "metadata": {}}, {"text": "[24] TechPowerUp.", "metadata": {}}, {"text": "NVIDIA A100 SXM4 40GB Specs , 2024.", "metadata": {}}, {"text": "[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,\nRoman Castagné, Alexandra Sasha Luccioni, François Yvon, et al.", "metadata": {}}, {"text": "BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.05100, 2022.", "metadata": {}}, {"text": "[26] Norman P.", "metadata": {}}, {"text": "Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B.", "metadata": {}}, {"text": "Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei\nZhou, and David Patterson.", "metadata": {}}, {"text": "Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product.", "metadata": {}}, {"text": "In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) , pages 1–14, 2021.", "metadata": {}}, {"text": "[27] NVIDIA Corporation.", "metadata": {}}, {"text": "NVIDIA DGX H100 Datasheet , 2023.", "metadata": {}}, {"text": "[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeff Dean.", "metadata": {}}, {"text": "Carbon Emissions and Large Neural Network Training.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2104.10350, 2021.", "metadata": {}}, {"text": "[29] NVIDIA Corporation.", "metadata": {}}, {"text": "NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems) , 4 2023.", "metadata": {}}, {"text": "Version 01.", "metadata": {}}, {"text": "[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi.", "metadata": {}}, {"text": "The Datacenter As a\nComputer: Designing Warehouse-scale Machines, 2018.", "metadata": {}}, {"text": "[31] Meta.", "metadata": {}}, {"text": "Data centers - Meta sustainability.", "metadata": {}}, {"text": "https://sustainability.fb.com/data-centers/, 2024.", "metadata": {}}, {"text": "Ac-\ncessed: 2024-05-30.", "metadata": {}}, {"text": "[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros.", "metadata": {}}, {"text": "AI Datacenter Energy Dilemma - Race for AI\nDatacenter Space.", "metadata": {}}, {"text": "https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race , 2024.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[33] Electric Power Monthly.", "metadata": {}}, {"text": "https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=\nepmt_5_6_a, 2024.", "metadata": {}}, {"text": "Accessed: 2024-01-15.", "metadata": {}}, {"text": "[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al.", "metadata": {}}, {"text": "OPT: Open pre-trained transformer language models.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2205.01068, 2022.", "metadata": {}}, {"text": "[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al.", "metadata": {}}, {"text": "Sustainable AI: Environmental implications, challenges and\nopportunities.", "metadata": {}}, {"text": "Proceedings of Machine Learning and Systems , 4:795–813, 2022.", "metadata": {}}, {"text": "[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.", "metadata": {}}, {"text": "Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model.", "metadata": {}}, {"text": "arXiv preprint arXiv:2211.02001, 2022.", "metadata": {}}, {"text": "[37] NVIDIA Corporation.", "metadata": {}}, {"text": "Why GPUs are great for AI.", "metadata": {}}, {"text": "https://blogs.nvidia.com/blog/\nwhy-gpus-are-great-for-ai/ , 2023.", "metadata": {}}, {"text": "Accessed: 2024-05-30.", "metadata": {}}, {"text": "[38] Electricity generation, capacity, and sales in the United States.", "metadata": {}}, {"text": "https://web.archive.", "metadata": {}}, {"text": "org/web/20240407085026/https://www.eia.gov/energyexplained/electricity/\nelectricity-in-the-us-top-10.php , 2022.", "metadata": {}}, {"text": "Accessed: 2024-05-29.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "A Training cost estimation\nA.1 Hardware acquisition cost\nThe frontier AI models in our dataset were trained on clusters of many GPU or TPU chips (or “chips” for short). We\nset out to estimate the total cost of the chips, servers, and networking equipment in these clusters, which we call the\nhardware acquisition cost. This cost is calculated as follows:\nHardware acquisition cost = Acquisition cost per chip × Number of chips\nWhere “Acquisition cost per chip” accounts for the GPU or TPU chip itself, other server costs (CPUs, memory,\nchip-to-chip networking, and markup), and the cost of server-to-server networking equipment. Table 2 shows how we\ncalculated this quantity depending on what was known about the training hardware.\nKnown information Formula for “Acquisition cost per chip”\nSingle GPU price GPU chip price × Chip-to-server factor × Server-to-cluster factor\nGPU server price GPU server price / GPUs per server × Server-to-cluster factor\nChips are TPUs TPU chip cost × Chip-to-server factor × Server-to-cluster factor\nTable 2: The formula to estimate “Acquisition cost per chip”, depending on what is known about the training hardware.\nIf the training hardware was a GPU, we looked up the earliest known price linked to that GPU. If the price we found\nwas for a single GPU, we multiplied that price by a “chip-to-server” cost factor (detailed later). If the price we found\nwas instead for a DGX server, we divided that price by the number of GPUs per server.3 Finally, if the training hardware\nwas a Google TPU, we used the “Geometric mean” production cost estimate from Table 3. This represents the cost of\nthe TPU chip itself, which we then multiplied by a chip-to-server cost factor.\nWe calculated chip-to-server cost factors based on known DGX and single-GPU prices near release, using the formula\n(DGX cost)/(8×GPU cost). We were able to estimate this for the NVIDIA P100 (1.54×), V100 (1.69×), and A100\n(1.66×). For other NVIDIA chips, and for TPUs, we used the mean of these three known factors (1.64×). We assumed\nthat the DGX server prices included the cost of chip-to-chip interconnect switches and transceivers.4 We did not account\nfor financing, i.e. the interest paid on a loan to purchase the hardware up-front.\nOnce we had the cost per chip for a single server, we added the cost of server-to-server networking equipment. We\nused an estimate by Kostovic (forthcoming), based on the reference architecture of the NVIDIA H100 SuperPOD [15].\nAccording to this estimate, approximately 19% of the total cost of the SuperPOD goes towards cluster-level interconnect\nfor configurations with less than 4096 GPUs, and 20% for 4096 GPUs and above, due to an additional third layer of\nswitches. Consistent with these figures, another expert in AI hardware estimated a range of 10% to 20% for A100-based\nclusters with an Infiniband network [16].\nFor simplicity, we assumed that 19% of the hardware acquisition cost was for server-to-server networking equipment. We\ntherefore multiplied the cost per chip for a single server by a “server-to-cluster” factor of100%/(100%−19%) ≈ 1.23×,\nresulting in the final “Acquisition cost per chip”. We assumed that the overhead factor is accurate for TPU servers as\nwell as GPU servers, though we have substantial uncertainty about this. In reality, the proportion of costs varies with\nthe cluster architecture and size.\nA.2 Cost of Google TPUs\nTensor Processing Units are a class of proprietary AI accelerator hardware developed by Google, and used in their\ninternal computing projects and employed in Google Cloud datacenters [17]. These chips are not available for sale, but\nsome of them can be rented on the cloud. Since they have never been sold, there are no available purchase prices, which\nmakes it more difficult to estimate the amortized capital expenses for Google Brain, DeepMind, and other Google labs.\n3HGX servers are more suited to large-scale, customized AI supercomputers, but we found very little information on their pricing,\nso we used DGX pricing.\n4Some of the NVIDIA product pages where we found hardware prices listed NVSwitches as components, but it was unclear\nwhether NVLink cables for chip-to-chip links were included.\n13", "sentences": [{"text": "A Training cost estimation\nA.1 Hardware acquisition cost\nThe frontier AI models in our dataset were trained on clusters of many GPU or TPU chips (or “chips” for short).", "metadata": {}}, {"text": "We\nset out to estimate the total cost of the chips, servers, and networking equipment in these clusters, which we call the\nhardware acquisition cost.", "metadata": {}}, {"text": "This cost is calculated as follows:\nHardware acquisition cost = Acquisition cost per chip × Number of chips\nWhere “Acquisition cost per chip” accounts for the GPU or TPU chip itself, other server costs (CPUs, memory,\nchip-to-chip networking, and markup), and the cost of server-to-server networking equipment.", "metadata": {}}, {"text": "Table 2 shows how we\ncalculated this quantity depending on what was known about the training hardware.", "metadata": {}}, {"text": "Known information Formula for “Acquisition cost per chip”\nSingle GPU price GPU chip price × Chip-to-server factor × Server-to-cluster factor\nGPU server price GPU server price / GPUs per server × Server-to-cluster factor\nChips are TPUs TPU chip cost × Chip-to-server factor × Server-to-cluster factor\nTable 2: The formula to estimate “Acquisition cost per chip”, depending on what is known about the training hardware.", "metadata": {}}, {"text": "If the training hardware was a GPU, we looked up the earliest known price linked to that GPU.", "metadata": {}}, {"text": "If the price we found\nwas for a single GPU, we multiplied that price by a “chip-to-server” cost factor (detailed later).", "metadata": {}}, {"text": "If the price we found\nwas instead for a DGX server, we divided that price by the number of GPUs per server.3 Finally, if the training hardware\nwas a Google TPU, we used the “Geometric mean” production cost estimate from Table 3.", "metadata": {}}, {"text": "This represents the cost of\nthe TPU chip itself, which we then multiplied by a chip-to-server cost factor.", "metadata": {}}, {"text": "We calculated chip-to-server cost factors based on known DGX and single-GPU prices near release, using the formula\n(DGX cost)/(8×GPU cost).", "metadata": {}}, {"text": "We were able to estimate this for the NVIDIA P100 (1.54×), V100 (1.69×), and A100\n(1.66×).", "metadata": {}}, {"text": "For other NVIDIA chips, and for TPUs, we used the mean of these three known factors (1.64×).", "metadata": {}}, {"text": "We assumed\nthat the DGX server prices included the cost of chip-to-chip interconnect switches and transceivers.4 We did not account\nfor financing, i.e.", "metadata": {}}, {"text": "the interest paid on a loan to purchase the hardware up-front.", "metadata": {}}, {"text": "Once we had the cost per chip for a single server, we added the cost of server-to-server networking equipment.", "metadata": {}}, {"text": "We\nused an estimate by Kostovic (forthcoming), based on the reference architecture of the NVIDIA H100 SuperPOD [15].", "metadata": {}}, {"text": "According to this estimate, approximately 19% of the total cost of the SuperPOD goes towards cluster-level interconnect\nfor configurations with less than 4096 GPUs, and 20% for 4096 GPUs and above, due to an additional third layer of\nswitches.", "metadata": {}}, {"text": "Consistent with these figures, another expert in AI hardware estimated a range of 10% to 20% for A100-based\nclusters with an Infiniband network [16].", "metadata": {}}, {"text": "For simplicity, we assumed that 19% of the hardware acquisition cost was for server-to-server networking equipment.", "metadata": {}}, {"text": "We\ntherefore multiplied the cost per chip for a single server by a “server-to-cluster” factor of100%/(100%−19%) ≈ 1.23×,\nresulting in the final “Acquisition cost per chip”.", "metadata": {}}, {"text": "We assumed that the overhead factor is accurate for TPU servers as\nwell as GPU servers, though we have substantial uncertainty about this.", "metadata": {}}, {"text": "In reality, the proportion of costs varies with\nthe cluster architecture and size.", "metadata": {}}, {"text": "A.2 Cost of Google TPUs\nTensor Processing Units are a class of proprietary AI accelerator hardware developed by Google, and used in their\ninternal computing projects and employed in Google Cloud datacenters [17].", "metadata": {}}, {"text": "These chips are not available for sale, but\nsome of them can be rented on the cloud.", "metadata": {}}, {"text": "Since they have never been sold, there are no available purchase prices, which\nmakes it more difficult to estimate the amortized capital expenses for Google Brain, DeepMind, and other Google labs.", "metadata": {}}, {"text": "3HGX servers are more suited to large-scale, customized AI supercomputers, but we found very little information on their pricing,\nso we used DGX pricing.", "metadata": {}}, {"text": "4Some of the NVIDIA product pages where we found hardware prices listed NVSwitches as components, but it was unclear\nwhether NVLink cables for chip-to-chip links were included.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "To estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU\nmanufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end\nestimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead\nof co-designing TPUs with Broadcom [18]. The second approach models the equivalent purchase prices of Google\nTPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We\nconsider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We\ninterpolated hardware costs based on price-performance:\nTPU effective cost = GPU cost × TPU performance\nGPU performance × date adjustment factor\nwhere the date adjustment factor adjusts costs compared on different dates to make them comparable, based on the\ntrend that GPU performance per unit cost improves at a rate of 0.14 orders of magnitude per year.\nFor the manufacturing cost approach, we estimated the manufacturing cost for the NVIDIA DGX SuperPOD at $8,665\nper GPU. This estimate was informed by [19] and [20]—calculations are available in ‘h100_manufacturing_cost.ipynb‘.\nAfter converting this to the TPU cost using the above formula, we divided by the average server-to-chip cost ratio\nof 1.64 that we estimated from NVIDIA GPU prices (see Appendix A.1). The results are listed in Table 3. For the\nequivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google\nML GPUs, listed in Table 4 [21, 22, 23, 24].\nH100 TPU v1 TPU v2 TPU v3 TPU v4\nRelease date 2022-09-21 2015-05-20 2017 2018 2021\nPerformance ratio to H100 100% 5% 9% 12% 28%\nDate adjustment factor 1.00 10.66 5.38 3.90 1.48\nServer manufacturing cost (per chip) $8,665 $4,295 $4,244 $4,200 $3,570\nChip manufacturing cost (estimate) $5,346 $2,650 $2,619 $2,591 $2,203\nPrice of equivalent GPU (estimate) - $11,263 $9,752 $10,742 $12,119\nGeometric mean - $5,463 $5,054 $5,276 $5,176\nTable 3: Cost and performance comparison between Google TPUs and the NVIDIA H100. Performance ratios to the\nNVIDIA H100 use the same number format and are without sparsity. Our overall estimate of TPU costs is the geometric\nmean of estimates for the chip manufacturing cost and the price of an equivalent-performance GPU.\nK80 P100 PCIe 16GB V100 SXM2 32GB A100 SXM4 40GB\nRelease date 2014-11-17 2016-06-20 2018-03-27 2020-05-14\nPerformance in TFLOPS 8 (FP32) 19 (FP16) 125 (FP16) 312 (FP16)\nMemory 24 GB 16 GB 32 GB 40 GB\nSale price $5,000 (at release) $5,699 (at release) $11,458 (2018-05-08) $15,000 (2020)\n$3,700 (2016) $12,500 (2022)\nTable 4: Comparison of GPU specifications. By interpolation between GPUs, and their price-performance data, we\nestimate performance-equivalent prices for TPU versions.\nAs explained above, we consider the manufacturing costs to be low estimates and the equivalent GPU prices to be\nhigh-end estimates of the full production cost. To aggregate the two approaches into a final estimate, we took the\ngeometric mean, as shown in Table 3. Each TPU version has an estimated cost (for Google) of about $5,000.\n14", "sentences": [{"text": "To estimate the cost of TPUs used by Google labs, we aggregated two approaches.", "metadata": {}}, {"text": "The first approach estimates TPU\nmanufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU.", "metadata": {}}, {"text": "We consider this a low-end\nestimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead\nof co-designing TPUs with Broadcom [18].", "metadata": {}}, {"text": "The second approach models the equivalent purchase prices of Google\nTPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications.", "metadata": {}}, {"text": "We\nconsider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips.", "metadata": {}}, {"text": "We\ninterpolated hardware costs based on price-performance:\nTPU effective cost = GPU cost × TPU performance\nGPU performance × date adjustment factor\nwhere the date adjustment factor adjusts costs compared on different dates to make them comparable, based on the\ntrend that GPU performance per unit cost improves at a rate of 0.14 orders of magnitude per year.", "metadata": {}}, {"text": "For the manufacturing cost approach, we estimated the manufacturing cost for the NVIDIA DGX SuperPOD at $8,665\nper GPU.", "metadata": {}}, {"text": "This estimate was informed by [19] and [20]—calculations are available in ‘h100_manufacturing_cost.ipynb‘.", "metadata": {}}, {"text": "After converting this to the TPU cost using the above formula, we divided by the average server-to-chip cost ratio\nof 1.64 that we estimated from NVIDIA GPU prices (see Appendix A.1).", "metadata": {}}, {"text": "The results are listed in Table 3.", "metadata": {}}, {"text": "For the\nequivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google\nML GPUs, listed in Table 4 [21, 22, 23, 24].", "metadata": {}}, {"text": "H100 TPU v1 TPU v2 TPU v3 TPU v4\nRelease date 2022-09-21 2015-05-20 2017 2018 2021\nPerformance ratio to H100 100% 5% 9% 12% 28%\nDate adjustment factor 1.00 10.66 5.38 3.90 1.48\nServer manufacturing cost (per chip) $8,665 $4,295 $4,244 $4,200 $3,570\nChip manufacturing cost (estimate) $5,346 $2,650 $2,619 $2,591 $2,203\nPrice of equivalent GPU (estimate) - $11,263 $9,752 $10,742 $12,119\nGeometric mean - $5,463 $5,054 $5,276 $5,176\nTable 3: Cost and performance comparison between Google TPUs and the NVIDIA H100.", "metadata": {}}, {"text": "Performance ratios to the\nNVIDIA H100 use the same number format and are without sparsity.", "metadata": {}}, {"text": "Our overall estimate of TPU costs is the geometric\nmean of estimates for the chip manufacturing cost and the price of an equivalent-performance GPU.", "metadata": {}}, {"text": "K80 P100 PCIe 16GB V100 SXM2 32GB A100 SXM4 40GB\nRelease date 2014-11-17 2016-06-20 2018-03-27 2020-05-14\nPerformance in TFLOPS 8 (FP32) 19 (FP16) 125 (FP16) 312 (FP16)\nMemory 24 GB 16 GB 32 GB 40 GB\nSale price $5,000 (at release) $5,699 (at release) $11,458 (2018-05-08) $15,000 (2020)\n$3,700 (2016) $12,500 (2022)\nTable 4: Comparison of GPU specifications.", "metadata": {}}, {"text": "By interpolation between GPUs, and their price-performance data, we\nestimate performance-equivalent prices for TPU versions.", "metadata": {}}, {"text": "As explained above, we consider the manufacturing costs to be low estimates and the equivalent GPU prices to be\nhigh-end estimates of the full production cost.", "metadata": {}}, {"text": "To aggregate the two approaches into a final estimate, we took the\ngeometric mean, as shown in Table 3.", "metadata": {}}, {"text": "Each TPU version has an estimated cost (for Google) of about $5,000.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "A.3 Amortization model\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware\navailability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware\nannouncement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU\nactually being shipped to the buyer. Our results are robust to variations in this buffer time—see Appendix B.4.\nFor the training start date, there were a few known cases—for example, GPT-4 finished training in August 2022 [ 12].\nOtherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account\nfor time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the\ntraining time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\nThe precise way to amortize the training cost through exponential depreciation is:\nAmortized training cost = Start value per chip × Number of chips × Depreciation during training\n= Start value per chip × Number of chips ×\n\u0010\n1 − exp\n\u0002\n− Training time × r ln 10\n\u0003\u0011\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training\ntime or the number of chips separately. This is because chip-hours can also be estimated from training compute in\nFLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these\nchip-hour estimates:\nAmortized training cost = Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nThis approximation is valid if (Training time) × r ln 10 is small, and this is the case for the training times in our data and\nour choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 × 0.14 ln(10) ∼= 32% deprecation\ncompared to 1 − exp(−1 × 0.14 ln(10)) ∼= 28% depreciation. This is not a large difference relative to other sources\nof uncertainty.\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a\nsignificant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per\nweek occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all\ncatastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs\ndefective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.\nA.4 Energy cost estimation\nTo model the cost of energy consumed by hardware during a training run, we started with the thermal design power\n(TDP) of the GPU or TPU used for training. We then scaled this up to estimate the TDP of one server. For TPUs, the\nserver scale-up was based on data from Table 1 of [26]. For NVIDIA GPUs, we used specifications such as [27].\nNext, we converted TDP to the average power actually consumed during training. For TPUs we used an average value\nof 43% using data on TDP and average power in [28, Table 4], as well as data on TDP in [26, Table 1]. For GPUs we\nalso aggregated multiple sources ([29],[28], and [30, p. 133]), arriving at an all-things-considered estimate of 75%.\nTo account for power consumed by data center power distribution and cooling, we multiplied average server power by\nthe power usage effectiveness. We used a PUE of 1.1 for data centers owned by hyperscalers such as Alphabet and\nMicrosoft, based on [28, Table 4] and a statement by Meta [31]. Otherwise, we used 1.25 [32].\nTo get the total energy cost of the training run, we multiplied the energy consumption by the average industrial electricity\nprice in the model publication year [33].\nA.5 Cloud price selection\nTo estimate training costs from cloud rental prices, we matched the hardware type and publication date of each ML\nmodel with a price from the hardware price database. To do this, we first filtered the database for prices which matched\n15", "sentences": [{"text": "A.3 Amortization model\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\nStart value per chip = Acquisition cost per chip\nexp\n\u0010\u0002\nTraining start date − Hardware availability date\n\u0003\n· r ln 10\n\u0011\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years.", "metadata": {}}, {"text": "The hardware\navailability date depended on the type of hardware.", "metadata": {}}, {"text": "If the hardware was a Google TPU, we used the hardware\nannouncement date.", "metadata": {}}, {"text": "For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU\nactually being shipped to the buyer.", "metadata": {}}, {"text": "Our results are robust to variations in this buffer time—see Appendix B.4.", "metadata": {}}, {"text": "For the training start date, there were a few known cases—for example, GPT-4 finished training in August 2022 [ 12].", "metadata": {}}, {"text": "Otherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account\nfor time spent evaluating the model and writing the paper.", "metadata": {}}, {"text": "Again, our results are robust to variations in this buffer.", "metadata": {}}, {"text": "If the\ntraining time was unknown, we used the median of known values in our dataset, which was approximately 33 days.", "metadata": {}}, {"text": "The precise way to amortize the training cost through exponential depreciation is:\nAmortized training cost = Start value per chip × Number of chips × Depreciation during training\n= Start value per chip × Number of chips ×\n\u0010\n1 − exp\n\u0002\n− Training time × r ln 10\n\u0003\u0011\nwhere training time is in years.", "metadata": {}}, {"text": "However, we could estimate chip-hours more often and more reliably than the training\ntime or the number of chips separately.", "metadata": {}}, {"text": "This is because chip-hours can also be estimated from training compute in\nFLOP divided by the FLOP/s achieved during training.", "metadata": {}}, {"text": "We used a linear approximation to take advantage of these\nchip-hour estimates:\nAmortized training cost = Start value per chip × Training chip-hours\n(365 × 24) hours/year × r ln 10\nThis approximation is valid if (Training time) × r ln 10 is small, and this is the case for the training times in our data and\nour choice of r = 0.14.", "metadata": {}}, {"text": "In an extreme case, a training time of 1 year results in 1 × 0.14 ln(10) ∼= 32% deprecation\ncompared to 1 − exp(−1 × 0.14 ln(10)) ∼= 28% depreciation.", "metadata": {}}, {"text": "This is not a large difference relative to other sources\nof uncertainty.", "metadata": {}}, {"text": "Due to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a\nsignificant source of depreciation relative to hardware progress.", "metadata": {}}, {"text": "As one data point, an average of 1 to 2 failures per\nweek occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25].", "metadata": {}}, {"text": "Even if these were all\ncatastrophic failures, the expected hardware lifetime would be 3.7 years.", "metadata": {}}, {"text": "We expect that NVIDIA replaces or repairs\ndefective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.", "metadata": {}}, {"text": "A.4 Energy cost estimation\nTo model the cost of energy consumed by hardware during a training run, we started with the thermal design power\n(TDP) of the GPU or TPU used for training.", "metadata": {}}, {"text": "We then scaled this up to estimate the TDP of one server.", "metadata": {}}, {"text": "For TPUs, the\nserver scale-up was based on data from Table 1 of [26].", "metadata": {}}, {"text": "For NVIDIA GPUs, we used specifications such as [27].", "metadata": {}}, {"text": "Next, we converted TDP to the average power actually consumed during training.", "metadata": {}}, {"text": "For TPUs we used an average value\nof 43% using data on TDP and average power in [28, Table 4], as well as data on TDP in [26, Table 1].", "metadata": {}}, {"text": "For GPUs we\nalso aggregated multiple sources ([29],[28], and [30, p.", "metadata": {}}, {"text": "133]), arriving at an all-things-considered estimate of 75%.", "metadata": {}}, {"text": "To account for power consumed by data center power distribution and cooling, we multiplied average server power by\nthe power usage effectiveness.", "metadata": {}}, {"text": "We used a PUE of 1.1 for data centers owned by hyperscalers such as Alphabet and\nMicrosoft, based on [28, Table 4] and a statement by Meta [31].", "metadata": {}}, {"text": "Otherwise, we used 1.25 [32].", "metadata": {}}, {"text": "To get the total energy cost of the training run, we multiplied the energy consumption by the average industrial electricity\nprice in the model publication year [33].", "metadata": {}}, {"text": "A.5 Cloud price selection\nTo estimate training costs from cloud rental prices, we matched the hardware type and publication date of each ML\nmodel with a price from the hardware price database.", "metadata": {}}, {"text": "To do this, we first filtered the database for prices which matched\n15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "the hardware type and the most likely cloud provider that would be used, with the latter based on the developer of\nthe ML model, e.g. using Google Cloud for any Google lab, and using Microsoft Azure for OpenAI based on their\npartnership with Microsoft. We then estimated the date of hardware procurement as the publication date minus the\ntraining time, and minus a further 2 months to account for preparation before the training run.5 If the training time was\nunavailable, we used the median value of approximately 33 days.\nFinally, we searched the price database for the price per chip-hour that was dated nearest to the estimated procurement\ndate. We defaulted to the price for a 3-year rental commitment. Based on a few custom quotes we requested from cloud\nproviders, we found that actual cloud computing prices are negotiable and can be substantially lower than publicly\nlisted prices. We concluded that a 3-year commitment price is the closest on average to what developers would be\nquoted, even if they make a shorter commitment.\nPrices were not available for every specific combination of hardware, cloud provider, and rental date, so we used\nseveral fallbacks to select the most closely applicable cloud rental price, for example using nearest prices in time, using\nprices for similar hardware models, etc. The full procedure is provided at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/prices.py#L210-L294 .\nA.6 Accounting for compute used throughout model development\nIt is important to consider compute used throughout model development. The cost of experiments, evaluations, and\nfine-tuning reflects actual costs for developers to research and possibly deploy a useful ML model. This compute is not\nonly important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges\nfrom 1.2x to 4x, with a median of 2.2x.\nOne source of evidence on the allocation of compute is the training of smaller model sizes for a given architecture. For\nexample, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6 × parameters × tokens) [14, Table\n2.1]. This shows at least 14% of compute was spent outside the main training run. Similar reasoning for BLOOM\nreveals about 63% of compute was used on smaller models [25, Table 5].\nAnother source of evidence is reports of how compute budgets are allocated. For example, the OPT-175B developers\nestimated total cost at “roughly 2x higher” than the largest training run [34]. Meanwhile, across Meta’s AI infrastructure,\none estimate in the literature suggested a 1:2 ratio between experimentation and training, where training includes\nadditional hyper-parameter tuning and retraining [35].\nFor GPT-3, the true ratio is almost certainly higher than 1.14x due to failures and other experiments. We believe the\nMeta, BLOOM and OPT-175B cases are the more central examples as they account better for all experiments. So\na factor close to 2x seems like a reasonable median estimate. On the high end, it’s plausible that several large-scale\nexperiments are necessary before training—say, 4x. We sampled from the range of plausible values using a log-normal\ndistribution. The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.\nA.7 Cost uncertainty analysis\nOur cost estimation methods have many sources of uncertainty, making it important to measure overall uncertainty in\nthe estimates. To do this, we first made a rough estimate of the relative uncertainty in each input variable, based on\nempirical data. For example, for the overhead of per-GPU server cost relative to single GPU cost we assigned a 90%\ncredible interval of 1.3x to 2.1x, which is wider than the range of values in our data and from industry sources.6\nWe then used a simulation to sample from distributions over each input variable. The simulation, along\nwith details of the bounds for each input variable, are available at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/uncertainty.ipynb. The simulation used log-normal distributions for all\nvariables except depreciation rate and utilization rate, which used normal distributions. The sampled variables were\ncombined into a sample of final costs, using the relevant formula. The cost sample was then normalized to have a\nmedian value of 1. The 90% CI of this normalized sample represents the relative uncertainty in cost.\nThe relative uncertainties in cost are listed in Table 5. Hardware acquisition cost involves fewer variables with less\nuncertainty, so estimates are generally accurate within a factor of two for models trained on GPUs, and within a factor of\n4 for models trained on TPUs. Meanwhile, amortized hardware CapEx + energy is generally accurate within a factor of\nthree or four for models trained on GPUs, and a factor of five for models trained on TPUs. The cost estimates are most\n5The choice of 2 months was an educated guess. The matching of models to cloud prices was not very sensitive to this choice\nbecause the price data was sparse and stable over time.\n6The three actual values we calculated ranged from 1.54 (P100) to 1.69 (V100). A pre-existing cost breakdown of a DGX H100\nimplies a ratio of approximately 1.4 (total cost divided by \"8 GPU + 4 NVSwitch Baseboard\" cost) [19].\n16", "sentences": [{"text": "the hardware type and the most likely cloud provider that would be used, with the latter based on the developer of\nthe ML model, e.g.", "metadata": {}}, {"text": "using Google Cloud for any Google lab, and using Microsoft Azure for OpenAI based on their\npartnership with Microsoft.", "metadata": {}}, {"text": "We then estimated the date of hardware procurement as the publication date minus the\ntraining time, and minus a further 2 months to account for preparation before the training run.5 If the training time was\nunavailable, we used the median value of approximately 33 days.", "metadata": {}}, {"text": "Finally, we searched the price database for the price per chip-hour that was dated nearest to the estimated procurement\ndate.", "metadata": {}}, {"text": "We defaulted to the price for a 3-year rental commitment.", "metadata": {}}, {"text": "Based on a few custom quotes we requested from cloud\nproviders, we found that actual cloud computing prices are negotiable and can be substantially lower than publicly\nlisted prices.", "metadata": {}}, {"text": "We concluded that a 3-year commitment price is the closest on average to what developers would be\nquoted, even if they make a shorter commitment.", "metadata": {}}, {"text": "Prices were not available for every specific combination of hardware, cloud provider, and rental date, so we used\nseveral fallbacks to select the most closely applicable cloud rental price, for example using nearest prices in time, using\nprices for similar hardware models, etc.", "metadata": {}}, {"text": "The full procedure is provided at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/prices.py#L210-L294 .", "metadata": {}}, {"text": "A.6 Accounting for compute used throughout model development\nIt is important to consider compute used throughout model development.", "metadata": {}}, {"text": "The cost of experiments, evaluations, and\nfine-tuning reflects actual costs for developers to research and possibly deploy a useful ML model.", "metadata": {}}, {"text": "This compute is not\nonly important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges\nfrom 1.2x to 4x, with a median of 2.2x.", "metadata": {}}, {"text": "One source of evidence on the allocation of compute is the training of smaller model sizes for a given architecture.", "metadata": {}}, {"text": "For\nexample, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6 × parameters × tokens) [14, Table\n2.1].", "metadata": {}}, {"text": "This shows at least 14% of compute was spent outside the main training run.", "metadata": {}}, {"text": "Similar reasoning for BLOOM\nreveals about 63% of compute was used on smaller models [25, Table 5].", "metadata": {}}, {"text": "Another source of evidence is reports of how compute budgets are allocated.", "metadata": {}}, {"text": "For example, the OPT-175B developers\nestimated total cost at “roughly 2x higher” than the largest training run [34].", "metadata": {}}, {"text": "Meanwhile, across Meta’s AI infrastructure,\none estimate in the literature suggested a 1:2 ratio between experimentation and training, where training includes\nadditional hyper-parameter tuning and retraining [35].", "metadata": {}}, {"text": "For GPT-3, the true ratio is almost certainly higher than 1.14x due to failures and other experiments.", "metadata": {}}, {"text": "We believe the\nMeta, BLOOM and OPT-175B cases are the more central examples as they account better for all experiments.", "metadata": {}}, {"text": "So\na factor close to 2x seems like a reasonable median estimate.", "metadata": {}}, {"text": "On the high end, it’s plausible that several large-scale\nexperiments are necessary before training—say, 4x.", "metadata": {}}, {"text": "We sampled from the range of plausible values using a log-normal\ndistribution.", "metadata": {}}, {"text": "The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.", "metadata": {}}, {"text": "A.7 Cost uncertainty analysis\nOur cost estimation methods have many sources of uncertainty, making it important to measure overall uncertainty in\nthe estimates.", "metadata": {}}, {"text": "To do this, we first made a rough estimate of the relative uncertainty in each input variable, based on\nempirical data.", "metadata": {}}, {"text": "For example, for the overhead of per-GPU server cost relative to single GPU cost we assigned a 90%\ncredible interval of 1.3x to 2.1x, which is wider than the range of values in our data and from industry sources.6\nWe then used a simulation to sample from distributions over each input variable.", "metadata": {}}, {"text": "The simulation, along\nwith details of the bounds for each input variable, are available at https://github.com/epoch-research/\ntraining-cost-trends/blob/main/uncertainty.ipynb.", "metadata": {}}, {"text": "The simulation used log-normal distributions for all\nvariables except depreciation rate and utilization rate, which used normal distributions.", "metadata": {}}, {"text": "The sampled variables were\ncombined into a sample of final costs, using the relevant formula.", "metadata": {}}, {"text": "The cost sample was then normalized to have a\nmedian value of 1.", "metadata": {}}, {"text": "The 90% CI of this normalized sample represents the relative uncertainty in cost.", "metadata": {}}, {"text": "The relative uncertainties in cost are listed in Table 5.", "metadata": {}}, {"text": "Hardware acquisition cost involves fewer variables with less\nuncertainty, so estimates are generally accurate within a factor of two for models trained on GPUs, and within a factor of\n4 for models trained on TPUs.", "metadata": {}}, {"text": "Meanwhile, amortized hardware CapEx + energy is generally accurate within a factor of\nthree or four for models trained on GPUs, and a factor of five for models trained on TPUs.", "metadata": {}}, {"text": "The cost estimates are most\n5The choice of 2 months was an educated guess.", "metadata": {}}, {"text": "The matching of models to cloud prices was not very sensitive to this choice\nbecause the price data was sparse and stable over time.", "metadata": {}}, {"text": "6The three actual values we calculated ranged from 1.54 (P100) to 1.69 (V100).", "metadata": {}}, {"text": "A pre-existing cost breakdown of a DGX H100\nimplies a ratio of approximately 1.4 (total cost divided by \"8 GPU + 4 NVSwitch Baseboard\" cost) [19].", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "sensitive to the GPU and TPU unit cost (accurate within factors of 2 and 4 respectively) and the training chip-hours\n(factor of 3).\nCost quantity 90% CI\nHardware acquisition (GPUs) 0.5x to 2x\nHardware acquisition (TPUs) 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) 0.2x to 5x\nTable 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger\nuncertainty due to the additional uncertainty in estimating their equivalent costs.\nA.8 Ground truth cost comparison\nIn order to verify that our results are reasonable, we sought to compare our cost estimates with true costs reported by\ndevelopers and other sources. However, there are very few models where the developers report both the computing\nresource usage and the total cost. Training costs and compute resources are independently known for BLOOM-176B\nand OPT-175B, so we compare our estimates with these.\nBLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs\nutilization and a cost of $3 million (including experiments) [25]. We estimated a cloud compute cost of $1.99M or an\namortized cost of $0.8M for BLOOM-176B. The accuracy of this estimate depends on how much of the grant was spent\non experiments versus the final training run. According to BLOOM’s model page on Hugging Face, the “Estimated\ncost of training” is the “Equivalent of $2–5M in cloud computing (including preliminary experiments)”. Preliminary\nexperiments included training smaller BLOOM models. The final training run for the 176B model used 37.24% of the\nenergy of the BLOOM project [36]; if the total cost of the project was C3M as in the grant description, this implies that\nBLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized\ncost approach ($900K) than the cloud cost approach ($2M).\nOPT-175B was trained for 793.5 hours, at a cost of $2500/hour as reported in the training logbook [ 34], for a total\ncost of $1.98 million. We estimated a cloud compute cost of $1.5M for the final training run of OPT-175B, which is\noff by 25%, and an amortized hardware and energy cost of $700K, off by 65%. OPT’s cluster cost rate per hour was\nlikely greater than what we estimate from the quantity of GPUs, or less than the approximate figure mentioned by the\ndevelopers in the training log.\nB Sensitivity analysis\nB.1 Selection of historic frontier models\nIn order to analyse trends in frontier ML models, we must define what counts as a frontier model at any point in time.7\nOur preferred approach is to select models from the database that were in the top 10 most compute-intensive models as\nof their release date, although we considered others as shown in Figure 7.\nFor the most part, different selection approaches gave similar results. The exception was selecting frontier models based\non distance from the compute trend. This approach imposes an artificially flat floor on the eligible models. Due to this,\nit leaves out many earlier models, and produces a flatter cost trend than the other methods.\nOur preferred approach has an advantage over alternatives: the selection is more robust to the sampling of our dataset.\nApproaches based on quantiles, or distance from the historic trend, are influenced by data collected on models outside\nthe frontier. Selecting the top-ranked models, in comparison, is merely influenced by whether the dataset contains those\nfrontier models.\n7Models in the database meet one or more of the following criteria: (i) advanced the state of the art on a qualifying benchmark,\n(ii) at least 1000 citations, (iii) at least one million monthly active users, or (iv) equivalent historical significance [7]. However, this\nmeans the database includes many models that were far from the frontier of compute.\n17", "sentences": [{"text": "sensitive to the GPU and TPU unit cost (accurate within factors of 2 and 4 respectively) and the training chip-hours\n(factor of 3).", "metadata": {}}, {"text": "Cost quantity 90% CI\nHardware acquisition (GPUs) 0.5x to 2x\nHardware acquisition (TPUs) 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) 0.2x to 5x\nTable 5: Estimated relative uncertainty in individual cost estimates, for different methods.", "metadata": {}}, {"text": "TPU estimates have larger\nuncertainty due to the additional uncertainty in estimating their equivalent costs.", "metadata": {}}, {"text": "A.8 Ground truth cost comparison\nIn order to verify that our results are reasonable, we sought to compare our cost estimates with true costs reported by\ndevelopers and other sources.", "metadata": {}}, {"text": "However, there are very few models where the developers report both the computing\nresource usage and the total cost.", "metadata": {}}, {"text": "Training costs and compute resources are independently known for BLOOM-176B\nand OPT-175B, so we compare our estimates with these.", "metadata": {}}, {"text": "BLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs\nutilization and a cost of $3 million (including experiments) [25].", "metadata": {}}, {"text": "We estimated a cloud compute cost of $1.99M or an\namortized cost of $0.8M for BLOOM-176B.", "metadata": {}}, {"text": "The accuracy of this estimate depends on how much of the grant was spent\non experiments versus the final training run.", "metadata": {}}, {"text": "According to BLOOM’s model page on Hugging Face, the “Estimated\ncost of training” is the “Equivalent of $2–5M in cloud computing (including preliminary experiments)”.", "metadata": {}}, {"text": "Preliminary\nexperiments included training smaller BLOOM models.", "metadata": {}}, {"text": "The final training run for the 176B model used 37.24% of the\nenergy of the BLOOM project [36];", "metadata": {}}, {"text": "if the total cost of the project was C3M as in the grant description, this implies that\nBLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized\ncost approach ($900K) than the cloud cost approach ($2M).", "metadata": {}}, {"text": "OPT-175B was trained for 793.5 hours, at a cost of $2500/hour as reported in the training logbook [ 34], for a total\ncost of $1.98 million.", "metadata": {}}, {"text": "We estimated a cloud compute cost of $1.5M for the final training run of OPT-175B, which is\noff by 25%, and an amortized hardware and energy cost of $700K, off by 65%.", "metadata": {}}, {"text": "OPT’s cluster cost rate per hour was\nlikely greater than what we estimate from the quantity of GPUs, or less than the approximate figure mentioned by the\ndevelopers in the training log.", "metadata": {}}, {"text": "B Sensitivity analysis\nB.1 Selection of historic frontier models\nIn order to analyse trends in frontier ML models, we must define what counts as a frontier model at any point in time.7\nOur preferred approach is to select models from the database that were in the top 10 most compute-intensive models as\nof their release date, although we considered others as shown in Figure 7.", "metadata": {}}, {"text": "For the most part, different selection approaches gave similar results.", "metadata": {}}, {"text": "The exception was selecting frontier models based\non distance from the compute trend.", "metadata": {}}, {"text": "This approach imposes an artificially flat floor on the eligible models.", "metadata": {}}, {"text": "Due to this,\nit leaves out many earlier models, and produces a flatter cost trend than the other methods.", "metadata": {}}, {"text": "Our preferred approach has an advantage over alternatives: the selection is more robust to the sampling of our dataset.", "metadata": {}}, {"text": "Approaches based on quantiles, or distance from the historic trend, are influenced by data collected on models outside\nthe frontier.", "metadata": {}}, {"text": "Selecting the top-ranked models, in comparison, is merely influenced by whether the dataset contains those\nfrontier models.", "metadata": {}}, {"text": "7Models in the database meet one or more of the following criteria: (i) advanced the state of the art on a qualifying benchmark,\n(ii) at least 1000 citations, (iii) at least one million monthly active users, or (iv) equivalent historical significance [7].", "metadata": {}}, {"text": "However, this\nmeans the database includes many models that were far from the frontier of compute.", "metadata": {}}, {"text": "17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop-N=10 Top 20% of models in year before/after\nTop 15% of models in year before Top 20% of residuals from trend\n2.4x per year 2.3x per year\n2.2x per year 1.6x per year\nFigure 7: Comparison of hardware capex + energy cost regression using different frontier model selection methods.\nResults are fairly similar across methods, although taking the top 20% of residuals leads to a flatter trend.\nB.2 Varying N in top-N model selection\nWhen selecting frontier models by the top- N method, there is a question of how to choose N. We chose N = 10\nto produce a large enough sample size while still focusing on models near the frontier. The estimated growth rate is\nmoderately robust to the choice of N, as it is similar for N = 3, N = 5 and N = 20 (see Figure 8).\nB.3 Varying the depreciation of hardware value\nThe growth in price-performance for ML GPUs running at FP32 precision has been estimated at 0.14 OOMs/year with\na 90% CI of 0.10 to 0.18 OOMs/year [ 9]. Substituting the lower and upper bounds of that CI for the depreciation\nrate did not significantly change the growth rate of amortized hardware CapEx + energy. For the lower bound of 0.10\nOOMs/year, cost estimates decreased by 15% on average, while for the upper bound of 0.18 OOMs/year, cost estimates\nincreased by 10% on average. Note that increasing the depreciation rate has two effects that partially cancel out: 1. the\nvalue of hardware at the start of training is smaller, 2. the proportion of value used up by training is larger.\nWe also tested 0.3 OOMs/year as an extreme case, based on a claim that single-GPU inference performance has\nimproved by 1000× in the last decade [37]. This did not significantly change the growth rate either, but it increased\ncosts by an average of 30%.\nB.4 Varying the time between hardware acquisition and the start of training\nWe tested different estimates of the hardware acquisition date relative to the release date, as well as the training start\ndate relative to the model publication date. These dates affect the time over which hardware value depreciates. To\nmake the depreciation times long, we removed the minimum buffer of 90 days between hardware release and hardware\nacquisition, and pushed the default training start date back by 15 days relative to the publication date. This decreased\nthe estimated costs by 4% on average, and did not change the growth rate. Similarly, we tested a short depreciation time\nby extending the hardware acquisition buffer time to 180 days and bringing the default training start date forward by 60\ndays. This increased costs by 10% on average and did not change the growth rate.\n18", "sentences": [{"text": "2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop-N=10 Top 20% of models in year before/after\nTop 15% of models in year before Top 20% of residuals from trend\n2.4x per year 2.3x per year\n2.2x per year 1.6x per year\nFigure 7: Comparison of hardware capex + energy cost regression using different frontier model selection methods.", "metadata": {}}, {"text": "Results are fairly similar across methods, although taking the top 20% of residuals leads to a flatter trend.", "metadata": {}}, {"text": "B.2 Varying N in top-N model selection\nWhen selecting frontier models by the top- N method, there is a question of how to choose N.", "metadata": {}}, {"text": "We chose N = 10\nto produce a large enough sample size while still focusing on models near the frontier.", "metadata": {}}, {"text": "The estimated growth rate is\nmoderately robust to the choice of N, as it is similar for N = 3, N = 5 and N = 20 (see Figure 8).", "metadata": {}}, {"text": "B.3 Varying the depreciation of hardware value\nThe growth in price-performance for ML GPUs running at FP32 precision has been estimated at 0.14 OOMs/year with\na 90% CI of 0.10 to 0.18 OOMs/year [ 9].", "metadata": {}}, {"text": "Substituting the lower and upper bounds of that CI for the depreciation\nrate did not significantly change the growth rate of amortized hardware CapEx + energy.", "metadata": {}}, {"text": "For the lower bound of 0.10\nOOMs/year, cost estimates decreased by 15% on average, while for the upper bound of 0.18 OOMs/year, cost estimates\nincreased by 10% on average.", "metadata": {}}, {"text": "Note that increasing the depreciation rate has two effects that partially cancel out: 1.", "metadata": {}}, {"text": "the\nvalue of hardware at the start of training is smaller, 2.", "metadata": {}}, {"text": "the proportion of value used up by training is larger.", "metadata": {}}, {"text": "We also tested 0.3 OOMs/year as an extreme case, based on a claim that single-GPU inference performance has\nimproved by 1000× in the last decade [37].", "metadata": {}}, {"text": "This did not significantly change the growth rate either, but it increased\ncosts by an average of 30%.", "metadata": {}}, {"text": "B.4 Varying the time between hardware acquisition and the start of training\nWe tested different estimates of the hardware acquisition date relative to the release date, as well as the training start\ndate relative to the model publication date.", "metadata": {}}, {"text": "These dates affect the time over which hardware value depreciates.", "metadata": {}}, {"text": "To\nmake the depreciation times long, we removed the minimum buffer of 90 days between hardware release and hardware\nacquisition, and pushed the default training start date back by 15 days relative to the publication date.", "metadata": {}}, {"text": "This decreased\nthe estimated costs by 4% on average, and did not change the growth rate.", "metadata": {}}, {"text": "Similarly, we tested a short depreciation time\nby extending the hardware acquisition buffer time to 180 days and bringing the default training start date forward by 60\ndays.", "metadata": {}}, {"text": "This increased costs by 10% on average and did not change the growth rate.", "metadata": {}}, {"text": "18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop 3 Top 5\nTop 10 Top 20\n2.3x per year 2.3x per year\n2.4x per year 2.8x per year\nFigure 8: Comparison of amortized hardware capex + energy regression for varying top-N selection.\nB.5 Excluding equity from R&D staff costs\nTo measure the impact of equity on the total amortized model development cost, Figure 9a shows the cost breakdown\nwith equity excluded from the R&D staff cost. The proportion of cost on R&D staff decreases from 29–49% with equity\nincluded, to 19–33% with equity excluded.\nC Power capacity for model training\nFigure 10 shows the trend in the power capacity of the compute cluster needed for training frontier models. This was\nbased on the following formula:\nPower capacity (kW) = Hardware quantity × Hardware TDP (kW) × Data center PUE\nwhere “Hardware TDP” includes all server hardware. We find a growth rate of 2.2x per year (90% CI: 1.9x to 2.6x).\nGemini Ultra has the largest estimated power capacity, at around 35 MW. Projecting the trend forward from Gemini\nUltra, the most power-intensive training run would draw 1 GW at some point in 2028. To put this in context, the top ten\nlargest power plants in the United States have a capacity ranging from 3 GW to 7 GW [38].\n19", "sentences": [{"text": "2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\n2016 2018 2020 2022 2024\n100\n10k\n1M\n100M\nPublication date Publication date\nCost (2023 USD, log scale)Cost (2023 USD, log scale)\nTop 3 Top 5\nTop 10 Top 20\n2.3x per year 2.3x per year\n2.4x per year 2.8x per year\nFigure 8: Comparison of amortized hardware capex + energy regression for varying top-N selection.", "metadata": {}}, {"text": "B.5 Excluding equity from R&D staff costs\nTo measure the impact of equity on the total amortized model development cost, Figure 9a shows the cost breakdown\nwith equity excluded from the R&D staff cost.", "metadata": {}}, {"text": "The proportion of cost on R&D staff decreases from 29–49% with equity\nincluded, to 19–33% with equity excluded.", "metadata": {}}, {"text": "C Power capacity for model training\nFigure 10 shows the trend in the power capacity of the compute cluster needed for training frontier models.", "metadata": {}}, {"text": "This was\nbased on the following formula:\nPower capacity (kW) = Hardware quantity × Hardware TDP (kW) × Data center PUE\nwhere “Hardware TDP” includes all server hardware.", "metadata": {}}, {"text": "We find a growth rate of 2.2x per year (90% CI: 1.9x to 2.6x).", "metadata": {}}, {"text": "Gemini Ultra has the largest estimated power capacity, at around 35 MW.", "metadata": {}}, {"text": "Projecting the trend forward from Gemini\nUltra, the most power-intensive training run would draw 1 GW at some point in 2028.", "metadata": {}}, {"text": "To put this in context, the top ten\nlargest power plants in the United States have a capacity ranging from 3 GW to 7 GW [38].", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "A m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 . 0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( e x c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 4 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 . 0  U l t r a\n2 1 % 3 6 % 2 5 % 1 4 % 3 %\n3 2 % 3 2 % 2 1 % 1 3 %\n1 9 % 3 6 % 1 4 % 7 %\n3 3 % 3 0 % 1 9 % 1 1 % 6 %\n(b)\nFigure 9: (a) Breakdown of total amortized model development costs for selected models, with equity excluded from\nthe R&D staff cost. Hardware costs are amortized to the total number of chip-hours spent on experiments and training,\nwhile R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate\n90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based\non median estimates.\nFigure 10: The trend in AI compute cluster power (in kilowatts) required to train frontier models over time. Power is\ncalculated as the product of the number of servers, server TDP, and power usage effectiveness.\n20", "sentences": [{"text": "A m o rt i z e d  h a r d w a r e ,  e n e r g y ,  a n d  R & D  s t a ff  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s\nC o s t  ( 2 0 2 3  U S D ,  l o g  s c a l e )\n2 0 0 M\n1 0 0 M\n5 0 M\n2 0 M\n1 0 M\n5 M\n2 M\n1 M\n5 0 0 k\n2 0 0 k\n1 0 0 k\n5 0 k\n2 0 k\n1 0 k\nG P T - 3  1 7 5 B  ( d a v i n c i ) O P T - 1 7 5 B G P T - 4 G e m i n i  1 .", "metadata": {}}, {"text": "0  U l t r a\nR & D  s t a f f  ( i n c l u d i n g  e q u i t y )\nA I  a c c e l e r a t o r  c h i p s\nO t h e r  s e rv e r  c o m p o n e n t s\nC l u s t e r - l e v e l  i n t e r c o n n e c t\nE n e r g y\n(a)\nP e r c e n t a g e  o f  c o s t s  f o r  t r a i n i n g  a n d  e x p e r i m e n t s  o f  M L  m o d e l s\nR & D  s t a f f  ( e x c l u d i n g  e q u i t y ) A I  a c c e l e r a t o r  c h i p s O t h e r  s e rv e r  c o m p o n e n t s C l u s t e r - l e v e l  i n t e r c o n n e c t E n e r g y\n0 % 1 0 % 2 0 % 3 0 % 4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 % 1 0 0 %\nP r o p o r t i o n\nX X\n%\n2 4 %\nG P T - 3  1 7 5 B  ( d a v i n c i )\nO P T - 1 7 5 B\nG P T - 4\nG e m i n i  1 .", "metadata": {}}, {"text": "0  U l t r a\n2 1 % 3 6 % 2 5 % 1 4 % 3 %\n3 2 % 3 2 % 2 1 % 1 3 %\n1 9 % 3 6 % 1 4 % 7 %\n3 3 % 3 0 % 1 9 % 1 1 % 6 %\n(b)\nFigure 9: (a) Breakdown of total amortized model development costs for selected models, with equity excluded from\nthe R&D staff cost.", "metadata": {}}, {"text": "Hardware costs are amortized to the total number of chip-hours spent on experiments and training,\nwhile R&D staff costs cover the duration of development from initial experiments to publication.", "metadata": {}}, {"text": "Error bars indicate\n90% credible intervals, while the main bar values are medians.", "metadata": {}}, {"text": "(b) Costs components as a percentage of the total, based\non median estimates.", "metadata": {}}, {"text": "Figure 10: The trend in AI compute cluster power (in kilowatts) required to train frontier models over time.", "metadata": {}}, {"text": "Power is\ncalculated as the product of the number of servers, server TDP, and power usage effectiveness.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}]}