{"document_id": "schwartz2019", "title": "Green AI", "text": "Green AI\nRoy Schwartz∗♦ Jesse Dodge∗♦♣ Noah A. Smith♦♥ Oren Etzioni♦\n♦Allen Institute for AI, Seattle, Washington, USA\n♣ Carnegie Mellon University, Pittsburgh, Pennsylvania, USA\n♥ University of Washington, Seattle, Washington, USA\nJuly 2019\nAbstract\nThe computations required for deep learning research have been doubling every few months, resulting in an\nestimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint\n[40]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efﬁcient. Moreover, the\nﬁnancial cost of the computations can make it difﬁcult for academics, students, and researchers, in particular those\nfrom emerging economies, to engage in deep learning research.\nThis position paper advocates a practical solution by makingefﬁciency an evaluation criterion for research along-\nside accuracy and related measures. In addition, we propose reporting the ﬁnancial cost or “price tag” of developing,\ntraining, and running models to provide baselines for the investigation of increasingly efﬁcient methods. Our goal is\nto make AI both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality\nresearch papers. Green AI is an emerging focus at the Allen Institute for AI.\n1 Introduction and Motivation\nSince 2012, the ﬁeld of artiﬁcial intelligence has reported remarkable progress on a broad range of capabilities in-\ncluding object recognition, game playing, machine translation, and more [36]. This progress has been achieved by\nincreasingly large and computationally-intensive deep learning models. 1 Figure 1 reproduced from [2] plots training\ncost increase over time for state-of-the-art deep learning models starting with AlexNet in 2012 [20] to AlphaZero in\n2017 [38]. The chart shows an overall increase of 300,000x, with training cost doubling every few months. An even\nsharper trend can be observed in NLP word embedding approaches by looking at ELMo [29] followed by BERT [8],\nopenGPT-2 [30], and XLNet [48]. An important paper [40] has estimated the carbon footprint of several NLP models\nand argued that this trend is both environmentally unfriendly (which we refer to as Red AI) and expensive, raising\nbarriers to participation in NLP research.\nThis trend is driven by the strong focus of the AI community on obtaining “state-of-the-art” results,2 as exempliﬁed\nby the rising popularity of leaderboards [46, 45], which typically report accuracy measures but omit any mention of\ncost or efﬁciency (see, for example, leaderboards.allenai.org). Despite the clear beneﬁts of improving\nmodel accuracy in AI, the focus on this single metric ignores the economic, environmental, or social cost of reaching\nthe reported accuracy.\nWe advocate increasing research activity in Green AI—AI research that is more environmentally friendly and\ninclusive. We emphasize that Red AI research has been yielding valuable contributions to the ﬁeld of AI, but it’s been\noverly dominant. We want to shift the balance towards the Green AIoption—to ensure that any inspired undergraduate\nwith a laptop has the opportunity to write high-quality papers that could be accepted at premier research conferences.\n∗The ﬁrst two authors contributed equally. The research was done at the Allen Institute for AI.\n1For brevity, we refer to AI throughout this paper, but our focus is on AI research that relies on deep learning methods.\n2Meaning, in practice, that a system’s accuracy on some benchmark is greater than any previously reported system’s accuracy.\n1\narXiv:1907.10597v3  [cs.CY]  13 Aug 2019\n\nFigure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years. Figure taken\nfrom [2].\nSpeciﬁcally, we propose makingefﬁciency a more common evaluation criterion for AI papers alongside accuracy and\nrelated measures.\nAI research can be computationally expensive in a number of ways, but each provides opportunities for efﬁcient\nimprovements; for example, papers could be required to plot accuracy as a function of computational cost and of\ntraining set size, providing a baseline for more data-efﬁcient research in the future. Reporting the computational price\ntag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing\ntransparency, price tags are baselines that other researchers could improve on.\nOur empirical analysis in Figure 2 suggests that the AI research community has paid relatively little attention to\ncomputational efﬁciency. In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially,\nat a pace that far exceeds Moore’s Law [28]. Red AI is on the rise despite the well-known diminishing returns of\nincreased cost (e.g., Figure 3). This paper identiﬁes key factors that contribute to Red AI and advocates the introduction\nof a simple, easy-to-compute efﬁciency metric that could help make some AI research greener, more inclusive, and\nperhaps more cognitively plausible. Green AI is part of a broader, long-standing interest in environmentally-friendly\nscientiﬁc research (e.g., see the journal Green Chemistry). Computer science, in particular, has a long history of\ninvestigating sustainable and energy-efﬁcient computing (e.g., see the journal Sustainable Computing: Informatics\nand Systems).\nThe remainder of this paper is organized as follows. Section 2 analyzes practices that move deep-learning research\ninto the realm of Red AI. Section 3 discusses our proposals for Green AI. Section 4 considers related work, and we\nconclude with a discussion of directions for future research.\n2 Red AI\nRed AI refers to AI research that seeks to obtain state-of-the-art results in accuracy (or related measures) through\nthe use of massive computational power—essentially “buying” stronger results. Yet the relationship between model\nperformance and model complexity (measured as number of parameters or inference time) has long been understood\nto be at best logarithmic; for a linear gain in performance, an exponentially larger model is required [18]. Similar\ntrends exist with increasing the quantity of training data [41, 13] and the number of experiments [9]. In each of these\ncases, diminishing returns come at increased computational cost.\nThis section analyzes the factors contributing to Red AI and shows how it is resulting in diminishing returns over\ntime (see Figure 3). We note again that Red AI work is valuable, and in fact, much of it contributes to what we know\n2\n\n[Image page=2 idx=1 name=Im1.png] Size: 2400x1880, Data: 143862 bytes\n\nFigure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that\ntarget accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.\nby pushing the boundaries of AI. Our exposition here is meant to highlight areas where computational expense is high,\nand to present each as an opportunity for developing more efﬁcient techniques.\nTo demonstrate the prevalence of Red AI, we sampled 60 papers from top AI conferences (ACL, 3 NeurIPS,4 and\nCVPR5). For each paper we noted whether the authors claim their main contribution to be (a) an improvement to\naccuracy or some related measure, (b) an improvement to efﬁciency, (c) both, or (d) other. As shown in Figure 2, in all\nconferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers\nand 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10%\nand 20% respectively) argue for a new efﬁciency result.6 This highlights the focus of the AI community on measures\nof performance such as accuracy, at the expense of measures of efﬁciency such as speed or model size. In this paper\nwe argue that a larger weight should be given to the latter.\nTo better understand the different ways in which AI research can be red, consider an AI result reported in a scientiﬁc\npaper. This result typically includes a model trained on a training dataset and evaluated on a test dataset. The process\nof developing that model often involves multiple experiments to tune its hyperparameters. When considering the\ndifferent factors that increase the computational and environmental cost of producing such a result, three factors come\nto mind: the cost of executing the model on a single ( E)xample (either during training or at inference time); the size\nof the training (D)ataset, which controls the number of times the model is executed during training, and the number of\n(H)yperparameter experiments, which controls how many times the model is trained during model development. The\ntotal cost of producing a ( R)esult in machine learning increases linearly with each of these quantities. This cost can\nbe estimated as follows:\nCost(R)∝ E· D· H\nEquation 1: The equation of Red AI: The cost of an AI ( R)esult grows linearly with the cost of processing a single\n(E)xample, the size of the training (D)ataset and the number of (H)yperparameter experiments.\nEquation 1 is a simpliﬁcation (e.g., different hyperparameter assignments can lead to different costs for processing\na single example). It also ignores other factors such as the number of training epochs. Nonetheless, it illustrates three\n3https://acl2018.org\n4https://nips.cc/Conferences/2018\n5http://cvpr2019.thecvf.com\n6Interestingly, many NeurIPS papers included convergence rates or regret bounds which describe performance as a function of examples or\niterations, thus targeting efﬁciency (55%). This indicates an increased awareness of the importance of this concept, at least in theoretical analyses.\n3\n\n[Image page=3 idx=1 name=Im2.png] Size: 3200x2400, Data: 164521 bytes\n\nquantities that are each an important factor in the total cost of generating a result. Below, we consider each quantity\nseparately.\nExpensive processing of one example Our focus is on neural models, where it is common for each training step\nto require inference, so we discuss training and inference cost together as “processing” an example. Some works\nhave used increasingly expensive models which require great amounts of resources, and as a result, in these models,\nperforming inference can require a lot of computation, and training even more so. For instance, Google’s BERT-large\n[8] contains roughly 350 million parameters. openAI’s openGPT2-XL model [30] contains 1.5 billion parameters.\nAI2, our home organization, recently released Grover [49], also containing 1.5 billion parameters. In the computer\nvision community, a similar trend is observed (Figure 1).\nSuch large models have high costs for processing each example, which leads to large training costs. BERT-large\nwas trained on 64 TPU chips for 4 days. Grover was trained on 256 TPU chips for two weeks, at an estimated cost of\n$25,000. XLNet had a similar architecture to BERT-large, but used a more expensive objective function (in addition\nto an order of magnitude more data), and was trained on 512 TPU chips for 2.5 days. 7 It is impossible to reproduce\nthe best BERT-large results8 or XLNet results9 using a single GPU. Specialized models can have even more extreme\ncosts, such as AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go\n[37] at a cost of over $1,000 per hour.10\nWhen examining variants of a single model (e.g., BERT-small and BERT-large) we see that larger models can have\nstronger performance, which is a valuable scientiﬁc contribution. However, this implies the ﬁnancial and environmen-\ntal cost of increasingly large AI models will not decrease soon, as the pace of model growth far exceeds the resulting\nincrease in model performance [16]. As a result, more and more resources are going to be required to keep improving\nAI models by simply making them larger.\nProcessing many examples Another way state-of-the-art performance has recently been progressing in AI is by\nsuccessively increasing the amount of training data models are trained on. BERT-large had top performance in 2018\nacross many NLP tasks after training on 3 billion word-pieces. XLNet recently outperformed BERT after training\non 32 billion word-pieces, including part of Common Crawl; openGPT-2-XL trained on 40 billion words; FAIR’s\nRoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours\nto train. In computer vision, researchers from Facebook [25] pretrained an image classiﬁcation model on 3.5 billion\nimages from Instagram, three orders of magnitude larger than existing labelled image datasets such as Open Images.11\nThe use of massive data creates barriers for many researchers for reproducing the results of these models, or\ntraining their own models on the same setup (especially as training for multiple epochs is standard). For example, the\nJune 2019 Common Crawl contains 242 TB of uncompressed data, 12 so even storing the data is expensive. Finally,\nas in the case of model size, relying on more data to improve performance is notoriously expensive because of the\ndiminishing return of adding more data [41]. For instance, Figure 3, taken from [25], shows a logarithmic relation\nbetween the object recognition top-1 accuracy and the number of training examples.\nMassive number of experiments Some projects have poured large amounts of computation into tuning hyperpa-\nrameters or searching over neural architectures, well beyond the reach of most researchers. For instance, researchers\nfrom Google [51] trained over 12,800 neural networks in their neural architecture search to improve performance on\nobject detection and language modeling. With a ﬁxed architecture, researchers from DeepMind [26] evaluated 1,500\nhyperparameter assignments to demonstrate that an LSTM language model [15] can reach state-of-the-art perplexity\nresults. Despite the value of this result in showing that the performance of an LSTM does not plateau after only a few\nhyperparameter trials, fully exploring the potential of other competitive models for a fair comparison is prohibitively\nexpensive.\n7Some estimates for the cost of this process reach $250,000 (twitter.com/eturner303/status/1143174828804857856).\n8See https://github.com/google-research/bert\n9See https://github.com/zihangdai/xlnet\n10Recent versions of AlphaGo are far more efﬁcient [39].\n11https://opensource.google.com/projects/open-images-dataset\n12http://commoncrawl.org/2019/07/\n4\n\nFigure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of\ntraining examples increases exponentially [25].\nThe topic of massive number of experiments is not as well studied as the ﬁrst two discussed above. In fact, the\nnumber of experiments performed during model construction is often underreported. Nonetheless, evidence for a\nlogarithmic relation exists here as well, between the number of experiments and performance gains [9].\nDiscussion The beneﬁts of pouring more resources into models are certainly of interest to the AI community. Indeed,\nthere is value in pushing the limits of model size, dataset size, and the hyperparameter search space. Currently, despite\nthe massive amount of resources put into recent AI models, such investment still pays off in terms of downstream\nperformance (albeit at an increasingly lower rate). Finding the point of saturation (if such exists) is an important\nquestion for the future of AI.\nOur goal in this paper is to raise awareness of the cost of Red AI, as well as encourage the AI community to\nrecognize the value of work by researchers that take a different path, optimizing efﬁciency rather than accuracy. Next\nwe turn to discuss concrete measures for making AI more green.\n3 Green AI\nThe term Green AI refers to AI research that yields novel results without increasing computational cost, and ideally\nreducing it. Whereas Red AI has resulted in rapidly escalating computational (and thus carbon) costs, Green AI has the\nopposite effect. If measures of efﬁciency are widely accepted as important evaluation metrics for research alongside\naccuracy, then researchers will have the option of focusing on the efﬁciency of their models with positive impact on\nboth the environment and inclusiveness. This section reviews several measures of efﬁciency that could be reported\nand optimized, and advocates one particular measure—FPO—which we argue should be reported when AI research\nﬁndings are published.\n3.1 Measures of Efﬁciency\nTo measure efﬁciency, we suggest reporting the amount of work required to generate a result in AI, that is, the amount\nof work required to train a model, and if applicable, the sum of works for all hyperparameter tuning experiments. As\n5\n\n[Image page=5 idx=1 name=Im3.png] Size: 1086x935, Data: 347062 bytes\n\nthe cost of an experiment decomposes into the cost of a processing a single example, the size of the dataset, and the\nnumber of experiments (Equation 1), reducing the amount of work in each of these steps will result in AI that is more\ngreen.\nWhen reporting the amount of work done by a model, we want to measure a quantity that allows for a fair com-\nparison between different models. As a result, this measure should ideally be stable across different labs, at different\ntimes, and using different hardware.\nCarbon emission Carbon emission is appealing as it is a quantity we want to directly minimize. Nonetheless it\nis impractical to measure the exact amount of carbon released by training or executing a model, and accordingly—\ngenerating an AI result, as this amount depends highly on the local electricity infrastructure. As a result, it is not\ncomparable between researchers in different locations or even the same location at different times.\nElectricity usage Electricity usage is correlated with carbon emission while being time- and location-agnostic.\nMoreover, GPUs often report the amount of electricity each of their cores consume at each time point, which facilitates\nthe estimation of the total amount of electricity consumed by generating an AI result. Nonetheless, this measure is\nhardware dependent, and as a result does not allow for a fair comparison between different models.\nElapsed real time The total running time for generating an AI result is a natural measure for efﬁciency, as all other\nthings being equal, a faster model is doing less computational work. Nonetheless, this measure is highly inﬂuenced\nby factors such as the underlying hardware, other jobs running on the same machine, and the number of cores used.\nThese factors hinder the comparison between different models, as well as the decoupling of modeling contributions\nfrom hardware improvements.\nNumber of parameters Another common measure of efﬁciency is the number of parameters (learnable or total)\nused by the model. As with run time, this measure is correlated with the amount of work. Unlike the other measures\ndescribed above, it does not depend on the underlying hardware. Moreover, this measure also highly correlates with the\namount of memory consumed by the model. Nonetheless, different algorithms make different use of their parameters,\nfor instance by making the model deeper vs. wider. As a result, different models with a similar number of parameters\noften perform different amounts of work.\nFPO As a concrete measure, we suggest reporting the total number of ﬂoating point operations (FPO) required to\ngenerate a result. 13 FPO provides an estimate to the amount of work performed by a computational process. It is\ncomputed analytically by deﬁning a cost to two base operations, ADD and MUL. Based on these operations, the FPO\ncost of any machine learning abstract operation (e.g., a tanh operation, a matrix multiplication, a convolution operation,\nor the BERT model) can be computed as a recursive function of these two operations. FPO has been used in the past\nto quantify the energy footprint of a model [27, 43, 12, 42], but is not widely adopted in AI.\nFPO has several appealing properties. First, it directly computes the amount of work done by the running machine\nwhen executing a speciﬁc instance of a model, and is thus tied to the amount of energy consumed. Second, FPO is\nagnostic to the hardware on which the model is run. This facilitates fair comparisons between different approaches,\nunlike the measures described above. Third, FPO is strongly correlated with the running time of the model [4]. Unlike\nasymptotic runtime, FPO also considers the amount of work done at each time step.\nSeveral packages exist for computing FPO in various neural network libraries,14 though none of them contains all\nthe building blocks required to construct all modern AI models. We encourage the builders of neural network libraries\nto implement such functionality directly.\n13Floating point operations are often referred to as FLOP(s), though this term is not uniquely deﬁned [12]. To avoid confusion, we use the term\nFPO.\n14E.g., https://github.com/Swall0w/torchstat ; https://github.com/Lyken17/pytorch-OpCounter\n6\n\n(a) Different models.\n (b) Different layers of the ResNet model.\nFigure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top):\nmodel parameters (in million), FPO (in billions), top-1 accuracy on ImageNet. (4a): Different models: AlexNet\n[20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17]. (4b): Comparison of different sizes (measured by the\nnumber of layers) of the ResNet model [14].\nDiscussion Efﬁcient machine learning approaches have received attention in the research community, but are gener-\nally not motivated by being green. For example, a signiﬁcant amount of work in the computer vision community has\naddressed efﬁcient inference, which is necessary for real-time processing of images for applications like self-driving\ncars [24, 31, 22], or for placing models on devices such as mobile phones [16, 34]. Most of these approaches target ef-\nﬁcient model inference [32, 50, 12],15 and thus only minimize the cost of processing a single example, while ignoring\nthe other two red practices discussed in Section 2.16\nThe above examples indicate that the path to making AI green depends on how it is used. When developing a new\nmodel, much of the research process involves training many model variants on a training set and performing inference\non a small development set. In such a setting, more efﬁcient training procedures can lead to greater savings, while in\na production setting more efﬁcient inference can be more important. We advocate for a holistic view of computational\nsavings which doesn’t sacriﬁce in some areas to make advances in others.\nFPO has some limitations. First, it targets the electricity consumption of a model, while ignoring other potential\nlimiting factors for researchers such as the memory consumption by the model, which can often lead to additional\nenergy and monetary costs [24]. Second, the amount of work done by a model largely depends on the model imple-\nmentation, as two different implementations of the same model could result in very different amounts of processing\nwork. Due to the focus on the modeling contribution, the AI community has traditionally ignored the quality or ef-\nﬁciency of models’ implementation.17 We argue that the time to reverse this norm has come, and that exceptionally\ngood implementations that lead to efﬁcient models should be credited by the AI community.\n3.2 FPO Cost of Existing Models\nTo demonstrate the importance of reporting the amount of work, we present FPO costs for several existing models. 18\nFigure 4a shows the number of parameters and FPO of several leading object recognition models, as well as their\n15Some very recent work also targeted efﬁcient training [7].\n16In fact, creating smaller models often results in longer running time, so mitigating the different trends might be at odds [44].\n17We consider this exclusive focus on the ﬁnal prediction another symptom of Red AI.\n18These numbers represent FPO per inference, i.e., the work required to process a single example.\n7\n\n[Image page=7 idx=1 name=Im4.png] Size: 4000x3000, Data: 303469 bytes\n\n[Image page=7 idx=2 name=Im5.png] Size: 4000x3000, Data: 278612 bytes\n\nperformance on the ImageNet dataset [6]. 19 A few trends are observable. First, as discussed in Section 2, models\nget more expensive with time, but the increase in FPO does not lead to similar performance gains. For instance, an\nincrease of almost 35% in FPO between ResNet and ResNext (second and third points in graph) resulted in a 0.5%\ntop-1 accuracy improvement. Similar patterns are observed when considering the effect of other increases in model\nwork. Second, the number of model parameters does not tell the whole story: AlexNet (ﬁrst point in the graph) actually\nhas more parameters than ResNet (second point), but dramatically less FPO, and also much lower accuracy.\nFigure 4b shows the same analysis for a single object recognition model, ResNet [14], while comparing different\nversions of the model with different number of layers. This creates a controlled comparison between the different\nmodels, as they are identical in architecture, except for their size (and accordingly, their FPO cost). Once again, we\nnotice the same trend: the large increase in FPO cost does not translate to a large increase in performance.\n3.3 Additional Ways to Promote Green AI\nIn addition to reporting the FPO cost of the ﬁnal reported number, we encourage researchers to report the bud-\nget/accuracy curve observed during training. In a recent paper [9], we observed that selecting the better performing\nmodel on a given task depends highly on the amount of compute available during model development. We introduced\na method for computing the expected best validation performance of a model as a function of the given budget. We\nargue that reporting this curve will allow users to make wiser decisions about their selection of models and highlight\nthe stability of different approaches.\nWe further advocate for making efﬁciency an ofﬁcial contribution in major AI conferences, by advising reviewers\nto recognize and value contributions that do not strictly improve state of the art, but have other beneﬁts such as\nefﬁciency. Finally, we note that the trend of releasing pretrained models publicly is a green success, and we would like\nto encourage organizations to continue to release their models in order to save others the costs of retraining them.\n4 Related Work\nRecent work has analyzed the carbon emissions of training deep NLP models [40] and concluded that computationally\nexpensive experiments can have a large environmental and economic impact. With modern experiments using such\nlarge budgets, many researchers (especially those in academia) lack the resources to work in many high-proﬁle areas;\nincreased value placed on computationally efﬁcient approaches will allow research contributions from more diverse\ngroups. We emphasize that the conclusions of [40] are the result of long-term trends, and are not isolated within NLP,\nbut hold true across machine learning.\nWhile some companies offset electricity usage by purchasing carbon credits, it is not clear that buying credits is\nas effective as using less energy. In addition, purchasing carbon credits is voluntary; Google cloud 20 and Microsoft\nAzure21 purchase carbon credits to offset their spent energy, but Amazon’s AWS22 (the largest cloud computing plat-\nform23) only covered ﬁfty percent of its power usage with renewable energy.\nThe push to improve state-of-the-art performance has focused the research community’s attention on reporting the\nsingle best result after running many experiments for model development and hyperparameter tuning. Failure to fully\nreport these experiments prevents future researchers from understanding how much effort is required to reproduce a\nresult or extend it [9].\nOur focus is on improving efﬁciency in the machine learning community, but machine learning can also be used\nas a tool for work in areas like climate change. For example, machine learning has been used for reducing emissions\nof cement plants [1] and tracking animal conservation outcomes [11], and is predicted to be useful for forest ﬁre\nmanagement [33]. Undoubtedly these are important applications of machine learning; we recognize that they are\northogonal to the content of this paper.\n19Numbers taken from https://github.com/sovrasov/flops-counter.pytorch\n20https://cloud.google.com/sustainability/\n21https://www.microsoft.com/en-us/environment/carbon\n22https://aws.amazon.com/about-aws/sustainability/\n23https://tinyurl.com/y2kob969\n8\n\n5 Conclusion\nThe vision of Green AI raises many exciting research directions that help to overcome the inclusiveness challenges of\nRed AI. Progress will reduce the computational expense with a minimal reduction in performance, or even improve\nperformance as more efﬁcient methods are discovered. Also, it would seem that Green AI could be moving us in a\nmore cognitively plausible direction as the brain is highly efﬁcient.\nIt’s important to reiterate that we see Green AI as a valuable option not an exclusive mandate—of course, both\nGreen AI and Red AI have contributions to make. We want to increase the prevalence of Green AI by highlighting its\nbeneﬁts, advocating a standard measure of efﬁciency. Below, we point to a few important green research directions,\nand highlight a few open questions.\nResearch on building space or time efﬁcient models is often motivated by ﬁtting a model on a small device (such\nas a phone) or fast enough to process examples in real time, such as image captioning for the blind (see Section 3.1).\nSome modern models don’t even ﬁt on a single GPU (see Section 2). Here we argue for a far broader approach.\nData efﬁciency has received signiﬁcant attention over the years [35, 19]. Modern research in vision and NLP often\ninvolves ﬁrst pretraining a model on large “raw” (unannotated) data then ﬁne-tuning it to a task of interest through\nsupervised learning. A strong result in this area often involves achieving similar performance to a baseline with\nfewer training examples or fewer gradient steps. Most recent work has addressed ﬁne-tuning data [29], but pretraining\nefﬁciency is also important. In either case, one simple technique to improve in this area is to simply report performance\nwith different amounts of training data. For example, reporting performance of contextual embedding models trained\non 10 million, 100 million, 1 billion, and 10 billion tokens would facilitate faster development of new models, as they\ncan ﬁrst be compared at the smallest data sizes. Research here is of value not just to make training less expensive, but\nbecause in areas such as low resource languages or historical domains it is extremely hard to generate more data, so to\nprogress we must make more efﬁcient use of what is available.\nFinally, the total number of experiments run to get a ﬁnal result is often underreported and underdiscussed [9]. The\nfew instances researchers have of full reporting of the hyperparameter search, architecture evaluations, and ablations\nthat went into a reported experimental result have surprised the community [40]. While many hyperparameter opti-\nmization algorithms exist which can reduce the computational expense required to reach a given level of performance\n[3, 10], simple improvements here can have a large impact. For example, stopping training early for models which are\nclearly underperforming can lead to great savings [21].\nReferences\n[1] Prabal Acharyya, Sean D Rosario, Roey Flor, Ritvik Joshi, Dian Li, Roberto Linares, and Hongbao Zhang.\nAutopilot of cement plants for reduction of fuel consumption and emissions, 2019. ICML Workshop on Climate\nChange.\n[2] Dario Amodei and Danny Hernandez. AI and compute, 2018. Blog post.\n[3] James S. Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter optimiza-\ntion. In Proc. of NeurIPS, 2011.\n[4] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for\npractical applications. In Proc. of ISCAS, 2017.\n[5] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks. In\nProc. of NeurIPS, 2017.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In Proc. of CVPR, 2009.\n[7] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance,\n2019. arXiv:1907.04840.\n9\n\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proc. of NAACL, 2019.\n[9] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your work: Improved\nreporting of experimental results. In Proc. of EMNLP, 2019.\n[10] Jesse Dodge, Kevin Jamieson, and Noah A. Smith. Open loop hyperparameter optimization and determinantal\npoint processes. In Proc. of AutoML, 2017.\n[11] Clement Duhart, Gershon Dublon, Brian Mayton, Glorianna Davenport, and Joseph A. Paradiso. Deep learning\nfor wildlife conservation and restoration efforts, 2019. ICML Workshop on Climate Change.\n[12] Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast &\nsimple resource-constrained structure learning of deep networks. In Proc. of CVPR, 2018.\n[13] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effectiveness of data. IEEE Intelligent\nSystems, 24:8–12, 2009.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProc. of CVPR, 2016.\n[15] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient convolutional neural networks for mobile vision applications,\n2017. arXiv:1704.04861.\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proc. of CVPR, 2018.\n[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbig-\nniew Wojna, Yang Song, Sergio Guadarrama, and Kevin Murphy. Speed/accuracy trade-offs for modern convo-\nlutional object detectors. In Proc. of CVPR, 2017.\n[19] Sanket Kamthe and Marc Peter Deisenroth. Data-efﬁcient reinforcement learning with probabilistic model pre-\ndictive control. In Proc. of AISTATS, 2018.\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Proc. of NeurIPS, 2012.\n[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-\nbased conﬁguration evaluation for hyperparameter optimization. InProc. of ICLR, 2017.\n[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.\nBerg. Ssd: Single shot multibox detector. In Proc. of ECCV, 2016.\n[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach, 2019.\narXiv:1907.11692.\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShufﬂeNet V2: Practical guidelines for efﬁcient\ncnn architecture design. In Proc. of ECCV, 2018.\n[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proc. ECCV,\n2018.\n10\n\n[26] G ´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In\nProc. of EMNLP, 2018.\n[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks\nfor resource efﬁcient inference. In Proc. of ICLR, 2017.\n[28] Gordon E. Moore. Cramming more components onto integrated circuits, 1965.\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.\n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners, 2019. OpenAI Blog.\n[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation\nusing binary convolutional neural networks. In Proc. of ECCV, 2016.\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object\ndetection. In Proc. of CVPR, 2016.\n[33] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, An-\ndrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan\nMaharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. K¨ording, Carla Gomes, Andrew Y . Ng, Demis Has-\nsabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and Yoshua Bengio. Tackling climate change with machine\nlearning, 2019. arXiv:1905.12616.\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2:\nInverted residuals and linear bottlenecks. In Proc. of CVPR, 2018.\n[35] Roy Schwartz, Sam Thomson, and Noah A. Smith. SoPa: Bridging CNNs, RNNs, and weighted ﬁnite-state\nmachines. In Proc. of ACL, 2018.\n[36] Yoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, James Manyika, Juan Carlos Niebles, Terah\nLyons, John Etchemendy, and Z Bauer. The AI index 2018 annual report. AI Index Steering Committee,\nHuman-Centered AI Initiative, Stanford University. Available at http://cdn.aiindex.org/2018/AI%\n20Index%202018%20Annual%20Report.pdf, 202018, 2018.\n[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore\nGraepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature,\n529(7587):484, 2016.\n[38] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis\nHassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.\narXiv:1712.01815.\n[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human\nknowledge. Nature, 550(7676):354, 2017.\n[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nNLP. In Proc. of ACL, 2019.\n11\n\n[41] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of\ndata in deep learning era. In Proc. of ICCV, 2017.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n[43] Tom Veniat and Ludovic Denoyer. Learning time/memory-efﬁcient deep architectures with budgeted super net-\nworks. In Proc. of CVPR, 2018.\n[44] Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, and Dieter Fox. Early\nfusion for goal directed robotic vision. In Proc. of IROS, 2019.\n[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems,\n2019. arXiv:1905.00537.\n[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR, 2019.\n[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations\nfor deep neural networks. In Proc. of CVPR, 2017.\n[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet:\nGeneralized autoregressive pretraining for language understanding, 2019. arXiv:1906.08237.\n[49] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.\nDefending against neural fake news, 2019. arXiv:1905.12616.\n[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShufﬂeNet: An extremely efﬁcient convolutional\nneural network for mobile devices. In Proc. of CVPR, 2018.\n[51] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In Proc. of ICLR, 2017.\n12", "metadata": {"url": "https://arxiv.org/pdf/1907.10597", "type": "paper", "year": "2019"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Green AI\nRoy Schwartz∗♦ Jesse Dodge∗♦♣ Noah A. Smith♦♥ Oren Etzioni♦\n♦Allen Institute for AI, Seattle, Washington, USA\n♣ Carnegie Mellon University, Pittsburgh, Pennsylvania, USA\n♥ University of Washington, Seattle, Washington, USA\nJuly 2019\nAbstract\nThe computations required for deep learning research have been doubling every few months, resulting in an\nestimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint\n[40]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efﬁcient. Moreover, the\nﬁnancial cost of the computations can make it difﬁcult for academics, students, and researchers, in particular those\nfrom emerging economies, to engage in deep learning research.\nThis position paper advocates a practical solution by makingefﬁciency an evaluation criterion for research along-\nside accuracy and related measures. In addition, we propose reporting the ﬁnancial cost or “price tag” of developing,\ntraining, and running models to provide baselines for the investigation of increasingly efﬁcient methods. Our goal is\nto make AI both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality\nresearch papers. Green AI is an emerging focus at the Allen Institute for AI.\n1 Introduction and Motivation\nSince 2012, the ﬁeld of artiﬁcial intelligence has reported remarkable progress on a broad range of capabilities in-\ncluding object recognition, game playing, machine translation, and more [36]. This progress has been achieved by\nincreasingly large and computationally-intensive deep learning models. 1 Figure 1 reproduced from [2] plots training\ncost increase over time for state-of-the-art deep learning models starting with AlexNet in 2012 [20] to AlphaZero in\n2017 [38]. The chart shows an overall increase of 300,000x, with training cost doubling every few months. An even\nsharper trend can be observed in NLP word embedding approaches by looking at ELMo [29] followed by BERT [8],\nopenGPT-2 [30], and XLNet [48]. An important paper [40] has estimated the carbon footprint of several NLP models\nand argued that this trend is both environmentally unfriendly (which we refer to as Red AI) and expensive, raising\nbarriers to participation in NLP research.\nThis trend is driven by the strong focus of the AI community on obtaining “state-of-the-art” results,2 as exempliﬁed\nby the rising popularity of leaderboards [46, 45], which typically report accuracy measures but omit any mention of\ncost or efﬁciency (see, for example, leaderboards.allenai.org). Despite the clear beneﬁts of improving\nmodel accuracy in AI, the focus on this single metric ignores the economic, environmental, or social cost of reaching\nthe reported accuracy.\nWe advocate increasing research activity in Green AI—AI research that is more environmentally friendly and\ninclusive. We emphasize that Red AI research has been yielding valuable contributions to the ﬁeld of AI, but it’s been\noverly dominant. We want to shift the balance towards the Green AIoption—to ensure that any inspired undergraduate\nwith a laptop has the opportunity to write high-quality papers that could be accepted at premier research conferences.\n∗The ﬁrst two authors contributed equally. The research was done at the Allen Institute for AI.\n1For brevity, we refer to AI throughout this paper, but our focus is on AI research that relies on deep learning methods.\n2Meaning, in practice, that a system’s accuracy on some benchmark is greater than any previously reported system’s accuracy.\n1\narXiv:1907.10597v3  [cs.CY]  13 Aug 2019", "sentences": [{"text": "Green AI\nRoy Schwartz∗♦ Jesse Dodge∗♦♣ Noah A.", "metadata": {}}, {"text": "Smith♦♥ Oren Etzioni♦\n♦Allen Institute for AI, Seattle, Washington, USA\n♣ Carnegie Mellon University, Pittsburgh, Pennsylvania, USA\n♥ University of Washington, Seattle, Washington, USA\nJuly 2019\nAbstract\nThe computations required for deep learning research have been doubling every few months, resulting in an\nestimated 300,000x increase from 2012 to 2018 [2].", "metadata": {}}, {"text": "These computations have a surprisingly large carbon footprint\n[40].", "metadata": {}}, {"text": "Ironically, deep learning was inspired by the human brain, which is remarkably energy efﬁcient.", "metadata": {}}, {"text": "Moreover, the\nﬁnancial cost of the computations can make it difﬁcult for academics, students, and researchers, in particular those\nfrom emerging economies, to engage in deep learning research.", "metadata": {}}, {"text": "This position paper advocates a practical solution by makingefﬁciency an evaluation criterion for research along-\nside accuracy and related measures.", "metadata": {}}, {"text": "In addition, we propose reporting the ﬁnancial cost or “price tag” of developing,\ntraining, and running models to provide baselines for the investigation of increasingly efﬁcient methods.", "metadata": {}}, {"text": "Our goal is\nto make AI both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality\nresearch papers.", "metadata": {}}, {"text": "Green AI is an emerging focus at the Allen Institute for AI.", "metadata": {}}, {"text": "1 Introduction and Motivation\nSince 2012, the ﬁeld of artiﬁcial intelligence has reported remarkable progress on a broad range of capabilities in-\ncluding object recognition, game playing, machine translation, and more [36].", "metadata": {}}, {"text": "This progress has been achieved by\nincreasingly large and computationally-intensive deep learning models.", "metadata": {}}, {"text": "1 Figure 1 reproduced from [2] plots training\ncost increase over time for state-of-the-art deep learning models starting with AlexNet in 2012 [20] to AlphaZero in\n2017 [38].", "metadata": {}}, {"text": "The chart shows an overall increase of 300,000x, with training cost doubling every few months.", "metadata": {}}, {"text": "An even\nsharper trend can be observed in NLP word embedding approaches by looking at ELMo [29] followed by BERT [8],\nopenGPT-2 [30], and XLNet [48].", "metadata": {}}, {"text": "An important paper [40] has estimated the carbon footprint of several NLP models\nand argued that this trend is both environmentally unfriendly (which we refer to as Red AI) and expensive, raising\nbarriers to participation in NLP research.", "metadata": {}}, {"text": "This trend is driven by the strong focus of the AI community on obtaining “state-of-the-art” results,2 as exempliﬁed\nby the rising popularity of leaderboards [46, 45], which typically report accuracy measures but omit any mention of\ncost or efﬁciency (see, for example, leaderboards.allenai.org).", "metadata": {}}, {"text": "Despite the clear beneﬁts of improving\nmodel accuracy in AI, the focus on this single metric ignores the economic, environmental, or social cost of reaching\nthe reported accuracy.", "metadata": {}}, {"text": "We advocate increasing research activity in Green AI—AI research that is more environmentally friendly and\ninclusive.", "metadata": {}}, {"text": "We emphasize that Red AI research has been yielding valuable contributions to the ﬁeld of AI, but it’s been\noverly dominant.", "metadata": {}}, {"text": "We want to shift the balance towards the Green AIoption—to ensure that any inspired undergraduate\nwith a laptop has the opportunity to write high-quality papers that could be accepted at premier research conferences.", "metadata": {}}, {"text": "∗The ﬁrst two authors contributed equally.", "metadata": {}}, {"text": "The research was done at the Allen Institute for AI.", "metadata": {}}, {"text": "1For brevity, we refer to AI throughout this paper, but our focus is on AI research that relies on deep learning methods.", "metadata": {}}, {"text": "2Meaning, in practice, that a system’s accuracy on some benchmark is greater than any previously reported system’s accuracy.", "metadata": {}}, {"text": "1\narXiv:1907.10597v3  [cs.CY]  13 Aug 2019", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years. Figure taken\nfrom [2].\nSpeciﬁcally, we propose makingefﬁciency a more common evaluation criterion for AI papers alongside accuracy and\nrelated measures.\nAI research can be computationally expensive in a number of ways, but each provides opportunities for efﬁcient\nimprovements; for example, papers could be required to plot accuracy as a function of computational cost and of\ntraining set size, providing a baseline for more data-efﬁcient research in the future. Reporting the computational price\ntag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing\ntransparency, price tags are baselines that other researchers could improve on.\nOur empirical analysis in Figure 2 suggests that the AI research community has paid relatively little attention to\ncomputational efﬁciency. In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially,\nat a pace that far exceeds Moore’s Law [28]. Red AI is on the rise despite the well-known diminishing returns of\nincreased cost (e.g., Figure 3). This paper identiﬁes key factors that contribute to Red AI and advocates the introduction\nof a simple, easy-to-compute efﬁciency metric that could help make some AI research greener, more inclusive, and\nperhaps more cognitively plausible. Green AI is part of a broader, long-standing interest in environmentally-friendly\nscientiﬁc research (e.g., see the journal Green Chemistry). Computer science, in particular, has a long history of\ninvestigating sustainable and energy-efﬁcient computing (e.g., see the journal Sustainable Computing: Informatics\nand Systems).\nThe remainder of this paper is organized as follows. Section 2 analyzes practices that move deep-learning research\ninto the realm of Red AI. Section 3 discusses our proposals for Green AI. Section 4 considers related work, and we\nconclude with a discussion of directions for future research.\n2 Red AI\nRed AI refers to AI research that seeks to obtain state-of-the-art results in accuracy (or related measures) through\nthe use of massive computational power—essentially “buying” stronger results. Yet the relationship between model\nperformance and model complexity (measured as number of parameters or inference time) has long been understood\nto be at best logarithmic; for a linear gain in performance, an exponentially larger model is required [18]. Similar\ntrends exist with increasing the quantity of training data [41, 13] and the number of experiments [9]. In each of these\ncases, diminishing returns come at increased computational cost.\nThis section analyzes the factors contributing to Red AI and shows how it is resulting in diminishing returns over\ntime (see Figure 3). We note again that Red AI work is valuable, and in fact, much of it contributes to what we know\n2", "sentences": [{"text": "Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.", "metadata": {}}, {"text": "Figure taken\nfrom [2].", "metadata": {}}, {"text": "Speciﬁcally, we propose makingefﬁciency a more common evaluation criterion for AI papers alongside accuracy and\nrelated measures.", "metadata": {}}, {"text": "AI research can be computationally expensive in a number of ways, but each provides opportunities for efﬁcient\nimprovements;", "metadata": {}}, {"text": "for example, papers could be required to plot accuracy as a function of computational cost and of\ntraining set size, providing a baseline for more data-efﬁcient research in the future.", "metadata": {}}, {"text": "Reporting the computational price\ntag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).", "metadata": {}}, {"text": "In addition to providing\ntransparency, price tags are baselines that other researchers could improve on.", "metadata": {}}, {"text": "Our empirical analysis in Figure 2 suggests that the AI research community has paid relatively little attention to\ncomputational efﬁciency.", "metadata": {}}, {"text": "In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially,\nat a pace that far exceeds Moore’s Law [28].", "metadata": {}}, {"text": "Red AI is on the rise despite the well-known diminishing returns of\nincreased cost (e.g., Figure 3).", "metadata": {}}, {"text": "This paper identiﬁes key factors that contribute to Red AI and advocates the introduction\nof a simple, easy-to-compute efﬁciency metric that could help make some AI research greener, more inclusive, and\nperhaps more cognitively plausible.", "metadata": {}}, {"text": "Green AI is part of a broader, long-standing interest in environmentally-friendly\nscientiﬁc research (e.g., see the journal Green Chemistry).", "metadata": {}}, {"text": "Computer science, in particular, has a long history of\ninvestigating sustainable and energy-efﬁcient computing (e.g., see the journal Sustainable Computing: Informatics\nand Systems).", "metadata": {}}, {"text": "The remainder of this paper is organized as follows.", "metadata": {}}, {"text": "Section 2 analyzes practices that move deep-learning research\ninto the realm of Red AI.", "metadata": {}}, {"text": "Section 3 discusses our proposals for Green AI.", "metadata": {}}, {"text": "Section 4 considers related work, and we\nconclude with a discussion of directions for future research.", "metadata": {}}, {"text": "2 Red AI\nRed AI refers to AI research that seeks to obtain state-of-the-art results in accuracy (or related measures) through\nthe use of massive computational power—essentially “buying” stronger results.", "metadata": {}}, {"text": "Yet the relationship between model\nperformance and model complexity (measured as number of parameters or inference time) has long been understood\nto be at best logarithmic;", "metadata": {}}, {"text": "for a linear gain in performance, an exponentially larger model is required [18].", "metadata": {}}, {"text": "Similar\ntrends exist with increasing the quantity of training data [41, 13] and the number of experiments [9].", "metadata": {}}, {"text": "In each of these\ncases, diminishing returns come at increased computational cost.", "metadata": {}}, {"text": "This section analyzes the factors contributing to Red AI and shows how it is resulting in diminishing returns over\ntime (see Figure 3).", "metadata": {}}, {"text": "We note again that Red AI work is valuable, and in fact, much of it contributes to what we know\n2", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.png] Size: 2400x1880, Data: 143862 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.png] Size: 2400x1880, Data: 143862 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.png", "image_width": 2400, "image_height": 1880, "attachment_type": "image", "has_image_data": true, "image_data_size": 143862}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "Figure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that\ntarget accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.\nby pushing the boundaries of AI. Our exposition here is meant to highlight areas where computational expense is high,\nand to present each as an opportunity for developing more efﬁcient techniques.\nTo demonstrate the prevalence of Red AI, we sampled 60 papers from top AI conferences (ACL, 3 NeurIPS,4 and\nCVPR5). For each paper we noted whether the authors claim their main contribution to be (a) an improvement to\naccuracy or some related measure, (b) an improvement to efﬁciency, (c) both, or (d) other. As shown in Figure 2, in all\nconferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers\nand 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10%\nand 20% respectively) argue for a new efﬁciency result.6 This highlights the focus of the AI community on measures\nof performance such as accuracy, at the expense of measures of efﬁciency such as speed or model size. In this paper\nwe argue that a larger weight should be given to the latter.\nTo better understand the different ways in which AI research can be red, consider an AI result reported in a scientiﬁc\npaper. This result typically includes a model trained on a training dataset and evaluated on a test dataset. The process\nof developing that model often involves multiple experiments to tune its hyperparameters. When considering the\ndifferent factors that increase the computational and environmental cost of producing such a result, three factors come\nto mind: the cost of executing the model on a single ( E)xample (either during training or at inference time); the size\nof the training (D)ataset, which controls the number of times the model is executed during training, and the number of\n(H)yperparameter experiments, which controls how many times the model is trained during model development. The\ntotal cost of producing a ( R)esult in machine learning increases linearly with each of these quantities. This cost can\nbe estimated as follows:\nCost(R)∝ E· D· H\nEquation 1: The equation of Red AI: The cost of an AI ( R)esult grows linearly with the cost of processing a single\n(E)xample, the size of the training (D)ataset and the number of (H)yperparameter experiments.\nEquation 1 is a simpliﬁcation (e.g., different hyperparameter assignments can lead to different costs for processing\na single example). It also ignores other factors such as the number of training epochs. Nonetheless, it illustrates three\n3https://acl2018.org\n4https://nips.cc/Conferences/2018\n5http://cvpr2019.thecvf.com\n6Interestingly, many NeurIPS papers included convergence rates or regret bounds which describe performance as a function of examples or\niterations, thus targeting efﬁciency (55%). This indicates an increased awareness of the importance of this concept, at least in theoretical analyses.\n3", "sentences": [{"text": "Figure 2: AI papers tend to target accuracy rather than efﬁciency.", "metadata": {}}, {"text": "The ﬁgure shows the proportion of papers that\ntarget accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.", "metadata": {}}, {"text": "by pushing the boundaries of AI.", "metadata": {}}, {"text": "Our exposition here is meant to highlight areas where computational expense is high,\nand to present each as an opportunity for developing more efﬁcient techniques.", "metadata": {}}, {"text": "To demonstrate the prevalence of Red AI, we sampled 60 papers from top AI conferences (ACL, 3 NeurIPS,4 and\nCVPR5).", "metadata": {}}, {"text": "For each paper we noted whether the authors claim their main contribution to be (a) an improvement to\naccuracy or some related measure, (b) an improvement to efﬁciency, (c) both, or (d) other.", "metadata": {}}, {"text": "As shown in Figure 2, in all\nconferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers\nand 75% of CVPR papers).", "metadata": {}}, {"text": "Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10%\nand 20% respectively) argue for a new efﬁciency result.6 This highlights the focus of the AI community on measures\nof performance such as accuracy, at the expense of measures of efﬁciency such as speed or model size.", "metadata": {}}, {"text": "In this paper\nwe argue that a larger weight should be given to the latter.", "metadata": {}}, {"text": "To better understand the different ways in which AI research can be red, consider an AI result reported in a scientiﬁc\npaper.", "metadata": {}}, {"text": "This result typically includes a model trained on a training dataset and evaluated on a test dataset.", "metadata": {}}, {"text": "The process\nof developing that model often involves multiple experiments to tune its hyperparameters.", "metadata": {}}, {"text": "When considering the\ndifferent factors that increase the computational and environmental cost of producing such a result, three factors come\nto mind: the cost of executing the model on a single ( E)xample (either during training or at inference time);", "metadata": {}}, {"text": "the size\nof the training (D)ataset, which controls the number of times the model is executed during training, and the number of\n(H)yperparameter experiments, which controls how many times the model is trained during model development.", "metadata": {}}, {"text": "The\ntotal cost of producing a ( R)esult in machine learning increases linearly with each of these quantities.", "metadata": {}}, {"text": "This cost can\nbe estimated as follows:\nCost(R)∝ E· D· H\nEquation 1: The equation of Red AI: The cost of an AI ( R)esult grows linearly with the cost of processing a single\n(E)xample, the size of the training (D)ataset and the number of (H)yperparameter experiments.", "metadata": {}}, {"text": "Equation 1 is a simpliﬁcation (e.g., different hyperparameter assignments can lead to different costs for processing\na single example).", "metadata": {}}, {"text": "It also ignores other factors such as the number of training epochs.", "metadata": {}}, {"text": "Nonetheless, it illustrates three\n3https://acl2018.org\n4https://nips.cc/Conferences/2018\n5http://cvpr2019.thecvf.com\n6Interestingly, many NeurIPS papers included convergence rates or regret bounds which describe performance as a function of examples or\niterations, thus targeting efﬁciency (55%).", "metadata": {}}, {"text": "This indicates an increased awareness of the importance of this concept, at least in theoretical analyses.", "metadata": {}}, {"text": "3", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=Im2.png] Size: 3200x2400, Data: 164521 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=Im2.png] Size: 3200x2400, Data: 164521 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "Im2.png", "image_width": 3200, "image_height": 2400, "attachment_type": "image", "has_image_data": true, "image_data_size": 164521}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "quantities that are each an important factor in the total cost of generating a result. Below, we consider each quantity\nseparately.\nExpensive processing of one example Our focus is on neural models, where it is common for each training step\nto require inference, so we discuss training and inference cost together as “processing” an example. Some works\nhave used increasingly expensive models which require great amounts of resources, and as a result, in these models,\nperforming inference can require a lot of computation, and training even more so. For instance, Google’s BERT-large\n[8] contains roughly 350 million parameters. openAI’s openGPT2-XL model [30] contains 1.5 billion parameters.\nAI2, our home organization, recently released Grover [49], also containing 1.5 billion parameters. In the computer\nvision community, a similar trend is observed (Figure 1).\nSuch large models have high costs for processing each example, which leads to large training costs. BERT-large\nwas trained on 64 TPU chips for 4 days. Grover was trained on 256 TPU chips for two weeks, at an estimated cost of\n$25,000. XLNet had a similar architecture to BERT-large, but used a more expensive objective function (in addition\nto an order of magnitude more data), and was trained on 512 TPU chips for 2.5 days. 7 It is impossible to reproduce\nthe best BERT-large results8 or XLNet results9 using a single GPU. Specialized models can have even more extreme\ncosts, such as AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go\n[37] at a cost of over $1,000 per hour.10\nWhen examining variants of a single model (e.g., BERT-small and BERT-large) we see that larger models can have\nstronger performance, which is a valuable scientiﬁc contribution. However, this implies the ﬁnancial and environmen-\ntal cost of increasingly large AI models will not decrease soon, as the pace of model growth far exceeds the resulting\nincrease in model performance [16]. As a result, more and more resources are going to be required to keep improving\nAI models by simply making them larger.\nProcessing many examples Another way state-of-the-art performance has recently been progressing in AI is by\nsuccessively increasing the amount of training data models are trained on. BERT-large had top performance in 2018\nacross many NLP tasks after training on 3 billion word-pieces. XLNet recently outperformed BERT after training\non 32 billion word-pieces, including part of Common Crawl; openGPT-2-XL trained on 40 billion words; FAIR’s\nRoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours\nto train. In computer vision, researchers from Facebook [25] pretrained an image classiﬁcation model on 3.5 billion\nimages from Instagram, three orders of magnitude larger than existing labelled image datasets such as Open Images.11\nThe use of massive data creates barriers for many researchers for reproducing the results of these models, or\ntraining their own models on the same setup (especially as training for multiple epochs is standard). For example, the\nJune 2019 Common Crawl contains 242 TB of uncompressed data, 12 so even storing the data is expensive. Finally,\nas in the case of model size, relying on more data to improve performance is notoriously expensive because of the\ndiminishing return of adding more data [41]. For instance, Figure 3, taken from [25], shows a logarithmic relation\nbetween the object recognition top-1 accuracy and the number of training examples.\nMassive number of experiments Some projects have poured large amounts of computation into tuning hyperpa-\nrameters or searching over neural architectures, well beyond the reach of most researchers. For instance, researchers\nfrom Google [51] trained over 12,800 neural networks in their neural architecture search to improve performance on\nobject detection and language modeling. With a ﬁxed architecture, researchers from DeepMind [26] evaluated 1,500\nhyperparameter assignments to demonstrate that an LSTM language model [15] can reach state-of-the-art perplexity\nresults. Despite the value of this result in showing that the performance of an LSTM does not plateau after only a few\nhyperparameter trials, fully exploring the potential of other competitive models for a fair comparison is prohibitively\nexpensive.\n7Some estimates for the cost of this process reach $250,000 (twitter.com/eturner303/status/1143174828804857856).\n8See https://github.com/google-research/bert\n9See https://github.com/zihangdai/xlnet\n10Recent versions of AlphaGo are far more efﬁcient [39].\n11https://opensource.google.com/projects/open-images-dataset\n12http://commoncrawl.org/2019/07/\n4", "sentences": [{"text": "quantities that are each an important factor in the total cost of generating a result.", "metadata": {}}, {"text": "Below, we consider each quantity\nseparately.", "metadata": {}}, {"text": "Expensive processing of one example Our focus is on neural models, where it is common for each training step\nto require inference, so we discuss training and inference cost together as “processing” an example.", "metadata": {}}, {"text": "Some works\nhave used increasingly expensive models which require great amounts of resources, and as a result, in these models,\nperforming inference can require a lot of computation, and training even more so.", "metadata": {}}, {"text": "For instance, Google’s BERT-large\n[8] contains roughly 350 million parameters.", "metadata": {}}, {"text": "openAI’s openGPT2-XL model [30] contains 1.5 billion parameters.", "metadata": {}}, {"text": "AI2, our home organization, recently released Grover [49], also containing 1.5 billion parameters.", "metadata": {}}, {"text": "In the computer\nvision community, a similar trend is observed (Figure 1).", "metadata": {}}, {"text": "Such large models have high costs for processing each example, which leads to large training costs.", "metadata": {}}, {"text": "BERT-large\nwas trained on 64 TPU chips for 4 days.", "metadata": {}}, {"text": "Grover was trained on 256 TPU chips for two weeks, at an estimated cost of\n$25,000.", "metadata": {}}, {"text": "XLNet had a similar architecture to BERT-large, but used a more expensive objective function (in addition\nto an order of magnitude more data), and was trained on 512 TPU chips for 2.5 days.", "metadata": {}}, {"text": "7 It is impossible to reproduce\nthe best BERT-large results8 or XLNet results9 using a single GPU.", "metadata": {}}, {"text": "Specialized models can have even more extreme\ncosts, such as AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go\n[37] at a cost of over $1,000 per hour.10\nWhen examining variants of a single model (e.g., BERT-small and BERT-large) we see that larger models can have\nstronger performance, which is a valuable scientiﬁc contribution.", "metadata": {}}, {"text": "However, this implies the ﬁnancial and environmen-\ntal cost of increasingly large AI models will not decrease soon, as the pace of model growth far exceeds the resulting\nincrease in model performance [16].", "metadata": {}}, {"text": "As a result, more and more resources are going to be required to keep improving\nAI models by simply making them larger.", "metadata": {}}, {"text": "Processing many examples Another way state-of-the-art performance has recently been progressing in AI is by\nsuccessively increasing the amount of training data models are trained on.", "metadata": {}}, {"text": "BERT-large had top performance in 2018\nacross many NLP tasks after training on 3 billion word-pieces.", "metadata": {}}, {"text": "XLNet recently outperformed BERT after training\non 32 billion word-pieces, including part of Common Crawl;", "metadata": {}}, {"text": "openGPT-2-XL trained on 40 billion words;", "metadata": {}}, {"text": "FAIR’s\nRoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours\nto train.", "metadata": {}}, {"text": "In computer vision, researchers from Facebook [25] pretrained an image classiﬁcation model on 3.5 billion\nimages from Instagram, three orders of magnitude larger than existing labelled image datasets such as Open Images.11\nThe use of massive data creates barriers for many researchers for reproducing the results of these models, or\ntraining their own models on the same setup (especially as training for multiple epochs is standard).", "metadata": {}}, {"text": "For example, the\nJune 2019 Common Crawl contains 242 TB of uncompressed data, 12 so even storing the data is expensive.", "metadata": {}}, {"text": "Finally,\nas in the case of model size, relying on more data to improve performance is notoriously expensive because of the\ndiminishing return of adding more data [41].", "metadata": {}}, {"text": "For instance, Figure 3, taken from [25], shows a logarithmic relation\nbetween the object recognition top-1 accuracy and the number of training examples.", "metadata": {}}, {"text": "Massive number of experiments Some projects have poured large amounts of computation into tuning hyperpa-\nrameters or searching over neural architectures, well beyond the reach of most researchers.", "metadata": {}}, {"text": "For instance, researchers\nfrom Google [51] trained over 12,800 neural networks in their neural architecture search to improve performance on\nobject detection and language modeling.", "metadata": {}}, {"text": "With a ﬁxed architecture, researchers from DeepMind [26] evaluated 1,500\nhyperparameter assignments to demonstrate that an LSTM language model [15] can reach state-of-the-art perplexity\nresults.", "metadata": {}}, {"text": "Despite the value of this result in showing that the performance of an LSTM does not plateau after only a few\nhyperparameter trials, fully exploring the potential of other competitive models for a fair comparison is prohibitively\nexpensive.", "metadata": {}}, {"text": "7Some estimates for the cost of this process reach $250,000 (twitter.com/eturner303/status/1143174828804857856).", "metadata": {}}, {"text": "8See https://github.com/google-research/bert\n9See https://github.com/zihangdai/xlnet\n10Recent versions of AlphaGo are far more efﬁcient [39].", "metadata": {}}, {"text": "11https://opensource.google.com/projects/open-images-dataset\n12http://commoncrawl.org/2019/07/\n4", "metadata": {}}], "metadata": {"page": 4}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "Figure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of\ntraining examples increases exponentially [25].\nThe topic of massive number of experiments is not as well studied as the ﬁrst two discussed above. In fact, the\nnumber of experiments performed during model construction is often underreported. Nonetheless, evidence for a\nlogarithmic relation exists here as well, between the number of experiments and performance gains [9].\nDiscussion The beneﬁts of pouring more resources into models are certainly of interest to the AI community. Indeed,\nthere is value in pushing the limits of model size, dataset size, and the hyperparameter search space. Currently, despite\nthe massive amount of resources put into recent AI models, such investment still pays off in terms of downstream\nperformance (albeit at an increasingly lower rate). Finding the point of saturation (if such exists) is an important\nquestion for the future of AI.\nOur goal in this paper is to raise awareness of the cost of Red AI, as well as encourage the AI community to\nrecognize the value of work by researchers that take a different path, optimizing efﬁciency rather than accuracy. Next\nwe turn to discuss concrete measures for making AI more green.\n3 Green AI\nThe term Green AI refers to AI research that yields novel results without increasing computational cost, and ideally\nreducing it. Whereas Red AI has resulted in rapidly escalating computational (and thus carbon) costs, Green AI has the\nopposite effect. If measures of efﬁciency are widely accepted as important evaluation metrics for research alongside\naccuracy, then researchers will have the option of focusing on the efﬁciency of their models with positive impact on\nboth the environment and inclusiveness. This section reviews several measures of efﬁciency that could be reported\nand optimized, and advocates one particular measure—FPO—which we argue should be reported when AI research\nﬁndings are published.\n3.1 Measures of Efﬁciency\nTo measure efﬁciency, we suggest reporting the amount of work required to generate a result in AI, that is, the amount\nof work required to train a model, and if applicable, the sum of works for all hyperparameter tuning experiments. As\n5", "sentences": [{"text": "Figure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of\ntraining examples increases exponentially [25].", "metadata": {}}, {"text": "The topic of massive number of experiments is not as well studied as the ﬁrst two discussed above.", "metadata": {}}, {"text": "In fact, the\nnumber of experiments performed during model construction is often underreported.", "metadata": {}}, {"text": "Nonetheless, evidence for a\nlogarithmic relation exists here as well, between the number of experiments and performance gains [9].", "metadata": {}}, {"text": "Discussion The beneﬁts of pouring more resources into models are certainly of interest to the AI community.", "metadata": {}}, {"text": "Indeed,\nthere is value in pushing the limits of model size, dataset size, and the hyperparameter search space.", "metadata": {}}, {"text": "Currently, despite\nthe massive amount of resources put into recent AI models, such investment still pays off in terms of downstream\nperformance (albeit at an increasingly lower rate).", "metadata": {}}, {"text": "Finding the point of saturation (if such exists) is an important\nquestion for the future of AI.", "metadata": {}}, {"text": "Our goal in this paper is to raise awareness of the cost of Red AI, as well as encourage the AI community to\nrecognize the value of work by researchers that take a different path, optimizing efﬁciency rather than accuracy.", "metadata": {}}, {"text": "Next\nwe turn to discuss concrete measures for making AI more green.", "metadata": {}}, {"text": "3 Green AI\nThe term Green AI refers to AI research that yields novel results without increasing computational cost, and ideally\nreducing it.", "metadata": {}}, {"text": "Whereas Red AI has resulted in rapidly escalating computational (and thus carbon) costs, Green AI has the\nopposite effect.", "metadata": {}}, {"text": "If measures of efﬁciency are widely accepted as important evaluation metrics for research alongside\naccuracy, then researchers will have the option of focusing on the efﬁciency of their models with positive impact on\nboth the environment and inclusiveness.", "metadata": {}}, {"text": "This section reviews several measures of efﬁciency that could be reported\nand optimized, and advocates one particular measure—FPO—which we argue should be reported when AI research\nﬁndings are published.", "metadata": {}}, {"text": "3.1 Measures of Efﬁciency\nTo measure efﬁciency, we suggest reporting the amount of work required to generate a result in AI, that is, the amount\nof work required to train a model, and if applicable, the sum of works for all hyperparameter tuning experiments.", "metadata": {}}, {"text": "As\n5", "metadata": {}}], "metadata": {"page": 5}}, {"text": "[Image page=5 idx=1 name=Im3.png] Size: 1086x935, Data: 347062 bytes", "sentences": [{"text": "[Image page=5 idx=1 name=Im3.png] Size: 1086x935, Data: 347062 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 1, "image_name": "Im3.png", "image_width": 1086, "image_height": 935, "attachment_type": "image", "has_image_data": true, "image_data_size": 347062}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "the cost of an experiment decomposes into the cost of a processing a single example, the size of the dataset, and the\nnumber of experiments (Equation 1), reducing the amount of work in each of these steps will result in AI that is more\ngreen.\nWhen reporting the amount of work done by a model, we want to measure a quantity that allows for a fair com-\nparison between different models. As a result, this measure should ideally be stable across different labs, at different\ntimes, and using different hardware.\nCarbon emission Carbon emission is appealing as it is a quantity we want to directly minimize. Nonetheless it\nis impractical to measure the exact amount of carbon released by training or executing a model, and accordingly—\ngenerating an AI result, as this amount depends highly on the local electricity infrastructure. As a result, it is not\ncomparable between researchers in different locations or even the same location at different times.\nElectricity usage Electricity usage is correlated with carbon emission while being time- and location-agnostic.\nMoreover, GPUs often report the amount of electricity each of their cores consume at each time point, which facilitates\nthe estimation of the total amount of electricity consumed by generating an AI result. Nonetheless, this measure is\nhardware dependent, and as a result does not allow for a fair comparison between different models.\nElapsed real time The total running time for generating an AI result is a natural measure for efﬁciency, as all other\nthings being equal, a faster model is doing less computational work. Nonetheless, this measure is highly inﬂuenced\nby factors such as the underlying hardware, other jobs running on the same machine, and the number of cores used.\nThese factors hinder the comparison between different models, as well as the decoupling of modeling contributions\nfrom hardware improvements.\nNumber of parameters Another common measure of efﬁciency is the number of parameters (learnable or total)\nused by the model. As with run time, this measure is correlated with the amount of work. Unlike the other measures\ndescribed above, it does not depend on the underlying hardware. Moreover, this measure also highly correlates with the\namount of memory consumed by the model. Nonetheless, different algorithms make different use of their parameters,\nfor instance by making the model deeper vs. wider. As a result, different models with a similar number of parameters\noften perform different amounts of work.\nFPO As a concrete measure, we suggest reporting the total number of ﬂoating point operations (FPO) required to\ngenerate a result. 13 FPO provides an estimate to the amount of work performed by a computational process. It is\ncomputed analytically by deﬁning a cost to two base operations, ADD and MUL. Based on these operations, the FPO\ncost of any machine learning abstract operation (e.g., a tanh operation, a matrix multiplication, a convolution operation,\nor the BERT model) can be computed as a recursive function of these two operations. FPO has been used in the past\nto quantify the energy footprint of a model [27, 43, 12, 42], but is not widely adopted in AI.\nFPO has several appealing properties. First, it directly computes the amount of work done by the running machine\nwhen executing a speciﬁc instance of a model, and is thus tied to the amount of energy consumed. Second, FPO is\nagnostic to the hardware on which the model is run. This facilitates fair comparisons between different approaches,\nunlike the measures described above. Third, FPO is strongly correlated with the running time of the model [4]. Unlike\nasymptotic runtime, FPO also considers the amount of work done at each time step.\nSeveral packages exist for computing FPO in various neural network libraries,14 though none of them contains all\nthe building blocks required to construct all modern AI models. We encourage the builders of neural network libraries\nto implement such functionality directly.\n13Floating point operations are often referred to as FLOP(s), though this term is not uniquely deﬁned [12]. To avoid confusion, we use the term\nFPO.\n14E.g., https://github.com/Swall0w/torchstat ; https://github.com/Lyken17/pytorch-OpCounter\n6", "sentences": [{"text": "the cost of an experiment decomposes into the cost of a processing a single example, the size of the dataset, and the\nnumber of experiments (Equation 1), reducing the amount of work in each of these steps will result in AI that is more\ngreen.", "metadata": {}}, {"text": "When reporting the amount of work done by a model, we want to measure a quantity that allows for a fair com-\nparison between different models.", "metadata": {}}, {"text": "As a result, this measure should ideally be stable across different labs, at different\ntimes, and using different hardware.", "metadata": {}}, {"text": "Carbon emission Carbon emission is appealing as it is a quantity we want to directly minimize.", "metadata": {}}, {"text": "Nonetheless it\nis impractical to measure the exact amount of carbon released by training or executing a model, and accordingly—\ngenerating an AI result, as this amount depends highly on the local electricity infrastructure.", "metadata": {}}, {"text": "As a result, it is not\ncomparable between researchers in different locations or even the same location at different times.", "metadata": {}}, {"text": "Electricity usage Electricity usage is correlated with carbon emission while being time- and location-agnostic.", "metadata": {}}, {"text": "Moreover, GPUs often report the amount of electricity each of their cores consume at each time point, which facilitates\nthe estimation of the total amount of electricity consumed by generating an AI result.", "metadata": {}}, {"text": "Nonetheless, this measure is\nhardware dependent, and as a result does not allow for a fair comparison between different models.", "metadata": {}}, {"text": "Elapsed real time The total running time for generating an AI result is a natural measure for efﬁciency, as all other\nthings being equal, a faster model is doing less computational work.", "metadata": {}}, {"text": "Nonetheless, this measure is highly inﬂuenced\nby factors such as the underlying hardware, other jobs running on the same machine, and the number of cores used.", "metadata": {}}, {"text": "These factors hinder the comparison between different models, as well as the decoupling of modeling contributions\nfrom hardware improvements.", "metadata": {}}, {"text": "Number of parameters Another common measure of efﬁciency is the number of parameters (learnable or total)\nused by the model.", "metadata": {}}, {"text": "As with run time, this measure is correlated with the amount of work.", "metadata": {}}, {"text": "Unlike the other measures\ndescribed above, it does not depend on the underlying hardware.", "metadata": {}}, {"text": "Moreover, this measure also highly correlates with the\namount of memory consumed by the model.", "metadata": {}}, {"text": "Nonetheless, different algorithms make different use of their parameters,\nfor instance by making the model deeper vs.", "metadata": {}}, {"text": "wider.", "metadata": {}}, {"text": "As a result, different models with a similar number of parameters\noften perform different amounts of work.", "metadata": {}}, {"text": "FPO As a concrete measure, we suggest reporting the total number of ﬂoating point operations (FPO) required to\ngenerate a result.", "metadata": {}}, {"text": "13 FPO provides an estimate to the amount of work performed by a computational process.", "metadata": {}}, {"text": "It is\ncomputed analytically by deﬁning a cost to two base operations, ADD and MUL.", "metadata": {}}, {"text": "Based on these operations, the FPO\ncost of any machine learning abstract operation (e.g., a tanh operation, a matrix multiplication, a convolution operation,\nor the BERT model) can be computed as a recursive function of these two operations.", "metadata": {}}, {"text": "FPO has been used in the past\nto quantify the energy footprint of a model [27, 43, 12, 42], but is not widely adopted in AI.", "metadata": {}}, {"text": "FPO has several appealing properties.", "metadata": {}}, {"text": "First, it directly computes the amount of work done by the running machine\nwhen executing a speciﬁc instance of a model, and is thus tied to the amount of energy consumed.", "metadata": {}}, {"text": "Second, FPO is\nagnostic to the hardware on which the model is run.", "metadata": {}}, {"text": "This facilitates fair comparisons between different approaches,\nunlike the measures described above.", "metadata": {}}, {"text": "Third, FPO is strongly correlated with the running time of the model [4].", "metadata": {}}, {"text": "Unlike\nasymptotic runtime, FPO also considers the amount of work done at each time step.", "metadata": {}}, {"text": "Several packages exist for computing FPO in various neural network libraries,14 though none of them contains all\nthe building blocks required to construct all modern AI models.", "metadata": {}}, {"text": "We encourage the builders of neural network libraries\nto implement such functionality directly.", "metadata": {}}, {"text": "13Floating point operations are often referred to as FLOP(s), though this term is not uniquely deﬁned [12].", "metadata": {}}, {"text": "To avoid confusion, we use the term\nFPO.", "metadata": {}}, {"text": "14E.g., https://github.com/Swall0w/torchstat ;", "metadata": {}}, {"text": "https://github.com/Lyken17/pytorch-OpCounter\n6", "metadata": {}}], "metadata": {"page": 6}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "(a) Different models.\n (b) Different layers of the ResNet model.\nFigure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top):\nmodel parameters (in million), FPO (in billions), top-1 accuracy on ImageNet. (4a): Different models: AlexNet\n[20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17]. (4b): Comparison of different sizes (measured by the\nnumber of layers) of the ResNet model [14].\nDiscussion Efﬁcient machine learning approaches have received attention in the research community, but are gener-\nally not motivated by being green. For example, a signiﬁcant amount of work in the computer vision community has\naddressed efﬁcient inference, which is necessary for real-time processing of images for applications like self-driving\ncars [24, 31, 22], or for placing models on devices such as mobile phones [16, 34]. Most of these approaches target ef-\nﬁcient model inference [32, 50, 12],15 and thus only minimize the cost of processing a single example, while ignoring\nthe other two red practices discussed in Section 2.16\nThe above examples indicate that the path to making AI green depends on how it is used. When developing a new\nmodel, much of the research process involves training many model variants on a training set and performing inference\non a small development set. In such a setting, more efﬁcient training procedures can lead to greater savings, while in\na production setting more efﬁcient inference can be more important. We advocate for a holistic view of computational\nsavings which doesn’t sacriﬁce in some areas to make advances in others.\nFPO has some limitations. First, it targets the electricity consumption of a model, while ignoring other potential\nlimiting factors for researchers such as the memory consumption by the model, which can often lead to additional\nenergy and monetary costs [24]. Second, the amount of work done by a model largely depends on the model imple-\nmentation, as two different implementations of the same model could result in very different amounts of processing\nwork. Due to the focus on the modeling contribution, the AI community has traditionally ignored the quality or ef-\nﬁciency of models’ implementation.17 We argue that the time to reverse this norm has come, and that exceptionally\ngood implementations that lead to efﬁcient models should be credited by the AI community.\n3.2 FPO Cost of Existing Models\nTo demonstrate the importance of reporting the amount of work, we present FPO costs for several existing models. 18\nFigure 4a shows the number of parameters and FPO of several leading object recognition models, as well as their\n15Some very recent work also targeted efﬁcient training [7].\n16In fact, creating smaller models often results in longer running time, so mitigating the different trends might be at odds [44].\n17We consider this exclusive focus on the ﬁnal prediction another symptom of Red AI.\n18These numbers represent FPO per inference, i.e., the work required to process a single example.\n7", "sentences": [{"text": "(a) Different models.", "metadata": {}}, {"text": "(b) Different layers of the ResNet model.", "metadata": {}}, {"text": "Figure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy.", "metadata": {}}, {"text": "Plots (bottom to top):\nmodel parameters (in million), FPO (in billions), top-1 accuracy on ImageNet.", "metadata": {}}, {"text": "(4a): Different models: AlexNet\n[20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17].", "metadata": {}}, {"text": "(4b): Comparison of different sizes (measured by the\nnumber of layers) of the ResNet model [14].", "metadata": {}}, {"text": "Discussion Efﬁcient machine learning approaches have received attention in the research community, but are gener-\nally not motivated by being green.", "metadata": {}}, {"text": "For example, a signiﬁcant amount of work in the computer vision community has\naddressed efﬁcient inference, which is necessary for real-time processing of images for applications like self-driving\ncars [24, 31, 22], or for placing models on devices such as mobile phones [16, 34].", "metadata": {}}, {"text": "Most of these approaches target ef-\nﬁcient model inference [32, 50, 12],15 and thus only minimize the cost of processing a single example, while ignoring\nthe other two red practices discussed in Section 2.16\nThe above examples indicate that the path to making AI green depends on how it is used.", "metadata": {}}, {"text": "When developing a new\nmodel, much of the research process involves training many model variants on a training set and performing inference\non a small development set.", "metadata": {}}, {"text": "In such a setting, more efﬁcient training procedures can lead to greater savings, while in\na production setting more efﬁcient inference can be more important.", "metadata": {}}, {"text": "We advocate for a holistic view of computational\nsavings which doesn’t sacriﬁce in some areas to make advances in others.", "metadata": {}}, {"text": "FPO has some limitations.", "metadata": {}}, {"text": "First, it targets the electricity consumption of a model, while ignoring other potential\nlimiting factors for researchers such as the memory consumption by the model, which can often lead to additional\nenergy and monetary costs [24].", "metadata": {}}, {"text": "Second, the amount of work done by a model largely depends on the model imple-\nmentation, as two different implementations of the same model could result in very different amounts of processing\nwork.", "metadata": {}}, {"text": "Due to the focus on the modeling contribution, the AI community has traditionally ignored the quality or ef-\nﬁciency of models’ implementation.17 We argue that the time to reverse this norm has come, and that exceptionally\ngood implementations that lead to efﬁcient models should be credited by the AI community.", "metadata": {}}, {"text": "3.2 FPO Cost of Existing Models\nTo demonstrate the importance of reporting the amount of work, we present FPO costs for several existing models.", "metadata": {}}, {"text": "18\nFigure 4a shows the number of parameters and FPO of several leading object recognition models, as well as their\n15Some very recent work also targeted efﬁcient training [7].", "metadata": {}}, {"text": "16In fact, creating smaller models often results in longer running time, so mitigating the different trends might be at odds [44].", "metadata": {}}, {"text": "17We consider this exclusive focus on the ﬁnal prediction another symptom of Red AI.", "metadata": {}}, {"text": "18These numbers represent FPO per inference, i.e., the work required to process a single example.", "metadata": {}}, {"text": "7", "metadata": {}}], "metadata": {"page": 7}}, {"text": "[Image page=7 idx=1 name=Im4.png] Size: 4000x3000, Data: 303469 bytes", "sentences": [{"text": "[Image page=7 idx=1 name=Im4.png] Size: 4000x3000, Data: 303469 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 1, "image_name": "Im4.png", "image_width": 4000, "image_height": 3000, "attachment_type": "image", "has_image_data": true, "image_data_size": 303469}}, {"text": "[Image page=7 idx=2 name=Im5.png] Size: 4000x3000, Data: 278612 bytes", "sentences": [{"text": "[Image page=7 idx=2 name=Im5.png] Size: 4000x3000, Data: 278612 bytes", "metadata": {}}], "metadata": {"page": 7, "image_index": 2, "image_name": "Im5.png", "image_width": 4000, "image_height": 3000, "attachment_type": "image", "has_image_data": true, "image_data_size": 278612}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "performance on the ImageNet dataset [6]. 19 A few trends are observable. First, as discussed in Section 2, models\nget more expensive with time, but the increase in FPO does not lead to similar performance gains. For instance, an\nincrease of almost 35% in FPO between ResNet and ResNext (second and third points in graph) resulted in a 0.5%\ntop-1 accuracy improvement. Similar patterns are observed when considering the effect of other increases in model\nwork. Second, the number of model parameters does not tell the whole story: AlexNet (ﬁrst point in the graph) actually\nhas more parameters than ResNet (second point), but dramatically less FPO, and also much lower accuracy.\nFigure 4b shows the same analysis for a single object recognition model, ResNet [14], while comparing different\nversions of the model with different number of layers. This creates a controlled comparison between the different\nmodels, as they are identical in architecture, except for their size (and accordingly, their FPO cost). Once again, we\nnotice the same trend: the large increase in FPO cost does not translate to a large increase in performance.\n3.3 Additional Ways to Promote Green AI\nIn addition to reporting the FPO cost of the ﬁnal reported number, we encourage researchers to report the bud-\nget/accuracy curve observed during training. In a recent paper [9], we observed that selecting the better performing\nmodel on a given task depends highly on the amount of compute available during model development. We introduced\na method for computing the expected best validation performance of a model as a function of the given budget. We\nargue that reporting this curve will allow users to make wiser decisions about their selection of models and highlight\nthe stability of different approaches.\nWe further advocate for making efﬁciency an ofﬁcial contribution in major AI conferences, by advising reviewers\nto recognize and value contributions that do not strictly improve state of the art, but have other beneﬁts such as\nefﬁciency. Finally, we note that the trend of releasing pretrained models publicly is a green success, and we would like\nto encourage organizations to continue to release their models in order to save others the costs of retraining them.\n4 Related Work\nRecent work has analyzed the carbon emissions of training deep NLP models [40] and concluded that computationally\nexpensive experiments can have a large environmental and economic impact. With modern experiments using such\nlarge budgets, many researchers (especially those in academia) lack the resources to work in many high-proﬁle areas;\nincreased value placed on computationally efﬁcient approaches will allow research contributions from more diverse\ngroups. We emphasize that the conclusions of [40] are the result of long-term trends, and are not isolated within NLP,\nbut hold true across machine learning.\nWhile some companies offset electricity usage by purchasing carbon credits, it is not clear that buying credits is\nas effective as using less energy. In addition, purchasing carbon credits is voluntary; Google cloud 20 and Microsoft\nAzure21 purchase carbon credits to offset their spent energy, but Amazon’s AWS22 (the largest cloud computing plat-\nform23) only covered ﬁfty percent of its power usage with renewable energy.\nThe push to improve state-of-the-art performance has focused the research community’s attention on reporting the\nsingle best result after running many experiments for model development and hyperparameter tuning. Failure to fully\nreport these experiments prevents future researchers from understanding how much effort is required to reproduce a\nresult or extend it [9].\nOur focus is on improving efﬁciency in the machine learning community, but machine learning can also be used\nas a tool for work in areas like climate change. For example, machine learning has been used for reducing emissions\nof cement plants [1] and tracking animal conservation outcomes [11], and is predicted to be useful for forest ﬁre\nmanagement [33]. Undoubtedly these are important applications of machine learning; we recognize that they are\northogonal to the content of this paper.\n19Numbers taken from https://github.com/sovrasov/flops-counter.pytorch\n20https://cloud.google.com/sustainability/\n21https://www.microsoft.com/en-us/environment/carbon\n22https://aws.amazon.com/about-aws/sustainability/\n23https://tinyurl.com/y2kob969\n8", "sentences": [{"text": "performance on the ImageNet dataset [6].", "metadata": {}}, {"text": "19 A few trends are observable.", "metadata": {}}, {"text": "First, as discussed in Section 2, models\nget more expensive with time, but the increase in FPO does not lead to similar performance gains.", "metadata": {}}, {"text": "For instance, an\nincrease of almost 35% in FPO between ResNet and ResNext (second and third points in graph) resulted in a 0.5%\ntop-1 accuracy improvement.", "metadata": {}}, {"text": "Similar patterns are observed when considering the effect of other increases in model\nwork.", "metadata": {}}, {"text": "Second, the number of model parameters does not tell the whole story: AlexNet (ﬁrst point in the graph) actually\nhas more parameters than ResNet (second point), but dramatically less FPO, and also much lower accuracy.", "metadata": {}}, {"text": "Figure 4b shows the same analysis for a single object recognition model, ResNet [14], while comparing different\nversions of the model with different number of layers.", "metadata": {}}, {"text": "This creates a controlled comparison between the different\nmodels, as they are identical in architecture, except for their size (and accordingly, their FPO cost).", "metadata": {}}, {"text": "Once again, we\nnotice the same trend: the large increase in FPO cost does not translate to a large increase in performance.", "metadata": {}}, {"text": "3.3 Additional Ways to Promote Green AI\nIn addition to reporting the FPO cost of the ﬁnal reported number, we encourage researchers to report the bud-\nget/accuracy curve observed during training.", "metadata": {}}, {"text": "In a recent paper [9], we observed that selecting the better performing\nmodel on a given task depends highly on the amount of compute available during model development.", "metadata": {}}, {"text": "We introduced\na method for computing the expected best validation performance of a model as a function of the given budget.", "metadata": {}}, {"text": "We\nargue that reporting this curve will allow users to make wiser decisions about their selection of models and highlight\nthe stability of different approaches.", "metadata": {}}, {"text": "We further advocate for making efﬁciency an ofﬁcial contribution in major AI conferences, by advising reviewers\nto recognize and value contributions that do not strictly improve state of the art, but have other beneﬁts such as\nefﬁciency.", "metadata": {}}, {"text": "Finally, we note that the trend of releasing pretrained models publicly is a green success, and we would like\nto encourage organizations to continue to release their models in order to save others the costs of retraining them.", "metadata": {}}, {"text": "4 Related Work\nRecent work has analyzed the carbon emissions of training deep NLP models [40] and concluded that computationally\nexpensive experiments can have a large environmental and economic impact.", "metadata": {}}, {"text": "With modern experiments using such\nlarge budgets, many researchers (especially those in academia) lack the resources to work in many high-proﬁle areas;", "metadata": {}}, {"text": "increased value placed on computationally efﬁcient approaches will allow research contributions from more diverse\ngroups.", "metadata": {}}, {"text": "We emphasize that the conclusions of [40] are the result of long-term trends, and are not isolated within NLP,\nbut hold true across machine learning.", "metadata": {}}, {"text": "While some companies offset electricity usage by purchasing carbon credits, it is not clear that buying credits is\nas effective as using less energy.", "metadata": {}}, {"text": "In addition, purchasing carbon credits is voluntary;", "metadata": {}}, {"text": "Google cloud 20 and Microsoft\nAzure21 purchase carbon credits to offset their spent energy, but Amazon’s AWS22 (the largest cloud computing plat-\nform23) only covered ﬁfty percent of its power usage with renewable energy.", "metadata": {}}, {"text": "The push to improve state-of-the-art performance has focused the research community’s attention on reporting the\nsingle best result after running many experiments for model development and hyperparameter tuning.", "metadata": {}}, {"text": "Failure to fully\nreport these experiments prevents future researchers from understanding how much effort is required to reproduce a\nresult or extend it [9].", "metadata": {}}, {"text": "Our focus is on improving efﬁciency in the machine learning community, but machine learning can also be used\nas a tool for work in areas like climate change.", "metadata": {}}, {"text": "For example, machine learning has been used for reducing emissions\nof cement plants [1] and tracking animal conservation outcomes [11], and is predicted to be useful for forest ﬁre\nmanagement [33].", "metadata": {}}, {"text": "Undoubtedly these are important applications of machine learning;", "metadata": {}}, {"text": "we recognize that they are\northogonal to the content of this paper.", "metadata": {}}, {"text": "19Numbers taken from https://github.com/sovrasov/flops-counter.pytorch\n20https://cloud.google.com/sustainability/\n21https://www.microsoft.com/en-us/environment/carbon\n22https://aws.amazon.com/about-aws/sustainability/\n23https://tinyurl.com/y2kob969\n8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "5 Conclusion\nThe vision of Green AI raises many exciting research directions that help to overcome the inclusiveness challenges of\nRed AI. Progress will reduce the computational expense with a minimal reduction in performance, or even improve\nperformance as more efﬁcient methods are discovered. Also, it would seem that Green AI could be moving us in a\nmore cognitively plausible direction as the brain is highly efﬁcient.\nIt’s important to reiterate that we see Green AI as a valuable option not an exclusive mandate—of course, both\nGreen AI and Red AI have contributions to make. We want to increase the prevalence of Green AI by highlighting its\nbeneﬁts, advocating a standard measure of efﬁciency. Below, we point to a few important green research directions,\nand highlight a few open questions.\nResearch on building space or time efﬁcient models is often motivated by ﬁtting a model on a small device (such\nas a phone) or fast enough to process examples in real time, such as image captioning for the blind (see Section 3.1).\nSome modern models don’t even ﬁt on a single GPU (see Section 2). Here we argue for a far broader approach.\nData efﬁciency has received signiﬁcant attention over the years [35, 19]. Modern research in vision and NLP often\ninvolves ﬁrst pretraining a model on large “raw” (unannotated) data then ﬁne-tuning it to a task of interest through\nsupervised learning. A strong result in this area often involves achieving similar performance to a baseline with\nfewer training examples or fewer gradient steps. Most recent work has addressed ﬁne-tuning data [29], but pretraining\nefﬁciency is also important. In either case, one simple technique to improve in this area is to simply report performance\nwith different amounts of training data. For example, reporting performance of contextual embedding models trained\non 10 million, 100 million, 1 billion, and 10 billion tokens would facilitate faster development of new models, as they\ncan ﬁrst be compared at the smallest data sizes. Research here is of value not just to make training less expensive, but\nbecause in areas such as low resource languages or historical domains it is extremely hard to generate more data, so to\nprogress we must make more efﬁcient use of what is available.\nFinally, the total number of experiments run to get a ﬁnal result is often underreported and underdiscussed [9]. The\nfew instances researchers have of full reporting of the hyperparameter search, architecture evaluations, and ablations\nthat went into a reported experimental result have surprised the community [40]. While many hyperparameter opti-\nmization algorithms exist which can reduce the computational expense required to reach a given level of performance\n[3, 10], simple improvements here can have a large impact. For example, stopping training early for models which are\nclearly underperforming can lead to great savings [21].\nReferences\n[1] Prabal Acharyya, Sean D Rosario, Roey Flor, Ritvik Joshi, Dian Li, Roberto Linares, and Hongbao Zhang.\nAutopilot of cement plants for reduction of fuel consumption and emissions, 2019. ICML Workshop on Climate\nChange.\n[2] Dario Amodei and Danny Hernandez. AI and compute, 2018. Blog post.\n[3] James S. Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter optimiza-\ntion. In Proc. of NeurIPS, 2011.\n[4] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for\npractical applications. In Proc. of ISCAS, 2017.\n[5] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks. In\nProc. of NeurIPS, 2017.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In Proc. of CVPR, 2009.\n[7] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance,\n2019. arXiv:1907.04840.\n9", "sentences": [{"text": "5 Conclusion\nThe vision of Green AI raises many exciting research directions that help to overcome the inclusiveness challenges of\nRed AI.", "metadata": {}}, {"text": "Progress will reduce the computational expense with a minimal reduction in performance, or even improve\nperformance as more efﬁcient methods are discovered.", "metadata": {}}, {"text": "Also, it would seem that Green AI could be moving us in a\nmore cognitively plausible direction as the brain is highly efﬁcient.", "metadata": {}}, {"text": "It’s important to reiterate that we see Green AI as a valuable option not an exclusive mandate—of course, both\nGreen AI and Red AI have contributions to make.", "metadata": {}}, {"text": "We want to increase the prevalence of Green AI by highlighting its\nbeneﬁts, advocating a standard measure of efﬁciency.", "metadata": {}}, {"text": "Below, we point to a few important green research directions,\nand highlight a few open questions.", "metadata": {}}, {"text": "Research on building space or time efﬁcient models is often motivated by ﬁtting a model on a small device (such\nas a phone) or fast enough to process examples in real time, such as image captioning for the blind (see Section 3.1).", "metadata": {}}, {"text": "Some modern models don’t even ﬁt on a single GPU (see Section 2).", "metadata": {}}, {"text": "Here we argue for a far broader approach.", "metadata": {}}, {"text": "Data efﬁciency has received signiﬁcant attention over the years [35, 19].", "metadata": {}}, {"text": "Modern research in vision and NLP often\ninvolves ﬁrst pretraining a model on large “raw” (unannotated) data then ﬁne-tuning it to a task of interest through\nsupervised learning.", "metadata": {}}, {"text": "A strong result in this area often involves achieving similar performance to a baseline with\nfewer training examples or fewer gradient steps.", "metadata": {}}, {"text": "Most recent work has addressed ﬁne-tuning data [29], but pretraining\nefﬁciency is also important.", "metadata": {}}, {"text": "In either case, one simple technique to improve in this area is to simply report performance\nwith different amounts of training data.", "metadata": {}}, {"text": "For example, reporting performance of contextual embedding models trained\non 10 million, 100 million, 1 billion, and 10 billion tokens would facilitate faster development of new models, as they\ncan ﬁrst be compared at the smallest data sizes.", "metadata": {}}, {"text": "Research here is of value not just to make training less expensive, but\nbecause in areas such as low resource languages or historical domains it is extremely hard to generate more data, so to\nprogress we must make more efﬁcient use of what is available.", "metadata": {}}, {"text": "Finally, the total number of experiments run to get a ﬁnal result is often underreported and underdiscussed [9].", "metadata": {}}, {"text": "The\nfew instances researchers have of full reporting of the hyperparameter search, architecture evaluations, and ablations\nthat went into a reported experimental result have surprised the community [40].", "metadata": {}}, {"text": "While many hyperparameter opti-\nmization algorithms exist which can reduce the computational expense required to reach a given level of performance\n[3, 10], simple improvements here can have a large impact.", "metadata": {}}, {"text": "For example, stopping training early for models which are\nclearly underperforming can lead to great savings [21].", "metadata": {}}, {"text": "References\n[1] Prabal Acharyya, Sean D Rosario, Roey Flor, Ritvik Joshi, Dian Li, Roberto Linares, and Hongbao Zhang.", "metadata": {}}, {"text": "Autopilot of cement plants for reduction of fuel consumption and emissions, 2019.", "metadata": {}}, {"text": "ICML Workshop on Climate\nChange.", "metadata": {}}, {"text": "[2] Dario Amodei and Danny Hernandez.", "metadata": {}}, {"text": "AI and compute, 2018.", "metadata": {}}, {"text": "Blog post.", "metadata": {}}, {"text": "[3] James S.", "metadata": {}}, {"text": "Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl.", "metadata": {}}, {"text": "Algorithms for hyper-parameter optimiza-\ntion.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of NeurIPS, 2011.", "metadata": {}}, {"text": "[4] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.", "metadata": {}}, {"text": "An analysis of deep neural network models for\npractical applications.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ISCAS, 2017.", "metadata": {}}, {"text": "[5] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng.", "metadata": {}}, {"text": "Dual path networks.", "metadata": {}}, {"text": "In\nProc.", "metadata": {}}, {"text": "of NeurIPS, 2017.", "metadata": {}}, {"text": "[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.", "metadata": {}}, {"text": "ImageNet: A large-scale hierarchical\nimage database.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2009.", "metadata": {}}, {"text": "[7] Tim Dettmers and Luke Zettlemoyer.", "metadata": {}}, {"text": "Sparse networks from scratch: Faster training without losing performance,\n2019.", "metadata": {}}, {"text": "arXiv:1907.04840.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proc. of NAACL, 2019.\n[9] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your work: Improved\nreporting of experimental results. In Proc. of EMNLP, 2019.\n[10] Jesse Dodge, Kevin Jamieson, and Noah A. Smith. Open loop hyperparameter optimization and determinantal\npoint processes. In Proc. of AutoML, 2017.\n[11] Clement Duhart, Gershon Dublon, Brian Mayton, Glorianna Davenport, and Joseph A. Paradiso. Deep learning\nfor wildlife conservation and restoration efforts, 2019. ICML Workshop on Climate Change.\n[12] Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast &\nsimple resource-constrained structure learning of deep networks. In Proc. of CVPR, 2018.\n[13] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effectiveness of data. IEEE Intelligent\nSystems, 24:8–12, 2009.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProc. of CVPR, 2016.\n[15] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient convolutional neural networks for mobile vision applications,\n2017. arXiv:1704.04861.\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proc. of CVPR, 2018.\n[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbig-\nniew Wojna, Yang Song, Sergio Guadarrama, and Kevin Murphy. Speed/accuracy trade-offs for modern convo-\nlutional object detectors. In Proc. of CVPR, 2017.\n[19] Sanket Kamthe and Marc Peter Deisenroth. Data-efﬁcient reinforcement learning with probabilistic model pre-\ndictive control. In Proc. of AISTATS, 2018.\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Proc. of NeurIPS, 2012.\n[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-\nbased conﬁguration evaluation for hyperparameter optimization. InProc. of ICLR, 2017.\n[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.\nBerg. Ssd: Single shot multibox detector. In Proc. of ECCV, 2016.\n[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach, 2019.\narXiv:1907.11692.\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShufﬂeNet V2: Practical guidelines for efﬁcient\ncnn architecture design. In Proc. of ECCV, 2018.\n[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proc. ECCV,\n2018.\n10", "sentences": [{"text": "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.", "metadata": {}}, {"text": "BERT: Pre-training of deep bidirectional\ntransformers for language understanding.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of NAACL, 2019.", "metadata": {}}, {"text": "[9] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A.", "metadata": {}}, {"text": "Smith.", "metadata": {}}, {"text": "Show your work: Improved\nreporting of experimental results.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of EMNLP, 2019.", "metadata": {}}, {"text": "[10] Jesse Dodge, Kevin Jamieson, and Noah A.", "metadata": {}}, {"text": "Smith.", "metadata": {}}, {"text": "Open loop hyperparameter optimization and determinantal\npoint processes.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of AutoML, 2017.", "metadata": {}}, {"text": "[11] Clement Duhart, Gershon Dublon, Brian Mayton, Glorianna Davenport, and Joseph A.", "metadata": {}}, {"text": "Paradiso.", "metadata": {}}, {"text": "Deep learning\nfor wildlife conservation and restoration efforts, 2019.", "metadata": {}}, {"text": "ICML Workshop on Climate Change.", "metadata": {}}, {"text": "[12] Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi.", "metadata": {}}, {"text": "MorphNet: Fast &\nsimple resource-constrained structure learning of deep networks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2018.", "metadata": {}}, {"text": "[13] Alon Halevy, Peter Norvig, and Fernando Pereira.", "metadata": {}}, {"text": "The unreasonable effectiveness of data.", "metadata": {}}, {"text": "IEEE Intelligent\nSystems, 24:8–12, 2009.", "metadata": {}}, {"text": "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "metadata": {}}, {"text": "Deep residual learning for image recognition.", "metadata": {}}, {"text": "In\nProc.", "metadata": {}}, {"text": "of CVPR, 2016.", "metadata": {}}, {"text": "[15] Sepp Hochreiter and J ¨urgen Schmidhuber.", "metadata": {}}, {"text": "Long short-term memory.", "metadata": {}}, {"text": "Neural computation, 9(8):1735–1780,\n1997.", "metadata": {}}, {"text": "[16] Andrew G.", "metadata": {}}, {"text": "Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam.", "metadata": {}}, {"text": "MobileNets: Efﬁcient convolutional neural networks for mobile vision applications,\n2017.", "metadata": {}}, {"text": "arXiv:1704.04861.", "metadata": {}}, {"text": "[17] Jie Hu, Li Shen, and Gang Sun.", "metadata": {}}, {"text": "Squeeze-and-excitation networks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2018.", "metadata": {}}, {"text": "[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbig-\nniew Wojna, Yang Song, Sergio Guadarrama, and Kevin Murphy.", "metadata": {}}, {"text": "Speed/accuracy trade-offs for modern convo-\nlutional object detectors.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2017.", "metadata": {}}, {"text": "[19] Sanket Kamthe and Marc Peter Deisenroth.", "metadata": {}}, {"text": "Data-efﬁcient reinforcement learning with probabilistic model pre-\ndictive control.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of AISTATS, 2018.", "metadata": {}}, {"text": "[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.", "metadata": {}}, {"text": "Imagenet classiﬁcation with deep convolutional neural\nnetworks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of NeurIPS, 2012.", "metadata": {}}, {"text": "[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar.", "metadata": {}}, {"text": "Hyperband: Bandit-\nbased conﬁguration evaluation for hyperparameter optimization.", "metadata": {}}, {"text": "InProc.", "metadata": {}}, {"text": "of ICLR, 2017.", "metadata": {}}, {"text": "[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.", "metadata": {}}, {"text": "Berg.", "metadata": {}}, {"text": "Ssd: Single shot multibox detector.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ECCV, 2016.", "metadata": {}}, {"text": "[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov.", "metadata": {}}, {"text": "RoBERTa: A robustly optimized bert pretraining approach, 2019.", "metadata": {}}, {"text": "arXiv:1907.11692.", "metadata": {}}, {"text": "[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.", "metadata": {}}, {"text": "ShufﬂeNet V2: Practical guidelines for efﬁcient\ncnn architecture design.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ECCV, 2018.", "metadata": {}}, {"text": "[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten.", "metadata": {}}, {"text": "Exploring the limits of weakly supervised pretraining.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "ECCV,\n2018.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "[26] G ´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In\nProc. of EMNLP, 2018.\n[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks\nfor resource efﬁcient inference. In Proc. of ICLR, 2017.\n[28] Gordon E. Moore. Cramming more components onto integrated circuits, 1965.\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.\n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners, 2019. OpenAI Blog.\n[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation\nusing binary convolutional neural networks. In Proc. of ECCV, 2016.\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object\ndetection. In Proc. of CVPR, 2016.\n[33] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, An-\ndrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan\nMaharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. K¨ording, Carla Gomes, Andrew Y . Ng, Demis Has-\nsabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and Yoshua Bengio. Tackling climate change with machine\nlearning, 2019. arXiv:1905.12616.\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2:\nInverted residuals and linear bottlenecks. In Proc. of CVPR, 2018.\n[35] Roy Schwartz, Sam Thomson, and Noah A. Smith. SoPa: Bridging CNNs, RNNs, and weighted ﬁnite-state\nmachines. In Proc. of ACL, 2018.\n[36] Yoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, James Manyika, Juan Carlos Niebles, Terah\nLyons, John Etchemendy, and Z Bauer. The AI index 2018 annual report. AI Index Steering Committee,\nHuman-Centered AI Initiative, Stanford University. Available at http://cdn.aiindex.org/2018/AI%\n20Index%202018%20Annual%20Report.pdf, 202018, 2018.\n[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore\nGraepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature,\n529(7587):484, 2016.\n[38] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis\nHassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.\narXiv:1712.01815.\n[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human\nknowledge. Nature, 550(7676):354, 2017.\n[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nNLP. In Proc. of ACL, 2019.\n11", "sentences": [{"text": "[26] G ´abor Melis, Chris Dyer, and Phil Blunsom.", "metadata": {}}, {"text": "On the state of the art of evaluation in neural language models.", "metadata": {}}, {"text": "In\nProc.", "metadata": {}}, {"text": "of EMNLP, 2018.", "metadata": {}}, {"text": "[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.", "metadata": {}}, {"text": "Pruning convolutional neural networks\nfor resource efﬁcient inference.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ICLR, 2017.", "metadata": {}}, {"text": "[28] Gordon E.", "metadata": {}}, {"text": "Moore.", "metadata": {}}, {"text": "Cramming more components onto integrated circuits, 1965.", "metadata": {}}, {"text": "[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer.", "metadata": {}}, {"text": "Deep contextualized word representations.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of NAACL, 2018.", "metadata": {}}, {"text": "[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.", "metadata": {}}, {"text": "Language models are\nunsupervised multitask learners, 2019.", "metadata": {}}, {"text": "OpenAI Blog.", "metadata": {}}, {"text": "[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.", "metadata": {}}, {"text": "Xnor-net: Imagenet classiﬁcation\nusing binary convolutional neural networks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ECCV, 2016.", "metadata": {}}, {"text": "[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.", "metadata": {}}, {"text": "You only look once: Uniﬁed, real-time object\ndetection.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2016.", "metadata": {}}, {"text": "[33] David Rolnick, Priya L.", "metadata": {}}, {"text": "Donti, Lynn H.", "metadata": {}}, {"text": "Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, An-\ndrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan\nMaharaj, Evan D.", "metadata": {}}, {"text": "Sherwin, S.", "metadata": {}}, {"text": "Karthik Mukkavilli, Konrad P.", "metadata": {}}, {"text": "K¨ording, Carla Gomes, Andrew Y .", "metadata": {}}, {"text": "Ng, Demis Has-\nsabis, John C.", "metadata": {}}, {"text": "Platt, Felix Creutzig, Jennifer Chayes, and Yoshua Bengio.", "metadata": {}}, {"text": "Tackling climate change with machine\nlearning, 2019.", "metadata": {}}, {"text": "arXiv:1905.12616.", "metadata": {}}, {"text": "[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.", "metadata": {}}, {"text": "MobileNetV2:\nInverted residuals and linear bottlenecks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2018.", "metadata": {}}, {"text": "[35] Roy Schwartz, Sam Thomson, and Noah A.", "metadata": {}}, {"text": "Smith.", "metadata": {}}, {"text": "SoPa: Bridging CNNs, RNNs, and weighted ﬁnite-state\nmachines.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ACL, 2018.", "metadata": {}}, {"text": "[36] Yoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, James Manyika, Juan Carlos Niebles, Terah\nLyons, John Etchemendy, and Z Bauer.", "metadata": {}}, {"text": "The AI index 2018 annual report.", "metadata": {}}, {"text": "AI Index Steering Committee,\nHuman-Centered AI Initiative, Stanford University.", "metadata": {}}, {"text": "Available at http://cdn.aiindex.org/2018/AI%\n20Index%202018%20Annual%20Report.pdf, 202018, 2018.", "metadata": {}}, {"text": "[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore\nGraepel, and Demis Hassabis.", "metadata": {}}, {"text": "Mastering the game of Go with deep neural networks and tree search.", "metadata": {}}, {"text": "Nature,\n529(7587):484, 2016.", "metadata": {}}, {"text": "[38] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis\nHassabis.", "metadata": {}}, {"text": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.", "metadata": {}}, {"text": "arXiv:1712.01815.", "metadata": {}}, {"text": "[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis.", "metadata": {}}, {"text": "Mastering the game of Go without human\nknowledge.", "metadata": {}}, {"text": "Nature, 550(7676):354, 2017.", "metadata": {}}, {"text": "[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum.", "metadata": {}}, {"text": "Energy and policy considerations for deep learning in\nNLP.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ACL, 2019.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "[41] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of\ndata in deep learning era. In Proc. of ICCV, 2017.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n[43] Tom Veniat and Ludovic Denoyer. Learning time/memory-efﬁcient deep architectures with budgeted super net-\nworks. In Proc. of CVPR, 2018.\n[44] Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, and Dieter Fox. Early\nfusion for goal directed robotic vision. In Proc. of IROS, 2019.\n[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems,\n2019. arXiv:1905.00537.\n[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR, 2019.\n[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations\nfor deep neural networks. In Proc. of CVPR, 2017.\n[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet:\nGeneralized autoregressive pretraining for language understanding, 2019. arXiv:1906.08237.\n[49] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.\nDefending against neural fake news, 2019. arXiv:1905.12616.\n[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShufﬂeNet: An extremely efﬁcient convolutional\nneural network for mobile devices. In Proc. of CVPR, 2018.\n[51] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In Proc. of ICLR, 2017.\n12", "sentences": [{"text": "[41] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.", "metadata": {}}, {"text": "Revisiting unreasonable effectiveness of\ndata in deep learning era.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ICCV, 2017.", "metadata": {}}, {"text": "[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.", "metadata": {}}, {"text": "Gomez, Lukasz Kaiser,\nand Illia Polosukhin.", "metadata": {}}, {"text": "Attention is all you need.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of NeurIPS, 2017.", "metadata": {}}, {"text": "[43] Tom Veniat and Ludovic Denoyer.", "metadata": {}}, {"text": "Learning time/memory-efﬁcient deep architectures with budgeted super net-\nworks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2018.", "metadata": {}}, {"text": "[44] Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, and Dieter Fox.", "metadata": {}}, {"text": "Early\nfusion for goal directed robotic vision.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of IROS, 2019.", "metadata": {}}, {"text": "[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R.", "metadata": {}}, {"text": "Bowman.", "metadata": {}}, {"text": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems,\n2019.", "metadata": {}}, {"text": "arXiv:1905.00537.", "metadata": {}}, {"text": "[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.", "metadata": {}}, {"text": "Bowman.", "metadata": {}}, {"text": "GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ICLR, 2019.", "metadata": {}}, {"text": "[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.", "metadata": {}}, {"text": "Aggregated residual transformations\nfor deep neural networks.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2017.", "metadata": {}}, {"text": "[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .", "metadata": {}}, {"text": "Le.", "metadata": {}}, {"text": "XLNet:\nGeneralized autoregressive pretraining for language understanding, 2019.", "metadata": {}}, {"text": "arXiv:1906.08237.", "metadata": {}}, {"text": "[49] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.", "metadata": {}}, {"text": "Defending against neural fake news, 2019.", "metadata": {}}, {"text": "arXiv:1905.12616.", "metadata": {}}, {"text": "[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.", "metadata": {}}, {"text": "ShufﬂeNet: An extremely efﬁcient convolutional\nneural network for mobile devices.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of CVPR, 2018.", "metadata": {}}, {"text": "[51] Barret Zoph and Quoc V .", "metadata": {}}, {"text": "Le.", "metadata": {}}, {"text": "Neural architecture search with reinforcement learning.", "metadata": {}}, {"text": "In Proc.", "metadata": {}}, {"text": "of ICLR, 2017.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}]}