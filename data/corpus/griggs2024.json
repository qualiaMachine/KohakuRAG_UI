{"document_id": "griggs2024", "title": "M�lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity", "text": "Mélange: Cost Efficient Large Language Model\nServing by Exploiting GPU Heterogeneity\nTyler Griggs∗\nUC Berkeley\nXiaoxuan Liu∗\nUC Berkeley\nJiaxiang Yu\nNational University of Singapore\nDoyoung Kim\nUC Berkeley\nWei-Lin Chiang\nUC Berkeley\nAlvin Cheung\nUC Berkeley\nIon Stoica\nUC Berkeley\nAbstract\nLarge language models (LLMs) are increasingly integrated into many online ser-\nvices, yet they remain cost-prohibitive to deploy due to the requirement of expensive\nGPU instances. Prior work has addressed the high cost of LLM serving by improv-\ning the inference engine, but less attention has been given to selecting the most\ncost-efficient GPU type(s) for a specific LLM service. There is a large and growing\nlandscape of GPU types and, within these options, higher cost does not always\nlead to increased performance. Instead, through a comprehensive investigation, we\nfind that three key LLM service characteristics (request size, request rate, SLO)\nstrongly influence GPU cost efficiency, and differing GPU types are most cost\nefficient for differing LLM service settings. As a result, the most cost-efficient\nallocation for a given service is typically amix of heterogeneous GPU types. Based\non this analysis, we introduce Mélange, a GPU allocation framework that navigates\nthese diverse LLM service characteristics and heterogeneous GPU option space to\nautomatically and efficiently derive the minimal-cost GPU allocation for a given\nLLM service. We formulate the GPU allocation task as a cost-aware bin packing\nproblem where GPUs are bins and items are slices of the service workload. Our\nformulation’s constraints account for a service’s unique characteristics, allowing\nMélange to be flexible to support diverse service settings and heterogeneity-aware\nto adapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, Mélange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n1 Introduction\nLarge language models (LLMs) [35, 43, 44] are increasingly integrated into many online services,\nincluding search engines [37, 24], chatbots [34], and virtual assistants [28, 47, 48]. These services are\noften hosted by deploying models on cloud resources. However, deploying LLMs is expensive. The\nsubstantial size and computational demands of LLMs require the use of costly hardware accelerators,\ntypically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB\nGPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.\nPrior work [8, 16, 54, 57, 60] addresses the high cost of LLM serving by focusing on inference\nthroughput, but less attention has been given to selecting the most cost-efficient GPU type(s) for a\nspecific LLM service. The large and growing landscape of hardware accelerators — ranging from\nNVIDIA GPUs [33] and AMD GPUs [45] to Google TPUs [17], CPUs [23], and others [4] — offers\n∗Equal contribution\n2For brevity, we use “accelerator” and “GPU” interchangeably in this work.\nPreprint. Under review.\narXiv:2404.14527v4  [cs.DC]  22 Jul 2024\n\na wide array of choices with varying performance specifications and on-demand cloud costs. Within\nthese hardware options, higher cost does not always lead to increased performance. To investigate this\nphenomenon further, we examine GPU cost efficiency, defined based on common pricing models [34]\nas the number of input and output tokens processed per dollar cost (T/$) of on-demand cloud GPUs.\nWe find that GPU cost efficiency is determined by three key LLM service characteristics:\n1. Request Size: An LLM request’s size is made up of its input and output token lengths. For small\nrequest sizes, lower-end GPUs generally produce greater T/$ than high-end GPUs.\n2. Request Rate: To maximize utilization, provisioned GPU capacity should align with request\nvolume. At low request rates, services can reduce costs by right-sizing from expensive high-end\nGPUs to cheap low-end GPUs. Further, leveraging a mix of GPU types facilitates finer-grained\nresource scaling to better match request volume.\n3. Service-level Objective: Services typically establish latency SLOs to ensure service quality.\nBecause low-end GPUs generally incur higher latency than high-end GPUs, high-end GPUs are\nrequired for stringent SLOs while low-end GPUs can reduce costs in loose-SLO settings.\nConsider a GPU allocation strategy that integrates each of the three observations above: high-cost\nA100 GPUs handle large requests and meet stringent SLOs, but lower-cost A10G GPUs serve\nsmaller requests (1) and looser SLOs (3) at higher T/$. Then, during periods of low service activity,\nthe service right-sizes to the even-cheaper L4 GPU to maintain service availability at lowest cost\n(2). Consequently, we find that GPU heterogeneity presents opportunities for increasing GPU cost\nefficiency, but such opportunities are highly dependent on LLM service characteristics. The key\nchallenge, then, is creating a GPU allocation framework that can navigate the diversity of LLM\nservices (request sizes, request rates, latency SLOs) and GPU types to find the optimal GPU allocation.\nFigure 1: Mélange framework.\nWe present Mélange3 (Fig. 1), a GPU allocation framework that derives the minimal-cost GPU\nallocation for a given LLM service. In Mélange, each GPU type ( 1a) passes through a one-time\noffline profiling step (2 ) to measure GPU performance across request sizes and rates. Then, given\nthe profiling results and an LLM service definition (1b), Mélange’s objective is to choose a GPU\nallocation for the service workload that minimizes cost. This task is a natural application of the\ncost-aware bin packing problem, where bins are GPUs and items are slices of the workload. We\nformulate the problem as an integer linear program (ILP) and efficiently solve with an off-the-shelf\nsolver ( 3 ). Upon solution, Mélange produces the GPU allocation that can serve the LLM service at\nminimal cost while adhering to the service SLO ( 4 ).\nMélange’s strength stems from two key properties. First, it is heterogeneity-aware. Our analysis\nshows that request size, request rate, and SLOs jointly impact cost efficiency, but their impacts differ\nfor each GPU type. Mélange’s profiling and ILP formulation account for each of these dimensions,\nenabling efficient navigation of heterogeneous GPU types given a service specification. Second,\nMélange is flexible. The inputs ( 1a, 1b) can be flexibly modified to include new generations of\nGPUs or alternative definitions of SLO, ensuring Mélange is effective for diverse services. Further, to\nthe best of our knowledge, Mélange is the first GPU allocation framework that utilizes multiple GPU\ntypes for LLM serving. In summary, this paper makes the following contributions:\n• We analyze three key LLM service characteristics and their influence on GPU cost efficiency:\nrequest size, request rate, and latency SLO (§ 4).\n3Mélange is French for “mixture”\n2\n\n[Image page=2 idx=1 name=Im1.png] Size: 1740x626, Data: 166688 bytes\n\n• We introduce Mélange, an allocation framework that automatically derives the minimal-cost GPU\nallocation for a given LLM service while satisfying an SLO requirement (§ 5).\n• We evaluate Mélange across four GPU types—NVIDIA L4, A10G, A100, and H100. Mélange\nreduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks\n(document-based), and 4-51% in mixed-context workloads (§ 6).\n2 Related Work\n2.1 LLM Inference Optimization\nA significant body of research has focused on optimizing LLM inference efficiency. One stream\nconcentrates on memory optimization, particularly through improved key-value cache reuse [56] and\nmanagement strategies [19]. Another avenue seeks to minimize latency, such as scheduling optimiza-\ntion [51, 1, 46], speculative decoding [20, 18], kernel optimization [8, 40] and early exiting [41, 59].\nAdditional optimizations include quantization [10, 21, 49, 50] and sparsification [9, 52]. Instead of\naltering inference logic, our work assumes a fixed inference engine configuration and concentrates on\nreducing LLM deployment costs by choosing cost-effective GPU instance types.\n2.2 Machine Learning with Cloud Resources\nRecent studies have explored various strategies for reducing the cost of machine learning (ML) infer-\nence or training. Several focus on utilizing spot instances [42, 12, 53, 11], which is complementary to\nour work. Other work targets deployment on heterogeneous resources [5, 6, 30, 26, 27], but focuses\nprimarily on model training rather than serving. Also, lveraging serverless instances for inference\ncost reduction has been examined in [2]. Nonetheless, these prior work predominantly concentrate\non machine learning prior to the advent of LLMs, which we show to have unique characteristics that\nsignificantly impact cost efficiency. More recent studies, such as [25, 15], focus on LLMs, but they\npropose strategies for reducing costs via optimal migration plans and parallelism with heterogeneous\nresources. They do not identify key LLM service characteristics that impact cost efficiency and\nconsider them in GPU deployment, which our work highlights. Another line of work [ 58, 36]\nexplores splitting LLM inference into its two phases (prefill and decode) and performing the two\nphases on separate nodes, perhaps with different GPU types. Our work shows that, even within a\nphase, the best GPU type can change based on LLM service specifications.\n3 Background\n3.1 LLM Request Size Variance\n(a) LLaMA-7B\n85X (b) LLaMA-70B\nFigure 2: Request latency of different input/output lengths on A100-80G.\nUnlike traditional machine learning workloads, LLM tasks exhibit significant variance in request\nsizes, defined by input and output lengths. For example, ResNet [13] requires a fixed-dimension input\n(image size) and generates a fixed-dimension output (classification size). Conversely, transformer-\nbased language models are flexible to support variable-length prompts and produce variable-length\ngeneration sequences. For instance, Figure 10 illustrates the request size distributions of Chatbot\nArena, demonstrating the extensive diversity of request sizes in practical scenarios. As a result, high\n3\n\n[Image page=3 idx=1 name=Im2.png] Size: 1179x1204, Data: 153570 bytes\n\n[Image page=3 idx=2 name=Im1.png] Size: 800x400, Data: 27064 bytes\n\n(a) Equivalent input and output lengths\n (b) Input and output lengths vary independently\nFigure 3: Figure (a) depicts A10G and A100’s relativeT/$ across request sizes. Figure (b) expands\n(a) into separate input and output length dimensions. Tile colors indicate which GPU achieves higher\nT/$, and values represent the percent increase of T/$ relative to the less cost efficient GPU.\nvariance in request sizes introduces significant variation in request latency. As illustrated in Figure 2,\nrequest latency can increase by 110× when the input/output length expands from 25 tokens to 2000\ntokens for the Llama2-7B model served on an A100 GPU. Consequently, it is crucial to recognize\nthat LLM requests, unlike non-autoregressive models, impose varied loads on GPU resources.\n4 GPU Cost Efficiency Analysis\nIn this section, we analyze GPU cost efficiency for LLM services by serving Llama2-7b on NVIDIA\nA100 [32] and A10G [31] as a representative example. We show that GPU cost efficiency is influenced\nby three key LLM service characteristics: request size (§ 4.2), latency SLO (§ 4.3), and request rate\n(§ 4.4). For each characteristic, we demonstrate opportunities to exploit the heterogeneity of GPU\ntypes to increase cost efficiency and reduce deployment cost. Each plot is tagged with the request\nsize, request rate, and SLO used to generate the plot. We use vLLM-0.2.7 as the serving engine [19].\n4.1 Definitions\nService-level Objective (SLO). SLOs are performance targets that define the acceptable quality\nof service, and a specific SLO varies according to the service’s interactivity needs. As in prior\nwork [19, 58, 51], we use the averageTime Per Output Token (TPOT)as our SLO. TPOT is determined\nby dividing request latency by the number of generated tokens. SLOs are application dependent:\nin-line code editors (e.g., GitHub Copilot [28]) require tight latency deadlines to suggest real-time\ncode additions, whereas summarization services may permit additional processing time. There are\nother common definitions of SLO, such as time to first token and request latency, and Mélange is\nflexible to support these and other alternative definitions of SLO.\nCost Efficiency Metric. We use tokens per dollar (T/$) to measure GPU cost efficiency, calculated\nby summing input and output tokens and dividing the total by the GPU’s on-demand rental cost\nfor a given time period. Cost models are orthogonal to Mélange; we chose this cost model for\nits simplicity, but cost efficiency can be computed with alternative formulations without affecting\nMélange’s efficacy. In general, we deriveT/$ by finding the input and output token rates while at the\nhighest GPU saturation for which TPOT still meets a specified SLO.\n4.2 Request Size and Cost Efficiency\nUnlike many traditional DNNs, LLMs exhibit significant variance in model request sizes (input\nand output lengths) [36]. In this section, we show that request size variance influences GPU cost\nefficiency and can even determine which GPU is most cost efficient.\nExperiment: We serve Llama2-7b on A10G and A100 GPUs, and derive each GPU’s T/$ at\nmaximum GPU saturation across a range of request sizes ( Fig. 3a). Interestingly, no single GPU\nconsistently delivers the highest tokens per dollar (T/$) across all request sizes. Instead, both GPUs\nare most cost efficient in separate regions of the request size spectrum. For smaller request sizes,\n4\n\n[Image page=4 idx=1 name=Im4.png] Size: 1634x974, Data: 216242 bytes\n\n[Image page=4 idx=2 name=Im5.png] Size: 878x564, Data: 173530 bytes\n\n(a) Absolute batch sizes\n (b) Dollar-normalized batch sizes\nFigure 4: (a) depicts the absolute batch sizes of A10G and A100 serving Llama2-7b at maximum\nsaturation, (b) reports the same batch sizes divided by GPU cost, plotting with respect to A10G.\n(a) Best GPU relative to second best GPU\n (b) Best GPU relative to worst GPU\nFigure 5: Comparison of L4, A10G, A100, and H100. Tile colors indicates the GPU with greatest\nT/$. (a) tile values are the T/$ %-increase of the best GPU compared to the second best for that tile.\n(b) compares the best GPU to the worst GPU. In black boxes, only A100 and H100 are compared.\n.\nA10G exhibits up to 2.6× greater T/$ than A100. Conversely, for larger request sizes, A100 achieves\nup to 1.5× the cost efficiency of A10G.\nWe extend this exploration to show the separate impacts of input and output lengths on T/$ ( Fig. 3b).\nEach dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and\nlarger sizes are best served on A100. Note that the difference can be significant, as using a single\nGPU type to serve requests across the entire request size space misses opportunities to produce up to\n72% more output tokens for the same cost. This reveals the opportunity to use a mix of GPU types to\nserve requests for which they are most cost effective.\nSource of Cost Efficiency Gains: To isolate how request size influences relative cost efficiency, we\nexamine request size’s effects on batch size, which serves as a proxy for throughput. Fig. 4 depicts\nabsolute batch sizes and batch sizes normalized by instance cost of each GPU at maximum saturation.\nA10G and A100 have similar cost-normalized batch sizes at 250 input/output tokens, but as the\nrequest size increases to 2K input/output tokens, A10G’s absolute batch size decreases by9× whereas\nA100’s only decreases by6× due to its superior memory size and bandwidth. As a result, A100’s\ncost efficiency advantage over A10G increases with the increase in request size. In contrast, reducing\nthe size from 250 to 25 input/output tokens expands A10G’s batch size by15.2×, whereas A100’s\ngrowth is 5.89×.Because A100’s batch sizes are larger, A100 is more significantly constrained by\nper-request latency overheads (e.g., due to interference of prefill and decode [14]) As a result, A10G’s\ncost-normalized batch size exceeds A100’s at short request lengths, leading to greater overallT/$.\nOther Hardware and Model Size We extend our analysis to more GPU types and a larger model\nvariant (Llama2-70b). Fig. 5 depicts the relative cost efficiency across four GPU types. Once again,\nas request sizes increase, we observe a progression of the most cost efficient GPU from lower-end\nto higher-end GPUs, matching our observations above. Similar trends are observed in the larger\nLlama2-70B model when comparing H100 and A100 GPUs, as detailed in Fig. 8.\nKey Takeaways: There is no universally most cost-efficient GPU for a given LLM. Instead, GPU cost\nefficiency is highly dependent on request sizes. Lower-end GPUs are more cost-effective for small\n5\n\n[Image page=5 idx=1 name=Im6.png] Size: 1520x808, Data: 214520 bytes\n\n[Image page=5 idx=2 name=Im7.png] Size: 1662x942, Data: 239889 bytes\n\n[Image page=5 idx=3 name=Im8.png] Size: 1942x1340, Data: 496522 bytes\n\n[Image page=5 idx=4 name=Im9.png] Size: 1828x1268, Data: 503486 bytes\n\nrequest sizes whereas higher-end GPUs are best for large request sizes. These findings generalize to\nsettings with more GPU types and larger model sizes.\n4.3 SLO and Cost Efficiency\nFigure 6: T/$ comparison between A10G\nand A100 across a range of TPOT SLO\nparameters.\n Figure 7: Relative increase in T/$ when combining\nSLO and request size.\nIn this section, we examine the impact of TPOT SLOs on GPU cost efficiency and highlight the joint\neffects of SLO and request size.\nExperiment: We serve Llama2-7b on A10G and A100 and measure T/$ by maximally saturating\neach GPU while keeping TPOT below SLO, repeating this across several TPOT deadlines ( Fig. 6).\nUnder tight SLO constraints ( <60ms), A100 demonstrates significantly greater T/$ than A10G\n(2×). A10G’s higher processing latency restricts the throughput it can achieve within a tight TPOT\ndeadline, while A100 maintains much higher throughput even at low latency. However, as the SLO\ngradually loosens (60-160ms), A10G’s higher latency is less problematic, dramatically increasing its\nT/$ and surpassing that of A100 (by > 40%). Importantly, this example uses a small request size (64\ninput/output tokens), which was shown in § 4.2 to be best served on A10G. However, a tight SLO\ndegrades A10G’s cost efficiency much more severely than A100’s and pushes the advantage to A100,\nexemplifying the tight interplay between SLO and request size explored further below.\nSLO and Request Size Interplay: Fig. 7 presents relative cost efficiency between A10G and A100\nfor a broad range of TPOT SLOs and request sizes. At tight SLOs (40-60ms), A100 always has\nhigher T/$ (up to 2×). At 80ms, A10G begins showing modest benefit over A100 for small request\nsizes. Finally, at 100-160ms, A10G demonstrates much greater T/$ advantage over A100 for the\nsame request sizes (up to 1.5×), yet A100 is always more cost efficient for larger requests. As\ndemonstrated, a modification to TPOT SLO shifts the boundary within the request size space between\nwhich different GPU types are most cost effective and significantly influences the magnitude of cost\nefficiency differences between GPUs. As a result, both request size and SLO must be considered in\ntandem when determining cost efficiency.\nKey Takeaways: To meet strict SLOs, expensive GPUs are necessary due to the higher latency of\ncheaper GPUs. However, as SLO is loosened, lower-end GPUs can be used to cut deployment costs.\nFigure 8: T/$ comparison between H100x2 and\nA100x2 serving Llama2-70b.\nFigure 9: GPU on-demand cost for three GPU\nprovisioning strategies.\n6\n\n[Image page=6 idx=1 name=Im10.png] Size: 1194x602, Data: 128592 bytes\n\n[Image page=6 idx=2 name=Im11.png] Size: 1140x650, Data: 244436 bytes\n\n[Image page=6 idx=3 name=Im12.png] Size: 1418x872, Data: 328265 bytes\n\n[Image page=6 idx=4 name=Im13.png] Size: 1474x946, Data: 202403 bytes\n\n4.4 Request Rate and Cost Efficiency\nIn this section, we investigate the relationship between request rate and GPU cost efficiency.\nExperiment: Fig. 9 illustrates the cost of serving Llama2-7b at a range of request rates using three\nstrategies: A10G-only, A100-only, or a mix of both. The y-axis is absolute cost instead of T/$\nbecause each provisioning strategy serves the same request rates and thus the same number of tokens;\nonly the cost varies across strategies.\nAs request rate increases, A100-only is increasingly more cost effective than A10G-only. This is\nbecause the requests are of size [1000 in tokens, 250 out tokens], which § 4.2 shows is more cost\neffective on A100. However, A10G-only still presents benefits at low request rates (0-1 req/s). Periods\nof idleness or low activity are common in real-world services [38], and the service should right-size\nto a cheaper GPU (here, A10G) when a higher-end GPU (here, A100) is drastically underutilized.\nMixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs\nconsistently yields the lowest deployment cost. Because A100s have such large capacity, scaling\nwith only A100s is coarse-grained and often leads to underutilized resources. Instead, A10Gs and\nA100s can be mixed such that A100s satisfy the bulk of the service demands, while A10Gs handle\nthe remaining load at reduced cost. Fig. 9 highlights a case where using 2 A100s and 1 A10G results\nin a 24% cost saving over A100-only and 31% over A10G-only.\nKey Takeaways: During low activity periods, LLM services should right-size to cheaper low-end\nGPUs. Provisioning a mix of GPU types enables finer-grained resource scaling, which better aligns\nthe allocated GPU capacity with request load. This increases GPU utilization and consistently\nachieves lowest serving cost.\n5 Mélange: Automating Cost-Efficient GPU Selection\nBuilding on the observations in § 4 that request size, request rate, and SLO all jointly determine\nGPU cost efficiency, we present Mélange, an allocation framework that considers each of these three\ndimensions in-tandem to derive the minimal-cost GPU allocation that meets an LLM service’s request\nload while adhering to SLO constraints. Fig. 1 depicts the Mélange framework. Mélange flexibly\nsupports any GPU type ( 1a) and LLM service definition (1b), uses a one-time offline profiling step\nto measure GPU performance ( 2 ), formulates the task of GPU allocation as a bin packing problem\n( 3 ), then computes the minimal-cost GPU allocation ( 4 ).\n5.1 Problem Formulation\nWe begin by defining the key terms utilized in our problem formulation and solution. An LLM service\nworkload is characterized by its overall request rate along with a distribution of input and output\nsizes. A distribution of request sizes is used rather than fixed values due to the inherent variability of\nLLM request sizes. Specifically, a workload is a histogram where each bucket corresponds to a range\nof request sizes and a bucket’s value is the request rate of requests within the bucket’s size range.\nThe service cost is computed by summing the hourly on-demand cloud renatl rates for each of the\nselected GPUs. We define SLO based on average TPOT, however, Mélange can be extended to other\ndefinitions of SLO such as time to first token (TTFT).\nProblem Definition: Given a workload, GPU costs, and SLO requirements, our objective is to\nprovision GPUs that can minimize deployment cost while adhering to latency SLO constraints.\n5.2 Inputs\nMélange takes as input the set of available GPU types ( 1a) and the LLM service definition ( 1b)\nmade up of the workload profile and SLO. Each of these inputs can be modified, such as adding a\nnew hardware accelerator or redefining SLO based on end-to-end request latency, and Mélange’s\ndownstream components still derive the minimal-cost allocation. Due to the large diversity of\nhardware accelerators and LLM services, Mélange’s extensibility is critical for usability.\n7\n\n5.3 Offline Profiling\nA one-time offline profiling step (2 ) is required to measure the performance of each GPU. For each\nrequest size bucket in the workload histogram, we gradually increase the request rate until the GPU is\nsaturated. We record per-request TTFT and TPOT as the request rate is increased, which are sufficient\nmetrics to capture the timing behavior of a request end-to-end [22]. Then, given an SLO, Mélange\ncan quickly find the maximum throughput each GPU achieves across request sizes while adhering to\nthe SLO. Empirically, the one-time profiling is not time-consuming (<1hr).\n5.4 Allocation Algorithm\nThe allocation algorithm’s (3 ) objective is to map the workload to a minimal-cost set of GPUs that\nare constrained by adhering to SLO. Our insight is that this task can be formulated as a cost-aware\nvariant of the bin packing problem. Mélange partitions workload buckets into smaller slices for\nfine-grained packing, then assigns the slices (items) to GPUs (bins). We first define a slice (§ 5.4.1),\ncompute the load of a slice (§ 5.4.2), then create the ILP formulation (§ 5.4.3).\n5.4.1 Request Buckets and Slices\nA workload histogram has two dimensions, input length and output length, and each histogram\nbucket’s value is the aggregate request rate for requests within the bucket’s size range. We further\nbreak each bucket down into slices for finer-grained bin packing. A parameter, slice factor, indicates\nthe number of slices that each bucket is divided into. In a setting with a slice factor of 8 and a bucket\nwith a request rate of 4, the bucket would be segmented into 8 slices each corresponding to a request\nrate of 0.5 requests/s. The slice factor can be tuned to reach the desired balance between granularity\nand solution complexity, but we have not found overall performance to be sensitive to slice factor.\n5.4.2 Load\nThe solver requires an estimate of the load of each slice to ensure that a GPU’s capacity is not exceeded\nand SLO is not violated. The load of a slice with request size s and rate r on GPU G is calculated\nas r\nM axT put(G,s,SLO), where M axT put(G, s, SLO) is the maximum request/s G can achieve for\nrequests of size s while adhering to SLO. For instance, if M axT put(G, s, SLO) = 10 reqs/s and\nr = 1, the load is calculated as 1/10 = 0 .1 . Each GPU’s maximum capacity is defined as 1. This\napproximation allows us to calculate the aggregate load of slices with differing sizes and rates. Based\non offline profiling, we compute M axT put(G, s, SLO) for each bucket in the workload histogram.\n5.4.3 ILP Formulation\nWe formulate the ILP with two decision variables. First, let A be a matrix {0, 1}N ×M , where N is\nthe number of slices, and M is the number of GPU types. Ai,j = 1 if slice i is assigned to GPU\ntype j, and 0 otherwise. The second decision variable, B, is a vector ZM\n≥0 of non-negative integers,\nwhere Bj specifies the number of GPUs of type j to be allocated. L is a matrix of size N × M where\nLi,j ∈ [0, 1] is the fractional load of slice i on GPU type j. L is computed offline by the process\ndescribed in § 5.4.2. cj denotes the cost of GPU type j.\nOur objective is to minimize the total GPU\nallocation cost:\nThe ILP constraints are as follows. First, each\ntask slice is assigned to exactly one GPU type:\nSecond, for each GPU type, the number of\nGPUs designated in vector B must satisfy the\ncumulative load prescribed to it in matrix A:\nLastly, elements of matrix A are binary, and\nelements of vector B are non-negative:\narg min\nB\n(\nMX\nj=1\nBj · cj) (1)\n∀i ∈ {1, . . . , N},\nMX\nj=1\nAi,j = 1 (2)\n∀j ∈ {1, . . . , M},\nNX\ni=1\nAi,j · Li,j ≤ Bj (3)\n∀i, ∀j, A i,j ∈ {0, 1} (4)\n∀j ∈ {1, . . . , M}, B j ≥ 0 (5)\nThe solution is computed using an off-the-shelf solver [29]. Upon solution, the decision variable B\nholds the minimal-cost GPU allocation ( 4 ) that meets the workload demand and adheres to SLO.\n8\n\n6 Evaluation\nWe assess Mélange’s performance across diverse hardware, request sizes, rates, and SLOs. Mélange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n6.1 Experiment Setup\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons,\nwe normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We\ncalculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then\nadjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) × 3.67 = $7 .516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].\nType L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A. N.A.\nMemory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\nDatasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios.\nFor short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.\nThe input and output length distributions are shown in Fig. 10. We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable. Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.\n0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.\n0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.\nFigure 10: Dataset input and output length distributions.\nMélange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of\n60 · 8 = 480 slices.\nBaselines. We compare Mélange to allocations that use a single GPU type. To derive baseline\nallocations, we use Mélange’s ILP formulation (§ 5.4.3) but restrict the solver to a single GPU type.\n9\n\n6.2 Cost Savings Analysis\nWe compare the deployment costs of Mélange to the single-GPU-type baselines across datasets and\nSLOs. Fig. 11 displays costs normalized against the cost of Mélange (purple dotted lines), and the\ndetailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only\nbaselines are only included for the Arena dataset because the PubMed and Mixed datasets contain\nlarge requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in\nMélange’s allocation but are limited to serving requests smaller than12, 000 tokens. We now discuss\neach dataset in detail:\nH100A100A10GL4Mélange\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n2\n4Cost (w.r.t Mélange)\n(a) Arena, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0\n1.5Cost (w.r.t Mélange) (b) PubMed, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (c) Mixed, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2\n3Cost (w.r.t Mélange)\n(d) Arena, SLO = 40ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0Cost (w.r.t Mélange) (e) PubMed, SLO = 40ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (f) Mixed, SLO = 40ms.\nFigure 11: Deployment cost across different datasets and SLOs.\n• Short-context Dataset (Arena). In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction\n(120ms SLO) and 9-68% reduction (40ms SLO). For both SLOs, L4/A10G are more cost efficient\nthan A100/H100 at low request rates because they achieve greater utilization. For example, at\n1-2 req/s, H100 is significantly underutilized and incurs exorbitant costs. However, as the rate\nincreases, L4/A10G’s cost advantage reduces as A100/H100 are better utilized. Further, with a\n120ms SLO, L4/A10G remain competitive with A100 even at higher request rates due to their T/$\nadvantage for smaller request sizes (which the Arena dataset is skewed towards). Conversely, with\na 40ms SLO, A10G/L4 show much higher relative costs due to their increased latency, requiring\nmore instances to meet the tight deadline. Mélange adapts by allocating more L4/A10G at 120ms\nSLO and more A100 at 40ms SLO, consistently reducing overall cost.\n• Long-context Dataset (PubMed). In Figs. 11b and 11e, Mélange achieves 15-33% cost reduction\n(120ms SLO) and 2-22% reduction (40ms SLO). A100 generally achieves higher T/$ for the\nrequest sizes in PubMed, evidenced by the 120ms setting where A100-only is consistently cheaper\nthan H100-only. However, when SLO tightens to 40ms, H100 is the clear winner due to H100’s\nlower inference latency. Again, Mélange adapts to these dynamics by allocating a greater share of\nA100s at a looser SLO, and more H100s as the SLO is tightened.\n• Mixed-context Dataset. In Figs. 11c and 11f, Mélange achieves 13-51% cost reduction (120ms\nSLO) and 4-51% reduction (40ms SLO). Compared to the PubMed workload, A100-only has much\ngreater cost efficiency in the Mixed workload than H100 due to a greater portion of short-context\nrequests, for which A100 achieves greater T/$. Mélange capitalizes by using more A100 than\nH100, but it also uses L4/A10Gs for small requests, enabling even further cost reduction.\nTakeaways. These results exemplify the core observations from § 4, which show that request size,\nSLO, and request rate all jointly determine cost efficiency. As any of these LLM service characteristics\nvary, Mélange flexibly adjusts its GPU allocation and mixes GPU types to exploit their heterogeneity.\nThis consistently delivers the most cost efficient allocation across each evaluated dataset with both\nstrict (40ms) and loose (120ms) SLOs, achieving up to a 77% cost reduction.\n10\n\n6.3 SLO Satisfaction\nFigure 12: Mélange TPOT CDFs.\nNext, we assess Mélange adherence to TPOT\nSLOs. We provision cloud GPU instances based\non Mélange’s allocation for each dataset and SLO\nat a rate of 4 req/s. We deploy Llama-2-7b on\neach GPU and sample requests randomly from\nthe chosen dataset to serve 2K total requests. We\nrecord the average TPOT for each request.\nLoad Balancer. A load balancer (LB) is required\nto balance requests across GPUs. Our LB design\nis detailed in Appendix A.2. In short, the LB uses\npreviously-served requests to estimate the output\nlength of a new request, which is then routed to\na GPU based on a weighted random selection.\nWeights are computed based on each GPU’s performance for the request’s estimated size.\nResults. Fig. 12 presents CDFs of the observed per-request average TPOTs across experiments.\nWith an SLO of 120ms, over 99.95% of requests met SLO. When the SLO was tightened to 40ms,\n99.5% of requests met SLO. These results validate Mélange’s ability to choose GPU allocations that\nmeet workload demand, however, we recognize that services may require even higher SLO adherence,\nso we investigated the source of SLO violations in our experiment.\nSLO Violation Investigation. 84% of our experiment’s SLO violations were due toa) request rate\nbursts or b) co-location with large requests. We send requests by a Poisson process, which occasionally\ncreates short-lived bursts that overload GPU capacity. Further, we randomly sample request sizes from\nthe chosen dataset. Occasionally, a series of large requests are chosen in sequence and temporarily\nexceed service capacity. In an online production environment, resource over-provisioning is used to\nabsorb such bursts and other load variations. In Mélange, a desired over-provisioning rate (e.g., 10%)\ncan be achieved by increasing the request rate input to the solver by the same proportion.\n6.4 Solver Time\nWe detail the solver execution time in Tab. 2. Across all datasets and request rates, the solver’s\nexecution time remains under 1.2 seconds, which is negligible compared to service lifetime. We\nobserve a modest increase in solver time with higher request volumes due to greater complexity in\nslice assignment. However, this increase is empirically sub-linear relative to the increase in request\nrate, and the solver’s execution time remains practical.\n7 Limitations and Conclusion\nLimitations. Mélange derives the optimal GPU allocation for a fixed workload distribution and\nrequest rate, but does not address other deployment challenges such as GPU unavailability or auto-\nscaling for dynamic request rates and size distributions. Mélange is only intended to make allocation\ndecisions, a key component to be plugged into a broader serving system that handles these deployment\nchallenges. Given the vast number of LLM deployment configurations (quantization and compression,\ndisaggregated prefill, speculative decoding), we have not exhaustively evaluated each setting. We\nexpect, however, that Mélange’s framework is flexible to support each of these settings.\nConclusion. We introduce Mélange, a framework for deriving the minimal-cost GPU allocation for a\ngiven LLM service. Mélange is based on our analysis of GPU cost efficiency, which identifies three\nkey service characteristics (request sizes, request rates, and SLOs) as significant influences on cost\nefficiency. We formulate the GPU allocation task as a cost-aware bin packing problem that accounts\nfor each service characteristic, enabling flexibility and heterogeneity-awareness. In evaluations on a\nrange of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates\nsignificant reductions in deployment costs (up to 77%) while providing high SLO attainment.\n11\n\n[Image page=11 idx=1 name=Im23.png] Size: 1042x481, Data: 86785 bytes\n\nReferences\n[1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and\nRamachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked\nprefills. arXiv preprint arXiv:2308.16369, 2023.\n[2] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni. Optimizing inference serving on\nserverless platforms. Proceedings of the VLDB Endowment, 15(10), 2022.\n[3] AnyScale. Anyscale: Llmperf leaderboard. https://github.com/ray-project/\nllmperf-leaderboard, 2024. [Accessed 13-03-2024].\n[4] AWS. Ai accelerator-aws trainium. https://aws.amazon.com/machine-learning/\ntrainium/, 2020. [Accessed 14-03-2024].\n[5] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem\nChumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning\nof large models. arXiv preprint arXiv:2209.01188, 2022.\n[6] Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, and Srinidhi\nViswanatha. Balancing efficiency and fairness in heterogeneous gpu clusters for deep learning.\nIn Proceedings of the Fifteenth European Conference on Computer Systems, pages 1–16, 2020.\n[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian. A discourse-aware attention model for abstractive summarization of\nlong documents. Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short\nPapers), 2018.\n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.Advances in Neural Information Processing\nSystems, 35:16344–16359, 2022.\n[9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned\nin one-shot, 2023.\n[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[11] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma,\nMahmut Taylan Kandemir, and Chita R Das. Cocktail: A multidimensional optimization\nfor model serving in cloud. In 19th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 22), pages 1041–1057, 2022.\n[12] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, and Phillip B Gibbons.\nTributary: spot-dancing for elastic services with latency {SLOs}. In 2018 USENIX Annual\nTechnical Conference (USENIX ATC 18), pages 1–14, 2018.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition, 2015.\n[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao\nFeng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate\nllm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024.\n[15] Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang Yuan. Hexgen: Generative\ninference of foundation model over heterogeneous decentralized environment. arXiv preprint\narXiv:2311.11514, 2023.\n[16] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization\nduring generative inference for higher throughput. Advances in Neural Information Processing\nSystems, 36, 2024.\n12\n\n[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance\nanalysis of a tensor processing unit. In Proceedings of the 44th annual international symposium\non computer architecture, pages 1–12, 2017.\n[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir\nGholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural\nInformation Processing Systems, 36, 2024.\n[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems\nPrinciples, pages 611–626, 2023.\n[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\nspeculative decoding. In International Conference on Machine Learning, pages 19274–19286.\nPMLR, 2023.\n[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\n[22] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.\nAndes: Defining and enhancing quality-of-experience in llm-based text streaming services.\narXiv preprint arXiv:2404.16283, 2024.\n[23] Liang Luo, Peter West, Pratyush Patel, Arvind Krishnamurthy, and Luis Ceze. Srifty: Swift and\nthrifty distributed neural network training on the cloud. Proceedings of Machine Learning and\nSystems, 4:833–847, 2022.\n[24] Yusuf Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot\nfor the web, 2023. Accessed: 2024-02-21.\n[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia.\nSpotserve: Serving generative large language models on preemptible instances. arXiv preprint\narXiv:2311.15566, 2023.\n[26] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia. Sdpipe: A semi-decentralized\nframework for heterogeneity-aware pipeline-parallel training. Proceedings of the VLDB Endow-\nment, 16(9):2354–2363, 2023.\n[27] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui.\nGalvatron: Efficient transformer training over multiple gpus using automatic parallelism. arXiv\npreprint arXiv:2211.13878, 2022.\n[28] Microsoft. Copilot, 2023. Accessed: 2024-02-21.\n[29] Stuart Mitchell. PuLP: A linear programming toolkit for python. https://github.com/\ncoin-or/pulp, 2023. Accessed: 2024-02-25.\n[30] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei\nZaharia. {Heterogeneity-Aware} cluster scheduling policies for deep learning workloads. In\n14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages\n481–498, 2020.\n[31] Nvidia. A10 gpu spec, 2024. Accessed: 2024-03-10.\n[32] Nvidia. A100 gpu spec, 2024. Accessed: 2024-03-10.\n[33] Nvidia. Gpus, 2024. Accessed: 2024-03-10.\n[34] OpenAI. Chatgpt, 2022. Accessed: 2024-02-21.\n[35] OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.\n13\n\n[36] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and\nRicardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. arXiv\npreprint arXiv:2311.18677, 2023.\n[37] Elizabeth Reid. Supercharging search with generative ai, 2023. Accessed: 2024-02-21.\n[38] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. {INFaaS}: Auto-\nmated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), pages 397–411, 2021.\n[39] RunPod. Runpod, 2024. Accessed: 2024-02-24.\n[40] FlashInfer team. Accelerating self-attentions for llm serving with flashinfer, 2024. Accessed:\n2024-02-24.\n[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd international conference on pattern\nrecognition (ICPR), pages 2464–2469. IEEE, 2016.\n[42] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang,\nRavi Netravali, and Guoqing Harry Xu. Bamboo: Making preemptible instances resilient for\naffordable training of large {DNNs}. In 20th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 23), pages 497–513, 2023.\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[45] Abhi Venigalla. Databricks: Training llms at scale with amd mi250 gpus. https://www.\ndatabricks.com/blog/training-llms-scale-amd-mi250-gpus , 2023. [Accessed 14-\n03-2024].\n[46] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast\ndistributed inference serving for large language models. arXiv preprint arXiv:2305.05920,\n2023.\n[47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and\nChi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\n[48] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard\nPeng, Qingyun Wu, and Chi Wang. An empirical study on challenging math problem solving\nwith gpt-4. In ArXiv preprint arXiv:2306.01337, 2023.\n[49] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\nSmoothquant: Accurate and efficient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages 38087–38099. PMLR, 2023.\n[50] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.\nAdvances in Neural Information Processing Systems, 35:27168–27183, 2022.\n[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca:\nA distributed serving system for {Transformer-Based} generative models. In 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n[52] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\nago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems, 33:17283–17297,\n2020.\n14\n\n[53] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan.{MArk}: Exploiting cloud services\nfor {Cost-Effective},{SLO-Aware} machine learning inference serving. In 2019 USENIX\nAnnual Technical Conference (USENIX ATC 19), pages 1049–1062, 2019.\n[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao\nSong, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient\ngenerative inference of large language models. Advances in Neural Information Processing\nSystems, 36, 2024.\n[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi\nCao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large\nlanguage models using sglang. arXiv preprint arXiv:2312.07104, 2023.\n[57] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response\nlength perception and sequence scheduling: An llm-empowered llm inference pipeline. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n[58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving. arXiv preprint arXiv:2401.09670, 2024.\n[59] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses\npatience: Fast and robust inference with early exit. Advances in Neural Information Processing\nSystems, 33:18330–18341, 2020.\n[60] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao.\nTowards optimal caching and model selection for large model inference. Advances in Neural\nInformation Processing Systems, 36, 2024.\nA Experiment Setup\nA.1 Dataset\nWe test Mélange’s performance on three different datasets listed below:\n• Short context: This scenario simulates real-time conversational dynamics by employing the\nChatbot Arena dataset (lmsys/lmsys-chat-1m) [55], which is derived from real-world\nchatbot conversations. The dataset is skewed towards shorter context ( < 2000 tokens)\nbecause much of the data was generated in conversation with models that did not yet have a\nlarger context window.\n• Long context: This scenario represents tasks with extensive input, such as summariza-\ntion. We utilize the PubMed dataset ( ccdv/pubmed-summarization) [7], comprising\n133 thousand scientific papers from PubMed.com, a popular dataset for large-scale text\nsummarization studies.\n• Mixed long/short context: This scenario captures settings with a combination of long and\nshort context, such as an assistant that engages in succinct dialogue and responds to large\ndocument-based queries. To model this, we create a synthetic dataset by sampling 80% of\nrequests from the Arena dataset and 20% of requests from the PubMed dataset.\nA.2 Load Balancer\nThe load balancer (LB) policy used in our evaluations in § 6.3 is as follows. For each input length\nbucket range (§ 5.4.1), the LB tracks the average of all previously-seen output lengths. Upon receiving\na new request, the LB uses this average as an estimate for the new request’s output length, allowing the\nLB to identify the specific request size bucket the request belongs in. The LB then makes a weighted\nrandom selection of a GPU backend to forward the request to. A GPU’s weights are computed based\n15\n\non the proportion of the GPU’s maximum throughput for request sizes of the new request’s bucket to\nthe aggregate throughput of all GPUs. This is a simple policy we use to demonstrate the efficacy of\nMélange, and leave it as future work to develop load balancers for serving LLMs on heterogeneous\nGPUs.\nB Solver Time\nWe present the solver execution time from each experiment in Table 2.\nRequest RateArena, SLO=120msArena, SLO=40msPubMed, SLO=120msPubMed, SLO=40msMix, SLO=120msMix, SLO=40ms\n1 0.137 0.177 0.232 0.295 0.168 0.3362 0.194 0.265 0.234 0.334 0.253 0.3814 0.192 0.346 0.287 0.381 0.297 0.4598 0.248 0.433 0.269 0.384 0.321 0.54516 0.299 0.448 0.389 0.509 0.439 0.53732 0.316 0.494 0.791 0.96 0.912 1.14\nTable 2: Solver execution time.\nC Instance Allocations\nWe present the instance allocations for each experiment in the tables below.\n16\n\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 1 1.71 N/A\nH100-only 1 7.516 77.25%\nA100-only 1 3.67 53.41%\nA10G-only 2 2.02 15.35%\nL4-only 3 2.1 18.57%\n2 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n4 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-only 2 7.34 40.46%\nA10G-only 6 6.06 27.89%\nL4-only 9 6.3 30.63%\n8 Mélange 1 3 1 7.4 N/A\nH100-only 2 15.032 50.77%\nA100-only 3 11.01 32.79%\nA10G-only 11 11.1 33.39%\nL4-only 17 11.9 37.82%\n16 Mélange 2 2 3 14.43 N/A\nH100-only 4 30.064 52.00%\nA100-only 6 22.02 34.47%\nA10G-only 20 20.2 28.56%\nL4-only 33 23.1 37.53%\n32 Mélange 2 6 5 25.81 N/A\nH100-only 8 60.128 57.07%\nA100-only 9 33.03 21.86%\nA10G-only 39 39.39 34.48%\nL4-only 65 45.5 43.27%\nTable 3: Instance allocations for the short-context Arena dataset, SLO=120ms.\n17\n\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 1 11.186 N/A\nH100-only 2 15.032 25.59%\nA100-Only 4 14.68 23.80%\n2 Mélange 3 1 2 21.732 N/A\nH100-only 4 30.064 27.71%\nA100-Only 7 25.69 15.41%\n4 Mélange 3 4 3 40.258 N/A\nH100-only 8 60.128 33.05%\nA100-Only 14 51.38 21.65%\n8 Mélange 7 7 78.302 N/A\nH100-only 14 105.224 25.59%\nA100-Only 27 99.09 20.98%\n16 Mélange 12 15 156.78 N/A\nH100-only 28 210.448 25.50%\nA100-Only 53 194.51 19.40%\n32 Mélange 1 1 20 32 315.622 N/A\nH100-only 55 413.38 23.65%\nA100-Only 106 389.02 18.87%\nTable 4: Instance allocations for the long-context PubMed dataset, SLO=120ms.\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-Only 1 3.67 0%\n2 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-Only 2 7.34 40.46%\n4 Mélange 2 1 9.536 N/A\nH100-only 2 15.032 36.56%\nA100-Only 3 11.01 13.39%\n8 Mélange 1 2 1 1 13.906 N/A\nH100-only 3 22.548 38.33%\nA100-Only 5 18.35 24.22%\n16 Mélange 1 2 3 2 28.762 N/A\nH100-only 6 45.096 36.22%\nA100-Only 10 36.7 21.63%\n32 Mélange 1 5 6 4 57.834 N/A\nH100-only 12 90.192 35.88%\nA100-Only 20 73.4 21.21%\nTable 5: Instance allocations for the mixed context dataset, SLO=120ms.\n18\n\nRate Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n2 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\nA10G-only 5 5.05 27.33%\nL4-only 9 6.3 41.75%\n4 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\nA10G-only 10 10.1 46.73%\nL4-only 17 11.9 54.79%\n8 Mélange 1 1 2 9.05 N/A\nH100-only 3 15.032 39.80%\nA100-only 3 11.01 17.80%\nA10G-only 16 16.16 44.00%\nL4-only 34 23.8 61.97%\n16 Mélange 6 3 17.07 N/A\nH100-only 4 30.064 43.22%\nA100-only 6 22.02 22.48%\nA10G-only 40 40.4 57.75%\nL4-only 68 47.6 64.14%\n32 Mélange 8 6 30.1 N/A\nH100-only 7 52.612 42.79%\nA100-only 9 33.03 8.87%\nA10G-only 80 80.8 62.75%\nL4-only 135 94.5 68.15%\nTable 6: Instance allocations for the short-context Arena dataset, SLO=40ms.\n19\n\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 4 14.68 N/A\nH100-only 2 15.032 2.34%\nA100-Only 4 14.68 0.00%\n2 Mélange 1 3 26.218 N/A\nH100-only 4 30.064 12.79%\nA100-Only 9 33.03 20.62%\n4 Mélange 3 5 48.59 N/A\nH100-only 7 52.612 7.64%\nA100-Only 17 62.39 22.12%\n8 Mélange 3 12 101.202 N/A\nH100-only 14 105.224 3.82%\nA100-Only 34 124.78 18.90%\n16 Mélange 11 21 198.206 N/A\nH100-only 28 210.448 5.82%\nA100-Only 67 245.89 19.39%\n32 Mélange 24 40 388.72 N/A\nH100-only 56 420.896 7.64%\nA100-Only 133 488.11 20.36%\nTable 7: Instance allocations for the long-context PubMed dataset, SLO=40ms.\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\n2 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\n4 Mélange 3 1 10.546 N/A\nH100-only 2 15.032 29.84%\nA100-only 3 11.01 4.21%\n8 Mélange 1 3 2 1 18.586 N/A\nH100-only 4 30.064 38.18%\nA100-only 6 22.02 15.59%\n16 Mélange 2 7 2 3 38.358 N/A\nH100-only 7 52.612 27.09%\nA100-only 12 44.04 12.90%\n32 Mélange 15 6 5 74.75 N/A\nH100-only 13 97.708 23.50%\nA100-only 24 88.08 15.13%\nTable 8: Instance allocations for the mixed long/short context dataset, SLO=40ms.\n20", "metadata": {"url": "https://arxiv.org/pdf/2404.14527", "type": "paper", "year": "2024"}, "sections": [{"title": "Page 1", "paragraphs": [{"text": "Mélange: Cost Efficient Large Language Model\nServing by Exploiting GPU Heterogeneity\nTyler Griggs∗\nUC Berkeley\nXiaoxuan Liu∗\nUC Berkeley\nJiaxiang Yu\nNational University of Singapore\nDoyoung Kim\nUC Berkeley\nWei-Lin Chiang\nUC Berkeley\nAlvin Cheung\nUC Berkeley\nIon Stoica\nUC Berkeley\nAbstract\nLarge language models (LLMs) are increasingly integrated into many online ser-\nvices, yet they remain cost-prohibitive to deploy due to the requirement of expensive\nGPU instances. Prior work has addressed the high cost of LLM serving by improv-\ning the inference engine, but less attention has been given to selecting the most\ncost-efficient GPU type(s) for a specific LLM service. There is a large and growing\nlandscape of GPU types and, within these options, higher cost does not always\nlead to increased performance. Instead, through a comprehensive investigation, we\nfind that three key LLM service characteristics (request size, request rate, SLO)\nstrongly influence GPU cost efficiency, and differing GPU types are most cost\nefficient for differing LLM service settings. As a result, the most cost-efficient\nallocation for a given service is typically amix of heterogeneous GPU types. Based\non this analysis, we introduce Mélange, a GPU allocation framework that navigates\nthese diverse LLM service characteristics and heterogeneous GPU option space to\nautomatically and efficiently derive the minimal-cost GPU allocation for a given\nLLM service. We formulate the GPU allocation task as a cost-aware bin packing\nproblem where GPUs are bins and items are slices of the service workload. Our\nformulation’s constraints account for a service’s unique characteristics, allowing\nMélange to be flexible to support diverse service settings and heterogeneity-aware\nto adapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, Mélange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n1 Introduction\nLarge language models (LLMs) [35, 43, 44] are increasingly integrated into many online services,\nincluding search engines [37, 24], chatbots [34], and virtual assistants [28, 47, 48]. These services are\noften hosted by deploying models on cloud resources. However, deploying LLMs is expensive. The\nsubstantial size and computational demands of LLMs require the use of costly hardware accelerators,\ntypically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB\nGPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.\nPrior work [8, 16, 54, 57, 60] addresses the high cost of LLM serving by focusing on inference\nthroughput, but less attention has been given to selecting the most cost-efficient GPU type(s) for a\nspecific LLM service. The large and growing landscape of hardware accelerators — ranging from\nNVIDIA GPUs [33] and AMD GPUs [45] to Google TPUs [17], CPUs [23], and others [4] — offers\n∗Equal contribution\n2For brevity, we use “accelerator” and “GPU” interchangeably in this work.\nPreprint. Under review.\narXiv:2404.14527v4  [cs.DC]  22 Jul 2024", "sentences": [{"text": "Mélange: Cost Efficient Large Language Model\nServing by Exploiting GPU Heterogeneity\nTyler Griggs∗\nUC Berkeley\nXiaoxuan Liu∗\nUC Berkeley\nJiaxiang Yu\nNational University of Singapore\nDoyoung Kim\nUC Berkeley\nWei-Lin Chiang\nUC Berkeley\nAlvin Cheung\nUC Berkeley\nIon Stoica\nUC Berkeley\nAbstract\nLarge language models (LLMs) are increasingly integrated into many online ser-\nvices, yet they remain cost-prohibitive to deploy due to the requirement of expensive\nGPU instances.", "metadata": {}}, {"text": "Prior work has addressed the high cost of LLM serving by improv-\ning the inference engine, but less attention has been given to selecting the most\ncost-efficient GPU type(s) for a specific LLM service.", "metadata": {}}, {"text": "There is a large and growing\nlandscape of GPU types and, within these options, higher cost does not always\nlead to increased performance.", "metadata": {}}, {"text": "Instead, through a comprehensive investigation, we\nfind that three key LLM service characteristics (request size, request rate, SLO)\nstrongly influence GPU cost efficiency, and differing GPU types are most cost\nefficient for differing LLM service settings.", "metadata": {}}, {"text": "As a result, the most cost-efficient\nallocation for a given service is typically amix of heterogeneous GPU types.", "metadata": {}}, {"text": "Based\non this analysis, we introduce Mélange, a GPU allocation framework that navigates\nthese diverse LLM service characteristics and heterogeneous GPU option space to\nautomatically and efficiently derive the minimal-cost GPU allocation for a given\nLLM service.", "metadata": {}}, {"text": "We formulate the GPU allocation task as a cost-aware bin packing\nproblem where GPUs are bins and items are slices of the service workload.", "metadata": {}}, {"text": "Our\nformulation’s constraints account for a service’s unique characteristics, allowing\nMélange to be flexible to support diverse service settings and heterogeneity-aware\nto adapt the GPU allocation to a specific service.", "metadata": {}}, {"text": "Compared to using only a single\nGPU type, Mélange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.", "metadata": {}}, {"text": "1 Introduction\nLarge language models (LLMs) [35, 43, 44] are increasingly integrated into many online services,\nincluding search engines [37, 24], chatbots [34], and virtual assistants [28, 47, 48].", "metadata": {}}, {"text": "These services are\noften hosted by deploying models on cloud resources.", "metadata": {}}, {"text": "However, deploying LLMs is expensive.", "metadata": {}}, {"text": "The\nsubstantial size and computational demands of LLMs require the use of costly hardware accelerators,\ntypically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB\nGPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.", "metadata": {}}, {"text": "Prior work [8, 16, 54, 57, 60] addresses the high cost of LLM serving by focusing on inference\nthroughput, but less attention has been given to selecting the most cost-efficient GPU type(s) for a\nspecific LLM service.", "metadata": {}}, {"text": "The large and growing landscape of hardware accelerators — ranging from\nNVIDIA GPUs [33] and AMD GPUs [45] to Google TPUs [17], CPUs [23], and others [4] — offers\n∗Equal contribution\n2For brevity, we use “accelerator” and “GPU” interchangeably in this work.", "metadata": {}}, {"text": "Preprint.", "metadata": {}}, {"text": "Under review.", "metadata": {}}, {"text": "arXiv:2404.14527v4  [cs.DC]  22 Jul 2024", "metadata": {}}], "metadata": {"page": 1}}], "metadata": {"page": 1}}, {"title": "Page 2", "paragraphs": [{"text": "a wide array of choices with varying performance specifications and on-demand cloud costs. Within\nthese hardware options, higher cost does not always lead to increased performance. To investigate this\nphenomenon further, we examine GPU cost efficiency, defined based on common pricing models [34]\nas the number of input and output tokens processed per dollar cost (T/$) of on-demand cloud GPUs.\nWe find that GPU cost efficiency is determined by three key LLM service characteristics:\n1. Request Size: An LLM request’s size is made up of its input and output token lengths. For small\nrequest sizes, lower-end GPUs generally produce greater T/$ than high-end GPUs.\n2. Request Rate: To maximize utilization, provisioned GPU capacity should align with request\nvolume. At low request rates, services can reduce costs by right-sizing from expensive high-end\nGPUs to cheap low-end GPUs. Further, leveraging a mix of GPU types facilitates finer-grained\nresource scaling to better match request volume.\n3. Service-level Objective: Services typically establish latency SLOs to ensure service quality.\nBecause low-end GPUs generally incur higher latency than high-end GPUs, high-end GPUs are\nrequired for stringent SLOs while low-end GPUs can reduce costs in loose-SLO settings.\nConsider a GPU allocation strategy that integrates each of the three observations above: high-cost\nA100 GPUs handle large requests and meet stringent SLOs, but lower-cost A10G GPUs serve\nsmaller requests (1) and looser SLOs (3) at higher T/$. Then, during periods of low service activity,\nthe service right-sizes to the even-cheaper L4 GPU to maintain service availability at lowest cost\n(2). Consequently, we find that GPU heterogeneity presents opportunities for increasing GPU cost\nefficiency, but such opportunities are highly dependent on LLM service characteristics. The key\nchallenge, then, is creating a GPU allocation framework that can navigate the diversity of LLM\nservices (request sizes, request rates, latency SLOs) and GPU types to find the optimal GPU allocation.\nFigure 1: Mélange framework.\nWe present Mélange3 (Fig. 1), a GPU allocation framework that derives the minimal-cost GPU\nallocation for a given LLM service. In Mélange, each GPU type ( 1a) passes through a one-time\noffline profiling step (2 ) to measure GPU performance across request sizes and rates. Then, given\nthe profiling results and an LLM service definition (1b), Mélange’s objective is to choose a GPU\nallocation for the service workload that minimizes cost. This task is a natural application of the\ncost-aware bin packing problem, where bins are GPUs and items are slices of the workload. We\nformulate the problem as an integer linear program (ILP) and efficiently solve with an off-the-shelf\nsolver ( 3 ). Upon solution, Mélange produces the GPU allocation that can serve the LLM service at\nminimal cost while adhering to the service SLO ( 4 ).\nMélange’s strength stems from two key properties. First, it is heterogeneity-aware. Our analysis\nshows that request size, request rate, and SLOs jointly impact cost efficiency, but their impacts differ\nfor each GPU type. Mélange’s profiling and ILP formulation account for each of these dimensions,\nenabling efficient navigation of heterogeneous GPU types given a service specification. Second,\nMélange is flexible. The inputs ( 1a, 1b) can be flexibly modified to include new generations of\nGPUs or alternative definitions of SLO, ensuring Mélange is effective for diverse services. Further, to\nthe best of our knowledge, Mélange is the first GPU allocation framework that utilizes multiple GPU\ntypes for LLM serving. In summary, this paper makes the following contributions:\n• We analyze three key LLM service characteristics and their influence on GPU cost efficiency:\nrequest size, request rate, and latency SLO (§ 4).\n3Mélange is French for “mixture”\n2", "sentences": [{"text": "a wide array of choices with varying performance specifications and on-demand cloud costs.", "metadata": {}}, {"text": "Within\nthese hardware options, higher cost does not always lead to increased performance.", "metadata": {}}, {"text": "To investigate this\nphenomenon further, we examine GPU cost efficiency, defined based on common pricing models [34]\nas the number of input and output tokens processed per dollar cost (T/$) of on-demand cloud GPUs.", "metadata": {}}, {"text": "We find that GPU cost efficiency is determined by three key LLM service characteristics:\n1.", "metadata": {}}, {"text": "Request Size: An LLM request’s size is made up of its input and output token lengths.", "metadata": {}}, {"text": "For small\nrequest sizes, lower-end GPUs generally produce greater T/$ than high-end GPUs.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Request Rate: To maximize utilization, provisioned GPU capacity should align with request\nvolume.", "metadata": {}}, {"text": "At low request rates, services can reduce costs by right-sizing from expensive high-end\nGPUs to cheap low-end GPUs.", "metadata": {}}, {"text": "Further, leveraging a mix of GPU types facilitates finer-grained\nresource scaling to better match request volume.", "metadata": {}}, {"text": "3.", "metadata": {}}, {"text": "Service-level Objective: Services typically establish latency SLOs to ensure service quality.", "metadata": {}}, {"text": "Because low-end GPUs generally incur higher latency than high-end GPUs, high-end GPUs are\nrequired for stringent SLOs while low-end GPUs can reduce costs in loose-SLO settings.", "metadata": {}}, {"text": "Consider a GPU allocation strategy that integrates each of the three observations above: high-cost\nA100 GPUs handle large requests and meet stringent SLOs, but lower-cost A10G GPUs serve\nsmaller requests (1) and looser SLOs (3) at higher T/$.", "metadata": {}}, {"text": "Then, during periods of low service activity,\nthe service right-sizes to the even-cheaper L4 GPU to maintain service availability at lowest cost\n(2).", "metadata": {}}, {"text": "Consequently, we find that GPU heterogeneity presents opportunities for increasing GPU cost\nefficiency, but such opportunities are highly dependent on LLM service characteristics.", "metadata": {}}, {"text": "The key\nchallenge, then, is creating a GPU allocation framework that can navigate the diversity of LLM\nservices (request sizes, request rates, latency SLOs) and GPU types to find the optimal GPU allocation.", "metadata": {}}, {"text": "Figure 1: Mélange framework.", "metadata": {}}, {"text": "We present Mélange3 (Fig.", "metadata": {}}, {"text": "1), a GPU allocation framework that derives the minimal-cost GPU\nallocation for a given LLM service.", "metadata": {}}, {"text": "In Mélange, each GPU type ( 1a) passes through a one-time\noffline profiling step (2 ) to measure GPU performance across request sizes and rates.", "metadata": {}}, {"text": "Then, given\nthe profiling results and an LLM service definition (1b), Mélange’s objective is to choose a GPU\nallocation for the service workload that minimizes cost.", "metadata": {}}, {"text": "This task is a natural application of the\ncost-aware bin packing problem, where bins are GPUs and items are slices of the workload.", "metadata": {}}, {"text": "We\nformulate the problem as an integer linear program (ILP) and efficiently solve with an off-the-shelf\nsolver ( 3 ).", "metadata": {}}, {"text": "Upon solution, Mélange produces the GPU allocation that can serve the LLM service at\nminimal cost while adhering to the service SLO ( 4 ).", "metadata": {}}, {"text": "Mélange’s strength stems from two key properties.", "metadata": {}}, {"text": "First, it is heterogeneity-aware.", "metadata": {}}, {"text": "Our analysis\nshows that request size, request rate, and SLOs jointly impact cost efficiency, but their impacts differ\nfor each GPU type.", "metadata": {}}, {"text": "Mélange’s profiling and ILP formulation account for each of these dimensions,\nenabling efficient navigation of heterogeneous GPU types given a service specification.", "metadata": {}}, {"text": "Second,\nMélange is flexible.", "metadata": {}}, {"text": "The inputs ( 1a, 1b) can be flexibly modified to include new generations of\nGPUs or alternative definitions of SLO, ensuring Mélange is effective for diverse services.", "metadata": {}}, {"text": "Further, to\nthe best of our knowledge, Mélange is the first GPU allocation framework that utilizes multiple GPU\ntypes for LLM serving.", "metadata": {}}, {"text": "In summary, this paper makes the following contributions:\n• We analyze three key LLM service characteristics and their influence on GPU cost efficiency:\nrequest size, request rate, and latency SLO (§ 4).", "metadata": {}}, {"text": "3Mélange is French for “mixture”\n2", "metadata": {}}], "metadata": {"page": 2}}, {"text": "[Image page=2 idx=1 name=Im1.png] Size: 1740x626, Data: 166688 bytes", "sentences": [{"text": "[Image page=2 idx=1 name=Im1.png] Size: 1740x626, Data: 166688 bytes", "metadata": {}}], "metadata": {"page": 2, "image_index": 1, "image_name": "Im1.png", "image_width": 1740, "image_height": 626, "attachment_type": "image", "has_image_data": true, "image_data_size": 166688}}], "metadata": {"page": 2}}, {"title": "Page 3", "paragraphs": [{"text": "• We introduce Mélange, an allocation framework that automatically derives the minimal-cost GPU\nallocation for a given LLM service while satisfying an SLO requirement (§ 5).\n• We evaluate Mélange across four GPU types—NVIDIA L4, A10G, A100, and H100. Mélange\nreduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks\n(document-based), and 4-51% in mixed-context workloads (§ 6).\n2 Related Work\n2.1 LLM Inference Optimization\nA significant body of research has focused on optimizing LLM inference efficiency. One stream\nconcentrates on memory optimization, particularly through improved key-value cache reuse [56] and\nmanagement strategies [19]. Another avenue seeks to minimize latency, such as scheduling optimiza-\ntion [51, 1, 46], speculative decoding [20, 18], kernel optimization [8, 40] and early exiting [41, 59].\nAdditional optimizations include quantization [10, 21, 49, 50] and sparsification [9, 52]. Instead of\naltering inference logic, our work assumes a fixed inference engine configuration and concentrates on\nreducing LLM deployment costs by choosing cost-effective GPU instance types.\n2.2 Machine Learning with Cloud Resources\nRecent studies have explored various strategies for reducing the cost of machine learning (ML) infer-\nence or training. Several focus on utilizing spot instances [42, 12, 53, 11], which is complementary to\nour work. Other work targets deployment on heterogeneous resources [5, 6, 30, 26, 27], but focuses\nprimarily on model training rather than serving. Also, lveraging serverless instances for inference\ncost reduction has been examined in [2]. Nonetheless, these prior work predominantly concentrate\non machine learning prior to the advent of LLMs, which we show to have unique characteristics that\nsignificantly impact cost efficiency. More recent studies, such as [25, 15], focus on LLMs, but they\npropose strategies for reducing costs via optimal migration plans and parallelism with heterogeneous\nresources. They do not identify key LLM service characteristics that impact cost efficiency and\nconsider them in GPU deployment, which our work highlights. Another line of work [ 58, 36]\nexplores splitting LLM inference into its two phases (prefill and decode) and performing the two\nphases on separate nodes, perhaps with different GPU types. Our work shows that, even within a\nphase, the best GPU type can change based on LLM service specifications.\n3 Background\n3.1 LLM Request Size Variance\n(a) LLaMA-7B\n85X (b) LLaMA-70B\nFigure 2: Request latency of different input/output lengths on A100-80G.\nUnlike traditional machine learning workloads, LLM tasks exhibit significant variance in request\nsizes, defined by input and output lengths. For example, ResNet [13] requires a fixed-dimension input\n(image size) and generates a fixed-dimension output (classification size). Conversely, transformer-\nbased language models are flexible to support variable-length prompts and produce variable-length\ngeneration sequences. For instance, Figure 10 illustrates the request size distributions of Chatbot\nArena, demonstrating the extensive diversity of request sizes in practical scenarios. As a result, high\n3", "sentences": [{"text": "• We introduce Mélange, an allocation framework that automatically derives the minimal-cost GPU\nallocation for a given LLM service while satisfying an SLO requirement (§ 5).", "metadata": {}}, {"text": "• We evaluate Mélange across four GPU types—NVIDIA L4, A10G, A100, and H100.", "metadata": {}}, {"text": "Mélange\nreduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks\n(document-based), and 4-51% in mixed-context workloads (§ 6).", "metadata": {}}, {"text": "2 Related Work\n2.1 LLM Inference Optimization\nA significant body of research has focused on optimizing LLM inference efficiency.", "metadata": {}}, {"text": "One stream\nconcentrates on memory optimization, particularly through improved key-value cache reuse [56] and\nmanagement strategies [19].", "metadata": {}}, {"text": "Another avenue seeks to minimize latency, such as scheduling optimiza-\ntion [51, 1, 46], speculative decoding [20, 18], kernel optimization [8, 40] and early exiting [41, 59].", "metadata": {}}, {"text": "Additional optimizations include quantization [10, 21, 49, 50] and sparsification [9, 52].", "metadata": {}}, {"text": "Instead of\naltering inference logic, our work assumes a fixed inference engine configuration and concentrates on\nreducing LLM deployment costs by choosing cost-effective GPU instance types.", "metadata": {}}, {"text": "2.2 Machine Learning with Cloud Resources\nRecent studies have explored various strategies for reducing the cost of machine learning (ML) infer-\nence or training.", "metadata": {}}, {"text": "Several focus on utilizing spot instances [42, 12, 53, 11], which is complementary to\nour work.", "metadata": {}}, {"text": "Other work targets deployment on heterogeneous resources [5, 6, 30, 26, 27], but focuses\nprimarily on model training rather than serving.", "metadata": {}}, {"text": "Also, lveraging serverless instances for inference\ncost reduction has been examined in [2].", "metadata": {}}, {"text": "Nonetheless, these prior work predominantly concentrate\non machine learning prior to the advent of LLMs, which we show to have unique characteristics that\nsignificantly impact cost efficiency.", "metadata": {}}, {"text": "More recent studies, such as [25, 15], focus on LLMs, but they\npropose strategies for reducing costs via optimal migration plans and parallelism with heterogeneous\nresources.", "metadata": {}}, {"text": "They do not identify key LLM service characteristics that impact cost efficiency and\nconsider them in GPU deployment, which our work highlights.", "metadata": {}}, {"text": "Another line of work [ 58, 36]\nexplores splitting LLM inference into its two phases (prefill and decode) and performing the two\nphases on separate nodes, perhaps with different GPU types.", "metadata": {}}, {"text": "Our work shows that, even within a\nphase, the best GPU type can change based on LLM service specifications.", "metadata": {}}, {"text": "3 Background\n3.1 LLM Request Size Variance\n(a) LLaMA-7B\n85X (b) LLaMA-70B\nFigure 2: Request latency of different input/output lengths on A100-80G.", "metadata": {}}, {"text": "Unlike traditional machine learning workloads, LLM tasks exhibit significant variance in request\nsizes, defined by input and output lengths.", "metadata": {}}, {"text": "For example, ResNet [13] requires a fixed-dimension input\n(image size) and generates a fixed-dimension output (classification size).", "metadata": {}}, {"text": "Conversely, transformer-\nbased language models are flexible to support variable-length prompts and produce variable-length\ngeneration sequences.", "metadata": {}}, {"text": "For instance, Figure 10 illustrates the request size distributions of Chatbot\nArena, demonstrating the extensive diversity of request sizes in practical scenarios.", "metadata": {}}, {"text": "As a result, high\n3", "metadata": {}}], "metadata": {"page": 3}}, {"text": "[Image page=3 idx=1 name=Im2.png] Size: 1179x1204, Data: 153570 bytes", "sentences": [{"text": "[Image page=3 idx=1 name=Im2.png] Size: 1179x1204, Data: 153570 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 1, "image_name": "Im2.png", "image_width": 1179, "image_height": 1204, "attachment_type": "image", "has_image_data": true, "image_data_size": 153570}}, {"text": "[Image page=3 idx=2 name=Im1.png] Size: 800x400, Data: 27064 bytes", "sentences": [{"text": "[Image page=3 idx=2 name=Im1.png] Size: 800x400, Data: 27064 bytes", "metadata": {}}], "metadata": {"page": 3, "image_index": 2, "image_name": "Im1.png", "image_width": 800, "image_height": 400, "attachment_type": "image", "has_image_data": true, "image_data_size": 27064}}], "metadata": {"page": 3}}, {"title": "Page 4", "paragraphs": [{"text": "(a) Equivalent input and output lengths\n (b) Input and output lengths vary independently\nFigure 3: Figure (a) depicts A10G and A100’s relativeT/$ across request sizes. Figure (b) expands\n(a) into separate input and output length dimensions. Tile colors indicate which GPU achieves higher\nT/$, and values represent the percent increase of T/$ relative to the less cost efficient GPU.\nvariance in request sizes introduces significant variation in request latency. As illustrated in Figure 2,\nrequest latency can increase by 110× when the input/output length expands from 25 tokens to 2000\ntokens for the Llama2-7B model served on an A100 GPU. Consequently, it is crucial to recognize\nthat LLM requests, unlike non-autoregressive models, impose varied loads on GPU resources.\n4 GPU Cost Efficiency Analysis\nIn this section, we analyze GPU cost efficiency for LLM services by serving Llama2-7b on NVIDIA\nA100 [32] and A10G [31] as a representative example. We show that GPU cost efficiency is influenced\nby three key LLM service characteristics: request size (§ 4.2), latency SLO (§ 4.3), and request rate\n(§ 4.4). For each characteristic, we demonstrate opportunities to exploit the heterogeneity of GPU\ntypes to increase cost efficiency and reduce deployment cost. Each plot is tagged with the request\nsize, request rate, and SLO used to generate the plot. We use vLLM-0.2.7 as the serving engine [19].\n4.1 Definitions\nService-level Objective (SLO). SLOs are performance targets that define the acceptable quality\nof service, and a specific SLO varies according to the service’s interactivity needs. As in prior\nwork [19, 58, 51], we use the averageTime Per Output Token (TPOT)as our SLO. TPOT is determined\nby dividing request latency by the number of generated tokens. SLOs are application dependent:\nin-line code editors (e.g., GitHub Copilot [28]) require tight latency deadlines to suggest real-time\ncode additions, whereas summarization services may permit additional processing time. There are\nother common definitions of SLO, such as time to first token and request latency, and Mélange is\nflexible to support these and other alternative definitions of SLO.\nCost Efficiency Metric. We use tokens per dollar (T/$) to measure GPU cost efficiency, calculated\nby summing input and output tokens and dividing the total by the GPU’s on-demand rental cost\nfor a given time period. Cost models are orthogonal to Mélange; we chose this cost model for\nits simplicity, but cost efficiency can be computed with alternative formulations without affecting\nMélange’s efficacy. In general, we deriveT/$ by finding the input and output token rates while at the\nhighest GPU saturation for which TPOT still meets a specified SLO.\n4.2 Request Size and Cost Efficiency\nUnlike many traditional DNNs, LLMs exhibit significant variance in model request sizes (input\nand output lengths) [36]. In this section, we show that request size variance influences GPU cost\nefficiency and can even determine which GPU is most cost efficient.\nExperiment: We serve Llama2-7b on A10G and A100 GPUs, and derive each GPU’s T/$ at\nmaximum GPU saturation across a range of request sizes ( Fig. 3a). Interestingly, no single GPU\nconsistently delivers the highest tokens per dollar (T/$) across all request sizes. Instead, both GPUs\nare most cost efficient in separate regions of the request size spectrum. For smaller request sizes,\n4", "sentences": [{"text": "(a) Equivalent input and output lengths\n (b) Input and output lengths vary independently\nFigure 3: Figure (a) depicts A10G and A100’s relativeT/$ across request sizes.", "metadata": {}}, {"text": "Figure (b) expands\n(a) into separate input and output length dimensions.", "metadata": {}}, {"text": "Tile colors indicate which GPU achieves higher\nT/$, and values represent the percent increase of T/$ relative to the less cost efficient GPU.", "metadata": {}}, {"text": "variance in request sizes introduces significant variation in request latency.", "metadata": {}}, {"text": "As illustrated in Figure 2,\nrequest latency can increase by 110× when the input/output length expands from 25 tokens to 2000\ntokens for the Llama2-7B model served on an A100 GPU.", "metadata": {}}, {"text": "Consequently, it is crucial to recognize\nthat LLM requests, unlike non-autoregressive models, impose varied loads on GPU resources.", "metadata": {}}, {"text": "4 GPU Cost Efficiency Analysis\nIn this section, we analyze GPU cost efficiency for LLM services by serving Llama2-7b on NVIDIA\nA100 [32] and A10G [31] as a representative example.", "metadata": {}}, {"text": "We show that GPU cost efficiency is influenced\nby three key LLM service characteristics: request size (§ 4.2), latency SLO (§ 4.3), and request rate\n(§ 4.4).", "metadata": {}}, {"text": "For each characteristic, we demonstrate opportunities to exploit the heterogeneity of GPU\ntypes to increase cost efficiency and reduce deployment cost.", "metadata": {}}, {"text": "Each plot is tagged with the request\nsize, request rate, and SLO used to generate the plot.", "metadata": {}}, {"text": "We use vLLM-0.2.7 as the serving engine [19].", "metadata": {}}, {"text": "4.1 Definitions\nService-level Objective (SLO).", "metadata": {}}, {"text": "SLOs are performance targets that define the acceptable quality\nof service, and a specific SLO varies according to the service’s interactivity needs.", "metadata": {}}, {"text": "As in prior\nwork [19, 58, 51], we use the averageTime Per Output Token (TPOT)as our SLO.", "metadata": {}}, {"text": "TPOT is determined\nby dividing request latency by the number of generated tokens.", "metadata": {}}, {"text": "SLOs are application dependent:\nin-line code editors (e.g., GitHub Copilot [28]) require tight latency deadlines to suggest real-time\ncode additions, whereas summarization services may permit additional processing time.", "metadata": {}}, {"text": "There are\nother common definitions of SLO, such as time to first token and request latency, and Mélange is\nflexible to support these and other alternative definitions of SLO.", "metadata": {}}, {"text": "Cost Efficiency Metric.", "metadata": {}}, {"text": "We use tokens per dollar (T/$) to measure GPU cost efficiency, calculated\nby summing input and output tokens and dividing the total by the GPU’s on-demand rental cost\nfor a given time period.", "metadata": {}}, {"text": "Cost models are orthogonal to Mélange;", "metadata": {}}, {"text": "we chose this cost model for\nits simplicity, but cost efficiency can be computed with alternative formulations without affecting\nMélange’s efficacy.", "metadata": {}}, {"text": "In general, we deriveT/$ by finding the input and output token rates while at the\nhighest GPU saturation for which TPOT still meets a specified SLO.", "metadata": {}}, {"text": "4.2 Request Size and Cost Efficiency\nUnlike many traditional DNNs, LLMs exhibit significant variance in model request sizes (input\nand output lengths) [36].", "metadata": {}}, {"text": "In this section, we show that request size variance influences GPU cost\nefficiency and can even determine which GPU is most cost efficient.", "metadata": {}}, {"text": "Experiment: We serve Llama2-7b on A10G and A100 GPUs, and derive each GPU’s T/$ at\nmaximum GPU saturation across a range of request sizes ( Fig.", "metadata": {}}, {"text": "3a).", "metadata": {}}, {"text": "Interestingly, no single GPU\nconsistently delivers the highest tokens per dollar (T/$) across all request sizes.", "metadata": {}}, {"text": "Instead, both GPUs\nare most cost efficient in separate regions of the request size spectrum.", "metadata": {}}, {"text": "For smaller request sizes,\n4", "metadata": {}}], "metadata": {"page": 4}}, {"text": "[Image page=4 idx=1 name=Im4.png] Size: 1634x974, Data: 216242 bytes", "sentences": [{"text": "[Image page=4 idx=1 name=Im4.png] Size: 1634x974, Data: 216242 bytes", "metadata": {}}], "metadata": {"page": 4, "image_index": 1, "image_name": "Im4.png", "image_width": 1634, "image_height": 974, "attachment_type": "image", "has_image_data": true, "image_data_size": 216242}}, {"text": "[Image page=4 idx=2 name=Im5.png] Size: 878x564, Data: 173530 bytes", "sentences": [{"text": "[Image page=4 idx=2 name=Im5.png] Size: 878x564, Data: 173530 bytes", "metadata": {}}], "metadata": {"page": 4, "image_index": 2, "image_name": "Im5.png", "image_width": 878, "image_height": 564, "attachment_type": "image", "has_image_data": true, "image_data_size": 173530}}], "metadata": {"page": 4}}, {"title": "Page 5", "paragraphs": [{"text": "(a) Absolute batch sizes\n (b) Dollar-normalized batch sizes\nFigure 4: (a) depicts the absolute batch sizes of A10G and A100 serving Llama2-7b at maximum\nsaturation, (b) reports the same batch sizes divided by GPU cost, plotting with respect to A10G.\n(a) Best GPU relative to second best GPU\n (b) Best GPU relative to worst GPU\nFigure 5: Comparison of L4, A10G, A100, and H100. Tile colors indicates the GPU with greatest\nT/$. (a) tile values are the T/$ %-increase of the best GPU compared to the second best for that tile.\n(b) compares the best GPU to the worst GPU. In black boxes, only A100 and H100 are compared.\n.\nA10G exhibits up to 2.6× greater T/$ than A100. Conversely, for larger request sizes, A100 achieves\nup to 1.5× the cost efficiency of A10G.\nWe extend this exploration to show the separate impacts of input and output lengths on T/$ ( Fig. 3b).\nEach dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and\nlarger sizes are best served on A100. Note that the difference can be significant, as using a single\nGPU type to serve requests across the entire request size space misses opportunities to produce up to\n72% more output tokens for the same cost. This reveals the opportunity to use a mix of GPU types to\nserve requests for which they are most cost effective.\nSource of Cost Efficiency Gains: To isolate how request size influences relative cost efficiency, we\nexamine request size’s effects on batch size, which serves as a proxy for throughput. Fig. 4 depicts\nabsolute batch sizes and batch sizes normalized by instance cost of each GPU at maximum saturation.\nA10G and A100 have similar cost-normalized batch sizes at 250 input/output tokens, but as the\nrequest size increases to 2K input/output tokens, A10G’s absolute batch size decreases by9× whereas\nA100’s only decreases by6× due to its superior memory size and bandwidth. As a result, A100’s\ncost efficiency advantage over A10G increases with the increase in request size. In contrast, reducing\nthe size from 250 to 25 input/output tokens expands A10G’s batch size by15.2×, whereas A100’s\ngrowth is 5.89×.Because A100’s batch sizes are larger, A100 is more significantly constrained by\nper-request latency overheads (e.g., due to interference of prefill and decode [14]) As a result, A10G’s\ncost-normalized batch size exceeds A100’s at short request lengths, leading to greater overallT/$.\nOther Hardware and Model Size We extend our analysis to more GPU types and a larger model\nvariant (Llama2-70b). Fig. 5 depicts the relative cost efficiency across four GPU types. Once again,\nas request sizes increase, we observe a progression of the most cost efficient GPU from lower-end\nto higher-end GPUs, matching our observations above. Similar trends are observed in the larger\nLlama2-70B model when comparing H100 and A100 GPUs, as detailed in Fig. 8.\nKey Takeaways: There is no universally most cost-efficient GPU for a given LLM. Instead, GPU cost\nefficiency is highly dependent on request sizes. Lower-end GPUs are more cost-effective for small\n5", "sentences": [{"text": "(a) Absolute batch sizes\n (b) Dollar-normalized batch sizes\nFigure 4: (a) depicts the absolute batch sizes of A10G and A100 serving Llama2-7b at maximum\nsaturation, (b) reports the same batch sizes divided by GPU cost, plotting with respect to A10G.", "metadata": {}}, {"text": "(a) Best GPU relative to second best GPU\n (b) Best GPU relative to worst GPU\nFigure 5: Comparison of L4, A10G, A100, and H100.", "metadata": {}}, {"text": "Tile colors indicates the GPU with greatest\nT/$.", "metadata": {}}, {"text": "(a) tile values are the T/$ %-increase of the best GPU compared to the second best for that tile.", "metadata": {}}, {"text": "(b) compares the best GPU to the worst GPU.", "metadata": {}}, {"text": "In black boxes, only A100 and H100 are compared.", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": "A10G exhibits up to 2.6× greater T/$ than A100.", "metadata": {}}, {"text": "Conversely, for larger request sizes, A100 achieves\nup to 1.5× the cost efficiency of A10G.", "metadata": {}}, {"text": "We extend this exploration to show the separate impacts of input and output lengths on T/$ ( Fig.", "metadata": {}}, {"text": "3b).", "metadata": {}}, {"text": "Each dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and\nlarger sizes are best served on A100.", "metadata": {}}, {"text": "Note that the difference can be significant, as using a single\nGPU type to serve requests across the entire request size space misses opportunities to produce up to\n72% more output tokens for the same cost.", "metadata": {}}, {"text": "This reveals the opportunity to use a mix of GPU types to\nserve requests for which they are most cost effective.", "metadata": {}}, {"text": "Source of Cost Efficiency Gains: To isolate how request size influences relative cost efficiency, we\nexamine request size’s effects on batch size, which serves as a proxy for throughput.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "4 depicts\nabsolute batch sizes and batch sizes normalized by instance cost of each GPU at maximum saturation.", "metadata": {}}, {"text": "A10G and A100 have similar cost-normalized batch sizes at 250 input/output tokens, but as the\nrequest size increases to 2K input/output tokens, A10G’s absolute batch size decreases by9× whereas\nA100’s only decreases by6× due to its superior memory size and bandwidth.", "metadata": {}}, {"text": "As a result, A100’s\ncost efficiency advantage over A10G increases with the increase in request size.", "metadata": {}}, {"text": "In contrast, reducing\nthe size from 250 to 25 input/output tokens expands A10G’s batch size by15.2×, whereas A100’s\ngrowth is 5.89×.Because A100’s batch sizes are larger, A100 is more significantly constrained by\nper-request latency overheads (e.g., due to interference of prefill and decode [14]) As a result, A10G’s\ncost-normalized batch size exceeds A100’s at short request lengths, leading to greater overallT/$.", "metadata": {}}, {"text": "Other Hardware and Model Size We extend our analysis to more GPU types and a larger model\nvariant (Llama2-70b).", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "5 depicts the relative cost efficiency across four GPU types.", "metadata": {}}, {"text": "Once again,\nas request sizes increase, we observe a progression of the most cost efficient GPU from lower-end\nto higher-end GPUs, matching our observations above.", "metadata": {}}, {"text": "Similar trends are observed in the larger\nLlama2-70B model when comparing H100 and A100 GPUs, as detailed in Fig.", "metadata": {}}, {"text": "8.", "metadata": {}}, {"text": "Key Takeaways: There is no universally most cost-efficient GPU for a given LLM.", "metadata": {}}, {"text": "Instead, GPU cost\nefficiency is highly dependent on request sizes.", "metadata": {}}, {"text": "Lower-end GPUs are more cost-effective for small\n5", "metadata": {}}], "metadata": {"page": 5}}, {"text": "[Image page=5 idx=1 name=Im6.png] Size: 1520x808, Data: 214520 bytes", "sentences": [{"text": "[Image page=5 idx=1 name=Im6.png] Size: 1520x808, Data: 214520 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 1, "image_name": "Im6.png", "image_width": 1520, "image_height": 808, "attachment_type": "image", "has_image_data": true, "image_data_size": 214520}}, {"text": "[Image page=5 idx=2 name=Im7.png] Size: 1662x942, Data: 239889 bytes", "sentences": [{"text": "[Image page=5 idx=2 name=Im7.png] Size: 1662x942, Data: 239889 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 2, "image_name": "Im7.png", "image_width": 1662, "image_height": 942, "attachment_type": "image", "has_image_data": true, "image_data_size": 239889}}, {"text": "[Image page=5 idx=3 name=Im8.png] Size: 1942x1340, Data: 496522 bytes", "sentences": [{"text": "[Image page=5 idx=3 name=Im8.png] Size: 1942x1340, Data: 496522 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 3, "image_name": "Im8.png", "image_width": 1942, "image_height": 1340, "attachment_type": "image", "has_image_data": true, "image_data_size": 496522}}, {"text": "[Image page=5 idx=4 name=Im9.png] Size: 1828x1268, Data: 503486 bytes", "sentences": [{"text": "[Image page=5 idx=4 name=Im9.png] Size: 1828x1268, Data: 503486 bytes", "metadata": {}}], "metadata": {"page": 5, "image_index": 4, "image_name": "Im9.png", "image_width": 1828, "image_height": 1268, "attachment_type": "image", "has_image_data": true, "image_data_size": 503486}}], "metadata": {"page": 5}}, {"title": "Page 6", "paragraphs": [{"text": "request sizes whereas higher-end GPUs are best for large request sizes. These findings generalize to\nsettings with more GPU types and larger model sizes.\n4.3 SLO and Cost Efficiency\nFigure 6: T/$ comparison between A10G\nand A100 across a range of TPOT SLO\nparameters.\n Figure 7: Relative increase in T/$ when combining\nSLO and request size.\nIn this section, we examine the impact of TPOT SLOs on GPU cost efficiency and highlight the joint\neffects of SLO and request size.\nExperiment: We serve Llama2-7b on A10G and A100 and measure T/$ by maximally saturating\neach GPU while keeping TPOT below SLO, repeating this across several TPOT deadlines ( Fig. 6).\nUnder tight SLO constraints ( <60ms), A100 demonstrates significantly greater T/$ than A10G\n(2×). A10G’s higher processing latency restricts the throughput it can achieve within a tight TPOT\ndeadline, while A100 maintains much higher throughput even at low latency. However, as the SLO\ngradually loosens (60-160ms), A10G’s higher latency is less problematic, dramatically increasing its\nT/$ and surpassing that of A100 (by > 40%). Importantly, this example uses a small request size (64\ninput/output tokens), which was shown in § 4.2 to be best served on A10G. However, a tight SLO\ndegrades A10G’s cost efficiency much more severely than A100’s and pushes the advantage to A100,\nexemplifying the tight interplay between SLO and request size explored further below.\nSLO and Request Size Interplay: Fig. 7 presents relative cost efficiency between A10G and A100\nfor a broad range of TPOT SLOs and request sizes. At tight SLOs (40-60ms), A100 always has\nhigher T/$ (up to 2×). At 80ms, A10G begins showing modest benefit over A100 for small request\nsizes. Finally, at 100-160ms, A10G demonstrates much greater T/$ advantage over A100 for the\nsame request sizes (up to 1.5×), yet A100 is always more cost efficient for larger requests. As\ndemonstrated, a modification to TPOT SLO shifts the boundary within the request size space between\nwhich different GPU types are most cost effective and significantly influences the magnitude of cost\nefficiency differences between GPUs. As a result, both request size and SLO must be considered in\ntandem when determining cost efficiency.\nKey Takeaways: To meet strict SLOs, expensive GPUs are necessary due to the higher latency of\ncheaper GPUs. However, as SLO is loosened, lower-end GPUs can be used to cut deployment costs.\nFigure 8: T/$ comparison between H100x2 and\nA100x2 serving Llama2-70b.\nFigure 9: GPU on-demand cost for three GPU\nprovisioning strategies.\n6", "sentences": [{"text": "request sizes whereas higher-end GPUs are best for large request sizes.", "metadata": {}}, {"text": "These findings generalize to\nsettings with more GPU types and larger model sizes.", "metadata": {}}, {"text": "4.3 SLO and Cost Efficiency\nFigure 6: T/$ comparison between A10G\nand A100 across a range of TPOT SLO\nparameters.", "metadata": {}}, {"text": "Figure 7: Relative increase in T/$ when combining\nSLO and request size.", "metadata": {}}, {"text": "In this section, we examine the impact of TPOT SLOs on GPU cost efficiency and highlight the joint\neffects of SLO and request size.", "metadata": {}}, {"text": "Experiment: We serve Llama2-7b on A10G and A100 and measure T/$ by maximally saturating\neach GPU while keeping TPOT below SLO, repeating this across several TPOT deadlines ( Fig.", "metadata": {}}, {"text": "6).", "metadata": {}}, {"text": "Under tight SLO constraints ( <60ms), A100 demonstrates significantly greater T/$ than A10G\n(2×).", "metadata": {}}, {"text": "A10G’s higher processing latency restricts the throughput it can achieve within a tight TPOT\ndeadline, while A100 maintains much higher throughput even at low latency.", "metadata": {}}, {"text": "However, as the SLO\ngradually loosens (60-160ms), A10G’s higher latency is less problematic, dramatically increasing its\nT/$ and surpassing that of A100 (by > 40%).", "metadata": {}}, {"text": "Importantly, this example uses a small request size (64\ninput/output tokens), which was shown in § 4.2 to be best served on A10G.", "metadata": {}}, {"text": "However, a tight SLO\ndegrades A10G’s cost efficiency much more severely than A100’s and pushes the advantage to A100,\nexemplifying the tight interplay between SLO and request size explored further below.", "metadata": {}}, {"text": "SLO and Request Size Interplay: Fig.", "metadata": {}}, {"text": "7 presents relative cost efficiency between A10G and A100\nfor a broad range of TPOT SLOs and request sizes.", "metadata": {}}, {"text": "At tight SLOs (40-60ms), A100 always has\nhigher T/$ (up to 2×).", "metadata": {}}, {"text": "At 80ms, A10G begins showing modest benefit over A100 for small request\nsizes.", "metadata": {}}, {"text": "Finally, at 100-160ms, A10G demonstrates much greater T/$ advantage over A100 for the\nsame request sizes (up to 1.5×), yet A100 is always more cost efficient for larger requests.", "metadata": {}}, {"text": "As\ndemonstrated, a modification to TPOT SLO shifts the boundary within the request size space between\nwhich different GPU types are most cost effective and significantly influences the magnitude of cost\nefficiency differences between GPUs.", "metadata": {}}, {"text": "As a result, both request size and SLO must be considered in\ntandem when determining cost efficiency.", "metadata": {}}, {"text": "Key Takeaways: To meet strict SLOs, expensive GPUs are necessary due to the higher latency of\ncheaper GPUs.", "metadata": {}}, {"text": "However, as SLO is loosened, lower-end GPUs can be used to cut deployment costs.", "metadata": {}}, {"text": "Figure 8: T/$ comparison between H100x2 and\nA100x2 serving Llama2-70b.", "metadata": {}}, {"text": "Figure 9: GPU on-demand cost for three GPU\nprovisioning strategies.", "metadata": {}}, {"text": "6", "metadata": {}}], "metadata": {"page": 6}}, {"text": "[Image page=6 idx=1 name=Im10.png] Size: 1194x602, Data: 128592 bytes", "sentences": [{"text": "[Image page=6 idx=1 name=Im10.png] Size: 1194x602, Data: 128592 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 1, "image_name": "Im10.png", "image_width": 1194, "image_height": 602, "attachment_type": "image", "has_image_data": true, "image_data_size": 128592}}, {"text": "[Image page=6 idx=2 name=Im11.png] Size: 1140x650, Data: 244436 bytes", "sentences": [{"text": "[Image page=6 idx=2 name=Im11.png] Size: 1140x650, Data: 244436 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 2, "image_name": "Im11.png", "image_width": 1140, "image_height": 650, "attachment_type": "image", "has_image_data": true, "image_data_size": 244436}}, {"text": "[Image page=6 idx=3 name=Im12.png] Size: 1418x872, Data: 328265 bytes", "sentences": [{"text": "[Image page=6 idx=3 name=Im12.png] Size: 1418x872, Data: 328265 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 3, "image_name": "Im12.png", "image_width": 1418, "image_height": 872, "attachment_type": "image", "has_image_data": true, "image_data_size": 328265}}, {"text": "[Image page=6 idx=4 name=Im13.png] Size: 1474x946, Data: 202403 bytes", "sentences": [{"text": "[Image page=6 idx=4 name=Im13.png] Size: 1474x946, Data: 202403 bytes", "metadata": {}}], "metadata": {"page": 6, "image_index": 4, "image_name": "Im13.png", "image_width": 1474, "image_height": 946, "attachment_type": "image", "has_image_data": true, "image_data_size": 202403}}], "metadata": {"page": 6}}, {"title": "Page 7", "paragraphs": [{"text": "4.4 Request Rate and Cost Efficiency\nIn this section, we investigate the relationship between request rate and GPU cost efficiency.\nExperiment: Fig. 9 illustrates the cost of serving Llama2-7b at a range of request rates using three\nstrategies: A10G-only, A100-only, or a mix of both. The y-axis is absolute cost instead of T/$\nbecause each provisioning strategy serves the same request rates and thus the same number of tokens;\nonly the cost varies across strategies.\nAs request rate increases, A100-only is increasingly more cost effective than A10G-only. This is\nbecause the requests are of size [1000 in tokens, 250 out tokens], which § 4.2 shows is more cost\neffective on A100. However, A10G-only still presents benefits at low request rates (0-1 req/s). Periods\nof idleness or low activity are common in real-world services [38], and the service should right-size\nto a cheaper GPU (here, A10G) when a higher-end GPU (here, A100) is drastically underutilized.\nMixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs\nconsistently yields the lowest deployment cost. Because A100s have such large capacity, scaling\nwith only A100s is coarse-grained and often leads to underutilized resources. Instead, A10Gs and\nA100s can be mixed such that A100s satisfy the bulk of the service demands, while A10Gs handle\nthe remaining load at reduced cost. Fig. 9 highlights a case where using 2 A100s and 1 A10G results\nin a 24% cost saving over A100-only and 31% over A10G-only.\nKey Takeaways: During low activity periods, LLM services should right-size to cheaper low-end\nGPUs. Provisioning a mix of GPU types enables finer-grained resource scaling, which better aligns\nthe allocated GPU capacity with request load. This increases GPU utilization and consistently\nachieves lowest serving cost.\n5 Mélange: Automating Cost-Efficient GPU Selection\nBuilding on the observations in § 4 that request size, request rate, and SLO all jointly determine\nGPU cost efficiency, we present Mélange, an allocation framework that considers each of these three\ndimensions in-tandem to derive the minimal-cost GPU allocation that meets an LLM service’s request\nload while adhering to SLO constraints. Fig. 1 depicts the Mélange framework. Mélange flexibly\nsupports any GPU type ( 1a) and LLM service definition (1b), uses a one-time offline profiling step\nto measure GPU performance ( 2 ), formulates the task of GPU allocation as a bin packing problem\n( 3 ), then computes the minimal-cost GPU allocation ( 4 ).\n5.1 Problem Formulation\nWe begin by defining the key terms utilized in our problem formulation and solution. An LLM service\nworkload is characterized by its overall request rate along with a distribution of input and output\nsizes. A distribution of request sizes is used rather than fixed values due to the inherent variability of\nLLM request sizes. Specifically, a workload is a histogram where each bucket corresponds to a range\nof request sizes and a bucket’s value is the request rate of requests within the bucket’s size range.\nThe service cost is computed by summing the hourly on-demand cloud renatl rates for each of the\nselected GPUs. We define SLO based on average TPOT, however, Mélange can be extended to other\ndefinitions of SLO such as time to first token (TTFT).\nProblem Definition: Given a workload, GPU costs, and SLO requirements, our objective is to\nprovision GPUs that can minimize deployment cost while adhering to latency SLO constraints.\n5.2 Inputs\nMélange takes as input the set of available GPU types ( 1a) and the LLM service definition ( 1b)\nmade up of the workload profile and SLO. Each of these inputs can be modified, such as adding a\nnew hardware accelerator or redefining SLO based on end-to-end request latency, and Mélange’s\ndownstream components still derive the minimal-cost allocation. Due to the large diversity of\nhardware accelerators and LLM services, Mélange’s extensibility is critical for usability.\n7", "sentences": [{"text": "4.4 Request Rate and Cost Efficiency\nIn this section, we investigate the relationship between request rate and GPU cost efficiency.", "metadata": {}}, {"text": "Experiment: Fig.", "metadata": {}}, {"text": "9 illustrates the cost of serving Llama2-7b at a range of request rates using three\nstrategies: A10G-only, A100-only, or a mix of both.", "metadata": {}}, {"text": "The y-axis is absolute cost instead of T/$\nbecause each provisioning strategy serves the same request rates and thus the same number of tokens;", "metadata": {}}, {"text": "only the cost varies across strategies.", "metadata": {}}, {"text": "As request rate increases, A100-only is increasingly more cost effective than A10G-only.", "metadata": {}}, {"text": "This is\nbecause the requests are of size [1000 in tokens, 250 out tokens], which § 4.2 shows is more cost\neffective on A100.", "metadata": {}}, {"text": "However, A10G-only still presents benefits at low request rates (0-1 req/s).", "metadata": {}}, {"text": "Periods\nof idleness or low activity are common in real-world services [38], and the service should right-size\nto a cheaper GPU (here, A10G) when a higher-end GPU (here, A100) is drastically underutilized.", "metadata": {}}, {"text": "Mixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs\nconsistently yields the lowest deployment cost.", "metadata": {}}, {"text": "Because A100s have such large capacity, scaling\nwith only A100s is coarse-grained and often leads to underutilized resources.", "metadata": {}}, {"text": "Instead, A10Gs and\nA100s can be mixed such that A100s satisfy the bulk of the service demands, while A10Gs handle\nthe remaining load at reduced cost.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "9 highlights a case where using 2 A100s and 1 A10G results\nin a 24% cost saving over A100-only and 31% over A10G-only.", "metadata": {}}, {"text": "Key Takeaways: During low activity periods, LLM services should right-size to cheaper low-end\nGPUs.", "metadata": {}}, {"text": "Provisioning a mix of GPU types enables finer-grained resource scaling, which better aligns\nthe allocated GPU capacity with request load.", "metadata": {}}, {"text": "This increases GPU utilization and consistently\nachieves lowest serving cost.", "metadata": {}}, {"text": "5 Mélange: Automating Cost-Efficient GPU Selection\nBuilding on the observations in § 4 that request size, request rate, and SLO all jointly determine\nGPU cost efficiency, we present Mélange, an allocation framework that considers each of these three\ndimensions in-tandem to derive the minimal-cost GPU allocation that meets an LLM service’s request\nload while adhering to SLO constraints.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "1 depicts the Mélange framework.", "metadata": {}}, {"text": "Mélange flexibly\nsupports any GPU type ( 1a) and LLM service definition (1b), uses a one-time offline profiling step\nto measure GPU performance ( 2 ), formulates the task of GPU allocation as a bin packing problem\n( 3 ), then computes the minimal-cost GPU allocation ( 4 ).", "metadata": {}}, {"text": "5.1 Problem Formulation\nWe begin by defining the key terms utilized in our problem formulation and solution.", "metadata": {}}, {"text": "An LLM service\nworkload is characterized by its overall request rate along with a distribution of input and output\nsizes.", "metadata": {}}, {"text": "A distribution of request sizes is used rather than fixed values due to the inherent variability of\nLLM request sizes.", "metadata": {}}, {"text": "Specifically, a workload is a histogram where each bucket corresponds to a range\nof request sizes and a bucket’s value is the request rate of requests within the bucket’s size range.", "metadata": {}}, {"text": "The service cost is computed by summing the hourly on-demand cloud renatl rates for each of the\nselected GPUs.", "metadata": {}}, {"text": "We define SLO based on average TPOT, however, Mélange can be extended to other\ndefinitions of SLO such as time to first token (TTFT).", "metadata": {}}, {"text": "Problem Definition: Given a workload, GPU costs, and SLO requirements, our objective is to\nprovision GPUs that can minimize deployment cost while adhering to latency SLO constraints.", "metadata": {}}, {"text": "5.2 Inputs\nMélange takes as input the set of available GPU types ( 1a) and the LLM service definition ( 1b)\nmade up of the workload profile and SLO.", "metadata": {}}, {"text": "Each of these inputs can be modified, such as adding a\nnew hardware accelerator or redefining SLO based on end-to-end request latency, and Mélange’s\ndownstream components still derive the minimal-cost allocation.", "metadata": {}}, {"text": "Due to the large diversity of\nhardware accelerators and LLM services, Mélange’s extensibility is critical for usability.", "metadata": {}}, {"text": "7", "metadata": {}}], "metadata": {"page": 7}}], "metadata": {"page": 7}}, {"title": "Page 8", "paragraphs": [{"text": "5.3 Offline Profiling\nA one-time offline profiling step (2 ) is required to measure the performance of each GPU. For each\nrequest size bucket in the workload histogram, we gradually increase the request rate until the GPU is\nsaturated. We record per-request TTFT and TPOT as the request rate is increased, which are sufficient\nmetrics to capture the timing behavior of a request end-to-end [22]. Then, given an SLO, Mélange\ncan quickly find the maximum throughput each GPU achieves across request sizes while adhering to\nthe SLO. Empirically, the one-time profiling is not time-consuming (<1hr).\n5.4 Allocation Algorithm\nThe allocation algorithm’s (3 ) objective is to map the workload to a minimal-cost set of GPUs that\nare constrained by adhering to SLO. Our insight is that this task can be formulated as a cost-aware\nvariant of the bin packing problem. Mélange partitions workload buckets into smaller slices for\nfine-grained packing, then assigns the slices (items) to GPUs (bins). We first define a slice (§ 5.4.1),\ncompute the load of a slice (§ 5.4.2), then create the ILP formulation (§ 5.4.3).\n5.4.1 Request Buckets and Slices\nA workload histogram has two dimensions, input length and output length, and each histogram\nbucket’s value is the aggregate request rate for requests within the bucket’s size range. We further\nbreak each bucket down into slices for finer-grained bin packing. A parameter, slice factor, indicates\nthe number of slices that each bucket is divided into. In a setting with a slice factor of 8 and a bucket\nwith a request rate of 4, the bucket would be segmented into 8 slices each corresponding to a request\nrate of 0.5 requests/s. The slice factor can be tuned to reach the desired balance between granularity\nand solution complexity, but we have not found overall performance to be sensitive to slice factor.\n5.4.2 Load\nThe solver requires an estimate of the load of each slice to ensure that a GPU’s capacity is not exceeded\nand SLO is not violated. The load of a slice with request size s and rate r on GPU G is calculated\nas r\nM axT put(G,s,SLO), where M axT put(G, s, SLO) is the maximum request/s G can achieve for\nrequests of size s while adhering to SLO. For instance, if M axT put(G, s, SLO) = 10 reqs/s and\nr = 1, the load is calculated as 1/10 = 0 .1 . Each GPU’s maximum capacity is defined as 1. This\napproximation allows us to calculate the aggregate load of slices with differing sizes and rates. Based\non offline profiling, we compute M axT put(G, s, SLO) for each bucket in the workload histogram.\n5.4.3 ILP Formulation\nWe formulate the ILP with two decision variables. First, let A be a matrix {0, 1}N ×M , where N is\nthe number of slices, and M is the number of GPU types. Ai,j = 1 if slice i is assigned to GPU\ntype j, and 0 otherwise. The second decision variable, B, is a vector ZM\n≥0 of non-negative integers,\nwhere Bj specifies the number of GPUs of type j to be allocated. L is a matrix of size N × M where\nLi,j ∈ [0, 1] is the fractional load of slice i on GPU type j. L is computed offline by the process\ndescribed in § 5.4.2. cj denotes the cost of GPU type j.\nOur objective is to minimize the total GPU\nallocation cost:\nThe ILP constraints are as follows. First, each\ntask slice is assigned to exactly one GPU type:\nSecond, for each GPU type, the number of\nGPUs designated in vector B must satisfy the\ncumulative load prescribed to it in matrix A:\nLastly, elements of matrix A are binary, and\nelements of vector B are non-negative:\narg min\nB\n(\nMX\nj=1\nBj · cj) (1)\n∀i ∈ {1, . . . , N},\nMX\nj=1\nAi,j = 1 (2)\n∀j ∈ {1, . . . , M},\nNX\ni=1\nAi,j · Li,j ≤ Bj (3)\n∀i, ∀j, A i,j ∈ {0, 1} (4)\n∀j ∈ {1, . . . , M}, B j ≥ 0 (5)\nThe solution is computed using an off-the-shelf solver [29]. Upon solution, the decision variable B\nholds the minimal-cost GPU allocation ( 4 ) that meets the workload demand and adheres to SLO.\n8", "sentences": [{"text": "5.3 Offline Profiling\nA one-time offline profiling step (2 ) is required to measure the performance of each GPU.", "metadata": {}}, {"text": "For each\nrequest size bucket in the workload histogram, we gradually increase the request rate until the GPU is\nsaturated.", "metadata": {}}, {"text": "We record per-request TTFT and TPOT as the request rate is increased, which are sufficient\nmetrics to capture the timing behavior of a request end-to-end [22].", "metadata": {}}, {"text": "Then, given an SLO, Mélange\ncan quickly find the maximum throughput each GPU achieves across request sizes while adhering to\nthe SLO.", "metadata": {}}, {"text": "Empirically, the one-time profiling is not time-consuming (<1hr).", "metadata": {}}, {"text": "5.4 Allocation Algorithm\nThe allocation algorithm’s (3 ) objective is to map the workload to a minimal-cost set of GPUs that\nare constrained by adhering to SLO.", "metadata": {}}, {"text": "Our insight is that this task can be formulated as a cost-aware\nvariant of the bin packing problem.", "metadata": {}}, {"text": "Mélange partitions workload buckets into smaller slices for\nfine-grained packing, then assigns the slices (items) to GPUs (bins).", "metadata": {}}, {"text": "We first define a slice (§ 5.4.1),\ncompute the load of a slice (§ 5.4.2), then create the ILP formulation (§ 5.4.3).", "metadata": {}}, {"text": "5.4.1 Request Buckets and Slices\nA workload histogram has two dimensions, input length and output length, and each histogram\nbucket’s value is the aggregate request rate for requests within the bucket’s size range.", "metadata": {}}, {"text": "We further\nbreak each bucket down into slices for finer-grained bin packing.", "metadata": {}}, {"text": "A parameter, slice factor, indicates\nthe number of slices that each bucket is divided into.", "metadata": {}}, {"text": "In a setting with a slice factor of 8 and a bucket\nwith a request rate of 4, the bucket would be segmented into 8 slices each corresponding to a request\nrate of 0.5 requests/s.", "metadata": {}}, {"text": "The slice factor can be tuned to reach the desired balance between granularity\nand solution complexity, but we have not found overall performance to be sensitive to slice factor.", "metadata": {}}, {"text": "5.4.2 Load\nThe solver requires an estimate of the load of each slice to ensure that a GPU’s capacity is not exceeded\nand SLO is not violated.", "metadata": {}}, {"text": "The load of a slice with request size s and rate r on GPU G is calculated\nas r\nM axT put(G,s,SLO), where M axT put(G, s, SLO) is the maximum request/s G can achieve for\nrequests of size s while adhering to SLO.", "metadata": {}}, {"text": "For instance, if M axT put(G, s, SLO) = 10 reqs/s and\nr = 1, the load is calculated as 1/10 = 0 .1 .", "metadata": {}}, {"text": "Each GPU’s maximum capacity is defined as 1.", "metadata": {}}, {"text": "This\napproximation allows us to calculate the aggregate load of slices with differing sizes and rates.", "metadata": {}}, {"text": "Based\non offline profiling, we compute M axT put(G, s, SLO) for each bucket in the workload histogram.", "metadata": {}}, {"text": "5.4.3 ILP Formulation\nWe formulate the ILP with two decision variables.", "metadata": {}}, {"text": "First, let A be a matrix {0, 1}N ×M , where N is\nthe number of slices, and M is the number of GPU types.", "metadata": {}}, {"text": "Ai,j = 1 if slice i is assigned to GPU\ntype j, and 0 otherwise.", "metadata": {}}, {"text": "The second decision variable, B, is a vector ZM\n≥0 of non-negative integers,\nwhere Bj specifies the number of GPUs of type j to be allocated.", "metadata": {}}, {"text": "L is a matrix of size N × M where\nLi,j ∈ [0, 1] is the fractional load of slice i on GPU type j.", "metadata": {}}, {"text": "L is computed offline by the process\ndescribed in § 5.4.2.", "metadata": {}}, {"text": "cj denotes the cost of GPU type j.", "metadata": {}}, {"text": "Our objective is to minimize the total GPU\nallocation cost:\nThe ILP constraints are as follows.", "metadata": {}}, {"text": "First, each\ntask slice is assigned to exactly one GPU type:\nSecond, for each GPU type, the number of\nGPUs designated in vector B must satisfy the\ncumulative load prescribed to it in matrix A:\nLastly, elements of matrix A are binary, and\nelements of vector B are non-negative:\narg min\nB\n(\nMX\nj=1\nBj · cj) (1)\n∀i ∈ {1, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ", N},\nMX\nj=1\nAi,j = 1 (2)\n∀j ∈ {1, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ", M},\nNX\ni=1\nAi,j · Li,j ≤ Bj (3)\n∀i, ∀j, A i,j ∈ {0, 1} (4)\n∀j ∈ {1, .", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ".", "metadata": {}}, {"text": ", M}, B j ≥ 0 (5)\nThe solution is computed using an off-the-shelf solver [29].", "metadata": {}}, {"text": "Upon solution, the decision variable B\nholds the minimal-cost GPU allocation ( 4 ) that meets the workload demand and adheres to SLO.", "metadata": {}}, {"text": "8", "metadata": {}}], "metadata": {"page": 8}}], "metadata": {"page": 8}}, {"title": "Page 9", "paragraphs": [{"text": "6 Evaluation\nWe assess Mélange’s performance across diverse hardware, request sizes, rates, and SLOs. Mélange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n6.1 Experiment Setup\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons,\nwe normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We\ncalculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then\nadjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) × 3.67 = $7 .516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].\nType L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A. N.A.\nMemory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\nDatasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios.\nFor short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.\nThe input and output length distributions are shown in Fig. 10. We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable. Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.\n0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.\n0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.\nFigure 10: Dataset input and output length distributions.\nMélange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of\n60 · 8 = 480 slices.\nBaselines. We compare Mélange to allocations that use a single GPU type. To derive baseline\nallocations, we use Mélange’s ILP formulation (§ 5.4.3) but restrict the solver to a single GPU type.\n9", "sentences": [{"text": "6 Evaluation\nWe assess Mélange’s performance across diverse hardware, request sizes, rates, and SLOs.", "metadata": {}}, {"text": "Mélange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.", "metadata": {}}, {"text": "6.1 Experiment Setup\nEnvironment.", "metadata": {}}, {"text": "We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab.", "metadata": {}}, {"text": "1.", "metadata": {}}, {"text": "In increasing price order, we use L4, A10G, A100-80G, and H100.", "metadata": {}}, {"text": "To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP).", "metadata": {}}, {"text": "Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability.", "metadata": {}}, {"text": "To ensure fair cost comparisons,\nwe normalize RunPod’s H100 pricing to match the pricing structures of major platforms.", "metadata": {}}, {"text": "We\ncalculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then\nadjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) × 3.67 = $7 .516 for H100.", "metadata": {}}, {"text": "In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].", "metadata": {}}, {"text": "Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A.", "metadata": {}}, {"text": "N.A.", "metadata": {}}, {"text": "Memory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.", "metadata": {}}, {"text": "Datasets and SLOs.", "metadata": {}}, {"text": "We evaluate across three datasets to cover a wide range of application scenarios.", "metadata": {}}, {"text": "For short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.", "metadata": {}}, {"text": "The input and output length distributions are shown in Fig.", "metadata": {}}, {"text": "10.", "metadata": {}}, {"text": "We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable.", "metadata": {}}, {"text": "Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.", "metadata": {}}, {"text": "0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.", "metadata": {}}, {"text": "0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.", "metadata": {}}, {"text": "Figure 10: Dataset input and output length distributions.", "metadata": {}}, {"text": "Mélange Configuration.", "metadata": {}}, {"text": "Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets).", "metadata": {}}, {"text": "The slice factor is set to 8 for a total of\n60 · 8 = 480 slices.", "metadata": {}}, {"text": "Baselines.", "metadata": {}}, {"text": "We compare Mélange to allocations that use a single GPU type.", "metadata": {}}, {"text": "To derive baseline\nallocations, we use Mélange’s ILP formulation (§ 5.4.3) but restrict the solver to a single GPU type.", "metadata": {}}, {"text": "9", "metadata": {}}], "metadata": {"page": 9}}], "metadata": {"page": 9}}, {"title": "Page 10", "paragraphs": [{"text": "6.2 Cost Savings Analysis\nWe compare the deployment costs of Mélange to the single-GPU-type baselines across datasets and\nSLOs. Fig. 11 displays costs normalized against the cost of Mélange (purple dotted lines), and the\ndetailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only\nbaselines are only included for the Arena dataset because the PubMed and Mixed datasets contain\nlarge requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in\nMélange’s allocation but are limited to serving requests smaller than12, 000 tokens. We now discuss\neach dataset in detail:\nH100A100A10GL4Mélange\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n2\n4Cost (w.r.t Mélange)\n(a) Arena, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0\n1.5Cost (w.r.t Mélange) (b) PubMed, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (c) Mixed, SLO = 120ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2\n3Cost (w.r.t Mélange)\n(d) Arena, SLO = 40ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0Cost (w.r.t Mélange) (e) PubMed, SLO = 40ms.\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (f) Mixed, SLO = 40ms.\nFigure 11: Deployment cost across different datasets and SLOs.\n• Short-context Dataset (Arena). In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction\n(120ms SLO) and 9-68% reduction (40ms SLO). For both SLOs, L4/A10G are more cost efficient\nthan A100/H100 at low request rates because they achieve greater utilization. For example, at\n1-2 req/s, H100 is significantly underutilized and incurs exorbitant costs. However, as the rate\nincreases, L4/A10G’s cost advantage reduces as A100/H100 are better utilized. Further, with a\n120ms SLO, L4/A10G remain competitive with A100 even at higher request rates due to their T/$\nadvantage for smaller request sizes (which the Arena dataset is skewed towards). Conversely, with\na 40ms SLO, A10G/L4 show much higher relative costs due to their increased latency, requiring\nmore instances to meet the tight deadline. Mélange adapts by allocating more L4/A10G at 120ms\nSLO and more A100 at 40ms SLO, consistently reducing overall cost.\n• Long-context Dataset (PubMed). In Figs. 11b and 11e, Mélange achieves 15-33% cost reduction\n(120ms SLO) and 2-22% reduction (40ms SLO). A100 generally achieves higher T/$ for the\nrequest sizes in PubMed, evidenced by the 120ms setting where A100-only is consistently cheaper\nthan H100-only. However, when SLO tightens to 40ms, H100 is the clear winner due to H100’s\nlower inference latency. Again, Mélange adapts to these dynamics by allocating a greater share of\nA100s at a looser SLO, and more H100s as the SLO is tightened.\n• Mixed-context Dataset. In Figs. 11c and 11f, Mélange achieves 13-51% cost reduction (120ms\nSLO) and 4-51% reduction (40ms SLO). Compared to the PubMed workload, A100-only has much\ngreater cost efficiency in the Mixed workload than H100 due to a greater portion of short-context\nrequests, for which A100 achieves greater T/$. Mélange capitalizes by using more A100 than\nH100, but it also uses L4/A10Gs for small requests, enabling even further cost reduction.\nTakeaways. These results exemplify the core observations from § 4, which show that request size,\nSLO, and request rate all jointly determine cost efficiency. As any of these LLM service characteristics\nvary, Mélange flexibly adjusts its GPU allocation and mixes GPU types to exploit their heterogeneity.\nThis consistently delivers the most cost efficient allocation across each evaluated dataset with both\nstrict (40ms) and loose (120ms) SLOs, achieving up to a 77% cost reduction.\n10", "sentences": [{"text": "6.2 Cost Savings Analysis\nWe compare the deployment costs of Mélange to the single-GPU-type baselines across datasets and\nSLOs.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "11 displays costs normalized against the cost of Mélange (purple dotted lines), and the\ndetailed GPU allocations and cost savings are included in App.", "metadata": {}}, {"text": "C.", "metadata": {}}, {"text": "The A10G-only and L4-only\nbaselines are only included for the Arena dataset because the PubMed and Mixed datasets contain\nlarge requests that exceed A10G and L4’s GPU memory capacity.", "metadata": {}}, {"text": "L4 and A10G are included in\nMélange’s allocation but are limited to serving requests smaller than12, 000 tokens.", "metadata": {}}, {"text": "We now discuss\neach dataset in detail:\nH100A100A10GL4Mélange\n1 2 4 8 16 32\nRequest Rate (req/s)\n0\n2\n4Cost (w.r.t Mélange)\n(a) Arena, SLO = 120ms.", "metadata": {}}, {"text": "1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0\n1.5Cost (w.r.t Mélange) (b) PubMed, SLO = 120ms.", "metadata": {}}, {"text": "1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (c) Mixed, SLO = 120ms.", "metadata": {}}, {"text": "1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2\n3Cost (w.r.t Mélange)\n(d) Arena, SLO = 40ms.", "metadata": {}}, {"text": "1 2 4 8 16 32\nRequest Rate (req/s)\n0.0\n0.5\n1.0Cost (w.r.t Mélange) (e) PubMed, SLO = 40ms.", "metadata": {}}, {"text": "1 2 4 8 16 32\nRequest Rate (req/s)\n0\n1\n2Cost (w.r.t Mélange) (f) Mixed, SLO = 40ms.", "metadata": {}}, {"text": "Figure 11: Deployment cost across different datasets and SLOs.", "metadata": {}}, {"text": "• Short-context Dataset (Arena).", "metadata": {}}, {"text": "In Figs.", "metadata": {}}, {"text": "11a and 11d, Mélange achieves 15-77% cost reduction\n(120ms SLO) and 9-68% reduction (40ms SLO).", "metadata": {}}, {"text": "For both SLOs, L4/A10G are more cost efficient\nthan A100/H100 at low request rates because they achieve greater utilization.", "metadata": {}}, {"text": "For example, at\n1-2 req/s, H100 is significantly underutilized and incurs exorbitant costs.", "metadata": {}}, {"text": "However, as the rate\nincreases, L4/A10G’s cost advantage reduces as A100/H100 are better utilized.", "metadata": {}}, {"text": "Further, with a\n120ms SLO, L4/A10G remain competitive with A100 even at higher request rates due to their T/$\nadvantage for smaller request sizes (which the Arena dataset is skewed towards).", "metadata": {}}, {"text": "Conversely, with\na 40ms SLO, A10G/L4 show much higher relative costs due to their increased latency, requiring\nmore instances to meet the tight deadline.", "metadata": {}}, {"text": "Mélange adapts by allocating more L4/A10G at 120ms\nSLO and more A100 at 40ms SLO, consistently reducing overall cost.", "metadata": {}}, {"text": "• Long-context Dataset (PubMed).", "metadata": {}}, {"text": "In Figs.", "metadata": {}}, {"text": "11b and 11e, Mélange achieves 15-33% cost reduction\n(120ms SLO) and 2-22% reduction (40ms SLO).", "metadata": {}}, {"text": "A100 generally achieves higher T/$ for the\nrequest sizes in PubMed, evidenced by the 120ms setting where A100-only is consistently cheaper\nthan H100-only.", "metadata": {}}, {"text": "However, when SLO tightens to 40ms, H100 is the clear winner due to H100’s\nlower inference latency.", "metadata": {}}, {"text": "Again, Mélange adapts to these dynamics by allocating a greater share of\nA100s at a looser SLO, and more H100s as the SLO is tightened.", "metadata": {}}, {"text": "• Mixed-context Dataset.", "metadata": {}}, {"text": "In Figs.", "metadata": {}}, {"text": "11c and 11f, Mélange achieves 13-51% cost reduction (120ms\nSLO) and 4-51% reduction (40ms SLO).", "metadata": {}}, {"text": "Compared to the PubMed workload, A100-only has much\ngreater cost efficiency in the Mixed workload than H100 due to a greater portion of short-context\nrequests, for which A100 achieves greater T/$.", "metadata": {}}, {"text": "Mélange capitalizes by using more A100 than\nH100, but it also uses L4/A10Gs for small requests, enabling even further cost reduction.", "metadata": {}}, {"text": "Takeaways.", "metadata": {}}, {"text": "These results exemplify the core observations from § 4, which show that request size,\nSLO, and request rate all jointly determine cost efficiency.", "metadata": {}}, {"text": "As any of these LLM service characteristics\nvary, Mélange flexibly adjusts its GPU allocation and mixes GPU types to exploit their heterogeneity.", "metadata": {}}, {"text": "This consistently delivers the most cost efficient allocation across each evaluated dataset with both\nstrict (40ms) and loose (120ms) SLOs, achieving up to a 77% cost reduction.", "metadata": {}}, {"text": "10", "metadata": {}}], "metadata": {"page": 10}}], "metadata": {"page": 10}}, {"title": "Page 11", "paragraphs": [{"text": "6.3 SLO Satisfaction\nFigure 12: Mélange TPOT CDFs.\nNext, we assess Mélange adherence to TPOT\nSLOs. We provision cloud GPU instances based\non Mélange’s allocation for each dataset and SLO\nat a rate of 4 req/s. We deploy Llama-2-7b on\neach GPU and sample requests randomly from\nthe chosen dataset to serve 2K total requests. We\nrecord the average TPOT for each request.\nLoad Balancer. A load balancer (LB) is required\nto balance requests across GPUs. Our LB design\nis detailed in Appendix A.2. In short, the LB uses\npreviously-served requests to estimate the output\nlength of a new request, which is then routed to\na GPU based on a weighted random selection.\nWeights are computed based on each GPU’s performance for the request’s estimated size.\nResults. Fig. 12 presents CDFs of the observed per-request average TPOTs across experiments.\nWith an SLO of 120ms, over 99.95% of requests met SLO. When the SLO was tightened to 40ms,\n99.5% of requests met SLO. These results validate Mélange’s ability to choose GPU allocations that\nmeet workload demand, however, we recognize that services may require even higher SLO adherence,\nso we investigated the source of SLO violations in our experiment.\nSLO Violation Investigation. 84% of our experiment’s SLO violations were due toa) request rate\nbursts or b) co-location with large requests. We send requests by a Poisson process, which occasionally\ncreates short-lived bursts that overload GPU capacity. Further, we randomly sample request sizes from\nthe chosen dataset. Occasionally, a series of large requests are chosen in sequence and temporarily\nexceed service capacity. In an online production environment, resource over-provisioning is used to\nabsorb such bursts and other load variations. In Mélange, a desired over-provisioning rate (e.g., 10%)\ncan be achieved by increasing the request rate input to the solver by the same proportion.\n6.4 Solver Time\nWe detail the solver execution time in Tab. 2. Across all datasets and request rates, the solver’s\nexecution time remains under 1.2 seconds, which is negligible compared to service lifetime. We\nobserve a modest increase in solver time with higher request volumes due to greater complexity in\nslice assignment. However, this increase is empirically sub-linear relative to the increase in request\nrate, and the solver’s execution time remains practical.\n7 Limitations and Conclusion\nLimitations. Mélange derives the optimal GPU allocation for a fixed workload distribution and\nrequest rate, but does not address other deployment challenges such as GPU unavailability or auto-\nscaling for dynamic request rates and size distributions. Mélange is only intended to make allocation\ndecisions, a key component to be plugged into a broader serving system that handles these deployment\nchallenges. Given the vast number of LLM deployment configurations (quantization and compression,\ndisaggregated prefill, speculative decoding), we have not exhaustively evaluated each setting. We\nexpect, however, that Mélange’s framework is flexible to support each of these settings.\nConclusion. We introduce Mélange, a framework for deriving the minimal-cost GPU allocation for a\ngiven LLM service. Mélange is based on our analysis of GPU cost efficiency, which identifies three\nkey service characteristics (request sizes, request rates, and SLOs) as significant influences on cost\nefficiency. We formulate the GPU allocation task as a cost-aware bin packing problem that accounts\nfor each service characteristic, enabling flexibility and heterogeneity-awareness. In evaluations on a\nrange of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates\nsignificant reductions in deployment costs (up to 77%) while providing high SLO attainment.\n11", "sentences": [{"text": "6.3 SLO Satisfaction\nFigure 12: Mélange TPOT CDFs.", "metadata": {}}, {"text": "Next, we assess Mélange adherence to TPOT\nSLOs.", "metadata": {}}, {"text": "We provision cloud GPU instances based\non Mélange’s allocation for each dataset and SLO\nat a rate of 4 req/s.", "metadata": {}}, {"text": "We deploy Llama-2-7b on\neach GPU and sample requests randomly from\nthe chosen dataset to serve 2K total requests.", "metadata": {}}, {"text": "We\nrecord the average TPOT for each request.", "metadata": {}}, {"text": "Load Balancer.", "metadata": {}}, {"text": "A load balancer (LB) is required\nto balance requests across GPUs.", "metadata": {}}, {"text": "Our LB design\nis detailed in Appendix A.2.", "metadata": {}}, {"text": "In short, the LB uses\npreviously-served requests to estimate the output\nlength of a new request, which is then routed to\na GPU based on a weighted random selection.", "metadata": {}}, {"text": "Weights are computed based on each GPU’s performance for the request’s estimated size.", "metadata": {}}, {"text": "Results.", "metadata": {}}, {"text": "Fig.", "metadata": {}}, {"text": "12 presents CDFs of the observed per-request average TPOTs across experiments.", "metadata": {}}, {"text": "With an SLO of 120ms, over 99.95% of requests met SLO.", "metadata": {}}, {"text": "When the SLO was tightened to 40ms,\n99.5% of requests met SLO.", "metadata": {}}, {"text": "These results validate Mélange’s ability to choose GPU allocations that\nmeet workload demand, however, we recognize that services may require even higher SLO adherence,\nso we investigated the source of SLO violations in our experiment.", "metadata": {}}, {"text": "SLO Violation Investigation.", "metadata": {}}, {"text": "84% of our experiment’s SLO violations were due toa) request rate\nbursts or b) co-location with large requests.", "metadata": {}}, {"text": "We send requests by a Poisson process, which occasionally\ncreates short-lived bursts that overload GPU capacity.", "metadata": {}}, {"text": "Further, we randomly sample request sizes from\nthe chosen dataset.", "metadata": {}}, {"text": "Occasionally, a series of large requests are chosen in sequence and temporarily\nexceed service capacity.", "metadata": {}}, {"text": "In an online production environment, resource over-provisioning is used to\nabsorb such bursts and other load variations.", "metadata": {}}, {"text": "In Mélange, a desired over-provisioning rate (e.g., 10%)\ncan be achieved by increasing the request rate input to the solver by the same proportion.", "metadata": {}}, {"text": "6.4 Solver Time\nWe detail the solver execution time in Tab.", "metadata": {}}, {"text": "2.", "metadata": {}}, {"text": "Across all datasets and request rates, the solver’s\nexecution time remains under 1.2 seconds, which is negligible compared to service lifetime.", "metadata": {}}, {"text": "We\nobserve a modest increase in solver time with higher request volumes due to greater complexity in\nslice assignment.", "metadata": {}}, {"text": "However, this increase is empirically sub-linear relative to the increase in request\nrate, and the solver’s execution time remains practical.", "metadata": {}}, {"text": "7 Limitations and Conclusion\nLimitations.", "metadata": {}}, {"text": "Mélange derives the optimal GPU allocation for a fixed workload distribution and\nrequest rate, but does not address other deployment challenges such as GPU unavailability or auto-\nscaling for dynamic request rates and size distributions.", "metadata": {}}, {"text": "Mélange is only intended to make allocation\ndecisions, a key component to be plugged into a broader serving system that handles these deployment\nchallenges.", "metadata": {}}, {"text": "Given the vast number of LLM deployment configurations (quantization and compression,\ndisaggregated prefill, speculative decoding), we have not exhaustively evaluated each setting.", "metadata": {}}, {"text": "We\nexpect, however, that Mélange’s framework is flexible to support each of these settings.", "metadata": {}}, {"text": "Conclusion.", "metadata": {}}, {"text": "We introduce Mélange, a framework for deriving the minimal-cost GPU allocation for a\ngiven LLM service.", "metadata": {}}, {"text": "Mélange is based on our analysis of GPU cost efficiency, which identifies three\nkey service characteristics (request sizes, request rates, and SLOs) as significant influences on cost\nefficiency.", "metadata": {}}, {"text": "We formulate the GPU allocation task as a cost-aware bin packing problem that accounts\nfor each service characteristic, enabling flexibility and heterogeneity-awareness.", "metadata": {}}, {"text": "In evaluations on a\nrange of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates\nsignificant reductions in deployment costs (up to 77%) while providing high SLO attainment.", "metadata": {}}, {"text": "11", "metadata": {}}], "metadata": {"page": 11}}, {"text": "[Image page=11 idx=1 name=Im23.png] Size: 1042x481, Data: 86785 bytes", "sentences": [{"text": "[Image page=11 idx=1 name=Im23.png] Size: 1042x481, Data: 86785 bytes", "metadata": {}}], "metadata": {"page": 11, "image_index": 1, "image_name": "Im23.png", "image_width": 1042, "image_height": 481, "attachment_type": "image", "has_image_data": true, "image_data_size": 86785}}], "metadata": {"page": 11}}, {"title": "Page 12", "paragraphs": [{"text": "References\n[1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and\nRamachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked\nprefills. arXiv preprint arXiv:2308.16369, 2023.\n[2] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni. Optimizing inference serving on\nserverless platforms. Proceedings of the VLDB Endowment, 15(10), 2022.\n[3] AnyScale. Anyscale: Llmperf leaderboard. https://github.com/ray-project/\nllmperf-leaderboard, 2024. [Accessed 13-03-2024].\n[4] AWS. Ai accelerator-aws trainium. https://aws.amazon.com/machine-learning/\ntrainium/, 2020. [Accessed 14-03-2024].\n[5] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem\nChumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning\nof large models. arXiv preprint arXiv:2209.01188, 2022.\n[6] Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, and Srinidhi\nViswanatha. Balancing efficiency and fairness in heterogeneous gpu clusters for deep learning.\nIn Proceedings of the Fifteenth European Conference on Computer Systems, pages 1–16, 2020.\n[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian. A discourse-aware attention model for abstractive summarization of\nlong documents. Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short\nPapers), 2018.\n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.Advances in Neural Information Processing\nSystems, 35:16344–16359, 2022.\n[9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned\nin one-shot, 2023.\n[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[11] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma,\nMahmut Taylan Kandemir, and Chita R Das. Cocktail: A multidimensional optimization\nfor model serving in cloud. In 19th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 22), pages 1041–1057, 2022.\n[12] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, and Phillip B Gibbons.\nTributary: spot-dancing for elastic services with latency {SLOs}. In 2018 USENIX Annual\nTechnical Conference (USENIX ATC 18), pages 1–14, 2018.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition, 2015.\n[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao\nFeng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate\nllm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024.\n[15] Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang Yuan. Hexgen: Generative\ninference of foundation model over heterogeneous decentralized environment. arXiv preprint\narXiv:2311.11514, 2023.\n[16] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization\nduring generative inference for higher throughput. Advances in Neural Information Processing\nSystems, 36, 2024.\n12", "sentences": [{"text": "References\n[1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and\nRamachandran Ramjee.", "metadata": {}}, {"text": "Sarathi: Efficient llm inference by piggybacking decodes with chunked\nprefills.", "metadata": {}}, {"text": "arXiv preprint arXiv:2308.16369, 2023.", "metadata": {}}, {"text": "[2] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni.", "metadata": {}}, {"text": "Optimizing inference serving on\nserverless platforms.", "metadata": {}}, {"text": "Proceedings of the VLDB Endowment, 15(10), 2022.", "metadata": {}}, {"text": "[3] AnyScale.", "metadata": {}}, {"text": "Anyscale: Llmperf leaderboard.", "metadata": {}}, {"text": "https://github.com/ray-project/\nllmperf-leaderboard, 2024.", "metadata": {}}, {"text": "[Accessed 13-03-2024].", "metadata": {}}, {"text": "[4] AWS.", "metadata": {}}, {"text": "Ai accelerator-aws trainium.", "metadata": {}}, {"text": "https://aws.amazon.com/machine-learning/\ntrainium/, 2020.", "metadata": {}}, {"text": "[Accessed 14-03-2024].", "metadata": {}}, {"text": "[5] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem\nChumachenko, Pavel Samygin, and Colin Raffel.", "metadata": {}}, {"text": "Petals: Collaborative inference and fine-tuning\nof large models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2209.01188, 2022.", "metadata": {}}, {"text": "[6] Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, and Srinidhi\nViswanatha.", "metadata": {}}, {"text": "Balancing efficiency and fairness in heterogeneous gpu clusters for deep learning.", "metadata": {}}, {"text": "In Proceedings of the Fifteenth European Conference on Computer Systems, pages 1–16, 2020.", "metadata": {}}, {"text": "[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian.", "metadata": {}}, {"text": "A discourse-aware attention model for abstractive summarization of\nlong documents.", "metadata": {}}, {"text": "Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short\nPapers), 2018.", "metadata": {}}, {"text": "[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.", "metadata": {}}, {"text": "Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.Advances in Neural Information Processing\nSystems, 35:16344–16359, 2022.", "metadata": {}}, {"text": "[9] Elias Frantar and Dan Alistarh.", "metadata": {}}, {"text": "Sparsegpt: Massive language models can be accurately pruned\nin one-shot, 2023.", "metadata": {}}, {"text": "[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.", "metadata": {}}, {"text": "Gptq: Accurate post-training\nquantization for generative pre-trained transformers.", "metadata": {}}, {"text": "arXiv preprint arXiv:2210.17323, 2022.", "metadata": {}}, {"text": "[11] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma,\nMahmut Taylan Kandemir, and Chita R Das.", "metadata": {}}, {"text": "Cocktail: A multidimensional optimization\nfor model serving in cloud.", "metadata": {}}, {"text": "In 19th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 22), pages 1041–1057, 2022.", "metadata": {}}, {"text": "[12] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, and Phillip B Gibbons.", "metadata": {}}, {"text": "Tributary: spot-dancing for elastic services with latency {SLOs}.", "metadata": {}}, {"text": "In 2018 USENIX Annual\nTechnical Conference (USENIX ATC 18), pages 1–14, 2018.", "metadata": {}}, {"text": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "metadata": {}}, {"text": "Deep residual learning for image\nrecognition, 2015.", "metadata": {}}, {"text": "[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao\nFeng, Chenxi Wang, Sa Wang, Yungang Bao, et al.", "metadata": {}}, {"text": "Inference without interference: Disaggregate\nllm inference for mixed downstream workloads.", "metadata": {}}, {"text": "arXiv preprint arXiv:2401.11181, 2024.", "metadata": {}}, {"text": "[15] Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang Yuan.", "metadata": {}}, {"text": "Hexgen: Generative\ninference of foundation model over heterogeneous decentralized environment.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2311.11514, 2023.", "metadata": {}}, {"text": "[16] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei.", "metadata": {}}, {"text": "S3: Increasing gpu utilization\nduring generative inference for higher throughput.", "metadata": {}}, {"text": "Advances in Neural Information Processing\nSystems, 36, 2024.", "metadata": {}}, {"text": "12", "metadata": {}}], "metadata": {"page": 12}}], "metadata": {"page": 12}}, {"title": "Page 13", "paragraphs": [{"text": "[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance\nanalysis of a tensor processing unit. In Proceedings of the 44th annual international symposium\non computer architecture, pages 1–12, 2017.\n[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir\nGholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural\nInformation Processing Systems, 36, 2024.\n[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems\nPrinciples, pages 611–626, 2023.\n[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\nspeculative decoding. In International Conference on Machine Learning, pages 19274–19286.\nPMLR, 2023.\n[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\n[22] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.\nAndes: Defining and enhancing quality-of-experience in llm-based text streaming services.\narXiv preprint arXiv:2404.16283, 2024.\n[23] Liang Luo, Peter West, Pratyush Patel, Arvind Krishnamurthy, and Luis Ceze. Srifty: Swift and\nthrifty distributed neural network training on the cloud. Proceedings of Machine Learning and\nSystems, 4:833–847, 2022.\n[24] Yusuf Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot\nfor the web, 2023. Accessed: 2024-02-21.\n[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia.\nSpotserve: Serving generative large language models on preemptible instances. arXiv preprint\narXiv:2311.15566, 2023.\n[26] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia. Sdpipe: A semi-decentralized\nframework for heterogeneity-aware pipeline-parallel training. Proceedings of the VLDB Endow-\nment, 16(9):2354–2363, 2023.\n[27] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui.\nGalvatron: Efficient transformer training over multiple gpus using automatic parallelism. arXiv\npreprint arXiv:2211.13878, 2022.\n[28] Microsoft. Copilot, 2023. Accessed: 2024-02-21.\n[29] Stuart Mitchell. PuLP: A linear programming toolkit for python. https://github.com/\ncoin-or/pulp, 2023. Accessed: 2024-02-25.\n[30] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei\nZaharia. {Heterogeneity-Aware} cluster scheduling policies for deep learning workloads. In\n14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages\n481–498, 2020.\n[31] Nvidia. A10 gpu spec, 2024. Accessed: 2024-03-10.\n[32] Nvidia. A100 gpu spec, 2024. Accessed: 2024-03-10.\n[33] Nvidia. Gpus, 2024. Accessed: 2024-03-10.\n[34] OpenAI. Chatgpt, 2022. Accessed: 2024-02-21.\n[35] OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.\n13", "sentences": [{"text": "[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al.", "metadata": {}}, {"text": "In-datacenter performance\nanalysis of a tensor processing unit.", "metadata": {}}, {"text": "In Proceedings of the 44th annual international symposium\non computer architecture, pages 1–12, 2017.", "metadata": {}}, {"text": "[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir\nGholami, and Kurt Keutzer.", "metadata": {}}, {"text": "Speculative decoding with big little decoder.", "metadata": {}}, {"text": "Advances in Neural\nInformation Processing Systems, 36, 2024.", "metadata": {}}, {"text": "[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica.", "metadata": {}}, {"text": "Efficient memory management for large language model\nserving with pagedattention.", "metadata": {}}, {"text": "In Proceedings of the 29th Symposium on Operating Systems\nPrinciples, pages 611–626, 2023.", "metadata": {}}, {"text": "[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias.", "metadata": {}}, {"text": "Fast inference from transformers via\nspeculative decoding.", "metadata": {}}, {"text": "In International Conference on Machine Learning, pages 19274–19286.", "metadata": {}}, {"text": "PMLR, 2023.", "metadata": {}}, {"text": "[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.", "metadata": {}}, {"text": "Awq:\nActivation-aware weight quantization for llm compression and acceleration.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2306.00978, 2023.", "metadata": {}}, {"text": "[22] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury.", "metadata": {}}, {"text": "Andes: Defining and enhancing quality-of-experience in llm-based text streaming services.", "metadata": {}}, {"text": "arXiv preprint arXiv:2404.16283, 2024.", "metadata": {}}, {"text": "[23] Liang Luo, Peter West, Pratyush Patel, Arvind Krishnamurthy, and Luis Ceze.", "metadata": {}}, {"text": "Srifty: Swift and\nthrifty distributed neural network training on the cloud.", "metadata": {}}, {"text": "Proceedings of Machine Learning and\nSystems, 4:833–847, 2022.", "metadata": {}}, {"text": "[24] Yusuf Mehdi.", "metadata": {}}, {"text": "Reinventing search with a new ai-powered microsoft bing and edge, your copilot\nfor the web, 2023.", "metadata": {}}, {"text": "Accessed: 2024-02-21.", "metadata": {}}, {"text": "[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia.", "metadata": {}}, {"text": "Spotserve: Serving generative large language models on preemptible instances.", "metadata": {}}, {"text": "arXiv preprint\narXiv:2311.15566, 2023.", "metadata": {}}, {"text": "[26] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia.", "metadata": {}}, {"text": "Sdpipe: A semi-decentralized\nframework for heterogeneity-aware pipeline-parallel training.", "metadata": {}}, {"text": "Proceedings of the VLDB Endow-\nment, 16(9):2354–2363, 2023.", "metadata": {}}, {"text": "[27] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui.", "metadata": {}}, {"text": "Galvatron: Efficient transformer training over multiple gpus using automatic parallelism.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2211.13878, 2022.", "metadata": {}}, {"text": "[28] Microsoft.", "metadata": {}}, {"text": "Copilot, 2023.", "metadata": {}}, {"text": "Accessed: 2024-02-21.", "metadata": {}}, {"text": "[29] Stuart Mitchell.", "metadata": {}}, {"text": "PuLP: A linear programming toolkit for python.", "metadata": {}}, {"text": "https://github.com/\ncoin-or/pulp, 2023.", "metadata": {}}, {"text": "Accessed: 2024-02-25.", "metadata": {}}, {"text": "[30] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei\nZaharia.", "metadata": {}}, {"text": "{Heterogeneity-Aware} cluster scheduling policies for deep learning workloads.", "metadata": {}}, {"text": "In\n14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages\n481–498, 2020.", "metadata": {}}, {"text": "[31] Nvidia.", "metadata": {}}, {"text": "A10 gpu spec, 2024.", "metadata": {}}, {"text": "Accessed: 2024-03-10.", "metadata": {}}, {"text": "[32] Nvidia.", "metadata": {}}, {"text": "A100 gpu spec, 2024.", "metadata": {}}, {"text": "Accessed: 2024-03-10.", "metadata": {}}, {"text": "[33] Nvidia.", "metadata": {}}, {"text": "Gpus, 2024.", "metadata": {}}, {"text": "Accessed: 2024-03-10.", "metadata": {}}, {"text": "[34] OpenAI.", "metadata": {}}, {"text": "Chatgpt, 2022.", "metadata": {}}, {"text": "Accessed: 2024-02-21.", "metadata": {}}, {"text": "[35] OpenAI.", "metadata": {}}, {"text": "Gpt-4 technical report.", "metadata": {}}, {"text": "arXiv, pages 2303–08774, 2023.", "metadata": {}}, {"text": "13", "metadata": {}}], "metadata": {"page": 13}}], "metadata": {"page": 13}}, {"title": "Page 14", "paragraphs": [{"text": "[36] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and\nRicardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. arXiv\npreprint arXiv:2311.18677, 2023.\n[37] Elizabeth Reid. Supercharging search with generative ai, 2023. Accessed: 2024-02-21.\n[38] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. {INFaaS}: Auto-\nmated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), pages 397–411, 2021.\n[39] RunPod. Runpod, 2024. Accessed: 2024-02-24.\n[40] FlashInfer team. Accelerating self-attentions for llm serving with flashinfer, 2024. Accessed:\n2024-02-24.\n[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In 2016 23rd international conference on pattern\nrecognition (ICPR), pages 2464–2469. IEEE, 2016.\n[42] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang,\nRavi Netravali, and Guoqing Harry Xu. Bamboo: Making preemptible instances resilient for\naffordable training of large {DNNs}. In 20th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 23), pages 497–513, 2023.\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[45] Abhi Venigalla. Databricks: Training llms at scale with amd mi250 gpus. https://www.\ndatabricks.com/blog/training-llms-scale-amd-mi250-gpus , 2023. [Accessed 14-\n03-2024].\n[46] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast\ndistributed inference serving for large language models. arXiv preprint arXiv:2305.05920,\n2023.\n[47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and\nChi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\n[48] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard\nPeng, Qingyun Wu, and Chi Wang. An empirical study on challenging math problem solving\nwith gpt-4. In ArXiv preprint arXiv:2306.01337, 2023.\n[49] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\nSmoothquant: Accurate and efficient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages 38087–38099. PMLR, 2023.\n[50] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.\nAdvances in Neural Information Processing Systems, 35:27168–27183, 2022.\n[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca:\nA distributed serving system for {Transformer-Based} generative models. In 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n[52] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\nago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems, 33:17283–17297,\n2020.\n14", "sentences": [{"text": "[36] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and\nRicardo Bianchini.", "metadata": {}}, {"text": "Splitwise: Efficient generative llm inference using phase splitting.", "metadata": {}}, {"text": "arXiv\npreprint arXiv:2311.18677, 2023.", "metadata": {}}, {"text": "[37] Elizabeth Reid.", "metadata": {}}, {"text": "Supercharging search with generative ai, 2023.", "metadata": {}}, {"text": "Accessed: 2024-02-21.", "metadata": {}}, {"text": "[38] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis.", "metadata": {}}, {"text": "{INFaaS}: Auto-\nmated model-less inference serving.", "metadata": {}}, {"text": "In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), pages 397–411, 2021.", "metadata": {}}, {"text": "[39] RunPod.", "metadata": {}}, {"text": "Runpod, 2024.", "metadata": {}}, {"text": "Accessed: 2024-02-24.", "metadata": {}}, {"text": "[40] FlashInfer team.", "metadata": {}}, {"text": "Accelerating self-attentions for llm serving with flashinfer, 2024.", "metadata": {}}, {"text": "Accessed:\n2024-02-24.", "metadata": {}}, {"text": "[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.", "metadata": {}}, {"text": "Branchynet: Fast inference\nvia early exiting from deep neural networks.", "metadata": {}}, {"text": "In 2016 23rd international conference on pattern\nrecognition (ICPR), pages 2464–2469.", "metadata": {}}, {"text": "IEEE, 2016.", "metadata": {}}, {"text": "[42] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang,\nRavi Netravali, and Guoqing Harry Xu.", "metadata": {}}, {"text": "Bamboo: Making preemptible instances resilient for\naffordable training of large {DNNs}.", "metadata": {}}, {"text": "In 20th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 23), pages 497–513, 2023.", "metadata": {}}, {"text": "[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.", "metadata": {}}, {"text": "Llama: Open\nand efficient foundation language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2302.13971, 2023.", "metadata": {}}, {"text": "[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.", "metadata": {}}, {"text": "Llama 2: Open\nfoundation and fine-tuned chat models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2307.09288, 2023.", "metadata": {}}, {"text": "[45] Abhi Venigalla.", "metadata": {}}, {"text": "Databricks: Training llms at scale with amd mi250 gpus.", "metadata": {}}, {"text": "https://www.", "metadata": {}}, {"text": "databricks.com/blog/training-llms-scale-amd-mi250-gpus , 2023.", "metadata": {}}, {"text": "[Accessed 14-\n03-2024].", "metadata": {}}, {"text": "[46] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin.", "metadata": {}}, {"text": "Fast\ndistributed inference serving for large language models.", "metadata": {}}, {"text": "arXiv preprint arXiv:2305.05920,\n2023.", "metadata": {}}, {"text": "[47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and\nChi Wang.", "metadata": {}}, {"text": "Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.", "metadata": {}}, {"text": "[48] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard\nPeng, Qingyun Wu, and Chi Wang.", "metadata": {}}, {"text": "An empirical study on challenging math problem solving\nwith gpt-4.", "metadata": {}}, {"text": "In ArXiv preprint arXiv:2306.01337, 2023.", "metadata": {}}, {"text": "[49] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.", "metadata": {}}, {"text": "Smoothquant: Accurate and efficient post-training quantization for large language models.", "metadata": {}}, {"text": "In International Conference on Machine Learning, pages 38087–38099.", "metadata": {}}, {"text": "PMLR, 2023.", "metadata": {}}, {"text": "[50] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe.", "metadata": {}}, {"text": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.", "metadata": {}}, {"text": "Advances in Neural Information Processing Systems, 35:27168–27183, 2022.", "metadata": {}}, {"text": "[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.", "metadata": {}}, {"text": "Orca:\nA distributed serving system for {Transformer-Based} generative models.", "metadata": {}}, {"text": "In 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.", "metadata": {}}, {"text": "[52] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\nago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.", "metadata": {}}, {"text": "Big bird: Transformers\nfor longer sequences.", "metadata": {}}, {"text": "Advances in neural information processing systems, 33:17283–17297,\n2020.", "metadata": {}}, {"text": "14", "metadata": {}}], "metadata": {"page": 14}}], "metadata": {"page": 14}}, {"title": "Page 15", "paragraphs": [{"text": "[53] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan.{MArk}: Exploiting cloud services\nfor {Cost-Effective},{SLO-Aware} machine learning inference serving. In 2019 USENIX\nAnnual Technical Conference (USENIX ATC 19), pages 1049–1062, 2019.\n[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao\nSong, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient\ngenerative inference of large language models. Advances in Neural Information Processing\nSystems, 36, 2024.\n[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi\nCao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large\nlanguage models using sglang. arXiv preprint arXiv:2312.07104, 2023.\n[57] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response\nlength perception and sequence scheduling: An llm-empowered llm inference pipeline. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n[58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving. arXiv preprint arXiv:2401.09670, 2024.\n[59] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses\npatience: Fast and robust inference with early exit. Advances in Neural Information Processing\nSystems, 33:18330–18341, 2020.\n[60] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao.\nTowards optimal caching and model selection for large model inference. Advances in Neural\nInformation Processing Systems, 36, 2024.\nA Experiment Setup\nA.1 Dataset\nWe test Mélange’s performance on three different datasets listed below:\n• Short context: This scenario simulates real-time conversational dynamics by employing the\nChatbot Arena dataset (lmsys/lmsys-chat-1m) [55], which is derived from real-world\nchatbot conversations. The dataset is skewed towards shorter context ( < 2000 tokens)\nbecause much of the data was generated in conversation with models that did not yet have a\nlarger context window.\n• Long context: This scenario represents tasks with extensive input, such as summariza-\ntion. We utilize the PubMed dataset ( ccdv/pubmed-summarization) [7], comprising\n133 thousand scientific papers from PubMed.com, a popular dataset for large-scale text\nsummarization studies.\n• Mixed long/short context: This scenario captures settings with a combination of long and\nshort context, such as an assistant that engages in succinct dialogue and responds to large\ndocument-based queries. To model this, we create a synthetic dataset by sampling 80% of\nrequests from the Arena dataset and 20% of requests from the PubMed dataset.\nA.2 Load Balancer\nThe load balancer (LB) policy used in our evaluations in § 6.3 is as follows. For each input length\nbucket range (§ 5.4.1), the LB tracks the average of all previously-seen output lengths. Upon receiving\na new request, the LB uses this average as an estimate for the new request’s output length, allowing the\nLB to identify the specific request size bucket the request belongs in. The LB then makes a weighted\nrandom selection of a GPU backend to forward the request to. A GPU’s weights are computed based\n15", "sentences": [{"text": "[53] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan.{MArk}: Exploiting cloud services\nfor {Cost-Effective},{SLO-Aware} machine learning inference serving.", "metadata": {}}, {"text": "In 2019 USENIX\nAnnual Technical Conference (USENIX ATC 19), pages 1049–1062, 2019.", "metadata": {}}, {"text": "[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao\nSong, Yuandong Tian, Christopher Ré, Clark Barrett, et al.", "metadata": {}}, {"text": "H2o: Heavy-hitter oracle for efficient\ngenerative inference of large language models.", "metadata": {}}, {"text": "Advances in Neural Information Processing\nSystems, 36, 2024.", "metadata": {}}, {"text": "[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric.", "metadata": {}}, {"text": "P Xing, Hao Zhang, Joseph E.", "metadata": {}}, {"text": "Gonzalez, and Ion Stoica.", "metadata": {}}, {"text": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.", "metadata": {}}, {"text": "[56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi\nCao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al.", "metadata": {}}, {"text": "Efficiently programming large\nlanguage models using sglang.", "metadata": {}}, {"text": "arXiv preprint arXiv:2312.07104, 2023.", "metadata": {}}, {"text": "[57] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You.", "metadata": {}}, {"text": "Response\nlength perception and sequence scheduling: An llm-empowered llm inference pipeline.", "metadata": {}}, {"text": "Ad-\nvances in Neural Information Processing Systems, 36, 2024.", "metadata": {}}, {"text": "[58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\nZhang.", "metadata": {}}, {"text": "Distserve: Disaggregating prefill and decoding for goodput-optimized large language\nmodel serving.", "metadata": {}}, {"text": "arXiv preprint arXiv:2401.09670, 2024.", "metadata": {}}, {"text": "[59] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei.", "metadata": {}}, {"text": "Bert loses\npatience: Fast and robust inference with early exit.", "metadata": {}}, {"text": "Advances in Neural Information Processing\nSystems, 33:18330–18341, 2020.", "metadata": {}}, {"text": "[60] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao.", "metadata": {}}, {"text": "Towards optimal caching and model selection for large model inference.", "metadata": {}}, {"text": "Advances in Neural\nInformation Processing Systems, 36, 2024.", "metadata": {}}, {"text": "A Experiment Setup\nA.1 Dataset\nWe test Mélange’s performance on three different datasets listed below:\n• Short context: This scenario simulates real-time conversational dynamics by employing the\nChatbot Arena dataset (lmsys/lmsys-chat-1m) [55], which is derived from real-world\nchatbot conversations.", "metadata": {}}, {"text": "The dataset is skewed towards shorter context ( < 2000 tokens)\nbecause much of the data was generated in conversation with models that did not yet have a\nlarger context window.", "metadata": {}}, {"text": "• Long context: This scenario represents tasks with extensive input, such as summariza-\ntion.", "metadata": {}}, {"text": "We utilize the PubMed dataset ( ccdv/pubmed-summarization) [7], comprising\n133 thousand scientific papers from PubMed.com, a popular dataset for large-scale text\nsummarization studies.", "metadata": {}}, {"text": "• Mixed long/short context: This scenario captures settings with a combination of long and\nshort context, such as an assistant that engages in succinct dialogue and responds to large\ndocument-based queries.", "metadata": {}}, {"text": "To model this, we create a synthetic dataset by sampling 80% of\nrequests from the Arena dataset and 20% of requests from the PubMed dataset.", "metadata": {}}, {"text": "A.2 Load Balancer\nThe load balancer (LB) policy used in our evaluations in § 6.3 is as follows.", "metadata": {}}, {"text": "For each input length\nbucket range (§ 5.4.1), the LB tracks the average of all previously-seen output lengths.", "metadata": {}}, {"text": "Upon receiving\na new request, the LB uses this average as an estimate for the new request’s output length, allowing the\nLB to identify the specific request size bucket the request belongs in.", "metadata": {}}, {"text": "The LB then makes a weighted\nrandom selection of a GPU backend to forward the request to.", "metadata": {}}, {"text": "A GPU’s weights are computed based\n15", "metadata": {}}], "metadata": {"page": 15}}], "metadata": {"page": 15}}, {"title": "Page 16", "paragraphs": [{"text": "on the proportion of the GPU’s maximum throughput for request sizes of the new request’s bucket to\nthe aggregate throughput of all GPUs. This is a simple policy we use to demonstrate the efficacy of\nMélange, and leave it as future work to develop load balancers for serving LLMs on heterogeneous\nGPUs.\nB Solver Time\nWe present the solver execution time from each experiment in Table 2.\nRequest RateArena, SLO=120msArena, SLO=40msPubMed, SLO=120msPubMed, SLO=40msMix, SLO=120msMix, SLO=40ms\n1 0.137 0.177 0.232 0.295 0.168 0.3362 0.194 0.265 0.234 0.334 0.253 0.3814 0.192 0.346 0.287 0.381 0.297 0.4598 0.248 0.433 0.269 0.384 0.321 0.54516 0.299 0.448 0.389 0.509 0.439 0.53732 0.316 0.494 0.791 0.96 0.912 1.14\nTable 2: Solver execution time.\nC Instance Allocations\nWe present the instance allocations for each experiment in the tables below.\n16", "sentences": [{"text": "on the proportion of the GPU’s maximum throughput for request sizes of the new request’s bucket to\nthe aggregate throughput of all GPUs.", "metadata": {}}, {"text": "This is a simple policy we use to demonstrate the efficacy of\nMélange, and leave it as future work to develop load balancers for serving LLMs on heterogeneous\nGPUs.", "metadata": {}}, {"text": "B Solver Time\nWe present the solver execution time from each experiment in Table 2.", "metadata": {}}, {"text": "Request RateArena, SLO=120msArena, SLO=40msPubMed, SLO=120msPubMed, SLO=40msMix, SLO=120msMix, SLO=40ms\n1 0.137 0.177 0.232 0.295 0.168 0.3362 0.194 0.265 0.234 0.334 0.253 0.3814 0.192 0.346 0.287 0.381 0.297 0.4598 0.248 0.433 0.269 0.384 0.321 0.54516 0.299 0.448 0.389 0.509 0.439 0.53732 0.316 0.494 0.791 0.96 0.912 1.14\nTable 2: Solver execution time.", "metadata": {}}, {"text": "C Instance Allocations\nWe present the instance allocations for each experiment in the tables below.", "metadata": {}}, {"text": "16", "metadata": {}}], "metadata": {"page": 16}}], "metadata": {"page": 16}}, {"title": "Page 17", "paragraphs": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 1 1.71 N/A\nH100-only 1 7.516 77.25%\nA100-only 1 3.67 53.41%\nA10G-only 2 2.02 15.35%\nL4-only 3 2.1 18.57%\n2 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n4 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-only 2 7.34 40.46%\nA10G-only 6 6.06 27.89%\nL4-only 9 6.3 30.63%\n8 Mélange 1 3 1 7.4 N/A\nH100-only 2 15.032 50.77%\nA100-only 3 11.01 32.79%\nA10G-only 11 11.1 33.39%\nL4-only 17 11.9 37.82%\n16 Mélange 2 2 3 14.43 N/A\nH100-only 4 30.064 52.00%\nA100-only 6 22.02 34.47%\nA10G-only 20 20.2 28.56%\nL4-only 33 23.1 37.53%\n32 Mélange 2 6 5 25.81 N/A\nH100-only 8 60.128 57.07%\nA100-only 9 33.03 21.86%\nA10G-only 39 39.39 34.48%\nL4-only 65 45.5 43.27%\nTable 3: Instance allocations for the short-context Arena dataset, SLO=120ms.\n17", "sentences": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 1 1 1.71 N/A\nH100-only 1 7.516 77.25%\nA100-only 1 3.67 53.41%\nA10G-only 2 2.02 15.35%\nL4-only 3 2.1 18.57%\n2 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n4 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-only 2 7.34 40.46%\nA10G-only 6 6.06 27.89%\nL4-only 9 6.3 30.63%\n8 Mélange 1 3 1 7.4 N/A\nH100-only 2 15.032 50.77%\nA100-only 3 11.01 32.79%\nA10G-only 11 11.1 33.39%\nL4-only 17 11.9 37.82%\n16 Mélange 2 2 3 14.43 N/A\nH100-only 4 30.064 52.00%\nA100-only 6 22.02 34.47%\nA10G-only 20 20.2 28.56%\nL4-only 33 23.1 37.53%\n32 Mélange 2 6 5 25.81 N/A\nH100-only 8 60.128 57.07%\nA100-only 9 33.03 21.86%\nA10G-only 39 39.39 34.48%\nL4-only 65 45.5 43.27%\nTable 3: Instance allocations for the short-context Arena dataset, SLO=120ms.", "metadata": {}}, {"text": "17", "metadata": {}}], "metadata": {"page": 17}}], "metadata": {"page": 17}}, {"title": "Page 18", "paragraphs": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 1 11.186 N/A\nH100-only 2 15.032 25.59%\nA100-Only 4 14.68 23.80%\n2 Mélange 3 1 2 21.732 N/A\nH100-only 4 30.064 27.71%\nA100-Only 7 25.69 15.41%\n4 Mélange 3 4 3 40.258 N/A\nH100-only 8 60.128 33.05%\nA100-Only 14 51.38 21.65%\n8 Mélange 7 7 78.302 N/A\nH100-only 14 105.224 25.59%\nA100-Only 27 99.09 20.98%\n16 Mélange 12 15 156.78 N/A\nH100-only 28 210.448 25.50%\nA100-Only 53 194.51 19.40%\n32 Mélange 1 1 20 32 315.622 N/A\nH100-only 55 413.38 23.65%\nA100-Only 106 389.02 18.87%\nTable 4: Instance allocations for the long-context PubMed dataset, SLO=120ms.\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-Only 1 3.67 0%\n2 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-Only 2 7.34 40.46%\n4 Mélange 2 1 9.536 N/A\nH100-only 2 15.032 36.56%\nA100-Only 3 11.01 13.39%\n8 Mélange 1 2 1 1 13.906 N/A\nH100-only 3 22.548 38.33%\nA100-Only 5 18.35 24.22%\n16 Mélange 1 2 3 2 28.762 N/A\nH100-only 6 45.096 36.22%\nA100-Only 10 36.7 21.63%\n32 Mélange 1 5 6 4 57.834 N/A\nH100-only 12 90.192 35.88%\nA100-Only 20 73.4 21.21%\nTable 5: Instance allocations for the mixed context dataset, SLO=120ms.\n18", "sentences": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 1 1 11.186 N/A\nH100-only 2 15.032 25.59%\nA100-Only 4 14.68 23.80%\n2 Mélange 3 1 2 21.732 N/A\nH100-only 4 30.064 27.71%\nA100-Only 7 25.69 15.41%\n4 Mélange 3 4 3 40.258 N/A\nH100-only 8 60.128 33.05%\nA100-Only 14 51.38 21.65%\n8 Mélange 7 7 78.302 N/A\nH100-only 14 105.224 25.59%\nA100-Only 27 99.09 20.98%\n16 Mélange 12 15 156.78 N/A\nH100-only 28 210.448 25.50%\nA100-Only 53 194.51 19.40%\n32 Mélange 1 1 20 32 315.622 N/A\nH100-only 55 413.38 23.65%\nA100-Only 106 389.02 18.87%\nTable 4: Instance allocations for the long-context PubMed dataset, SLO=120ms.", "metadata": {}}, {"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-Only 1 3.67 0%\n2 Mélange 1 1 4.37 N/A\nH100-only 1 7.516 41.86%\nA100-Only 2 7.34 40.46%\n4 Mélange 2 1 9.536 N/A\nH100-only 2 15.032 36.56%\nA100-Only 3 11.01 13.39%\n8 Mélange 1 2 1 1 13.906 N/A\nH100-only 3 22.548 38.33%\nA100-Only 5 18.35 24.22%\n16 Mélange 1 2 3 2 28.762 N/A\nH100-only 6 45.096 36.22%\nA100-Only 10 36.7 21.63%\n32 Mélange 1 5 6 4 57.834 N/A\nH100-only 12 90.192 35.88%\nA100-Only 20 73.4 21.21%\nTable 5: Instance allocations for the mixed context dataset, SLO=120ms.", "metadata": {}}, {"text": "18", "metadata": {}}], "metadata": {"page": 18}}], "metadata": {"page": 18}}, {"title": "Page 19", "paragraphs": [{"text": "Rate Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n2 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\nA10G-only 5 5.05 27.33%\nL4-only 9 6.3 41.75%\n4 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\nA10G-only 10 10.1 46.73%\nL4-only 17 11.9 54.79%\n8 Mélange 1 1 2 9.05 N/A\nH100-only 3 15.032 39.80%\nA100-only 3 11.01 17.80%\nA10G-only 16 16.16 44.00%\nL4-only 34 23.8 61.97%\n16 Mélange 6 3 17.07 N/A\nH100-only 4 30.064 43.22%\nA100-only 6 22.02 22.48%\nA10G-only 40 40.4 57.75%\nL4-only 68 47.6 64.14%\n32 Mélange 8 6 30.1 N/A\nH100-only 7 52.612 42.79%\nA100-only 9 33.03 8.87%\nA10G-only 80 80.8 62.75%\nL4-only 135 94.5 68.15%\nTable 6: Instance allocations for the short-context Arena dataset, SLO=40ms.\n19", "sentences": [{"text": "Rate Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 2 1 2.41 N/A\nH100-only 1 7.516 67.94%\nA100-only 1 3.67 34.33%\nA10G-only 3 3.03 20.46%\nL4-only 5 3.5 31.14%\n2 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\nA10G-only 5 5.05 27.33%\nL4-only 9 6.3 41.75%\n4 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\nA10G-only 10 10.1 46.73%\nL4-only 17 11.9 54.79%\n8 Mélange 1 1 2 9.05 N/A\nH100-only 3 15.032 39.80%\nA100-only 3 11.01 17.80%\nA10G-only 16 16.16 44.00%\nL4-only 34 23.8 61.97%\n16 Mélange 6 3 17.07 N/A\nH100-only 4 30.064 43.22%\nA100-only 6 22.02 22.48%\nA10G-only 40 40.4 57.75%\nL4-only 68 47.6 64.14%\n32 Mélange 8 6 30.1 N/A\nH100-only 7 52.612 42.79%\nA100-only 9 33.03 8.87%\nA10G-only 80 80.8 62.75%\nL4-only 135 94.5 68.15%\nTable 6: Instance allocations for the short-context Arena dataset, SLO=40ms.", "metadata": {}}, {"text": "19", "metadata": {}}], "metadata": {"page": 19}}], "metadata": {"page": 19}}, {"title": "Page 20", "paragraphs": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 4 14.68 N/A\nH100-only 2 15.032 2.34%\nA100-Only 4 14.68 0.00%\n2 Mélange 1 3 26.218 N/A\nH100-only 4 30.064 12.79%\nA100-Only 9 33.03 20.62%\n4 Mélange 3 5 48.59 N/A\nH100-only 7 52.612 7.64%\nA100-Only 17 62.39 22.12%\n8 Mélange 3 12 101.202 N/A\nH100-only 14 105.224 3.82%\nA100-Only 34 124.78 18.90%\n16 Mélange 11 21 198.206 N/A\nH100-only 28 210.448 5.82%\nA100-Only 67 245.89 19.39%\n32 Mélange 24 40 388.72 N/A\nH100-only 56 420.896 7.64%\nA100-Only 133 488.11 20.36%\nTable 7: Instance allocations for the long-context PubMed dataset, SLO=40ms.\nRate (req/s) Solver L4 A10G A100 H100 Norm. Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\n2 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\n4 Mélange 3 1 10.546 N/A\nH100-only 2 15.032 29.84%\nA100-only 3 11.01 4.21%\n8 Mélange 1 3 2 1 18.586 N/A\nH100-only 4 30.064 38.18%\nA100-only 6 22.02 15.59%\n16 Mélange 2 7 2 3 38.358 N/A\nH100-only 7 52.612 27.09%\nA100-only 12 44.04 12.90%\n32 Mélange 15 6 5 74.75 N/A\nH100-only 13 97.708 23.50%\nA100-only 24 88.08 15.13%\nTable 8: Instance allocations for the mixed long/short context dataset, SLO=40ms.\n20", "sentences": [{"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 4 14.68 N/A\nH100-only 2 15.032 2.34%\nA100-Only 4 14.68 0.00%\n2 Mélange 1 3 26.218 N/A\nH100-only 4 30.064 12.79%\nA100-Only 9 33.03 20.62%\n4 Mélange 3 5 48.59 N/A\nH100-only 7 52.612 7.64%\nA100-Only 17 62.39 22.12%\n8 Mélange 3 12 101.202 N/A\nH100-only 14 105.224 3.82%\nA100-Only 34 124.78 18.90%\n16 Mélange 11 21 198.206 N/A\nH100-only 28 210.448 5.82%\nA100-Only 67 245.89 19.39%\n32 Mélange 24 40 388.72 N/A\nH100-only 56 420.896 7.64%\nA100-Only 133 488.11 20.36%\nTable 7: Instance allocations for the long-context PubMed dataset, SLO=40ms.", "metadata": {}}, {"text": "Rate (req/s) Solver L4 A10G A100 H100 Norm.", "metadata": {}}, {"text": "Cost\n($/hr) Savings\n1 Mélange 1 3.67 N/A\nH100-only 1 7.516 51.17%\nA100-only 1 3.67 0.00%\n2 Mélange 1 1 1 5.38 N/A\nH100-only 1 7.516 28.42%\nA100-only 2 7.34 26.70%\n4 Mélange 3 1 10.546 N/A\nH100-only 2 15.032 29.84%\nA100-only 3 11.01 4.21%\n8 Mélange 1 3 2 1 18.586 N/A\nH100-only 4 30.064 38.18%\nA100-only 6 22.02 15.59%\n16 Mélange 2 7 2 3 38.358 N/A\nH100-only 7 52.612 27.09%\nA100-only 12 44.04 12.90%\n32 Mélange 15 6 5 74.75 N/A\nH100-only 13 97.708 23.50%\nA100-only 24 88.08 15.13%\nTable 8: Instance allocations for the mixed long/short context dataset, SLO=40ms.", "metadata": {}}, {"text": "20", "metadata": {}}], "metadata": {"page": 20}}], "metadata": {"page": 20}}]}