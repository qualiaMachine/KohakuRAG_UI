id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation,Usage,Quote,Table,Figure,Math,is_NA
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4,4,percent,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],Sustainability: 'The total energy consumption of the US data centers increased by about 4% from 2010-2014',Quote,Private,1,0,0,0,0
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year-the equivalent of taking more than 13,900 cars off the road.",Quote,Private,1,0,0,0,0
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,data centers,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24",Quote,Public,1,0,0,0,0
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg/GPU,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],"""NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions [-] equal 463 kg per GPU.""",Quote,Public,1,0,0,0,0
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400x,400,ratio,"['cottier2025', 'li2025a']","['https://arxiv.org/pdf/2405.21015', 'https://arxiv.org/pdf/2309.03852']","GPT-4 cost (cottier2025), Quote: ""OpenAI's GPT-4 at $40M"". FLM-101B budget (li2025), Quote: ""trained with our growth strategy under a budget of $100K""","The cost for GPT-4 was $40,000,000, and the budget for FLM-101B was 100,000.Calculation:40,000,000/100,000=400.",Public,0,0,0,1,0
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,~1.2 tCO2e,1.2,tCO2e,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Table 2,Table,Public,0,1,0,0,0
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,score,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. ""On average, FLM-101B achieves a score of 43.94""","Table 4, Quote",Public,1,1,0,0,0
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,6750,6750,fold,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"Microprocessors at 50: 'This is a more than 6,750 fold improvement in processor clock speed...'",6750,Public,1,0,0,0,0
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",~14.8 days,14.8,days,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Training time (Table 1, row 12; Table 4, row 15) : We have the exact training time for Google DNNs. OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS. For the CO 2 e calculation, it doesn?t actually matter whether it takes 2 weeks on 10,000 GPUs or 20 weeks on 1,000 GPUs, but we need one number for Table 4, so we used NVIDIA?s suggestion of 10,000 GPUs.""",Quote,Public,1,1,0,0,0
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],Table 3,Table,Private,0,1,0,0,0
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Approximately 13000 tons,13000,tons,['han2024'],['https://arxiv.org/pdf/2412.06288'],"For instance, the total permitted annual emission limits for data centers in northern Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.",Quote,Public,1,0,0,0,0
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,percent,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).",Quote,Public,1,0,0,0,0
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,1300,1300,deaths,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Under McKinsey's projection with a medium growth rate, the scope-2 pollutants of U.S. data centers in 2030 alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 deaths...",Quote,Private,1,0,0,0,0
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"This model was not trained to completion, but only until 13%; a full training run would take 60 days.",Quote,Public,1,0,0,0,0
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],Appendix Table 2,Table 2 in the Appendix shows the GPU Energy (Wh) for various models. The highest value listed is 3426.12 Wh for the Command-R Plus model.,Public,0,1,0,0,0
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,year,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.""",Quote,Public,1,0,0,0,0
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,percent,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"The UN-s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].",Quote,Public,1,0,0,0,0
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,520 MWh,520,MWh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],Appendix Table 1,Table 1 in the Appendix.,Private,0,1,0,0,0
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.10%,0.1,percent,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models 1500B parameters but only 0.1% activated per token""can deliver up to 7x increases in pre-training speed with the same computational resources. We estimated it used 179 MWh and produced 59 net tCO 2 e.""",Quote,Public,1,0,0,0,0
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8,8,experts,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""Then, we set the same number of experts to 8 and top-k to 2 for every layer. Table 1 shows the key hyperparameters in JetMoE-8B.""",Table 1,Private,0,1,0,0,0
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",1 second,1,second,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 5,Figure,Private,0,0,1,0,0
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,['li2025a'],['https://arxiv.org/pdf/2309.03852'],The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,Quote and Table 4,Public,1,1,0,0,0
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Intel Core i7-1165G7,Intel Core i7-1165G7,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Section IV.A: '11th Gen Intel(R) Core(TM) i7-1165G7 processor',Quote,Private,1,0,0,0,0
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88,88,models,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency.",Quote,Public,1,0,0,0,0
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3x,3,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 9 and text: ""Figure 9 illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3x.""","Figure, Quote",Public,1,0,1,0,0
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2,4]",multiplier,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.",Quote,Public,1,0,0,0,0
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh...",Quote,Public,1,0,0,0,0
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"This paper examines how the problem of Jevons- Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.",Quote,Public,1,0,0,0,0
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 to 6.6 billion cubic meters,"[4.2,6.6]",billion cubic meters,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""More critically, the global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in 2027...""",Quote,Public,1,0,0,0,0
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""Red AI is on the rise despite the well-known diminishing returns of increased cost""",Quote,Public,1,0,0,0,0
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).",Quote,Public,1,0,0,0,0
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 10 and caption: ""A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.""","Figure, Quote",Public,1,0,1,0,0
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh of electricity and over 700 kL of water",1287,MWh,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO2e [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13]...",Quote,Public,1,0,0,0,0
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"For instance, the AI Energy Score project provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.",Quote,Public,1,0,0,0,0
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",1000 microseconds,1000,microseconds,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 6,Figure,Private,0,0,1,0,0
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2,2,experts,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""Then, we set the same number of experts to 8 and top-k to 2 for every layer.""",Table 1,Public,0,1,0,0,0
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""This progress has been achieved by increasingly large and computationally-intensive deep learning models. The amount of compute used to train deep learning models has increased 300,000x in 6 years.""",Quote,Private,1,0,0,0,0
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4,6.4,percent,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],Sustainability: 'global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion...',Quote,Public,1,0,0,0,0
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22 data centers,22,data centers,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],…100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources,Quote,Public,1,0,0,0,0
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""While the rate of progress in AI has been patchy and unpredictable, there have been significant advances since the field's inception sixty years ago.""",Quote,Public,1,0,0,0,0
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural architecture search (NAS),Neural architecture search (NAS),is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"The ""five cars"" number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large... In the case of the latter, they estimated that the NAS approach... could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.",Quote,Private,1,0,0,0,0
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",44%,44,percent,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.",Quote,Public,1,0,0,0,0
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,8 samples per batch,8,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Table 3,Table,Private,0,1,0,0,0
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],We invest in energy storage […] We now hold 1.3 GW of storage capacity,Quote,Private,1,0,0,0,0
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"Approximately 2,300",2300,flights,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 2,300 transatlantic flights between Boston and London.",Quote,Private,1,0,0,0,0
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,PUE,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2 billion,2000000000,parameters,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.""",Quote,Public,1,0,0,0,0
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],Appendix Table 1,Table 1 in the Appendix.,Public,0,1,0,0,0
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"21,600 electric delivery vans",21600,electric delivery vans,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],Goals Summary Table,Table. Add across both 2023 and 2023: 2600+19000,Public,0,1,0,0,0
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data centers using cooling.""",Quote,Private,1,0,0,0,0
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,39.223 Wh,39.223,Wh,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"In contrast, o3 consumes 39.223 Wh, while DeepSeek-R1 and GPT-4.5 consume 33.634 Wh and 30.495 Wh, respectively...",Quote,Private,1,0,0,0,0
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,year,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""",Quote,Private,1,0,0,0,0
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,WUE,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"Demographic Inclusion: 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [IEA]'",Quote,Private,1,0,0,0,0
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3 to 4 joules per token,"[3,4]",joules per token,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"""with length 512, we see that it takes about 3-4 Joules for a output token""",Quote,Private,1,0,0,0,0
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,percent,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"""By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.""",Quote,Public,1,0,0,0,0
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed... Applying observations made from individual projects to the entire planet-s GHG emissions lacks any scientific grounding...,Quote,Public,1,0,0,0,0
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters""",Quote,Public,1,0,0,0,0
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000 ",25000,USD,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""AI2, our home organization, recently released Grover...Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.""",Quote,Public,1,0,0,0,0
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,percent,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"""The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)""",Quote,Public,1,0,0,0,0
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],...inference happens far more frequently than model training as many as billions of times a day for a model powering a popular user-facing product such as Google Translate... [Table 3 shows Flan-T5-xxl energy as 0.083 kWh],"Daily 1k-query blocks: 1,000,000,000 / 1,000 = 1,000,000. Daily energy: 1,000,000 * 0.083 kWh = 83,000 kWh, which is 83 MWh.",Public,0,1,0,1,0
q067,What was the average global data center PUE in 2023?,1.58,1.58,PUE,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 2 Technical Background: 'The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.',Quote,Public,1,0,0,0,0
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,wind turbines,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,percent,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.",Figure 6 and Quote,Private,1,0,1,0,0
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17,17,people,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""The Standing Committee defined a Study Panel charge for the inaugural Study Panel in the summer of 2015 and recruited Professor Peter Stone, at the University of Texas at Austin, to chair the panel. The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.""",Quote,Private,1,0,0,0,0
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,percent,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"""Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.""",Quote,Public,1,0,0,0,0
q072,True or False: A model with more parameters will always consume more energy during inference.,FALSE,0,is_blank,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [21].",Quote,Public,1,0,1,0,0
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.""",Quote,Public,1,0,0,0,0
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,tCO2e,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tons CO2e",11390,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"...and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the ""five cars"" estimate.",Quote,Public,1,0,0,0,0
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9x,2.9,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 2(d) and surrounding text: ""Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9Ã- increase in AI training infrastructure capacity over the 1.5 years.""","Figure, Quote",Public,1,0,1,0,0
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,miles,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning""",Quote,Public,1,0,0,0,0
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,['fernandez2025'],['https://arxiv.org/pdf/2504.17674'],"""Static batching
maintains a fixed batch size throughout inference,
which leads to GPU under-utilization when generation lengths vary and idle compute accumulates
after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization
and reducing idle time (Yu et al., 2022).""",Quote,Private,1,0,0,0,0
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60,60,H100 GPU hours,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""The entire alignment process takes 60 H100 GPU hours.""",Quote,Private,1,0,0,0,0
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.70%,26.7,percent,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.",Quote,Public,1,0,0,0,0
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",1594 g CO2eq,1594,g CO2eq,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]...",Quote,Private,1,0,0,0,0
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to over 3,426 Wh","[0.06,3,426]",Wh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Inference workloads also show wide variation depending on model size, architecture and task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity...",Quote and Table 2,Public,1,1,0,0,0
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""We recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies like AI, it can be difficult to define what progress looks like and how it can be achieved. We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)...""",Quote,Public,1,0,0,0,0
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429,0.429,kg of CO2e/KWh,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO 2 e/KWh [USE21].""",Quote,Public,1,0,0,0,0
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Section 2.1: 'Hivemind [39] is a PyTorch-based framework... can handle peers that drop out at any stage of the training.',Quote,Public,1,0,0,0,0
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.""",Quote,Public,1,0,0,0,0
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Linear Embedding,Linear Embedding,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Table B1: Linear Embedding - Accuracy 0.57, Energy 0.12 Wh",Linear Embedding outperformed all LLMs in accuracy while being highly energy-efficient,Public,0,1,0,0,0
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,['chen2024'],['https://arxiv.org/pdf/2405.01814'],"To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.",Quote,Public,1,0,0,0,0
q093,How many parameters does the largest T5 model have?,11 billion,11000000000,parameters,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable application of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO 2 e.""",Quote,Public,1,0,0,0,0
q094,What is the total number of parameters in the JetMoE-8B model?,8 billion,8000000000,parameters,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token...""",Quote,Public,1,0,0,0,0
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,~20%,20,percent,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""Importantly, the company's data center water consumption increased by ~20% from 2021 to 2022 and by ~17% from 2022 to 2023 [4]...""",Quote,Public,1,0,0,0,0
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Table I: 'Carbon Intensity - gCO₂/kWh',Quote,Private,1,1,0,0,0
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,percent,['li2025a'],['https://arxiv.org/pdf/2309.03852'],Table 2: Parallel strategies and throughput for different growth stages.,Table 2,Public,0,1,0,0,0
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40 million,40000000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.,Quote,Public,1,0,0,0,0
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810x,810,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"""Efficiency Optimization: Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810x.""",Quote,Public,1,0,0,0,0
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.59,0.59,multiplier,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Figure 9: NLP throughput 2.15× speedup (C-8) vs 3.67× speedup (A-8),2.15/3.67=0.59,Private,0,0,1,1,0
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],3.5B Liters of water returned to communities from replenishment projects in 2023,Quote,Private,1,0,0,0,0
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,['rubei2025'],['https://arxiv.org/pdf/2501.05899'],"""Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.""",Quote,Public,1,0,0,0,0
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million,3700000,GPUs,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"While efficiency improvements are being made to the hardware used for training and deploying AI models [9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].",Quote,Public,1,0,0,0,0
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,percent,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.",Quote,Public,1,0,1,0,0
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.1,1.1,PUE,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"""To quantify the emissions of Facebook's models we measure the total energy consumed, assume location-based carbon intensities for energy mixes, and use a data center Power Usage Effectiveness (PUE) of 1.1.""",Quote,Private,1,0,0,0,0
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS (Ethical AI for the Governance of the Society),ETAIROS,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""From a regulatory perspective, the Finnish ETAIROS (Ethical Al for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]...""",Quote,Private,1,0,0,0,0
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30 million,30000000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.,Quote,Public,1,0,0,0,0
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,TRUE,1,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 4.4 Environmental Risk Assessment: 'these provisions relate to risks... which, within the AI Act, must be interpreted as including environmental risks.'",Quote,Public,1,0,0,0,0
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m³,9,µg/m³,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Concretely, the EPA's recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³, considerably higher than the WHO's recommended level of 5 µg/m³ [30,31].",Quote,Public,1,0,0,0,0
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"For instance, a life cycle assessment (LCA)... has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].",Quote,Public,1,0,0,0,0
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,multiplier,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Further, the public health costs unevenly impact economically-disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.",Quote,Private,1,0,0,0,0
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,702.06,702.06,Wh,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Table B1: DS Llama 70B - Energy 702.06 Wh, Accuracy 0.46",This was the highest recorded energy consumption across all models tested,Public,0,1,0,0,0
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion,6100000000,parameters,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"In this paper, we provide a framework for measuring software carbon intensity... including pretraining of a 6.1 billion parameter language model.",Quote,Public,1,0,0,0,0
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons paradox,Jevons paradox,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use [93].""",Quote,Private,1,0,0,0,0
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,~5 times,5,multiplier,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Table 4,"Find the energy efficiency of GShard-600B. We need its energy consumption and total computation.
From Table 4 (page 6), GShard-600B used 24.1 MWh for 1.33E+22 FLOPS.
Efficiency_GShard = 24.1 MWh / 1.33E+22 FLOPS = 1.81E-22 MWh/FLOP.

Find the total computation for GPT-3.
From Table 4, GPT-3 required 3.14E+23 FLOPS.

Calculate the hypothetical energy cost for GPT-3 with GShard's efficiency.
Hypothetical Energy_GPT3 = 3.14E+23 FLOPS * 1.81E-22 MWh/FLOP = 56.8 MWh.

Calculate the energy savings.
Actual Energy_GPT3 (from Table 4) = 1,287 MWh.
Energy Saved = 1,287 MWh - 56.8 MWh = 1,230.2 MWh.

Find the energy cost to train Meena.
From Table 4, training Meena costs 232 MWh.

Calculate how many times Meena could be trained with the saved energy.
Number of Trainings = 1,230.2 MWh / 232 MWh= 5.3 times.",Public,0,1,0,1,0
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.",Table,Public,0,1,0,0,0
q120,How many pounds of CO2e are estimated for an average American life in one year?,36156 lbs,36156,lbs,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],Table 1,Table,Public,0,1,0,0,0
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Mason County,Mason County,is_blank,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Figure 3(c): ""Top-10 counties by per-household health cost"" table.","Figure, Table",Public,0,1,0,0,0
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,1.33,1.33,multiplier,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Table III: Mistral-small - 0.020→0.015 kg CO2,0.020/0.015=1.33,Private,0,1,0,1,0
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"Table 5. [Training energy (kWh) for BLOOMz-7B is 51,686, Finetuning energy (kWh) is 7,571]","Based on Table 5, the training energy is 51,686 kWh and the fine-tuning energy is 7,571 kWh. Calculation: 51,686 + 7,571 = 59,257 kWh.",Public,0,1,0,1,0
q125,What is the total number of parameters in the final FLM-101B model?,101 billion parameters,1.01E+11,parameters,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.",Table 3,Private,0,1,0,0,0
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",1.035 billion inferences,1035000000,inferences,"['dodge2022', 'luccioni2024']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2311.16863']","Training Energy (dodge2022), Quote: ""we estimate the total energy consumption to train this model to completion would be approximately... 103.5 MWh, or 103,500 kWh"". Inference Energy (luccioni2024), Table 5: ""Inference energy (kWh) for BLOOMz-7B is 1.0 x 10-4""","Divide the total training energy by the energy per inference to find the cost parity point. Calculation: 103,500 kWh / (1.0e-4 kWh/inference) = 1,035,000,000 inferences.",Public,0,0,0,1,0
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.",Quote,Public,1,0,0,0,0
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000",592570000,inferences,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","Based on Table 5, the total training and finetuning energy is 51,686 kWh + 7,571 kWh = 59,257 kWh. The inference energy is 1.0e-4 kWh per inference. The cost parity point is therefore 59,257 kWh / (1.0e-4 kWh/inference) = 592,570,000 inferences.",Public,0,1,0,1,0
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,FKTG,FKTG,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Section 3: 'published as the FKTG-dataset',Quote,Public,1,0,0,0,0
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,liters,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,~3 passengers,3,passengers,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.""",Quote,Public,1,0,0,0,0
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,percent,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.",Figure and Caption,Public,1,0,1,0,0
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 NVIDIA A100 80 GB GPU,1,A100_80GB_GPU,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 13B model requires 1 NVIDIA A100 GPU (80GB each) as the minimum hardware to run inference,Private,0,1,0,0,0
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21,78]",metric tons,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).",Quote,Private,1,0,0,0,0
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,tCO2e,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,percent,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.,"Figure 9, Quote",Private,1,0,1,0,0
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",$4.63/hr for H20,4.63,USD per hour,['chen2024'],['https://arxiv.org/pdf/2405.01814'],"Table 1: H100, H20, and TPU v6e specifications.",Table 1,Public,0,1,0,0,0
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""In fact, most carbon footprint analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of Al papers to get information needed to estimate the carbon footprint of their models...""",Quote,Public,1,0,0,0,0
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",43%,43,percent,['han2024'],['https://arxiv.org/pdf/2412.06288'],Table 1,Table,Public,0,1,0,0,0
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA A100 80 GB GPU,1,A100_80GB_GPU,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 7B model requires 1 NVIDIA A100 GPU (80GB each) as the minimum hardware to run inference,Private,0,1,0,0,0
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Abstract: 'reduce energy consumption and carbon emissions by up to 45% post quantization',Quote,Public,1,0,0,0,0
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95,95,answers,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"""For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of Al papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers...""",Quote,Private,1,0,0,0,0
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",~$3.33 per hour,3.33,USD per hour,['shen2024'],['https://arxiv.org/pdf/2404.07413']," ""trained with less than $0.1 million … and 30,000 H100 GPU hours … The entire alignment process takes 60 H100 GPU hours.""","Using the stated pretraining budget upper bound and GPU-hours → cost_per_GPUh ≈ $100,000 ÷ 30,000 GPUh = $3.33/GPUh (upper-bound estimate since budget is ""< $0.1M"").",Public,1,0,0,1,0
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,percent,['han2024'],['https://arxiv.org/pdf/2412.06288'],Table 3,Table,Public,0,1,0,0,0
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion,1.25E+12,tokens,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.",Quote,Public,1,0,0,0,0
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36 projects,36,projects,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],Table pg. 26,Table,Public,0,1,0,0,0
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",53.20%,53.2,percent,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],Amazon Representation by the Numbers Figure,Figure,Private,0,0,1,0,0
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,percent,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].""",Quote,Public,1,0,0,0,0
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",1.5 seconds,1.5,seconds,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 4,Figure,Public,0,0,1,0,0
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,Granularity,Granularity,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Section 3: 'We propose the granularity metric to compare model suitability for distributed spot training',Quote,Public,1,0,0,0,0
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times (or 640%),6.4,times,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production... could add up to 640 percent more carbon emissions compared to the company-s carbon removal targets for the year [119]...",Quote,Private,1,0,0,0,0
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses...""",Quote,Public,1,0,0,0,0
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,every 5 years,5,years,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.""",Quote,Public,1,0,0,0,0
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25,25,devices,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"Deloitte 2021 survey: connected devices include smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines. Average reported = 25 per U.S. household.",Quote,Public,1,0,0,0,0
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 to 3,500 MWh","[0.8,3500]",MWh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"From the limited data that is publicly available, we can observe significant disparities in energy use and emissions across models. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).",Quote and Table 1,Public,1,1,0,0,0
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"""IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.""",Quote,Public,1,0,0,0,0
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10 to 50 queries,"[10,50]",queries,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10-50 queries on GPT-3 consumes around half a liter of water [68].",Quote,Private,1,0,0,0,0
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,score,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.""","Table 4, Quote",Public,1,1,0,0,0
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10 to 50 responses,"[10,50]",responses,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""Additionally, GPT-3 needs to -drink- (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed."", ""More specifically, we consider a medium-sized request, each with approximately=800 words of input and 150 - 300 words of output [30]. """,Quote,Public,1,0,0,0,0
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,percent,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.",Quote,Public,1,0,0,0,0
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4  A100 80 GB GPUs,4,A100_80GB_GPUs,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 65B model requires 4 NVIDIA A100 GPUs (80GB each) as the minimum hardware to run inference,Public,0,1,0,0,0
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,More than 10000,10000,round trips,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Our findings reveal that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.",Quote,Public,1,0,0,0,0
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80-90%,"[80,90]",percent,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""For example, NVIDIA estimated that 80-90% of the ML workload is inference processing [Leo19]""",Quote,Private,1,0,0,0,0
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2eq,178.97,kg CO2eq,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.",Quote,Public,1,0,0,0,0
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,['chung2025'],['https://arxiv.org/pdf/2505.06371'],Estimations using TDP are nearly always an overestimation since it is rare for a GPU or any computing device to draw its maximum power at every moment in time.,Quote,Public,1,0,0,0,0
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,FALSE,0,is_blank,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Interestingly, GPT-4o mini, although substantially smaller in parameter count, consumes slightly more energy per query than GPT-4o due to its deployment on less efficient A100 hardware...",Quote,Public,1,0,0,0,0
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",0.4 queries/sec,4,queries/sec,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 15,Figure,Public,0,0,1,0,0
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.",Quote,Private,1,0,0,0,0
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.52 per hour,7.52,USD per hour,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"We calculate this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) x 3.67 = $7.516 for H100.",Table 1 and Quote,Public,1,1,0,0,0
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,liters of water,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$3.61 per hour,3.61,USD per hour,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","$5200/(30*24)/2
- Divide monthly cost by total hours, then divide by total GPUs used",Public,0,0,0,1,0
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000x larger,1000,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 2(a) caption: ""For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000Ã- larger in size.""","Figure, Quote",Public,1,0,1,0,0
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","Approximately 730,000 miles",730000,miles,"['strubell2019', 'luccioni2024']","[https://arxiv.org/abs/1906.02243, https://arxiv.org/pdf/2311.16863']","Transformer+NAS emissions (strubell2019), Table 3: ""CO2e (lbs) 626,155"". Conversion rate (luccioni2024), Quote: ""...1,594 grams of CO2eq ... equivalent to 4.1 miles driven...""","First, convert emissions from lbs to grams: 626,155 lbs * 453.592 g/lb ≈ 284,018,490 g. Then, use the ratio from luccioni2024 (1594 g / 4.1 miles ≈ 388.8 g/mile) to find the equivalent miles. Calculation: 284,018,490 g / 388.8 g/mile ≈ 730,500 miles.",Private,0,0,0,1,0
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","Approximately 60,610 MWh",60610,MWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"...at the time of writing this article (November 2023), BLOOMz-7B has been downloaded 606,096 times... [Table 5 shows Inference energy (kWh) for BLOOMz-7B is 1.0 x 10^-4]","Calculation: 606,096 downloads * 1,000,000 inferences/download * 1.0e-4 kWh/inference = 60,609,600 kWh, which is ~60,610 MWh.",Private,0,1,0,1,0
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,30000,30000,H100 GPU hours,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.""",Quote,Private,1,0,0,0,0
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",$1 billion,1000000000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.",Quote,Public,1,0,0,0,0
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23,3.14E+23,FLOPS,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Training time (Table 1, row 12; Table 4, row 15) : We have the exact training time for Google DNNs. OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS. For the CO 2 e calculation, it doesn't actually matter whether it takes 2 weeks on 10,000 GPUs or 20 weeks on 1,000 GPUs, but we need one number for Table 4, so we used NVIDIA's suggestion of 10,000 GPUs.""",Quote,Public,1,1,0,0,0
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 NVIDIA V100 32 GB GPUs,8,V100_32GB_GPUs,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 65B model requires 8 NVIDIA V100 GPUs (32GB each) as the minimum hardware to run inference,Private,0,1,0,0,0
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Approximately 17.9 zettaFLOPs,17.9,zettaFLOPs,['li2025a'],['https://arxiv.org/pdf/2309.03852'],[Table 2 shows the 101B stage used 192 GPUs at 165 teraFLOP/s per GPU]... [Table 1 shows the 101B stage took 6.54 days],"Total throughput = 192 GPUs * 165e12 FLOPs/sec/GPU= 3.168e16 FLOPs/sec. Total seconds = 6.54 days * 24 * 3600= 565,056 seconds. Total FLOPs = 3.168e16 * 565,056= 1.789e22 FLOPs, which is ~17.9 zettaFLOPs.",Public,0,1,0,1,0
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,56.40%,56.4,percent,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],Figure 4,Figure,Private,0,0,1,0,0
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192,192,GPUs,['li2025a'],['https://arxiv.org/pdf/2309.03852'],FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8-80G) servers... Number of GPUs 192,Quote and Table 2,Public,1,1,0,0,0
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","626,155 lbs, equivalent to 17.31 American lifetimes",17.31,lifetimes,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"[Table 1, Table 3]",Table,Public,0,1,0,0,0
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 hours",25000,hours,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""FAIR's RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.""",Quote,Private,1,0,0,0,0
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,metric tons,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"These on-site solar energy systems are estimated to generate 123,000 MWh annually - enough energy to power over 33,600 European homes - and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.	Quote	Public	OK
q008	How many metric tons of CO2e, in millions, do Amazon's on-site solar projects across Amazon and Whole Foods Market facilities avoid as of 2023?	17 million metric tons of CO2e	17000000	metric tons CO2e	['amazon2023']	['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report']	Altogether, these projects avoid roughly 17 million metric tons of CO2e each year.""",Quote,Public,1,0,0,0,0
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,vLLM,vLLM,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Section 3.3: 'LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm)',Quote,Private,1,0,0,0,0
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,1.952263374,1.952263374,multiplier,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Table B2: Llama 3.1 70B - Single node 48.60 Wh vs Double node 94.88 Wh,94.88/48.60,Public,0,1,0,1,0
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,gallons of water,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,3500000%,35000,homes,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"These include electricity use comparable to 35,000 U.S. homes...",Quote,Private,1,0,0,0,0
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,percent,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].",Quote,Public,1,0,0,0,0
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Table B4: Linear Embedding - Accuracy 0.43; Llama 3.1 70B - Accuracy 0.67,0.67/0.43,Public,0,1,0,1,0
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,PUE,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.""",Quote,Private,1,0,0,0,0
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",Approximately 772 billion,7.72E+11,queries,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],...yielding a total of approximately 772 billion GPT-4o queries in 2025...,Quote,Public,1,0,0,0,0
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53,53,score,['shen2024'],['https://arxiv.org/pdf/2404.07413'],Table 3: OpenLLM leaderboard and code benchmarks results from four different models.,Table 3,Public,0,1,0,0,0
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,training runs,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 4.3 Transparency: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk... Recital 102... does not mandate the disclosure of energy consumption.',Quote,Public,1,0,0,0,0
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,PUE,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""In 2020, it was 1.59""",Quote,Private,1,0,0,0,0
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312 GB,5.312,GB,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","Quote, Figure 1",Private,1,0,1,0,0
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29,49]",percent,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.",Quote,Public,1,0,0,0,0
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Section 3.3: 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package',Quote,Private,1,0,0,0,0
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,percent,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search...",Figure and Text,Public,0,0,1,0,0
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function,is_blank,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.",Quote,Public,1,0,0,0,0
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Figures 8-9,Energy cost per response consistently rose as more GPUs were used for sharding.,Private,0,0,1,0,0
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",11 kL,11,kL,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],"""We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water""",Quote,Public,1,0,0,0,0
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 4.3 Transparency: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk... does not mandate the disclosure of energy consumption.',Quote,Private,1,0,0,0,0
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",Almost 30%,30,percent,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.",Quote,Public,1,0,0,0,0
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$5.6 billion,5600000000,USD,['han2024'],['https://arxiv.org/pdf/2412.06288'],Table 1,Table,Public,0,1,0,0,0
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,More than 70 times,70,multiplier,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano...",Quote,Public,1,0,0,0,0
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",15-77%,"[15,77]",percent,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","Quote, Figure 11",Public,1,0,1,0,0
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 tCO2e,26,tCO2e,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.",Table 3,Public,0,1,0,0,0
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",2 seconds,2,seconds,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 5,Figure,Public,0,0,1,0,0
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Critically, the health costs are unevenly distributed across counties and communities, disproportionately affecting low-income counties...",Quote,Private,1,0,0,0,0
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"Figure 2: 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019]'",Quote,Public,1,0,1,0,0
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],"Section III.B.1: 'We apply quantization through Ollama [19], an open-source platform...'",Quote,Private,1,0,0,0,0
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze B2,Backblaze B2,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"Section 3: 'we chose an independent S3 storage provider, Backblaze (B2)'",Quote,Private,1,0,0,0,0
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",TRUE,1,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Figure 6: Relationship between duration and energy consumption (R² ≈ 0.998),R²=0.998,Private,0,0,1,0,0
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Edward J. Markey,Edward J. Markey,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 3.4: 'On 1 Feb 2024, the bill for an AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA).'",Quote,Public,1,0,0,0,0
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06/hr for H100,11.06,USD per hour,['chen2024'],['https://arxiv.org/pdf/2405.01814'],"Table 1: H100, H20, and TPU v6e specifications.",Table 1,Public,0,1,0,0,0
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,years,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 NVIDIA V100 32 GB GPUs,2,V100_32GB_GPUs,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 13B model requires 2 NVIDIA V100 GPUs (32GB each) as the minimum hardware to run inference,Private,0,1,0,0,0
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?",1247.61 tons CO2e,1247.61,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the ""five cars"" number...",Quote,Public,1,0,0,0,0
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"[Table 3, ""We train all models on
a single NVIDIA Titan X GPU, with the exception
of ELMo which was trained on 3 NVIDIA
GTX 1080 Ti GPUs.""]","Table, Quote",Private,1,1,0,0,0
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.""",Quote,Public,1,0,0,0,0
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.1,1.1,PUE,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"Figure 1: 'PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021)'",Quote,Private,1,0,1,0,0
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,percent,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy-a goal that Amazon, including AWS, achieved in 2023",Quote,Public,1,0,0,0,0
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,"$3,460 ",3460,USD,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"""Our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""",Quote,Public,1,0,0,0,0
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50-70%,"[50,70]",percent,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50-70% of the total provisioned power in the datacenter [50].",Quote,Private,1,0,0,0,0
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96,96,H100 GPUs,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""We conduct training on a cluster containing 12 nodes and 96 H100s.""",Quote,Public,1,0,0,0,0
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",Over 600W,600,Watts,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],"""Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo2 7B training-When actively training, the average GPU power is over 600W""",Quote,Public,1,0,1,0,0
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,11023 lbs,11023,lbs,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],Table 1,Table,Private,0,1,0,0,0
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25x,1.25,multiplier,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"anywhere from a 2 times (7B) to a 1.25 times increase (13B) … on the A100 when compared to the V100""",Quote,Private,1,0,0,0,0
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.43 Wh,0.43,Wh,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts.",Quote,Public,1,0,0,0,0
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280.00%,280,percent,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave's top choice.",Quote and Table V,Public,1,1,0,0,0
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,V100,V100,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Section 4.1.3: 'For models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference.'",Quote,Private,1,0,0,0,0
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""In addition, we propose reporting the financial cost or ""price tag"" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Reporting the computational price tag of finding, training, and running models is a key Green AI practice""",Quote,Public,1,0,0,0,0
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,metric tons,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"Al-s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.",Quote,Private,1,0,0,0,0
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Table 3 math: 325 (V100 GPU) -221 (TPU v2),Table,Public,0,1,0,0,0
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.""",Quote,Public,1,0,0,0,0
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20x,20,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 2(c), ""Facebook's recommendation and ranking model sizes have increased by 20 times during the same time period [11].""","Figure, Quote",Public,1,0,1,0,0
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Claude-3.7 Sonnet,Claude-3.7 Sonnet,is_blank,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Claude-3.7 Sonnet scored highest (0.886), combining strong reasoning with an efficient infrastructure footprint.","Figure, Quote",Public,1,0,1,0,0
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],Sustainability: 'significantly longer lifetimes than the current averages of less than 3 years for cell phones...',Quote,Private,1,0,0,0,0
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,TRUE,1,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"Figure 7b: CV per-GPU speedup 0.43, 0.42, 0.43, 0.41, 0.41",Figure,Private,0,0,1,0,0
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.",Quote,Public,1,0,0,0,0
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [33, 50]. This leads to the GPU's computation throughput being bottlenecked by VRAM bandwidth...",Quote,Public,1,0,0,0,0
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",31.60%,31.6,percent,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],Amazon Representation by the Numbers Figure,Figure,Public,0,0,1,0,0
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61% to 76%,"[61,76]",percent,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"However, if we exclude equity the fraction for R&D staff drops to 19-33%, and the fractions of computing hardware costs and energy rise to 61-76% and 2-7% respectively.",Quote,Public,1,0,0,0,0
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",FALSE,0,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Section V.A: 'metrics like accuracy and F1 score are slightly lower after optimization',Quote,Public,1,0,0,0,0
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"""The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions:  0.954pt",Quote,Public,1,0,0,0,0
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 to 134 TWh,"[85,134]",TWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""For example, a recent study suggests that the global AI could consume 85 - 134 TWh of electricity in 2027 [7]...""",Quote,Public,1,0,0,0,0
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150000000,packages,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],We delivered 150 million packages via EVs,Quote,Public,1,0,0,0,0
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens... The workload evaluates a total of 3000 requests.","The workload consisted of 3000 requests, each with 128 input and 512 output tokens. Calculation: 3000 * (128 + 512) = 1,920,000 total tokens.",Private,0,0,0,1,0
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 4.3 Transparency: 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications... Leaving such applications with significant climate effects out of scope creates a notable reporting gap.',Quote,Private,1,0,0,0,0
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",Up to 80%,80,percent,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US...",Figure 3a and Caption,Public,1,0,1,0,0
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","Over 1,450 times",1450,times,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.,Quote,Public,1,0,0,0,0
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Table B4: Linear Embedding - Accuracy 0.43; Llama 3.1 70B - Accuracy 0.67,0.67/0.43,Private,0,1,0,1,0
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244 projects,244,projects,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],Table pg. 26,Table,Private,0,1,0,0,0
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,percent,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""but Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy.""",Quote,Private,1,0,0,0,0
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,Cumulative server level,Cumulative server level,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 5.2.5 Interim Conclusion: 'we argue that energy consumption should be measured and reported at the cumulative server level.',Quote,Private,1,0,0,0,0
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,percent,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],The GPU alone accounts for 74% of the total energy consumption due to these components.,Table 1 and Caption,Private,1,1,0,0,0
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,NVIDIA A100-80GB GPUs,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.",Quote,Public,1,0,0,0,0
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.50%,28.5,percent,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Figure 8 caption: ""The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).""","Figure, Quote",Public,1,0,1,0,0
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,kilometers of fiberoptic cable,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800 million,800000000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.",Quote,Public,1,0,0,0,0
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,['https://arxiv.org/pdf/2504.00797'],"""The umbrella term 'Sustainable Al' was initially proposed by van Wynsberghe as a field of practice that both aims to use Al in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].""",Quote,Public,1,0,0,0,0
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,5 samples per batch,5,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 13,Figure,Public,0,0,1,0,0
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"Figure 8 and text: ""It can be seen that when the server is overloaded, Swapping consistently consumes less energy.""","Figure, Quote",Public,1,0,1,0,0
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,percent,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ""increases in data center energy consumption"" [42]...",Quote,Private,1,0,0,0,0
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",12%,11.7,percent,['han2024'],['https://arxiv.org/pdf/2412.06288'],"According to McKinsey projections, under a medium-growth scenario [4], the U.S. data centers are anticipated to account for 11.7% of national electricity consumption in 2030...",Quote,Public,1,0,0,0,0
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Up to 25%,25,percent,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"...for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.",Figure 4b and Caption,Public,1,0,1,0,0
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,percent,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.""",Quote,Private,1,0,0,0,0
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 pounds of CO2 emissions",626155,lbs CO2e,luccioni2025b,['https://arxiv.org/pdf/2504.00797'],"""The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions [192].""",Quote,Public,1,0,0,0,0
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].""",Quote,Private,1,0,0,0,0
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"""Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning""",Quote,Public,1,0,0,0,0
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples per batch,2,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Table 3,Table,Private,0,1,0,0,0
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",TRUE,1,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"Section 4.C: 'for CV, even distributing VMs over four continents only slows down performance by 7%'",Quote,Public,1,0,0,0,0
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,hectares,is_blank,is_blank,is_blank,is_blank,Private,0,0,0,0,1
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g CO2eq,0.32,g CO2eq,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"...for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMZ-7B.",Quote,Public,1,0,0,0,0
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","7,000 to 26,000 grams","[7000,26000]",grams,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).",Quote,Public,1,0,0,0,0
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,year,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.",Quote,Public,1,0,0,0,0
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",5 days,5,days,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],Table 2,Table,Public,0,1,0,0,0
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,liters of freshwater,is_blank,is_blank,is_blank,is_blank,Public,0,0,0,0,1
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"""A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers""",Quote,Private,1,0,0,0,0
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","40,000 kWh (or 40 MWh)",40000,kWh,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.",Table 3,Private,0,1,0,0,0
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",$20 billion,20000000000,USD,['han2024'],['https://arxiv.org/pdf/2412.06288'],The total public health burden of U.S. data centers in 2030 is valued at up to more than $20 billion per year...,Quote,Private,1,0,0,0,0
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.70 ,32.7,USD,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Table 4,Table,Public,0,1,0,0,0
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",32 samples,32,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 6,Figure,Public,0,0,1,0,0
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",4 seconds,4,seconds,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 4,Figure,Private,0,0,1,0,0
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 5.2.3: 'Therefore, we do not advocate for using GPU-level or other component-based power consumption tracking as a suitable method for measuring overall energy usage.'",Quote,Public,1,0,0,0,0
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,percent,luccioni2025b,['https://arxiv.org/pdf/2504.00797'],"""...finding that training accounted for only half of the model's overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.""",Quote,Public,1,0,0,0,0
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA V100 32 GB GPU,1,V100_32GB_GPU,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],Table 2,Table II shows that the 7B model requires 1 NVIDIA V100 GPU (32GB each) as the minimum hardware to run inference,Private,0,1,0,0,0
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Approximately 16.7 requests,16.7,requests,['li2025b'],['https://arxiv.org/pdf/2304.03271'],Table 1,The table states water consumption for GPT-3 in a US-Arizona data center is 0.03 L/inference. A 500ml bottle is 0.5 L. Calculation: 0.5 L / 0.03 L/inference= 16.7 requests.,Public,0,1,0,1,0
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"...one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)...",Quote,Public,1,0,0,0,0
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,score,['shen2024'],['https://arxiv.org/pdf/2404.07413'],Table 3: OpenLLM leaderboard and code benchmarks results from four different models.,Table 3,Public,0,1,0,0,0
