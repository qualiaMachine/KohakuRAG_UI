"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context provides information about the increase in U.S. data center electricity consumption between 2010 and 2014. According to the text, the total energy consumption of the US data centers increased by about 4% from 2010-2014.","4","percent","[""[ref_id=wu2021b]""]","is_blank","is_blank","The context provides information about the increase in U.S. data center electricity consumption between 2010 and 2014. According to the text, the total energy consumption of the US data centers increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context mentions that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This information directly supports the answer to the question.","13900","cars","[""amazon2023""]","is_blank","is_blank","The context mentions that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This information directly supports the answer to the question."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This information directly answers the question about how many data centers AWS began using recycled water for cooling in 2023.","4","data centers","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This information directly answers the question about how many data centers AWS began using recycled water for cooling in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides an estimate of embodied carbon emissions per GPU from the ref_id=morrison2025 snippet, which assumes the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The context provides an estimate of embodied carbon emissions per GPU from the ref_id=morrison2025 snippet, which assumes the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. The factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B is calculated by dividing the cost of GPT-4 by the budget of FLM-101B.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. The factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B is calculated by dividing the cost of GPT-4 by the budget of FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context provides information about the performance of FLM-101B on the Open LLM Leaderboard, which includes four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. The average score achieved by FLM-101B is mentioned in the context.","43.94","score","[""[ref_id=li2025a]""]","is_blank","is_blank","The context provides information about the performance of FLM-101B on the Open LLM Leaderboard, which includes four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. The average score achieved by FLM-101B is mentioned in the context."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that there is a more than 6,750 fold improvement in processor clock speed between the Intel 4004 in 1971 and typical microprocessors in 2021.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context states that there is a more than 6,750 fold improvement in processor clock speed between the Intel 4004 in 1971 and typical microprocessors in 2021."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides information about the training time of GPT-3, which is estimated to be over 3.5 million hours. With 10,000 GPUs, this translates to approximately 14.8 days. However, the question asks about training GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec, but the context does not provide a direct answer to this specific question. Therefore, the answer is 'is_blank'.","is_blank","days","[""is_blank""]","is_blank","is_blank","The context provides information about the training time of GPT-3, which is estimated to be over 3.5 million hours. With 10,000 GPUs, this translates to approximately 14.8 days. However, the question asks about training GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec, but the context does not provide a direct answer to this specific question. Therefore, the answer is 'is_blank'."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 3) that lists the GPU Power Usage in kWh for various models, including Llama 3.2 1B, at different request frequencies. The table shows that at a request frequency of 8 req/s, the GPU Power Usage for Llama 3.2 1B is 12.0 kWh.","12.0","kWh","[""morrison2025""]","is_blank","is_blank","The context provides a table (Table 3) that lists the GPU Power Usage in kWh for various models, including Llama 3.2 1B, at different request frequencies. The table shows that at a request frequency of 8 req/s, the GPU Power Usage for Llama 3.2 1B is 12.0 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides information about the total permitted annual emission limits for diesel generators in Northern Virginia, which includes the amount of NOx emissions. According to the text, the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","is_blank","The context provides information about the total permitted annual emission limits for diesel generators in Northern Virginia, which includes the amount of NOx emissions. According to the text, the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. This indicates that the growth strategy used for the 101B model saved 72% of the time it would have taken to train the model from scratch.","72","percent","[""[ref_id=li2025a]""]","is_blank","is_blank","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. This indicates that the growth strategy used for the 101B model saved 72% of the time it would have taken to train the model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context does not provide a direct answer to the question about premature deaths in 2030 due to scope-2 pollutants from U.S. data centers alone.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context does not provide a direct answer to the question about premature deaths in 2030 due to scope-2 pollutants from U.S. data centers alone."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context from [ref_id=dodge2022] mentions that the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion and estimates a full training run would take 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","The context from [ref_id=dodge2022] mentions that the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion and estimates a full training run would take 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","Unable to answer with confidence based on the provided documents.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, providing a specific time frame for the launch.","2014","year","[""stone2022""]","is_blank","is_blank","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, providing a specific time frame for the launch."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been formally collected and recycled. This information is provided in the context snippets from documents with ref_id luccioni2025a.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been formally collected and recycled. This information is provided in the context snippets from documents with ref_id luccioni2025a."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information about the energy consumption of various AI models, including the BLOOM model. According to the context, the training energy cost for the BLOOMz-7B model is 51,686 kWh. To convert this to MWh, we can divide by 1000, which gives us 51.686 MWh.","51.686","MWh","[""[ref_id=luccioni2024]""]","is_blank","is_blank","The context provides information about the energy consumption of various AI models, including the BLOOM model. According to the context, the training energy cost for the BLOOMz-7B model is 51,686 kWh. To convert this to MWh, we can divide by 1000, which gives us 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","","","percent","[]","is_blank","is_blank",""
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context provides information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. According to Table 1 in the context, the number of experts (Nexperts) in each MoE layer is 8.","8","experts","[""shen2024""]","is_blank","is_blank","The context provides information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. According to Table 1 in the context, the number of experts (Nexperts) in each MoE layer is 8."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","is_blank","is_blank","second","[""is_blank""]","is_blank","is_blank","is_blank"
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides a clear statement of the total cost of FLM-101B in zettaFLOPs, with a breakdown of the cost for English and Chinese. The cost for the English portion of the FLM-101B model is explicitly stated as 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context provides a clear statement of the total cost of FLM-101B in zettaFLOPs, with a breakdown of the cost for English and Chinese. The cost for the English portion of the FLM-101B model is explicitly stated as 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context provides information about the experimental setup for energy-efficient local inference in financial sentiment classification. According to the context, the hardware processor used was an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","11th Gen Intel(R) Core(TM) i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","is_blank","The context provides information about the experimental setup for energy-efficient local inference in financial sentiment classification. According to the context, the hardware processor used was an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide information on the number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs. Therefore, the answer cannot be determined with confidence based on the provided documents.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs. Therefore, the answer cannot be determined with confidence based on the provided documents."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context supports the answer by stating that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This information is found in the text snippet from [ref_id=wu2021a].","3","multiplier","[""wu2021a""]","is_blank","is_blank","The context supports the answer by stating that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This information is found in the text snippet from [ref_id=wu2021a]."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context provides information about the total compute for model development being 1.2x to 4x larger than the final training run alone, as estimated by Cottier et al. Based on this, we can determine that the total compute for model development is indeed larger than the final training run alone.","is_blank","multiplier","[""cottier2024""]","is_blank","is_blank","The context provides information about the total compute for model development being 1.2x to 4x larger than the final training run alone, as estimated by Cottier et al. Based on this, we can determine that the total compute for model development is indeed larger than the final training run alone."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides an estimate of the total energy consumption to train a 6.1 billion parameter transformer model to completion, which is approximately 103.5 MWh. This value is calculated by multiplying the energy consumed during 8 days of training (13.8 MWh) by the ratio of the total training time to the time already trained (60/8).","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The context provides an estimate of the total energy consumption to train a 6.1 billion parameter transformer model to completion, which is approximately 103.5 MWh. This value is calculated by multiplying the energy consumed during 8 days of training (13.8 MWh) by the ratio of the total training time to the time already trained (60/8)."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context clearly supports the answer through the mention of Jevons' Paradox, which is an economic principle that describes how efficiency gains can lead to increased consumption. This principle is directly related to the question of whether technical efficiency gains in AI may not lead to net environmental benefits.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The context clearly supports the answer through the mention of Jevons' Paradox, which is an economic principle that describes how efficiency gains can lead to increased consumption. This principle is directly related to the question of whether technical efficiency gains in AI may not lead to net environmental benefits."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a clear estimate of the global AI demand's water withdrawal in 2027, which is stated to be between 4.2 and 6.6 billion cubic meters. This information is consistently mentioned across multiple snippets, reinforcing the accuracy of the estimate.","4.2-6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context provides a clear estimate of the global AI demand's water withdrawal in 2027, which is stated to be between 4.2 and 6.6 billion cubic meters. This information is consistently mentioned across multiple snippets, reinforcing the accuracy of the estimate."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context clearly states that Red AI is on the rise despite the well-known diminishing returns of increased cost, which directly contradicts the statement that Red AI is on the decline. This indicates that the statement in question is false.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context clearly states that Red AI is on the rise despite the well-known diminishing returns of increased cost, which directly contradicts the statement that Red AI is on the decline. This indicates that the statement in question is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context supports the answer by providing the total time cost for training FLM-101B, which is 21.54 days, as stated in the text 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days'.","21.54","days","[""[ref_id=li2025a]""]","is_blank","is_blank","The context supports the answer by providing the total time cost for training FLM-101B, which is 21.54 days, as stated in the text 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days'."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context does not clearly support an answer to the question about the utilization of GPUs at Facebook.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context does not clearly support an answer to the question about the utilization of GPUs at Facebook."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context provides an estimated training energy of 1287 MWh for GPT-3, which directly answers the question about the amount of electricity consumed during its training.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context provides an estimated training energy of 1287 MWh for GPT-3, which directly answers the question about the amount of electricity consumed during its training."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score project, which aims to establish a unified approach for comparing the inference efficiency of AI models. This project is mentioned in multiple references, including luccioni2025c.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the AI Energy Score project, which aims to establish a unified approach for comparing the inference efficiency of AI models. This project is mentioned in multiple references, including luccioni2025c."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not clearly support an answer to the question.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context provides information about the JetMoE-8B model, specifically the hyperparameters used in its training. Table 1 in the context lists the hyperparameters, including the number of experts and the top-k value. The top-k value represents the number of experts selected for activation for a given token.","2","experts","[""shen2024""]","is_blank","is_blank","The context provides information about the JetMoE-8B model, specifically the hyperparameters used in its training. Table 1 in the context lists the hyperparameters, including the number of experts and the top-k value. The top-k value represents the number of experts selected for activation for a given token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context supports the answer as it mentions the increase in compute used to train deep learning models over a six-year span. According to the text, the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","0","is_blank","[""[ref_id=schwartz2019]""]","is_blank","is_blank","The context supports the answer as it mentions the increase in compute used to train deep learning models over a six-year span. According to the text, the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from [ref_id=wu2021b] mentions that the global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from [ref_id=wu2021b] mentions that the global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, Amazon's energy supply from utilities, combined with the renewable energy it procures globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. This information directly answers the question about the number of AWS data center regions where 100% of the electricity consumed was matched with renewable energy sources in 2023.","22","data centers","[""[ref_id=amazon2023]""]","is_blank","is_blank","The context states that in 2023, Amazon's energy supply from utilities, combined with the renewable energy it procures globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. This information directly answers the question about the number of AWS data center regions where 100% of the electricity consumed was matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. To find the approximate age of the field of AI in 2025, we need to subtract the birth year from 2025. Therefore, the age of AI in 2025 would be approximately 69 years. The context provides evidence of the birth year of AI, which is used to calculate the age.","69","years","[""stone2022""]","is_blank","is_blank","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. To find the approximate age of the field of AI in 2025, we need to subtract the birth year from 2025. Therefore, the age of AI in 2025 would be approximately 69 years. The context provides evidence of the birth year of AI, which is used to calculate the age."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context supports the answer by mentioning that the 'five cars' estimate is based on a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The study used an approach called neural architecture search (NAS) to estimate the energy required to automate the process of model development. The NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context supports the answer by mentioning that the 'five cars' estimate is based on a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The study used an approach called neural architecture search (NAS) to estimate the energy required to automate the process of model development. The NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context supports the answer by stating that targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency reduces energy consumption per generation by 44% for the Llama 3.1 8B model.","44","percent","[""[ref_id=chung2025]""]","is_blank","is_blank","The context supports the answer by stating that targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency reduces energy consumption per generation by 44% for the Llama 3.1 8B model."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides information about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory.","8","samples","[""[ref_id=xia2024]""]","is_blank","is_blank","The context provides information about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context mentions that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022, which directly answers the question about the energy storage capacity held by Amazon as of 2023.","1.3","GW","[""[ref_id=amazon2023]""]","is_blank","is_blank","The context mentions that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022, which directly answers the question about the energy storage capacity held by Amazon as of 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context provides information about the estimated annual carbon emissions of GPT-4o inference, which are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The context provides information about the estimated annual carbon emissions of GPT-4o inference, which are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide information about the percentage of AI inference workloads in Asia powered by coal in 2023. The provided context discusses the energy consumption and environmental impact of AI systems, but it does not specifically address the question.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about the percentage of AI inference workloads in Asia powered by coal in 2023. The provided context discusses the energy consumption and environmental impact of AI systems, but it does not specifically address the question."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides information about the average power usage effectiveness (PUE) of data centers worldwide in 2023. According to the text, the average data center PUE in 2023 was 1.58 globally. However, it does not specifically mention the PUE of AI-dedicated data centers. Therefore, the answer is based on the general average PUE of data centers.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context provides information about the average power usage effectiveness (PUE) of data centers worldwide in 2023. According to the text, the average data center PUE in 2023 was 1.58 globally. However, it does not specifically mention the PUE of AI-dedicated data centers. Therefore, the answer is based on the general average PUE of data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context clearly states that JetMoE-8B has 8B parameters while only activating 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B has 8B parameters while only activating 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table with the GHG emissions associated with pre-training various LLMs, including the Llama 7B model. According to the table, the Llama 7B model has GHG emissions of 14 tCO2e. This information is found in the snippet with ref_id 'luccioni2025c'.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides a table with the GHG emissions associated with pre-training various LLMs, including the Llama 7B model. According to the table, the Llama 7B model has GHG emissions of 14 tCO2e. This information is found in the snippet with ref_id 'luccioni2025c'."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in the U.S. fleet in 2022 and 2023. To find the total number of vans added, we subtract the number of vans in 2022 from the number in 2023. The calculation is 11,800 - 2,600 = 9,200. Additionally, the context mentions the number of electric delivery vans in Europe and India, but it does not provide enough information to calculate the total number added across all regions.","9200","electric delivery vans","[""[ref_id=amazon2023]""]","is_blank","is_blank","The context provides the number of electric delivery vans in the U.S. fleet in 2022 and 2023. To find the total number of vans added, we subtract the number of vans in 2022 from the number in 2023. The calculation is 11,800 - 2,600 = 9,200. Additionally, the context mentions the number of electric delivery vans in Europe and India, but it does not provide enough information to calculate the total number added across all regions."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the statement in question, which claims that operational environmental impacts do not include GHG emissions from servers and data centers using cooling.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the statement in question, which claims that operational environmental impacts do not include GHG emissions from servers and data centers using cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides a table (Table 4) that lists the energy consumption of various models, including the o3 model, for different prompt sizes. The energy consumption for the o3 model for a long prompt (10k input-1.5k output) is given as 12.222 Wh. This value is used to determine the answer.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The context provides a table (Table 4) that lists the energy consumption of various models, including the o3 model, for different prompt sizes. The energy consumption for the o3 model for a long prompt (10k input-1.5k output) is given as 12.222 Wh. This value is used to determine the answer."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","is_blank","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The provided text discusses AI's water footprint, data center water consumption, and the importance of addressing water efficiency, but it does not specifically mention Google's AI-dedicated data centers or their WUE in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The provided text discusses AI's water footprint, data center water consumption, and the importance of addressing water efficiency, but it does not specifically mention Google's AI-dedicated data centers or their WUE in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, which directly supports the statement in question.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, which directly supports the statement in question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for LLaMA 65B, it takes about 3-4 Joules for an output token at a maximum generation length of 512. This information is provided in the text and is used to determine the energy per token.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","The context states that for LLaMA 65B, it takes about 3-4 Joules for an output token at a maximum generation length of 512. This information is provided in the text and is used to determine the energy per token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context supports the answer by providing information about the reduction in model size after quantization. According to the text, 'By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'","15","percent","[""[ref_id=wu2021a]""]","is_blank","is_blank","The context supports the answer by providing information about the reduction in model size after quantization. According to the text, 'By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'"
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context does not provide clear, publicly available calculations and sound scientific grounding to support the claim that AI can reduce global GHG emissions by 5-10%. In fact, it states that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed. Therefore, the answer is 0, indicating that the statement is false.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context does not provide clear, publicly available calculations and sound scientific grounding to support the claim that AI can reduce global GHG emissions by 5-10%. In fact, it states that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed. Therefore, the answer is 0, indicating that the statement is false."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context does not provide a direct comparison between sparsely activated DNNs and large, dense DNNs in terms of energy consumption and accuracy.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a direct comparison between sparsely activated DNNs and large, dense DNNs in terms of energy consumption and accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","","","USD","[]","is_blank","is_blank",""
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context supports the answer by providing information about the execution time breakdown of the BlackMamba fine-tuning process.","53","percent","[""xia2024""]","is_blank","is_blank","The context supports the answer by providing information about the execution time breakdown of the BlackMamba fine-tuning process."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The daily energy consumption is estimated by multiplying the average energy consumption per query by the number of queries per day, using the given average energy for a comparable multi-purpose model.","83","MWh","[""luccioni2024""]","is_blank","is_blank","The daily energy consumption is estimated by multiplying the average energy consumption per query by the number of queries per day, using the given average energy for a comparable multi-purpose model."
"q067","What was the average global data center PUE in 2023?","The average global data center PUE in 2023 was provided in the context as 1.58 globally. This information is directly stated in the text and provides a clear answer to the question.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The average global data center PUE in 2023 was provided in the context as 1.58 globally. This information is directly stated in the text and provides a clear answer to the question."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that R&D staff costs make up between 29% and 49% of total amortized model development costs for selected frontier models, including Gemini Ultra. Specifically, it is mentioned that Gemini Ultra has the highest fraction of R&D staff cost at 49%. This information directly supports the answer to the question about the percentage of the cost of developing Gemini Ultra attributed to R&D staff, including equity.","49","percent","[""cottier2024""]","is_blank","is_blank","The context states that R&D staff costs make up between 29% and 49% of total amortized model development costs for selected frontier models, including Gemini Ultra. Specifically, it is mentioned that Gemini Ultra has the highest fraction of R&D staff cost at 49%. This information directly supports the answer to the question about the percentage of the cost of developing Gemini Ultra attributed to R&D staff, including equity."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members, as mentioned in multiple references.","17","people","[""stone2022""]","is_blank","is_blank","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members, as mentioned in multiple references."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context provides information about the carbon footprint of client devices, stating that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","is_blank","is_blank","The context provides information about the carbon footprint of client devices, stating that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not support the statement that a model with more parameters will always consume more energy during inference. In fact, it highlights cases where models with more parameters can consume less energy due to differences in architecture and computation.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context does not support the statement that a model with more parameters will always consume more energy during inference. In fact, it highlights cases where models with more parameters can consume less energy due to differences in architecture and computation."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context clearly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, which directly answers the question.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context clearly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, which directly answers the question."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide information about the carbon emissions of OpenAI's API requests in January 2024. The provided context discusses the environmental impact of training large language models, but it does not include specific data about OpenAI's API requests.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information about the carbon emissions of OpenAI's API requests in January 2024. The provided context discusses the environmental impact of training large language models, but it does not include specific data about OpenAI's API requests."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides information about the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models, which is 11,390 tons CO2e, and compares it to the 'five cars' estimate. The 'five cars' estimate is equivalent to 284 metric tons CO2e.","11390","tCO2e","[""[ref_id=luccioni2025c]""]","is_blank","is_blank","The context provides information about the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models, which is 11,390 tons CO2e, and compares it to the 'five cars' estimate. The 'five cars' estimate is equivalent to 284 metric tons CO2e."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context provides information about the growth of AI training infrastructure capacity at Facebook over a 1.5-year period, with Figure 2(d) illustrating a 2.9× increase.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides information about the growth of AI training infrastructure capacity at Facebook over a 1.5-year period, with Figure 2(d) illustrating a 2.9× increase."
"q079","How many miles is the Earth from the Sun?","The context does not provide information about the distance from the Earth to the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide information about the distance from the Earth to the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match, which supports the answer to the question.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context clearly states that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match, which supports the answer to the question."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions 'Continuous Batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, but it does not explicitly state this as the name of the batching strategy that achieves the described effect. However, based on the information provided, Continuous Batching is the strategy that improves GPU utilization and reduces idle time by replacing completed requests with new ones.","Continuous Batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The context mentions 'Continuous Batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, but it does not explicitly state this as the name of the batching strategy that achieves the described effect. However, based on the information provided, Continuous Batching is the strategy that improves GPU utilization and reduces idle time by replacing completed requests with new ones."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context supports the answer by providing information about the entire alignment process, which includes both dSFT and dDPO fine-tuning, and its required H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context supports the answer by providing information about the entire alignment process, which includes both dSFT and dDPO fine-tuning, and its required H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context provides the results of the experiment in Table VI, which shows the instance selection and total price for each policy.","26.7","percent","[""[ref_id=kim2025]""]","is_blank","is_blank","The context provides the results of the experiment in Table VI, which shows the instance selection and total price for each policy."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context provides information about the carbon intensity of various AI models, including the stable-diffusion-xl-base-1.0 model. According to the text, this model generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides information about the carbon intensity of various AI models, including the stable-diffusion-xl-base-1.0 model. According to the text, this model generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides information about the energy consumption of various models, including the range of GPU energy usage for 1,000 inference queries. According to the snippet from [ref_id=luccioni2025c], the GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on the model size, architecture, and task complexity.","0.06-3426","Wh","[""[ref_id=luccioni2025c]""]","is_blank","is_blank","The context provides information about the energy consumption of various models, including the range of GPU energy usage for 1,000 inference queries. According to the snippet from [ref_id=luccioni2025c], the GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on the model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context does not clearly support an answer to the question of whether researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. In fact, the text suggests that there is no one-size-fits-all solution for either ethics or sustainability, implying that a universal approach may not be possible.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question of whether researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. In fact, the text suggests that there is no one-size-fits-all solution for either ethics or sustainability, implying that a universal approach may not be possible."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context does not provide information about the gross carbon intensity of energy according to the U.S. average mix in 2021.","is_blank","kg of CO2e/KWh","[""is_blank""]","is_blank","is_blank","The context does not provide information about the gross carbon intensity of energy according to the U.S. average mix in 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind, a PyTorch-based framework, as a decentralized framework that enables collaborative DL training and can handle peers that drop out at any stage of the training, making it suitable for distributed spot training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions Hivemind, a PyTorch-based framework, as a decentralized framework that enables collaborative DL training and can handle peers that drop out at any stage of the training, making it suitable for distributed spot training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context supports the answer by mentioning the concept of 'social transparency' which involves integrating socio-technical aspects in the description and understanding of AI systems, including their societal impacts, ethical considerations, and environmental footprint.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context supports the answer by mentioning the concept of 'social transparency' which involves integrating socio-technical aspects in the description and understanding of AI systems, including their societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings in classification experiments on German public administration texts.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings in classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the authors develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This suggests that Lamina is the name of the LLM inference system developed in the paper.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context mentions that the authors develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This suggests that Lamina is the name of the LLM inference system developed in the paper."
"q093","How many parameters does the largest T5 model have?","The context mentions the size of various models, including the largest T5 model, which has 13 billion parameters. This information is found in the snippet from morrison2025.","13 billion","parameters","[""morrison2025""]","is_blank","is_blank","The context mentions the size of various models, including the largest T5 model, which has 13 billion parameters. This information is found in the snippet from morrison2025."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context provides information about the JetMoE-8B model, including its hyperparameters and architecture. Specifically, it states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This information is found in multiple references, including [ref_id=shen2024].","8B","parameters","[""[ref_id=shen2024]""]","is_blank","is_blank","The context provides information about the JetMoE-8B model, including its hyperparameters and architecture. Specifically, it states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This information is found in multiple references, including [ref_id=shen2024]."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context supports the answer by providing information about the increase in Google's data center water consumption from 2021 to 2022. According to the text, Google observed a 20% uptick in water consumption during this period.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context supports the answer by providing information about the increase in Google's data center water consumption from 2021 to 2022. According to the text, Google observed a 20% uptick in water consumption during this period."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context provides a definition of the metric in the framework for evaluating energy-efficient large language models. According to the context, the metric 'Carbon Intensity' is defined as 'CO2 emissions per unit of electricity consumed'.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The context provides a definition of the metric in the framework for evaluating energy-efficient large language models. According to the context, the metric 'Carbon Intensity' is defined as 'CO2 emissions per unit of electricity consumed'."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides a table (Table 2) that lists the parallelism configurations and training throughput in each stage of FLM-101B training. The table includes the FLOPs utilization percentage for each stage. The final growth stage is the 101B stage, which has a FLOPs utilization percentage of 52.88%.","52.88","percent","[""[ref_id=li2025a]""]","is_blank","is_blank","The context provides a table (Table 2) that lists the parallelism configurations and training throughput in each stage of FLM-101B training. The table includes the FLOPs utilization percentage for each stage. The final growth stage is the 101B stage, which has a FLOPs utilization percentage of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context provides information about the estimated costs of training OpenAI's GPT-4 model. According to ref_id=cottier2024, the amortized hardware CapEx + energy cost for GPT-4 is $40M. This suggests that the estimated amortized training costs for OpenAI's GPT-4 are $40M.","40","USD","[""cottier2024""]","is_blank","is_blank","The context provides information about the estimated costs of training OpenAI's GPT-4 model. According to ref_id=cottier2024, the amortized hardware CapEx + energy cost for GPT-4 is $40M. This suggests that the estimated amortized training costs for OpenAI's GPT-4 are $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context supports the answer by providing information about the reduction in operational carbon footprint of a Transformer-based universal translation model through full-stack optimization. According to the text, optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint by 810×.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context supports the answer by providing information about the reduction in operational carbon footprint of a Transformer-based universal translation model through full-stack optimization. According to the text, optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint by 810×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information about the performance drop of NLP tasks when training is spread across four continents instead of remaining local. According to the text, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","0.59","multiplier","[""[ref_id=erben2023]""]","is_blank","is_blank","The context provides information about the performance drop of NLP tasks when training is spread across four continents instead of remaining local. According to the text, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8)."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information is provided in multiple sections of the document, including 'Delivering Water Replenishment' and 'Water Use and Replenishment across Amazon'.","3.5 billion","liters","[""amazon2023""]","is_blank","is_blank","The context clearly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information is provided in multiple sections of the document, including 'Delivering Water Replenishment' and 'Water Use and Replenishment across Amazon'."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context provides evidence that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs. This is stated in the research question RQ1 and the findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. The results show that custom tags can contribute to reducing the energy consumption of the video card.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context provides evidence that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs. This is stated in the research question RQ1 and the findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. The results show that custom tags can contribute to reducing the energy consumption of the video card."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023, due to increased demand despite improvements in efficiency.","3.7 million","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023, due to increased demand despite improvements in efficiency."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides information about the cost breakdown of AI model development, including the percentage of costs attributed to AI accelerator chips. According to the text, on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","The context provides information about the cost breakdown of AI model development, including the percentage of costs attributed to AI accelerator chips. According to the text, on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in the context as being around 1.10, which indicates the ratio of the total energy needed by a data center to the energy used solely by computational devices.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in the context as being around 1.10, which indicates the ratio of the total energy needed by a data center to the energy used solely by computational devices."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym 'ETAIROS' is explicitly mentioned in the context.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions the Finnish ETAIROS project, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym 'ETAIROS' is explicitly mentioned in the context."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context does not provide a clear estimate of the amortized training costs for Google's Gemini Ultra. However, it mentions that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","30","USD","[""cottier2024""]","is_blank","is_blank","The context does not provide a clear estimate of the amortized training costs for Google's Gemini Ultra. However, it mentions that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, and it can be interpreted that these assessments should include environmental risks, as environmental protection is one of the Act's objectives and is related to fundamental rights.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, and it can be interpreted that these assessments should include environmental risks, as environmental protection is one of the Act's objectives and is related to fundamental rights."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context clearly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is the answer to the question. This information is provided in multiple snippets from the same reference [han2024].","9","µg/m³","[""han2024""]","is_blank","is_blank","The context clearly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is the answer to the question. This information is provided in multiple snippets from the same reference [han2024]."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device, according to the provided context.","115","books","[""[ref_id=luccioni2025a]""]","is_blank","is_blank","A life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device, according to the provided context."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the per-household health burden from air pollutants in disadvantaged communities could be 200x more than that in less-impacted communities, according to a study on the public health impacts of AI.","200","multiplier","[""han2024""]","is_blank","is_blank","The context states that the per-household health burden from air pollutants in disadvantaged communities could be 200x more than that in less-impacted communities, according to a study on the public health impacts of AI."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is provided in Table B1 of the reference zschache2025. The table lists the energy consumed by the model in Wh.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is provided in Table B1 of the reference zschache2025. The table lists the energy consumed by the model in Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the phenomenon where technological progress improves efficiency, but results in increased usage and overall resource consumption, citing Jevons' Paradox as an example. This concept is relevant to AI, as improvements in efficiency can lead to increased adoption and usage, ultimately resulting in higher resource consumption.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The context describes the phenomenon where technological progress improves efficiency, but results in increased usage and overall resource consumption, citing Jevons' Paradox as an example. This concept is relevant to AI, as improvements in efficiency can lead to increased adoption and usage, ultimately resulting in higher resource consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context does not provide a direct comparison of energy consumption between Meena and GPT-3 training runs.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide a direct comparison of energy consumption between Meena and GPT-3 training runs."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides a table (Table 2) that lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for image generation is listed as 2.907 kWh per 1,000 queries.","2.907","kWh","[""[ref_id=luccioni2024]""]","is_blank","is_blank","The context provides a table (Table 2) that lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for image generation is listed as 2.907 kWh per 1,000 queries."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a table that lists the estimated CO2e emissions for various activities, including the average American life in one year. According to the table, the estimated CO2e emissions for an average American life in one year is 36,156 lbs.","36156","lbs","[""[ref_id=strubell2019]""]","is_blank","is_blank","The context provides a table that lists the estimated CO2e emissions for various activities, including the average American life in one year. According to the table, the estimated CO2e emissions for an average American life in one year is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a table that lists the top 10 counties by per-household health cost, with Marion County in West Virginia having the highest cost.","Marion County","is_blank","[""[ref_id=han2024]""]","is_blank","is_blank","The context provides a table that lists the top 10 counties by per-household health cost, with Marion County in West Virginia having the highest cost."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides a table (TABLE III) that compares performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the carbon emissions before optimization are 0.020 kg, and after optimization, they are 0.015 kg.","0.75","multiplier","[""[ref_id=khan2025]""]","is_blank","is_blank","The context provides a table (TABLE III) that compares performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the carbon emissions before optimization are 0.020 kg, and after optimization, they are 0.015 kg."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for fine-tuning the BLOOMz-7B model as 7,571 kWh and the entire training process as 51,686 kWh. To find the combined training and fine-tuning energy costs, we add these two values together.","59257","kWh","[""[ref_id=luccioni2024], [ref_id=ebert2024]""]","is_blank","is_blank","The context provides the energy usage for fine-tuning the BLOOMz-7B model as 7,571 kWh and the entire training process as 51,686 kWh. To find the combined training and fine-tuning energy costs, we add these two values together."
"q125","What is the total number of parameters in the final FLM-101B model?","The context does not provide a clear answer to the question about the total number of parameters in the final FLM-101B model. However, it does mention that FLM-101B has 101B parameters.","101","parameters","[""li2025a""]","is_blank","is_blank","The context does not provide a clear answer to the question about the total number of parameters in the final FLM-101B model. However, it does mention that FLM-101B has 101B parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","We use the energy consumption data from the 6.1B parameter model training and the inference energy cost of the BLOOMz-7B model to estimate the number of inferences needed to match the training energy cost.","1035930000","inferences","[""[ref_id=dodge2022, ref_id=luccioni2024]""]","is_blank","is_blank","We use the energy consumption data from the 6.1B parameter model training and the inference energy cost of the BLOOMz-7B model to estimate the number of inferences needed to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context provides information about the total energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing'. The study states that a total of 754.66 kWh of energy was used.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides information about the total energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing'. The study states that a total of 754.66 kWh of energy was used."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for different BLOOMz models. The table shows that for the BLOOMz-7B model, the cost parity is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for different BLOOMz models. The table shows that for the BLOOMz-7B model, the cost parity is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany and that the statements from the population were categorized, processed, and published as the FKTG-dataset.","FKTG-dataset","is_blank","[""[ref_id=zschache2025]""]","is_blank","is_blank","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany and that the statements from the population were categorized, processed, and published as the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The provided text discusses the environmental impact of AI and GPU manufacturing but does not mention the use of recycled rare earth metals in NVIDIA H100 GPUs produced in 2024.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The provided text discusses the environmental impact of AI and GPU manufacturing but does not mention the use of recycled rare earth metals in NVIDIA H100 GPUs produced in 2024."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","","","passengers","[]","is_blank","is_blank",""
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context provides data from the API platform OpenRouter, which shows that 84% of LLM usage is through models with no disclosure of environmental impact. This information is used to determine the percentage of LLM token usage that occurs through models that do not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The context provides data from the API platform OpenRouter, which shows that 84% of LLM usage is through models with no disclosure of environmental impact. This information is used to determine the percentage of LLM token usage that occurs through models that do not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for different LLaMA models. According to Table II, the 13B model requires 1 A100 GPU with 80GB of memory. Therefore, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1.","1","A100_80GB_GPU","[""[ref_id=samsi2024]""]","is_blank","is_blank","The context provides information about the bare minimum hardware requirements for different LLaMA models. According to Table II, the 13B model requires 1 A100 GPU with 80GB of memory. Therefore, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context does not provide a clear estimate of the CO2 emissions range in metric tons for a complete training run of a 6.1 billion parameter transformer model. However, it does provide information on the energy consumption and carbon emissions for training various models, including a 6 billion parameter transformer model.","is_blank","metric tons","[""is_blank""]","is_blank","is_blank","The context does not provide a clear estimate of the CO2 emissions range in metric tons for a complete training run of a 6.1 billion parameter transformer model. However, it does provide information on the energy consumption and carbon emissions for training various models, including a 6 billion parameter transformer model."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. The provided text discusses the environmental impact of large language models, the importance of sustainable practices, and techniques for reducing energy consumption, but it does not include specific data on emissions avoided in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. The provided text discusses the environmental impact of large language models, the importance of sustainable practices, and techniques for reducing energy consumption, but it does not include specific data on emissions avoided in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context supports the answer by providing a specific scenario where using 2 A100s and 1 A10G results in a cost saving over an A100-only strategy. This information is found in section 4.4 of the document with ref_id griggs2024.","24","percent","[""griggs2024""]","is_blank","is_blank","The context supports the answer by providing a specific scenario where using 2 A100s and 1 A10G results in a cost saving over an A100-only strategy. This information is found in section 4.4 of the document with ref_id griggs2024."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides a table with the price per hour for different GPUs, including the NVIDIA H20, which is estimated to be $4.63/hr.","4.63","USD per hour","[""[chen2024]""]","is_blank","is_blank","The context provides a table with the price per hour for different GPUs, including the NVIDIA H20, which is estimated to be $4.63/hr."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather information manually by writing to authors, which implies that the information is not gathered automatically without needing to contact authors.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that most carbon footprint analyses gather information manually by writing to authors, which implies that the information is not gathered automatically without needing to contact authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for different LLaMA models. According to Table II, the 7B model requires 1 A100 GPU with 80GB of memory. Therefore, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization is 1.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information about the bare minimum hardware requirements for different LLaMA models. According to Table II, the 7B model requires 1 A100 GPU with 80GB of memory. Therefore, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization is 1."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context supports the answer as it mentions that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which directly addresses the question about the reduction in carbon emissions after quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context supports the answer as it mentions that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which directly addresses the question about the reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context clearly states that researchers were able to collect 95 answers after reaching out to over 500 authors for their carbon footprint analysis.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The context clearly states that researchers were able to collect 95 answers after reaching out to over 500 authors for their carbon footprint analysis."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The cost per H100 GPU-hour is calculated by dividing the total training budget by the total number of H100 GPU hours used, which gives $100,000 / 30,000 = $3.33 per hour.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The cost per H100 GPU-hour is calculated by dividing the total training budget by the total number of H100 GPU hours used, which gives $100,000 / 30,000 = $3.33 per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table (Table 2) that lists the public health cost of training a large AI model in selected U.S. data centers. The table includes the location, electricity price, electricity cost, health cost, and percentage of electricity cost for each location. According to the table, the health cost for training a Llama-3.1 scale model in Altoona, Iowa is $2.51 million, which is 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","is_blank","The context provides a table (Table 2) that lists the public health cost of training a large AI model in selected U.S. data centers. The table includes the location, electricity price, electricity cost, health cost, and percentage of electricity cost for each location. According to the table, the health cost for training a Llama-3.1 scale model in Altoona, Iowa is $2.51 million, which is 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context supports the answer by providing information about the training data used for the JetMoE-8B model. Specifically, it states that the model was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The context supports the answer by providing information about the training data used for the JetMoE-8B model. Specifically, it states that the model was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects and total MW capacity for each location. The table lists 36 projects in the United Kingdom with a total capacity of 901 MW.","36","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects and total MW capacity for each location. The table lists 36 projects in the United Kingdom with a total capacity of 901 MW."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides information about Amazon's workforce demographics, including the percentage of men and women in the U.S. workforce. According to the table 'Amazon Representation by the Numbers', the percentage of men in the U.S. workforce is 46.9% in 2023.","46.9","percent","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's workforce demographics, including the percentage of men and women in the U.S. workforce. According to the table 'Amazon Representation by the Numbers', the percentage of men in the U.S. workforce is 46.9% in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context supports the answer by stating that Apple's supply chain accounts for 99% of its total water footprint, as reported by Apple.","99","percent","[""li2025b""]","is_blank","is_blank","The context supports the answer by stating that Apple's supply chain accounts for 99% of its total water footprint, as reported by Apple."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide a clear answer to the question about the total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about the total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the granularity metric as a way to assess the ratio of computation to communication time when scaling distributed training across continents. This metric is used to compare model suitability for distributed spot training and estimate training performance with additional spot VMs.","Granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","The context introduces the granularity metric as a way to assess the ratio of computation to communication time when scaling distributed training across continents. This metric is used to compare model suitability for distributed spot training and estimate training performance with additional spot VMs."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","7.4","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is referred to as 'water withdrawal'. This concept is clearly defined in the provided context snippets, specifically in sections related to water usage and AI's environmental impact.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is referred to as 'water withdrawal'. This concept is clearly defined in the provided context snippets, specifically in sections related to water usage and AI's environmental impact."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee of the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. This information is provided in multiple snippets, including the preface of the report and the description of the study's purpose.","5","years","[""stone2022""]","is_blank","is_blank","The context states that the Standing Committee of the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. This information is provided in multiple snippets, including the preface of the report and the description of the study's purpose."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context clearly states that the average US household has 25 connected devices, as reported by Deloitte in 2021. This information is provided in multiple snippets from the same reference, wu2021b.","25","devices","[""wu2021b""]","is_blank","is_blank","The context clearly states that the average US household has 25 connected devices, as reported by Deloitte in 2021. This information is provided in multiple snippets from the same reference, wu2021b."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides information about the energy consumption of large language models (LLMs) during pre-training, with values ranging from 0.8 MWh to 3,500 MWh. This range is explicitly mentioned in the context as the energy required to pre-train an LLM.","is_blank","MWh","[""luccioni2025c""]","is_blank","is_blank","The context provides information about the energy consumption of large language models (LLMs) during pre-training, with values ranging from 0.8 MWh to 3,500 MWh. This range is explicitly mentioned in the context as the energy required to pre-train an LLM."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement that IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement that IBM's Watson program did NOT beat human contenders in the Jeopardy challenge."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","One study estimates that 10–50 queries on GPT-3 consumes around half a liter of water. This information is found in the provided context snippet from the document with ref_id=luccioni2025a.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","One study estimates that 10–50 queries on GPT-3 consumes around half a liter of water. This information is found in the provided context snippet from the document with ref_id=luccioni2025a."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context provides a table comparing MT-Bench scores of various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to the table, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment.","6.681","score","[""shen2024""]","is_blank","is_blank","The context provides a table comparing MT-Bench scores of various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to the table, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context directly provides the information that GPT-3 consumes a 500ml bottle of water for roughly 10 – 50 medium-length responses.","is_blank","responses","[""li2025b""]","is_blank","is_blank","The context directly provides the information that GPT-3 consumes a 500ml bottle of water for roughly 10 – 50 medium-length responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context clearly states that Mélange reduces deployment costs by up to 77% in conversational settings, which directly answers the question.","77","percent","[""griggs2024""]","is_blank","is_blank","The context clearly states that Mélange reduces deployment costs by up to 77% in conversational settings, which directly answers the question."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This information is provided in multiple references, including [ref_id=samsi2024] and [ref_id=rubei2025].","4","A100_80GB_GPUs","[""[ref_id=samsi2024], [ref_id=rubei2025]""]","is_blank","is_blank","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This information is provided in multiple references, including [ref_id=samsi2024] and [ref_id=rubei2025]."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context provides information about the air pollutants produced by training an AI model of the Llama-3.1 scale, which is equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","The context provides information about the air pollutants produced by training an AI model of the Llama-3.1 scale, which is equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context does not provide NVIDIA's estimate of the percentage of ML workload that is inference processing for 2019.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide NVIDIA's estimate of the percentage of ML workload that is inference processing for 2019."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is mentioned in the context as 178.97 kg of CO2eq. This value is provided in the 'ETHICAL CONSIDERATIONS STATEMENT' section, where the authors discuss the energy consumption and carbon emissions of their study.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is mentioned in the context as 178.97 kg of CO2eq. This value is provided in the 'ETHICAL CONSIDERATIONS STATEMENT' section, where the authors discuss the energy consumption and carbon emissions of their study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context clearly states that estimations using TDP are nearly always an overestimation since it is rare for a GPU to draw its maximum power at every moment in time. This implies that estimating GPU energy consumption based on TDP is not a reliable method.","0","is_blank","[""[ref_id=chung2025]""]","is_blank","is_blank","The context clearly states that estimations using TDP are nearly always an overestimation since it is rare for a GPU to draw its maximum power at every moment in time. This implies that estimating GPU energy consumption based on TDP is not a reliable method."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes, and also provides a comparison of energy consumption between GPT-4o and GPT-4o mini, with GPT-4o consuming around 2.875 Wh while GPT-4o mini consumes slightly higher at 3.098 Wh.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes, and also provides a comparison of energy consumption between GPT-4o and GPT-4o mini, with GPT-4o consuming around 2.875 Wh while GPT-4o mini consumes slightly higher at 3.098 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","is_blank","is_blank","queries/sec","[""is_blank""]","is_blank","is_blank","is_blank"
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context does not clearly support the answer, as it discusses the trend of environmental transparency in AI models but does not provide a clear analysis of the trend after 2022. However, it does mention that the direct release of environmental information peaked in 2022 and then decreased.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context does not clearly support the answer, as it discusses the trend of environmental transparency in AI models but does not provide a clear analysis of the trend after 2022. However, it does mention that the direct release of environmental information peaked in 2022 and then decreased."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the on-demand price for an H100 GPU in the table, which is $7.5164 per hour.","7.5164","USD per hour","[""[ref_id=griggs2024]""]","is_blank","is_blank","The context provides the on-demand price for an H100 GPU in the table, which is $7.5164 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide a clear answer to the question about the amount of water used for cooling during OpenAI's GPT-4 training run.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about the amount of water used for cooling during OpenAI's GPT-4 training run."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","","","USD per hour","[]","is_blank","is_blank",""
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is required.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is required."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The energy consumption per inference for BLOOMz-7B is given as 1.0 × 10−4 kWh. The total energy for 606,096 downloads with 1 million inferences each is calculated as (606,096 * 1,000,000) * 1.0 × 10−4 kWh, then converted to MWh.","60.6096","MWh","[""luccioni2024""]","is_blank","is_blank","The energy consumption per inference for BLOOMz-7B is given as 1.0 × 10−4 kWh. The total energy for 606,096 downloads with 1 million inferences each is calculated as (606,096 * 1,000,000) * 1.0 × 10−4 kWh, then converted to MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is provided in multiple snippets, including the introduction and the section on model pretraining.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is provided in multiple snippets, including the introduction and the section on model pretraining."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context supports the answer by stating that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This information is provided in the abstract and introduction of the paper 'THE RISING COSTS OF TRAINING FRONTIER AI MODELS' by Ben Cottier et al.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context supports the answer by stating that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This information is provided in the abstract and introduction of the paper 'THE RISING COSTS OF TRAINING FRONTIER AI MODELS' by Ben Cottier et al."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context does not provide a direct answer to the question about the total number of floating point operations to train GPT-3. However, one of the references provides an estimate of the compute used for training GPT-3.","4.5e22","FLOPS","[""cottier2024""]","is_blank","is_blank","The context does not provide a direct answer to the question about the total number of floating point operations to train GPT-3. However, one of the references provides an estimate of the compute used for training GPT-3."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This information is provided in multiple references, including samsi2024 and rubei2025.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This information is provided in multiple references, including samsi2024 and rubei2025."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides a direct estimate of the computational work for FLM-101B's training in zettaFLOPs.","28.22","zettaFLOPs","[""[ref_id=li2025a]""]","is_blank","is_blank","The context provides a direct estimate of the computational work for FLM-101B's training in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not clearly support an answer to the question about the top-1 accuracy on ImageNet associated with AlexNet 2012.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question about the top-1 accuracy on ImageNet associated with AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","","","GPUs","[]","is_blank","is_blank",""
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides information about the estimated CO2 emissions from training a Transformer-based model with Neural Architecture Search (NAS). According to the study by Strubell et al., the emissions are comparable to the lifetime carbon emissions of five cars, which is equivalent to 626,155 pounds (284 metric tons) of CO2-equivalent GHG emissions. To determine the equivalent number of average American lifetimes, we can use the information from Table 1 in the context, which states that the lifetime emissions of a car are 126,000 pounds of CO2, and the average American lifetime emissions are 36,156 pounds of CO2 per year.","17","lifetimes","[""strubell2019"", ""luccioni2025c""]","is_blank","is_blank","The context provides information about the estimated CO2 emissions from training a Transformer-based model with Neural Architecture Search (NAS). According to the study by Strubell et al., the emissions are comparable to the lifetime carbon emissions of five cars, which is equivalent to 626,155 pounds (284 metric tons) of CO2-equivalent GHG emissions. To determine the equivalent number of average American lifetimes, we can use the information from Table 1 in the context, which states that the lifetime emissions of a car are 126,000 pounds of CO2, and the average American lifetime emissions are 36,156 pounds of CO2 per year."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context supports the answer by providing information about the training time of FAIR's RoBERTa model. According to the text, FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context supports the answer by providing information about the training time of FAIR's RoBERTa model. According to the text, FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context provides information about Amazon's on-site solar energy systems and their estimated annual avoidance of CO2e emissions. According to the text, these systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources.","47500","metric tons","[""[ref_id=amazon2023]""]","is_blank","is_blank","The context provides information about Amazon's on-site solar energy systems and their estimated annual avoidance of CO2e emissions. According to the text, these systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context does not explicitly mention a specific framework used for deploying large language models across multiple GPUs and nodes.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not explicitly mention a specific framework used for deploying large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides information about the energy consumption of various models when deployed on single and double nodes. Specifically, Table B2 compares the energy consumption of models on single and double nodes, including the Llama 3.1 70B model. The table shows that the energy consumption of the Llama 3.1 70B model increases by a factor of 1.95 when deployed on two nodes instead of one.","1.95","multiplier","[""[ref_id=zschache2025]""]","is_blank","is_blank","The context provides information about the energy consumption of various models when deployed on single and double nodes. Specifically, Table B2 compares the energy consumption of models on single and double nodes, including the Llama 3.1 70B model. The table shows that the energy consumption of the Llama 3.1 70B model increases by a factor of 1.95 when deployed on two nodes instead of one."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide a clear answer to the question of how many gallons of water were consumed per ChatGPT user session in 2023. While there are estimates of water consumption for training and inference of AI models, there is no specific information provided for ChatGPT user sessions in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question of how many gallons of water were consumed per ChatGPT user session in 2023. While there are estimates of water consumption for training and inference of AI models, there is no specific information provided for ChatGPT user sessions in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides information about the energy consumption of GPT-4o and its comparison to the energy consumption of U.S. homes. According to the text, even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","is_blank","The context provides information about the energy consumption of GPT-4o and its comparison to the energy consumption of U.S. homes. According to the text, even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context supports the answer through a statement from a 2025 paper, which reports Microsoft's increase in global water consumption between 2021 and 2022. This information is provided in the snippet with ref_id=luccioni2025a.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The context supports the answer through a statement from a 2025 paper, which reports Microsoft's increase in global water consumption between 2021 and 2022. This information is provided in the snippet with ref_id=luccioni2025a."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context does not support the statement that traditional models achieved accuracy comparable to large language models in Yelp sentiment analysis benchmarks. In fact, it states that traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context does not support the statement that traditional models achieved accuracy comparable to large language models in Yelp sentiment analysis benchmarks. In fact, it states that traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context does not provide the specific PUE value for Google's Iowa datacenter when the Evolved Transformer was run, but it does mention the PUE of hyperscale datacenters like Google's.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide the specific PUE value for Google's Iowa datacenter when the Evolved Transformer was run, but it does mention the PUE of hyperscale datacenters like Google's."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides an estimate of the total number of GPT-4o queries in 2025, which is approximately 772 billion, as stated in the snippet 'This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025'.","772 billion","queries","[""jegham2025""]","is_blank","is_blank","The context provides an estimate of the total number of GPT-4o queries in 2025, which is approximately 772 billion, as stated in the snippet 'This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025'."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides a table (Table 3) that shows the OpenLLM Leaderboard results for various models, including JetMoE-8B. According to this table, JetMoE-8B achieves an average score of 53.0 on the OpenLLM Leaderboard.","53.0","score","[""shen2024""]","is_blank","is_blank","The context provides a table (Table 3) that shows the OpenLLM Leaderboard results for various models, including JetMoE-8B. According to this table, JetMoE-8B achieves an average score of 53.0 on the OpenLLM Leaderboard."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context does not clearly support an answer to the question. However, it does mention that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models, and that the AI Act fails to address indirect greenhouse gas emissions from AI applications. But it does not explicitly state that open-source general-purpose AI models are fully exempt from reporting their energy consumption unless they pose systemic risk.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question. However, it does mention that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models, and that the AI Act fails to address indirect greenhouse gas emissions from AI applications. But it does not explicitly state that open-source general-purpose AI models are fully exempt from reporting their energy consumption unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context does not provide information about the US national datacenter average Power Usage Effectiveness (PUE) in 2020.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide information about the US national datacenter average Power Usage Effectiveness (PUE) in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides information about the KV Cache size growth for the OPT-2.7B model. Specifically, it states that when the batch size increases to 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","is_blank","The context provides information about the KV Cache size growth for the OPT-2.7B model. Specifically, it states that when the batch size increases to 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context clearly states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized cost for the four notable models studied by Cottier et al.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context clearly states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized cost for the four notable models studied by Cottier et al."
"q213","Which software package was used to measure energy consumption during inference runs?","The context supports the answer by mentioning the software package used to measure energy consumption during inference runs, which is CodeCarbon.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","is_blank","The context supports the answer by mentioning the software package used to measure energy consumption during inference runs, which is CodeCarbon."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides information about an analysis of 100 news articles on ChatGPT's energy use. According to the analysis, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context provides information about an analysis of 100 news articles on ChatGPT's energy use. According to the analysis, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context supports the answer by mentioning the Compute Time Calibration Function (CTCF) as a function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The context supports the answer by mentioning the Compute Time Calibration Function (CTCF) as a function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context provides information about the energy costs of LLaMA 65B across different batch sizes and degrees of sharding. Specifically, it states that increasing the number of shards always increases the wattage, and the energy per second increases with the number of shards even at the same batch size. However, the question asks about the energy cost per response, which is not directly addressed in the provided context.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context provides information about the energy costs of LLaMA 65B across different batch sizes and degrees of sharding. Specifically, it states that increasing the number of shards always increases the wattage, and the energy per second increases with the number of shards even at the same batch size. However, the question asks about the energy cost per response, which is not directly addressed in the provided context."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","","","kL","[]","is_blank","is_blank",""
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context does not clearly support an answer to the question about current EU rules. However, it discusses proposed policy recommendations and interpretations that could be integrated into the evaluation report of the AI Act, including the elimination of the open-source exemption from reporting obligations.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question about current EU rules. However, it discusses proposed policy recommendations and interpretations that could be integrated into the evaluation report of the AI Act, including the elimination of the open-source exemption from reporting obligations."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This information is provided in multiple references, including luccioni2025a and wu2021b.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This information is provided in multiple references, including luccioni2025a and wu2021b."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides information about the total public health cost of U.S. data centers in 2023. According to the text, the total public health cost of U.S. data centers is about $6.7 billion in 2023.","6.7 billion","USD","[""han2024""]","is_blank","is_blank","The context provides information about the total public health cost of U.S. data centers in 2023. According to the text, the total public health cost of U.S. data centers is about $6.7 billion in 2023."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides a table (Table 4) that lists the energy consumption of various models, including o3 and GPT-4.1 nano, for different prompt sizes. By comparing the energy consumption values for o3 and GPT-4.1 nano for long prompts, we can determine the factor by which o3's energy consumption is greater.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","The context provides a table (Table 4) that lists the energy consumption of various models, including o3 and GPT-4.1 nano, for different prompt sizes. By comparing the energy consumption values for o3 and GPT-4.1 nano for long prompts, we can determine the factor by which o3's energy consumption is greater."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context supports the answer by providing the cost reduction range achieved by Mélange compared to single-GPU baselines in the evaluation of short-context workloads with a 120ms SLO. Specifically, it states that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO.","15-77","percent","[""griggs2024""]","is_blank","is_blank","The context supports the answer by providing the cost reduction range achieved by Mélange compared to single-GPU baselines in the evaluation of short-context workloads with a 120ms SLO. Specifically, it states that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and well-known LLMs. According to this table, the net tCO2e (carbon footprint) of FLM-101B is 26.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and well-known LLMs. According to this table, the net tCO2e (carbon footprint) of FLM-101B is 26."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","","","seconds","[]","is_blank","is_blank",""
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain communities, which directly supports the answer to the question.","0","is_blank","[""han2024""]","is_blank","is_blank","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain communities, which directly supports the answer to the question."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context supports the answer as it mentions that GPU theoretical performance per watt doubles every 3-4 years, which is stated in the snippet from [ref_id=wu2021b].","1","is_blank","[""[ref_id=wu2021b]""]","is_blank","is_blank","The context supports the answer as it mentions that GPU theoretical performance per watt doubles every 3-4 years, which is stated in the snippet from [ref_id=wu2021b]."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context supports the answer by mentioning Ollama, an open-source platform, as the tool used for local AI model deployment and quantization in the financial sentiment case study. Specifically, it states that 'We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context supports the answer by mentioning Ollama, an open-source platform, as the tool used for local AI model deployment and quantization in the financial sentiment case study. Specifically, it states that 'We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'"
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the authors used an independent S3 storage provider, Backblaze (B2), to store and access their datasets. This is relevant to the question about the storage service used to shard and stream datasets for spot VMs.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that the authors used an independent S3 storage provider, Backblaze (B2), to store and access their datasets. This is relevant to the question about the storage service used to shard and stream datasets for spot VMs."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context does not clearly support an answer to the question of whether the relationship between runtime and energy consumption is nearly linear in experiments measuring inference with large language models. While some references mention a strong correlation between inference energy consumption and model runtime, they do not explicitly state that the relationship is nearly linear.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question of whether the relationship between runtime and energy consumption is nearly linear in experiments measuring inference with large language models. While some references mention a strong correlation between inference energy consumption and model runtime, they do not explicitly state that the relationship is nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024.","Senator Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","","","USD per hour","[]","is_blank","is_blank",""
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide a clear estimate of the average GPU lifetime before retirement in AI data centers in 2024, but mentions an improvement in server lifetime.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context does not provide a clear estimate of the average GPU lifetime before retirement in AI data centers in 2024, but mentions an improvement in server lifetime."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for different LLaMA models, including LLaMA 13B. According to Table II, the bare minimum hardware required for LLaMA 13B is 2 V100 32GB GPUs, with a maximum batch size of 64.","2","V100_32GB_GPUs","[""[ref_id=samsi2024]""]","is_blank","is_blank","The context provides information about the bare minimum hardware requirements for different LLaMA models, including LLaMA 13B. According to Table II, the bare minimum hardware required for LLaMA 13B is 2 V100 32GB GPUs, with a maximum batch size of 64."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides information about the estimated GHG emissions from the pre-training process for Google's Gemma family of language models, which is reported to be 1247.61 tons CO2e. This is compared to the 'five cars' estimate, which is exceeded by the Gemma family emissions. The context does not provide a direct comparison of water consumption or other environmental impacts.","1247.61","tCO2e","[""[ref_id=luccioni2025c]""]","is_blank","is_blank","The context provides information about the estimated GHG emissions from the pre-training process for Google's Gemma family of language models, which is reported to be 1247.61 tons CO2e. This is compared to the 'five cars' estimate, which is exceeded by the Gemma family emissions. The context does not provide a direct comparison of water consumption or other environmental impacts."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This information is provided in the snippet from [ref_id=strubell2019].","336","hours","[""[ref_id=strubell2019]""]","is_blank","is_blank","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This information is provided in the snippet from [ref_id=strubell2019]."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides an estimate of the U.S. national average water consumption for electricity generation, which is 3.1 L/kWh. This value is mentioned in the text as the estimated national average water consumption.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context provides an estimate of the U.S. national average water consumption for electricity generation, which is 3.1 L/kWh. This value is mentioned in the text as the estimated national average water consumption."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides information about the Power Usage Effectiveness (PUE) of Google's hyperscale data centers in 2021. According to the text, Google's PUE improved from 1.21 in 2008 to 1.10 in 2021. This information is also supported by another reference, which states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","The context provides information about the Power Usage Effectiveness (PUE) of Google's hyperscale data centers in 2021. According to the text, Google's PUE improved from 1.21 in 2008 to 1.10 in 2021. This information is also supported by another reference, which states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This information directly supports the answer to the question about the percent reduction in carbon footprint that customers can expect by moving workloads from on-premises data centers to AWS in North America.","96","percent","[""amazon2023""]","is_blank","is_blank","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This information directly supports the answer to the question about the percent reduction in carbon footprint that customers can expect by moving workloads from on-premises data centers to AWS in North America."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides information about the cost of fine-tuning a sparse Mixtral model using a realistic data size of 2M queries with NVIDIA H100 GPU. According to the text, the cost is $3460.","3460","USD","[""[ref_id=xia2024]""]","is_blank","is_blank","The context provides information about the cost of fine-tuning a sparse Mixtral model using a realistic data size of 2M queries with NVIDIA H100 GPU. According to the text, the cost is $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context provides information about the proportion of electricity consumption attributed to GPUs in data centers. According to the text, 'The GPU alone accounts for 74% of the total energy consumption due to these components.' This suggests that GPUs account for approximately 74% of the total provisioned power in a typical datacenter.","74","percent","[""dodge2022""]","is_blank","is_blank","The context provides information about the proportion of electricity consumption attributed to GPUs in data centers. According to the text, 'The GPU alone accounts for 74% of the total energy consumption due to these components.' This suggests that GPUs account for approximately 74% of the total provisioned power in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context mentions that the training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes and 96 H100s.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context mentions that the training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes and 96 H100s."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides information about the average GPU power consumption during the training of OLMo 2 7B. According to the text, when actively training, the average GPU power is over 600W. This information is found in multiple references, including [ref_id=morrison2025].","600","Watts","[""[ref_id=morrison2025]""]","is_blank","is_blank","The context provides information about the average GPU power consumption during the training of OLMo 2 7B. According to the text, when actively training, the average GPU power is over 600W. This information is found in multiple references, including [ref_id=morrison2025]."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a table with estimated CO2e emissions for different activities, including human life. The table lists 'Human life, avg, 1 year' as 11,023 lbs.","11023","lbs","[""[ref_id=strubell2019]""]","is_blank","is_blank","The context provides a table with estimated CO2e emissions for different activities, including human life. The table lists 'Human life, avg, 1 year' as 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides information about the performance of LLaMA models on different GPUs, including the A100 and V100. Specifically, it mentions that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 1.25 times increase in inference latency for the 13B model. This suggests that the A100 provides a speedup in inference throughput for the LLaMA-13B model compared to the V100.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides information about the performance of LLaMA models on different GPUs, including the A100 and V100. Specifically, it mentions that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 1.25 times increase in inference latency for the 13B model. This suggests that the A100 provides a speedup in inference throughput for the LLaMA-13B model compared to the V100."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides information about the energy consumption of a single short query to GPT-4o, which is estimated to be 0.42 Wh (±0.13 Wh). This value is mentioned in multiple parts of the context, including section 6.1 and figure 5, indicating a consistent estimate across different analyses.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context provides information about the energy consumption of a single short query to GPT-4o, which is estimated to be 0.42 Wh (±0.13 Wh). This value is mentioned in multiple parts of the context, including section 6.1 and figure 5, indicating a consistent estimate across different analyses."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","280","percent","[""[ref_id=kim2025]""]","is_blank","is_blank","The context states that with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context does not provide information about the most energy-efficient GPU architecture for models generating only a single classification token. However, it does mention that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about the most energy-efficient GPU architecture for models generating only a single classification token. However, it does mention that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context supports the answer as it mentions that reporting the computational price tag of finding, training, and running models is a key Green AI practice, which includes providing the financial cost.","1","is_blank","[""[ref_id=schwartz2019]""]","is_blank","is_blank","The context supports the answer as it mentions that reporting the computational price tag of finding, training, and running models is a key Green AI practice, which includes providing the financial cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that the global generation of electronic waste reached 62 million tonnes in 2022, according to the provided text from ref_id=luccioni2025a.","62 million","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context states that the global generation of electronic waste reached 62 million tonnes in 2022, according to the provided text from ref_id=luccioni2025a."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","","","Watts","[]","is_blank","is_blank",""
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context supports the answer by providing information about the water consumption of training the GPT-3 language model in Microsoft's U.S. data centers. According to the text, training GPT-3 can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","is_blank","The context supports the answer by providing information about the water consumption of training the GPT-3 language model in Microsoft's U.S. data centers. According to the text, training GPT-3 can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021, which directly answers the question.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021, which directly answers the question."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides information about a recent eco-efficiency analysis using DEA, where OpenAI's reasoning models dominate the eco-efficiency frontier. Specifically, o3-mini achieved the highest cross-efficiency score, indicating its high eco-efficiency.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides information about a recent eco-efficiency analysis using DEA, where OpenAI's reasoning models dominate the eco-efficiency frontier. Specifically, o3-mini achieved the highest cross-efficiency score, indicating its high eco-efficiency."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context supports the answer as it mentions that smartphones currently have lifetimes of less than 3 years, which contributes to e-waste concerns. This information is found in the snippet from [ref_id=wu2021b].","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context supports the answer as it mentions that smartphones currently have lifetimes of less than 3 years, which contributes to e-waste concerns. This information is found in the snippet from [ref_id=wu2021b]."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models, with a maximum speedup of up to 3.2x at eight GPUs. The context supports this answer by providing experimental results in Figure 7, which shows the intra-zone performance for CV and NLP models. The CV model's per-GPU speedup is almost linear, with values of 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively.","1","is_blank","[""[ref_id=erben2023]""]","is_blank","is_blank","Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models, with a maximum speedup of up to 3.2x at eight GPUs. The context supports this answer by providing experimental results in Figure 7, which shows the intra-zone performance for CV and NLP models. The CV model's per-GPU speedup is almost linear, with values of 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context supports the answer by explicitly stating the context window size of the FLM-101B model in tokens. The relevant information is found in the section detailing the FLM-101B configurations.","2048","tokens","[""[ref_id=li2025a]""]","is_blank","is_blank","The context supports the answer by explicitly stating the context window size of the FLM-101B model in tokens. The relevant information is found in the section detailing the FLM-101B configurations."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, whereas Diffusion models are significantly more compute-intensive. This suggests that LLMs generally have lower power draw during inference than diffusion models. However, the statement about LLM decoding being less compute-intensive and bottlenecked by VRAM bandwidth is supported by the context, which implies that the power draw difference is due to the compute-intensity of the models.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, whereas Diffusion models are significantly more compute-intensive. This suggests that LLMs generally have lower power draw during inference than diffusion models. However, the statement about LLM decoding being less compute-intensive and bottlenecked by VRAM bandwidth is supported by the context, which implies that the power draw difference is due to the compute-intensity of the models."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides information about Amazon's workforce diversity, including the percentage of women in people manager positions. According to the table 'Amazon Representation by the Numbers', in 2023, 31.9% of people managers identified as women.","31.9","percent","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's workforce diversity, including the percentage of women in people manager positions. According to the table 'Amazon Representation by the Numbers', in 2023, 31.9% of people managers identified as women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides information about the cost breakdown of AI model development, specifically the percentage range of total amortized cost attributed to computing hardware for four key models analyzed by Cottier et al. When excluding equity, the fraction of computing hardware costs rises to 61-76%. This information is found in the text: 'However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.'","61-76","percent","[""cottier2024""]","is_blank","is_blank","The context provides information about the cost breakdown of AI model development, specifically the percentage range of total amortized cost attributed to computing hardware for four key models analyzed by Cottier et al. When excluding equity, the fraction of computing hardware costs rises to 61-76%. This information is found in the text: 'However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.'"
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context does not support the statement that accuracy and F1 scores always improved after optimization. In fact, it states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context does not support the statement that accuracy and F1 scores always improved after optimization. In fact, it states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","is_blank","is_blank","lbs/kWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a study that suggests the global AI could consume 85 – 134 TWh of electricity in 2027. This information is used to estimate the potential water usage for global AI in 2027.","85-134","TWh","[""li2025b""]","is_blank","is_blank","The context provides a study that suggests the global AI could consume 85 – 134 TWh of electricity in 2027. This information is used to estimate the potential water usage for global AI in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023. This information is found in the section 'Europe' under the heading 'In 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles'.","150","packages","[""amazon2023""]","is_blank","is_blank","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023. This information is found in the section 'Europe' under the heading 'In 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles'."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The total number of tokens processed can be calculated by multiplying the number of requests by the sum of input and output tokens per request.","1920000","tokens","[""[ref_id=kim2025]""]","is_blank","is_blank","The total number of tokens processed can be calculated by multiplying the number of requests by the sum of input and output tokens per request."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not clearly support an answer to the question. The AI Act's requirements regarding greenhouse gas emissions disclosure are not explicitly stated in the provided context.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question. The AI Act's requirements regarding greenhouse gas emissions disclosure are not explicitly stated in the provided context."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context supports the answer by providing information about the Flexible Start optimization analysis in the 2022 Dodge et al. paper, which found significant emissions reductions for shorter jobs like DenseNet 201. Specifically, it states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.","80","percent","[""dodge2022""]","is_blank","is_blank","The context supports the answer by providing information about the Flexible Start optimization analysis in the 2022 Dodge et al. paper, which found significant emissions reductions for shorter jobs like DenseNet 201. Specifically, it states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context provides a comparison of the energy consumption of different tasks, including text classification and image generation. According to the text, image generation uses, on average, over 1450 times more energy than text classification. This information is presented in the context as a mean consumption of 0.002 KwH per 1,000 inferences for text classification and 2.9kWh for image generation.","1450","times","[""luccioni2024""]","is_blank","is_blank","The context provides a comparison of the energy consumption of different tasks, including text classification and image generation. According to the text, image generation uses, on average, over 1450 times more energy than text classification. This information is presented in the context as a mean consumption of 0.002 KwH per 1,000 inferences for text classification and 2.9kWh for image generation."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context does not support the statement that traditional models achieved accuracy comparable to large language models in Yelp sentiment analysis benchmarks. In fact, it states that traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context does not support the statement that traditional models achieved accuracy comparable to large language models in Yelp sentiment analysis benchmarks. In fact, it states that traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table that lists the number of Amazon Renewable Energy Projects announced as of January 2024, with a specific entry for the United States. This table supports the answer to the question about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024.","244","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table that lists the number of Amazon Renewable Energy Projects announced as of January 2024, with a specific entry for the United States. This table supports the answer to the question about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide information about the percentage of power usage covered by Amazon's AWS with renewable energy in 2018. However, Table 2 from the strubell2019 reference provides the percentage of energy sourced from renewable sources for Amazon-AWS in a different year, which is 17% for renewable energy, 24% for natural gas, 30% for coal, and 26% for nuclear. Since the question specifically asks for the 2018 data, which is not available in the provided context, the answer cannot be determined with confidence.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about the percentage of power usage covered by Amazon's AWS with renewable energy in 2018. However, Table 2 from the strubell2019 reference provides the percentage of energy sourced from renewable sources for Amazon-AWS in a different year, which is 17% for renewable energy, 24% for natural gas, 30% for coal, and 26% for nuclear. Since the question specifically asks for the 2018 data, which is not available in the provided context, the answer cannot be determined with confidence."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context supports the answer through the statement 'Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.' This indicates that the authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The context supports the answer through the statement 'Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.' This indicates that the authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides information about the electricity consumption of different components during the training of a BERT-base model. According to Table 1, the GPU accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","is_blank","is_blank","The context provides information about the electricity consumption of different components during the training of a BERT-base model. According to Table 1, the GPU accounts for 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from [ref_id=griggs2024] states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context from [ref_id=griggs2024] states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context mentions that the iterative optimization process has led to a 28.5% operational energy footprint reduction over a two-year time period at Facebook. This information is provided in multiple sections of the document, including Figure 8 and Section III-B.","28.5","percent","[""wu2021a""]","is_blank","is_blank","The context mentions that the iterative optimization process has led to a 28.5% operational energy footprint reduction over a two-year time period at Facebook. This information is provided in multiple sections of the document, including Figure 8 and Section III-B."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide information about the installation of fiber optic cables to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide information about the installation of fiber optic cables to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides an estimate of the upfront hardware acquisition cost to train GPT-4, which is $800M, as stated in the snippet from [ref_id=cottier2024].","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context provides an estimate of the upfront hardware acquisition cost to train GPT-4, which is $800M, as stated in the snippet from [ref_id=cottier2024]."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context does not clearly support an answer to the question of whether the umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context does not clearly support an answer to the question of whether the umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","is_blank","is_blank","samples","[""is_blank""]","is_blank","is_blank","is_blank"
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context provides information about two preemption mechanisms, Recomputation and Swapping, used in LLM inference servers. According to Figure 8, when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running computation, whereas Recomputation performs extra computation when restoring requests.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The context provides information about two preemption mechanisms, Recomputation and Swapping, used in LLM inference servers. According to Figure 8, when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running computation, whereas Recomputation performs extra computation when restoring requests."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context provides a projection of data center electricity consumption as a percentage of total US energy demand by 2030. According to the text, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030.","9.1-11.7","percent","[""fernandez2025""]","is_blank","is_blank","The context provides a projection of data center electricity consumption as a percentage of total US energy demand by 2030. According to the text, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides information about the 'Pause and Resume' optimization for training of the 6B parameter transformer, and its potential emissions savings. According to the text, for very long runs like the 6 billion parameter language model training run, doubling the duration can lead to significant savings up to about 25%. This suggests that the maximum potential emissions saving for this optimization is around 25%.","25","percent","[""dodge2022""]","is_blank","is_blank","The context provides information about the 'Pause and Resume' optimization for training of the 6B parameter transformer, and its potential emissions savings. According to the text, for very long runs like the 6 billion parameter language model training run, doubling the duration can lead to significant savings up to about 25%. This suggests that the maximum potential emissions saving for this optimization is around 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context clearly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, which directly answers the question.","70","percent","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, which directly answers the question."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context mentions that the carbon footprint of training BERT, a large language model, was quantified in a seminal 2019 study as reaching 626,155 pounds of emissions. This information is provided in the snippet with ref_id=luccioni2025b.","626,155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context mentions that the carbon footprint of training BERT, a large language model, was quantified in a seminal 2019 study as reaching 626,155 pounds of emissions. This information is provided in the snippet with ref_id=luccioni2025b."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides an estimated training energy of 1287 MWh for the full GPT-3 model, as stated in the snippet from ref_id=li2025b. This value is also mentioned in another snippet from ref_id=jegham2025, confirming the estimate.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context provides an estimated training energy of 1287 MWh for the full GPT-3 model, as stated in the snippet from ref_id=li2025b. This value is also mentioned in another snippet from ref_id=jegham2025, confirming the estimate."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context supports the answer as it mentions that the MoE layer is the most time-consuming part of LLM fine-tuning, accounting for 85% of the overall execution time on average, and that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context supports the answer as it mentions that the MoE layer is the most time-consuming part of LLM fine-tuning, accounting for 85% of the overall execution time on average, and that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","is_blank","is_blank","samples","[""is_blank""]","is_blank","is_blank","is_blank"
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context supports the answer as it mentions that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%. This is stated in the sentence: 'In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.'","1","is_blank","[""[ref_id=erben2023]""]","is_blank","is_blank","The context supports the answer as it mentions that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%. This is stated in the sentence: 'In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.'"
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide information about the land area occupied by new AI data centers globally in 2022. The provided context discusses various topics related to AI, data centers, and their environmental impact, but it does not mention land area occupation.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide information about the land area occupied by new AI data centers globally in 2022. The provided context discusses various topics related to AI, data centers, and their environmental impact, but it does not mention land area occupation."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides information about the emissions of various models, including the BERT-based model bert-base-multilingual-uncased-sentiment, which emits 0.32g of CO2eq per 1,000 queries for text classification tasks.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides information about the emissions of various models, including the BERT-based model bert-base-multilingual-uncased-sentiment, which emits 0.32g of CO2eq per 1,000 queries for text classification tasks."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context provides information about the emissions from training a BERT model in different regions. It states that there is a large variation between the least carbon-intensive regions and the most carbon-intensive regions, with emissions ranging from 7k grams to 26k grams for the most efficient vs. least efficient regions.","19","grams","[""dodge2022""]","is_blank","is_blank","The context provides information about the emissions from training a BERT model in different regions. It states that there is a large variation between the least carbon-intensive regions and the most carbon-intensive regions, with emissions ranging from 7k grams to 26k grams for the most efficient vs. least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This information is found in the provided context snippets from the document with ref_id luccioni2025c.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This information is found in the provided context snippets from the document with ref_id luccioni2025c."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table that lists the water consumption equivalent for one person in the US for the OLMo 60M model.","5","days","[""morrison2025""]","is_blank","is_blank","The context provides a table that lists the water consumption equivalent for one person in the US for the OLMo 60M model."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. However, it mentions that one technology company's self-owned data centers alone directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. However, it mentions that one technology company's self-owned data centers alone directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is given in the context as 40 MkWh, which can be converted to kWh by multiplying by 1,000,000 (since 1 MkWh = 1,000,000 kWh). However, the context does not explicitly provide the conversion, but it does mention the energy consumption in MkWh.","40000000","kWh","[""li2025a""]","is_blank","is_blank","The total energy consumption for training the FLM-101B model is given in the context as 40 MkWh, which can be converted to kWh by multiplying by 1,000,000 (since 1 MkWh = 1,000,000 kWh). However, the context does not explicitly provide the conversion, but it does mention the energy consumption in MkWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides a projection for the total public health burden of U.S. data centers in 2030, but the specific value is not mentioned. However, it does mention that the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028. Since the question asks for the value in 2030 and the context does not provide this information, the answer is 'is_blank'.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context provides a projection for the total public health burden of U.S. data centers in 2030, but the specific value is not mentioned. However, it does mention that the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028. Since the question asks for the value in 2030 and the context does not provide this information, the answer is 'is_blank'."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GS dataset with sparse MoE.","32.7","USD","[""[ref_id=xia2024]""]","is_blank","is_blank","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GS dataset with sparse MoE."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","","","samples","[]","is_blank","is_blank",""
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that the authors advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements, which directly answers the question.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the authors advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements, which directly answers the question."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that training accounted for only half of the model's overall emissions, according to Luccioni et al.'s 2023 article estimating the carbon footprint of BLOOM.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context states that training accounted for only half of the model's overall emissions, according to Luccioni et al.'s 2023 article estimating the carbon footprint of BLOOM."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information about the minimum hardware requirements for running LLaMA models. According to Table II, the 7B model requires 1 V100 GPU with 32GB of RAM. This suggests that the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization is 1.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information about the minimum hardware requirements for running LLaMA models. According to Table II, the 7B model requires 1 V100 GPU with 32GB of RAM. This suggests that the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization is 1."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides a table (Table 1) from [ref_id=li2025b] that estimates the operational water consumption footprint of GPT-3 for different locations, including Arizona. This table directly gives the number of requests for 500ml of water in Arizona.","16.7","requests","[""li2025b""]","is_blank","is_blank","The context provides a table (Table 1) from [ref_id=li2025b] that estimates the operational water consumption footprint of GPT-3 for different locations, including Arizona. This table directly gives the number of requests for 500ml of water in Arizona."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides information about the estimated CO2 emission in metric tons for one year of average US home energy use, which is explicitly stated as 8.3 metric tons CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context provides information about the estimated CO2 emission in metric tons for one year of average US home energy use, which is explicitly stated as 8.3 metric tons CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not mention the GSM8k benchmark or the score achieved by the JetMoE-8B model on this benchmark.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not mention the GSM8k benchmark or the score achieved by the JetMoE-8B model on this benchmark."
