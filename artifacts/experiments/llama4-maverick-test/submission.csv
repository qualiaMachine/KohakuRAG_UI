"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","","","percent","[]","is_blank","is_blank",""
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13,900","cars","[""amazon2023""]","is_blank","is_blank","The context states that Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24.","4","data centers","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides estimates of embodied carbon emissions per GPU, with Morrison et al. (2025) assuming 463 kg per GPU based on Luccioni et al. (2023)'s estimate of 3700 kg of CO2eq per 8x server node.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The context provides estimates of embodied carbon emissions per GPU, with Morrison et al. (2025) assuming 463 kg per GPU based on Luccioni et al. (2023)'s estimate of 3700 kg of CO2eq per 8x server node."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. To find the factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B, we need to divide the cost of GPT-4 by the cost of FLM-101B.","400","ratio","[""[\""li2025a\"", \""cottier2024\""]""]","is_blank","is_blank","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. To find the factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B, we need to divide the cost of GPT-4 by the cost of FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides a table from the paper by Emma Strubell et al., which lists the estimated CO2 emissions from various activities, including air travel. It states that a round trip from New York to San Francisco for one passenger emits 1984 lbs of CO2e.","900","tCO2e","[""strubell2019""]","is_blank","is_blank","The context provides a table from the paper by Emma Strubell et al., which lists the estimated CO2 emissions from various activities, including air travel. It states that a round trip from New York to San Francisco for one passenger emits 1984 lbs of CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context provides a detailed evaluation of FLM-101B on the Open LLM Leaderboard, which includes four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. The average score achieved by FLM-101B is explicitly stated as 43.94.","43.94","score","[""li2025a""]","is_blank","is_blank","The context provides a detailed evaluation of FLM-101B on the Open LLM Leaderboard, which includes four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. The average score achieved by FLM-101B is explicitly stated as 43.94."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides a comparison between the Intel 4004 microprocessor from 1971 and typical microprocessors in 2021, stating that there was a more than 6,750 fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context provides a comparison between the Intel 4004 microprocessor from 1971 and typical microprocessors in 2021, stating that there was a more than 6,750 fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides an estimate of the total training time of GPT-3 using 10,000 GPUs. According to the snippet from [ref_id=luccioni2023], the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to about 14.8 days with 10,000 GPUs.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context provides an estimate of the total training time of GPT-3 using 10,000 GPUs. According to the snippet from [ref_id=luccioni2023], the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to about 14.8 days with 10,000 GPUs."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 3) from a conference paper published at ICLR 2025, which includes measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The table lists the GPU Power Usage in kWh for different models and request frequencies.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The context provides a table (Table 3) from a conference paper published at ICLR 2025, which includes measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The table lists the GPU Power Usage in kWh for different models and request frequencies."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides information about the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties. The total permitted annual emission limits for NOx are approximately 13,000 tons.","13000","tons","[""han2024""]","is_blank","is_blank","The context provides information about the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties. The total permitted annual emission limits for NOx are approximately 13,000 tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context clearly states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch.","72","percent","[""li2025a""]","is_blank","is_blank","The context clearly states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides information about the public health impacts of U.S. data centers, specifically mentioning that in 2028, U.S. data centers could contribute to approximately 1,300 premature deaths. The question asks about the number of premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context provides information about the public health impacts of U.S. data centers, specifically mentioning that in 2028, U.S. data centers could contribute to approximately 1,300 premature deaths. The question asks about the number of premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that a full training run of the 6.1 billion parameter model would take 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","Dodge et al. (2022) state that a full training run of the 6.1 billion parameter model would take 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context clearly states that 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.' This directly supports the answer to the question.","2014","year","[""stone2022""]","is_blank","is_blank","The context clearly states that 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.' This directly supports the answer to the question."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information about the energy consumption for pre-training the BLOOM model. According to the table in the context, the training energy for BLOOMz-7B, which is a fine-tuned version of BLOOM-176B, is 51,686 kWh.","51,686","MWh","[""luccioni2024"", ""ebert2024""]","is_blank","is_blank","The context provides information about the energy consumption for pre-training the BLOOM model. According to the table in the context, the training energy for BLOOMz-7B, which is a fine-tuned version of BLOOM-176B, is 51,686 kWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context does not directly provide the percentage of the Switch Transformer's 1500 billion parameters that are activated per token. However, the paper 'Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity' by Fedus et al. (2021) is referenced, which is likely to contain the relevant information.","is_blank","percent","[""shen2024"", ""wu2021a"", ""fedus2021""]","is_blank","is_blank","The context does not directly provide the percentage of the Switch Transformer's 1500 billion parameters that are activated per token. However, the paper 'Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity' by Fedus et al. (2021) is referenced, which is likely to contain the relevant information."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including the number of experts (Nexperts) which is 8.","8","experts","[""shen2024""]","is_blank","is_blank","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including the number of experts (Nexperts) which is 8."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models, including a figure (Fig. 4) that illustrates the execution time breakdown in seconds for different batch sizes. However, the exact total execution time for a dense BlackMamba model with a batch size of 30 is not directly provided.","is_blank","second","[""xia2024""]","is_blank","is_blank","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models, including a figure (Fig. 4) that illustrates the execution time breakdown in seconds for different batch sizes. However, the exact total execution time for a dense BlackMamba model with a batch size of 30 is not directly provided."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides a clear statement about the total cost of training the FLM-101B model and its breakdown for English and Chinese portions. It is stated that the total cost is 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context provides a clear statement about the total cost of training the FLM-101B model and its breakdown for English and Chinese portions. It is stated that the total cost is 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context snippet from [ref_id=khan2025] provides details about the experimental setup for energy-efficient local inference in financial sentiment classification. It mentions that the hardware used includes an '11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency)'.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","is_blank","The context snippet from [ref_id=khan2025] provides details about the experimental setup for energy-efficient local inference in financial sentiment classification. It mentions that the hardware used includes an '11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency)'."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide a clear number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs. The study is mentioned, but specific details about the number of models are not given.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide a clear number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs. The study is mentioned, but specific details about the number of models are not given."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context provides a clear statement that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3×.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides a clear statement that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2024) find that the total compute for model development is 1.2x to 4x larger than the compute for the final training run alone, with a median of 2.2x.","2.2","multiplier","[""cottier2024""]","is_blank","is_blank","Cottier et al. (2024) find that the total compute for model development is 1.2x to 4x larger than the compute for the final training run alone, with a median of 2.2x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides an estimate of the total energy consumption for a full training run of a 6.1 billion parameter transformer model. According to the text, the model was trained for 8 days on 256 NVIDIA A100s and consumed 13.8 MWh. The full training run is estimated to take 60 days, and the total energy consumption is calculated as (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The context provides an estimate of the total energy consumption for a full training run of a 6.1 billion parameter transformer model. According to the text, the model was trained for 8 days on 256 NVIDIA A100s and consumed 13.8 MWh. The full training run is estimated to take 60 days, and the total energy consumption is calculated as (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The 2025 paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate' by Alexandra Sasha Luccioni, Emma Strubell, and Kate Crawford argues that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox. This economic principle suggests that efficiency gains can paradoxically spur increased consumption.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate' by Alexandra Sasha Luccioni, Emma Strubell, and Kate Crawford argues that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox. This economic principle suggests that efficiency gains can paradoxically spur increased consumption."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a clear projection of the global AI demand's water withdrawal in 2027, stating it will account for 4.2 – 6.6 billion cubic meters.","4.2 - 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context provides a clear projection of the global AI demand's water withdrawal in 2027, stating it will account for 4.2 – 6.6 billion cubic meters."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost', which directly contradicts the statement that Red AI is on the decline.","0","is_blank","[""[\""schwartz2019\""]""]","is_blank","is_blank","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost', which directly contradicts the statement that Red AI is on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context directly states that 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days', which is the required information to answer the question.","21.54","days","[""li2025a""]","is_blank","is_blank","The context directly states that 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days', which is the required information to answer the question."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, which directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context states that a vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, which directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippets provide information about the estimated electricity consumption for training GPT-3. Multiple sources consistently state that the training of GPT-3 consumed approximately 1287 MWh of electricity.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context snippets provide information about the estimated electricity consumption for training GPT-3. Multiple sources consistently state that the training of GPT-3 consumed approximately 1287 MWh of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project, which aims to establish a unified approach for comparing the inference efficiency of AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the 'AI Energy Score' project, which aims to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a detailed analysis of the execution time breakdown for different layers and kernels in the MoE layer for both Mixtral and BlackMamba models. Fig. 6 shows the kernel-level MoE time breakdown, which includes the execution time for various kernels such as matrix multiplication (matmul) and other operations. The figure is labeled with execution time in microseconds (μs).","2000","microseconds","[""xia2024""]","is_blank","is_blank","The context provides a detailed analysis of the execution time breakdown for different layers and kernels in the MoE layer for both Mixtral and BlackMamba models. Fig. 6 shows the kernel-level MoE time breakdown, which includes the execution time for various kernels such as matrix multiplication (matmul) and other operations. The figure is labeled with execution time in microseconds (μs)."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including 'Top-k' which is set to 2. This indicates that for each layer of the JetMoE-8B model, 2 experts are selected for activation for a given token.","2","experts","[""shen2024""]","is_blank","is_blank","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including 'Top-k' which is set to 2. This indicates that for each layer of the JetMoE-8B model, 2 experts are selected for activation for a given token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context supports the answer by providing information about the increase in computational resources used for training deep learning models. According to [schwartz2019], there was an estimated 300,000x increase in computations from 2012 to 2018, which is close to the 200,000x increase stated in the question.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context supports the answer by providing information about the increase in computational resources used for training deep learning models. According to [schwartz2019], there was an estimated 300,000x increase in computations from 2012 to 2018, which is close to the 200,000x increase stated in the question."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from [ref_id=wu2021b] directly states that 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].'","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from [ref_id=wu2021b] directly states that 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].'"
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources' in 2023, which directly answers the question.","22","data centers","[""amazon2023""]","is_blank","is_blank","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources' in 2023, which directly answers the question."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context provides information about the inception of the field of Artificial Intelligence (AI). It states that AI was officially born and christened at a 1956 workshop. To find the age of AI in 2025, we need to calculate the difference between 2025 and 1956.","69","years","[""stone2022""]","is_blank","is_blank","The context provides information about the inception of the field of Artificial Intelligence (AI). It states that AI was officially born and christened at a 1956 workshop. To find the age of AI in 2025, we need to calculate the difference between 2025 and 1956."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 2019 study by Strubell et al. estimated the environmental impacts of AI, including the monetary costs, energy use, and GHG emissions required to train various NLP models. The 'five cars' estimate is based on the energy required for neural architecture search (NAS), a large-scale procedure used to automate model development. NAS is performed less frequently than average AI model training due to its scale and resource requirements.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The 2019 study by Strubell et al. estimated the environmental impacts of AI, including the monetary costs, energy use, and GHG emissions required to train various NLP models. The 'five cars' estimate is based on the energy required for neural architecture search (NAS), a large-scale procedure used to automate model development. NAS is performed less frequently than average AI model training due to its scale and resource requirements."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context provides information about the energy consumption of the Llama 3.1 8B model under different configurations. Specifically, it mentions that targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency reduces energy consumption per generation by 44%.","44","percent","[""[ref_id=chung2025]""]","is_blank","is_blank","The context provides information about the energy consumption of the Llama 3.1 8B model under different configurations. Specifically, it mentions that targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency reduces energy consumption per generation by 44%."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III. The table lists the maximum batch size for different model and dataset combinations.","8","samples","[""xia2024""]","is_blank","is_blank","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III. The table lists the maximum batch size for different model and dataset combinations."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context mentions that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The context mentions that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context provides the estimated annual carbon emissions from GPT-4o inference, which are projected to be comparable to the emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The context provides the estimated annual carbon emissions from GPT-4o inference, which are projected to be comparable to the emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides information about the average data center PUE in 2023, which was 1.58 globally. Although it does not specifically mention 'AI-dedicated data centers', it is the most relevant information available in the context.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context provides information about the average data center PUE in 2023, which was 1.58 globally. Although it does not specifically mention 'AI-dedicated data centers', it is the most relevant information available in the context."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context clearly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table listing the environmental impacts of various AI models, including the Llama 7B model. The table includes the GHG emissions associated with pre-training the Llama 7B model.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides a table listing the environmental impacts of various AI models, including the Llama 7B model. The table includes the GHG emissions associated with pre-training the Llama 7B model."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of Amazon electric delivery vans in 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, there were 11,800 electric delivery vans.","14400","electric delivery vans","[""amazon2023""]","is_blank","is_blank","The context provides the number of Amazon electric delivery vans in 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, there were 11,800 electric delivery vans."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""[\""morrison2025\""]""]","is_blank","is_blank","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides a table (Table 4) that lists the energy consumption of various models across three prompt sizes. The energy consumption for the 'o3' model for a long prompt (10k input-1.5k output) is given as 12.222 ± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The context provides a table (Table 4) that lists the energy consumption of various models across three prompt sizes. The energy consumption for the 'o3' model for a long prompt (10k input-1.5k output) is given as 12.222 ± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context snippets consistently state that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","is_blank","The context snippets consistently state that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","is_blank","is_blank","WUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, as reported by the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, as reported by the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides information about the energy per output token estimates of LLaMA 65B across different batch sizes and shards for a maximum generation length of 512 tokens. It is stated that 'with length 512, we see that it takes about 3-4 Joules for a output token'.","3.5","joules per token","[""samsi2024""]","is_blank","is_blank","The context provides information about the energy per output token estimates of LLaMA 65B across different batch sizes and shards for a maximum generation length of 512 tokens. It is stated that 'with length 512, we see that it takes about 3-4 Joules for a output token'."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context clearly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The context clearly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the claim 'AI can reduce global GHG emissions by 5-10%' originates from Boston Consulting Group (BCG) reports. However, the reasoning behind this estimate is unclear, and the underlying calculations are not detailed. The context suggests that applying observations from individual projects to global GHG emissions lacks scientific grounding and rigorous calculation of avoided emissions requires specific methodological requirements.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the claim 'AI can reduce global GHG emissions by 5-10%' originates from Boston Consulting Group (BCG) reports. However, the reasoning behind this estimate is unclear, and the underlying calculations are not detailed. The context suggests that applying observations from individual projects to global GHG emissions lacks scientific grounding and rigorous calculation of avoided emissions requires specific methodological requirements."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context snippet from [ref_id=wu2021a] states that 'training large, sparsely-activated neural networks improves model scalability, achieving higher accuracy at lower operational energy footprint.' This directly supports the statement that sparsely activated DNNs consume less energy than large, dense DNNs without sacrificing accuracy.","1","is_blank","[""wu2021a""]","is_blank","is_blank","The context snippet from [ref_id=wu2021a] states that 'training large, sparsely-activated neural networks improves model scalability, achieving higher accuracy at lower operational energy footprint.' This directly supports the statement that sparsely activated DNNs consume less energy than large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context provides a clear statement that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly answers the question regarding the percentage of running time the optimizer stage takes.","53","percent","[""xia2024""]","is_blank","is_blank","The context provides a clear statement that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly answers the question regarding the percentage of running time the optimizer stage takes."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) as 0.083 kWh per 1,000 queries. To estimate the daily energy consumption for 1 billion queries, we calculate: (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh or 83 MWh.","83","MWh","[""luccioni2024""]","is_blank","is_blank","The context provides the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) as 0.083 kWh per 1,000 queries. To estimate the daily energy consumption for 1 billion queries, we calculate: (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh or 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context provides the average global data center PUE in 2023 as 1.58, directly supporting the answer.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context provides the average global data center PUE in 2023 as 1.58, directly supporting the answer."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","is_blank","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","is_blank"
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a detailed analysis of the total model development costs by Cottier et al. (2024), which includes the percentage of the cost attributed to R&D staff for developing Gemini Ultra. According to the context, R&D staff costs make up 49% of the total amortized model development costs for Gemini Ultra when equity is included.","49","percent","[""cottier2024""]","is_blank","is_blank","The context provides a detailed analysis of the total model development costs by Cottier et al. (2024), which includes the percentage of the cost attributed to R&D staff for developing Gemini Ultra. According to the context, R&D staff costs make up 49% of the total amortized model development costs for Gemini Ultra when equity is included."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was comprised of seventeen members.","17","people","[""stone2022""]","is_blank","is_blank","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was comprised of seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context provides information about the carbon footprint of client devices, stating that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly supports the answer to the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing.","74","percent","[""wu2021a""]","is_blank","is_blank","The context provides information about the carbon footprint of client devices, stating that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly supports the answer to the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context provides evidence that models with more parameters do not always consume more energy during inference. For example, Phi-3 Small (7B) consumes less energy than Phi-3 Mini (3.8B) at larger batch sizes due to differences in attention mechanisms and KV cache memory usage.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context provides evidence that models with more parameters do not always consume more energy during inference. For example, Phi-3 Small (7B) consumes less energy than Phi-3 Mini (3.8B) at larger batch sizes due to differences in attention mechanisms and KV cache memory usage."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind, as stated in the report.","0","is_blank","[""[ref_id=stone2022]""]","is_blank","is_blank","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind, as stated in the report."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides information about the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models and compares it to the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides information about the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models and compares it to the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context clearly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years between 2019 and 2021.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context snippets multiple times mention that AlphaGo defeated the human Go champion, providing evidence for the statement.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context snippets multiple times mention that AlphaGo defeated the human Go champion, providing evidence for the statement."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context clearly states that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This indicates that the batching strategy referred to is 'Continuous batching'.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The context clearly states that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This indicates that the batching strategy referred to is 'Continuous batching'."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context directly states that 'The entire alignment process takes 60 H100 GPU hours.' This information is found in the section discussing the alignment details of JetMoE-8B, specifically after describing the dSFT and dDPO fine-tuning processes.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context directly states that 'The entire alignment process takes 60 H100 GPU hours.' This information is found in the section discussing the alignment details of JetMoE-8B, specifically after describing the dSFT and dDPO fine-tuning processes."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context provides a comparison of instance selection results by SLO constraints for both online and offline workloads. For the offline workload with a 100 TPS SLO, InferSave-1st selected g4dn.xlarge at $2.13, while Max-Performance selected g6e.xlarge at $2.699. The percentage more expensive is calculated as ((2.699 - 2.13) / 2.13) * 100.","26.7","percent","[""kim2025""]","is_blank","is_blank","The context provides a comparison of instance selection results by SLO constraints for both online and offline workloads. For the offline workload with a 100 TPS SLO, InferSave-1st selected g4dn.xlarge at $2.13, while Max-Performance selected g6e.xlarge at $2.699. The percentage more expensive is calculated as ((2.699 - 2.13) / 2.13) * 100."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context provides a clear statement about the carbon emissions of the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, which is 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides a clear statement about the carbon emissions of the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, which is 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context snippet from [ref_id=luccioni2025c] provides the range of GPU energy usage for performing 1,000 inference queries, which is between 0.06 Wh and 3,426 Wh, depending on model size, architecture, and task complexity.","0.06 to 3426","Wh","[""luccioni2025c""]","is_blank","is_blank","The context snippet from [ref_id=luccioni2025c] provides the range of GPU energy usage for performing 1,000 inference queries, which is between 0.06 Wh and 3,426 Wh, depending on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context suggests that researchers believe a nuanced and context-specific approach to AI ethics and sustainability is necessary, rather than a universal, one-size-fits-all approach. This is evident in statements such as 'There is no one-size-fits-all solution for either ethics or sustainability' and the discussion around the need for contextual sensitivity in AI ethics.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context suggests that researchers believe a nuanced and context-specific approach to AI ethics and sustainability is necessary, rather than a universal, one-size-fits-all approach. This is evident in statements such as 'There is no one-size-fits-all solution for either ethics or sustainability' and the discussion around the need for contextual sensitivity in AI ethics."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context does not provide a direct answer to the question about the gross carbon intensity of energy according to the U.S. average mix in 2021. However, it mentions that the carbon intensity of the energy grid used in AWS's us-west-2 region, which is based in Oregon, was 297.6 grams of CO2eq per kWh.","is_blank","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","The context does not provide a direct answer to the question about the gross carbon intensity of energy according to the U.S. average mix in 2021. However, it mentions that the carbon intensity of the energy grid used in AWS's us-west-2 region, which is based in Oregon, was 297.6 grams of CO2eq per kWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions that Hivemind is a PyTorch-based framework developed to enable collaborative DL training in a decentralized fashion. It is used for distributed spot training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that Hivemind is a PyTorch-based framework developed to enable collaborative DL training in a decentralized fashion. It is used for distributed spot training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This term is mentioned in the context as being proposed by Ehsan et al. and involves integrating socio-technical aspects in the description and understanding of AI systems.","social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This term is mentioned in the context as being proposed by Ehsan et al. and involves integrating socio-technical aspects in the description and understanding of AI systems."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings in classification experiments on German public administration texts.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings in classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context clearly mentions that the LLM inference system developed in the 2025 Chen et al. paper is called 'Lamina', which incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context clearly mentions that the LLM inference system developed in the 2025 Chen et al. paper is called 'Lamina', which incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context mentions that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. Although it does not specifically mention T5, it provides information about large language models.","13000000000","parameters","[""morrison2025""]","is_blank","is_blank","The context mentions that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. Although it does not specifically mention T5, it provides information about large language models."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including the total number of parameters (Ptotal).","8B","parameters","[""shen2024""]","is_blank","is_blank","The context provides a table (Table 1) that lists the hyperparameters of JetMoE-8B, including the total number of parameters (Ptotal)."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context provides information about Google's data center water consumption increase from 2021 to 2022.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context provides information about Google's data center water consumption increase from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context provides a table (TABLE I) that lists common carbon emission metrics in Green AI, including 'Carbon Intensity' defined as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The context provides a table (TABLE I) that lists common carbon emission metrics in Green AI, including 'Carbon Intensity' defined as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides a table (Table 2) that lists the FLOPs utilization percentage for different growth stages of FLM-101B training. The final growth stage is the 101B stage, and according to the table, the FLOPs utilization percentage achieved in this stage is 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","The context provides a table (Table 2) that lists the FLOPs utilization percentage for different growth stages of FLM-101B training. The final growth stage is the 101B stage, and according to the table, the FLOPs utilization percentage achieved in this stage is 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context provides multiple estimates for the training costs of OpenAI's GPT-4. The most relevant information is found in references [ref_id=xia2024] and [ref_id=cottier2024].","100000000","USD","[""xia2024"", ""cottier2024""]","is_blank","is_blank","The context provides multiple estimates for the training costs of OpenAI's GPT-4. The most relevant information is found in references [ref_id=xia2024] and [ref_id=cottier2024]."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context provides a detailed analysis of the operational carbon footprint reduction achieved through full-stack optimization for a Transformer-based universal translation model. According to Figure 7 in the context, the optimizations reduce the infrastructure resources required to serve the model at scale by over 800×.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides a detailed analysis of the operational carbon footprint reduction achieved through full-stack optimization for a Transformer-based universal translation model. According to Figure 7 in the context, the optimizations reduce the infrastructure resources required to serve the model at scale by over 800×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information on the performance drop when training NLP models across four continents compared to local training. Specifically, it mentions that intercontinental training leads to a significant penalty on tasks with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The context provides information on the performance drop when training NLP models across four continents compared to local training. Specifically, it mentions that intercontinental training leads to a significant penalty on tasks with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8)."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3500000000","liters","[""amazon2023""]","is_blank","is_blank","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context supports the answer by stating that the study reveals custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context supports the answer by stating that the study reveals custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context provides information about NVIDIA's data center GPU shipments in 2023 and 2024. According to the text, NVIDIA shipped 3.7 million data center GPUs in 2024, which is more than a million units than in 2023.","3.7","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context provides information about NVIDIA's data center GPU shipments in 2023 and 2024. According to the text, NVIDIA shipped 3.7 million data center GPUs in 2024, which is more than a million units than in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides a clear percentage attributed to AI accelerator chips in the amortized hardware and energy cost.","44","percent","[""cottier2024""]","is_blank","is_blank","The context provides a clear percentage attributed to AI accelerator chips in the amortized hardware and energy cost."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context provides information about Facebook's data center efficiency, specifically mentioning that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The context provides information about Facebook's data center efficiency, specifically mentioning that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS (Ethical AI for the Governance of the Society) project, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym for this project is ETAIROS.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions the Finnish ETAIROS (Ethical AI for the Governance of the Society) project, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym for this project is ETAIROS."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context provides an estimate of the training costs for Google's Gemini Ultra. According to the document, the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","30","USD","[""cottier2024""]","is_blank","is_blank","The context provides an estimate of the training costs for Google's Gemini Ultra. According to the document, the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and HRAI systems. The context suggests that these measures should include environmental risks, as the Act's purpose includes protecting fundamental rights, which can be interpreted to encompass environmental protection.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and HRAI systems. The context suggests that these measures should include environmental risks, as the Act's purpose includes protecting fundamental rights, which can be interpreted to encompass environmental protection."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context clearly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³.","9","µg/m³","[""han2024""]","is_blank","is_blank","The context clearly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context provides a life cycle assessment (LCA) comparison between print books and e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","is_blank","The context provides a life cycle assessment (LCA) comparison between print books and e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context clearly states that the per-household health burden in disadvantaged communities could be 200 times more than that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","is_blank","The context clearly states that the per-household health burden in disadvantaged communities could be 200 times more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is directly provided in Table B1 of Appendix B in the context snippet from [ref_id=zschache2025].","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is directly provided in Table B1 of Appendix B in the context snippet from [ref_id=zschache2025]."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency but results in increased usage and overall resource consumption. This is discussed in the context of AI and its environmental impacts.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency but results in increased usage and overall resource consumption. This is discussed in the context of AI and its environmental impacts."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The energy consumption for training GPT-3 is given as 1287 MWh. The carbon footprint of training Meena is equivalent to 242,231 miles driven by an average passenger vehicle. To find how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to compare their energy consumptions. However, the direct energy consumption of Meena is not provided, but it is mentioned that training a 6 billion parameter transformer consumed 13.8 MWh and a full training run would consume approximately 103.5 MWh.","12.44","multiplier","[""li2025b"", ""wu2021a"", ""dodge2022""]","is_blank","is_blank","The energy consumption for training GPT-3 is given as 1287 MWh. The carbon footprint of training Meena is equivalent to 242,231 miles driven by an average passenger vehicle. To find how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to compare their energy consumptions. However, the direct energy consumption of Meena is not provided, but it is mentioned that training a 6 billion parameter transformer consumed 13.8 MWh and a full training run would consume approximately 103.5 MWh."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides a table (Table 2) from a 2024 study on AI's power consumption, which lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for image generation is given as 2.907 kWh per 1,000 inferences.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 2) from a 2024 study on AI's power consumption, which lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for image generation is given as 2.907 kWh per 1,000 inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a table comparing CO2e emissions from various activities, including 'Human life, avg, 1 year' and 'American life, avg, 1 year', with the latter being 36,156 lbs.","36156","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table comparing CO2e emissions from various activities, including 'Human life, avg, 1 year' and 'American life, avg, 1 year', with the latter being 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a table with the top-10 counties by per-household health cost, where Marion County in West Virginia is listed with the highest per-household health cost of $1218.3.","Marion","is_blank","[""han2024""]","is_blank","is_blank","The context provides a table with the top-10 counties by per-household health cost, where Marion County in West Virginia is listed with the highest per-household health cost of $1218.3."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides a table (TABLE III) comparing performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the CO2 emissions decreased from 0.020 kg to 0.015 kg after optimization.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","The context provides a table (TABLE III) comparing performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the CO2 emissions decreased from 0.020 kg to 0.015 kg after optimization."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy costs for training and fine-tuning the BLOOMz-7B model. According to the table in the context, the training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh. The combined energy cost is the sum of these two values.","59,257","kWh","[""luccioni2024"", ""ebert2024""]","is_blank","is_blank","The context provides the energy costs for training and fine-tuning the BLOOMz-7B model. According to the table in the context, the training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh. The combined energy cost is the sum of these two values."
"q125","What is the total number of parameters in the final FLM-101B model?","The context provides detailed information about the FLM-101B model, including its architecture and configuration. It states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. However, it does not directly state the total number of parameters.","101","parameters","[""li2025a""]","is_blank","is_blank","The context provides detailed information about the FLM-101B model, including its architecture and configuration. It states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. However, it does not directly state the total number of parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The energy required for a full training run of a 6.1B parameter model is 103,500 kWh. Using the inference energy cost of BLOOMz-7B (1.0 × 10−4 kWh), we can estimate the number of inferences needed to match the training energy cost.","1,035,000,000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","The energy required for a full training run of a 6.1B parameter model is 103,500 kWh. Using the inference energy cost of BLOOMz-7B (1.0 × 10−4 kWh), we can estimate the number of inferences needed to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context provides a clear statement regarding the total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing'.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides a clear statement regarding the total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing'."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model. According to the table, the cost parity for BLOOMz-7B is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model. According to the table, the cost parity for BLOOMz-7B is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context clearly states that the dataset used for the German nuclear waste site objection texts is called the 'FKTG-dataset'. This information is found in the section '3 Data and methods' of the document with reference id 'zschache2025'.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that the dataset used for the German nuclear waste site objection texts is called the 'FKTG-dataset'. This information is found in the section '3 Data and methods' of the document with reference id 'zschache2025'."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","is_blank","is_blank","liters","[""is_blank""]","is_blank","is_blank","is_blank"
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The relevant snippets discuss the environmental impact of manufacturing GPUs and the use of rare earth metals, but do not mention the use of recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The relevant snippets discuss the environmental impact of manufacturing GPUs and the use of rare earth metals, but do not mention the use of recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a table comparing CO2 emissions from various activities, including air travel between New York and San Francisco. According to the table, air travel for one passenger between NY and SF emits 1984 lbs of CO2e. The question asks for the equivalent number of passengers for 3.2 tCO2e (or 3200 kg CO2e, which is approximately 7055 lbs) emitted by the Evolved Transformer NAS. To find the number of passengers, we divide the total CO2e emissions by the emissions per passenger trip.","3.55","passengers","[""strubell2019""]","is_blank","is_blank","The context provides a table comparing CO2 emissions from various activities, including air travel between New York and San Francisco. According to the table, air travel for one passenger between NY and SF emits 1984 lbs of CO2e. The question asks for the equivalent number of passengers for 3.2 tCO2e (or 3200 kg CO2e, which is approximately 7055 lbs) emitted by the Evolved Transformer NAS. To find the number of passengers, we divide the total CO2e emissions by the emissions per passenger trip."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context provides data from May 2025 on the OpenRouter API platform, indicating that 84% of LLM token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The context provides data from May 2025 on the OpenRouter API platform, indicating that 84% of LLM token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-13B, it specifies that 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-13B, it specifies that 1 A100 80GB GPU is required."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides an estimate of the total energy consumption for a complete training run of a 6.1 billion parameter transformer model, which is approximately 103,593 kWh. However, it does not directly provide the CO2 emissions. Other references discuss CO2 emissions for different models and scenarios, but none directly answer the question for the specific model in question.","is_blank","metric tons","[""is_blank""]","is_blank","is_blank","The context provides an estimate of the total energy consumption for a complete training run of a 6.1 billion parameter transformer model, which is approximately 103,593 kWh. However, it does not directly provide the CO2 emissions. Other references discuss CO2 emissions for different models and scenarios, but none directly answer the question for the specific model in question."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide a specific value for the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post-quantization.","is_blank","tCO2e","[""khan2025""]","is_blank","is_blank","The context does not provide a specific value for the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post-quantization."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context supports the answer by providing a specific scenario where blending A100 and A10G GPUs results in a 24% cost saving over an A100-only strategy.","24","percent","[""[griggs2024]""]","is_blank","is_blank","The context supports the answer by providing a specific scenario where blending A100 and A10G GPUs results in a 24% cost saving over an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides a table from Chen et al. (2024) that lists the price per chip for various NVIDIA GPUs, including the H20. The price per hour for the NVIDIA H20 is listed as $4.63/hr.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The context provides a table from Chen et al. (2024) that lists the price per chip for various NVIDIA GPUs, including the H20. The price per hour for the NVIDIA H20 is listed as $4.63/hr."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context clearly states that 'most carbon footprint analyses gather the information manually by writing to authors' [luccioni2025b], indicating that automatic gathering without contacting authors is not the norm.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context clearly states that 'most carbon footprint analyses gather the information manually by writing to authors' [luccioni2025b], indicating that automatic gathering without contacting authors is not the norm."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context provides information about the public health cost of U.S. data centers in 2023, stating that it is equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","The context provides information about the public health cost of U.S. data centers in 2023, stating that it is equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for running LLaMA models. Specifically, Table II lists the minimum hardware required for LLaMA 7B, 13B, and 65B models. For LLaMA 7B, it is indicated that 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information about the bare minimum hardware requirements for running LLaMA models. Specifically, Table II lists the minimum hardware required for LLaMA 7B, 13B, and 65B models. For LLaMA 7B, it is indicated that 1 A100 80GB GPU is required."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context clearly states that experimental results reveal a reduction in energy consumption and carbon emissions by up to 45% post quantization, supporting the True statement.","1","is_blank","[""[khan2025]""]","is_blank","is_blank","The context clearly states that experimental results reveal a reduction in energy consumption and carbon emissions by up to 45% post quantization, supporting the True statement."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context provides information about researchers reaching out to over 500 authors for their carbon footprint analysis and collecting 95 answers.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The context provides information about researchers reaching out to over 500 authors for their carbon footprint analysis and collecting 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","","","USD per hour","[]","is_blank","is_blank",""
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table (Table 2) that lists the public health cost of training a large AI model in selected U.S. data centers. For Altoona, IA, the health cost is $2.51 million, and the electricity cost is $2.1 million. The percentage of health cost relative to electricity cost is given as 122%.","122","percent","[""han2024""]","is_blank","is_blank","The context provides a table (Table 2) that lists the public health cost of training a large AI model in selected U.S. data centers. For Altoona, IA, the health cost is $2.51 million, and the electricity cost is $2.1 million. The percentage of health cost relative to electricity cost is given as 122%."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table listing Amazon Renewable Energy Projects announced as of January 2024, including the number of projects by location. For the United Kingdom, it lists 36 projects.","36","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table listing Amazon Renewable Energy Projects announced as of January 2024, including the number of projects by location. For the United Kingdom, it lists 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides a table titled 'Amazon Representation by the Numbers' which includes data on the gender distribution of Amazon's workforce in the United States for the year 2023. According to this table, in 2023, 53.2% of Amazon's workforce across all levels identified as women, and 46.6% identified as men.","46.6","percent","[""amazon2023""]","is_blank","is_blank","The context provides a table titled 'Amazon Representation by the Numbers' which includes data on the gender distribution of Amazon's workforce in the United States for the year 2023. According to this table, in 2023, 53.2% of Amazon's workforce across all levels identified as women, and 46.6% identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context clearly states that Apple's supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The context clearly states that Apple's supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models during fine-tuning. Fig. 4 illustrates the execution time breakdown for different batch sizes, including sparse fine-tuning with a batch size of 84 for BlackMamba. The total execution time for sparse BlackMamba fine-tuning with a batch size of 84 is approximately 1.5 seconds.","1.5","seconds","[""xia2024""]","is_blank","is_blank","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models during fine-tuning. Fig. 4 illustrates the execution time breakdown for different batch sizes, including sparse fine-tuning with a batch size of 84 for BlackMamba. The total execution time for sparse BlackMamba fine-tuning with a batch size of 84 is approximately 1.5 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity' metric, which is defined as the ratio of calculation to communication time. This metric is used to assess the suitability of models for distributed training across different hardware setups and network conditions.","granularity","is_blank","[""erben2023""]","is_blank","is_blank","The context introduces the 'granularity' metric, which is defined as the ratio of calculation to communication time. This metric is used to assess the suitability of models for distributed training across different hardware setups and network conditions."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context snippets provide information about a deal between Microsoft and Exxon Mobil to expand oil production, which could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets for the year.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","The context snippets provide information about a deal between Microsoft and Exxon Mobil to expand oil production, which could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The term 'water withdrawal' is defined in the context as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The term 'water withdrawal' is defined in the context as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context clearly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","is_blank","The context clearly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context provides information from a Deloitte report in 2021 stating that the average U.S. household is equipped with an average of 25 connected devices.","25","devices","[""wu2021b""]","is_blank","is_blank","The context provides information from a Deloitte report in 2021 stating that the average U.S. household is equipped with an average of 25 connected devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides information on the energy consumption required to pre-train large language models (LLMs). According to the provided data, the energy consumption ranges from 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout.","0.8-3500","MWh","[""luccioni2025c""]","is_blank","is_blank","The context provides information on the energy consumption required to pre-train large language models (LLMs). According to the provided data, the energy consumption ranges from 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement that it did NOT beat human contenders.","0","is_blank","[""[ref_id=stone2022]""]","is_blank","is_blank","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement that it did NOT beat human contenders."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context provides information from multiple studies estimating the water usage of the GPT-3 model. One study suggests that 10–50 queries on GPT-3 consume around half a liter of water.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","The context provides information from multiple studies estimating the water usage of the GPT-3 model. One study suggests that 10–50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context provides a table comparing the MT-Bench scores of various models, including JetMoE-8B-chat and Llama-2-13b-chat. According to the table, JetMoE-8B-chat achieved a score of 6.681, while Llama-2-13b-chat achieved a score of 6.650. This indicates that after model alignment, JetMoE-8B-Chat surpassed Llama-2-13b-Chat with an MT-Bench score of 6.681.","6.681","score","[""shen2024""]","is_blank","is_blank","The context provides a table comparing the MT-Bench scores of various models, including JetMoE-8B-chat and Llama-2-13b-chat. According to the table, JetMoE-8B-chat achieved a score of 6.681, while Llama-2-13b-chat achieved a score of 6.650. This indicates that after model alignment, JetMoE-8B-Chat surpassed Llama-2-13b-Chat with an MT-Bench score of 6.681."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context provides information about the water consumption of GPT-3. According to the text, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses.","30","responses","[""[\""li2025b\""]""]","is_blank","is_blank","The context provides information about the water consumption of GPT-3. According to the text, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings.","77","percent","[""griggs2024""]","is_blank","is_blank","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context clearly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This directly supports the answer to the question about the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context clearly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This directly supports the answer to the question about the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","","","round trips","[]","is_blank","is_blank",""
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context snippets, NVIDIA estimated that inference processing makes up a significant portion of the machine learning (ML) workload in 2019. The exact percentage is mentioned in the context provided by multiple sources.","85","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","According to the context snippets, NVIDIA estimated that inference processing makes up a significant portion of the machine learning (ML) workload in 2019. The exact percentage is mentioned in the context provided by multiple sources."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context provides the total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study. It is stated that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides the total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study. It is stated that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'"
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method because it nearly always results in an overestimation. The document states that 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.'","0","is_blank","[""chung2025""]","is_blank","is_blank","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method because it nearly always results in an overestimation. The document states that 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.'"
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and has a slightly higher energy consumption (3.098 Wh vs 2.875 Wh) due to deployment on older A100 GPU nodes instead of H100s.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and has a slightly higher energy consumption (3.098 Wh vs 2.875 Wh) due to deployment on older A100 GPU nodes instead of H100s."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a figure (Fig. 8) that shows the query throughput of Mixtral and BlackMamba. For Mixtral-CS-A100-40GB with a dense batch size of 1, the throughput is approximately 0.3 queries/second.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","The context provides a figure (Fig. 8) that shows the query throughput of Mixtral and BlackMamba. For Mixtral-CS-A100-40GB with a dense batch size of 1, the throughput is approximately 0.3 queries/second."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context indicates that the direct release of environmental information by AI developers peaked in 2022 and then decreased. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category.","0","is_blank","[""[\""luccioni2025c\""]""]","is_blank","is_blank","The context indicates that the direct release of environmental information by AI developers peaked in 2022 and then decreased. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides a table with on-demand prices for various NVIDIA GPU types, including the H100. The normalized on-demand hourly price for an H100 GPU is directly stated in the table.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The context provides a table with on-demand prices for various NVIDIA GPU types, including the H100. The normalized on-demand hourly price for an H100 GPU is directly stated in the table."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides information about the water consumption during the training of GPT-3, but not GPT-4. However, it mentions that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone. Since 1 kiloliter = 1000 liters, 700 kL equals 700,000 liters. There is no direct information about GPT-4's water usage.","is_blank","liters of water","[""jegham2025""]","is_blank","is_blank","The context provides information about the water consumption during the training of GPT-3, but not GPT-4. However, it mentions that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone. Since 1 kiloliter = 1000 liters, 700 kL equals 700,000 liters. There is no direct information about GPT-4's water usage."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the monthly on-demand rental cost for serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs, which is over $5,200. To estimate the hourly cost, we need to calculate the total hours in a month and divide the monthly cost by this number. Assuming a 30-day month, there are 30 * 24 = 720 hours.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context provides the monthly on-demand rental cost for serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs, which is over $5,200. To estimate the hourly cost, we need to calculate the total hours in a month and divide the monthly cost by this number. Assuming a 30-day month, there are 30 * 24 = 720 hours."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the amount of CO2 emitted for training a Transformer model with Neural Architecture Search as 626,155 lbs. Although it does not directly provide the emissions-to-driving-distance ratio, it mentions that the emissions are comparable to the lifetime carbon emissions of five cars. Using the given lifetime emissions of a car as 126,000 lbs, we can infer the equivalent driving distance.","2268.69","miles","[""strubell2019""]","is_blank","is_blank","The context provides the amount of CO2 emitted for training a Transformer model with Neural Architecture Search as 626,155 lbs. Although it does not directly provide the emissions-to-driving-distance ratio, it mentions that the emissions are comparable to the lifetime carbon emissions of five cars. Using the given lifetime emissions of a car as 126,000 lbs, we can infer the equivalent driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads by the inference energy cost and the number of inferences. The BLOOMz-7B model was downloaded 606,096 times.","60.61","MWh","[""luccioni2024""]","is_blank","is_blank","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads by the inference energy cost and the number of inferences. The BLOOMz-7B model was downloaded 606,096 times."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context clearly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context clearly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context provides an estimate of the total number of floating point operations to train GPT-3. According to [ref_id=cottier2024], smaller versions of GPT-3 used 4.5e22 FLOP.","4.5e22","FLOPS","[""cottier2024""]","is_blank","is_blank","The context provides an estimate of the total number of floating point operations to train GPT-3. According to [ref_id=cottier2024], smaller versions of GPT-3 used 4.5e22 FLOP."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context clearly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This directly answers the question about the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context clearly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This directly answers the question about the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput data for the final 101B training stage of FLM-101B. The single-GPU throughput for the 101B stage is 165 teraFLOPs/sec, and the total number of GPUs used is 2160. The total computational work performed during this stage can be estimated using the formula: total FLOPs = throughput * number of GPUs * training time. The training time for the 101B stage is not directly given, but the total time cost for training FLM-101B is 21.54 days, and the number of tokens processed in the 101B stage is 26.54B.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context provides the throughput data for the final 101B training stage of FLM-101B. The single-GPU throughput for the 101B stage is 165 teraFLOPs/sec, and the total number of GPUs used is 2160. The total computational work performed during this stage can be estimated using the formula: total FLOPs = throughput * number of GPUs * training time. The training time for the 101B stage is not directly given, but the total time cost for training FLM-101B is 21.54 days, and the number of tokens processed in the 101B stage is 26.54B."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context snippet from [ref_id=schwartz2019] mentions AlexNet and its performance on ImageNet, indicating that AlexNet achieved lower accuracy compared to ResNet. Although the exact top-1 accuracy of AlexNet is not directly stated, it is implied that AlexNet has much lower accuracy than ResNet, which achieved around 76.1% top-1 accuracy after 90 epochs of fully-supervised training as mentioned in [ref_id=wu2021a]. However, the exact top-1 accuracy associated with AlexNet 2012 is not directly provided in the given context.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippet from [ref_id=schwartz2019] mentions AlexNet and its performance on ImageNet, indicating that AlexNet achieved lower accuracy compared to ResNet. Although the exact top-1 accuracy of AlexNet is not directly stated, it is implied that AlexNet has much lower accuracy than ResNet, which achieved around 76.1% top-1 accuracy after 90 epochs of fully-supervised training as mentioned in [ref_id=wu2021a]. However, the exact top-1 accuracy associated with AlexNet 2012 is not directly provided in the given context."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8 = 192.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8 = 192."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides information about the estimated CO2 emissions from training a Transformer model with Neural Architecture Search (NAS). According to Strubell et al. (2019), the estimated CO2 emissions are 626,155 pounds, which is comparable to the lifetime carbon emissions of five US cars. The average American lifetime emissions are given as 36,156 pounds per year.","17.32","lifetimes","[""strubell2019"", ""luccioni2025c""]","is_blank","is_blank","The context provides information about the estimated CO2 emissions from training a Transformer model with Neural Architecture Search (NAS). According to Strubell et al. (2019), the estimated CO2 emissions are 626,155 pounds, which is comparable to the lifetime carbon emissions of five US cars. The average American lifetime emissions are given as 36,156 pounds per year."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context provides information about the number of GPU hours required to train FAIR's RoBERTa on 160GB of text. According to the text, FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25,000","hours","[""schwartz2019""]","is_blank","is_blank","The context provides information about the number of GPU hours required to train FAIR's RoBERTa on 160GB of text. According to the text, FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context provides information about Amazon's on-site solar energy systems and their impact on CO2e emissions. It states that these systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources.","47500","metric tons","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's on-site solar energy systems and their impact on CO2e emissions. It states that these systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context snippets provide information about various frameworks and techniques used for deploying large language models (LLMs) across multiple GPUs and nodes. Specifically, the snippet from [ref_id=zschache2025] mentions the use of the vllm library, which runs on a ray cluster for multi-node computations, and discusses tensor model parallelism (TMP) and pipeline model parallelism (PMP) for distributing model weights across GPUs.","is_blank","is_blank","[""[ref_id=zschache2025]""]","is_blank","is_blank","The context snippets provide information about various frameworks and techniques used for deploying large language models (LLMs) across multiple GPUs and nodes. Specifically, the snippet from [ref_id=zschache2025] mentions the use of the vllm library, which runs on a ray cluster for multi-node computations, and discusses tensor model parallelism (TMP) and pipeline model parallelism (PMP) for distributing model weights across GPUs."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table B2) that compares the energy consumption of various models, including Llama 3.1 70B, when deployed on a single node versus two nodes. For Llama 3.1 70B, the energy consumption increased from 48.60 Wh to 94.88 Wh when deployed on two nodes instead of one, representing a factor increase of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","The context provides a table (Table B2) that compares the energy consumption of various models, including Llama 3.1 70B, when deployed on a single node versus two nodes. For Llama 3.1 70B, the energy consumption increased from 48.60 Wh to 94.88 Wh when deployed on two nodes instead of one, representing a factor increase of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context provides estimates of water consumption for GPT-3, but not specifically for ChatGPT user sessions in 2023. One study mentioned that 10–50 queries on GPT-3 consume around half a liter of water, but this is not directly related to ChatGPT user sessions in 2023.","is_blank","gallons of water","[""luccioni2025a"", ""li2025b""]","is_blank","is_blank","The context provides estimates of water consumption for GPT-3, but not specifically for ChatGPT user sessions in 2023. One study mentioned that 10–50 queries on GPT-3 consume around half a liter of water, but this is not directly related to ChatGPT user sessions in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides a study on the environmental impact of GPT-4o, including its energy consumption. It states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes.","35,000","homes","[""jegham2025""]","is_blank","is_blank","The context provides a study on the environmental impact of GPT-4o, including its energy consumption. It states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context provides information from a 2025 paper that mentions Microsoft's reported percentage increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The context provides information from a 2025 paper that mentions Microsoft's reported percentage increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis benchmarks."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context provides information about the PUE of Google's Iowa datacenter, which is referred to as 'Augusta'. The trailing twelve-month average PUE for Augusta was 1.12.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","The context provides information about the PUE of Google's Iowa datacenter, which is referred to as 'Augusta'. The trailing twelve-month average PUE for Augusta was 1.12."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides a clear estimate of the total number of GPT-4o queries in 2025, stating 'yielding a total of approximately 772 billion GPT-4o queries in 2025'.","772000000000","queries","[""jegham2025""]","is_blank","is_blank","The context provides a clear estimate of the total number of GPT-4o queries in 2025, stating 'yielding a total of approximately 772 billion GPT-4o queries in 2025'."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides a table (Table 3) that includes the OpenLLM leaderboard average score for JetMoE-8B, which is 53.0.","53.0","score","[""shen2024""]","is_blank","is_blank","The context provides a table (Table 3) that includes the OpenLLM leaderboard average score for JetMoE-8B, which is 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context snippets do not provide a direct answer to the question about the number of AI training runs conducted globally on renewable-only power in 2022. However, there are some relevant details about AI's environmental impact and energy consumption.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context snippets do not provide a direct answer to the question about the number of AI training runs conducted globally on renewable-only power in 2022. However, there are some relevant details about AI's environmental impact and energy consumption."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source general-purpose AI models are currently exempt from reporting their energy consumption under the AI Act, but there is a proposal to remove this exemption. The AI Act does not clearly state that open-source models are fully exempt unless they pose systemic risk.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that open-source general-purpose AI models are currently exempt from reporting their energy consumption under the AI Act, but there is a proposal to remove this exemption. The AI Act does not clearly state that open-source models are fully exempt unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context does not provide a direct answer to the US national datacenter average Power Usage Effectiveness (PUE) in 2020. However, it mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. Since the question asks for the 2020 value, and the context does not provide it directly, we cannot infer the exact value for 2020 based on the given information.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide a direct answer to the US national datacenter average Power Usage Effectiveness (PUE) in 2020. However, it mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. Since the question asks for the 2020 value, and the context does not provide it directly, we cannot infer the exact value for 2020 based on the given information."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a figure (Figure 1) that shows the analysis of KV Cache size growth for the OPT-2.7B model. According to the figure, when the batch size is 32, the KV Cache size is approximately 5.312GB.","5.312","GB","[""[\""kim2025\""]""]","is_blank","is_blank","The context provides a figure (Figure 1) that shows the analysis of KV Cache size growth for the OPT-2.7B model. According to the figure, when the batch size is 32, the KV Cache size is approximately 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides a clear percentage range for R&D staff costs, including equity, as a fraction of the total amortized cost for the four notable models studied by Cottier et al.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context provides a clear percentage range for R&D staff costs, including equity, as a fraction of the total amortized cost for the four notable models studied by Cottier et al."
"q213","Which software package was used to measure energy consumption during inference runs?","The context snippets from multiple references ([ref_id=morrison2025], [ref_id=rubei2025], [ref_id=zschache2025]) consistently mention that CodeCarbon was used to measure energy consumption during inference runs.","CodeCarbon","is_blank","[""[ref_id=morrison2025], [ref_id=rubei2025], [ref_id=zschache2025]""]","is_blank","is_blank","The context snippets from multiple references ([ref_id=morrison2025], [ref_id=rubei2025], [ref_id=zschache2025]) consistently mention that CodeCarbon was used to measure energy consumption during inference runs."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides an analysis of 100 news articles on ChatGPT's energy use, where 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context provides an analysis of 100 news articles on ChatGPT's energy use, where 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context provides information about the Compute Time Calibration Function (CTCF), which is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The context provides information about the Compute Time Calibration Function (CTCF), which is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context indicates that as the number of shards increases, the energy per output token also increases for LLaMA 65B. Figures 8 and 9 show energy per response estimates across different batch sizes and shards, supporting the statement.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context indicates that as the number of shards increases, the energy per output token also increases for LLaMA 65B. Figures 8 and 9 show energy per response estimates across different batch sizes and shards, supporting the statement."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information on the water consumption for mining rare earth materials used in manufacturing an H100 GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and that an H100 GPU is 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","0.0022","kL","[""morrison2025""]","is_blank","is_blank","The context provides information on the water consumption for mining rare earth materials used in manufacturing an H100 GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and that an H100 GPU is 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses various policy proposals related to AI and environmental impact, including the elimination of the open-source exemption from reporting obligations. It is proposed that open-source models should adhere to the same reporting standards as proprietary models, implying that currently, they are exempt. Therefore, under current EU rules, open-source general-purpose AI models do not have to report their energy consumption to authorities.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context discusses various policy proposals related to AI and environmental impact, including the elimination of the open-source exemption from reporting obligations. It is proposed that open-source models should adhere to the same reporting standards as proprietary models, implying that currently, they are exempt. Therefore, under current EU rules, open-source general-purpose AI models do not have to report their energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippets from multiple references (luccioni2025a, wu2021b) clearly state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context snippets from multiple references (luccioni2025a, wu2021b) clearly state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides a clear statement about the total public health cost of U.S. data centers in 2023, based on the average attribution method.","6.7","USD","[""han2024""]","is_blank","is_blank","The context provides a clear statement about the total public health cost of U.S. data centers in 2023, based on the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model and GPT-4.1 nano for a long prompt is found in Table 4. The o3 model consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. To find the factor by which o3 is greater than GPT-4.1 nano, we divide the energy consumption of o3 by that of GPT-4.1 nano.","14.78","multiplier","[""[\""jegham2025\""]""]","is_blank","is_blank","The energy consumption of the o3 model and GPT-4.1 nano for a long prompt is found in Table 4. The o3 model consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. To find the factor by which o3 is greater than GPT-4.1 nano, we divide the energy consumption of o3 by that of GPT-4.1 nano."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context provides a detailed analysis of Mélange's cost reduction compared to single-GPU baselines across different datasets and SLOs. For the short-context dataset (Arena) with a 120ms SLO, Mélange achieves a 15-77% cost reduction.","15-77","percent","[""griggs2024""]","is_blank","is_blank","The context provides a detailed analysis of Mélange's cost reduction compared to single-GPU baselines across different datasets and SLOs. For the short-context dataset (Arena) with a 120ms SLO, Mélange achieves a 15-77% cost reduction."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and other well-known LLMs. According to this table, the net tCO2e for FLM-101B is 26.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and other well-known LLMs. According to this table, the net tCO2e for FLM-101B is 26."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a detailed analysis of the execution time breakdown, throughput, and cost estimation for fine-tuning Mixtral models on different GPUs. However, it does not directly provide the total execution time in seconds for a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides a detailed analysis of the execution time breakdown, throughput, and cost estimation for fine-tuning Mixtral models on different GPUs. However, it does not directly provide the total execution time in seconds for a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. This is supported by the statement 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.'","0","is_blank","[""han2024""]","is_blank","is_blank","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. This is supported by the statement 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.'"
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet from [ref_id=wu2021b] directly supports the statement that GPU theoretical performance per watt doubles every 3-4 years as of 2019 product data.","1","is_blank","[""[\""wu2021b\""]""]","is_blank","is_blank","The context snippet from [ref_id=wu2021b] directly supports the statement that GPU theoretical performance per watt doubles every 3-4 years as of 2019 product data."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama, an open-source tool, was used for local AI model deployment and applied 4-bit quantization to support the deployment of large language models in the financial sentiment case study.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context mentions that Ollama, an open-source tool, was used for local AI model deployment and applied 4-bit quantization to support the deployment of large language models in the financial sentiment case study."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the datasets were accessed on-demand via shards in the tar format with the WebDataset library, which has features like automatic local caching, streaming decompression, and streaming preprocessing.","Backblaze B2","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that the datasets were accessed on-demand via shards in the tar format with the WebDataset library, which has features like automatic local caching, streaming decompression, and streaming preprocessing."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context snippets discuss experiments measuring inference with large language models and their energy consumption. One snippet mentions a 'strong correlation between inference energy consumption and model runtime', indicating that execution time can serve as a practical proxy for energy usage.","1","is_blank","[""zschache2025"", ""samsi2024""]","is_blank","is_blank","The context snippets discuss experiments measuring inference with large language models and their energy consumption. One snippet mentions a 'strong correlation between inference energy consumption and model runtime', indicating that execution time can serve as a practical proxy for energy usage."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on 1 Feb 2024.","Senator Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides a table from Chen et al. (2024) that lists the price per chip for various NVIDIA GPUs, including the H100, which is $11.06 per hour.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The context provides a table from Chen et al. (2024) that lists the price per chip for various NVIDIA GPUs, including the H100, which is $11.06 per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context provides information about the average expected lifetime of servers in AI data centers. According to the Amazon Sustainability Report 2023, AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This information is directly relevant to the question about the estimated average GPU lifetime before retirement in AI data centers in 2024.","6","years","[""amazon2023""]","is_blank","is_blank","The context provides information about the average expected lifetime of servers in AI data centers. According to the Amazon Sustainability Report 2023, AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This information is directly relevant to the question about the estimated average GPU lifetime before retirement in AI data centers in 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for running LLaMA models. Specifically, Table II lists the bare minimum hardware required for different LLaMA variants. For LLaMA-13B, it is stated that 2 V100 32GB GPUs are required.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context provides information about the bare minimum hardware requirements for running LLaMA models. Specifically, Table II lists the bare minimum hardware required for different LLaMA variants. For LLaMA-13B, it is stated that 2 V100 32GB GPUs are required."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides information about the reported GHG emissions from the pre-training process for Google's Gemma family of language models. It states that Google reports 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides information about the reported GHG emissions from the pre-training process for Google's Gemma family of language models. It states that Google reports 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. According to Peters et al. (2018), ELMo was trained for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. According to Peters et al. (2018), ELMo was trained for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the estimated U.S. national average water consumption for electricity generation. According to the document, the U.S. national average water consumption is estimated at about 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context provides the estimated U.S. national average water consumption for electricity generation. According to the document, the U.S. national average water consumption is estimated at about 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides information about the PUE of Google's hyperscale data centers in 2021. According to Figure 1 in the context, Google's PUE improved to 1.10 in 2021. Additionally, other references [dodge2022] and [cottier2024] confirm that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","1.10","PUE","[""wu2021b"", ""dodge2022"", ""cottier2024""]","is_blank","is_blank","The context provides information about the PUE of Google's hyperscale data centers in 2021. According to Figure 1 in the context, Google's PUE improved to 1.10 in 2021. Additionally, other references [dodge2022] and [cottier2024] confirm that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy, a goal achieved by Amazon in 2023.","96","percent","[""amazon2023""]","is_blank","is_blank","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy, a goal achieved by Amazon in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a clear prediction from the model that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU costs $3460.","3460","USD","[""xia2024""]","is_blank","is_blank","The context provides a clear prediction from the model that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU costs $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context provides information about the percentage of electricity consumption attributed to GPUs in datacenters. According to the provided context snippets, GPUs account for almost 3/4 of electricity consumption when training AI models.","74","percent","[""dodge2022""]","is_blank","is_blank","The context provides information about the percentage of electricity consumption attributed to GPUs in datacenters. According to the provided context snippets, GPUs account for almost 3/4 of electricity consumption when training AI models."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides a clear statement about the average GPU power for a single node during OLMo 2 7B training. According to Figure 2 and the accompanying text, when actively training, the average GPU power is over 600W.","600","Watts","[""morrison2025""]","is_blank","is_blank","The context provides a clear statement about the average GPU power for a single node during OLMo 2 7B training. According to Figure 2 and the accompanying text, when actively training, the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a table listing estimated CO2 emissions from various activities, including 'Human life, avg, 1 year' with a value of 11,023 lbs.","11023","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table listing estimated CO2 emissions from various activities, including 'Human life, avg, 1 year' with a value of 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides a comparison of inference performance between LLaMA 7B, 13B, and 65B models on V100 and A100 GPUs. It states that for LLaMA 13B, there is a 1.25 times increase in inference throughput on A100 compared to V100 GPUs.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides a comparison of inference performance between LLaMA 7B, 13B, and 65B models on V100 and A100 GPUs. It states that for LLaMA 13B, there is a 1.25 times increase in inference throughput on A100 compared to V100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides a clear estimate of the energy consumption for a single short query to GPT-4o, stating it consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context provides a clear estimate of the energy consumption for a single short query to GPT-4o, stating it consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge, which is about 280% more expensive than InferSave's top choice.","280","percent","[""kim2025""]","is_blank","is_blank","The context states that with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge, which is about 280% more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context snippet from [ref_id=zschache2025] states that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.' This directly addresses the question about the most energy-efficient GPU architecture for models generating only a single classification token.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context snippet from [ref_id=zschache2025] states that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.' This directly addresses the question about the most energy-efficient GPU architecture for models generating only a single classification token."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context supports the answer by stating that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice' in the reference [ref_id=schwartz2019]. This indicates that Green AI involves providing the financial cost of these processes.","1","is_blank","[""[\""schwartz2019\""]""]","is_blank","is_blank","The context supports the answer by stating that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice' in the reference [ref_id=schwartz2019]. This indicates that Green AI involves providing the financial cost of these processes."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context snippet from the 2025 paper by Luccioni et al. directly states that the total amount of electronic waste generated worldwide in 2022 was 62 million tonnes.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context snippet from the 2025 paper by Luccioni et al. directly states that the total amount of electronic waste generated worldwide in 2022 was 62 million tonnes."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides information about the power consumption of various GPUs and TPUs. Specifically, it mentions the TDP of the V100 GPU and TPU v3. To answer the question, we need to find the average system power per processor for TPU v2 and V100 GPU.","is_blank","Watts","[""cottier2024"", ""luccioni2023""]","is_blank","is_blank","The context provides information about the power consumption of various GPUs and TPUs. Specifically, it mentions the TDP of the V100 GPU and TPU v3. To answer the question, we need to find the average system power per processor for TPU v2 and V100 GPU."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context provides information about the water consumption of training the GPT-3 language model in Microsoft's U.S. data centers. According to the text, training GPT-3 can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","is_blank","The context provides information about the water consumption of training the GPT-3 language model in Microsoft's U.S. data centers. According to the text, training GPT-3 can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides a detailed analysis of the eco-efficiency of various AI models using Data Envelopment Analysis (DEA). According to Figure 8 and the accompanying text, o3-mini achieved the highest cross-efficiency score (0.884) in the recent eco-efficiency analysis.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides a detailed analysis of the eco-efficiency of various AI models using Data Envelopment Analysis (DEA). According to Figure 8 and the accompanying text, o3-mini achieved the highest cross-efficiency score (0.884) in the recent eco-efficiency analysis."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context snippet from [ref_id=wu2021b] mentions that current averages for cell phone lifetimes are less than 3 years, which supports the statement.","1","is_blank","[""[\""wu2021b\""]""]","is_blank","is_blank","The context snippet from [ref_id=wu2021b] mentions that current averages for cell phone lifetimes are less than 3 years, which supports the statement."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that 'CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.' This indicates that for CV models, the per-GPU speedup is nearly linear.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that 'CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.' This indicates that for CV models, the per-GPU speedup is nearly linear."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context snippet [ref_id=li2025a] explicitly states that 'The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.' This directly answers the question about the context window size for the FLM-101B model.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context snippet [ref_id=li2025a] explicitly states that 'The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.' This directly answers the question about the context window size for the FLM-101B model."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","","","is_blank","[]","is_blank","is_blank",""
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides a table showing Amazon's representation by the numbers for People Managers globally in 2023, where 31.3% are women.","31.3","percent","[""amazon2023""]","is_blank","is_blank","The context provides a table showing Amazon's representation by the numbers for People Managers globally in 2023, where 31.3% are women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025) when excluding equity. According to the text, 'With equity excluded from R&D costs, the fraction of hardware cost... rise to 61–76%.'","68.5","percent","[""cottier2024""]","is_blank","is_blank","The context provides the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025) when excluding equity. According to the text, 'With equity excluded from R&D costs, the fraction of hardware cost... rise to 61–76%.'"
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context indicates that in the financial sentiment case study, accuracy and F1 scores did not always improve after optimization. In fact, they were slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context indicates that in the financial sentiment case study, accuracy and F1 scores did not always improve after optimization. In fact, they were slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context provides a conversion factor (0.954 lbs/kWh) used to estimate CO2 emissions based on power consumption in the U.S., as provided by the U.S. Environmental Protection Agency (EPA). This conversion factor is directly related to the average CO2 produced per kilowatt-hour for power consumed in the U.S.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The context provides a conversion factor (0.954 lbs/kWh) used to estimate CO2 emissions based on power consumption in the U.S., as provided by the U.S. Environmental Protection Agency (EPA). This conversion factor is directly related to the average CO2 produced per kilowatt-hour for power consumed in the U.S."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a clear statement from a recent study suggesting that the global AI could consume 85 – 134 TWh of electricity in 2027.","85-134","TWh","[""[\""li2025b\""]""]","is_blank","is_blank","The context provides a clear statement from a recent study suggesting that the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context provides information about Amazon's delivery statistics in 2023, including the number of packages delivered via electric vehicles (EVs) in different regions. Specifically, it states that in Europe, Amazon delivered 150 million packages via EVs.","150","packages","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's delivery statistics in 2023, including the number of packages delivered via electric vehicles (EVs) in different regions. Specifically, it states that in Europe, Amazon delivered 150 million packages via EVs."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The total number of tokens processed during the online inference workload evaluation is calculated by multiplying the number of requests by the sum of input and output tokens. The context states that the online inference workload uses 128 input tokens and 512 output tokens, and evaluates a total of 3000 requests.","1920000","tokens","[""kim2025""]","is_blank","is_blank","The total number of tokens processed during the online inference workload evaluation is calculated by multiplying the number of requests by the sum of input and output tokens. The context states that the online inference workload uses 128 input tokens and 512 output tokens, and evaluates a total of 3000 requests."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. It specifically states that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. It specifically states that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context supports the answer by stating that for very short experiments like DenseNet 201, which ran for less than half an hour, the Flexible Start optimization analysis found significant reduction in CO2 emissions, greater than 30% in multiple regions, and up to 80% in West US.","80","percent","[""dodge2022""]","is_blank","is_blank","The context supports the answer by stating that for very short experiments like DenseNet 201, which ran for less than half an hour, the Flexible Start optimization analysis found significant reduction in CO2 emissions, greater than 30% in multiple regions, and up to 80% in West US."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context provides a study from 2024 that compares the energy consumption of different tasks. It states that the least energy-intensive task is text classification with a mean consumption of 0.002 KwH per 1,000 inferences, and the most energy-intensive task is image generation with a mean consumption of 2.9 kWh per 1,000 inferences.","1453.5","times","[""luccioni2024""]","is_blank","is_blank","The context provides a study from 2024 that compares the energy consumption of different tasks. It states that the least energy-intensive task is text classification with a mean consumption of 0.002 KwH per 1,000 inferences, and the most energy-intensive task is image generation with a mean consumption of 2.9 kWh per 1,000 inferences."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis benchmarks."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table that lists Amazon Renewable Energy Projects by location as of January 2024. For the United States, it shows 244 projects.","244","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table that lists Amazon Renewable Energy Projects by location as of January 2024. For the United States, it shows 244 projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide a direct statement about the percentage of power usage covered by renewable energy for Amazon's AWS in 2018. However, it mentions that by 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. A table from a different source [ref_id=strubell2019] shows that in 2018, Amazon-AWS sourced 17% of its energy from renewable sources.","17","percent","[""strubell2019""]","is_blank","is_blank","The context does not provide a direct statement about the percentage of power usage covered by renewable energy for Amazon's AWS in 2018. However, it mentions that by 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. A table from a different source [ref_id=strubell2019] shows that in 2018, Amazon-AWS sourced 17% of its energy from renewable sources."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility.","cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides a detailed breakdown of the electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU. According to Table 1, the GPU accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","is_blank","is_blank","The context provides a detailed breakdown of the electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU. According to Table 1, the GPU accounts for 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context provides information about the hardware requirements for serving large language models. Specifically, it mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context provides information about the hardware requirements for serving large language models. Specifically, it mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context provides information about the operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization. According to Figure 8 and Section III-B, the iterative optimization process led to a 28.5% operational energy footprint reduction.","28.5","percent","[""wu2021a""]","is_blank","is_blank","The context provides information about the operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization. According to Figure 8 and Section III-B, the iterative optimization process led to a 28.5% operational energy footprint reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides an estimate of the upfront hardware acquisition cost to train GPT-4, stating it cost $800M.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context provides an estimate of the upfront hardware acquisition cost to train GPT-4, stating it cost $800M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context clearly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the sustainability of AI approaches themselves, not just the former.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context clearly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the sustainability of AI approaches themselves, not just the former."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a figure (Fig. 13) that correlates the projected maximum batch size with the ground truth for fine-tuning Mixtral on different NVIDIA GPUs. For the A100-40GB GPU, the ground truth maximum batch size is shown.","8","samples","[""xia2024""]","is_blank","is_blank","The context provides a figure (Fig. 13) that correlates the projected maximum batch size with the ground truth for fine-tuning Mixtral on different NVIDIA GPUs. For the A100-40GB GPU, the ground truth maximum batch size is shown."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context supports the answer by comparing the energy consumption of Recomputation and Swapping preemption mechanisms in an overloaded LLM inference server. Figure 8 shows that Swapping consistently consumes less energy than Recomputation when the server is overloaded.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The context supports the answer by comparing the energy consumption of Recomputation and Swapping preemption mechanisms in an overloaded LLM inference server. Figure 8 shows that Swapping consistently consumes less energy than Recomputation when the server is overloaded."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The Google 2024 Environmental Report is referenced in the context, and it is stated that Google reported a 48% increase in GHG emissions since 2019.","48","percent","[""jegham2025"", ""luccioni2025a"", ""han2024""]","is_blank","is_blank","The Google 2024 Environmental Report is referenced in the context, and it is stated that Google reported a 48% increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context provides projections from McKinsey and other sources regarding the percentage of U.S. national electricity consumption that data centers are anticipated to account for in 2030. According to the context snippet from [ref_id=fernandez2025], projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030.","10.4","percent","[""[\""fernandez2025\""]""]","is_blank","is_blank","The context provides projections from McKinsey and other sources regarding the percentage of U.S. national electricity consumption that data centers are anticipated to account for in 2030. According to the context snippet from [ref_id=fernandez2025], projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides a figure (Fig. 4) and tables (e.g., Table 5, Table 9) that show the CO2 emissions decrease in percent for the 'Pause and Resume' optimization for training a 6B Parameter Transformer. The maximum potential emissions saving is found to be around 25% when the job duration is increased by 100%.","25","percent","[""dodge2022""]","is_blank","is_blank","The context provides a figure (Fig. 4) and tables (e.g., Table 5, Table 9) that show the CO2 emissions decrease in percent for the 'Pause and Resume' optimization for training a 6B Parameter Transformer. The maximum potential emissions saving is found to be around 25% when the job duration is increased by 100%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context snippets clearly state that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","is_blank","The context snippets clearly state that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context provides information about a seminal 2019 study by Strubell et al. that quantified the carbon footprint of training BERT, a large language model. The study found that training BERT resulted in 626,155 pounds of CO2 emissions.","626,155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context provides information about a seminal 2019 study by Strubell et al. that quantified the carbon footprint of training BERT, a large language model. The study found that training BERT resulted in 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides a clear estimate of the training energy of GPT-3.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context provides a clear estimate of the training energy of GPT-3."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context snippets from the document [xia2024] clearly state that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. This indicates that the MoE layer is a prime target for optimization to enhance the performance of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context snippets from the document [xia2024] clearly state that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. This indicates that the MoE layer is a prime target for optimization to enhance the performance of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a detailed characterization study of fine-tuning Mixtral and BlackMamba models on a NVIDIA A40 GPU with 48GB memory. It includes experiments on different datasets, including Hellaswag, and discusses the maximum batch size supported for fine-tuning. However, it does not directly state the maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset.","is_blank","samples","[""xia2024""]","is_blank","is_blank","The context provides a detailed characterization study of fine-tuning Mixtral and BlackMamba models on a NVIDIA A40 GPU with 48GB memory. It includes experiments on different datasets, including Hellaswag, and discusses the maximum batch size supported for fine-tuning. However, it does not directly state the maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that 'for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.' This directly supports the answer to the True or False question.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that 'for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.' This directly supports the answer to the True or False question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides a comparison between the emissions of task-specific BERT-based models and multi-purpose models. Specifically, it states that 'bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries'.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides a comparison between the emissions of task-specific BERT-based models and multi-purpose models. Specifically, it states that 'bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries'."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context provides a clear comparison between the CO2 emissions from training a BERT model in the most and least efficient regions, stating '7k grams vs. 26k grams, for the most efficient vs. least efficient regions.'","19","grams","[""dodge2022""]","is_blank","is_blank","The context provides a clear comparison between the CO2 emissions from training a BERT model in the most and least efficient regions, stating '7k grams vs. 26k grams, for the most efficient vs. least efficient regions.'"
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context clearly states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context clearly states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table with estimated water consumption for training various models, including OLMo 60M. The equivalent water usage for one person in the US for training OLMo 60M on 1.7 to 5.6 trillion tokens is listed as 5 days.","5","days","[""morrison2025""]","is_blank","is_blank","The context provides a table with estimated water consumption for training various models, including OLMo 60M. The equivalent water usage for one person in the US for training OLMo 60M on 1.7 to 5.6 trillion tokens is listed as 5 days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. However, it mentions that one technology company's self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023.","is_blank","liters of freshwater","[""li2025b""]","is_blank","is_blank","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. However, it mentions that one technology company's self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This implies that adding compute resources to accelerate the MoE layers can reduce costs, not increase them. Therefore, the statement 'Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs' is false.","0","is_blank","[""[\""xia2024\""]""]","is_blank","is_blank","The context states that 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This implies that adding compute resources to accelerate the MoE layers can reduce costs, not increase them. Therefore, the statement 'Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs' is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is directly stated in the context as 40 MkWh.","40","kWh","[""li2025a""]","is_blank","is_blank","The total energy consumption for training the FLM-101B model is directly stated in the context as 40 MkWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides a study's projections for 2030 is not available, however, the projection for 2028 is given. According to the study, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario.","20.9","USD","[""han2024""]","is_blank","is_blank","The context provides a study's projections for 2030 is not available, however, the projection for 2028 is given. According to the study, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using different GPUs. For the NVIDIA A40-48GB GPU, the estimated cost is $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using different GPUs. For the NVIDIA A40-48GB GPU, the estimated cost is $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models during fine-tuning. It highlights that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average. The maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory is reported in Table III. For a sparse Mixtral model, the maximum batch size is 8 for the CS dataset and 3 for the MATH dataset.","8","samples","[""xia2024""]","is_blank","is_blank","The context provides a detailed analysis of the execution time breakdown for Mixtral and BlackMamba models during fine-tuning. It highlights that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average. The maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory is reported in Table III. For a sparse Mixtral model, the maximum batch size is 8 for the CS dataset and 3 for the MATH dataset."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides a detailed analysis of the fine-tuning performance of sparse Mixtral models on different GPUs, including the NVIDIA A40-48GB. Table III shows the maximum batch size supported by Mixtral sparse fine-tuning on the MATH dataset is 3. Fig. 14 shows the throughput of sparse Mixtral fine-tuning on the MATH dataset with different batch sizes on the A40 GPU. For a batch size of 3, the throughput is approximately 0.6 queries/second. To calculate the total execution time, we need to know the total number of queries. Although the exact number of queries is not given in the context for the specific question, it is mentioned that a realistic data size is 2M queries. Using the throughput value, we can estimate the total execution time as total_queries / throughput.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context provides a detailed analysis of the fine-tuning performance of sparse Mixtral models on different GPUs, including the NVIDIA A40-48GB. Table III shows the maximum batch size supported by Mixtral sparse fine-tuning on the MATH dataset is 3. Fig. 14 shows the throughput of sparse Mixtral fine-tuning on the MATH dataset with different batch sizes on the A40 GPU. For a batch size of 3, the throughput is approximately 0.6 queries/second. To calculate the total execution time, we need to know the total number of queries. Although the exact number of queries is not given in the context for the specific question, it is mentioned that a realistic data size is 2M queries. Using the throughput value, we can estimate the total execution time as total_queries / throughput."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context clearly states that GPU-level power consumption tracking is not recommended for overall energy measurements because it substantially under-represents the actual energy consumption by measuring just a single component.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that GPU-level power consumption tracking is not recommended for overall energy measurements because it substantially under-represents the actual energy consumption by measuring just a single component."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context snippet from [ref_id=luccioni2025b] states that Luccioni et al. found training accounted for only half of the model's overall emissions in a 2023 article estimating the carbon footprint of BLOOM.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context snippet from [ref_id=luccioni2025b] states that Luccioni et al. found training accounted for only half of the model's overall emissions in a 2023 article estimating the carbon footprint of BLOOM."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA variants. For LLaMA-7B, it is stated that 1 V100 32GB GPU is required.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA variants. For LLaMA-7B, it is stated that 1 V100 32GB GPU is required."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides information about the water consumption of GPT-3. According to the text, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. For GPT-3 trained in Arizona, the number of requests for 500ml of water is 16.7.","16.7","requests","[""li2025b""]","is_blank","is_blank","The context provides information about the water consumption of GPT-3. According to the text, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. For GPT-3 trained in Arizona, the number of requests for 500ml of water is 16.7."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides an estimate of CO2 emissions for the average US home energy use for a year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context provides an estimate of CO2 emissions for the average US home energy use for a year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not provide a specific score achieved by the JetMoE-8B model on the GSM8k benchmark.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide a specific score achieved by the JetMoE-8B model on the GSM8k benchmark."
