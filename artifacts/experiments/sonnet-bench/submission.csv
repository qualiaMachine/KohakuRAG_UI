"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","Based on the context, the benchmark suite presented is called the ML.ENERGY Benchmark, which is a tool for measuring inference energy consumption under realistic service environments.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","Based on the context, the benchmark suite presented is called the ML.ENERGY Benchmark, which is a tool for measuring inference energy consumption under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","Based on Figure 4 in wu2021a, the CO2e emissions from training GShard-600B model can be seen in the bar chart showing 'Operational Carbon Footprint of Large-Scale ML Tasks'. The training emissions for GShard-600B appear to be approximately 0.4 million kg CO2e (400,000 kg CO2e).","400000","tCO2e","[""wu2021a""]","is_blank","is_blank","Based on Figure 4 in wu2021a, the CO2e emissions from training GShard-600B model can be seen in the bar chart showing 'Operational Carbon Footprint of Large-Scale ML Tasks'. The training emissions for GShard-600B appear to be approximately 0.4 million kg CO2e (400,000 kg CO2e)."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","According to Table 3 in the Chen 2024 paper, the LLaMA-33B model has a size of 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","According to Table 3 in the Chen 2024 paper, the LLaMA-33B model has a size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific data about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. While there are discussions about data center energy usage and AI model training, none of the sources give this specific information.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific data about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. While there are discussions about data center energy usage and AI model training, none of the sources give this specific information."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","According to the context, hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020. This is explicitly stated in the wu2021b document.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context, hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020. This is explicitly stated in the wu2021b document."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the context, GPT-3 needs to consume a 500 mL bottle of water for roughly 10-50 medium-length responses.","0.02-0.1","500 mL bottles","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to consume a 500 mL bottle of water for roughly 10-50 medium-length responses."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","According to the context, 75% of CVPR papers target accuracy while 20% target efficiency. The difference between these percentages is therefore 75% - 20% = 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","According to the context, 75% of CVPR papers target accuracy while 20% target efficiency. The difference between these percentages is therefore 75% - 20% = 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","According to the context, under the AI Act, energy consumption data is restricted to authorities and is not accessible to the general public or downstream providers due to confidentiality clauses in Articles 21(3), 53(7), and 78(1). The limited availability of this data reduces transparency and accountability.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, under the AI Act, energy consumption data is restricted to authorities and is not accessible to the general public or downstream providers due to confidentiality clauses in Articles 21(3), 53(7), and 78(1). The limited availability of this data reduces transparency and accountability."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to Figure 13 and the explicit text in the paper, for a projected GPU capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","According to Figure 13 and the explicit text in the paper, for a projected GPU capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for LLaMA-7B model, the A100 GPUs provided a 2x increase in inference throughput compared to V100 GPUs across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","is_blank","According to the context, for LLaMA-7B model, the A100 GPUs provided a 2x increase in inference throughput compared to V100 GPUs across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","According to the context, GPT-3 was estimated to consume 5.439 million liters of water for training in Microsoft's U.S. data centers based on the U.S. average values shown in Table 1. This total water consumption includes both on-site (0.708 million L) and off-site (4.731 million L) water usage.","5.439","liters","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 was estimated to consume 5.439 million liters of water for training in Microsoft's U.S. data centers based on the U.S. average values shown in Table 1. This total water consumption includes both on-site (0.708 million L) and off-site (4.731 million L) water usage."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","According to the context from ebert2024, the authors explicitly state that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety' because the carbon footprint of AI models is unrelated to their risk classification.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context from ebert2024, the authors explicitly state that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety' because the carbon footprint of AI models is unrelated to their risk classification."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","According to Amazon's 2023 Sustainability Report, AWS improved its global data center Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, down from 0.19 L/kWh in 2022, representing a 5% improvement year over year.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","According to Amazon's 2023 Sustainability Report, AWS improved its global data center Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, down from 0.19 L/kWh in 2022, representing a 5% improvement year over year."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","According to the context, local inference was emphasized as a sustainability measure specifically because it reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers. This is directly stated in the framework overview section of khan2025.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, local inference was emphasized as a sustainability measure specifically because it reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers. This is directly stated in the framework overview section of khan2025."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context strongly supports this statement, showing that tracking runtime is critical for estimating compute costs. For example, Strubell et al. (2019) uses training time to calculate cloud compute costs for various models, while Luccioni (2023) shows how training time is a key factor in calculating both energy consumption and associated carbon emissions.","1","is_blank","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","The context strongly supports this statement, showing that tracking runtime is critical for estimating compute costs. For example, Strubell et al. (2019) uses training time to calculate cloud compute costs for various models, while Luccioni (2023) shows how training time is a key factor in calculating both energy consumption and associated carbon emissions."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the context, the LLaMA-65B model achieved up to a 13.2% performance improvement (latency reduction) through automated resource utilization overlapping, with the speedup being particularly notable for larger batch sizes due to larger KV tensors.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to the context, the LLaMA-65B model achieved up to a 13.2% performance improvement (latency reduction) through automated resource utilization overlapping, with the speedup being particularly notable for larger batch sizes due to larger KV tensors."
"q164","How much does an elephant weigh?","While the context mentions wild Asian elephants in the Western Ghats conservation zone in India, it does not provide any specific weight information for elephants.","is_blank","lbs","[""amazon2023""]","is_blank","is_blank","While the context mentions wild Asian elephants in the Western Ghats conservation zone in India, it does not provide any specific weight information for elephants."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","None of the provided context snippets contain direct energy consumption comparisons between Meena, T5, GPT-3, GShard-600B, and Switch Transformer. While T5 is mentioned once with 85.7 MWh energy consumption, the energy data for the other specific models is not provided in the context.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","None of the provided context snippets contain direct energy consumption comparisons between Meena, T5, GPT-3, GShard-600B, and Switch Transformer. While T5 is mentioned once with 85.7 MWh energy consumption, the energy data for the other specific models is not provided in the context."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","According to the Strubell et al. 2019 paper, training BERT base model produces 39 lbs of CO₂ emissions. Since an average American produces 36,156 lbs of CO₂ per year, BERT base training is equivalent to approximately 0.4 days of an average American's CO₂ emissions (39/36,156 * 365 = 0.4 days).","0.4","days","[""strubell2019""]","is_blank","is_blank","According to the Strubell et al. 2019 paper, training BERT base model produces 39 lbs of CO₂ emissions. Since an average American produces 36,156 lbs of CO₂ per year, BERT base training is equivalent to approximately 0.4 days of an average American's CO₂ emissions (39/36,156 * 365 = 0.4 days)."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","Based on the provided context snippets, there is no direct comparison of performance between Transformer and Evolved Transformer architectures on the WMT'24 EN-DE BLEU task as model sizes grow. While several documents discuss transformer architectures and model scaling, none specifically address this comparative performance claim.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, there is no direct comparison of performance between Transformer and Evolved Transformer architectures on the WMT'24 EN-DE BLEU task as model sizes grow. While several documents discuss transformer architectures and model scaling, none specifically address this comparative performance claim."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","Based on the context provided from fernandez2025, the BurstGPT dataset containing 5,842 labeled entries is not explicitly mentioned. While BurstGPT is referenced in Tables 1 and 2 showing sequence length statistics, the total number of entries in the dataset is not specified.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the context provided from fernandez2025, the BurstGPT dataset containing 5,842 labeled entries is not explicitly mentioned. While BurstGPT is referenced in Tables 1 and 2 showing sequence length statistics, the total number of entries in the dataset is not specified."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","According to the research, eight T4 spot instances can be more cost-efficient than a DGX-2 for computer vision (CV) models, where the 8xT4 setup is 58% cheaper than DGX-2 while being 37% slower. However, for NLP models and ASR models like Whisper, the DGX-2 remains more cost-efficient due to low granularity issues with the T4 setup.","0","is_blank","[""erben2023""]","is_blank","is_blank","According to the research, eight T4 spot instances can be more cost-efficient than a DGX-2 for computer vision (CV) models, where the 8xT4 setup is 58% cheaper than DGX-2 while being 37% slower. However, for NLP models and ASR models like Whisper, the DGX-2 remains more cost-efficient due to low granularity issues with the T4 setup."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context, the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions or energy usage, showing a disconnect between sustainability and ethics in AI regulation.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions or energy usage, showing a disconnect between sustainability and ethics in AI regulation."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to the German Energy Efficiency Act of 2023, data centers are required to run on 50% renewable energy initially, increasing to 100% renewable energy by January 1, 2027 (Section 11 of the Act).","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the German Energy Efficiency Act of 2023, data centers are required to run on 50% renewable energy initially, increasing to 100% renewable energy by January 1, 2027 (Section 11 of the Act)."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to Figure 2 in the paper, out of the sampled papers from ACL, only 0 papers targeted both accuracy and efficiency. The paper shows that 90% of ACL papers targeted accuracy alone, and 10% targeted efficiency alone, with no papers falling into the 'both' category.","0","papers","[""schwartz2019""]","is_blank","is_blank","According to Figure 2 in the paper, out of the sampled papers from ACL, only 0 papers targeted both accuracy and efficiency. The paper shows that 90% of ACL papers targeted accuracy alone, and 10% targeted efficiency alone, with no papers falling into the 'both' category."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","According to recent estimates cited in multiple sources, inference can account for up to 90% of a model's total lifecycle energy use. This is supported by AWS estimates showing inference makes up 80-90% of total ML cloud computing demand.","90","percent","[""jegham2025"", ""luccioni2024""]","is_blank","is_blank","According to recent estimates cited in multiple sources, inference can account for up to 90% of a model's total lifecycle energy use. This is supported by AWS estimates showing inference makes up 80-90% of total ML cloud computing demand."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","According to the context, while the AI Act requires reporting of energy consumption for general-purpose AI models during development/training (per Article 53(1)(a)), it does not mandate the disclosure of energy consumption during the inference phase, which is described as a 'crucial omission' given the long-term environmental impact.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, while the AI Act requires reporting of energy consumption for general-purpose AI models during development/training (per Article 53(1)(a)), it does not mandate the disclosure of energy consumption during the inference phase, which is described as a 'crucial omission' given the long-term environmental impact."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","Based on the context, the AI Act does not currently require reporting of energy consumption during the inference phase. This is explicitly mentioned as a 'crucial omission' in the text, and is one of the gaps identified in the current legislation that authors propose should be addressed in future amendments.","0","is_blank","[""ebert2024""]","is_blank","is_blank","Based on the context, the AI Act does not currently require reporting of energy consumption during the inference phase. This is explicitly mentioned as a 'crucial omission' in the text, and is one of the gaps identified in the current legislation that authors propose should be addressed in future amendments."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","According to the context, new data centers dedicated to AI training typically rely on liquid cooling, not air cooling, due to high server power densities.","0","is_blank","[""li2025b""]","is_blank","is_blank","According to the context, new data centers dedicated to AI training typically rely on liquid cooling, not air cooling, due to high server power densities."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to Figure 7 and the accompanying text in Wu et al. (2021), platform-level caching improved power efficiency by 6.7x compared to the CPU server baseline for the cross-lingual ML language translation task (LM).","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to Figure 7 and the accompanying text in Wu et al. (2021), platform-level caching improved power efficiency by 6.7x compared to the CPU server baseline for the cross-lingual ML language translation task (LM)."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to Table 3 in Strubell et al. 2019, training BERT base model using 64 V100 GPUs for 79 hours resulted in CO2 emissions of 1438 lbs.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to Table 3 in Strubell et al. 2019, training BERT base model using 64 V100 GPUs for 79 hours resulted in CO2 emissions of 1438 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to multiple sources in the context, including reports from AWS, ML inference accounts for 80-90% of total ML cloud computing demand. This figure is consistently cited across several documents.","85","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","According to multiple sources in the context, including reports from AWS, ML inference accounts for 80-90% of total ML cloud computing demand. This figure is consistently cited across several documents."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to dodge2022, training a 6.1B parameter language model to completion would consume approximately 103,500 kWh of electricity. However, the context does not provide a direct conversion to household-years of electricity consumption, making it impossible to definitively answer the question.","is_blank","household-years","[""dodge2022""]","is_blank","is_blank","According to dodge2022, training a 6.1B parameter language model to completion would consume approximately 103,500 kWh of electricity. However, the context does not provide a direct conversion to household-years of electricity consumption, making it impossible to definitively answer the question."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","According to the context, in geo-distributed NLP experiments (C-8), the external egress cost for Google Cloud was $4.329/h, which was more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, in geo-distributed NLP experiments (C-8), the external egress cost for Google Cloud was $4.329/h, which was more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","According to the context, JetMoE-8B used 30,000 H100 GPU hours and was trained on a cluster containing 96 H100s. Therefore, the wall-clock time can be calculated as: 30,000 GPU hours ÷ 96 GPUs = 312.5 hours ≈ 13 days.","13","days","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B used 30,000 H100 GPU hours and was trained on a cluster containing 96 H100s. Therefore, the wall-clock time can be calculated as: 30,000 GPU hours ÷ 96 GPUs = 312.5 hours ≈ 13 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","Based on the context, water consumption is defined as 'water withdrawal minus water discharge' and refers to the amount of water that is evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","Based on the context, water consumption is defined as 'water withdrawal minus water discharge' and refers to the amount of water that is evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the context, the energy per second for LLaMA-65B inference ranged from 300 Watts to 1 Kilowatt, varying based on the GPU shard configuration from 8 GPUs (lower end) to 32 GPUs (higher end).","300-1000","W","[""samsi2024""]","is_blank","is_blank","According to the context, the energy per second for LLaMA-65B inference ranged from 300 Watts to 1 Kilowatt, varying based on the GPU shard configuration from 8 GPUs (lower end) to 32 GPUs (higher end)."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context, when comparing Qwen models, the Qwen 2.5 72B model consumed seven times more energy than the Qwen 2.5 7B model while only achieving a minor accuracy improvement of 0.07 points.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context, when comparing Qwen models, the Qwen 2.5 72B model consumed seven times more energy than the Qwen 2.5 7B model while only achieving a minor accuracy improvement of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","According to the data table in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg per inference task after applying quantization and local inference techniques, representing a reduction of approximately 56%.","56","percent","[""khan2025""]","is_blank","is_blank","According to the data table in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg per inference task after applying quantization and local inference techniques, representing a reduction of approximately 56%."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the context, the early 2025 iteration of the ML.ENERGY Benchmark included measurements of 40 widely used model architectures across 6 different tasks.","40","models","[""chung2025""]","is_blank","is_blank","According to the context, the early 2025 iteration of the ML.ENERGY Benchmark included measurements of 40 widely used model architectures across 6 different tasks."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","According to the context, when training an AI model of Llama-3.1 scale in Iowa, the health cost will increase dramatically to $2.5 million due to factors such as wind direction and pollutant emission rate for electricity generation.","2500000","USD","[""han2024""]","is_blank","is_blank","According to the context, when training an AI model of Llama-3.1 scale in Iowa, the health cost will increase dramatically to $2.5 million due to factors such as wind direction and pollutant emission rate for electricity generation."
