"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The paper describes a benchmark suite called the ML.ENERGY Benchmark that is designed for measuring inference energy consumption.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The paper describes a benchmark suite called the ML.ENERGY Benchmark that is designed for measuring inference energy consumption."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The provided excerpts include a figure that lists CO2e emissions for several models, including GShard-600B, but the actual numeric value for the GShard-600B training emissions is not given in the text.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided excerpts include a figure that lists CO2e emissions for several models, including GShard-600B, but the actual numeric value for the GShard-600B training emissions is not given in the text."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","Table 3 lists the model parameters for LLaMA-33B, showing its size as 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","Table 3 lists the model parameters for LLaMA-33B, showing its size as 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The provided documents do not contain any information about the electricity consumption of Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The provided documents do not contain any information about the electricity consumption of Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The document states that between traditional and highly optimized hyperscale data centers, PUE shows a stark difference – more than 40% higher efficiency for hyperscale data centers, which supports the claim for 2020.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The document states that between traditional and highly optimized hyperscale data centers, PUE shows a stark difference – more than 40% higher efficiency for hyperscale data centers, which supports the claim for 2020."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The documents state that GPT‑3 consumes one 500 mL bottle of water for roughly 10 – 50 medium‑length responses. Therefore, each medium‑length completion uses about 1/10 to 1/50 of a bottle, i.e., roughly 0.02–0.1 bottles.","0.02–0.1","500 mL bottles","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The documents state that GPT‑3 consumes one 500 mL bottle of water for roughly 10 – 50 medium‑length responses. Therefore, each medium‑length completion uses about 1/10 to 1/50 of a bottle, i.e., roughly 0.02–0.1 bottles."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy and 20% of CVPR papers target efficiency. The difference between these percentages is 75% - 20% = 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context states that 75% of CVPR papers target accuracy and 20% of CVPR papers target efficiency. The difference between these percentages is 75% - 20% = 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context states that when the AI Act does require disclosure of energy consumption, the information is limited to authorities and is not accessible to the public, NGOs, or analysts, indicating the data is not publicly available.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that when the AI Act does require disclosure of energy consumption, the information is limited to authorities and is not accessible to the public, NGOs, or analysts, indicating the data is not publicly available."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The document states that for a GPU memory capacity of 100 GB, the analytical model predicts a maximum batch size of 28 for fine‑tuning Mixtral.","28","samples","[""xia2024""]","is_blank","is_blank","The document states that for a GPU memory capacity of 100 GB, the analytical model predicts a maximum batch size of 28 for fine‑tuning Mixtral."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The paper reports that for the LLaMA‑7B model the A100 GPU provides roughly a 2× increase in inference throughput (words/tokens/responses per second) compared to the V100.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The paper reports that for the LLaMA‑7B model the A100 GPU provides roughly a 2× increase in inference throughput (words/tokens/responses per second) compared to the V100."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","Table 1 lists the operational water consumption for training GPT‑3 (in million liters) for each U.S. Microsoft data‑center location. Adding the values for all U.S. locations (U.S. Average, Arizona, Georgia, Illinois, Iowa, Texas, Virginia, Washington, Wyoming) gives a total of about 47.5 million liters.","47.5","liters","[""li2025b""]","is_blank","is_blank","Table 1 lists the operational water consumption for training GPT‑3 (in million liters) for each U.S. Microsoft data‑center location. Adding the values for all U.S. locations (U.S. Average, Arizona, Georgia, Illinois, Iowa, Texas, Virginia, Washington, Wyoming) gives a total of about 47.5 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The authors explicitly state that sustainability impact assessments should not be limited to high‑risk AI models but should also apply to all AI systems.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The authors explicitly state that sustainability impact assessments should not be limited to high‑risk AI models but should also apply to all AI systems."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The AWS sustainability report states that the global data center Water Use Effectiveness (WUE) was improved to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The AWS sustainability report states that the global data center Water Use Effectiveness (WUE) was improved to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The document states that local inference is emphasized to reduce network overhead and carbon footprint, highlighting its role as a sustainability measure.","1","is_blank","[""khan2025""]","is_blank","is_blank","The document states that local inference is emphasized to reduce network overhead and carbon footprint, highlighting its role as a sustainability measure."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The documents explain that training time (runtime) is a key factor in calculating both carbon emissions and cloud compute cost, and they emphasize that authors should report training time when estimating costs.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","The documents explain that training time (runtime) is a key factor in calculating both carbon emissions and cloud compute cost, and they emphasize that authors should report training time when estimating costs."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The Chen et al. study reports that enabling automated resource utilization overlapping for LLaMA-65B yields up to a 13.2% improvement in performance, which corresponds to latency reduction.","13.2","percent","[""chen2024""]","is_blank","is_blank","The Chen et al. study reports that enabling automated resource utilization overlapping for LLaMA-65B yields up to a 13.2% improvement in performance, which corresponds to latency reduction."
"q164","How much does an elephant weigh?","The provided documents discuss AI model training, environmental impact, and Amazon sustainability initiatives, but they contain no information about the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The provided documents discuss AI model training, environmental impact, and Amazon sustainability initiatives, but they contain no information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The training of BERT base emits about 626,155 pounds of CO₂ (luccioni2025b). An average American life emits about 36,156 pounds of CO₂ per year (strubell2019), which is roughly 99 pounds per day. Dividing the BERT training emissions by the daily average yields about 6,300 days of emissions.","6325","days","[""luccioni2025b"", ""strubell2019""]","is_blank","is_blank","The training of BERT base emits about 626,155 pounds of CO₂ (luccioni2025b). An average American life emits about 36,156 pounds of CO₂ per year (strubell2019), which is roughly 99 pounds per day. Dividing the BERT training emissions by the daily average yields about 6,300 days of emissions."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The paper states that using eight T4 spot instances can be more cost‑efficient than a DGX‑2 node for distributed training.","1","is_blank","[""erben2023""]","is_blank","is_blank","The paper states that using eight T4 spot instances can be more cost‑efficient than a DGX‑2 node for distributed training."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage, indicating the statement is false.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage, indicating the statement is false."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German 2023 Energy Efficiency Act sets a target for data centers to use 50% renewable energy now and increase to 100% renewable energy by 1 January 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The German 2023 Energy Efficiency Act sets a target for data centers to use 50% renewable energy now and increase to 100% renewable energy by 1 January 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The excerpts report that 90% of ACL papers target accuracy and 10% target efficiency, but they do not give the proportion or count of papers that target both accuracy and efficiency, so the specific number cannot be derived.","is_blank","papers","[""is_blank""]","is_blank","is_blank","The excerpts report that 90% of ACL papers target accuracy and 10% target efficiency, but they do not give the proportion or count of papers that target both accuracy and efficiency, so the specific number cannot be derived."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The document by Jegham et al. states that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use.","90","percent","[""jegham2025""]","is_blank","is_blank","The document by Jegham et al. states that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context states that the AI Act does not mandate disclosure of energy consumption during the inference phase and that its reporting requirement only covers the development (training) phase, not inference.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Act does not mandate disclosure of energy consumption during the inference phase and that its reporting requirement only covers the development (training) phase, not inference."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context explicitly states that the AI Act does not mandate disclosure of energy consumption during the inference phase, indicating that such reporting is not currently required.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the AI Act does not mandate disclosure of energy consumption during the inference phase, indicating that such reporting is not currently required."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context states that new AI training data centers often rely on liquid cooling because of high server power densities, which contradicts the claim that they often rely on air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context states that new AI training data centers often rely on liquid cooling because of high server power densities, which contradicts the claim that they often rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The paper states that platform-level caching improves power efficiency by a factor of 6.7× compared to a CPU server baseline for the cross‑lingual Transformer language model.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The paper states that platform-level caching improves power efficiency by a factor of 6.7× compared to a CPU server baseline for the cross‑lingual Transformer language model."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","Table 3 in the Strubell et al. (2019) paper lists the BERT‑base model trained on 64 V100 GPUs with 79 training hours and reports its CO₂ emissions as 1438 lbs.","1438","lbs","[""strubell2019""]","is_blank","is_blank","Table 3 in the Strubell et al. (2019) paper lists the BERT‑base model trained on 64 V100 GPUs with 79 training hours and reports its CO₂ emissions as 1438 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The paper by Chung et al. (2025) states that ML inference accounts for 80–90% of the total compute demand.","80-90%","percent","[""chung2025""]","is_blank","is_blank","The paper by Chung et al. (2025) states that ML inference accounts for 80–90% of the total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","is_blank","is_blank","household-years","[""is_blank""]","is_blank","is_blank","is_blank"
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The paper reports that for the NLP experiments the external egress cost for GC is $4.329/h, which is more than 90% of the total per‑VM cost ($4.804/h), and notes that egress costs account for over 90% of the total cost in NLP experiments on GC and Azure.","1","is_blank","[""erben2023""]","is_blank","is_blank","The paper reports that for the NLP experiments the external egress cost for GC is $4.329/h, which is more than 90% of the total per‑VM cost ($4.804/h), and notes that egress costs account for over 90% of the total cost in NLP experiments on GC and Azure."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The report states that JetMoE-8B used 30,000 H100 GPU hours and was trained on a cluster with 96 H100 GPUs. Dividing the total GPU hours by the number of GPUs gives the wall‑clock time in hours (30,000 ÷ 96 ≈ 312.5 h), which equals about 13 days.","13","days","[""shen2024""]","is_blank","is_blank","The report states that JetMoE-8B used 30,000 H100 GPU hours and was trained on a cluster with 96 H100 GPUs. Dividing the total GPU hours by the number of GPUs gives the wall‑clock time in hours (30,000 ÷ 96 ≈ 312.5 h), which equals about 13 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as “water withdrawal minus water discharge”, meaning the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context defines water consumption as “water withdrawal minus water discharge”, meaning the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The document states that inference energy per second for LLaMA‑65B ranges from about 300 W at the lowest shard count (8 GPUs) up to about 1 kW at the highest shard count (32 GPUs).","300 Watts to 1 Kilowatt","W","[""samsi2024""]","is_blank","is_blank","The document states that inference energy per second for LLaMA‑65B ranges from about 300 W at the lowest shard count (8 GPUs) up to about 1 kW at the highest shard count (32 GPUs)."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The document states that the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, implying the 72B model uses seven times more energy than the 7B model.","7","multiplier","[""zschache2025""]","is_blank","is_blank","The document states that the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, implying the 72B model uses seven times more energy than the 7B model."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","Table III shows Qwen's carbon emissions decreasing from 0.009 kg before optimization to 0.004 kg after applying quantization and local inference. The reduction is (0.009‑0.004)/0.009 ≈ 55.6%, indicating a roughly 56% fall.","55.6","percent","[""khan2025""]","is_blank","is_blank","Table III shows Qwen's carbon emissions decreasing from 0.009 kg before optimization to 0.004 kg after applying quantization and local inference. The reduction is (0.009‑0.004)/0.009 ≈ 55.6%, indicating a roughly 56% fall."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The paper states that the early 2025 iteration of the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 tasks.","40","models","[""chung2025""]","is_blank","is_blank","The paper states that the early 2025 iteration of the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 tasks."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The document reports that the total health cost for training a Llama-3.1‑scale model in Iowa is $2.5 million, and Altoona is a location in Iowa, so this is the estimated health cost for Altoona.","2500000","USD","[""han2024""]","is_blank","is_blank","The document reports that the total health cost for training a Llama-3.1‑scale model in Iowa is $2.5 million, and Altoona is a location in Iowa, so this is the estimated health cost for Altoona."
