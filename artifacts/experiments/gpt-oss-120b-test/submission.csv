"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of U.S. data centers increased by about 4% from 2010‑2014, which directly gives the average increase over that period.","4","percent","[""wu2021b""]","is_blank","is_blank","The context states that the total energy consumption of U.S. data centers increased by about 4% from 2010‑2014, which directly gives the average increase over that period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The Amazon Sustainability Report states that the Maryland‑CPV Backbone solar farm will avoid over 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13,900","cars","[""amazon2023""]","is_blank","is_blank","The Amazon Sustainability Report states that the Maryland‑CPV Backbone solar farm will avoid over 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The report states that in 2023 AWS increased the number of data centers using recycled water for cooling from 20 to 24.","24","data centers","[""amazon2023""]","is_blank","is_blank","The report states that in 2023 AWS increased the number of data centers using recycled water for cooling from 20 to 24."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The authors state that, because NVIDIA does not disclose embodied emissions, they assume the same value as Luccioni et al. (2023), which is 3700 kg CO₂eq per 8‑GPU server node, resulting in an estimated 463 kg CO₂eq per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The authors state that, because NVIDIA does not disclose embodied emissions, they assume the same value as Luccioni et al. (2023), which is 3700 kg CO₂eq per 8‑GPU server node, resulting in an estimated 463 kg CO₂eq per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that GPT-4’s amortized training cost is $40 million, while FLM-101B was trained within a $100 thousand budget. Dividing $40 million by $100 thousand gives a factor of 400.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","The context states that GPT-4’s amortized training cost is $40 million, while FLM-101B was trained within a $100 thousand budget. Dividing $40 million by $100 thousand gives a factor of 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","Table 1 in the Strubell 2019 paper lists the CO2e emissions for air travel of one passenger on a round‑trip between New York and San Francisco as 1984 lbs.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","Table 1 in the Strubell 2019 paper lists the CO2e emissions for air travel of one passenger on a round‑trip between New York and San Francisco as 1984 lbs."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The document states that on the Open LLM Leaderboard, FLM-101B achieved an average score of 43.94, which is also shown in Table 4 under the Average column for FLM-101B.","43.94","score","[""li2025a""]","is_blank","is_blank","The document states that on the Open LLM Leaderboard, FLM-101B achieved an average score of 43.94, which is also shown in Table 4 under the Average column for FLM-101B."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The document states that the processor clock speed of typical 2021 microprocessors represents a more than 6,750‑fold improvement over the Intel 4004 from 1971.","6750","fold","[""wu2021b""]","is_blank","is_blank","The document states that the processor clock speed of typical 2021 microprocessors represents a more than 6,750‑fold improvement over the Intel 4004 from 1971."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which corresponds to 14.8 days when using 10,000 GPUs.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which corresponds to 14.8 days when using 10,000 GPUs."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","Table 3 (and its full version Table 4) from the Morrison 2025 paper lists the GPU Power Usage for Llama 3.2 1B at an 8 requests/second rate as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","Table 3 (and its full version Table 4) from the Morrison 2025 paper lists the GPU Power Usage for Llama 3.2 1B at an 8 requests/second rate as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The document states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","is_blank","The document states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that training FLM-101B with the growth schedule takes 21.54 days, which is a 72% time‑saving compared to training a 101B model from scratch (76.74 days).","72","percent","[""li2025a""]","is_blank","is_blank","The paper states that training FLM-101B with the growth schedule takes 21.54 days, which is a 72% time‑saving compared to training a 101B model from scratch (76.74 days)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The excerpts provide an estimate of about 1,300 premature deaths in 2028 from U.S. data centers, but they do not give any figure for 2030 or isolate scope‑2 pollutants specifically for that year.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The excerpts provide an estimate of about 1,300 premature deaths in 2028 from U.S. data centers, but they do not give any figure for 2030 or isolate scope‑2 pollutants specifically for that year."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The Dodge et al. (2022) snippet states that the 6.1 billion‑parameter transformer was trained for 8 days (13% of full training) and that a full training run would take 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","The Dodge et al. (2022) snippet states that the 6.1 billion‑parameter transformer was trained for 8 days (13% of full training) and that a full training run would take 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The prefacing text of the report states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, indicating the launch year.","2014","year","[""stone2022""]","is_blank","is_blank","The prefacing text of the report states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, indicating the launch year."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The provided excerpts state that the UN’s Global E‑Waste Monitor 2024 reported that about 22% of e‑waste has been formally collected and recycled.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The provided excerpts state that the UN’s Global E‑Waste Monitor 2024 reported that about 22% of e‑waste has been formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides the training energy for the BLOOM model as 51,686 kWh, which converts to 51.686 MWh.","51.686","MWh","[""ebert2024"", ""luccioni2024""]","is_blank","is_blank","The context provides the training energy for the BLOOM model as 51,686 kWh, which converts to 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The paper’s hyperparameter table lists Nexperts = 8, and the text states that the same number of experts (8) is set for every layer.","8","experts","[""shen2024""]","is_blank","is_blank","The paper’s hyperparameter table lists Nexperts = 8, and the text states that the same number of experts (8) is set for every layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","is_blank","is_blank","second","[""is_blank""]","is_blank","is_blank","is_blank"
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The document states that the total training cost of FLM-101B is 52.76 zettaFLOPs, broken down into 28.22 zettaFLOPs for the English portion and 24.54 zettaFLOPs for Chinese.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The document states that the total training cost of FLM-101B is 52.76 zettaFLOPs, broken down into 28.22 zettaFLOPs for the English portion and 24.54 zettaFLOPs for Chinese."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for the energy-efficient local inference on financial sentiment classification reports using an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","Intel Core i7-1165G7","is_blank","[""khan2025""]","is_blank","is_blank","The experimental setup for the energy-efficient local inference on financial sentiment classification reports using an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The provided excerpts do not mention how many machine learning models were sampled and analyzed in the 2024 Power Hungry Processing study.","is_blank","models","[""is_blank""]","is_blank","is_blank","The provided excerpts do not mention how many machine learning models were sampled and analyzed in the 2024 Power Hungry Processing study."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The document states that increasing GPU utilization up to 80% reduces the overall carbon footprint by three times.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that increasing GPU utilization up to 80% reduces the overall carbon footprint by three times."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The paper states that they sampled a multiplicative factor from a log‑normal distribution with a 90% credible interval of 1.2× to 4×, meaning total compute for model development is 1.2 to 4 times larger than the final training run.","1.2x to 4x","multiplier","[""cottier2024""]","is_blank","is_blank","The paper states that they sampled a multiplicative factor from a log‑normal distribution with a 90% credible interval of 1.2× to 4×, meaning total compute for model development is 1.2 to 4 times larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The document reports that a 6.1 billion‑parameter transformer used 13.8 MWh for 8 days of training (13% of full training). Scaling to the full 60‑day run gives (60/8) × 13.8 ≈ 103.5 MWh, which is also stated as about 103,500 kWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The document reports that a 6.1 billion‑parameter transformer used 13.8 MWh for 8 days of training (13% of full training). Scaling to the full 60‑day run gives (60/8) × 13.8 ≈ 103.5 MWh, which is also stated as about 103,500 kWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper argues that efficiency gains can lead to increased overall consumption because of Jevons’ Paradox, which creates rebound effects that offset environmental benefits.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The paper argues that efficiency gains can lead to increased overall consumption because of Jevons’ Paradox, which creates rebound effects that offset environmental benefits."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The paper states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The paper states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The document states that Red AI is on the rise despite the well‑known diminishing returns of increased cost, indicating it is not declining.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The document states that Red AI is on the rise despite the well‑known diminishing returns of increased cost, indicating it is not declining."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The document states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","is_blank","The document states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context repeatedly states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, indicating that most workflows are far below 80% utilization.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context repeatedly states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, indicating that most workflows are far below 80% utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","Both documents report that GPT‑3’s training consumed an estimated 1,287 MWh of electricity.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","Both documents report that GPT‑3’s training consumed an estimated 1,287 MWh of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context states that the AI Energy Score project was created to establish a unified approach for comparing the inference efficiency of AI models, which matches the description of a collaborative project for standardized comparison.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the AI Energy Score project was created to establish a unified approach for comparing the inference efficiency of AI models, which matches the description of a collaborative project for standardized comparison."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context includes figures and discussion of MoE kernel-level breakdown (Fig. 6) but does not provide explicit numeric values in microseconds for the longest kernel of the dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context includes figures and discussion of MoE kernel-level breakdown (Fig. 6) but does not provide explicit numeric values in microseconds for the longest kernel of the dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The hyperparameter table and accompanying description state that the model uses 8 experts per layer and selects the top‑k = 2 experts for each token.","2","experts","[""shen2024""]","is_blank","is_blank","The hyperparameter table and accompanying description state that the model uses 8 experts per layer and selects the top‑k = 2 experts for each token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that compute used to train deep learning models increased 300,000× from 2012 to 2018, not 200,000×, so the specific numeric claim is incorrect.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context states that compute used to train deep learning models increased 300,000× from 2012 to 2018, not 200,000×, so the specific numeric claim is incorrect."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The document states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The document states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The 2023 Amazon Sustainability Report states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources.","22","data centers","[""amazon2023""]","is_blank","is_blank","The 2023 Amazon Sustainability Report states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The documents state that AI was officially born at a 1956 workshop and note that the field was about sixty years old as of the 2016 report. Counting from 1956 to 2025 gives roughly 69–70 years, so the field is approximately 70 years old in 2025.","70","years","[""stone2022""]","is_blank","is_blank","The documents state that AI was officially born at a 1956 workshop and note that the field was about sixty years old as of the 2016 report. Counting from 1956 to 2025 gives roughly 69–70 years, so the field is approximately 70 years old in 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context explains that the five‑cars estimate comes from the energy needed to run a neural architecture search (NAS) workload, which is a large‑scale and rarely performed AI process.","Neural Architecture Search","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context explains that the five‑cars estimate comes from the energy needed to run a neural architecture search (NAS) workload, which is a large‑scale and rarely performed AI process."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The text states that targeting an average TPOT of 100 ms lands on a point where the average TPOT is 77 ms, which reduces energy consumption per generation by 44% compared to the latency‑minimizing configuration for Llama 3.1 8B.","44","percent","[""chung2025""]","is_blank","is_blank","The text states that targeting an average TPOT of 100 ms lands on a point where the average TPOT is 77 ms, which reduces energy consumption per generation by 44% compared to the latency‑minimizing configuration for Llama 3.1 8B."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","Table III only provides maximum batch sizes for the CS and MATH datasets; GSM8K (GS) is not listed. Additionally, the text states that BlackMamba is inadequate for fine-tuning GS, so no batch size is reported.","is_blank","samples","[""is_blank""]","is_blank","is_blank","Table III only provides maximum batch sizes for the CS and MATH datasets; GSM8K (GS) is not listed. Additionally, the text states that BlackMamba is inadequate for fine-tuning GS, so no batch size is reported."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The Amazon 2023 sustainability report states that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The Amazon 2023 sustainability report states that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The document reports that GPT-4o’s projected annual carbon emissions (138,125–163,441 tons CO2e) are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The document reports that GPT-4o’s projected annual carbon emissions (138,125–163,441 tons CO2e) are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context does not contain any data on the percentage of AI inference workloads in Asia that were powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any data on the percentage of AI inference workloads in Asia that were powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","is_blank","is_blank","PUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The document states that JetMoE-8B contains 8 billion parameters but only activates 2 billion of them for each input token during inference.","2B","parameters","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B contains 8 billion parameters but only activates 2 billion of them for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The table of pre‑training environmental impacts lists the Llama 7B model with a GHG emissions value of 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The table of pre‑training environmental impacts lists the Llama 7B model with a GHG emissions value of 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","is_blank","is_blank","electric delivery vans","[""is_blank""]","is_blank","is_blank","is_blank"
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from servers and data center cooling, contradicting the claim that they do not include such emissions.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from servers and data center cooling, contradicting the claim that they do not include such emissions."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The table of energy consumption lists the o3 model's energy use for three prompt sizes. For the long prompt (10k input‑1.5k output), the o3 model consumes 12.222 Wh (mean ± std dev).","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The table of energy consumption lists the o3 model's energy use for three prompt sizes. For the long prompt (10k input‑1.5k output), the o3 model consumes 12.222 Wh (mean ± std dev)."
"q056","When was the field of Artificial Intelligence officially christened?","The provided excerpts state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, indicating the year it was named.","1956","year","[""stone2022""]","is_blank","is_blank","The provided excerpts state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, indicating the year it was named."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The provided context does not contain any information about Google's AI-dedicated data centers or their water use effectiveness (WUE) for 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about Google's AI-dedicated data centers or their water use effectiveness (WUE) for 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, about one‑tenth of the world’s population, do not have access to a stable supply of electricity.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context explicitly states that approximately 770 million people, about one‑tenth of the world’s population, do not have access to a stable supply of electricity."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The document states that at a maximum generation length of 512 tokens, LLaMA‑65B consumes roughly 3‑4 Joules for each output token, and this value is similar for length 1024.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","The document states that at a maximum generation length of 512 tokens, LLaMA‑65B consumes roughly 3‑4 Joules for each output token, and this value is similar for length 1024."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The document states that converting the RM2 model from 32-bit floating-point to 16-bit reduces its overall size by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The document states that converting the RM2 model from 32-bit floating-point to 16-bit reduces its overall size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context traces the 5‑10% reduction claim to BCG reports but states that the reasoning is unclear, calculations are not detailed, and the estimate lacks scientific grounding and rigorous methodology.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context traces the 5‑10% reduction claim to BCG reports but states that the reasoning is unclear, calculations are not detailed, and the estimate lacks scientific grounding and rigorous methodology."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The document states that Grover was trained on 256 TPU chips for two weeks with an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The document states that Grover was trained on 256 TPU chips for two weeks with an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The paper reports that in BlackMamba sparse fine‑tuning on an NVIDIA A40 GPU (48 GB) with batch size = 1, the optimizer stage consumes up to 53% of the total running time.","53","percent","[""xia2024""]","is_blank","is_blank","The paper reports that in BlackMamba sparse fine‑tuning on an NVIDIA A40 GPU (48 GB) with batch size = 1, the optimizer stage consumes up to 53% of the total running time."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","is_blank","is_blank","MWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q067","What was the average global data center PUE in 2023?","The document ebert2024 states that the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The document ebert2024 states that the average data center PUE in 2023 was 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","is_blank","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","is_blank"
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The document states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, indicating that 49% of its total model development cost is attributed to R&D staff (including equity).","49","percent","[""cottier2024""]","is_blank","is_blank","The document states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, indicating that 49% of its total model development cost is attributed to R&D staff (including equity)."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The document states that the inaugural 2015 Study Panel was a seventeen‑member panel, indicating it had 17 members.","17","people","[""stone2022""]","is_blank","is_blank","The document states that the inaugural 2015 Study Panel was a seventeen‑member panel, indicating it had 17 members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The document states that manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices.","74","percent","[""wu2021a""]","is_blank","is_blank","The document states that manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context states that while models with more parameters generally consume more energy, this is not always the case, citing the Phi‑3 Mini (3.8B) vs. Phi‑3 Small (7B) example where the larger model can use less energy as batch size grows.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that while models with more parameters generally consume more energy, this is not always the case, citing the Phi‑3 Mini (3.8B) vs. Phi‑3 Small (7B) example where the larger model can use less energy as batch size grows."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The report states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating they are not concerned about such a threat.","0","is_blank","[""stone2022""]","is_blank","is_blank","The report states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating they are not concerned about such a threat."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reported the Llama 3 family emitted 11,390 tons CO2e, which is more than 40 times the emissions represented by the “five cars” estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context states that Meta reported the Llama 3 family emitted 11,390 tons CO2e, which is more than 40 times the emissions represented by the “five cars” estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The document states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5‑year period (Yr1‑Q1 to Yr2‑Q2).","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5‑year period (Yr1‑Q1 to Yr2‑Q2)."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context states that AlphaGo, a program by Google DeepMind, beat the human Go champion in a five-game match, confirming that it defeated the champion.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context states that AlphaGo, a program by Google DeepMind, beat the human Go champion in a five-game match, confirming that it defeated the champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The document describes a batching approach called ""Continuous Batching"" that mitigates GPU under‑utilization by dynamically replacing completed requests with new ones, thereby reducing idle GPU time.","Continuous Batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The document describes a batching approach called ""Continuous Batching"" that mitigates GPU under‑utilization by dynamically replacing completed requests with new ones, thereby reducing idle GPU time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The document states that after describing the dSFT and dDPO fine‑tuning steps, ""The entire alignment process takes 60 H100 GPU hours,"" indicating the total compute required for both stages.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The document states that after describing the dSFT and dDPO fine‑tuning steps, ""The entire alignment process takes 60 H100 GPU hours,"" indicating the total compute required for both stages."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The offline 100 TPS results show InferSave's selected instance costing $2.13, while Max-Performance's instance costs $2.699, which the text describes as an increase of about 26.7%.","26.7","percent","[""kim2025""]","is_blank","is_blank","The offline 100 TPS results show InferSave's selected instance costing $2.13, while Max-Performance's instance costs $2.699, which the text describes as an increase of about 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The Luccioni et al. (2024) study states that the most carbon‑intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO₂eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The Luccioni et al. (2024) study states that the most carbon‑intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO₂eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The Luccioni 2025 study reports that GPU energy usage for 1,000 inference queries varies from as low as 0.06 Wh for the bert‑tiny model up to more than 3,426 Wh for the Command‑R Plus model, as stated in the appendix tables.","0.06–3426","Wh","[""luccioni2025c""]","is_blank","is_blank","The Luccioni 2025 study reports that GPU energy usage for 1,000 inference queries varies from as low as 0.06 Wh for the bert‑tiny model up to more than 3,426 Wh for the Command‑R Plus model, as stated in the appendix tables."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The authors explicitly state that there is no one-size-fits-all solution for ethics or sustainability, indicating they do not believe a universal approach can be developed.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The authors explicitly state that there is no one-size-fits-all solution for ethics or sustainability, indicating they do not believe a universal approach can be developed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","is_blank","is_blank","kg of CO2e/KWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The document describes Hivemind as a PyTorch-based framework that runs in a decentralized fashion and is used to enable distributed spot instance training across multiple clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The document describes Hivemind as a PyTorch-based framework that runs in a decentralized fashion and is used to enable distributed spot instance training across multiple clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The documents state that the notion of transparency can be expanded to include socio‑technical aspects and the system's societal and environmental footprint, and they name this expanded concept ""social transparency"".","social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The documents state that the notion of transparency can be expanded to include socio‑technical aspects and the system's societal and environmental footprint, and they name this expanded concept ""social transparency""."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The document states that the highest accuracy in the experiments was achieved by a traditional linear model that uses pre‑trained sentence embeddings.","traditional linear model with pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","is_blank","The document states that the highest accuracy in the experiments was achieved by a traditional linear model that uses pre‑trained sentence embeddings."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The Chen et al. paper describes developing and deploying an LLM inference system named Lamina that incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The Chen et al. paper describes developing and deploying an LLM inference system named Lamina that incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q094","What is the total number of parameters in the JetMoE-8B model?","The document states that JetMoE-8B has 8B total parameters (Ptotal = 8B) and explicitly mentions ""JetMoE-8B has 8B parameters"".","8B","parameters","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B has 8B total parameters (Ptotal = 8B) and explicitly mentions ""JetMoE-8B has 8B parameters""."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","Both the Li et al. (2025) paper and the Luccioni et al. (2025) report state that Google's data center water consumption rose by about 20% between 2021 and 2022.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","Both the Li et al. (2025) paper and the Luccioni et al. (2025) report state that Google's data center water consumption rose by about 20% between 2021 and 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","Table I in the Khan et al. (2025) document lists the metric 'Carbon Intensity' with the definition 'CO₂ emissions per unit of electricity consumed', directly matching the description.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","Table I in the Khan et al. (2025) document lists the metric 'Carbon Intensity' with the definition 'CO₂ emissions per unit of electricity consumed', directly matching the description."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","Table 2 lists the FLOPs utilization for each growth stage. The row for the 101B stage shows a utilization of 52.88%.","52.88%","percent","[""li2025a""]","is_blank","is_blank","Table 2 lists the FLOPs utilization for each growth stage. The row for the 101B stage shows a utilization of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The document cottier2024 states that GPT-4’s largest amortized hardware and energy cost is $40 M, which is the estimated amortized training cost.","40M","USD","[""cottier2024""]","is_blank","is_blank","The document cottier2024 states that GPT-4’s largest amortized hardware and energy cost is $40 M, which is the estimated amortized training cost."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The paper reports that full-stack optimization (platform-level caching, GPU acceleration, and algorithmic changes) reduces the operational carbon footprint of the Transformer-based universal translation model by about 810 times compared to a CPU baseline.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The paper reports that full-stack optimization (platform-level caching, GPU acceleration, and algorithmic changes) reduces the operational carbon footprint of the Transformer-based universal translation model by about 810 times compared to a CPU baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The text states that training NLP across four continents (C-8) incurs a 41% performance drop compared to the fully local run (A-8), meaning the achieved throughput is 100% - 41% = 59% of the local throughput.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The text states that training NLP across four continents (C-8) incurs a 41% performance drop compared to the fully local run (A-8), meaning the achieved throughput is 100% - 41% = 59% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The report states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3500000000","liters","[""amazon2023""]","is_blank","is_blank","The report states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The study reports that custom tags reduce the energy consumption of LLMs for zero-shot, one-shot, and few-shot prompt techniques in code completion tasks, with specific percentage reductions shown in the experimental results.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The study reports that custom tags reduce the energy consumption of LLMs for zero-shot, one-shot, and few-shot prompt techniques in code completion tasks, with specific percentage reductions shown in the experimental results."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The 2025 paper by Luccioni et al. states that NVIDIA shipped 3.7 million data‑center GPUs in 2024, which is over a million more than in 2023.","3.7 million","GPUs","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni et al. states that NVIDIA shipped 3.7 million data‑center GPUs in 2024, which is over a million more than in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The document breaks down the amortized hardware CapEx plus energy cost and states that, on average, 44% of this total cost is allocated to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","The document breaks down the amortized hardware CapEx plus energy cost and states that, on average, 44% of this total cost is allocated to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The provided documents state that Facebook’s data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, indicating high energy efficiency.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The provided documents state that Facebook’s data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, indicating high energy efficiency."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context states that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The document states that the most expensive publicly‑announced training runs are GPT‑4 at $40M and Google’s Gemini Ultra at $30M, providing the estimated amortized training cost for Gemini Ultra.","30M","USD","[""cottier2024""]","is_blank","is_blank","The document states that the most expensive publicly‑announced training runs are GPT‑4 at $40M and Google’s Gemini Ultra at $30M, providing the estimated amortized training cost for Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment for providers of GPAI models with systemic risk and that, within the Act, these risks must be interpreted as including environmental risks.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Act mandates risk assessment for providers of GPAI models with systemic risk and that, within the Act, these risks must be interpreted as including environmental risks."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The provided excerpts state that the EPA’s recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³, confirming the value of the standard.","9","µg/m³","[""han2024""]","is_blank","is_blank","The provided excerpts state that the EPA’s recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³, confirming the value of the standard."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The life cycle assessment (LCA) comparison cited in the document states that 115 printed books would emit the same amount of CO2 as one Amazon Kindle e‑reader.","115","books","[""luccioni2025a""]","is_blank","is_blank","The life cycle assessment (LCA) comparison cited in the document states that 115 printed books would emit the same amount of CO2 as one Amazon Kindle e‑reader."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The study states that in disadvantaged communities the per‑household health burden from air pollutants could be about 200 times higher than in less‑impacted communities.","200","multiplier","[""han2024""]","is_blank","is_blank","The study states that in disadvantaged communities the per‑household health burden from air pollutants could be about 200 times higher than in less‑impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","Table B1 in the zschache2025 appendix lists the energy consumed (Wh) for each model on the FKTG dataset. For DS Llama 70B on a single node, the reported energy consumption is 6792.54 Wh.","6792.54","Wh","[""zschache2025""]","is_blank","is_blank","Table B1 in the zschache2025 appendix lists the energy consumed (Wh) for each model on the FKTG dataset. For DS Llama 70B on a single node, the reported energy consumption is 6792.54 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The provided context does not contain any mention of Dodge et al. (2022) or the parameter count of the large language model they analyzed.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The provided context does not contain any mention of Dodge et al. (2022) or the parameter count of the large language model they analyzed."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The documents describe that efficiency gains can lead to higher overall consumption, citing this effect as Jevons Paradox.","Jevons Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The documents describe that efficiency gains can lead to higher overall consumption, citing this effect as Jevons Paradox."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","is_blank","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","is_blank"
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","Table 2 in the study reports the mean energy consumption per 1,000 inferences for each task. For image generation, the mean value is listed as 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","Table 2 in the study reports the mean energy consumption per 1,000 inferences for each task. For image generation, the mean value is listed as 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","Table 1 in Strubell et al. (2019) lists the estimated CO₂e emissions for various activities, including ""American life, avg, 1 year"" as 36,156 pounds.","36156","lbs","[""strubell2019""]","is_blank","is_blank","Table 1 in Strubell et al. (2019) lists the estimated CO₂e emissions for various activities, including ""American life, avg, 1 year"" as 36,156 pounds."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","Table III lists Mistral-small's carbon emissions as 0.020 kg before optimization and 0.015 kg after optimization. Dividing the after value by the before value (0.015 / 0.020) gives a multiplier of 0.75, meaning emissions are 75% of the original level.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","Table III lists Mistral-small's carbon emissions as 0.020 kg before optimization and 0.015 kg after optimization. Dividing the after value by the before value (0.015 / 0.020) gives a multiplier of 0.75, meaning emissions are 75% of the original level."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context states that the BLOOMz-7B model required 51,686 kWh for training and an additional 7,571 kWh for fine‑tuning. Adding these gives the combined energy cost.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","is_blank","The context states that the BLOOMz-7B model required 51,686 kWh for training and an additional 7,571 kWh for fine‑tuning. Adding these gives the combined energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The context states that the FLM-101B model is produced with 101 B parameters, as shown in Table 2 and the Model Sizes description.","101B","parameters","[""li2025a""]","is_blank","is_blank","The context states that the FLM-101B model is produced with 101 B parameters, as shown in Table 2 and the Model Sizes description."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The Dodge et al. study reports that a full training run of a 6.1 B‑parameter transformer would consume approximately 103,593 kWh. The Luccioni et al. table gives the per‑inference energy cost for BLOOMz‑7B as 1.0 × 10⁻⁴ kWh, so dividing the training energy by this value yields roughly 1.0 × 10⁹ inferences needed to equal the training energy.","103600 kWh; 1.0e9","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","The Dodge et al. study reports that a full training run of a 6.1 B‑parameter transformer would consume approximately 103,593 kWh. The Luccioni et al. table gives the per‑inference energy cost for BLOOMz‑7B as 1.0 × 10⁻⁴ kWh, so dividing the training energy by this value yields roughly 1.0 × 10⁹ inferences needed to equal the training energy."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The study explicitly states that the total energy used for all model experimentation and evaluation was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The study explicitly states that the total energy used for all model experimentation and evaluation was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","Table 5 lists the cost parity for BLOOMz-7B as 592,570,000 inferences, which is the number of inferences needed for the cumulative deployment energy to match the combined training and fine‑tuning energy.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","Table 5 lists the cost parity for BLOOMz-7B as 592,570,000 inferences, which is the number of inferences needed for the cumulative deployment energy to match the combined training and fine‑tuning energy."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context states that the statements from the population were published as the FKTG-dataset, which is the dataset used for the German nuclear waste site objection texts in the experiments.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that the statements from the population were published as the FKTG-dataset, which is the dataset used for the German nuclear waste site objection texts in the experiments."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","is_blank","is_blank","liters","[""is_blank""]","is_blank","is_blank","is_blank"
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The documents describe the amount of rare earth metal in an H100 GPU and its environmental impact, but they do not mention any proportion of GPUs that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The documents describe the amount of rare earth metal in an H100 GPU and its environmental impact, but they do not mention any proportion of GPUs that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","Table 1 shows that a round‑trip flight between New York and San Francisco for one passenger emits 1,984 lbs of CO₂e. 3.2 tCO₂e equals 3.2 × 2,000 = 6,400 lbs, and 6,400 ÷ 1,984 ≈ 3.2, i.e., roughly three passengers.","3","passengers","[""strubell2019""]","is_blank","is_blank","Table 1 shows that a round‑trip flight between New York and San Francisco for one passenger emits 1,984 lbs of CO₂e. 3.2 tCO₂e equals 3.2 × 2,000 = 6,400 lbs, and 6,400 ÷ 1,984 ≈ 3.2, i.e., roughly three passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The May 2025 OpenRouter data reports that 84% of token usage was through models with no environmental impact disclosure.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The May 2025 OpenRouter data reports that 84% of token usage was through models with no environmental impact disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","Table II lists the bare minimum hardware required for each LLaMA variant. For the 13B model, it shows that only 1 A100 GPU with 80 GB memory is needed.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","Table II lists the bare minimum hardware required for each LLaMA variant. For the 13B model, it shows that only 1 A100 GPU with 80 GB memory is needed."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The provided context gives the estimated energy consumption (≈103,500 kWh) for a full training run of the 6.1 billion‑parameter transformer, but it does not provide a conversion to CO₂ emissions or any emission figures in metric tons, so the required range cannot be derived from the given documents.","is_blank","metric tons","[""is_blank""]","is_blank","is_blank","The provided context gives the estimated energy consumption (≈103,500 kWh) for a full training run of the 6.1 billion‑parameter transformer, but it does not provide a conversion to CO₂ emissions or any emission figures in metric tons, so the required range cannot be derived from the given documents."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The documents describe percentage reductions (e.g., up to 45% reduction after quantization) but do not provide a specific total amount of carbon emissions avoided in tCO2e for pruning and quantizing LLMs in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The documents describe percentage reductions (e.g., up to 45% reduction after quantization) but do not provide a specific total amount of carbon emissions avoided in tCO2e for pruning and quantizing LLMs in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The document describes a hybrid GPU configuration of 2 A100s and 1 A10G that achieves a 24% cost reduction compared to using only A100 GPUs.","24","percent","[""griggs2024""]","is_blank","is_blank","The document describes a hybrid GPU configuration of 2 A100s and 1 A10G that achieves a 24% cost reduction compared to using only A100 GPUs."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The table from Chen et al. lists the price per chip for the NVIDIA H20 as $4.63 per hour.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The table from Chen et al. lists the price per chip for the NVIDIA H20 as $4.63 per hour."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather information manually by writing to authors, indicating they do not gather it automatically.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that most carbon footprint analyses gather information manually by writing to authors, indicating they do not gather it automatically."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The document states that in 2023 the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","The document states that in 2023 the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","Table II lists the bare minimum hardware required for each LLaMA variant assuming no compression or quantization. For the 7B model, it shows a count of 1 A100 80GB GPU.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","Table II lists the bare minimum hardware required for each LLaMA variant assuming no compression or quantization. For the 7B model, it shows a count of 1 A100 80GB GPU."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The paper by Khan et al. states that experimental results show the methods can reduce energy consumption and carbon emissions by up to 45% after quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The paper by Khan et al. states that experimental results show the methods can reduce energy consumption and carbon emissions by up to 45% after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The documents state that after contacting over 500 authors, the researchers were only able to collect 95 answers for their carbon footprint analysis.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The documents state that after contacting over 500 authors, the researchers were only able to collect 95 answers for their carbon footprint analysis."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE report states a training budget of $100k and total usage of 30,000 H100 GPU hours; dividing the budget by the hours gives the cost per GPU hour.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The JetMoE report states a training budget of $100k and total usage of 30,000 H100 GPU hours; dividing the budget by the hours gives the cost per GPU hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","Table 2 lists the health cost as a percentage of electricity cost for each location. For Altoona, IA, the table shows a value of 122%.","122","percent","[""han2024""]","is_blank","is_blank","Table 2 lists the health cost as a percentage of electricity cost for each location. For Altoona, IA, the table shows a value of 122%."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The document states that JetMoE-8B was trained on 1.25 trillion tokens, which is the total amount of data used for pre‑training.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B was trained on 1.25 trillion tokens, which is the total amount of data used for pre‑training."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The table of Amazon Renewable Energy Projects lists the number of projects by country and notes that the figures are for projects announced as of January 2024. For the United Kingdom, the table shows 36 projects.","36","projects","[""amazon2023""]","is_blank","is_blank","The table of Amazon Renewable Energy Projects lists the number of projects by country and notes that the figures are for projects announced as of January 2024. For the United Kingdom, the table shows 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The Amazon 2023 Sustainability Report includes a table titled ""Amazon Workforce (All Levels)"" that lists gender percentages for Global and U.S. employees. For the U.S. column, the percentage of employees identifying as men is shown as 44.1%.","44.1","percent","[""amazon2023""]","is_blank","is_blank","The Amazon 2023 Sustainability Report includes a table titled ""Amazon Workforce (All Levels)"" that lists gender percentages for Global and U.S. employees. For the U.S. column, the percentage of employees identifying as men is shown as 44.1%."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The document li2025b states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The document li2025b states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The paper states that they introduce the granularity metric, defined as the ratio of calculation to communication time, to compare model scalability and predict performance when training across multiple continents.","granularity","is_blank","[""erben2023""]","is_blank","is_blank","The paper states that they introduce the granularity metric, defined as the ratio of calculation to communication time, to compare model scalability and predict performance when training across multiple continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The coalition of Microsoft employees estimated that the Exxon Mobil deal could add up to 640 % more carbon emissions than Microsoft’s yearly carbon removal targets, which corresponds to 6.4 times the target.","6.4","times","[""luccioni2025a""]","is_blank","is_blank","The coalition of Microsoft employees estimated that the Exxon Mobil deal could add up to 640 % more carbon emissions than Microsoft’s yearly carbon removal targets, which corresponds to 6.4 times the target."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines ""Water withdrawal"" as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context defines ""Water withdrawal"" as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","is_blank","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The document cites Deloitte 2021 data stating that the average U.S. household has about 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc.","25","devices","[""wu2021b""]","is_blank","is_blank","The document cites Deloitte 2021 data stating that the average U.S. household has about 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The cited source reports that publicly disclosed pre‑training energy use for LLMs varies from a low of 0.8 MWh for OLMo 20M up to a high of 3,500 MWh for LLaMa 4 Scout.","0.8-3500 MWh","MWh","[""luccioni2025c""]","is_blank","is_blank","The cited source reports that publicly disclosed pre‑training energy use for LLMs varies from a low of 0.8 MWh for OLMo 20M up to a high of 3,500 MWh for LLaMa 4 Scout."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011, so the claim that it did NOT beat them is false.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011, so the claim that it did NOT beat them is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The document states that a paper suggests that 10–50 queries on GPT-3 consume around half a liter of water.","10–50","queries","[""luccioni2025a""]","is_blank","is_blank","The document states that a paper suggests that 10–50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The provided table lists MT-Bench scores for several models, showing JetMoE-8B-Chat with a score of 6.681, which is higher than Llama-2-13b-Chat's 6.650 after alignment.","6.681","score","[""shen2024""]","is_blank","is_blank","The provided table lists MT-Bench scores for several models, showing JetMoE-8B-Chat with a score of 6.681, which is higher than Llama-2-13b-Chat's 6.650 after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The document states that GPT-3 consumes a 500 mL bottle of water for roughly 10–50 medium‑length responses, which directly answers how many such completions can be produced with that amount of water.","10-50","responses","[""li2025b""]","is_blank","is_blank","The document states that GPT-3 consumes a 500 mL bottle of water for roughly 10–50 medium‑length responses, which directly answers how many such completions can be produced with that amount of water."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The paper states that compared to using a single GPU type, Mélange reduces deployment costs by up to 77% in conversational (short‑context/interactive chat) settings.","77","percent","[""griggs2024""]","is_blank","is_blank","The paper states that compared to using a single GPU type, Mélange reduces deployment costs by up to 77% in conversational (short‑context/interactive chat) settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The documents state that the minimum hardware for LLaMA‑65B inference without any compression is either 8 V100 GPUs or 4 A100 GPUs with 80 GB memory, indicating that 4 A100 80GB GPUs are the bare minimum required.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","is_blank","The documents state that the minimum hardware for LLaMA‑65B inference without any compression is either 8 V100 GPUs or 4 A100 GPUs with 80 GB memory, indicating that 4 A100 80GB GPUs are the bare minimum required."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The documents state that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024""]","is_blank","is_blank","The documents state that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The provided context does not contain any statement about NVIDIA's 2019 estimate of the percentage of ML workload that is inference processing.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any statement about NVIDIA's 2019 estimate of the percentage of ML workload that is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The ethical considerations statement reports that the entire study used 754.66 kWh of energy and emitted 178.97 kg of CO₂eq, which represents the total emissions for the whole study.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The ethical considerations statement reports that the entire study used 754.66 kWh of energy and emitted 178.97 kg of CO₂eq, which represents the total emissions for the whole study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimations using a GPU's TDP are nearly always an overestimation and can be inaccurate, indicating that TDP-based estimation is not reliable or accurate.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that estimations using a GPU's TDP are nearly always an overestimation and can be inaccurate, indicating that TDP-based estimation is not reliable or accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy (3.098 Wh) than GPT-4o (2.875 Wh) and even about 20% more energy on long queries, indicating it uses more, not less, energy per query.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o mini consumes slightly more energy (3.098 Wh) than GPT-4o (2.875 Wh) and even about 20% more energy on long queries, indicating it uses more, not less, energy per query."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","is_blank","is_blank","queries/sec","[""is_blank""]","is_blank","is_blank","is_blank"
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The paper states that direct environmental disclosures peaked in 2022 and then declined, indicating the trend did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The paper states that direct environmental disclosures peaked in 2022 and then declined, indicating the trend did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The Griggs et al. (2024) table lists the on-demand price for each GPU type, showing H100 (SXM) at $7.5164 per hour, which is the normalized hourly price used in the evaluation.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The Griggs et al. (2024) table lists the on-demand price for each GPU type, showing H100 (SXM) at $7.5164 per hour, which is the normalized hourly price used in the evaluation."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","is_blank","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama-2-70B with 2 A100 GPUs costs over $5,200 per month. Dividing this monthly cost by the number of hours in a 30‑day month (30 days × 24 hours = 720 hours) gives an estimated hourly cost.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context states that serving Llama-2-70B with 2 A100 GPUs costs over $5,200 per month. Dividing this monthly cost by the number of hours in a 30‑day month (30 days × 24 hours = 720 hours) gives an estimated hourly cost."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The document states that increasing the BLEU score from 5 to 40 for GPT-3 language translation requires a model that is 1,000 times larger.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that increasing the BLEU score from 5 to 40 for GPT-3 language translation requires a model that is 1,000 times larger."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The provided context gives the CO2 emissions for training a Transformer model with neural architecture search (626,155 lbs) but does not include the emissions-to-driving-distance ratio needed to convert this amount to an equivalent driving distance in miles.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context gives the CO2 emissions for training a Transformer model with neural architecture search (626,155 lbs) but does not include the emissions-to-driving-distance ratio needed to convert this amount to an equivalent driving distance in miles."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that BLOOMz-7B was downloaded 606,096 times and that each inference consumes 1.0 × 10⁻⁴ kWh. Multiplying the downloads by 1 million inferences each gives 606,096 × 10⁶ inferences; multiplying by the per‑inference energy yields 60,609,600 kWh, which equals 60,609.6 MWh.","60609.6","MWh","[""luccioni2024"", ""luccioni2024""]","is_blank","is_blank","The context states that BLOOMz-7B was downloaded 606,096 times and that each inference consumes 1.0 × 10⁻⁴ kWh. Multiplying the downloads by 1 million inferences each gives 606,096 × 10⁶ inferences; multiplying by the per‑inference energy yields 60,609,600 kWh, which equals 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The document states that JetMoE-8B was trained using 30,000 H100 GPU hours, which directly answers the question about GPU hours consumed during pre‑training.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B was trained using 30,000 H100 GPU hours, which directly answers the question about GPU hours consumed during pre‑training."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The paper states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The paper states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","is_blank","is_blank","FLOPS","[""is_blank""]","is_blank","is_blank","is_blank"
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The documents state that at least eight NVIDIA V100 GPUs with 32 GB of memory are needed to run inference for the 65B LLaMA model when no compression or quantization is applied.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The documents state that at least eight NVIDIA V100 GPUs with 32 GB of memory are needed to run inference for the 65B LLaMA model when no compression or quantization is applied."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","is_blank","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","is_blank"
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The snippets discuss AlexNet's relative accuracy compared to other models but do not provide a specific top-1 accuracy percentage for AlexNet on ImageNet.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The snippets discuss AlexNet's relative accuracy compared to other models but do not provide a specific top-1 accuracy percentage for AlexNet on ImageNet."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The text states that FLM-101B was trained on a cluster of 24 DGX‑A800 servers, each containing 8 GPUs, which totals 24 × 8 = 192 GPUs. Table 2 also lists the total number of GPUs as 192.","192","GPUs","[""li2025a""]","is_blank","is_blank","The text states that FLM-101B was trained on a cluster of 24 DGX‑A800 servers, each containing 8 GPUs, which totals 24 × 8 = 192 GPUs. Table 2 also lists the total number of GPUs as 192."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The provided context gives the estimated CO2 emissions for NAS training (626,155 lb or 284 metric tons) and compares it to the lifetime emissions of five cars, but it does not state how many average American lifetimes this amount corresponds to.","is_blank","lifetimes","[""is_blank""]","is_blank","is_blank","The provided context gives the estimated CO2 emissions for NAS training (626,155 lb or 284 metric tons) and compares it to the lifetime emissions of five cars, but it does not state how many average American lifetimes this amount corresponds to."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The provided context states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The provided context states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The Amazon 2023 sustainability report states that the on‑site solar energy systems generate about 123,000 MWh annually and avoid roughly 47,400 metric tons of CO₂e each year compared with non‑renewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","is_blank","The Amazon 2023 sustainability report states that the on‑site solar energy systems generate about 123,000 MWh annually and avoid roughly 47,400 metric tons of CO₂e each year compared with non‑renewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The snippet states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations, indicating vllm was the framework used for deployment across multiple GPUs and nodes.","vllm","is_blank","[""zschache2025""]","is_blank","is_blank","The snippet states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations, indicating vllm was the framework used for deployment across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","Table B2 compares single‑node and double‑node deployments for Llama 3.1 70B, showing energy consumption of 48.60 Wh on one node and 94.88 Wh on two nodes, giving a ratio of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","Table B2 compares single‑node and double‑node deployments for Llama 3.1 70B, showing energy consumption of 48.60 Wh on one node and 94.88 Wh on two nodes, giving a ratio of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","is_blank","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The paper states that scaling a 0.42 Wh short query to 700 million queries per day results in annual electricity use comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","is_blank","The paper states that scaling a 0.42 Wh short query to 700 million queries per day results in annual electricity use comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The 2025 paper by Luccioni et al. reports that Microsoft indicated a 34% increase in its global water consumption between 2021 and 2022, exceeding 1.7 billion gallons.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni et al. reports that Microsoft indicated a 34% increase in its global water consumption between 2021 and 2022, exceeding 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The document explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than LLMs, showing they do not achieve comparable accuracy.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The document explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than LLMs, showing they do not achieve comparable accuracy."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The paper states that the Augusta cluster in Iowa had a trailing twelve‑month average PUE of 1.12, and the Evolved Transformer experiments were run on the Iowa (Augusta) data center.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","The paper states that the Augusta cluster in Iowa had a trailing twelve‑month average PUE of 1.12, and the Evolved Transformer experiments were run on the Iowa (Augusta) data center."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The analysis describes a growth pattern for GPT-4o usage and states that this results in a total of approximately 772 billion GPT-4o queries in 2025.","772 billion","queries","[""jegham2025""]","is_blank","is_blank","The analysis describes a growth pattern for GPT-4o usage and states that this results in a total of approximately 772 billion GPT-4o queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","Table 3 lists the OpenLLM Leaderboard average scores for several models, showing JetMoE-8B with a score of 53.0, which is the final average score for this model on the benchmark suite.","53.0","score","[""shen2024""]","is_blank","is_blank","Table 3 lists the OpenLLM Leaderboard average scores for several models, showing JetMoE-8B with a score of 53.0, which is the final average score for this model on the benchmark suite."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided documents do not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided documents do not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that the AI Act currently includes an open‑source exemption, but the authors propose eliminating this exemption so that open‑source models must follow the same reporting obligations as proprietary models. Therefore, open‑source general‑purpose AI models are not fully exempt from energy‑consumption reporting.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Act currently includes an open‑source exemption, but the authors propose eliminating this exemption so that open‑source models must follow the same reporting obligations as proprietary models. Therefore, open‑source general‑purpose AI models are not fully exempt from energy‑consumption reporting."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The provided context discusses average PUE values for data centers (e.g., 1.58 globally in 2023) but does not contain any information about the US national datacenter average PUE specifically for the year 2020.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The provided context discusses average PUE values for data centers (e.g., 1.58 globally in 2023) but does not contain any information about the US national datacenter average PUE specifically for the year 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The document states that for the OPT-2.7B model on a g4dn.xlarge instance, the KV Cache grows to 5.312 GB when the batch size is increased to 32.","5.312","GB","[""kim2025""]","is_blank","is_blank","The document states that for the OPT-2.7B model on a g4dn.xlarge instance, the KV Cache grows to 5.312 GB when the batch size is increased to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The document states that for the four notable models (GPT-3, OPT-175B, GPT-4, Gemini Ultra), R&D staff costs including equity constitute between 29% and 49% of the total amortized cost.","29-49%","percent","[""cottier2024""]","is_blank","is_blank","The document states that for the four notable models (GPT-3, OPT-175B, GPT-4, Gemini Ultra), R&D staff costs including equity constitute between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The documents state that the energy consumption during inference was measured using the CodeCarbon package.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","is_blank","is_blank","The documents state that the energy consumption during inference was measured using the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis of 100 news articles reported that 53% of the articles cited the estimate of 3 Wh per ChatGPT query or that it consumes ten times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The analysis of 100 news articles reported that 53% of the articles cited the estimate of 3 Wh per ChatGPT query or that it consumes ten times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The document states that the Compute Time Calibration Function (CTCF) is introduced to improve instance selection accuracy by correcting the gap between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The document states that the Compute Time Calibration Function (CTCF) is introduced to improve instance selection accuracy by correcting the gap between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The paper states that mining 1 kg of rare earth materials uses 11 kL of water, and that for an H100 GPU (0.1% rare earth by mass) this results in an additional 2.2 liters of water consumption per GPU. Converting 2.2 L to kiloliters gives 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","is_blank","The paper states that mining 1 kg of rare earth materials uses 11 kL of water, and that for an H100 GPU (0.1% rare earth by mass) this results in an additional 2.2 liters of water consumption per GPU. Converting 2.2 L to kiloliters gives 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context describes the removal of the open‑source exemption as a proposed policy change, not as an existing EU rule. Therefore, under the current EU AI Act, open‑source models are not yet required to report energy consumption to authorities.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context describes the removal of the open‑source exemption as a proposed policy change, not as an existing EU rule. Therefore, under the current EU AI Act, open‑source models are not yet required to report energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The paper by Luccioni et al. (2025) states that in 2020 Amazon, Microsoft, Meta, and Google together accounted for almost 30% of all corporate PPAs worldwide.","30","percent","[""luccioni2025a""]","is_blank","is_blank","The paper by Luccioni et al. (2025) states that in 2020 Amazon, Microsoft, Meta, and Google together accounted for almost 30% of all corporate PPAs worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The document states that U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023, which is based on the average attribution method used for the analysis.","6.7 billion","USD","[""han2024""]","is_blank","is_blank","The document states that U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023, which is based on the average attribution method used for the analysis."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","Table 4 lists the energy consumption for a long prompt (10k input‑1.5k output) as 12.222 Wh for the o3 model and 0.827 Wh for GPT‑4.1 nano. Dividing the o3 value by the GPT‑4.1 nano value gives the factor by which o3 consumes more energy.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","Table 4 lists the energy consumption for a long prompt (10k input‑1.5k output) as 12.222 Wh for the o3 model and 0.827 Wh for GPT‑4.1 nano. Dividing the o3 value by the GPT‑4.1 nano value gives the factor by which o3 consumes more energy."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The paper’s cost‑savings analysis for the short‑context (Arena) dataset with a 120 ms SLO states that Mélange achieves a 15‑77% cost reduction compared to single‑GPU‑type baselines.","15-77%","percent","[""griggs2024""]","is_blank","is_blank","The paper’s cost‑savings analysis for the short‑context (Arena) dataset with a 120 ms SLO states that Mélange achieves a 15‑77% cost reduction compared to single‑GPU‑type baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","Table 3 lists the carbon emissions for several models, including FLM-101B, and shows a net tCO2e value of 26 for FLM-101B.","26","tCO2e","[""li2025a""]","is_blank","is_blank","Table 3 lists the carbon emissions for several models, including FLM-101B, and shows a net tCO2e value of 26 for FLM-101B."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The documents state that the public health impact of AI is highly unevenly distributed across U.S. counties and communities, with disadvantaged areas bearing up to 200 times higher per‑household health costs.","0","is_blank","[""han2024""]","is_blank","is_blank","The documents state that the public health impact of AI is highly unevenly distributed across U.S. counties and communities, with disadvantaged areas bearing up to 200 times higher per‑household health costs."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The document states that GPU theoretical performance per watt doubles every 3-4 years, citing Sun et al., 2019, which aligns with the claim.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The document states that GPU theoretical performance per watt doubles every 3-4 years, citing Sun et al., 2019, which aligns with the claim."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The case study states that quantization was applied using Ollama, an open‑source platform that also enables local model deployment.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The case study states that quantization was applied using Ollama, an open‑source platform that also enables local model deployment."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The authors state that to handle spot VMs that may be terminated, they used an independent S3 storage provider, Backblaze (B2), and accessed the data via shards using the WebDataset library.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","is_blank","The authors state that to handle spot VMs that may be terminated, they used an independent S3 storage provider, Backblaze (B2), and accessed the data via shards using the WebDataset library."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D‑MA) on 1 Feb 2024.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D‑MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The table from Chen et al. lists the price per chip for the NVIDIA H100 as $11.06 per hour.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The table from Chen et al. lists the price per chip for the NVIDIA H100 as $11.06 per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The amortization model discussion notes that, based on observed failure rates, the expected hardware (GPU) lifetime would be about 3.7 years.","3.7","years","[""cottier2024""]","is_blank","is_blank","The amortization model discussion notes that, based on observed failure rates, the expected hardware (GPU) lifetime would be about 3.7 years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","Table II in the provided document lists the bare minimum hardware required for each LLaMA model. For the 13B model it specifies a count of 2 V100 GPUs with 32 GB memory.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","Table II in the provided document lists the bare minimum hardware required for each LLaMA model. For the 13B model it specifies a count of 2 V100 GPUs with 32 GB memory."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The document states that Google reports the Gemma family emitted 1247.61 tons CO2e, which is over four times the emissions represented by the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The document states that Google reports the Gemma family emitted 1247.61 tons CO2e, which is over four times the emissions represented by the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The documents state that ELMo was trained on 3 NVIDIA GTX 1080 (Ti) GPUs for 2 weeks, which equals 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","The documents state that ELMo was trained on 3 NVIDIA GTX 1080 (Ti) GPUs for 2 weeks, which equals 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","Both the Wu et al. (2021) figure and the Dodge et al. (2022) paper state that Google's hyperscale data centers reported a PUE of 1.10 in 2021.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","Both the Wu et al. (2021) figure and the Dodge et al. (2022) paper state that Google's hyperscale data centers reported a PUE of 1.10 in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The AWS sustainability report states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on‑premises computing when matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","is_blank","The AWS sustainability report states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on‑premises computing when matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The document states that the analytical model predicts fine‑tuning a sparse Mixtral model on 2 M queries with an NVIDIA H100 GPU costs $3460.","3460","USD","[""xia2024""]","is_blank","is_blank","The document states that the analytical model predicts fine‑tuning a sparse Mixtral model on 2 M queries with an NVIDIA H100 GPU costs $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The study measured power draw of a server running AI workloads and found that the GPU contributed 74% of the total electricity consumption of the components, indicating GPUs dominate provisioned power in a typical datacenter.","74","percent","[""dodge2022""]","is_blank","is_blank","The study measured power draw of a server running AI workloads and found that the GPU contributed 74% of the total electricity consumption of the components, indicating GPUs dominate provisioned power in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The document states that training was performed on a cluster with 12 nodes and 96 H100 GPUs, indicating the total number of GPUs used.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The document states that training was performed on a cluster with 12 nodes and 96 H100 GPUs, indicating the total number of GPUs used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The paper reports that during active training in the first 300 logging steps, the average GPU power for a single node is over 600W.","600","Watts","[""morrison2025""]","is_blank","is_blank","The paper reports that during active training in the first 300 logging steps, the average GPU power for a single node is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The Strubell et al. (2019) table lists estimated CO2e emissions for various activities, including ""Human life, avg, 1 year"" as 11,023 pounds.","11023","lbs","[""strubell2019""]","is_blank","is_blank","The Strubell et al. (2019) table lists estimated CO2e emissions for various activities, including ""Human life, avg, 1 year"" as 11,023 pounds."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The document states that for LLaMA‑13B the A100 provides about a 1.25× increase in inference latency (i.e., throughput) compared to the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The document states that for LLaMA‑13B the A100 provides about a 1.25× increase in inference latency (i.e., throughput) compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The document states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The document states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The text states that for the 400 TPS SLO, the Max‑Performance instance g6e.xlarge costs $2.699, which is about 280% more expensive than InferSave's top‑choice instance.","280","percent","[""kim2025""]","is_blank","is_blank","The text states that for the 400 TPS SLO, the Max‑Performance instance g6e.xlarge costs $2.699, which is about 280% more expensive than InferSave's top‑choice instance."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The document states that for models that generate only a single token per inference, a V100 GPU (or even an A30) is more efficient, indicating V100 is the most energy‑efficient architecture for this scenario.","V100","is_blank","[""zschache2025""]","is_blank","is_blank","The document states that for models that generate only a single token per inference, a V100 GPU (or even an A30) is more efficient, indicating V100 is the most energy‑efficient architecture for this scenario."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The Schwartz et al. (2019) paper states that reporting the financial cost (price tag) of finding, training, and running models is a key Green AI practice, indicating that Green AI involves providing these financial costs.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The Schwartz et al. (2019) paper states that reporting the financial cost (price tag) of finding, training, and running models is a key Green AI practice, indicating that Green AI involves providing these financial costs."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The 2025 paper by Luccioni et al. reports that electronic waste reached 62 million tonnes worldwide in 2022.","62 million","metric tons","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni et al. reports that electronic waste reached 62 million tonnes worldwide in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","is_blank","is_blank","Watts","[""is_blank""]","is_blank","is_blank","is_blank"
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The abstract of Li et al. (2025) states that training GPT-3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","is_blank","The abstract of Li et al. (2025) states that training GPT-3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The document states that Facebook’s recommendation and ranking model sizes grew by a factor of 20 between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that Facebook’s recommendation and ranking model sizes grew by a factor of 20 between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The cross-efficiency DEA results show that o3-mini achieved the highest score (0.884), indicating it ranked highest in the eco-efficiency analysis.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","The cross-efficiency DEA results show that o3-mini achieved the highest score (0.884), indicating it ranked highest in the eco-efficiency analysis."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The document states that the current average lifetime for cell phones is less than 3 years, and discusses sustainability goals to reduce e-waste, implying that short phone lifespans contribute to e-waste concerns.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The document states that the current average lifetime for cell phones is less than 3 years, and discusses sustainability goals to reduce e-waste, implying that short phone lifespans contribute to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The intra‑zone experiments with T4 GPUs show that CV models have a per‑GPU speedup that is described as ""almost linear"" (values 0.43, 0.42, 0.43, 0.41, 0.41), indicating near‑linear scaling.","1","is_blank","[""erben2023""]","is_blank","is_blank","The intra‑zone experiments with T4 GPUs show that CV models have a per‑GPU speedup that is described as ""almost linear"" (values 0.43, 0.42, 0.43, 0.41, 0.41), indicating near‑linear scaling."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The document explicitly states that the FLM-101B model is structured with a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","is_blank","The document explicitly states that the FLM-101B model is structured with a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The document states that LLM decoding is memory‑intensive, low compute‑intensity and bottlenecked by VRAM bandwidth, leading to lower power draw, while diffusion models are significantly more compute‑intensive and draw near‑maximum GPU power.","1","is_blank","[""chung2025""]","is_blank","is_blank","The document states that LLM decoding is memory‑intensive, low compute‑intensity and bottlenecked by VRAM bandwidth, leading to lower power draw, while diffusion models are significantly more compute‑intensive and draw near‑maximum GPU power."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The Amazon 2023 sustainability report includes a table of gender representation for People Managers. The row for People Managers shows the percentage of women globally as 26.1% for 2023.","26.1","percent","[""amazon2023""]","is_blank","is_blank","The Amazon 2023 sustainability report includes a table of gender representation for People Managers. The row for People Managers shows the percentage of women globally as 26.1% for 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The text states that when equity is excluded, the fraction of total amortized cost for computing hardware rises to 61–76% for the four models (GPT-3, OPT-175B, GPT-4, Gemini Ultra).","61-76","percent","[""cottier2024""]","is_blank","is_blank","The text states that when equity is excluded, the fraction of total amortized cost for computing hardware rises to 61–76% for the four models (GPT-3, OPT-175B, GPT-4, Gemini Ultra)."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The text states that metrics like accuracy and F1 score are slightly lower after optimization, indicating they did not always improve.","0","is_blank","[""khan2025""]","is_blank","is_blank","The text states that metrics like accuracy and F1 score are slightly lower after optimization, indicating they did not always improve."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","is_blank","is_blank","lbs/kWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The document states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027.","85-134","TWh","[""li2025b""]","is_blank","is_blank","The document states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The Europe section of the Amazon 2023 sustainability report states that Amazon delivered 150 million packages via electric vehicles in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","The Europe section of the Amazon 2023 sustainability report states that Amazon delivered 150 million packages via electric vehicles in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload uses 128 input tokens and 512 output tokens per request, totaling 640 tokens per request. With 3000 requests, the total tokens processed are 640 × 3000 = 1,920,000.","1920000","tokens","[""kim2025""]","is_blank","is_blank","The online inference workload uses 128 input tokens and 512 output tokens per request, totaling 640 tokens per request. With 3000 requests, the total tokens processed are 640 × 3000 = 1,920,000."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The provided excerpts state that the AI Act fails to address greenhouse gas emissions from AI applications and does not mandate their disclosure, indicating the claim is false.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The provided excerpts state that the AI Act fails to address greenhouse gas emissions from AI applications and does not mandate their disclosure, indicating the claim is false."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The paper states that for the short DenseNet 201 job, Flexible Start can achieve up to 80% emissions reduction in the West US region.","80","percent","[""dodge2022""]","is_blank","is_blank","The paper states that for the short DenseNet 201 job, Flexible Start can achieve up to 80% emissions reduction in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The study reports a mean energy consumption of 0.002 kWh per 1,000 inferences for text classification and 2.907 kWh per 1,000 inferences for image generation. Dividing 2.907 by 0.002 gives a factor of about 1,450, which the authors note as ""over 1450"".","1450","times","[""luccioni2024""]","is_blank","is_blank","The study reports a mean energy consumption of 0.002 kWh per 1,000 inferences for text classification and 2.907 kWh per 1,000 inferences for image generation. Dividing 2.907 by 0.002 gives a factor of about 1,450, which the authors note as ""over 1450""."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The documents state that for the Yelp sentiment analysis benchmark, traditional models perform considerably worse than large language models, indicating they do not achieve comparable accuracy.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The documents state that for the Yelp sentiment analysis benchmark, traditional models perform considerably worse than large language models, indicating they do not achieve comparable accuracy."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The table titled ""Amazon Renewable Energy Projects* Projects announced as of January 2024"" lists the United States with 244 projects, indicating that 244 projects were announced in the U.S. by that date.","244","projects","[""amazon2023""]","is_blank","is_blank","The table titled ""Amazon Renewable Energy Projects* Projects announced as of January 2024"" lists the United States with 244 projects, indicating that 244 projects were announced in the U.S. by that date."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The table from the Strubell 2019 source lists the percent of energy sourced from renewable sources for Amazon-AWS as 17%, which corresponds to the 2018 data.","17","percent","[""strubell2019""]","is_blank","is_blank","The table from the Strubell 2019 source lists the percent of energy sourced from renewable sources for Amazon-AWS as 17%, which corresponds to the 2018 data."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors compare different measurement levels in data centers and advise that AI energy consumption be reported at the cumulative server level to achieve a balance between precision and practicality.","cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The authors compare different measurement levels in data centers and advise that AI energy consumption be reported at the cumulative server level to achieve a balance between precision and practicality."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","Table 1 reports that the GPU contributed 74% of the total electricity consumption when training BERT‑base on a single NVIDIA TITAN X GPU, and the text confirms the GPU accounts for almost three‑quarters of the consumption.","74","percent","[""dodge2022""]","is_blank","is_blank","Table 1 reports that the GPU contributed 74% of the total electricity consumption when training BERT‑base on a single NVIDIA TITAN X GPU, and the text confirms the GPU accounts for almost three‑quarters of the consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The Griggs et al. document explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The Griggs et al. document explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The document states that the iterative optimization process (hardware‑software co‑design) led to a 28.5% operational energy footprint reduction over the two‑year period.","28.5","percent","[""wu2021a""]","is_blank","is_blank","The document states that the iterative optimization process (hardware‑software co‑design) led to a 28.5% operational energy footprint reduction over the two‑year period."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The document explicitly states the estimated upfront cost of acquiring the hardware used to train GPT-4.","800M","USD","[""cottier2024""]","is_blank","is_blank","The document explicitly states the estimated upfront cost of acquiring the hardware used to train GPT-4."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term ‘Sustainable AI’ was proposed to both aim at climate‑positive applications and improve the environmental sustainability of AI itself, not solely for climate‑positive use.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that the umbrella term ‘Sustainable AI’ was proposed to both aim at climate‑positive applications and improve the environmental sustainability of AI itself, not solely for climate‑positive use."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","Figure 13 presents the ground‑truth maximum batch size for Mixtral on various GPUs. The label for the A100‑40GB GPU shows bsz=28, indicating the ground‑truth batch size is 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","Figure 13 presents the ground‑truth maximum batch size for Mixtral on various GPUs. The label for the A100‑40GB GPU shows bsz=28, indicating the ground‑truth batch size is 28 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The document states that in overloaded conditions, Swapping consistently uses less energy than Recomputation.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The document states that in overloaded conditions, Swapping consistently uses less energy than Recomputation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The paper states that for the 6 billion‑parameter transformer, using Pause and Resume and allowing the job duration to double can achieve emissions reductions of up to about 25%.","25","percent","[""dodge2022""]","is_blank","is_blank","The paper states that for the 6 billion‑parameter transformer, using Pause and Resume and allowing the job duration to double can achieve emissions reductions of up to about 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The document states that JetMoE-8B activates only 2B parameters per token, which reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B activates only 2B parameters per token, which reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context cites the seminal 2019 study by Strubell et al. as reporting that training BERT emitted 626,155 pounds of CO2e.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context cites the seminal 2019 study by Strubell et al. as reporting that training BERT emitted 626,155 pounds of CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","Both documents state that GPT‑3’s training required an estimated 1287 MWh of electricity.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","Both documents state that GPT‑3’s training required an estimated 1287 MWh of electricity."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The paper states that optimizing the MoE layer is crucial and that it is the costliest layer, making it a prime target for optimization to improve LLM fine‑tuning performance.","1","is_blank","[""xia2024""]","is_blank","is_blank","The paper states that optimizing the MoE layer is crucial and that it is the costliest layer, making it a prime target for optimization to improve LLM fine‑tuning performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","is_blank","is_blank","samples","[""is_blank""]","is_blank","is_blank","is_blank"
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The document states that for high‑granularity CV tasks, distributing VMs across four continents results in only a 7% slowdown compared to fully local training, confirming the claim.","1","is_blank","[""erben2023""]","is_blank","is_blank","The document states that for high‑granularity CV tasks, distributing VMs across four continents results in only a 7% slowdown compared to fully local training, confirming the claim."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The study reports that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32 g of CO₂eq for each 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The study reports that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32 g of CO₂eq for each 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","Figure 1 reports emissions of 7 k grams for the most efficient region and 26 k grams for the least efficient region when training BERT, giving a difference of roughly 19 k grams.","19","grams","[""dodge2022""]","is_blank","is_blank","Figure 1 reports emissions of 7 k grams for the most efficient region and 26 k grams for the least efficient region when training BERT, giving a difference of roughly 19 k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The document states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information before the trend reversed.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The document states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information before the trend reversed."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The table in the Morrison 2025 paper lists OLMo 60M with a water consumption equivalence of 5 days for one US person.","5","days","[""morrison2025""]","is_blank","is_blank","The table in the Morrison 2025 paper lists OLMo 60M with a water consumption equivalence of 5 days for one US person."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","is_blank","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","is_blank"
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The document states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, indicating it does not increase costs.","0","is_blank","[""xia2024""]","is_blank","is_blank","The document states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, indicating it does not increase costs."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","Table 3 lists the carbon footprint statistics for FLM-101B, showing an energy consumption of 40 MkWh for the model.","40","kWh","[""li2025a""]","is_blank","is_blank","Table 3 lists the carbon footprint statistics for FLM-101B, showing an energy consumption of 40 MkWh for the model."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","is_blank","is_blank","USD","[""is_blank""]","is_blank","is_blank","is_blank"
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","Table IV provides the estimated total cost for fine‑tuning Mixtral on the GSM8K (GS) dataset with a sparse MoE using an NVIDIA A40 48 GB GPU, listed as $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","Table IV provides the estimated total cost for fine‑tuning Mixtral on the GSM8K (GS) dataset with a sparse MoE using an NVIDIA A40 48 GB GPU, listed as $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The paper’s Table IV reports the maximum batch size (MBS) used for fine‑tuning a sparse Mixtral model on an NVIDIA A40 GPU with 48 GB memory, which is 4 samples. This batch size corresponds to the largest workload (and thus the longest‑running MoE layer) evaluated for that configuration.","4","samples","[""xia2024""]","is_blank","is_blank","The paper’s Table IV reports the maximum batch size (MBS) used for fine‑tuning a sparse Mixtral model on an NVIDIA A40 GPU with 48 GB memory, which is 4 samples. This batch size corresponds to the largest workload (and thus the longest‑running MoE layer) evaluated for that configuration."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides throughput and cost estimates for a sparse Mixtral model on an A40 GPU, but it does not give the total execution time (in seconds) for a batch size of 10, nor enough data to compute it directly.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides throughput and cost estimates for a sparse Mixtral model on an A40 GPU, but it does not give the total execution time (in seconds) for a batch size of 10, nor enough data to compute it directly."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The document explicitly states that GPU-level power consumption monitoring should not be used for overall energy measurements, advocating against it.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The document explicitly states that GPU-level power consumption monitoring should not be used for overall energy measurements, advocating against it."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The 2023 article reported that training contributed only half of BLOOM's total emissions, which corresponds to 50 percent.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The 2023 article reported that training contributed only half of BLOOM's total emissions, which corresponds to 50 percent."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","Table II lists the bare minimum hardware required for each LLaMA variant. For the 7B model it shows a count of 1 V100 GPU with 32 GB memory, indicating only one such GPU is needed for inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","Table II lists the bare minimum hardware required for each LLaMA variant. For the 7B model it shows a count of 1 V100 GPU with 32 GB memory, indicating only one such GPU is needed for inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","Table 1 lists the number of user requests that would consume a 500 ml bottle of water for each data‑center location. The Arizona row shows a value of 16.7 requests.","16.7","requests","[""li2025b""]","is_blank","is_blank","Table 1 lists the number of user requests that would consume a 500 ml bottle of water for each data‑center location. The Arizona row shows a value of 16.7 requests."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The provided context states that the emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil for an average US home total 8.3 metric tons of CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The provided context states that the emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil for an average US home total 8.3 metric tons of CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","is_blank","is_blank","score","[""is_blank""]","is_blank","is_blank","is_blank"
