"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly mentions the name of the benchmark suite presented in a recent paper for measuring inference energy consumption. The benchmark suite is called the ML.ENERGY Benchmark.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly mentions the name of the benchmark suite presented in a recent paper for measuring inference energy consumption. The benchmark suite is called the ML.ENERGY Benchmark."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context does not provide a specific value for the net CO2e emissions from training the GShard-600B model.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide a specific value for the net CO2e emissions from training the GShard-600B model."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The model size in gigabytes (GB) for the LLaMA-33B model is clearly stated in the provided context.","64.7","GB","[""chen2024""]","is_blank","is_blank","The model size in gigabytes (GB) for the LLaMA-33B model is clearly stated in the provided context."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. However, it mentions that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. Additionally, it discusses the electricity consumption of various AI models and data centers but does not provide a direct answer to the question.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. However, it mentions that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. Additionally, it discusses the electricity consumption of various AI models and data centers but does not provide a direct answer to the question."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that hyperscale data centers achieve more than 40% higher efficiency compared to traditional data centers. This is supported by the power usage effectiveness (PUE) values, which indicate that hyperscale data centers have a stark difference in efficiency compared to traditional data centers.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that hyperscale data centers achieve more than 40% higher efficiency compared to traditional data centers. This is supported by the power usage effectiveness (PUE) values, which indicate that hyperscale data centers have a stark difference in efficiency compared to traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","is_blank","500 mL bottles","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context provides information about a sample of 60 papers from top AI conferences, including CVPR. It states that 75% of CVPR papers target accuracy and 20% target efficiency.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context provides information about a sample of 60 papers from top AI conferences, including CVPR. It states that 75% of CVPR papers target accuracy and 20% target efficiency."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act does mandate the disclosure of energy consumption, but this information is restricted to authorities and is not accessible to downstream providers or the general public due to confidentiality clauses in Articles 21(3), 53(7), and 78(1). The Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act does mandate the disclosure of energy consumption, but this information is restricted to authorities and is not accessible to downstream providers or the general public due to confidentiality clauses in Articles 21(3), 53(7), and 78(1). The Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The projected maximum batch size for fine-tuning a Mixtral model with a projected GPU capacity of 100 is 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","The projected maximum batch size for fine-tuning a Mixtral model with a projected GPU capacity of 100 is 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides information about the inference performance of LLaMA-7B on A100 and V100 GPUs. It mentions that the A100 outperforms the V100, with a 2 times increase in inference latency for LLaMA-7B across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides information about the inference performance of LLaMA-7B on A100 and V100 GPUs. It mentions that the A100 outperforms the V100, with a 2 times increase in inference latency for LLaMA-7B across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is provided in Table 1 under the 'U.S. Average' row.","5.439","liters","[""li2025b""]","is_blank","is_blank","The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is provided in Table 1 under the 'U.S. Average' row."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context clearly states that as of 2023, the water use effectiveness (WUE) for AWS data centers was 0.18 liters of water per kilowatt-hour (L/kWh).","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context clearly states that as of 2023, the water use effectiveness (WUE) for AWS data centers was 0.18 liters of water per kilowatt-hour (L/kWh)."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context clearly supports the statement that local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models. The text explicitly mentions that local inference 'significantly reduces both network overhead and carbon footprint' by minimizing data transmission between clients and remote servers.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context clearly supports the statement that local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models. The text explicitly mentions that local inference 'significantly reduces both network overhead and carbon footprint' by minimizing data transmission between clients and remote servers."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the importance of tracking the runtime of a training job for estimating compute cost in GPU-based or cloud environments. The amount of CO2eq emitted during model training can be decomposed into factors including power consumption of the hardware used, training time, and carbon intensity of the energy grid. Training time is a crucial factor in estimating carbon emissions and cloud compute cost.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","The context discusses the importance of tracking the runtime of a training job for estimating compute cost in GPU-based or cloud environments. The amount of CO2eq emitted during model training can be decomposed into factors including power consumption of the hardware used, training time, and carbon intensity of the energy grid. Training time is a crucial factor in estimating carbon emissions and cloud compute cost."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","","","percent","[]","is_blank","is_blank",""
"q164","How much does an elephant weigh?","The context does not provide information about the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides energy consumption values for various large NLP DNNs. According to the table in the context, DeepSeek-R1 (DS) has the highest energy consumption at 29.078 Wh for long prompts.","is_blank","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides energy consumption values for various large NLP DNNs. According to the table in the context, DeepSeek-R1 (DS) has the highest energy consumption at 29.078 Wh for long prompts."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides information on the carbon footprint of training BERT and other large language models. Specifically, it mentions that training BERT base requires 626,155 pounds of CO2 emissions. Additionally, it provides information on the daily CO2 emissions of an average American life, which is 36,156 pounds per year. To find out how many days of CO2 emissions from an average American life are equivalent to training BERT base, we can perform a calculation.","17.3","days","[""luccioni2025b"", ""strubell2019""]","is_blank","is_blank","The context provides information on the carbon footprint of training BERT and other large language models. Specifically, it mentions that training BERT base requires 626,155 pounds of CO2 emissions. Additionally, it provides information on the daily CO2 emissions of an average American life, which is 36,156 pounds per year. To find out how many days of CO2 emissions from an average American life are equivalent to training BERT base, we can perform a calculation."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide specific information about the performance comparison between the Transformer architecture and the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as model sizes grow.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the performance comparison between the Transformer architecture and the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not clearly support an answer to the specific dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the specific dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context supports the answer that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training. By leveraging multiple spot instances with one T4 GPU each, it is possible to be more cost-efficient than a DGX-2 node. The cost to throughput tradeoff for 8xT4 and DGX-2 shows that 8xT4 can be more cost-efficient for certain tasks.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context supports the answer that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training. By leveraging multiple spot instances with one T4 GPU each, it is possible to be more cost-efficient than a DGX-2 node. The cost to throughput tradeoff for 8xT4 and DGX-2 shows that 8xT4 can be more cost-efficient for certain tasks."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German Energy Efficiency Act of 8 Nov 2023 requires data centers to run on 50% renewable energy, increasing that factor to 100% by 1 Jan 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The German Energy Efficiency Act of 8 Nov 2023 requires data centers to run on 50% renewable energy, increasing that factor to 100% by 1 Jan 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context does not provide specific information on how many papers from ACL targeted both accuracy and efficiency out of the sample of 60 papers from top AI conferences.","is_blank","papers","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on how many papers from ACL targeted both accuracy and efficiency out of the sample of 60 papers from top AI conferences."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","According to recent estimates, inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025"", ""luccioni2024""]","is_blank","is_blank","According to recent estimates, inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context discusses the AI Act's requirements and gaps regarding energy consumption reporting for general-purpose AI models. It mentions that the AI Act requires providers of general-purpose AI models to meet transparency obligations, including reporting energy consumption during the model's development phase, but not during the inference phase.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context discusses the AI Act's requirements and gaps regarding energy consumption reporting for general-purpose AI models. It mentions that the AI Act requires providers of general-purpose AI models to meet transparency obligations, including reporting energy consumption during the model's development phase, but not during the inference phase."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The AI Act does not currently require providers to report energy use during the inference phase of AI models. The context suggests that this is a crucial omission given the long-term environmental impact of AI applications. The authors propose an interpretation of the AI Act to bring reporting on energy consumption from AI inferences back into the scope.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act does not currently require providers to report energy use during the inference phase of AI models. The context suggests that this is a crucial omission given the long-term environmental impact of AI applications. The authors propose an interpretation of the AI Act to bring reporting on energy consumption from AI inferences back into the scope."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context clearly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This implies that the statement 'New AI data centers often rely on air cooling due to high server power densities' is false.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context clearly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This implies that the statement 'New AI data centers often rely on air cooling due to high server power densities' is false."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context provides information on the power efficiency improvement of platform-level caching for the cross-lingual Transformer language model. According to the text, platform-level caching improves power efficiency by 6.7 ×.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides information on the power efficiency improvement of platform-level caching for the cross-lingual Transformer language model. According to the text, platform-level caching improves power efficiency by 6.7 ×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table (Table 3) that lists the estimated CO2 emissions for training various models, including BERT base on V100x64 GPUs for 79 hours, which resulted in 1438 lbs of CO2 emissions.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table (Table 3) that lists the estimated CO2 emissions for training various models, including BERT base on V100x64 GPUs for 79 hours, which resulted in 1438 lbs of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context clearly supports that according to recent papers, ML inference reportedly accounts for 80-90% of the total compute demand.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","The context clearly supports that according to recent papers, ML inference reportedly accounts for 80-90% of the total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides information on the energy consumption of training a 6.1 billion parameter transformer model. The training process consumed 13.8 MWh of electricity, and it is estimated that a full training run would consume approximately 103.5 MWh or 103,500 kWh. To convert this to U.S. household-years of electricity consumption, we need to know the average annual electricity consumption of a U.S. household.","is_blank","household-years","[""dodge2022""]","is_blank","is_blank","The context provides information on the energy consumption of training a 6.1 billion parameter transformer model. The training process consumed 13.8 MWh of electricity, and it is estimated that a full training run would consume approximately 103.5 MWh or 103,500 kWh. To convert this to U.S. household-years of electricity consumption, we need to know the average annual electricity consumption of a U.S. household."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context clearly states that for NLP experiments in geo-distributed settings, egress costs can account for more than 90% of the total cost per VM. Specifically, for GC, the external egress cost for NLP is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","is_blank","is_blank","The context clearly states that for NLP experiments in geo-distributed settings, egress costs can account for more than 90% of the total cost per VM. Specifically, for GC, the external egress cost for NLP is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides information about the training of JetMoE-8B, including the total GPU hours used (30,000 H100 GPU hours) but does not specify the number of GPUs used for training. Without the number of GPUs, we cannot accurately estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","is_blank","days","[""shen2024""]","is_blank","is_blank","The context provides information about the training of JetMoE-8B, including the total GPU hours used (30,000 H100 GPU hours) but does not specify the number of GPUs used for training. Without the number of GPUs, we cannot accurately estimate the total wall-clock time in days required to pre-train the JetMoE-8B model."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","","","is_blank","[]","is_blank","is_blank",""
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","","","W","[]","is_blank","is_blank",""
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B).","7","multiplier","[""zschache2025""]","is_blank","is_blank","The context states that among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B)."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The carbon emissions for Qwen before optimization were 0.009 kg CO2 and after optimization were 0.004 kg CO2. The percentage decrease in carbon emissions can be calculated as ((0.009 - 0.004) / 0.009) * 100 = 55.56%.","55.56","percent","[""khan2025""]","is_blank","is_blank","The carbon emissions for Qwen before optimization were 0.009 kg CO2 and after optimization were 0.004 kg CO2. The percentage decrease in carbon emissions can be calculated as ((0.009 - 0.004) / 0.009) * 100 = 55.56%."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration.","40","models","[""chung2025""]","is_blank","is_blank","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context provides information on the health costs associated with training a Llama-3.1 scale model in various locations. Specifically, it mentions that the total health cost can vary widely depending on the training data center locations, with a cost of $0.23 million in Oregon and $2.5 million in Iowa.","2.5","USD","[""han2024""]","is_blank","is_blank","The context provides information on the health costs associated with training a Llama-3.1 scale model in various locations. Specifically, it mentions that the total health cost can vary widely depending on the training data center locations, with a cost of $0.23 million in Oregon and $2.5 million in Iowa."
