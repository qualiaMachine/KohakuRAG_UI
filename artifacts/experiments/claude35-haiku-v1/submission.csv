"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context describes the ML.ENERGY Benchmark, which is a benchmark suite designed to measure inference energy consumption of generative AI models under realistic service environments.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context describes the ML.ENERGY Benchmark, which is a benchmark suite designed to measure inference energy consumption of generative AI models under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","According to Figure 4 in the context from Wu et al.'s paper, the net CO2e emissions for training the GShard-600B model is approximately 0.5 million kg CO2e, based on the graphical representation of carbon footprint for open-source machine learning models.","0.5","tCO2e","[""wu2021a""]","is_blank","is_blank","According to Figure 4 in the context from Wu et al.'s paper, the net CO2e emissions for training the GShard-600B model is approximately 0.5 million kg CO2e, based on the graphical representation of carbon footprint for open-source machine learning models."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context from [ref_id=chen2024] specifically lists the model size for LLaMA-33B as 64.7 GB in the various model parameter tables. This value is consistently reported across multiple references.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context from [ref_id=chen2024] specifically lists the model size for LLaMA-33B as 64.7 GB in the various model parameter tables. This value is consistently reported across multiple references."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The documents discuss electricity and energy consumption for various AI models and data centers, but lack the precise measurement requested in the question.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The documents discuss electricity and energy consumption for various AI models and data centers, but lack the precise measurement requested in the question."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","Multiple sources in the context confirm that hyperscale data centers achieved significantly higher efficiency in 2020, with power usage effectiveness (PUE) more than 40% better than traditional data centers.","1","is_blank","[""wu2021b""]","is_blank","is_blank","Multiple sources in the context confirm that hyperscale data centers achieved significantly higher efficiency in 2020, with power usage effectiveness (PUE) more than 40% better than traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","","","500 mL bottles","[]","is_blank","is_blank",""
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context shows that in a sample of 60 papers from top AI conferences, 75% of CVPR papers targeted accuracy, while only 20% argued for a new efficiency result. The difference between these percentages is 55 percentage points.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context shows that in a sample of 60 papers from top AI conferences, 75% of CVPR papers targeted accuracy, while only 20% argued for a new efficiency result. The difference between these percentages is 55 percentage points."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context indicates that the AI Act does NOT make energy consumption data publicly available. In fact, the document specifically notes that where the Act mandates energy consumption disclosure, this information is restricted to authorities and is not accessible to the general public, NGOs, or analysts due to confidentiality clauses.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does NOT make energy consumption data publicly available. In fact, the document specifically notes that where the Act mandates energy consumption disclosure, this information is restricted to authorities and is not accessible to the general public, NGOs, or analysts due to confidentiality clauses."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to the paper, the authors used an analytical model to project the maximum batch size for fine-tuning a Mixtral model with a 100GB GPU capacity. Their model predicts a maximum batch size of 28 samples for a GPU with 100GB memory.","28","samples","[""xia2024""]","is_blank","is_blank","According to the paper, the authors used an analytical model to project the maximum batch size for fine-tuning a Mixtral model with a 100GB GPU capacity. Their model predicts a maximum batch size of 28 samples for a GPU with 100GB memory."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","","","multiplier","[]","is_blank","is_blank",""
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","Based on the context snippet in Table 1, the total water consumption for GPT-3 training in U.S. data centers is estimated at 5.439 million liters, which includes both on-site (0.708 million liters) and off-site (4.731 million liters) water consumption.","5.439","liters","[""li2025b""]","is_blank","is_blank","Based on the context snippet in Table 1, the total water consumption for GPT-3 training in U.S. data centers is estimated at 5.439 million liters, which includes both on-site (0.708 million liters) and off-site (4.731 million liters) water consumption."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The authors explicitly propose that Sustainability Impact Assessments (SIAs) should apply across all AI systems, not just high-risk AI. Specifically, they argue that the carbon footprint of AI models is often unrelated to their risk classification, and therefore SIAs should cover the entire AI landscape.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The authors explicitly propose that Sustainability Impact Assessments (SIAs) should apply across all AI systems, not just high-risk AI. Specifically, they argue that the carbon footprint of AI models is often unrelated to their risk classification, and therefore SIAs should cover the entire AI landscape."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context from Amazon's 2023 sustainability report clearly states that AWS achieved a Water Use Effectiveness (WUE) of 0.18 liters per kilowatt-hour (L/kWh) in 2023, representing a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context from Amazon's 2023 sustainability report clearly states that AWS achieved a Water Use Effectiveness (WUE) of 0.18 liters per kilowatt-hour (L/kWh) in 2023, representing a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context supports this statement by explicitly stating that local inference reduces both network overhead and carbon footprint. Specifically, the paper describes local inference as a method that allows models to run directly on user devices, minimizing data transmission between clients and remote servers, which significantly reduces network overhead and carbon emissions.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context supports this statement by explicitly stating that local inference reduces both network overhead and carbon footprint. Specifically, the paper describes local inference as a method that allows models to run directly on user devices, minimizing data transmission between clients and remote servers, which significantly reduces network overhead and carbon emissions."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","Tracking runtime is crucial for estimating compute cost in GPU-based environments, as demonstrated by multiple sources showing the importance of understanding computational time, energy consumption, and associated costs during model training and inference.","1","is_blank","[""strubell2019"", ""luccioni2023"", ""kim2025""]","is_blank","is_blank","Tracking runtime is crucial for estimating compute cost in GPU-based environments, as demonstrated by multiple sources showing the importance of understanding computational time, energy consumption, and associated costs during model training and inference."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the context from Chen et al.'s study, the LLaMA-65B model achieved up to a 13.2% performance improvement through automated resource utilization overlapping. This improvement was particularly notable for larger batch sizes that produce larger key-value (KV) tensors.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to the context from Chen et al.'s study, the LLaMA-65B model achieved up to a 13.2% performance improvement through automated resource utilization overlapping. This improvement was particularly notable for larger batch sizes that produce larger key-value (KV) tensors."
"q164","How much does an elephant weigh?","There is no direct information about an elephant's weight in the provided context. The documents discuss AI model training, data center energy consumption, and wildlife conservation, but do not provide specific details about elephant weight.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","There is no direct information about an elephant's weight in the provided context. The documents discuss AI model training, data center energy consumption, and wildlife conservation, but do not provide specific details about elephant weight."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context does not provide energy consumption data for all five models mentioned in the question (Meena, T5, GPT-3, GShard-600B, and Switch Transformer). Only T5's energy consumption is briefly mentioned, and not in a comparative context with the other models.","is_blank","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context does not provide energy consumption data for all five models mentioned in the question (Meena, T5, GPT-3, GShard-600B, and Switch Transformer). Only T5's energy consumption is briefly mentioned, and not in a comparative context with the other models."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","According to the context from Strubell et al.'s 2019 paper, training the BERT base model produces about 626,155 pounds of CO₂ emissions. The context provides a comparison table that equates this to emissions from an average American life for approximately a year.","36","days","[""strubell2019""]","is_blank","is_blank","According to the context from Strubell et al.'s 2019 paper, training the BERT base model produces about 626,155 pounds of CO₂ emissions. The context provides a comparison table that equates this to emissions from an average American life for approximately a year."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not specify a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain. While the documents discuss energy consumption of various models and datasets, no dataset matching the specific description is identified.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not specify a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain. While the documents discuss energy consumption of various models and datasets, no dataset matching the specific description is identified."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The paper provides evidence that eight T4 spot instances can be more cost-efficient than a DGX-2 node. The authors demonstrate this by showing that using distributed training across eight T4 instances can provide competitive performance and cost benefits, especially for computer vision models with high granularity.","1","is_blank","[""erben2023""]","is_blank","is_blank","The paper provides evidence that eight T4 spot instances can be more cost-efficient than a DGX-2 node. The authors demonstrate this by showing that using distributed training across eight T4 instances can provide competitive performance and cost benefits, especially for computer vision models with high granularity."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context from the Luccioni et al. 2025 paper, the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage of AI. The document explicitly states that 'sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI's greenhouse gas emissions nor energy usage'.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context from the Luccioni et al. 2025 paper, the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage of AI. The document explicitly states that 'sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI's greenhouse gas emissions nor energy usage'."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to the context, specifically the section on the German 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy, increasing to 100% by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, specifically the section on the German 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy, increasing to 100% by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to the context, the authors sampled 60 papers from top AI conferences and analyzed the target focus of each paper. For ACL specifically, 90% of papers targeted accuracy, and a very small portion (10%) argued for a new efficiency result.","1.8","papers","[""schwartz2019""]","is_blank","is_blank","According to the context, the authors sampled 60 papers from top AI conferences and analyzed the target focus of each paper. For ACL specifically, 90% of papers targeted accuracy, and a very small portion (10%) argued for a new efficiency result."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Multiple recent studies consistently estimate that inference accounts for 80-90% of a model's total lifecycle energy use. Sources like AWS, Meta, and Google provide specific percentages within this range, highlighting the significant energy consumption during model inference compared to training.","90","percent","[""jegham2025"", ""luccioni2024"", ""chung2025""]","is_blank","is_blank","Multiple recent studies consistently estimate that inference accounts for 80-90% of a model's total lifecycle energy use. Sources like AWS, Meta, and Google provide specific percentages within this range, highlighting the significant energy consumption during model inference compared to training."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does NOT currently require reporting of both training and inference energy consumption. While the Act requires reporting of energy consumption during model development, it explicitly leaves out the inference phase, which is a significant omission according to the authors.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does NOT currently require reporting of both training and inference energy consumption. While the Act requires reporting of energy consumption during model development, it explicitly leaves out the inference phase, which is a significant omission according to the authors."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","According to the context, the AI Act currently does NOT require providers to report energy use during the inference phase of AI models. The authors argue this is a significant omission and propose that energy consumption from both single and cumulative inferences should be included in reporting requirements.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, the AI Act currently does NOT require providers to report energy use during the inference phase of AI models. The authors argue this is a significant omission and propose that energy consumption from both single and cumulative inferences should be included in reporting requirements."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, which contradicts the statement that they rely on air cooling. The research paper highlights that for server-level cooling, new AI data centers typically use liquid cooling methods like direct-to-chip cooling or immersion cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, which contradicts the statement that they rely on air cooling. The research paper highlights that for server-level cooling, new AI data centers typically use liquid cooling methods like direct-to-chip cooling or immersion cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to the context from Wu et al. (2021), platform-level caching improved the power efficiency of the cross-lingual Transformer language model by a factor of 6.7×. This was achieved by precomputing and caching frequently accessed embeddings for language translation tasks using DRAM and Flash storage devices.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context from Wu et al. (2021), platform-level caching improved the power efficiency of the cross-lingual Transformer language model by a factor of 6.7×. This was achieved by precomputing and caching frequently accessed embeddings for language translation tasks using DRAM and Flash storage devices."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to the context from Strubell et al. (2019), training the BERT base model on 64 V100 GPUs for 79 hours results in an estimated 1438 lbs of CO2 emissions.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to the context from Strubell et al. (2019), training the BERT base model on 64 V100 GPUs for 79 hours results in an estimated 1438 lbs of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","Multiple sources consistently report that ML inference accounts for 80-90% of total compute demand across major cloud providers like AWS, with varied percentages from different companies: Google reports 60% of ML energy use, Meta attributes about one-third of their carbon footprint to inference, and AWS estimates 80-90% of cloud computing demand.","80-90","percent","[""luccioni2024"", ""chung2025"", ""fernandez2025""]","is_blank","is_blank","Multiple sources consistently report that ML inference accounts for 80-90% of total compute demand across major cloud providers like AWS, with varied percentages from different companies: Google reports 60% of ML energy use, Meta attributes about one-third of their carbon footprint to inference, and AWS estimates 80-90% of cloud computing demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to the Dodge et al. paper, the training of a 6.1 billion parameter language model would consume 103,500 kWh for a full training run. Average U.S. household electricity consumption is approximately 10,715 kWh per year, which means this model training is equivalent to about 9.66 household-years of electricity consumption.","9.66","household-years","[""dodge2022""]","is_blank","is_blank","According to the Dodge et al. paper, the training of a 6.1 billion parameter language model would consume 103,500 kWh for a full training run. Average U.S. household electricity consumption is approximately 10,715 kWh per year, which means this model training is equivalent to about 9.66 household-years of electricity consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly confirms that for NLP experiments, egress costs can indeed account for more than 90% of the total cost per VM. Specifically, the paper notes that for GC, the external egress cost was $4.329/h, which is more than 90% of the total cost per VM of $4.804/h.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context explicitly confirms that for NLP experiments, egress costs can indeed account for more than 90% of the total cost per VM. Specifically, the paper notes that for GC, the external egress cost was $4.329/h, which is more than 90% of the total cost per VM of $4.804/h."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides precise information about the total GPU hours used for training JetMoE-8B. The model was trained using 30,000 H100 GPU hours on a training infrastructure of 12 nodes and 96 H100 GPUs. To estimate the total wall-clock time, we can calculate the total time required based on the number of GPUs used.","2.5","days","[""shen2024""]","is_blank","is_blank","The context provides precise information about the total GPU hours used for training JetMoE-8B. The model was trained using 30,000 H100 GPU hours on a training infrastructure of 12 nodes and 96 H100 GPUs. To estimate the total wall-clock time, we can calculate the total time required based on the number of GPUs used."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explicitly defines water consumption as 'water withdrawal minus water discharge', which refers to the amount of water evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context explicitly defines water consumption as 'water withdrawal minus water discharge', which refers to the amount of water evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the document, the observed range of inference energy per second for LLaMA-65B was between 300 Watts to 1 Kilowatt, which varied across different GPU shard configurations ranging from 8 to 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","is_blank","According to the document, the observed range of inference energy per second for LLaMA-65B was between 300 Watts to 1 Kilowatt, which varied across different GPU shard configurations ranging from 8 to 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","","","multiplier","[]","is_blank","is_blank",""
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","According to the context, Qwen's carbon emissions dropped from 0.009 kg CO2 to 0.004 kg CO2 when applying quantization and local inference techniques, which represents a significant reduction in carbon footprint.","55.56","percent","[""khan2025""]","is_blank","is_blank","According to the context, Qwen's carbon emissions dropped from 0.009 kg CO2 to 0.004 kg CO2 when applying quantization and local inference techniques, which represents a significant reduction in carbon footprint."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the paper's abstract, the ML.ENERGY Benchmark includes energy measurements for 40 widely used model architectures across 6 different tasks in their early 2025 iteration.","40","models","[""chung2025""]","is_blank","is_blank","According to the paper's abstract, the ML.ENERGY Benchmark includes energy measurements for 40 widely used model architectures across 6 different tasks in their early 2025 iteration."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","According to the context, when training a Llama-3.1 scale model in Altoona, Iowa, the estimated health cost is $2.5 million. The document highlights that the health cost varies dramatically depending on the data center location, with Iowa being an example of a high-cost location due to factors like wind direction and pollutant emission rates.","2.5","USD","[""han2024""]","is_blank","is_blank","According to the context, when training a Llama-3.1 scale model in Altoona, Iowa, the estimated health cost is $2.5 million. The document highlights that the health cost varies dramatically depending on the data center location, with Iowa being an example of a high-cost location due to factors like wind direction and pollutant emission rates."
