"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context describes the ML.ENERGY Benchmark, a benchmark suite for measuring inference energy consumption of generative AI models. No specific name for the benchmark suite is provided.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context describes the ML.ENERGY Benchmark, a benchmark suite for measuring inference energy consumption of generative AI models. No specific name for the benchmark suite is provided."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context does not provide any information about the net CO2e emissions from training the GShard-600B model. The discussion around GShard-600B only mentions its carbon footprint, not the net emissions.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the net CO2e emissions from training the GShard-600B model. The discussion around GShard-600B only mentions its carbon footprint, not the net emissions."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides the model size for the LLaMA-33B model as 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context provides the model size for the LLaMA-33B model as 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide any information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The available information focuses on measuring the electricity consumption and emissions of individual AI models and cloud instances, but does not mention anything about Google Cloud TPU pods.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The available information focuses on measuring the electricity consumption and emissions of individual AI models and cloud instances, but does not mention anything about Google Cloud TPU pods."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context indicates that for every medium-length GPT-3 completion (prompt = 800 words; response 150 - 300 words), the model needs to 'drink' (consume) around 0.5 liter of water, depending on when and where it is deployed.","0.5","500 mL bottles","[""li2025b""]","is_blank","is_blank","The context indicates that for every medium-length GPT-3 completion (prompt = 800 words; response 150 - 300 words), the model needs to 'drink' (consume) around 0.5 liter of water, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context indicates that 75% of CVPR papers target accuracy compared to only 20% that target efficiency. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage that target efficiency is 55 percentage points.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context indicates that 75% of CVPR papers target accuracy compared to only 20% that target efficiency. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage that target efficiency is 55 percentage points."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context states that for GPU memory capacities of 100GB and 120GB, the projected maximum batch sizes supported for fine-tuning Mixtral are 28 and 35 samples, respectively.","28 and 35","samples","[""xia2024""]","is_blank","is_blank","The context states that for GPU memory capacities of 100GB and 120GB, the projected maximum batch sizes supported for fine-tuning Mixtral are 28 and 35 samples, respectively."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context indicates that the A100 GPU outperformed the V100 GPU for the smaller LLaMA 7B and 13B models, with anywhere from a 2x to 1.25x increase in inference latency on the A100 compared to the V100. However, for the larger LLaMA 65B model, the improvement in using the A100 over the V100 was much smaller.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The context indicates that the A100 GPU outperformed the V100 GPU for the smaller LLaMA 7B and 13B models, with anywhere from a 2x to 1.25x increase in inference latency on the A100 compared to the V100. However, for the larger LLaMA 65B model, the improvement in using the A100 over the V100 was much smaller."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides estimates of the total water consumption for training the GPT-3 model at various Microsoft data center locations in the US and internationally. The total water consumption for training GPT-3 at the US average PUE and WUE is estimated to be 5.439 million liters.","5.439","liters","[""li2025b""]","is_blank","is_blank","The context provides estimates of the total water consumption for training the GPT-3 model at various Microsoft data center locations in the US and internationally. The total water consumption for training GPT-3 at the US average PUE and WUE is estimated to be 5.439 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context indicates that the authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems, as the carbon footprint of AI models is often unrelated to their classification as high or low risk.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems, as the carbon footprint of AI models is often unrelated to their classification as high or low risk."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context states that in 2023, AWS improved its data center water use effectiveness (WUE) to 0.18 liters per kilowatt-hour (L/kWh), a 5% improvement from 0.19 L/kWh in 2022.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, AWS improved its data center water use effectiveness (WUE) to 0.18 liters per kilowatt-hour (L/kWh), a 5% improvement from 0.19 L/kWh in 2022."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context does not mention local inference as a sustainability measure to reduce network overhead and carbon footprint. Instead, it focuses on optimization techniques like quantization to improve the energy efficiency of LLMs.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention local inference as a sustainability measure to reduce network overhead and carbon footprint. Instead, it focuses on optimization techniques like quantization to improve the energy efficiency of LLMs."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context indicates that tracking the runtime of a training job is an important step for estimating the compute cost in GPU-based or cloud environments. The context discusses how the total CO2 emissions from model training can be calculated by considering the power consumption, training time, and carbon intensity of the energy grid.","1","is_blank","[""luccioni2023""]","is_blank","is_blank","The context indicates that tracking the runtime of a training job is an important step for estimating the compute cost in GPU-based or cloud environments. The context discusses how the total CO2 emissions from model training can be calculated by considering the power consumption, training time, and carbon intensity of the energy grid."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The provided context states that the LLaMA-65B model experienced a significant performance improvement of up to 13.2% through automated resource utilization overlapping.","13.2","percent","[""chen2024""]","is_blank","is_blank","The provided context states that the LLaMA-65B model experienced a significant performance improvement of up to 13.2% through automated resource utilization overlapping."
"q164","How much does an elephant weigh?","is_blank","is_blank","lbs","[""is_blank""]","is_blank","is_blank","is_blank"
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context does not provide clear information about the relative energy consumption of the mentioned large NLP DNNs. There is no direct comparison of the models' energy consumption.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide clear information about the relative energy consumption of the mentioned large NLP DNNs. There is no direct comparison of the models' energy consumption."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context indicates that training the BERT base model, which has 110M parameters, consumed 626,155 pounds of CO₂ emissions. This is equivalent to the annual CO₂ emissions from an average American life.","626,155 lbs","days","[""strubell2019""]","is_blank","is_blank","The context indicates that training the BERT base model, which has 110M parameters, consumed 626,155 pounds of CO₂ emissions. This is equivalent to the annual CO₂ emissions from an average American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not clearly indicate whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow. The provided context does not mention this specific comparison.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly indicate whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow. The provided context does not mention this specific comparison."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not mention any dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain. The information provided is about energy consumption measurements of different language models in general.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention any dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain. The information provided is about energy consumption measurements of different language models in general."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context indicates that by leveraging multiple spot instances with one T4 GPU each, eight T4 instances could be more cost-efficient than a DGX-2 node from the same cloud provider.","True","is_blank","[""erben2023""]","is_blank","is_blank","The context indicates that by leveraging multiple spot instances with one T4 GPU each, eight T4 instances could be more cost-efficient than a DGX-2 node from the same cloud provider."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context indicates that the 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that the 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context indicates that under Germany's 2023 Energy Efficiency Act, data centers are required to run on 100% renewable energy by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that under Germany's 2023 Energy Efficiency Act, data centers are required to run on 100% renewable energy by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of a sample of 60 papers from top AI conferences, 10% of the ACL papers targeted both accuracy and efficiency.","6","papers","[""schwartz2019""]","is_blank","is_blank","The context states that out of a sample of 60 papers from top AI conferences, 10% of the ACL papers targeted both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context indicates that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025""]","is_blank","is_blank","The context indicates that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does not require providers to report both training and inference energy consumption for general-purpose AI models. The Act only mandates reporting of energy consumption during the model development phase, but does not cover the inference phase, which the authors argue is a crucial omission.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not require providers to report both training and inference energy consumption for general-purpose AI models. The Act only mandates reporting of energy consumption during the model development phase, but does not cover the inference phase, which the authors argue is a crucial omission."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context indicates that the AI Act currently does not require providers to report energy use during the inference phase of AI models. The article states 'the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act currently does not require providers to report energy use during the inference phase of AI models. The article states 'the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'"
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context indicates that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, rather than air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context indicates that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, rather than air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that platform-level caching improved the power efficiency of the language model by 6.7×.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that platform-level caching improved the power efficiency of the language model by 6.7×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context states that the BERT base model (110M parameters) was trained on 64 Tesla V100 GPUs for 79.2 hours, and that this training run consumed 1507 kWh of electricity. The context also provides a formula to estimate the CO2 emissions based on the electricity usage, which gives 1438 lbs of CO2.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context states that the BERT base model (110M parameters) was trained on 64 Tesla V100 GPUs for 79.2 hours, and that this training run consumed 1507 kWh of electricity. The context also provides a formula to estimate the CO2 emissions based on the electricity usage, which gives 1438 lbs of CO2."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","Several sources state that ML inference accounts for 80-90% of total compute demand or ML cloud computing demand.","80-90","percent","[""chung2025"", ""luccioni2024""]","is_blank","is_blank","Several sources state that ML inference accounts for 80-90% of total compute demand or ML cloud computing demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training a 6.1 billion parameter language model consumed 13,812.4 kWh of electricity over 8 days on 256 NVIDIA A100 GPUs. Assuming this electricity consumption is for the full training run, the household-years of electricity consumption is equivalent to this amount.","13,812.4","household-years","[""dodge2022""]","is_blank","is_blank","The context states that training a 6.1 billion parameter language model consumed 13,812.4 kWh of electricity over 8 days on 256 NVIDIA A100 GPUs. Assuming this electricity consumption is for the full training run, the household-years of electricity consumption is equivalent to this amount."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context indicates that for NLP experiments on Google Cloud, the external egress costs can account for more than 90% of the total cost per VM. This means that egress costs in geo-distributed NLP experiments could make up over 90% of the total costs.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context indicates that for NLP experiments on Google Cloud, the external egress costs can account for more than 90% of the total cost per VM. This means that egress costs in geo-distributed NLP experiments could make up over 90% of the total costs."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours, but does not provide the number of GPUs used. Without the number of GPUs, the total wall-clock time in days cannot be estimated.","is_blank","days","[""is_blank""]","is_blank","is_blank","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours, but does not provide the number of GPUs used. Without the number of GPUs, the total wall-clock time in days cannot be estimated."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as 'water withdrawal minus water discharge', and specifies that it refers to the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context defines water consumption as 'water withdrawal minus water discharge', and specifies that it refers to the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the context, the observed range of inference energy per second for LLaMA-65B was estimated to be between 300 Watts and 1 Kilowatt across the shard configurations from 8 GPUs to 32 GPUs.","300 - 1000","W","[""samsi2024""]","is_blank","is_blank","According to the context, the observed range of inference energy per second for LLaMA-65B was estimated to be between 300 Watts and 1 Kilowatt across the shard configurations from 8 GPUs to 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context, the 72B version of the Qwen model consumed 7 times more energy than the 7B version, with only a minor 0.07 point reduction in accuracy.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context, the 72B version of the Qwen model consumed 7 times more energy than the 7B version, with only a minor 0.07 point reduction in accuracy."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context shows that the Qwen model had a CO2 emission of 0.009 kg before optimization, which was reduced to 0.004 kg after applying quantization and local inference techniques, corresponding to a reduction of over 55%.","55","percent","[""khan2025""]","is_blank","is_blank","The context shows that the Qwen model had a CO2 emission of 0.009 kg before optimization, which was reduced to 0.004 kg after applying quantization and local inference techniques, corresponding to a reduction of over 55%."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context states that the latest iteration of the ML.ENERGY Benchmark included 40 widely used model architectures across 6 different tasks.","40","models","[""chung2025""]","is_blank","is_blank","The context states that the latest iteration of the ML.ENERGY Benchmark included 40 widely used model architectures across 6 different tasks."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context indicates that the total health cost for training an AI model of the Llama-3.1 scale can vary widely depending on the location, with the cost in Iowa being $2.5 million compared to only $0.23 million in Oregon. The health cost can even exceed 120% of the electricity cost for training.","2500000","USD","[""han2024""]","is_blank","is_blank","The context indicates that the total health cost for training an AI model of the Llama-3.1 scale can vary widely depending on the location, with the cost in Iowa being $2.5 million compared to only $0.23 million in Oregon. The health cost can even exceed 120% of the electricity cost for training."
