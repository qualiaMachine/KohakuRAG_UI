"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context indicates that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014, compared to 24% from 2005-2010 and 90% from 2000-2005.","4","percent","[""wu2021b""]","is_blank","is_blank","The context indicates that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014, compared to 24% from 2005-2010 and 90% from 2000-2005."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context indicates that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","4","data centers","[""amazon2023""]","is_blank","is_blank","The context indicates that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context does not provide any estimates of the embodied carbon emissions per GPU. The passage notes that there is currently no information available about the embodied emissions linked to manufacturing GPUs.","is_blank","kg/GPU","[""luccioni2025b"", ""morrison2025""]","is_blank","is_blank","The context does not provide any estimates of the embodied carbon emissions per GPU. The passage notes that there is currently no information available about the embodied emissions linked to manufacturing GPUs."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the estimated amortized hardware and energy cost for training GPT-4 was $40M, while the estimated acquisition cost for the hardware was $800M, which is 20 times larger.","20","ratio","[""cottier2024""]","is_blank","is_blank","The context states that the estimated amortized hardware and energy cost for training GPT-4 was $40M, while the estimated acquisition cost for the hardware was $800M, which is 20 times larger."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides an estimate of the average CO2 emissions for a round trip flight between San Francisco and New York to be 1984 lbs CO2e.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","The context provides an estimate of the average CO2 emissions for a round trip flight between San Francisco and New York to be 1984 lbs CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","43.94","score","[""li2025a""]","is_blank","is_blank","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the first commercially-produced microprocessor (the Intel 4004 in 1971) ran at 740 kHz, while typical 2021 microprocessors can run at 5,000,000 kHz, which is a more than 6,750 fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context states that the first commercially-produced microprocessor (the Intel 4004 in 1971) ran at 740 kHz, while typical 2021 microprocessors can run at 5,000,000 kHz, which is a more than 6,750 fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context indicates that training GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec was estimated to take 14.8 days.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context indicates that training GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec was estimated to take 14.8 days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table that shows the GPU Power Usage (in kWh) for the Llama 3.2 1B model at different request frequencies. At an 8 request/s frequency, the GPU Power Usage is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The context provides a table that shows the GPU Power Usage (in kWh) for the Llama 3.2 1B model at different request frequencies. At an 8 request/s frequency, the GPU Power Usage is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context indicates that the total permitted annual emission limits for nitrogen oxides (NOx) from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024 were approximately 13,000 tons.","13000","tons","[""han2024""]","is_blank","is_blank","The context indicates that the total permitted annual emission limits for nitrogen oxides (NOx) from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024 were approximately 13,000 tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","is_blank","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context indicates that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 air pollution from the power plants that supply electricity to the data centers.","1300","deaths","[""han2024""]","is_blank","is_blank","The context indicates that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 air pollution from the power plants that supply electricity to the data centers."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately 103,593 kWh.","103,593","days","[""dodge2022""]","is_blank","is_blank","The context states that Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately 103,593 kWh."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The provided context indicates that among the models listed in the appendix, the Qwen-32B model has the highest GPU energy consumption for 1,000 queries.","Qwen-32B","is_blank","[""fernandez2025""]","is_blank","is_blank","The provided context indicates that among the models listed in the appendix, the Qwen-32B model has the highest GPU energy consumption for 1,000 queries."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","is_blank","is_blank","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been shown to be formally collected and recycled.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been shown to be formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context does not provide information about the energy consumption for pre-training the BLOOM model specifically. The information given is about the energy consumption for fine-tuning the Bloomz-7B model compared to the entire training process, but does not mention the pre-training energy consumption.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide information about the energy consumption for pre-training the BLOOM model specifically. The information given is about the energy consumption for fine-tuning the Bloomz-7B model compared to the entire training process, but does not mention the pre-training energy consumption."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context does not provide the percentage of activated parameters in the Switch Transformer model. It only states that the Switch Transformer model has 1.5 trillion parameters.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide the percentage of activated parameters in the Switch Transformer model. It only states that the Switch Transformer model has 1.5 trillion parameters."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context indicates that the JetMoE-8B architecture is a Mixture-of-Experts (MoE) model, but it does not specify how many experts are included in each MoE layer.","is_blank","experts","[""is_blank""]","is_blank","is_blank","The context indicates that the JetMoE-8B architecture is a Mixture-of-Experts (MoE) model, but it does not specify how many experts are included in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context indicates that dense BlackMamba fine-tuning on NVIDIA A40 GPU with a batch size of 30 would take between 0.5 and 2.0 seconds, based on the execution time breakdowns shown in the figures.","1.5","second","[""xia2024""]","is_blank","is_blank","The context indicates that dense BlackMamba fine-tuning on NVIDIA A40 GPU with a batch size of 30 would take between 0.5 and 2.0 seconds, based on the execution time breakdowns shown in the figures."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context states that the total cost of training the English portion of the FLM-101B model was 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context states that the total cost of training the English portion of the FLM-101B model was 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","","","is_blank","[]","is_blank","is_blank",""
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The paper examines and compares the energy and carbon costs of different machine learning model architectures, including both task-specific and general-purpose models. The authors analyze the costs of performing 1,000 inferences using these models.","is_blank","models","[""luccioni2024"", ""luccioni2025b""]","is_blank","is_blank","The paper examines and compares the energy and carbon costs of different machine learning model architectures, including both task-specific and general-purpose models. The authors analyze the costs of performing 1,000 inferences using these models."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3×.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x.","2.2","multiplier","[""cottier2024""]","is_blank","is_blank","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context clearly states that the full training run of the 6.1 billion parameter transformer model would consume approximately 103,593 kWh of energy.","103,593","MWh","[""dodge2022""]","is_blank","is_blank","The context clearly states that the full training run of the 6.1 billion parameter transformer model would consume approximately 103,593 kWh of energy."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' Paradox, where efficiency gains can paradoxically lead to increased consumption.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' Paradox, where efficiency gains can paradoxically lead to increased consumption."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","4.2 - 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased computational cost. Therefore, the statement 'As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline' is false.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased computational cost. Therefore, the statement 'As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline' is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The provided context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","21.54","days","[""li2025a""]","is_blank","is_blank","The provided context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated)."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context indicates that a majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving room for efficiency improvements.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context indicates that a majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving room for efficiency improvements."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context indicates that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context indicates that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context describes a collaborative project called the 'AI Energy Score' that aims to establish a unified approach for comparing the inference efficiency of AI models.","The AI Energy Score project","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context describes a collaborative project called the 'AI Energy Score' that aims to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context indicates that the execution time breakdown for the longest kernel of the MoE layer of the dense BlackMamba model with a batch size of 30 fine-tuned on a NVIDIA A40-48 GB GPU is not provided. The context does not contain this specific information.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context indicates that the execution time breakdown for the longest kernel of the MoE layer of the dense BlackMamba model with a batch size of 30 fine-tuned on a NVIDIA A40-48 GB GPU is not provided. The context does not contain this specific information."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The JetMoE-8B model uses a sparsely-gated Mixture-of-Experts (SMoE) architecture with both attention and feedforward experts. Each layer in the model activates only 2 of the 8 experts, reducing the overall computational cost compared to a dense model.","2","experts","[""shen2024""]","is_blank","is_blank","The JetMoE-8B model uses a sparsely-gated Mixture-of-Experts (SMoE) architecture with both attention and feedforward experts. Each layer in the model activates only 2 of the 8 experts, reducing the overall computational cost compared to a dense model."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context indicates that deep learning models have experienced a 300,000x increase in the amount of compute used to train them from 2012 to 2018, which directly supports the claim that deep learning models are increasingly large and computationally-intensive.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context indicates that deep learning models have experienced a 300,000x increase in the amount of compute used to train them from 2012 to 2018, which directly supports the claim that deep learning models are increasingly large and computationally-intensive."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context states that global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context states that global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, up from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, up from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context indicates that the field of AI was officially 'born and christened' at a 1956 workshop, suggesting the field is around 70 years old as of 2025.","70","years","[""stone2022""]","is_blank","is_blank","The context indicates that the field of AI was officially 'born and christened' at a 1956 workshop, suggesting the field is around 70 years old as of 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context discusses a 2019 study by Strubell et al. that estimated the energy use and greenhouse gas emissions required to train natural language processing models, including a case study of neural architecture search (NAS) requiring 626,155 pounds (284 metric tons) of CO2-equivalent emissions, about five times the lifetime emissions of a car.","1","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context discusses a 2019 study by Strubell et al. that estimated the energy use and greenhouse gas emissions required to train natural language processing models, including a case study of neural architecture search (NAS) requiring 626,155 pounds (284 metric tons) of CO2-equivalent emissions, about five times the lifetime emissions of a car."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that by targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency, the energy consumption per generation of the Llama 3.1 8B model can be reduced by 44%.","44","percent","[""chung2025""]","is_blank","is_blank","The context states that by targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency, the energy consumption per generation of the Llama 3.1 8B model can be reduced by 44%."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context indicates that the maximum batch size for fine-tuning BlackMamba on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is 20 samples in the sparse setup.","20","samples","[""xia2024""]","is_blank","is_blank","The context indicates that the maximum batch size for fine-tuning BlackMamba on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is 20 samples in the sparse setup."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon had 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The context states that Amazon had 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o's annual carbon emissions are projected to be comparable to the emissions from approximately 272 transatlantic flights.","272","flights","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o's annual carbon emissions are projected to be comparable to the emissions from approximately 272 transatlantic flights."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide any information about the percentage of AI inference workloads in Asia that were powered by coal in 2023. The provided information is about the overall energy consumption and environmental impact of AI systems, but does not contain the specific data requested in the question.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of AI inference workloads in Asia that were powered by coal in 2023. The provided information is about the overall energy consumption and environmental impact of AI systems, but does not contain the specific data requested in the question."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The provided context indicates that the average data center PUE was 1.58 globally in 2023.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The provided context indicates that the average data center PUE was 1.58 globally in 2023."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context indicates that the Llama 7B model has an estimated energy consumption of 356 MWh and GHG emissions of 14 tCO2e for the pre-training phase.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the Llama 7B model has an estimated energy consumption of 356 MWh and GHG emissions of 14 tCO2e for the pre-training phase."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that Amazon's U.S. fleet of electric delivery vans increased from more than 2,600 in 2022 to 11,800 in 2023, for a total increase of over 9,000 electric delivery vans.","Over 9,000","electric delivery vans","[""amazon2023""]","is_blank","is_blank","The context states that Amazon's U.S. fleet of electric delivery vans increased from more than 2,600 in 2022 to 11,800 in 2023, for a total increase of over 9,000 electric delivery vans."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context clearly indicates that operational environmental impacts of LLMs include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context clearly indicates that operational environmental impacts of LLMs include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context states that the o3 model consumes 12.222 Wh for a long prompt (10k input and 1.5k output).","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The context states that the o3 model consumes 12.222 Wh for a long prompt (10k input and 1.5k output)."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy.","1956","year","[""stone2022""]","is_blank","is_blank","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide any information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The provided context is about estimating the water footprint of AI models in general, with a specific case study on GPT-3, but does not mention Google or provide any details about Google's data centers or their water efficiency in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The provided context is about estimating the water footprint of AI models in general, with a specific case study on GPT-3, but does not mention Google or provide any details about Google's data centers or their water efficiency in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context indicates that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context indicates that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, for a maximum generation length of 512 tokens, the LLaMA-65B model consumed approximately 3-4 Joules per output token.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","According to the context, for a maximum generation length of 512 tokens, the LLaMA-65B model consumed approximately 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that there is no clear, publicly available calculation or sound scientific grounding behind the claim that AI can reduce global GHG emissions by 5-10%. The reasoning behind this estimate is unclear, and the underlying calculations are not detailed.","is_blank","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context indicates that there is no clear, publicly available calculation or sound scientific grounding behind the claim that AI can reduce global GHG emissions by 5-10%. The reasoning behind this estimate is unclear, and the underlying calculations are not detailed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context snippets indicate that while training large, sparsely-activated neural networks can achieve higher accuracy at lower operational energy footprint, it can also incur higher embodied carbon footprint due to increased system resource requirements.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context snippets indicate that while training large, sparsely-activated neural networks can achieve higher accuracy at lower operational energy footprint, it can also incur higher embodied carbon footprint due to increased system resource requirements."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The context states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1.","53","percent","[""xia2024""]","is_blank","is_blank","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides detailed information on the energy consumption for different ML model inference tasks. It estimates that a large language model with 6.1 billion parameters consumed 13.8 MWh of electricity for a partial 8-day training run, and estimates the total energy for a full 60-day training run would be around 103.5 MWh.","103500","MWh","[""luccioni2024""]","is_blank","is_blank","The context provides detailed information on the energy consumption for different ML model inference tasks. It estimates that a large language model with 6.1 billion parameters consumed 13.8 MWh of electricity for a partial 8-day training run, and estimates the total energy for a full 60-day training run would be around 103.5 MWh."
"q067","What was the average global data center PUE in 2023?","The context indicates that the average global data center PUE in 2023 was 1.58 according to the Statista data source.","1.58","PUE","[""ebert2024"", ""ebert2024""]","is_blank","is_blank","The context indicates that the average global data center PUE in 2023 was 1.58 according to the Statista data source."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not mention anything about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. There is no information given about the specific number of wind turbines contracted by Microsoft for this purpose.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not mention anything about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. There is no information given about the specific number of wind turbines contracted by Microsoft for this purpose."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the context, the fraction of Gemini Ultra's total amortized model development cost attributed to R&D staff (including equity) was 49%, which is the highest among the frontier models analyzed.","49","percent","[""cottier2024""]","is_blank","is_blank","According to the context, the fraction of Gemini Ultra's total amortized model development cost attributed to R&D staff (including equity) was 49%, which is the highest among the frontier models analyzed."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members.","17","people","[""stone2022""]","is_blank","is_blank","The context states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context indicates that manufacturing and product use, rather than other life cycle stages, are the focus for analyzing AI's carbon footprint. The context also states that manufacturing carbon cost accounts for over 50% of the overall carbon footprint of large-scale ML tasks at Facebook.","is_blank","percent","[""wu2021a""]","is_blank","is_blank","The context indicates that manufacturing and product use, rather than other life cycle stages, are the focus for analyzing AI's carbon footprint. The context also states that manufacturing carbon cost accounts for over 50% of the overall carbon footprint of large-scale ML tasks at Facebook."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The provided context indicates that models with more parameters do not necessarily consume more energy during inference. The Phi-3 Mini and Phi-3 Small models have different parameter sizes but can have similar or different energy consumption depending on other factors like the underlying attention mechanism.","0","is_blank","[""chung2025""]","is_blank","is_blank","The provided context indicates that models with more parameters do not necessarily consume more energy during inference. The Phi-3 Mini and Phi-3 Small models have different parameter sizes but can have similar or different energy consumption depending on other factors like the underlying attention mechanism."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'","0","is_blank","[""stone2022""]","is_blank","is_blank","The context states that 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'"
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide any information about OpenAI's API requests in January 2024. There are no details about the emissions from OpenAI's systems during that specific timeframe.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about OpenAI's API requests in January 2024. There are no details about the emissions from OpenAI's systems during that specific timeframe."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, over 40x the 'five cars' estimate.","11,390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, over 40x the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context indicates that the explosive growth in AI use cases at Facebook drove a 2.9x increase in AI training infrastructure capacity and a 2.5x increase in AI inference infrastructure capacity over the 1.5 year period from Yr1-Q1 to Yr2-Q2.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The context indicates that the explosive growth in AI use cases at Facebook drove a 2.9x increase in AI training infrastructure capacity and a 2.5x increase in AI inference infrastructure capacity over the 1.5 year period from Yr1-Q1 to Yr2-Q2."
"q079","How many miles is the Earth from the Sun?","The context does not provide enough information to determine the exact distance between the Earth and the Sun. While some context is provided about distances traveled by vehicles, there is no direct information about the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide enough information to determine the exact distance between the Earth and the Sun. While some context is provided about distances traveled by vehicles, there is no direct information about the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program, developed by Google DeepMind, defeated the current human Go champion in a five-game match. This directly supports the claim that the AlphaGo program defeated the human Go champion.","1","is_blank","[""stone2022"", ""schwartz2019""]","is_blank","is_blank","The context clearly states that the AlphaGo program, developed by Google DeepMind, defeated the current human Go champion in a five-game match. This directly supports the claim that the AlphaGo program defeated the human Go champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context indicates that continuous batching, which dynamically replaces completed requests with new ones, improves GPU utilization and reduces idle time during autoregressive language model inference.","is_blank","is_blank","[""fernandez2025""]","is_blank","is_blank","The context indicates that continuous batching, which dynamically replaces completed requests with new ones, improves GPU utilization and reduces idle time during autoregressive language model inference."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The report states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The report states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that the Max-Performance policy selected a g6e.xlarge instance which was 26.7% more expensive than the g4dn.xlarge instance selected by InferSave for the 100 TPS SLO.","26.7","percent","[""kim2025""]","is_blank","is_blank","The context states that the Max-Performance policy selected a g6e.xlarge instance which was 26.7% more expensive than the g4dn.xlarge instance selected by InferSave for the 100 TPS SLO."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1,594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context states that the GPU energy usage for 1,000 inference queries spans from 0.06 Wh (for bert-tiny) to over 3,426 Wh (for Command-R Plus), depending on model size, architecture, and task complexity.","0.06 - 3,426","Wh","[""luccioni2025c""]","is_blank","is_blank","The context states that the GPU energy usage for 1,000 inference queries spans from 0.06 Wh (for bert-tiny) to over 3,426 Wh (for Command-R Plus), depending on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context does not clearly indicate whether researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. The context suggests that integrating ethics and sustainability in AI requires a more nuanced, context-specific approach rather than a one-size-fits-all solution.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly indicate whether researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. The context suggests that integrating ethics and sustainability in AI requires a more nuanced, context-specific approach rather than a one-size-fits-all solution."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context indicates that the carbon intensity of the U.S. average energy grid in 2021 was 297.6 grams of CO2 equivalents per kilowatt-hour (gCO2e/kWh).","297.6","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","The context indicates that the carbon intensity of the U.S. average energy grid in 2021 was 297.6 grams of CO2 equivalents per kilowatt-hour (gCO2e/kWh)."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The paper discusses the Hivemind framework, a decentralized PyTorch-based framework that was used to enable distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The paper discusses the Hivemind framework, a decentralized PyTorch-based framework that was used to enable distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context suggests that the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'.","social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context suggests that the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the linear model with sentence embeddings achieved the highest accuracy among the models tested for emotion classification.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that the linear model with sentence embeddings achieved the highest accuracy among the models tested for emotion classification."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The 2025 Chen et al. paper introduces 'Lamina', an LLM inference system that incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The 2025 Chen et al. paper introduces 'Lamina', an LLM inference system that incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context indicates that the largest T5 model has 13 billion active parameters.","13000000000","parameters","[""morrison2025""]","is_blank","is_blank","The context indicates that the largest T5 model has 13 billion active parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context states that the JetMoE-8B model has 8B total parameters, but only 2B parameters are activated for each input token, reducing inference computation by about 70% compared to the Llama2-7B model.","8B","parameters","[""shen2024""]","is_blank","is_blank","The context states that the JetMoE-8B model has 8B total parameters, but only 2B parameters are activated for each input token, reducing inference computation by about 70% compared to the Llama2-7B model."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by approximately 20% from 2021 to 2022.","20","percent","[""luccioni2025a""]","is_blank","is_blank","The context states that Google's data center water consumption increased by approximately 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'Carbon Intensity' as the metric for 'CO2 emissions per unit of electricity consumed'.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The context defines 'Carbon Intensity' as the metric for 'CO2 emissions per unit of electricity consumed'."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states that in the final 101B growth stage, the FLOPs utilization percentage was 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","The context states that in the final 101B growth stage, the FLOPs utilization percentage was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that 'the estimated cost of training OpenAI's GPT-4 model exceeds $100 million'.","is_blank","USD","[""xia2024""]","is_blank","is_blank","The context states that 'the estimated cost of training OpenAI's GPT-4 model exceeds $100 million'."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context indicates that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800 times.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context indicates that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800 times."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context indicates that when training was spread across four continents instead of remaining local, the throughput slowdown for the NLP task was 41% compared to the fully local experiment.","41","multiplier","[""erben2023""]","is_blank","is_blank","The context indicates that when training was spread across four continents instead of remaining local, the throughput slowdown for the NLP task was 41% compared to the fully local experiment."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","3.5B","liters","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context directly states that using custom tags with one-shot, zero-shot, and few-shots techniques can reduce the energy consumption of LLMs for code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context directly states that using custom tags with one-shot, zero-shot, and few-shots techniques can reduce the energy consumption of LLMs for code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that according to a study, NVIDIA shipped 3.76 million data-center GPUs in 2023.","3.76","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context states that according to a study, NVIDIA shipped 3.76 million data-center GPUs in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx and energy cost goes towards AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","The context states that on average, 44% of the amortized hardware CapEx and energy cost goes towards AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","According to the context, Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than small-scale, typical data centers.","1.10","PUE","[""wu2021a""]","is_blank","is_blank","According to the context, Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than small-scale, typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context indicates that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for interdisciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for interdisciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The provided context indicates that the estimated amortized training cost for Google's Gemini Ultra was $30 million.","30000000","USD","[""cottier2024""]","is_blank","is_blank","The provided context indicates that the estimated amortized training cost for Google's Gemini Ultra was $30 million."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context indicates that the AI Act does not explicitly require providers of general-purpose AI models with systemic risk to conduct environmental risk assessments as part of their mandatory risk assessments. The Act only requires consideration of risks to fundamental rights, which the authors argue should be interpreted to include environmental risks.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not explicitly require providers of general-purpose AI models with systemic risk to conduct environmental risk assessments as part of their mandatory risk assessments. The Act only requires consideration of risks to fundamental rights, which the authors argue should be interpreted to include environmental risks."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, which is considerably higher than the WHO's recommended level of 5µg/m3.","9","µg/m³","[""han2024""]","is_blank","is_blank","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, which is considerably higher than the WHO's recommended level of 5µg/m3."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that 115 print books produce the same amount of CO2 as one Amazon Kindle e-reader.","115","books","[""luccioni2025a""]","is_blank","is_blank","The context states that a life cycle assessment found that 115 print books produce the same amount of CO2 as one Amazon Kindle e-reader."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the context, the per-household health burden in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by more than 200-fold.","200","multiplier","[""han2024""]","is_blank","is_blank","According to the context, the per-household health burden in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by more than 200-fold."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context does not provide the energy consumption of the DS Llama 70B model for inference on the FKTG dataset. The closest relevant information is the energy consumption of the LLaMA 70B model, which was 48.60 Wh.","is_blank","Wh","[""samsi2024"", ""zschache2025""]","is_blank","is_blank","The context does not provide the energy consumption of the DS Llama 70B model for inference on the FKTG dataset. The closest relevant information is the energy consumption of the LLaMA 70B model, which was 48.60 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context states that the large language model OpenAI's openGPT2-XL contains 1.5 billion parameters.","1500000000","parameters","[""schwartz2019""]","is_blank","is_blank","The context states that the large language model OpenAI's openGPT2-XL contains 1.5 billion parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the phenomenon of Jevons' Paradox, where technological progress improving efficiency leads to increased overall resource consumption.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025"", ""wu2021a""]","is_blank","is_blank","The context describes the phenomenon of Jevons' Paradox, where technological progress improving efficiency leads to increased overall resource consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context indicates that a single training run of GPT-3 consumed an estimated 1287 MWh of energy, while Meena consumed 242,231 miles driven by an average passenger vehicle worth of carbon emissions. Therefore, one full training run of GPT-3 would likely consume more energy than multiple Meena training runs.","is_blank","multiplier","[""li2025b"", ""wu2021a""]","is_blank","is_blank","The context indicates that a single training run of GPT-3 consumed an estimated 1287 MWh of energy, while Meena consumed 242,231 miles driven by an average passenger vehicle worth of carbon emissions. Therefore, one full training run of GPT-3 would likely consume more energy than multiple Meena training runs."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2, the mean energy consumption for image generation tasks is 2.907 kWh per 1,000 inferences.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","According to Table 2, the mean energy consumption for image generation tasks is 2.907 kWh per 1,000 inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides an estimate of 36,156 lbs of CO2e per average American life per year.","36,156","lbs","[""strubell2019""]","is_blank","is_blank","The context provides an estimate of 36,156 lbs of CO2e per average American life per year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The information provided in the context does not clearly identify the West Virginia county with the highest projected per-household health cost from U.S. data centers in 2030. The context provides county-level data for the current period (2019-2023) but does not include projections for 2030.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The information provided in the context does not clearly identify the West Virginia county with the highest projected per-household health cost from U.S. data centers in 2030. The context provides county-level data for the current period (2019-2023) but does not include projections for 2030."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context indicates that Mistral-small's emissions changed by an unspecified multiplier after optimization in the financial sentiment classification task. No exact multiplier is provided.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context indicates that Mistral-small's emissions changed by an unspecified multiplier after optimization in the financial sentiment classification task. No exact multiplier is provided."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the training energy cost (51,686 kWh) and fine-tuning energy cost (7,571 kWh) for the BLOOMz-7B model.","59,257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","is_blank","The context provides the training energy cost (51,686 kWh) and fine-tuning energy cost (7,571 kWh) for the BLOOMz-7B model."
"q125","What is the total number of parameters in the final FLM-101B model?","The passage states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.","101","parameters","[""li2025a""]","is_blank","is_blank","The passage states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the training energy cost and number of inferences required to reach the training cost for the BLOOMz-7B model. The training energy cost is 51,686 kWh and the number of inferences required is 592,570,000.","592,570,000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides the training energy cost and number of inferences required to reach the training cost for the BLOOMz-7B model. The training energy cost is 51,686 kWh and the number of inferences required is 592,570,000."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context states that for all of the model experimentation and evaluation in the study, a total of 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context states that for all of the model experimentation and evaluation in the study, a total of 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context indicates that for the BLOOMz-7B model, it would take around 592,570,000 inferences for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning.","592,570,000","inferences","[""luccioni2024""]","is_blank","is_blank","The context indicates that for the BLOOMz-7B model, it would take around 592,570,000 inferences for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used in the experiments originates from the FKTG dataset (Fachkonferenz Teilgebiete) which contains objections submitted by the population during the process of selecting a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the dataset used in the experiments originates from the FKTG dataset (Fachkonferenz Teilgebiete) which contains objections submitted by the population during the process of selecting a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide any information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The information given is about the training energy consumption of the Llama model but does not mention anything about the water usage.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The information given is about the training energy consumption of the Llama model but does not mention anything about the water usage."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide any information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. There are only estimates of the environmental impact from mining rare earth metals used during manufacturing, but no specifics on the recycled content.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. There are only estimates of the environmental impact from mining rare earth metals used during manufacturing, but no specifics on the recycled content."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to the lifetime emissions of about 5 US cars.","1984","passengers","[""strubell2019""]","is_blank","is_blank","The context states that the actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to the lifetime emissions of about 5 US cars."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the May 2025 data from OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","is_blank","According to the May 2025 data from OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The study indicates that the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 4 GPUs.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The study indicates that the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 4 GPUs."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that the full training run of the 6 billion parameter transformer model is estimated to consume approximately 103,593 kWh of electricity.","103,593","metric tons","[""dodge2022""]","is_blank","is_blank","The context states that the full training run of the 6 billion parameter transformer model is estimated to consume approximately 103,593 kWh of electricity."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. The article focuses on strategies to reduce energy consumption and carbon emissions of large language models, but does not quantify the specific amount of emissions avoided.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. The article focuses on strategies to reduce energy consumption and carbon emissions of large language models, but does not quantify the specific amount of emissions avoided."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","According to the context, using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and a 31% cost saving over an A10G-only strategy.","24","percent","[""griggs2024""]","is_blank","is_blank","According to the context, using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and a 31% cost saving over an A10G-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context indicates that according to Chen et al. (2025), the price per hour for an NVIDIA H20 is $4.63.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The context indicates that according to Chen et al. (2025), the price per hour for an NVIDIA H20 is $4.63."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses for AI models gather information manually by reaching out to authors, rather than automatically gathering information. Therefore, the statement that most carbon footprint analyses gather information automatically without needing to contact authors is false.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that most carbon footprint analyses for AI models gather information manually by reaching out to authors, rather than automatically gathering information. Therefore, the statement that most carbon footprint analyses gather information automatically without needing to contact authors is false."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of their total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of their total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The passage states that a minimum of 4 NVIDIA A100 GPUs with 80GB of memory are required to run inferences with the 65B LLaMA model.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The passage states that a minimum of 4 NVIDIA A100 GPUs with 80GB of memory are required to run inferences with the 65B LLaMA model."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal the sustainable deployment techniques involving quantization can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context states that experimental results reveal the sustainable deployment techniques involving quantization can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","","","answers","[]","is_blank","is_blank",""
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B used 30,000 H100 GPU hours for training with a $100k budget. From this, we can estimate the approximate cost per H100 GPU-hour for the JetMoE project.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B used 30,000 H100 GPU hours for training with a $100k budget. From this, we can estimate the approximate cost per H100 GPU-hour for the JetMoE project."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost can exceed 120% of the electricity cost for training the Llama-3.1 scale model in Altoona, Iowa.","122","percent","[""han2024""]","is_blank","is_blank","The context states that the total health cost can exceed 120% of the electricity cost for training the Llama-3.1 scale model in Altoona, Iowa."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that the hyperparameters for the JetMoE-8B model include a total parameter count (Ptotal) of 8B and an active parameter count (Pactive) of 2B.","8B","tokens","[""shen2024""]","is_blank","is_blank","The context states that the hyperparameters for the JetMoE-8B model include a total parameter count (Ptotal) of 8B and an active parameter count (Pactive) of 2B."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context states that as of January 2024, Amazon had 36 renewable energy projects announced in the United Kingdom, with a total capacity of 901 MW.","36","projects","[""amazon2023""]","is_blank","is_blank","The context states that as of January 2024, Amazon had 36 renewable energy projects announced in the United Kingdom, with a total capacity of 901 MW."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context shows that in 2023, 43.1% of Amazon's workforce in the United States across all levels identified as men.","43.1","percent","[""amazon2023""]","is_blank","is_blank","The context shows that in 2023, 43.1% of Amazon's workforce in the United States across all levels identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The context states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context states that when conducting sparse fine-tuning of the BlackMamba model with a batch size of 84 on an NVIDIA A40-48GB GPU, the total execution time is not provided. The information needed to answer this question is not available in the given context.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context states that when conducting sparse fine-tuning of the BlackMamba model with a batch size of 84 on an NVIDIA A40-48GB GPU, the total execution time is not provided. The information needed to answer this question is not available in the given context."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity metric', which is defined as the ratio of calculation to communication time, as a way to compare model suitability for distributed spot training and estimate training performance with additional spot VMs.","is_blank","is_blank","[""erben2023""]","is_blank","is_blank","The context introduces the 'granularity metric', which is defined as the ratio of calculation to communication time, as a way to compare model suitability for distributed spot training and estimate training performance with additional spot VMs."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, a coalition of Microsoft employees estimated that a single deal Microsoft struck with Exxon Mobil to use AI to expand oil and gas production could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets for the year.","640","times","[""luccioni2025a""]","is_blank","is_blank","According to the context, a coalition of Microsoft employees estimated that a single deal Microsoft struck with Exxon Mobil to use AI to expand oil and gas production could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines freshwater taken from ground or surface sources, either temporarily or permanently, and used for agricultural, industrial, or municipal uses as 'water withdrawal'.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context defines freshwater taken from ground or surface sources, either temporarily or permanently, and used for agricultural, industrial, or municipal uses as 'water withdrawal'."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that ""As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.""","5","years","[""stone2022""]","is_blank","is_blank","The context states that ""As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI."""
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context from the Deloitte report states that the average US household has 25 connected devices, including smartphones, tablets, laptops, gaming consoles, and other smart home devices.","25","devices","[""wu2021b""]","is_blank","is_blank","The context from the Deloitte report states that the average US household has 25 connected devices, including smartphones, tablets, laptops, gaming consoles, and other smart home devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context indicates that the range of energy consumption to pre-train large language models spans from as little as 0.8 MWh (OLMo 20M) to as much as 3,500 MWh (LLaMa 4 Scout).","is_blank","MWh","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the range of energy consumption to pre-train large language models spans from as little as 0.8 MWh (OLMo 20M) to as much as 3,500 MWh (LLaMa 4 Scout)."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The provided context indicates that one study estimates that 10-50 queries to the GPT-3 model consume around half a liter of water.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","The provided context indicates that one study estimates that 10-50 queries to the GPT-3 model consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context indicates that the JetMoE-8B-Chat model achieved a higher MT-Bench score than the Llama-2-13b-Chat model after alignment, demonstrating its superior performance.","6.681","score","[""shen2024""]","is_blank","is_blank","The context indicates that the JetMoE-8B-Chat model achieved a higher MT-Bench score than the Llama-2-13b-Chat model after alignment, demonstrating its superior performance."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needed to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed.","is_blank","responses","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needed to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings.","77","percent","[""griggs2024""]","is_blank","is_blank","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that the bare minimum hardware required to run LLaMA-65B inference without compression or quantization is either 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory.","4","A100_80GB_GPUs","[""rubei2025"", ""samsi2024""]","is_blank","is_blank","The context states that the bare minimum hardware required to run LLaMA-65B inference without compression or quantization is either 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context directly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.",">=15","round trips","[""han2024""]","is_blank","is_blank","The context directly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context, NVIDIA estimated that inference accounts for 80 to 90% of total ML cloud computing demand according to AWS, the largest global cloud provider.","1","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","According to the context, NVIDIA estimated that inference accounts for 80 to 90% of total ML cloud computing demand according to AWS, the largest global cloud provider."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study reported that in total, across all model experimentation and evaluation, 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The study reported that in total, across all model experimentation and evaluation, 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","","","is_blank","[]","is_blank","is_blank",""
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that despite GPT-4o mini's smaller architecture, it consumes approximately 20% more energy than the larger GPT-4o model due to deployment on older A100 GPU nodes rather than newer H100 hardware.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context indicates that despite GPT-4o mini's smaller architecture, it consumes approximately 20% more energy than the larger GPT-4o model due to deployment on older A100 GPU nodes rather than newer H100 hardware."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context does not provide the ground truth throughput for a dense Mixtral-CS-A100-40GB model when the batch size is 1. The closest information is that for a batch size of 1, the throughput of the dense Mixtral-CS model is 0.3 queries/sec.","is_blank","queries/sec","[""xia2024""]","is_blank","is_blank","The context does not provide the ground truth throughput for a dense Mixtral-CS-A100-40GB model when the batch size is 1. The closest information is that for a batch size of 1, the throughput of the dense Mixtral-CS model is 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context indicates that the direct release of environmental information for notable AI models peaked in 2022, at 10% of models, but then declined after the introduction of more commercial and proprietary models like ChatGPT that provided less transparency. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the direct release of environmental information for notable AI models peaked in 2022, at 10% of models, but then declined after the introduction of more commercial and proprietary models like ChatGPT that provided less transparency. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The passage states that in the Griggs et al. (2024) evaluation, the normalized on-demand hourly price for an H100 GPU was $7.5164.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The passage states that in the Griggs et al. (2024) evaluation, the normalized on-demand hourly price for an H100 GPU was $7.5164."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context indicates that during the training of GPT-3, an estimated 1287 MWh of electricity was consumed, which roughly equates to over 700 kiloliters (kL) of water used for cooling.","Over 700","liters of water","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context indicates that during the training of GPT-3, an estimated 1287 MWh of electricity was consumed, which roughly equates to over 700 kiloliters (kL) of water used for cooling."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the hourly cost of running Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs as over $5,200 per month. Converting this to an hourly rate, assuming 30 days per month, gives $5,200 / (30 days * 24 hours/day) = $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context provides the hourly cost of running Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs as over $5,200 per month. Converting this to an hourly rate, assuming 30 days per month, gives $5,200 / (30 days * 24 hours/day) = $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context suggests that to increase the BLEU score of a GPT-3 based language translation task from 5 to 40 requires a model 1,000 times larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context suggests that to increase the BLEU score of a GPT-3 based language translation task from 5 to 40 requires a model 1,000 times larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to the context, training and fine-tuning a large Transformer model with Neural Architecture Search produced 626,155 lbs of CO2. The context also provides information that this amount is similar to the lifetime carbon emissions of five US cars.","626,155","miles","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","According to the context, training and fine-tuning a large Transformer model with Neural Architecture Search produced 626,155 lbs of CO2. The context also provides information that this amount is similar to the lifetime carbon emissions of five US cars."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the energy consumption per inference for the BLOOMz models, as well as the total training and fine-tuning energy costs. Using this information, we can calculate that the total energy that would be consumed if every download of the BLOOMz-7B model (606,096 downloads) resulted in 1 million inferences would be approximately 60,609.6 MWh.","60,609.6","MWh","[""luccioni2024""]","is_blank","is_blank","The context provides the energy consumption per inference for the BLOOMz models, as well as the total training and fine-tuning energy costs. Using this information, we can calculate that the total energy that would be consumed if every download of the BLOOMz-7B model (606,096 downloads) resulted in 1 million inferences would be approximately 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","According to the context, the JetMoE-8B model was trained using 30,000 H100 GPU hours.","30,000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","According to the context, the JetMoE-8B model was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the current trend of 2.4x per year growth in training cost continues, the most expensive publicly announced model will cost about $1 billion by the start of 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that if the current trend of 2.4x per year growth in training cost continues, the most expensive publicly announced model will cost about $1 billion by the start of 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context states that the estimated training energy for GPT-3 was 1287 MWh.","1287","FLOPS","[""li2025b""]","is_blank","is_blank","The context states that the estimated training energy for GPT-3 was 1287 MWh."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that the bare minimum hardware required to run the LLaMA-65B model is 8 NVIDIA V100 GPUs with 32GB of RAM or 4 NVIDIA A100 GPUs with 80GB of memory.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context states that the bare minimum hardware required to run the LLaMA-65B model is 8 NVIDIA V100 GPUs with 32GB of RAM or 4 NVIDIA A100 GPUs with 80GB of memory."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput and FLOPs utilization rate for the final 101B training stage of the FLM-101B model. Using this information, the total computational work performed during this stage can be estimated in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context provides the throughput and FLOPs utilization rate for the final 101B training stage of the FLM-101B model. Using this information, the total computational work performed during this stage can be estimated in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context indicates that the top-1 accuracy associated with AlexNet 2012 on ImageNet is not directly provided. The context focuses on the performance of other models like ResNet-50 and does not mention AlexNet's performance.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context indicates that the top-1 accuracy associated with AlexNet 2012 on ImageNet is not directly provided. The context focuses on the performance of other models like ResNet-50 and does not mention AlexNet's performance."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context indicates that FLM-101B was trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, but does not specify the total number of A800 GPUs used.","is_blank","GPUs","[""li2025a""]","is_blank","is_blank","The context indicates that FLM-101B was trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, but does not specify the total number of A800 GPUs used."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that the authors quantified the emissions produced by training a Transformer model with Neural Architecture Search, finding it to be equivalent to the lifetime carbon emissions of five cars.","5","lifetimes","[""strubell2019"", ""dodge2022"", ""luccioni2023""]","is_blank","is_blank","The context states that the authors quantified the emissions produced by training a Transformer model with Neural Architecture Search, finding it to be equivalent to the lifetime carbon emissions of five cars."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR's RoBERTa was trained on 160GB of text, which is roughly 40 billion word-pieces, and required around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context states that FAIR's RoBERTa was trained on 160GB of text, which is roughly 40 billion word-pieces, and required around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that the 50 new on-site solar energy systems added in 2023 at Amazon facilities and stores with a total capacity of 58 MW are estimated to generate 123,000 MWh annually and avoid the equivalent of roughly 47,500 metric tons of CO2e compared to nonrenewable electricity sources.","47,500","metric tons","[""amazon2023""]","is_blank","is_blank","The context states that the 50 new on-site solar energy systems added in 2023 at Amazon facilities and stores with a total capacity of 58 MW are estimated to generate 123,000 MWh annually and avoid the equivalent of roughly 47,500 metric tons of CO2e compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context indicates that Megatron-LM was used to enable efficient large-scale training of language models on GPU clusters. The paper discusses distributing the model across multiple GPUs and using model parallelism techniques to train large models.","Megatron-LM","is_blank","[""narayanan2021"", ""li2025a"", ""fernandez2025""]","is_blank","is_blank","The context indicates that Megatron-LM was used to enable efficient large-scale training of language models on GPU clusters. The paper discusses distributing the model across multiple GPUs and using model parallelism techniques to train large models."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context states that using two nodes instead of one for the Llama 3.1 70B model resulted in the energy consumption increasing by a factor of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","The context states that using two nodes instead of one for the Llama 3.1 70B model resulted in the energy consumption increasing by a factor of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context estimates that a single query to GPT-3 consumes around 0.5 liters of water. The context also notes that 10-50 queries on GPT-3 would consume around 0.5 liters of water.","0.5","gallons of water","[""luccioni2025a""]","is_blank","is_blank","The context estimates that a single query to GPT-3 consumes around 0.5 liters of water. The context also notes that 10-50 queries on GPT-3 would consume around 0.5 liters of water."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context estimates that 700 million daily GPT-4o queries would result in annual electricity usage comparable to 35,000 U.S. residential households, which consume around 377,685 MWh annually.","35,000","homes","[""jegham2025""]","is_blank","is_blank","The context estimates that 700 million daily GPT-4o queries would result in annual electricity usage comparable to 35,000 U.S. residential households, which consume around 377,685 MWh annually."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","1","percent","[""luccioni2025a""]","is_blank","is_blank","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","Based on the context, traditional models performed significantly worse than large language models (LLMs) on the Yelp sentiment analysis benchmark. The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment'.","0","is_blank","[""zschache2025""]","is_blank","is_blank","Based on the context, traditional models performed significantly worse than large language models (LLMs) on the Yelp sentiment analysis benchmark. The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment'."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that Facebook's data centers achieved a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than typical data centers.","1.10","PUE","[""wu2021a""]","is_blank","is_blank","The context states that Facebook's data centers achieved a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than typical data centers."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion.","772 billion","queries","[""jegham2025""]","is_blank","is_blank","The context states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context states that JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard, but it does not provide the final average score for the JetMoE-8B model.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context states that JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard, but it does not provide the final average score for the JetMoE-8B model."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. The context discusses the carbon footprint and energy consumption of AI in general, but does not provide data specifically on the number of training runs on renewable power.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. The context discusses the carbon footprint and energy consumption of AI in general, but does not provide data specifically on the number of training runs on renewable power."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. The AI Act currently has an exemption for open-source models, which the authors argue should be eliminated.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. The AI Act currently has an exemption for open-source models, which the authors argue should be eliminated."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","","","PUE","[]","is_blank","is_blank",""
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that when the batch size increases to 32 for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","5.312","GB","[""kim2025""]","is_blank","is_blank","The context states that when the batch size increases to 32 for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that for the four notable models studied in-depth, R&D staff costs (including equity) accounted for 29% to 49% of the total amortized cost.","is_blank","percent","[""cottier2024""]","is_blank","is_blank","The context states that for the four notable models studied in-depth, R&D staff costs (including equity) accounted for 29% to 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context indicates that the energy consumption during the inference runs was measured using the CodeCarbon software package.","CodeCarbon","is_blank","[""morrison2025"", ""zschache2025""]","is_blank","is_blank","The context indicates that the energy consumption during the inference runs was measured using the CodeCarbon software package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context reveals that 53% of the 100 analyzed news articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context reveals that 53% of the 100 analyzed news articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context describes the Compute Time Calibration Function (CTCF) which is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The context describes the Compute Time Calibration Function (CTCF) which is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards for the LLaMA-65B model always increases the energy consumption per second, even at the same batch size. This suggests that increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context states that increasing the number of shards for the LLaMA-65B model always increases the energy consumption per second, even at the same batch size. This suggests that increasing the number of GPU shards increased the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context indicates that mining 1 kg of rare earth materials consumes about 11 kL of water, and an H100 GPU contains 0.1% rare earth metal by mass. Therefore, the estimated water consumption of mining rare earth materials to manufacture a single H100 GPU is approximately 2.2 liters.","2.2","kL","[""morrison2025""]","is_blank","is_blank","The context indicates that mining 1 kg of rare earth materials consumes about 11 kL of water, and an H100 GPU contains 0.1% rare earth metal by mass. Therefore, the estimated water consumption of mining rare earth materials to manufacture a single H100 GPU is approximately 2.2 liters."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context does not mention any EU rules that require open-source general-purpose AI models to report their energy consumption to authorities. The text discusses policy proposals to expand reporting requirements, but does not state that such requirements currently exist.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The context does not mention any EU rules that require open-source general-purpose AI models to report their energy consumption to authorities. The text discusses policy proposals to expand reporting requirements, but does not state that such requirements currently exist."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs (Power Purchase Agreements) purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs (Power Purchase Agreements) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides a 2023 estimate of the total public health cost of U.S. data centers as around $6.7 billion, or $47.5 per household, which is about 44% of the data centers' total electricity cost.","6700000000","USD","[""han2024""]","is_blank","is_blank","The context provides a 2023 estimate of the total public health cost of U.S. data centers as around $6.7 billion, or $47.5 per household, which is about 44% of the data centers' total electricity cost."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context indicates that the energy consumption of the o3 model is 12.222 Wh for a long prompt, which is about 2.5 times greater than the 4.833 Wh consumed by the GPT-4.1 model for a long prompt.","2.5","multiplier","[""jegham2025""]","is_blank","is_blank","The context indicates that the energy consumption of the o3 model is 12.222 Wh for a long prompt, which is about 2.5 times greater than the 4.833 Wh consumed by the GPT-4.1 model for a long prompt."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) for the short-context Arena dataset.","1577","percent","[""griggs2024""]","is_blank","is_blank","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) for the short-context Arena dataset."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context does not provide any information about the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B. The information available is about the carbon footprint statistics of FLM-101B and other well-known LLMs, but there are no specific numbers given for the pre-training of FLM-101B.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B. The information available is about the carbon footprint statistics of FLM-101B and other well-known LLMs, but there are no specific numbers given for the pre-training of FLM-101B."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides an estimate that fine-tuning a sparse Mixtral model with 2M queries on an NVIDIA H100 GPU would cost $3460. However, the context does not specify the execution time of this sparse fine-tuning process.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context provides an estimate that fine-tuning a sparse Mixtral model with 2M queries on an NVIDIA H100 GPU would cost $3460. However, the context does not specify the execution time of this sparse fine-tuning process."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context indicates that the public health costs of AI are not evenly distributed across communities in the U.S., but rather disproportionately affect certain low-income communities.","0","is_blank","[""han2024""]","is_blank","is_blank","The context indicates that the public health costs of AI are not evenly distributed across communities in the U.S., but rather disproportionately affect certain low-income communities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that as a result of Moore's law scaling and architectural optimization, GPU theoretical performance per watt doubles every 3-4 years.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that as a result of Moore's law scaling and architectural optimization, GPU theoretical performance per watt doubles every 3-4 years."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that the framework employs 4-bit quantization through the Ollama open-source platform to achieve efficient local inference and reduce computational requirements.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context mentions that the framework employs 4-bit quantization through the Ollama open-source platform to achieve efficient local inference and reduce computational requirements."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that when running experiments on spot instances, the authors chose the Backblaze S3 storage provider to access datasets on-demand via shards in tar format with the WebDataset library, as spot instances can be terminated at any time.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that when running experiments on spot instances, the authors chose the Backblaze S3 storage provider to access datasets on-demand via shards in tar format with the WebDataset library, as spot instances can be terminated at any time."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates that the relationship between runtime and energy consumption was found to be nearly linear. It states 'Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.'","1","is_blank","[""zschache2025""]","is_blank","is_blank","The context indicates that the relationship between runtime and energy consumption was found to be nearly linear. It states 'Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.'"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context indicates that the AI Environmental Impacts Act of 2024 was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024.","Senator Edward J. Markey (D-MA)","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Environmental Impacts Act of 2024 was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context states that the price per chip for an NVIDIA H100 is $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The context states that the price per chip for an NVIDIA H100 is $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide any information about the estimated average GPU lifetime in AI data centers in 2024. The provided information is about hardware failures and GPU shipment trends, but does not mention the average GPU lifetime.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the estimated average GPU lifetime in AI data centers in 2024. The provided information is about hardware failures and GPU shipment trends, but does not mention the average GPU lifetime."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that the bare minimum hardware requirement for running LLaMA-13B inference is 2 NVIDIA V100 32GB GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context states that the bare minimum hardware requirement for running LLaMA-13B inference is 2 NVIDIA V100 32GB GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reported training their Gemma family of language models emitted 1247.61 tons CO2e, over 4x the 'five cars' estimate from the earlier study.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context states that Google reported training their Gemma family of language models emitted 1247.61 tons CO2e, over 4x the 'five cars' estimate from the earlier study."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The provided context indicates that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is 336 hours of training time.","336","hours","[""strubell2019""]","is_blank","is_blank","The provided context indicates that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is 336 hours of training time."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water withdrawal and consumption for electricity generation, which are 43.8 L/kWh and 3.1 L/kWh respectively.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context provides the U.S. national average water withdrawal and consumption for electricity generation, which are 43.8 L/kWh and 3.1 L/kWh respectively."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context indicates that Google reported a PUE of 1.10 across its hyperscale data centers in 2021.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","The context indicates that Google reported a PUE of 1.10 across its hyperscale data centers in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The provided context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","is_blank","The provided context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that the model predicted the cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU to be $3460.","3460","USD","[""xia2024""]","is_blank","is_blank","The context states that the model predicted the cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU to be $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that in a typical data center, the GPU accounts for almost 3/4 (74%) of the total electricity consumption among the key hardware components measured.","74","percent","[""dodge2022""]","is_blank","is_blank","The context indicates that in a typical data center, the GPU accounts for almost 3/4 (74%) of the total electricity consumption among the key hardware components measured."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The provided context indicates that the training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes with 96 total H100 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The provided context indicates that the training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes with 96 total H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","is_blank","The context states that during active training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context indicates that the average human life emits 11,023 lbs of CO2e in one year globally.","11,023","lbs","[""strubell2019""]","is_blank","is_blank","The context indicates that the average human life emits 11,023 lbs of CO2e in one year globally."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context indicates that for the smaller LLaMA 7B and 13B models, the NVIDIA A100 GPU provided around a 2 times (for 7B) to 1.25 times (for 13B) increase in inference latency compared to the V100 GPU.","1.25-2","multiplier","[""samsi2024""]","is_blank","is_blank","The context indicates that for the smaller LLaMA 7B and 13B models, the NVIDIA A100 GPU provided around a 2 times (for 7B) to 1.25 times (for 13B) increase in inference latency compared to the V100 GPU."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context indicates that the Max-Performance instance (g6e.xlarge) was about 280% more expensive than InferSave's top choice instance.","280","percent","[""kim2025""]","is_blank","is_blank","The context indicates that the Max-Performance instance (g6e.xlarge) was about 280% more expensive than InferSave's top choice instance."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context does not directly compare the energy efficiency of different GPU architectures for models generating only a single classification token. The information provided focuses more on the overall energy consumption trends across different model types and tasks.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not directly compare the energy efficiency of different GPU architectures for models generating only a single classification token. The information provided focuses more on the overall energy consumption trends across different model types and tasks."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context clearly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context clearly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context states that 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'"
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides details on the average system power per processor for the TPU v2 and V100 GPU. It states that the TPU v2 has a performance ratio of 9% compared to the NVIDIA H100, and the V100 has a performance of 125 TFLOPS (FP16). Using this information, we can calculate that the average system power per processor for the TPU v2 is lower than the V100.","is_blank","Watts","[""cottier2024""]","is_blank","is_blank","The context provides details on the average system power per processor for the TPU v2 and V100 GPU. It states that the TPU v2 has a performance ratio of 9% compared to the NVIDIA H100, and the V100 has a performance of 125 TFLOPS (FP16). Using this information, we can calculate that the average system power per processor for the TPU v2 is lower than the V100."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context indicates that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700,000","liters","[""li2025b""]","is_blank","is_blank","The context indicates that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to Figure 8 in the paper, the model that ranked highest in the recent eco-efficiency analysis using DEA was o3-mini, which achieved the highest cross-efficiency score of 0.884.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","According to Figure 8 in the paper, the model that ranked highest in the recent eco-efficiency analysis using DEA was o3-mini, which achieved the highest cross-efficiency score of 0.884."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context indicates that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context indicates that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context indicates that intra-zone scaling with T4 GPUs did not achieve linear per-GPU speedup for CV models. The per-GPU contribution to the speedup decreased as more GPUs were added, and the models showed a decreasing per-GPU speedup due to smaller granularity with more GPUs.","0","is_blank","[""erben2023""]","is_blank","is_blank","The context indicates that intra-zone scaling with T4 GPUs did not achieve linear per-GPU speedup for CV models. The per-GPU contribution to the speedup decreased as more GPUs were added, and the models showed a decreasing per-GPU speedup due to smaller granularity with more GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2,048","tokens","[""li2025a""]","is_blank","is_blank","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context suggests that LLMs generally have lower power draw during inference than diffusion models, because LLM decoding is less compute-intensive and does not fully utilize the GPU's compute resources, whereas diffusion models are significantly more compute-intensive and draw near the maximum power of the GPU.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context suggests that LLMs generally have lower power draw during inference than diffusion models, because LLM decoding is less compute-intensive and does not fully utilize the GPU's compute resources, whereas diffusion models are significantly more compute-intensive and draw near the maximum power of the GPU."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context indicates that in 2023, 31.3% of Amazon's People Managers globally identified as women.","31.3","percent","[""amazon2023""]","is_blank","is_blank","The context indicates that in 2023, 31.3% of Amazon's People Managers globally identified as women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of total amortized model development costs attributed to computing hardware for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) ranges from 61% to 76%.","61-76","percent","[""cottier2024""]","is_blank","is_blank","The context states that when excluding equity, the fraction of total amortized model development costs attributed to computing hardware for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) ranges from 61% to 76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context indicates that after optimization, while there were significant reductions in carbon emissions across all models, the impact on performance metrics such as accuracy, F1 score, recall, and precision was mixed, with some metrics improving and others declining slightly. The text specifically states that ""metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.""","0","is_blank","[""khan2025""]","is_blank","is_blank","The context indicates that after optimization, while there were significant reductions in carbon emissions across all models, the impact on performance metrics such as accuracy, F1 score, recall, and precision was mixed, with some metrics improving and others declining slightly. The text specifically states that ""metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."""
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The provided context states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The provided context states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context indicates that a recent study suggests the global AI could consume 85 - 134 TWh of electricity in 2027.","85 - 134","TWh","[""li2025b""]","is_blank","is_blank","The context indicates that a recent study suggests the global AI could consume 85 - 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe.","150","packages","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context provides the total number of requests evaluated for the online and offline inference workloads. For the online inference workload, 3000 requests were evaluated. For the offline inference workload, 1000 requests were evaluated. Adding these two numbers gives the total number of tokens processed.","4000","tokens","[""kim2025""]","is_blank","is_blank","The context provides the total number of requests evaluated for the online and offline inference workloads. For the online inference workload, 3000 requests were evaluated. For the offline inference workload, 1000 requests were evaluated. Adding these two numbers gives the total number of tokens processed."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications, such as those used in oil and gas exploration.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications, such as those used in oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to the figure, for the short DenseNet 201 experiment, the Flexible Start algorithm can achieve a maximum emission reduction of up to 80% in the West US region.","80","percent","[""dodge2022""]","is_blank","is_blank","According to the figure, for the short DenseNet 201 experiment, the Flexible Start algorithm can achieve a maximum emission reduction of up to 80% in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context indicates that the most energy-intensive task, image generation, uses on average 2.907 kWh of energy per 1,000 inferences, while the least energy-intensive task, text classification, uses only 0.002 kWh per 1,000 inferences. This means the energy required for image generation exceeds that of text classification by a factor of over 1450.","1450","times","[""luccioni2024""]","is_blank","is_blank","The context indicates that the most energy-intensive task, image generation, uses on average 2.907 kWh of energy per 1,000 inferences, while the least energy-intensive task, text classification, uses only 0.002 kWh per 1,000 inferences. This means the energy required for image generation exceeds that of text classification by a factor of over 1450."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The provided context indicates that in Yelp sentiment analysis benchmarks, traditional models performed considerably worse than large language models (LLMs), justifying the energy costs of deploying LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The provided context indicates that in Yelp sentiment analysis benchmarks, traditional models performed considerably worse than large language models (LLMs), justifying the energy costs of deploying LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that as of January 2024, Amazon had announced 244 new renewable energy projects in the United States, with a total capacity of 17,706 MW.","244","projects","[""amazon2023""]","is_blank","is_blank","The context states that as of January 2024, Amazon had announced 244 new renewable energy projects in the United States, with a total capacity of 17,706 MW."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context states that Amazon's renewable energy projects generated enough renewable energy to match 100% of the electricity consumed by 22 AWS data center regions in 2023, up from 19 regions in 2022.","1","percent","[""amazon2023""]","is_blank","is_blank","The context states that Amazon's renewable energy projects generated enough renewable energy to match 100% of the electricity consumed by 22 AWS data center regions in 2023, up from 19 regions in 2022."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context recommends that for AI systems, energy consumption should be reported at the cumulative server level, rather than computational resources, to improve accuracy and comparability.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The context recommends that for AI systems, energy consumption should be reported at the cumulative server level, rather than computational resources, to improve accuracy and comparability."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that the GPU accounts for almost 3/4 (74%) of the total electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU.","74","percent","[""dodge2022""]","is_blank","is_blank","The context states that the GPU accounts for almost 3/4 (74%) of the total electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process has led to a 28.5% operational energy footprint reduction over the two-year time period (2019 to 2021).","28.5","percent","[""wu2021a""]","is_blank","is_blank","The context states that the iterative optimization process has led to a 28.5% operational energy footprint reduction over the two-year time period (2019 to 2021)."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide any clear information on the total number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The provided context focuses on the growth trends of AI in terms of data, models, and infrastructure capacity, but does not quantify the fiber optic cable deployment for AI.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide any clear information on the total number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The provided context focuses on the growth trends of AI in terms of data, models, and infrastructure capacity, but does not quantify the fiber optic cable deployment for AI."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that the estimated hardware acquisition cost to train GPT-4 was $800M, which is one to two orders of magnitude higher than the amortized hardware and energy cost of $40M.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that the estimated hardware acquisition cost to train GPT-4 was $800M, which is one to two orders of magnitude higher than the amortized hardware and energy cost of $40M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications as well as improving the environmental sustainability of AI approaches themselves.","1","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications as well as improving the environmental sustainability of AI approaches themselves."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that using the analytical model, the maximum batch sizes supported for fine-tuning Mixtral on GPUs with 100GB and 120GB memory capacities are 28 and 35 samples, respectively.","35","samples","[""xia2024""]","is_blank","is_blank","The context states that using the analytical model, the maximum batch sizes supported for fine-tuning Mixtral on GPUs with 100GB and 120GB memory capacities are 28 and 35 samples, respectively."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context indicates that when an LLM inference server is overloaded, the Swapping preemption mechanism consistently consumes less energy than the Recomputation mechanism. Recomputation has extra computation when restoring requests, while Swapping only copies data without running computation, and the energy consumption of computation is larger than memory operations.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context indicates that when an LLM inference server is overloaded, the Swapping preemption mechanism consistently consumes less energy than the Recomputation mechanism. Recomputation has extra computation when restoring requests, while Swapping only copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The 2024 environmental report from Google stated that the company reported a 20% increase in greenhouse gas emissions since 2019.","20","percent","[""jegham2025"", ""luccioni2025a""]","is_blank","is_blank","The 2024 environmental report from Google stated that the company reported a 20% increase in greenhouse gas emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to McKinsey projections, data centers are anticipated to account for 6.7–12.0% of U.S. national electricity consumption in 2030.","6.7-12.0","percent","[""han2024""]","is_blank","is_blank","According to McKinsey projections, data centers are anticipated to account for 6.7–12.0% of U.S. national electricity consumption in 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that the 'Pause and Resume' optimization for training the 6B parameter transformer could potentially reduce emissions by up to 30% compared to an unoptimized run.","30","percent","[""dodge2022""]","is_blank","is_blank","The context indicates that the 'Pause and Resume' optimization for training the 6B parameter transformer could potentially reduce emissions by up to 30% compared to an unoptimized run."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","According to the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context states that a 2019 article by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions.","626,155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context states that a 2019 article by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context states that the estimated training energy of the full GPT-3 model was 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","is_blank","The context states that the estimated training energy of the full GPT-3 model was 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context indicates that the Mixture-of-Experts (MoE) layer is often a targeted layer for optimization to enhance the performance of LLM fine-tuning, as it accounts for a significant portion of the overall execution time.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context indicates that the Mixture-of-Experts (MoE) layer is often a targeted layer for optimization to enhance the performance of LLM fine-tuning, as it accounts for a significant portion of the overall execution time."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes supported for fine-tuning Mixtral are 28 and 35, respectively.","35","samples","[""xia2024""]","is_blank","is_blank","The context states that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes supported for fine-tuning Mixtral are 28 and 35, respectively."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide any information about the number of hectares of land occupied by new AI data centers globally in 2022. The information given is about the projected water and energy consumption of AI systems and data centers, but does not mention specific land area occupied.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the number of hectares of land occupied by new AI data centers globally in 2022. The information given is about the projected water and energy consumption of AI systems and data centers, but does not mention specific land area occupied."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits just 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits just 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context indicates that the difference in emissions between the most and least efficient regions for training a BERT model is about 19,000 grams (26,000 grams for the least efficient region minus 7,000 grams for the most efficient region).","19000","grams","[""dodge2022""]","is_blank","is_blank","The context indicates that the difference in emissions between the most and least efficient regions for training a BERT model is about 19,000 grams (26,000 grams for the least efficient region minus 7,000 grams for the most efficient region)."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context indicates that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens. The context states that training the OLMo 60M model consumed 1.6 kL of water, which is equivalent to 5 days of water usage by the average person in the US.","5","days","[""morrison2025""]","is_blank","is_blank","The context provides the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens. The context states that training the OLMo 60M model consumed 1.6 kL of water, which is equivalent to 5 days of water usage by the average person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide a clear answer to the question about Google's DeepMind AlphaFold servers' freshwater consumption in 2023. The passage discusses the general water consumption of AI and data centers, but does not mention specifics about DeepMind AlphaFold.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about Google's DeepMind AlphaFold servers' freshwater consumption in 2023. The passage discusses the general water consumption of AI and data centers, but does not mention specifics about DeepMind AlphaFold."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context indicates that adding compute resources to accelerate the MoE layers can decrease the cost of LLM fine-tuning. The authors state 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers'.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context indicates that adding compute resources to accelerate the MoE layers can decrease the cost of LLM fine-tuning. The authors state 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers'."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is not explicitly provided in the given context. The context only mentions the total cost of FLM-101B as 52.76 zettaFLOPs, which includes 28.22 zettaFLOPs for the English portion and 24.54 zettaFLOPs for the Chinese portion.","is_blank","kWh","[""li2025a""]","is_blank","is_blank","The total energy consumption for training the FLM-101B model is not explicitly provided in the given context. The context only mentions the total cost of FLM-101B as 52.76 zettaFLOPs, which includes 28.22 zettaFLOPs for the English portion and 24.54 zettaFLOPs for the Chinese portion."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to the context, a recent report projects the total public health impact of U.S. data centers could reach $20.9 billion in 2028 under the high-growth scenario, which would rival or even exceed the health burden of on-road emissions in the largest U.S. state, California.","20.9 billion","USD","[""han2024""]","is_blank","is_blank","According to the context, a recent report projects the total public health impact of U.S. data centers could reach $20.9 billion in 2028 under the high-growth scenario, which would rival or even exceed the health burden of on-road emissions in the largest U.S. state, California."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The paper provides an estimated total cost of $3460 for fine-tuning a sparse Mixtral model using the OpenOrca dataset of over 2M queries on an NVIDIA H100 GPU.","3460","USD","[""xia2024""]","is_blank","is_blank","The paper provides an estimated total cost of $3460 for fine-tuning a sparse Mixtral model using the OpenOrca dataset of over 2M queries on an NVIDIA H100 GPU."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context indicates that the maximum batch size supported for fine-tuning the Mixtral model on an NVIDIA A40 GPU with 48GB memory is 4 samples when using a sparse setup.","4","samples","[""xia2024""]","is_blank","is_blank","The context indicates that the maximum batch size supported for fine-tuning the Mixtral model on an NVIDIA A40 GPU with 48GB memory is 4 samples when using a sparse setup."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context indicates that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with an NVIDIA H100 GPU at a cost of $3460.","3460","seconds","[""xia2024""]","is_blank","is_blank","The context indicates that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with an NVIDIA H100 GPU at a cost of $3460."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context clearly advocates against using GPU-level or other component-based power consumption tracking for overall energy measurements, as it substantially underrepresents the actual energy consumption.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly advocates against using GPU-level or other component-based power consumption tracking for overall energy measurements, as it substantially underrepresents the actual energy consumption."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context indicates that in a 2023 article estimating the carbon footprint of the BLOOM 176B parameter language model, training accounted for only half of the model's overall emissions.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context indicates that in a 2023 article estimating the carbon footprint of the BLOOM 176B parameter language model, training accounted for only half of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","is_blank","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","Based on the provided context, GPT-3 would consume approximately 10 to 50 medium-length responses worth of a 500ml bottle of water during deployment, depending on when and where it is deployed.","10 to 50","requests","[""li2025b""]","is_blank","is_blank","Based on the provided context, GPT-3 would consume approximately 10 to 50 medium-length responses worth of a 500ml bottle of water during deployment, depending on when and where it is deployed."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context indicates that one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context indicates that one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not provide the score that the JetMoE-8B model achieved on the GSM8k benchmark.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide the score that the JetMoE-8B model achieved on the GSM8k benchmark."
