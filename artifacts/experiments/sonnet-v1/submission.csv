"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly states that they present 'the ML.ENERGY Benchmark', which is described as a benchmark suite and tool for measuring inference energy consumption under realistic service environments.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly states that they present 'the ML.ENERGY Benchmark', which is described as a benchmark suite and tool for measuring inference energy consumption under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","Based on the context from wu2021a which shows a graph of CO2e emissions for various models, GShard-600B's training emissions can be seen to be approximately 0.4 million kg CO2e (400,000 kg CO2e). The bar chart clearly shows the GShard-600B model's training footprint value.","400000","tCO2e","[""wu2021a""]","is_blank","is_blank","Based on the context from wu2021a which shows a graph of CO2e emissions for various models, GShard-600B's training emissions can be seen to be approximately 0.4 million kg CO2e (400,000 kg CO2e). The bar chart clearly shows the GShard-600B model's training footprint value."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","According to Table 3 in the context, the LLaMA-33B model has a model parameter size of 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","According to Table 3 in the context, the LLaMA-33B model has a model parameter size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The provided context snippets do not contain any specific data about the total worldwide electricity consumption of Google Cloud TPU pods in 2023. While there are discussions about AI model training energy consumption and data center electricity usage, none specifically address Google Cloud TPU pod electricity consumption for 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The provided context snippets do not contain any specific data about the total worldwide electricity consumption of Google Cloud TPU pods in 2023. While there are discussions about AI model training energy consumption and data center electricity usage, none specifically address Google Cloud TPU pod electricity consumption for 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","According to the context from wu2021b, hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020, as measured by Power Usage Effectiveness (PUE).","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context from wu2021b, hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020, as measured by Power Usage Effectiveness (PUE)."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the context, GPT-3 needs to consume approximately one 500mL bottle of water for every 10-50 medium-length responses, with the exact amount varying based on when and where the model is deployed.","0.02-0.1","500 mL bottles","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to consume approximately one 500mL bottle of water for every 10-50 medium-length responses, with the exact amount varying based on when and where the model is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","According to the context, 75% of CVPR papers target accuracy while 20% target efficiency. Therefore, the difference between these percentages is 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","According to the context, 75% of CVPR papers target accuracy while 20% target efficiency. Therefore, the difference between these percentages is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","According to the context, while the AI Act mandates energy consumption reporting, this information is restricted to authorities and is not accessible to downstream providers or the general public due to confidentiality clauses in Articles 21(3), 53(7), and 78(1).","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, while the AI Act mandates energy consumption reporting, this information is restricted to authorities and is not accessible to downstream providers or the general public due to confidentiality clauses in Articles 21(3), 53(7), and 78(1)."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to the analytical model presented in the paper, for a projected GPU memory capacity of 100GB, the maximum batch size supported for fine-tuning Mixtral will be 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","According to the analytical model presented in the paper, for a projected GPU memory capacity of 100GB, the maximum batch size supported for fine-tuning Mixtral will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for LLaMA-7B model, the A100 GPU provided a 2x increase in inference throughput compared to the V100 GPU across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","is_blank","According to the context, for LLaMA-7B model, the A100 GPU provided a 2x increase in inference throughput compared to the V100 GPU across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","According to Table 1 in the context, for U.S. Average data centers, GPT-3's training water consumption is shown as 5.439 million liters total (0.708 million liters on-site water + 4.731 million liters off-site water).","5.439","liters","[""li2025b""]","is_blank","is_blank","According to Table 1 in the context, for U.S. Average data centers, GPT-3's training water consumption is shown as 5.439 million liters total (0.708 million liters on-site water + 4.731 million liters off-site water)."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","According to the context from ebert2024, the authors explicitly state that sustainability impact assessments (SIAs) 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety' because the carbon footprint of AI models is often unrelated to their risk classification.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context from ebert2024, the authors explicitly state that sustainability impact assessments (SIAs) 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety' because the carbon footprint of AI models is often unrelated to their risk classification."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","According to Amazon's 2023 Sustainability Report, AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, down from 0.19 L/kWh in 2022, representing a 5% improvement year over year.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","According to Amazon's 2023 Sustainability Report, AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, down from 0.19 L/kWh in 2022, representing a 5% improvement year over year."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","According to the context, local inference was specifically emphasized as a sustainability measure because it reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers, as explicitly stated in the framework overview.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, local inference was specifically emphasized as a sustainability measure because it reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers, as explicitly stated in the framework overview."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context strongly supports that tracking runtime is important for cost estimation, as shown by multiple examples linking training time to compute costs and carbon emissions. For example, Strubell 2019 provides detailed analysis of training costs based on runtime hours, and Kim 2025 discusses how runtime tracking is essential for GPU instance selection and cost optimization.","1","is_blank","[""strubell2019"", ""kim2025""]","is_blank","is_blank","The context strongly supports that tracking runtime is important for cost estimation, as shown by multiple examples linking training time to compute costs and carbon emissions. For example, Strubell 2019 provides detailed analysis of training costs based on runtime hours, and Kim 2025 discusses how runtime tracking is essential for GPU instance selection and cost optimization."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the Chen et al. 2024 study, enabling automated resource utilization overlapping for the LLaMA-65B model achieved up to a 13.2% improvement in performance through latency reduction, with the speedup being particularly notable for larger batch sizes.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to the Chen et al. 2024 study, enabling automated resource utilization overlapping for the LLaMA-65B model achieved up to a 13.2% improvement in performance through latency reduction, with the speedup being particularly notable for larger batch sizes."
"q164","How much does an elephant weigh?","The context contains mentions of Asian elephants in a wildlife conservation zone but does not provide any specific weight information for elephants.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context contains mentions of Asian elephants in a wildlife conservation zone but does not provide any specific weight information for elephants."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","Based on the provided context snippets, we cannot definitively determine which of the five specific models mentioned in the question (Meena, T5, GPT-3, GShard-600B, or Switch Transformer) has the highest energy consumption. While the context mentions T5 with an energy consumption of 85.7 MWh and GPT-3 with 1,287 MWh, not all models from the question are included in the data, making a complete comparison impossible.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, we cannot definitively determine which of the five specific models mentioned in the question (Meena, T5, GPT-3, GShard-600B, or Switch Transformer) has the highest energy consumption. While the context mentions T5 with an energy consumption of 85.7 MWh and GPT-3 with 1,287 MWh, not all models from the question are included in the data, making a complete comparison impossible."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","According to the context, training the BERT base model produces 626,155 pounds of CO₂ emissions as reported by Strubell et al. To put this in perspective relative to an average American life, the context states that an average American life produces 36,156 pounds of CO₂ per year.","17.3","days","[""luccioni2025b"", ""strubell2019""]","is_blank","is_blank","According to the context, training the BERT base model produces 626,155 pounds of CO₂ emissions as reported by Strubell et al. To put this in perspective relative to an average American life, the context states that an average American life produces 36,156 pounds of CO₂ per year."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","Based on the provided context snippets, we cannot definitively determine if the Transformer architecture eventually outperforms the Evolved Transformer architecture on the WMT'24 EN-DE BLEU task as model sizes grow. While some documents mention Transformer and Evolved Transformer architectures, none specifically compare their performance scaling behavior on this task.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, we cannot definitively determine if the Transformer architecture eventually outperforms the Evolved Transformer architecture on the WMT'24 EN-DE BLEU task as model sizes grow. While some documents mention Transformer and Evolved Transformer architectures, none specifically compare their performance scaling behavior on this task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context shows the BurstGPT dataset has statistics with 256.80 ± 242.27 mean input sequence length and appears to be used for testing energy-efficient LLM inference, but the context does not explicitly state it contains 5,842 labeled entries.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context shows the BurstGPT dataset has statistics with 256.80 ± 242.27 mean input sequence length and appears to be used for testing energy-efficient LLM inference, but the context does not explicitly state it contains 5,842 labeled entries."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","According to the context, for computer vision (CV) tasks, 8xT4 instances are shown to be 58% cheaper than a DGX-2 while being 37% slower. However, for NLP tasks with low granularity, the DGX-2 is shown to be the best value proposition, with 8xT4 experiments being more expensive due to internal egress costs taking up more than half the costs.","FALSE","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, for computer vision (CV) tasks, 8xT4 instances are shown to be 58% cheaper than a DGX-2 while being 37% slower. However, for NLP tasks with low granularity, the DGX-2 is shown to be the best value proposition, with 8xT4 experiments being more expensive due to internal egress costs taking up more than half the costs."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context from document luccioni2025b, the 2023 US Executive Order on AI did not mention AI's greenhouse gas emissions or energy usage.","1","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context from document luccioni2025b, the 2023 US Executive Order on AI did not mention AI's greenhouse gas emissions or energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to the German 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy initially, increasing to 100% renewable energy by January 1, 2027 (Section 11). This specific requirement is mentioned in the context and makes the statement true.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the German 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy initially, increasing to 100% renewable energy by January 1, 2027 (Section 11). This specific requirement is mentioned in the context and makes the statement true."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","Looking at ACL papers specifically, the context shows that 90% of ACL papers targeted accuracy alone, and only a very small portion (10%) targeted efficiency alone. The papers that targeted both accuracy and efficiency are not explicitly mentioned for ACL, making it impossible to determine the exact number.","is_blank","papers","[""is_blank""]","is_blank","is_blank","Looking at ACL papers specifically, the context shows that 90% of ACL papers targeted accuracy alone, and only a very small portion (10%) targeted efficiency alone. The papers that targeted both accuracy and efficiency are not explicitly mentioned for ACL, making it impossible to determine the exact number."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Multiple sources indicate that inference can account for up to 90% of a model's total lifecycle energy use. This is supported by Jegham 2025 which directly states this estimate, and corroborated by AWS's estimate that inference makes up 80-90% of total ML cloud computing demand.","90","percent","[""jegham2025"", ""luccioni2024""]","is_blank","is_blank","Multiple sources indicate that inference can account for up to 90% of a model's total lifecycle energy use. This is supported by Jegham 2025 which directly states this estimate, and corroborated by AWS's estimate that inference makes up 80-90% of total ML cloud computing demand."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","Based on the context, the AI Act requires energy consumption reporting only during model development/training for general-purpose AI models, but not during inference. The source explicitly states this is 'a significant gap as it only covers the energy used during the model's development phase, but leaves out the inference phase.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","Based on the context, the AI Act requires energy consumption reporting only during model development/training for general-purpose AI models, but not during inference. The source explicitly states this is 'a significant gap as it only covers the energy used during the model's development phase, but leaves out the inference phase.'"
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context clearly indicates that the AI Act does not currently mandate disclosure of energy consumption during the inference phase, with multiple references explicitly stating this is a 'crucial omission' and a gap in the current legislation.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly indicates that the AI Act does not currently mandate disclosure of energy consumption during the inference phase, with multiple references explicitly stating this is a 'crucial omission' and a gap in the current legislation."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","According to the context, new data centers dedicated to AI training rely on liquid cooling, not air cooling, due to the high server power densities.","0","is_blank","[""li2025b""]","is_blank","is_blank","According to the context, new data centers dedicated to AI training rely on liquid cooling, not air cooling, due to the high server power densities."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to Wu et al. (2021), platform-level caching improved power efficiency by 6.7x for the cross-lingual Transformer language model, starting from a CPU server baseline. This improvement was achieved by pre-computing and caching frequently accessed embeddings for language translation tasks.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to Wu et al. (2021), platform-level caching improved power efficiency by 6.7x for the cross-lingual Transformer language model, starting from a CPU server baseline. This improvement was achieved by pre-computing and caching frequently accessed embeddings for language translation tasks."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to Strubell et al. (2019), training BERT base model using 64 V100 GPUs for 79.2 hours produces 1438 pounds of CO2 emissions, as shown in their Table 3.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to Strubell et al. (2019), training BERT base model using 64 V100 GPUs for 79.2 hours produces 1438 pounds of CO2 emissions, as shown in their Table 3."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","Multiple recent sources consistently report that ML inference accounts for 80-90% of total ML cloud computing demand according to AWS. This figure is specifically cited in papers from 2024-2025 by Chung et al., Luccioni et al., and Fernandez et al.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","Multiple recent sources consistently report that ML inference accounts for 80-90% of total ML cloud computing demand according to AWS. This figure is specifically cited in papers from 2024-2025 by Chung et al., Luccioni et al., and Fernandez et al."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to Dodge et al. 2022, training a 6.1B parameter language model to completion would consume approximately 103,593 kWh of electricity. However, the context does not provide information about how many U.S. household-years of electricity consumption this would be equivalent to.","is_blank","household-years","[""dodge2022""]","is_blank","is_blank","According to Dodge et al. 2022, training a 6.1B parameter language model to completion would consume approximately 103,593 kWh of electricity. However, the context does not provide information about how many U.S. household-years of electricity consumption this would be equivalent to."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","According to the context, in geo-distributed NLP experiments with GC (Google Cloud), the external egress cost was $4.329/h which represented more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, in geo-distributed NLP experiments with GC (Google Cloud), the external egress cost was $4.329/h which represented more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","Based on the context, JetMoE-8B used 30,000 H100 GPU hours across 96 H100 GPUs for training. With 96 GPUs running in parallel, the wall-clock time can be calculated as: 30,000 GPU hours / 96 GPUs = 312.5 hours ≈ 13 days.","13","days","[""shen2024""]","is_blank","is_blank","Based on the context, JetMoE-8B used 30,000 H100 GPU hours across 96 H100 GPUs for training. With 96 GPUs running in parallel, the wall-clock time can be calculated as: 30,000 GPU hours / 96 GPUs = 312.5 hours ≈ 13 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","According to the context, 'water consumption' is explicitly defined as 'water withdrawal minus water discharge', representing the amount of water that is evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","According to the context, 'water consumption' is explicitly defined as 'water withdrawal minus water discharge', representing the amount of water that is evaporated, transpired, incorporated into products, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the context, LLaMA-65B's inference energy consumption ranged from 300 Watts to 1 Kilowatt across GPU shard configurations, specifically when moving from 8 GPUs (lower shard configuration) to 32 GPUs (higher end).","300-1000","W","[""samsi2024""]","is_blank","is_blank","According to the context, LLaMA-65B's inference energy consumption ranged from 300 Watts to 1 Kilowatt across GPU shard configurations, specifically when moving from 8 GPUs (lower shard configuration) to 32 GPUs (higher end)."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context, when comparing Qwen 2.5 models in zero-shot classification tasks, the Qwen 2.5 72B model consumed 7 times more energy than the Qwen 2.5 7B model, with only a minor accuracy reduction of 0.07 points.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context, when comparing Qwen 2.5 models in zero-shot classification tasks, the Qwen 2.5 72B model consumed 7 times more energy than the Qwen 2.5 7B model, with only a minor accuracy reduction of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","Based on Table III in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg after optimization through quantization and local inference, representing a 55.6% reduction.","55.6","percent","[""khan2025""]","is_blank","is_blank","Based on Table III in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg after optimization through quantization and local inference, representing a 55.6% reduction."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the context, the early 2025 iteration of the ML.ENERGY Benchmark included energy measurements of 40 widely used model architectures across 6 different tasks, as explicitly stated in the paper's abstract and introduction sections.","40","models","[""chung2025""]","is_blank","is_blank","According to the context, the early 2025 iteration of the ML.ENERGY Benchmark included energy measurements of 40 widely used model architectures across 6 different tasks, as explicitly stated in the paper's abstract and introduction sections."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","According to the context, when training a Llama-3.1 scale model in Iowa, the total health cost was $2.5 million due to factors such as wind direction and pollutant emission rates from electricity generation.","2500000","USD","[""han2024""]","is_blank","is_blank","According to the context, when training a Llama-3.1 scale model in Iowa, the total health cost was $2.5 million due to factors such as wind direction and pollutant emission rates from electricity generation."
