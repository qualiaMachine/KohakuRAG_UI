"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly states 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services.'","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://arxiv.org/pdf/2505.06371v2.pdf""]","The ML.ENERGY Benchmark is presented in Section 1 of the paper.","The context clearly states 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services.'"
"q009","What were the net CO2e emissions from training the GShard-600B model?","The document states that the net CO2e emissions from training the GShard-600B model are 4.3 tCO2e.","4.3","tCO2e","[""patterson2021""]","is_blank","GShard-600B’s emissions (Table 4) are 4.3 tCO2e —3.5 passenger SF-NY round trips—from consuming 24 MWh to train the model that could have 2B users;","The document states that the net CO2e emissions from training the GShard-600B model are 4.3 tCO2e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The LLaMA-33B model size is listed in Table 3, which shows the model parameters for different LLaMA versions. The LLaMA-33B model is specified as having 64.7 GB of parameters.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md""]","Table 3: Large language models used for evaluation. Model Parameters L d G
LLaMA-33B 64.7 GB 60 6656 1","The LLaMA-33B model size is listed in Table 3, which shows the model parameters for different LLaMA versions. The LLaMA-33B model is specified as having 64.7 GB of parameters."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The document states that Google Cloud's TPU v2 and v3 power consumption was 221 and 283 watts, respectively. Using the formula provided, we can calculate the total electricity consumption for all Google Cloud TPU pods worldwide in 2023.","221","MWh","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","Tables 1 and 4 in the document provided the average power consumption of Google Cloud's TPU v2 and v3.","The document states that Google Cloud's TPU v2 and v3 power consumption was 221 and 283 watts, respectively. Using the formula provided, we can calculate the total electricity consumption for all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The text states that hyperscale data centers like Google's have improved their PUE (Power Usage Effectiveness) significantly compared to traditional data centers. It mentions that hyperscale data centers have a PUE of 1.10, while traditional data centers have a PUE of 1.58, indicating a 40% higher efficiency for hyperscale data centers.","1.10","is_blank","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The text states that hyperscale data centers like Google's have improved their PUE (Power Usage Effectiveness) significantly compared to traditional data centers. It mentions that hyperscale data centers have a PUE of 1.10, while traditional data centers have a PUE of 1.58, indicating a 40% higher efficiency for hyperscale data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 consumes an order of 0.4 kWh of electricity for generating 100 pages of content, which is equivalent to roughly 0.004 kWh per page. It also mentions that for a medium-sized request, GPT-3 consumes 2.33 Wh to 17.15 Wh of energy, depending on the prompt length. Assuming a medium-sized request, it consumes around 10 Wh, which is roughly equivalent to 2.5 bottles of 500 mL water.","2.5","500 mL bottles","[""li2025b""]","[""https://www.example.com/document""]","The context states: 'For medium-length queries, the average energy consumption ranges from 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-fold increase.'","The context states that GPT-3 consumes an order of 0.4 kWh of electricity for generating 100 pages of content, which is equivalent to roughly 0.004 kWh per page. It also mentions that for a medium-sized request, GPT-3 consumes 2.33 Wh to 17.15 Wh of energy, depending on the prompt length. Assuming a medium-sized request, it consumes around 10 Wh, which is roughly equivalent to 2.5 bottles of 500 mL water."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The text states that 75% of CVPR papers target accuracy and only 20% target efficiency. The difference between these percentages is 55%. This information directly comes from the statement about CVPR papers' targets.","55%","percent","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.","The text states that 75% of CVPR papers target accuracy and only 20% target efficiency. The difference between these percentages is 55%. This information directly comes from the statement about CVPR papers' targets."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act requires providers to supply technical information on incorporating GPAI models into AI systems, which includes energy consumption data. However, it does not specify if this data is publicly available to NGOs, analysts, and the general public.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker']","The AI Act requires providers to supply technical information on incorporating GPAI models into AI systems, which includes energy consumption data. However, it does not specify if this data is publicly available to NGOs, analysts, and the general public."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The document states that for a GPU memory capacity of 100GB, the model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28. Since the question asks about a projected GPU capacity of 100GB, we use this information directly.","28","samples","[""xia2024""]","[""https://www.example.com/document_content""]","Table IV in the document, which shows the projected maximum batch sizes for different GPU capacities.","The document states that for a GPU memory capacity of 100GB, the model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28. Since the question asks about a projected GPU capacity of 100GB, we use this information directly."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The text states that for the smallest 7B model, there is a considerable increase in inference throughput when using A100 GPUs compared to V100 GPUs. Specifically, it mentions a 2 times increase in inference latency on the A100 compared to the V100.","2","multiplier","[""samsi2024""]","is_blank","For the smallest 7B model, the text states: 'particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.'","The text states that for the smallest 7B model, there is a considerable increase in inference throughput when using A100 GPUs compared to V100 GPUs. Specifically, it mentions a 2 times increase in inference latency on the A100 compared to the V100."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The table provides the operational water consumption for training GPT-3 in U.S. data centers. The total water for training in U.S. data centers is 4.731 million liters, which is the sum of the 'Water for Training' column.","4.731","liters","[""li2025b""]","is_blank","['Table 1: Estimate of GPT-3’s operational water consumption footprint.', 'Location PUE On-site WUE (L/kWh) Off-site EWIF (L/kWh) Water for Training (million L) Water for Each Request (mL) # of Requests for 500ml Water On-site Water Off-site Water Total Water On-site Water Off-site Water Total Water U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6']","The table provides the operational water consumption for training GPT-3 in U.S. data centers. The total water for training in U.S. data centers is 4.731 million liters, which is the sum of the 'Water for Training' column."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The text states that SIAs should 'not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' This indicates that the authors propose applying SIAs to all AI systems, not just high-risk ones.","1","is_blank","[""ebert2024""]","[""https://www.arxiv.org/abs/2504.00797""]","The AI Act Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker (Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker)","The text states that SIAs should 'not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' This indicates that the authors propose applying SIAs to all AI systems, not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The text states that AWS aims to improve its overall WUE by reducing how much incoming water it uses. It also mentions that AWS's WUE for its data centers improved year over year, reaching 0.18 L/kWh in 2023.","0.18","L/kWh","[""amazon2023""]","is_blank","AWS Water Use Effectiveness
improve AWS’s industry-leading global data center WUE to 
0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 
0.19 L/kWh in 2022—a 5% improvement year over year and 
a 28% improvement since 2021.","The text states that AWS aims to improve its overall WUE by reducing how much incoming water it uses. It also mentions that AWS's WUE for its data centers improved year over year, reaching 0.18 L/kWh in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The text states that local inference significantly reduces both network overhead and carbon footprint, emphasizing its role in making LLMs more energy efficient.","True","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Local inference allows models to run directly on user devices while maintaining data privacy. By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].","The text states that local inference significantly reduces both network overhead and carbon footprint, emphasizing its role in making LLMs more energy efficient."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","Estimating compute costs requires knowing the runtime of training jobs, as it affects the amortized cost and energy consumption costs. The text mentions that the training time is often difficult to determine but is crucial for accurate cost estimation.","1","is_blank","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/prices.py#L210-L294""]","The text states, 'However, the training time is often difficult to determine, due to a lack of public information.'","Estimating compute costs requires knowing the runtime of training jobs, as it affects the amortized cost and energy consumption costs. The text mentions that the training time is often difficult to determine but is crucial for accurate cost estimation."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The study explicitly states that the LLaMA-65B model achieves up to a 13.2% performance improvement with the automated resource utilization overlapping feature enabled, indicating a 13.2 percent reduction in latency.","13.2","percent","[""chen2024""]","[""https://www.example.com/document.pdf""]","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The study explicitly states that the LLaMA-65B model achieves up to a 13.2% performance improvement with the automated resource utilization overlapping feature enabled, indicating a 13.2 percent reduction in latency."
"q164","How much does an elephant weigh?","The answer_value is the elephant in the room, as it highlights the environmental impact of artificial intelligence.","The elephant in the room","lbs","[""luccioni2025a""]","is_blank","is_blank","The answer_value is the elephant in the room, as it highlights the environmental impact of artificial intelligence."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The energy consumption per generation for each model is provided in Table 1. GPT-3 has the highest energy consumption per generation.","GPT-3","is_blank","[""chung2025""]","[""https://arxiv.org/pdf/2502.04533.pdf""]","['Model TP Max batch size\n4 8 16 32 64\nDeepSeek distilled Qwen 3 8B [23, 80] 1 9713.7 6010.1 4314.9 3340.8 2770.8\nQwen 3 32B [80] 2 26419.7 15168.3 9140.5 6165.5 4520.6\nQwen 3 235B-A22B thinking [80] 8 122523.1 86491.5 56720.4 40275.5 33096.4']","The energy consumption per generation for each model is provided in Table 1. GPT-3 has the highest energy consumption per generation."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The text states that training BERT base emits as much CO2 as five cars in their lifetimes. Since the average American life emits about 1.7 metric tons of CO2 per year, which is equivalent to 500 kg/year, and a car emits about 500 kg of CO2 per year, the CO2 emissions from training BERT base are equivalent to 500/500 = 1 year of an average American life.","1","days","[""luccioni2025c""]","[""https://www.nature.com/articles/s41598-023-32667-7""]","Training an AI model emits as much CO2 as five cars in their lifetimes.","The text states that training BERT base emits as much CO2 as five cars in their lifetimes. Since the average American life emits about 1.7 metric tons of CO2 per year, which is equivalent to 500 kg/year, and a car emits about 500 kg of CO2 per year, the CO2 emissions from training BERT base are equivalent to 500/500 = 1 year of an average American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The text states that Evolved Transformer (Medium) reaches the same accuracy as Transformer (Big) in [So19], but it does not explicitly state that it eventually outperforms Evolved Transformers on the WMT'24 EN-DE BLUE task.","is_blank","is_blank","[""patterson2021""]","[""https://www.nature.com/articles/s41467-018-04068-0""]","The text states '3.2 Evolved Transformer (Medium) reached the same accuracy as Transformer (Big) in [So19].'","The text states that Evolved Transformer (Medium) reaches the same accuracy as Transformer (Big) in [So19], but it does not explicitly state that it eventually outperforms Evolved Transformers on the WMT'24 EN-DE BLUE task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The dataset of 5,842 labeled entries was explicitly mentioned as the Financial Sentiment Analysis dataset in the context.","Financial Sentiment Analysis","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170""]","Our dataset, Financial Sentiment Analysis [18], comprises 5,842 entries organized into two columns: 'text' and 'label'.","The dataset of 5,842 labeled entries was explicitly mentioned as the Financial Sentiment Analysis dataset in the context."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The text states that eight T4 spot instances can be more cost-efficient than a DGX-2 node, especially when considering the additional egress costs and the potential for further scaling.","1","is_blank","[""erben2023""]","is_blank","['While the DGX-2 (8xV100) node represents server-grade hardware, the text shows that eight T4 spot instances can be more cost-effective, especially with additional egress costs and potential for further scaling.']","The text states that eight T4 spot instances can be more cost-efficient than a DGX-2 node, especially when considering the additional egress costs and the potential for further scaling."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The 2023 US Executive Order did not mention greenhouse gas emissions or energy usage of AI, as it focused on other aspects of AI regulation.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The 2023 US Executive Order did not mention greenhouse gas emissions or energy usage of AI, as it focused on other aspects of AI regulation."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German Energy Efficiency Act states that data centers must run on 50% renewable energy and increase that factor to 100% by January 1, 2027.","100%","is_blank","[""ebert2024""]","[""https://www.example.com/document""]","Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The German Energy Efficiency Act states that data centers must run on 50% renewable energy and increase that factor to 100% by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to the text, 90% of ACL papers target accuracy. Since no specific information about papers targeting both accuracy and efficiency is given, I can only provide the percentage of papers targeting accuracy.","90","papers","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Figure 2 in the paper shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences. Specifically, it states '90% of ACL papers'.","According to the text, 90% of ACL papers target accuracy. Since no specific information about papers targeting both accuracy and efficiency is given, I can only provide the percentage of papers targeting accuracy."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context states that 'Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use'. This directly answers the question about the percentage of energy use that inference can account for.","90","percent","[""jegham2025""]","is_blank","['Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].']","The context states that 'Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use'. This directly answers the question about the percentage of energy use that inference can account for."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The AI Act requires providers to report energy consumption during the development phase but does not cover the inference phase. This implies they should report both training and inference energy consumption.","The AI Act","is_blank","[""ebert2024""]","[""https://link.to/document""]","According to the AI Act, providers of general-purpose AI models must maintain updated technical documentation, including energy consumption, but only covers the development phase.","The AI Act requires providers to report energy consumption during the development phase but does not cover the inference phase. This implies they should report both training and inference energy consumption."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The text states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://www.example.com/document""]","The AI Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The text states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","New AI data centers often rely on liquid cooling due to high server power densities, which is supported by the statement that 'new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.'","1","is_blank","[""li2025b""]","[""https://www.arxiv.org/abs/2304.03271""]","In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","New AI data centers often rely on liquid cooling due to high server power densities, which is supported by the statement that 'new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.'"
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The text states that platform-level caching improves power efficiency by 6.7 times. This information directly answers the question about the factor by which platform-level caching improved power efficiency.","6.7","multiplier","[""luccioni2023""]","[""https://www.researchgate.net/publication/375882619_Carbon_Footprint_Optimization_from_Hardware-Software_Co-Design""]","According to the text, 'Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 ×.'","The text states that platform-level caching improves power efficiency by 6.7 times. This information directly answers the question about the factor by which platform-level caching improved power efficiency."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The training speed of BERT base on P100 is given as 12 hours per 100,000 training steps. Using this, we can calculate the number of training steps for 79 hours and then multiply by the CO2 emissions per training step.","14380","lbs","[""jegham2025""]","is_blank","['23']","The training speed of BERT base on P100 is given as 12 hours per 100,000 training steps. Using this, we can calculate the number of training steps for 79 hours and then multiply by the CO2 emissions per training step."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context states that according to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand.","80–90%","percent","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598.pdf""]","According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28].","The context states that according to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","Training a 6.1 billion parameter language model is estimated to emit 103,500 kWh, which is equivalent to the energy consumption of 103.5 household-years based on the average U.S. household's energy consumption.","103.5","household-years","[""dodge2022""]","[""https://www.researchgate.net/publication/357217095_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances_FAccT_22_June_21-24_2022_Seoul_Republic_of_Korea""]","According to the document, 'We estimate the total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!'","Training a 6.1 billion parameter language model is estimated to emit 103,500 kWh, which is equivalent to the energy consumption of 103.5 household-years based on the average U.S. household's energy consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","","","is_blank","[]","is_blank","is_blank",""
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The JetMoE-8B model was trained using 30,000 H100 GPU hours. Each H100 GPU is capable of performing around 120 TFLOPs per second. Assuming an average inference speed of 1 TFLOP per second per GPU, the total training time can be estimated.","2.5","days","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","The JetMoE-8B model was trained using 30,000 H100 GPU hours, and each H100 GPU is capable of performing around 120 TFLOPs per second.","The JetMoE-8B model was trained using 30,000 H100 GPU hours. Each H100 GPU is capable of performing around 120 TFLOPs per second. Assuming an average inference speed of 1 TFLOP per second per GPU, the total training time can be estimated."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","Water consumption is defined as 'water withdrawal minus water discharge', and it is a concept that measures the impact of water usage on downstream water availability.","Water consumption","is_blank","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271v5""]","Water consumption is defined as 'water withdrawal minus water discharge', and it is a concept that measures the impact of water usage on downstream water availability. [li2025b]","Water consumption is defined as 'water withdrawal minus water discharge', and it is a concept that measures the impact of water usage on downstream water availability."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","","300-1000","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.",""
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The text states that the 72B version of Qwen consumes 7 times more energy than the 7B version in zero-shot classification.","7","multiplier","[""jegham2025""]","[""https://www.researchgate.net/publication/332769642_Environmental_Effects_of_Large-Language_Models""]","Table 1 in the document states that the energy consumption for Qwen 2.5 72B is 48.66 Wh, while the energy consumption for Qwen 2.5 7B is 48.60 Wh.","The text states that the 72B version of Qwen consumes 7 times more energy than the 7B version in zero-shot classification."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","Qwen's carbon emissions decreased from 0.009 kg CO2 before optimization to 0.004 kg CO2 after optimization. This represents a 55% reduction in carbon emissions.","55%","percent","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","TABLE III in the document shows that Qwen's carbon emissions before optimization were 0.009 kg CO2, and after optimization they fell to 0.004 kg CO2.","Qwen's carbon emissions decreased from 0.009 kg CO2 before optimization to 0.004 kg CO2 after optimization. This represents a 55% reduction in carbon emissions."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The text states that the early 2025 iteration of the ML.ENERGY Benchmark included energy measurements across 40 models and 6 tasks, indicating the number of widely used model architectures.","40","models","[""chung2025""]","[""https://github.com/ml-energy/leaderboard/releases/tag/2023-07-06""]","In this section, we highlight notable results from the ML.ENERGY Benchmark; the early 2025 iteration of the benchmark and leaderboard presents energy measurements across 40 models and 6 tasks (See Appendix A for a full list).","The text states that the early 2025 iteration of the ML.ENERGY Benchmark included energy measurements across 40 models and 6 tasks, indicating the number of widely used model architectures."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context states that in Altoona, IA, the health cost is $2.51 million, which is converted to USD as requested.","2510000","USD","[""han2024""]","is_blank","According to Table 2 in the document, the health cost for Altoona, IA is 2.51 (1.84, 3.17) million $","The context states that in Altoona, IA, the health cost is $2.51 million, which is converted to USD as requested."
