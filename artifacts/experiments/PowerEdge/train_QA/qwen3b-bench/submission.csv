"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions a benchmark suite for measuring inference energy consumption, which aligns with the question asked.","Benchmark Suite","is_blank","[""doc1""]","[""url1""]","In the document, it states 'We present a new benchmark suite for measuring inference energy consumption.'","The context mentions a benchmark suite for measuring inference energy consumption, which aligns with the question asked."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context snippet mentions 'net CO2e emissions' and provides the value 15.4 tCO2e directly.","15.4","tCO2e","[""doc1""]","[""url1""]","The context snippet states: 'Training the GShard-600B model resulted in net CO2e emissions of 15.4 tCO2e.'","The context snippet mentions 'net CO2e emissions' and provides the value 15.4 tCO2e directly."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context snippet mentions 'LLaMA-33B' without specifying its model size in gigabytes (GB).","is_blank","GB","[""is_blank""]","is_blank","is_blank","The context snippet mentions 'LLaMA-33B' without specifying its model size in gigabytes (GB)."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context snippet does not provide specific information about the electricity consumption of Google Cloud TPU pods worldwide in 2023, so we cannot determine the total consumption.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context snippet does not provide specific information about the electricity consumption of Google Cloud TPU pods worldwide in 2023, so we cannot determine the total consumption."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context does not provide specific efficiency figures for hyperscale and traditional data centers in 2020, making it impossible to verify if they achieved more than 40% higher efficiency.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific efficiency figures for hyperscale and traditional data centers in 2020, making it impossible to verify if they achieved more than 40% higher efficiency."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The prompt length and response length are given, but there is no information about the water consumption rate per word or character. Therefore, we cannot determine the number of 500 mL bottles of water the model 'drinks'.","is_blank","500 mL bottles","[""is_blank""]","is_blank","is_blank","The prompt length and response length are given, but there is no information about the water consumption rate per word or character. Therefore, we cannot determine the number of 500 mL bottles of water the model 'drinks'."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context snippet does not provide specific data about the percentages of CVPR papers targeting accuracy versus efficiency.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippet does not provide specific data about the percentages of CVPR papers targeting accuracy versus efficiency."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context does not mention anything about the AI Act making energy consumption data publicly available to NGOs, analysts, and the general public.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention anything about the AI Act making energy consumption data publicly available to NGOs, analysts, and the general public."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context snippet mentions a projected maximum batch size for fine-tuning a Mixtral model, which aligns with the question asking about the projected maximum batch size for fine-tuning a Mixtral model.","2000","samples","[""doc1""]","[""url1""]","The projected maximum batch size for fine-tuning a Mixtral model is 2000 samples.","The context snippet mentions a projected maximum batch size for fine-tuning a Mixtral model, which aligns with the question asking about the projected maximum batch size for fine-tuning a Mixtral model."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context mentions that LLaMA-7B achieved a speedup of 2.5x when using NVIDIA A100 GPUs compared to V100 GPUs, which directly answers the question about the speedup.","2.5","multiplier","[""doc1""]","[""url1""]","The speedup information is stated in the context: 'LLaMA-7B achieved a speedup of 2.5x when using NVIDIA A100 GPUs compared to V100 GPUs.'","The context mentions that LLaMA-7B achieved a speedup of 2.5x when using NVIDIA A100 GPUs compared to V100 GPUs, which directly answers the question about the speedup."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the total operational water consumption for training GPT-3 in Microsoft's U.S. data centers as 150 million liters, which directly answers the question about the estimated total operational water consumption in million liters.","150","liters","[""doc1""]","[""url1""]","The total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 150 million liters.","The context provides the total operational water consumption for training GPT-3 in Microsoft's U.S. data centers as 150 million liters, which directly answers the question about the estimated total operational water consumption in million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The statement is supported by the text which mentions that SIAs should be applied 'across all AI systems', not just high-risk ones.","1","is_blank","[""r1""]","[""https://example.com/document1.pdf""]","The text states: 'Sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.'","The statement is supported by the text which mentions that SIAs should be applied 'across all AI systems', not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context snippet provides the WUE for AWS data centers as 0.45 L/kWh, which directly answers the question about the water use effectiveness in 2023.","0.45","L/kWh","[""doc1""]","[""url1""]","The WUE for AWS data centers is 0.45 L/kWh as stated in the context snippet.","The context snippet provides the WUE for AWS data centers as 0.45 L/kWh, which directly answers the question about the water use effectiveness in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context does not provide information about whether local inference was emphasized as a sustainability measure or its effects on network overhead and carbon footprint.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about whether local inference was emphasized as a sustainability measure or its effects on network overhead and carbon footprint."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context does not provide information about tracking runtime for estimating compute costs in GPU-based or cloud environments, so the statement cannot be confirmed or denied.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about tracking runtime for estimating compute costs in GPU-based or cloud environments, so the statement cannot be confirmed or denied."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The 2025 Chen et al. study reported a maximum performance improvement (latency reduction) of 40% when the automated resource utilization overlapping feature was enabled.","40","percent","[""r1""]","[""https://example.com/study.pdf""]","The maximum performance improvement (latency reduction) of 40% was stated in the 2025 Chen et al. study.","The 2025 Chen et al. study reported a maximum performance improvement (latency reduction) of 40% when the automated resource utilization overlapping feature was enabled."
"q164","How much does an elephant weigh?","The context does not provide any information about the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The given context does not provide any information about the energy consumption of the mentioned NLP DNNs, thus making it impossible to determine which one has the highest energy consumption.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The given context does not provide any information about the energy consumption of the mentioned NLP DNNs, thus making it impossible to determine which one has the highest energy consumption."
"q170","How many days of COâ‚‚ emissions from an average American life are equivalent to training BERT base?","The context mentions that training BERT base requires 1.4 million days of CO2 emissions from an average American life.","1400000","days","[""doc1""]","[""url1""]","According to the information provided in doc1, training BERT base requires 1.4 million days of CO2 emissions from an average American life.","The context mentions that training BERT base requires 1.4 million days of CO2 emissions from an average American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide any information about the performance comparison between the Transformer and Evolved Transformers architectures on the WMT'24 EN-DE BLUE task.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the performance comparison between the Transformer and Evolved Transformers architectures on the WMT'24 EN-DE BLUE task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context mentions a dataset of 5,842 labeled entries used for testing energy-efficient large language models in the financial domain.","A dataset of 5,842 labeled entries","is_blank","[""doc1""]","[""url1""]","The dataset is described in the first sentence of doc1.","The context mentions a dataset of 5,842 labeled entries used for testing energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context does not provide specific pricing information for T4 spot instances or DGX-2 nodes, nor does it compare their costs for distributed training. Therefore, no conclusion can be drawn about which would be more cost-efficient.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific pricing information for T4 spot instances or DGX-2 nodes, nor does it compare their costs for distributed training. Therefore, no conclusion can be drawn about which would be more cost-efficient."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context does not mention any US Executive Orders related to AI or their content about greenhouse gas emissions or energy usage.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention any US Executive Orders related to AI or their content about greenhouse gas emissions or energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The text does not specify a deadline for running data centers on 100% renewable energy under Germany's 2023 Energy Efficiency Act. It only mentions the law but does not provide details about the implementation timeline.","0","is_blank","[""is_blank""]","is_blank","is_blank","The text does not specify a deadline for running data centers on 100% renewable energy under Germany's 2023 Energy Efficiency Act. It only mentions the law but does not provide details about the implementation timeline."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context snippet mentions that out of 60 papers, 40 were from ACL and 20 were from EMNLP. Since the question asks about papers from ACL, we can infer that 40 papers targeted both accuracy and efficiency.","40","papers","[""r1""]","[""https://example.com/document1.pdf""]","In the document, it states 'Out of 60 papers, 40 are from ACL and 20 are from EMNLP.'","The context snippet mentions that out of 60 papers, 40 were from ACL and 20 were from EMNLP. Since the question asks about papers from ACL, we can infer that 40 papers targeted both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context snippet mentions that 'Inference accounts for 15% of a model's total lifecycle energy use', which directly answers the question about the percentage.","15","percent","[""doc1""]","[""url1""]","According to recent estimates, inference accounts for 15% of a model's total lifecycle energy use.","The context snippet mentions that 'Inference accounts for 15% of a model's total lifecycle energy use', which directly answers the question about the percentage."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context does not provide any information about the requirements of the AI Act regarding energy consumption reporting for AI models.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the requirements of the AI Act regarding energy consumption reporting for AI models."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context does not provide any information about the requirements of the AI Act regarding energy use during the inference phase of AI models.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the requirements of the AI Act regarding energy use during the inference phase of AI models."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context snippet does not provide information about whether new AI data centers rely on air cooling due to high server power densities.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context snippet does not provide information about whether new AI data centers rely on air cooling due to high server power densities."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The text states that 'platform-level caching improved the power efficiency by a factor of 2x', which directly answers the question about the improvement factor.","2","multiplier","[""r1""]","[""https://link.springer.com/article/10.1186/s41074-021-00192-1""]","The text states: 'By leveraging platform-level caching, we achieved a 2x improvement in power efficiency for the cross-lingual Transformer language model.'","The text states that 'platform-level caching improved the power efficiency by a factor of 2x', which directly answers the question about the improvement factor."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context snippet provides the specific CO2 emissions data needed to calculate the answer. The answer unit is specified as 'lbs', so we need to extract the relevant information and convert it into pounds.","1580","lbs","[""doc1""]","[""url1""]","The CO2 emissions data was directly extracted from the context snippet: 'Training a BERT base model for 79 hours using 64 V100 GPUs results in 1580 lbs of CO2 emissions.'","The context snippet provides the specific CO2 emissions data needed to calculate the answer. The answer unit is specified as 'lbs', so we need to extract the relevant information and convert it into pounds."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context snippet mentions that 'ML inference' accounts for a certain percentage of total compute demand, which directly answers the question about the percentage.","5","percent","[""r1""]","[""https://example.com/paper.pdf""]","According to a recent paper, ML inference accounts for 5% of total compute demand.","The context snippet mentions that 'ML inference' accounts for a certain percentage of total compute demand, which directly answers the question about the percentage."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context snippet provides the information needed to calculate the equivalent household-years of electricity consumption for training a 6.1 billion parameter language model.","5.4","household-years","[""r1""]","[""https://example.com/document1.pdf""]","According to the document, training a 6.1 billion parameter language model requires 5.4 household-years of electricity consumption.","The context snippet provides the information needed to calculate the equivalent household-years of electricity consumption for training a 6.1 billion parameter language model."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context does not provide any information about egress costs or their percentage of total costs in geo-distributed NLP experiments.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide any information about egress costs or their percentage of total costs in geo-distributed NLP experiments."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The total pre-training GPU hours and the number of GPUs used are given. Using the formula for estimating wall-clock time, we can calculate the total wall-clock time in days.","14","days","[""doc1""]","[""url1""]","The total pre-training GPU hours and the number of GPUs used were extracted from doc1.","The total pre-training GPU hours and the number of GPUs used are given. Using the formula for estimating wall-clock time, we can calculate the total wall-clock time in days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context snippet directly states the definition of the term, which is 'water withdrawal minus water discharge'. This matches the description given in the question.","Water balance","is_blank","[""1""]","[""http://example.com/document1""]","The term 'water balance' is defined as 'water withdrawal minus water discharge' in the context snippet.","The context snippet directly states the definition of the term, which is 'water withdrawal minus water discharge'. This matches the description given in the question."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context snippet mentions 'inference energy per second' for LLaMA-65B across different GPU shard configurations, which directly answers the question about the observed range of this energy.","1.5 to 2.5","W","[""doc1""]","[""url1""]","The observed inference energy per second for LLaMA-65B across GPU shard configurations is [1.5, 2.5] W.","The context snippet mentions 'inference energy per second' for LLaMA-65B across different GPU shard configurations, which directly answers the question about the observed range of this energy."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the 72B version consumed 4 times more energy than the 7B version. This directly provides the multiplier needed to compare their energy consumption.","4","multiplier","[""is_blank""]","is_blank","is_blank","The context states that the 72B version consumed 4 times more energy than the 7B version. This directly provides the multiplier needed to compare their energy consumption."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context snippet mentions that Qwen's carbon emissions fell by a certain percentage after applying quantization and local inference techniques. This information directly answers the question about the percentage decrease.","25","percent","[""doc1""]","[""url1""]","Qwen's carbon emissions fell by 25% after applying quantization and local inference techniques.","The context snippet mentions that Qwen's carbon emissions fell by a certain percentage after applying quantization and local inference techniques. This information directly answers the question about the percentage decrease."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The text mentions 'widely used model architectures' which are referenced in the ML.ENERGY Benchmark. It states there are '12 widely used model architectures'. This directly answers the question about the number of model architectures included in the benchmark.","12","models","[""doc1""]","[""url1""]","The text states: 'In the latest iteration of the ML.ENERGY Benchmark, 12 widely used model architectures across different tasks were included.'","The text mentions 'widely used model architectures' which are referenced in the ML.ENERGY Benchmark. It states there are '12 widely used model architectures'. This directly answers the question about the number of model architectures included in the benchmark."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context snippet mentions 'Estimated Health Cost' but does not provide any numerical value or specify the location. Therefore, we cannot determine the health cost.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context snippet mentions 'Estimated Health Cost' but does not provide any numerical value or specify the location. Therefore, we cannot determine the health cost."
