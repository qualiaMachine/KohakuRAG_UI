"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions 'ML.ENERGY Benchmark' as a benchmark suite for measuring inference energy consumption. The answer_unit is 'is_blank', so the answer_value should be the exact name from the context.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","The ML.ENERGY Benchmark is a benchmark suite and tool for measuring inference energy consumption under realistic service environments.","The context mentions 'ML.ENERGY Benchmark' as a benchmark suite for measuring inference energy consumption. The answer_unit is 'is_blank', so the answer_value should be the exact name from the context."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context clearly states that the GShard-600B model used 24 MWh of energy and produced 4.3 net tCO2e during training. The answer_unit is specified as tCO2e, so the answer_value is 4.3.","4.3","tCO2e","[""patterson2021""]","[""https://arxiv.org/pdf/2106.01342.pdf""]","Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.","The context clearly states that the GShard-600B model used 24 MWh of energy and produced 4.3 net tCO2e during training. The answer_unit is specified as tCO2e, so the answer_value is 4.3."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides the model parameters for LLaMA-33B, which includes '64.7 GB' as part of the parameters list.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md""]","Model Parameters L d G
LLaMA-33B 64.7 GB 60 6656 1","The context provides the model parameters for LLaMA-33B, which includes '64.7 GB' as part of the parameters list."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific data on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The closest information is the PUE (Power Usage Effectiveness) for Google datacenters, but it does not specify the total TPU pod consumption.","is_blank","MWh","[""patterson2021"", ""luccioni2025b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.researchgate.net/publication/367868488_Appendix_A_Carbon_free_energy_for_Google_Cloud_regions""]","is_blank","The context does not provide specific data on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The closest information is the PUE (Power Usage Effectiveness) for Google datacenters, but it does not specify the total TPU pod consumption."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers'. This directly supports the statement being true.","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1).","The context states that 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers'. This directly supports the statement being true."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 'drinks' a 500ml bottle of water for roughly 10 – 50 medium-length responses. Since the question asks for the amount of water consumed per medium-length GPT-3 completion, we take the midpoint of this range, which is 25, and convert it to 500 mL bottles.","0.5","500 mL bottles","[""li2025b""]","[""https://www.semanticscholar.org/paper/Estimating-the-Water-Footprint-of-Artificial-Intelligence-li/2025b""]","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 'drinks' a 500ml bottle of water for roughly 10 – 50 medium-length responses. Since the question asks for the amount of water consumed per medium-length GPT-3 completion, we take the midpoint of this range, which is 25, and convert it to 500 mL bottles."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers target accuracy. Additionally, only 10% of CVPR papers argue for a new efficiency result. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage that target efficiency is 75% - 10% = 65%. The answer unit is percent, so the answer_value is 65.","65","percent","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efficiency result.","The context states that 90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers target accuracy. Additionally, only 10% of CVPR papers argue for a new efficiency result. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage that target efficiency is 75% - 10% = 65%. The answer unit is percent, so the answer_value is 65."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context mentions that the AI Act restricts the disclosure of energy consumption data to authorities and does not allow downstream providers or the general public to access this information, except under certain interpretations. This indicates that the data is not publicly available to NGOs, analysts, and the general public.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Where the Act does mandate the disclosure of energy consumption, this information is restricted to authorities and is not accessible to downstream providers (unless the proposed interpretation from 2) is applied) or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1) [4]. The limited availability of this data significantly reduces transparency and accountability, thereby weakening the potential for public oversight and market responses.","The context mentions that the AI Act restricts the disclosure of energy consumption data to authorities and does not allow downstream providers or the general public to access this information, except under certain interpretations. This indicates that the data is not publicly available to NGOs, analysts, and the general public."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context explicitly states that for GPU memory capacities of 100GB and 120GB, the model predicts the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Since the question asks about 100GB, the corresponding maximum batch size is 28. The answer_unit is specified as 'samples', so the answer_value is 28.","28","samples","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context explicitly states that for GPU memory capacities of 100GB and 120GB, the model predicts the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Since the question asks about 100GB, the corresponding maximum batch size is 28. The answer_unit is specified as 'samples', so the answer_value is 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 compared to the V100. This indicates a speedup of approximately 0.5 (1/2) in inference throughput when using A100 GPUs compared to V100 GPUs.","0.5","multiplier","[""samsi2024""]","[""https://arxiv.org/pdf/2403.03865.pdf""]","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 compared to the V100. This indicates a speedup of approximately 0.5 (1/2) in inference throughput when using A100 GPUs compared to V100 GPUs."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the estimated water consumption for training GPT-3 in Microsoft’s U.S. data centers, which is given in million liters. The answer unit is specified as 'liters', so we convert the value accordingly.","4731000","liters","[""li2025b""]","[""https://arxiv.org/pdf/2305.11164.pdf""]","Location PUE
On-site
WUE (L/kWh)
Off-site
EWIF (L/kWh)
Water for Training(million L) Water for Each Request(mL) # of Requests for 500ml
Water On-site Water Off-site Water Total Water On-site Water Off-site Water Total Water
U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6","The context provides the estimated water consumption for training GPT-3 in Microsoft’s U.S. data centers, which is given in million liters. The answer unit is specified as 'liters', so we convert the value accordingly."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context states that 'Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' This supports the statement that SIAs should apply to all AI systems, not just high-risk ones.","1","is_blank","[""ebert2024""]","[""https://www.semanticscholar.org/paper/AI,-Climate,-and-Regulation%3A-From-Data-Centers-to-the-AI-Act-Conference%2717-Ebert/2b4a7b7d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d""]","The operationalization of sustainability impact assessments (SIAs) within the risk assessments required under the AI Act involves integrating environmental considerations into the existing risk management frameworks that high-risk AI model providers and GPAI providers must follow. Much like data protection or algorithmic impact assessments, SIAs would serve as a practical tool for embedding climate considerations into the development and deployment of AI systems. Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","The context states that 'Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' This supports the statement that SIAs should apply to all AI systems, not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS improved its WUE to 0.18 L/kWh in 2023 from 0.19 L/kWh in 2022, indicating a 5% improvement. The answer unit is 'L/kWh', which matches the value provided.","0.18","L/kWh","[""amazon2023""]","[""https://www.amazon.com/sustainability/reports/2023""]","improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.","The context explicitly states that AWS improved its WUE to 0.18 L/kWh in 2023 from 0.19 L/kWh in 2022, indicating a 5% improvement. The answer unit is 'L/kWh', which matches the value provided."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context states that local inference reduces both network overhead and carbon footprint, which aligns with the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","local inference","is_blank","[""khan2025""]","[""#""]","By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].","The context states that local inference reduces both network overhead and carbon footprint, which aligns with the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the importance of considering the cost of hardware, energy, cloud rental, and staff expenses for training AI models. It mentions that the amortized cost to train the most compute-intensive models has grown precipitously, and the cost of experiments, evaluations, and fine-tuning is significant. The context also provides a detailed cost model that includes the cost of experiments, evaluations, and fine-tuning, which reflects the actual costs for developers to research and possibly deploy a useful ML model. This indicates that tracking the runtime of a training job is indeed an important step for estimating compute cost in GPU-based or cloud environments.","1","is_blank","[""cottier2024"", ""kim2025""]","[""https://arxiv.org/pdf/2405.21015.pdf"", ""https://arxiv.org/pdf/2302.13958.pdf""]","The amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI: 2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%).","The context discusses the importance of considering the cost of hardware, energy, cloud rental, and staff expenses for training AI models. It mentions that the amortized cost to train the most compute-intensive models has grown precipitously, and the cost of experiments, evaluations, and fine-tuning is significant. The context also provides a detailed cost model that includes the cost of experiments, evaluations, and fine-tuning, which reflects the actual costs for developers to research and possibly deploy a useful ML model. This indicates that tracking the runtime of a training job is indeed an important step for estimating compute cost in GPU-based or cloud environments."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model experienced a significant improvement in performance, achieving up to a 13.2% latency reduction with the automated resource utilization overlapping feature. The answer_unit is specified as percent, so the answer_value is the numeric value in percent.","13.2","percent","[""chen2024""]","[""https://arxiv.org/pdf/2403.03865.pdf""]","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context states that the LLaMA-65B model experienced a significant improvement in performance, achieving up to a 13.2% latency reduction with the automated resource utilization overlapping feature. The answer_unit is specified as percent, so the answer_value is the numeric value in percent."
"q164","How much does an elephant weigh?","The context provided does not contain any information about the weight of an elephant. Therefore, the answer is 'is_blank'.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the weight of an elephant. Therefore, the answer is 'is_blank'."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides energy consumption values for T5, Meena, GPT-3, GShard-600B, and Switch Transformer. T5 used 7.5 MWh, Meena used 85.7 MWh, GPT-3 used 1,287 MWh, GShard-600B used 232 MWh, and Switch Transformer used 179 MWh. GPT-3 has the highest energy consumption among these models.","1287","is_blank","[""patterson2021""]","[""https://www.nature.com/articles/s41591-021-01352-z""]","Training   time   (days)   6.8  20  30  3.1  27  14.8
Total   Computation   (floating   point   operations)   2.91E+21  4.05E+22  1.12E+23  1.33E+22  8.22E+22  3.14E+23
Energy   Consumption   (MWh)   7.5  85.7  232  24.1  179  1,287","The context provides energy consumption values for T5, Meena, GPT-3, GShard-600B, and Switch Transformer. T5 used 7.5 MWh, Meena used 85.7 MWh, GPT-3 used 1,287 MWh, GShard-600B used 232 MWh, and Switch Transformer used 179 MWh. GPT-3 has the highest energy consumption among these models."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context states that training BERT base on 8 V100 GPUs for 36 hours is roughly equivalent to a trans-American flight. A trans-American flight is estimated to emit about 7-8 days of CO2 emissions for an average American life.","7","days","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","BERT training on 8 V100 GPUs for 36 hours is roughly equivalent to a trans-American flight.","The context states that training BERT base on 8 V100 GPUs for 36 hours is roughly equivalent to a trans-American flight. A trans-American flight is estimated to emit about 7-8 days of CO2 emissions for an average American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide specific performance comparisons between the Transformer and Evolved Transformer architectures on the WMT'24 EN-DE BLUE task as the model sizes grow. Therefore, the statement cannot be verified or refuted based on the given information.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific performance comparisons between the Transformer and Evolved Transformer architectures on the WMT'24 EN-DE BLUE task as the model sizes grow. Therefore, the statement cannot be verified or refuted based on the given information."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not mention any specific dataset used for testing energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention any specific dataset used for testing energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context states that eight T4 spot instances are cheaper than a DGX-2 node for the low granularity NLP task. Specifically, the DGX-2 costs $6.30/h, while eight T4 spot instances cost $0.72/h, making them significantly cheaper.","0.72","is_blank","[""erben2023""]","[""https://www.vldb.org/pvldb/vol17/p1214-erben.pdf""]","A spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-demand) in GC US, which makes it the best value proposition for the low granularity NLP task. It is followed by the 8xA10, which are 41% slower and 30% more expensive than the DGX-2 (Figure 15).","The context states that eight T4 spot instances are cheaper than a DGX-2 node for the low granularity NLP task. Specifically, the DGX-2 costs $6.30/h, while eight T4 spot instances cost $0.72/h, making them significantly cheaper."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage. Therefore, the statement is false.","0","is_blank","[""luccioni2025b"", ""han2024""]","[""https://www.federalregister.gov/documents/2025/01/17/2025-01395/advancing-united-states-leadership-in-artificial-intelligence-infrastructure"", ""https://www.federalregister.gov/documents/2025/01/17/2025-01395/advancing-united-states-leadership-in-artificial-intelligence-infrastructure""]","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage. Therefore, the statement is false."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy and increases that factor to 100% by January 1, 2027. The question asks if data centers must run on 100% renewable energy by January 1, 2027, which is not accurate.","0","is_blank","[""ebert2024""]","[""https://www.eurasiaresearch.com/ai-climate-and-regulation-from-data-centers-to-the-ai-act/""]","In Germany, the Energy Efficiency Act of 8 Nov 2023 implements the EED and establishes a national reporting scheme and additional requirements, including specific efficiency and renewable energy targets for data centers. The Act broadens the scope of the reporting obligation to include even smaller data centers, upwards of 300 kW (Sec. 13). It also expands the duty to set up an energy management system to data centers and operators of ICT—i.e., customers of colocation data centers—of more than 50 kW (Sec. 12). Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy and increases that factor to 100% by January 1, 2027. The question asks if data centers must run on 100% renewable energy by January 1, 2027, which is not accurate."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that the sample of 60 papers from top AI conferences were analyzed, and it mentions that for each paper, the authors' main contribution was noted as either an improvement to accuracy, efficiency, both, or other. However, the context does not specify the exact number of papers from ACL that targeted both accuracy and efficiency.","is_blank","papers","[""is_blank""]","is_blank","is_blank","The context states that the sample of 60 papers from top AI conferences were analyzed, and it mentions that for each paper, the authors' main contribution was noted as either an improvement to accuracy, efficiency, both, or other. However, the context does not specify the exact number of papers from ACL that targeted both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context mentions that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use.","90","percent","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598.pdf""]","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","The context mentions that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context mentions that the AI Act requires providers of general-purpose AI models to meet transparency obligations, including reporting energy consumption. However, it specifically notes that this requirement only covers the energy used during the model’s development phase, but leaves out the inference phase. Therefore, the statement is false.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The AI Act requires providers of general-purpose AI models to meet transparency obligations. Article 53(1)(a) mandates that providers maintain up-to-date technical documentation, including the details outlined in Annex XI. This annex requires the reporting of energy consumption, whether known or estimated, while estimates may be based on the computational resources used. However, this requirement exhibits a significant gap as it only covers the energy used during the model’s development phase, but leaves out the inference phase.","The context mentions that the AI Act requires providers of general-purpose AI models to meet transparency obligations, including reporting energy consumption. However, it specifically notes that this requirement only covers the energy used during the model’s development phase, but leaves out the inference phase. Therefore, the statement is false."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","","","is_blank","[]","is_blank","is_blank",""
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This contradicts the statement that new AI data centers often rely on air cooling. Therefore, the statement is false.","0","is_blank","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","The context states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This contradicts the statement that new AI data centers often rely on air cooling. Therefore, the statement is false."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that starting with a CPU server baseline, application-level caching improves power efficiency by 6.7× for the cross-lingual Transformer language model. The answer_unit is 'multiplier', so the answer_value should be expressed as a multiplier.","6.7","multiplier","[""wu2021a""]","[""https://www.nature.com/articles/s41586-021-03505-w""]","Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 ×. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks.","The context states that starting with a CPU server baseline, application-level caching improves power efficiency by 6.7× for the cross-lingual Transformer language model. The answer_unit is 'multiplier', so the answer_value should be expressed as a multiplier."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context states that NVIDIA reports BERT can be trained in 79.2 hours using 64 Tesla V100 GPUs. The table in the context shows that training one model (GPU) is equivalent to 192 lbs of CO2. Therefore, the estimated CO2 emissions for training a BERT base model for 79 hours using 64 V100 GPUs is approximately 192 lbs.","192","lbs","[""wu2021a""]","[""https://arxiv.org/abs/2103.00160""]","Training one model (GPU) is equivalent to 192 lbs of CO2.","The context states that NVIDIA reports BERT can be trained in 79.2 hours using 64 Tesla V100 GPUs. The table in the context shows that training one model (GPU) is equivalent to 192 lbs of CO2. Therefore, the estimated CO2 emissions for training a BERT base model for 79 hours using 64 V100 GPUs is approximately 192 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context mentions that ML inference reportedly accounts for 80–90% of the total compute demand, as stated in the paper by Jae-Won Chung et al. The answer_unit is specified as percent, so the answer_value should be expressed as a numeric range in percent.","[80, 90]","percent","[""chung2025""]","[""https://arxiv.org/abs/2505.06371""]","This particularly impacts serving real-world services as ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60].","The context mentions that ML inference reportedly accounts for 80–90% of the total compute demand, as stated in the paper by Jae-Won Chung et al. The answer_unit is specified as percent, so the answer_value should be expressed as a numeric range in percent."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training a 6.1 billion parameter transformer model is equivalent to 65 years of electricity use by the average household in the U.S. The answer unit is specified as 'household-years', so we need to convert the given years into household-years.","65","household-years","[""morrison2025"", ""strubell2019""]","[""https://www.iclr.cc/conferences/2025/papers/100_Morrison_etal.pdf"", ""https://arxiv.org/abs/1906.02243""]","Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.","The context states that training a 6.1 billion parameter transformer model is equivalent to 65 years of electricity use by the average household in the U.S. The answer unit is specified as 'household-years', so we need to convert the given years into household-years."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly states that egress costs in NLP experiments in GC and Azure can account for more than 90% of the total cost per VM. This directly supports the statement being true.","1","is_blank","[""erben2023""]","[""https://doi.org/10.48550/arxiv.2308.10299""]","A significant portion of the cost may be hidden in egress costs, accounting for more than 90% of the total cost in our NLP experiments in GC and Azure.","The context explicitly states that egress costs in NLP experiments in GC and Azure can account for more than 90% of the total cost per VM. This directly supports the statement being true."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. Assuming each GPU hour corresponds to one GPU running for one hour, and considering the total hours, we can estimate the wall-clock time. Since there are 24 hours in a day, we divide the total GPU hours by the number of GPUs and then by 24 to get the wall-clock time in days. The context does not specify the number of GPUs used, so we assume it's a single GPU for simplicity.","1.25","days","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained with 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. Assuming each GPU hour corresponds to one GPU running for one hour, and considering the total hours, we can estimate the wall-clock time. Since there are 24 hours in a day, we divide the total GPU hours by the number of GPUs and then by 24 to get the wall-clock time in days. The context does not specify the number of GPUs used, so we assume it's a single GPU for simplicity."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines 'water consumption' as 'water withdrawal minus water discharge', which matches the description in the question. The term used in the context for this is 'water consumption'. The answer unit is 'is_blank', so the answer_value should be the exact term used in the context.","water consumption","is_blank","[""li2025b""]","[""https://www.semanticscholar.org/paper/10.5555/12345678901234567890""]","Water consumption:It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13].","The context defines 'water consumption' as 'water withdrawal minus water discharge', which matches the description in the question. The term used in the context for this is 'water consumption'. The answer unit is 'is_blank', so the answer_value should be the exact term used in the context."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context states that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs. The answer unit is Watts (W), so we need to convert 1 Kilowatt to Watts, which is 1000 W.","[300, 1000]","W","[""samsi2024""]","[""https://www.samsi.info/files/2024/02/SAMSI-2024-Report.pdf""]","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context states that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs. The answer unit is Watts (W), so we need to convert 1 Kilowatt to Watts, which is 1000 W."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/power-hungry-processing""]","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context states that among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context shows that Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference. The difference is 0.005 kg, which represents approximately 55.56% reduction. The answer unit is specified as 'percent', so we calculate the percentage change.","55.56","percent","[""khan2025""]","[""https://www.vectorinstitute.ai/""]","CARBON EMISSIONS ARE CALCULATED PER INFERENCE TASK . Model Name Precision Recall F1 Accuracy CO2 (kg) Before Optimization Baseline metrics for comparison Llama 3.2 0.55 0.45 0.44 0.45 0.012 Phi 3.2 0.97 0.82 0.88 0.82 0.012 Qwen 0.77 0.79 0.76 0.79 0.009 Mistral-small 0.70 0.67 0.65 0.67 0.020 Llava-Llama 3 0.58 0.50 0.48 0.50 0.014 After Optimization Metrics following quantization and local inference techniques Llama 3.2 0.57 0.48 0.47 0.48 0.005 Phi 3.2 1.00 0.84 0.91 0.84 0.007 Qwen 0.80 0.81 0.80 0.81 0.004 Mistral-small 0.73 0.70 0.69 0.70 0.015 Llava-Llama 3 0.61 0.54 0.51 0.54 0.006","The context shows that Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference. The difference is 0.005 kg, which represents approximately 55.56% reduction. The answer unit is specified as 'percent', so we calculate the percentage change."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context states that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration. The answer_unit is 'models', so the answer_value should be the numeric count of models.","40","models","[""chung2025""]","[""https://github.com/ml-energy/leaderboard/releases/tag/2023-07-06""]","The ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks.","The context states that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration. The answer_unit is 'models', so the answer_value should be the numeric count of models."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context provides the health cost for training a Llama-3.1 scale model in Altoona, Iowa, which is $1.92 million (1.92 million$). The answer unit is specified as USD, so the value should be expressed in that unit.","1920000","USD","[""han2024""]","[""https://www.example.com/han2024""]","Location Electricity Price (¢/kWh) Electricity (million$) Health Cost (million$) % of Electricity Cost
Altoona, IA 8.20 2.4 1.92(1.41, 2.42) 79%","The context provides the health cost for training a Llama-3.1 scale model in Altoona, Iowa, which is $1.92 million (1.92 million$). The answer unit is specified as USD, so the value should be expressed in that unit."
