"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions 'the ML.ENERGY Benchmark' as the benchmark suite presented in the paper for measuring inference energy consumption.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://ml.energy/leaderboard""]","In this paper, we share the design principles we have established over time (Section 2) and present the ML.ENERGY Benchmark that embodies them (Section 3). It provides two key functionalities:","The context mentions 'the ML.ENERGY Benchmark' as the benchmark suite presented in the paper for measuring inference energy consumption."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context clearly states that GShard-600B's emissions are 4.3 tCO2e from consuming 24 MWh to train the model.","4.3","tCO2e","[""patterson2021""]","is_blank","GShard-60B‚Äôs emissions (Table 4) are 4.3 tCO2e ‚Äî3.5 passenger SF-NY round trips‚Äîfrom consuming 24 MWh to train the model that could have 2B users;","The context clearly states that GShard-600B's emissions are 4.3 tCO2e from consuming 24 MWh to train the model."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context clearly states that the LLaMA-33B model has 64.7 GB of parameters.","64.7","GB","[""chen2024""]","is_blank","LLaMA-33B 64.7 GB 60 6656 1","The context clearly states that the LLaMA-33B model has 64.7 GB of parameters."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context mentions that Google publishes its total energy consumption, which was 12.2 Terawatt-hours (TWh) in 2019. However, there is no specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""patterson2021""]","is_blank","Finally, Google publishes its total energy consumption, and for 2019 it was 12.2 TeraWatt-hours [Goo20].","The context mentions that Google publishes its total energy consumption, which was 12.2 Terawatt-hours (TWh) in 2019. However, there is no specific information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers'. This indicates that hyperscale data centers are indeed more efficient by more than 40% compared to traditional data centers.","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers (Figure 1).","The context states that 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers'. This indicates that hyperscale data centers are indeed more efficient by more than 40% compared to traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to ‚Äòdrink‚Äô (consume) 500 mL of water for roughly 10 ‚Äì 50 medium-length responses. Given that the prompt is 800 words and the response is 150 - 300 words, a medium-length response would fall within this range. Thus, the model drinks roughly 500 mL of water for each medium-length response.","500","500 mL bottles","[""li2025b"", ""jegham2025""]","is_blank","Additionally, a total of 0.38 ‚Äì 0.60 billion cubic meters of water will be evaporated and considered ‚Äúconsumption‚Äù due to the global AI demand in 2027. Moreover, these global estimates will be exceeded by the total water withdrawal and consumption attributed to AI in the U.S. alone in 2028 if the projection in [1] comes to fruition.","The context states that GPT-3 needs to ‚Äòdrink‚Äô (consume) 500 mL of water for roughly 10 ‚Äì 50 medium-length responses. Given that the prompt is 800 words and the response is 150 - 300 words, a medium-length response would fall within this range. Thus, the model drinks roughly 500 mL of water for each medium-length response."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy, while only 20% target efficiency. Therefore, the difference in percentage points is 75% - 20% = 55%. The answer is expressed as a percentage.","55","percent","[""schwartz2019""]","is_blank","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efficiency result.","The context states that 75% of CVPR papers target accuracy, while only 20% target efficiency. Therefore, the difference in percentage points is 75% - 20% = 55%. The answer is expressed as a percentage."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context discusses the AI Act's limitations in terms of energy consumption reporting and mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is crucial for long-term environmental impact.","0","is_blank","[""ebert2024""]","is_blank","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context discusses the AI Act's limitations in terms of energy consumption reporting and mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is crucial for long-term environmental impact."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context states that for a projected GPU capacity of 100GB, the model predicts the maximum batch size supported for fine-tuning Mixtral will be 28 samples.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context states that for a projected GPU capacity of 100GB, the model predicts the maximum batch size supported for fine-tuning Mixtral will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is listed in Table 1 as 4.731 million liters.","4.731","liters","[""li2025b""]","is_blank","Water for Training(million L) Water for Each Request(mL) # of Requests
for 500ml
Water
On-site
Water
Off-site
Water
Total
Water
On-site
Water
Off-site
Water
Total
Water
U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6","The context provides the operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is listed in Table 1 as 4.731 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context clearly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","1","is_blank","[""ebert2024"", ""luccioni2025b""]","[""https://example.com/ebert2024"", ""https://example.com/luccioni2025b""]","Much like data protection or algorithmic impact assessments, SIAs would serve as a practical tool for embedding climate considerations into the development and deployment of AI systems. Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","The context clearly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023.","0.18","L/kWh","[""amazon2023""]","is_blank","AWS Water Use Effectiveness
improve AWS‚Äôs industry-leading global data center WUE to 
0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 
0.19 L/kWh in 2022‚Äîa 5% improvement year over year and 
a 28% improvement since 2021.","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context mentions that local inference reduces both network overhead and carbon footprint, which supports the statement.","1","is_blank","[""khan2025""]","is_blank","By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].","The context mentions that local inference reduces both network overhead and carbon footprint, which supports the statement."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the importance of tracking the runtime of a training job for estimating compute cost in GPU-based or cloud environments. It mentions that the training time is a key factor in calculating carbon emissions and cost, and that tracking runtime helps in understanding the exact energy consumption during the training process.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","The unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh). The amount of CO2eq (ùê∂) emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used ( ùëÉ), the training time ( ùëá ) and the carbon intensity of the energy grid ( ùêº); or equivalently, the energy consumed (ùê∏) and the carbon intensity: ùê∂ = ùëÉ √ó ùëá √ó ùêº = ùê∏ √ó ùêº . (1)","The context discusses the importance of tracking the runtime of a training job for estimating compute cost in GPU-based or cloud environments. It mentions that the training time is a key factor in calculating carbon emissions and cost, and that tracking runtime helps in understanding the exact energy consumption during the training process."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model experiences up to a 13.2% performance improvement in latency reduction with the automated resource utilization overlapping feature.","13.2","percent","[""chen2024""]","is_blank","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context states that the LLaMA-65B model experiences up to a 13.2% performance improvement in latency reduction with the automated resource utilization overlapping feature."
"q164","How much does an elephant weigh?","The context mentions 'The Elephant in the Room' and refers to the Asian elephants in the Western Ghats, which is a UNESCO World Heritage Site and critical wildlife conservation zone.","is_blank","lbs","[""wu2021a"", ""amazon2023""]","is_blank","is_blank","The context mentions 'The Elephant in the Room' and refers to the Asian elephants in the Western Ghats, which is a UNESCO World Heritage Site and critical wildlife conservation zone."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides CO2e calculations for T5, Meena, GShard, and Switch Transformer, but does not directly compare their energy consumption. However, it mentions that GPT-3 has approximately 305% of the CO2e of a round trip between San Francisco and New York, while T5 has 26%, Meena 53%, Gshard-600B 2%, and Switch Transformer 32%. Given these percentages, GPT-3 likely has the highest energy consumption among the mentioned models.","is_blank","is_blank","[""patterson2021"", ""jegham2025""]","is_blank","GPT-3 is ~305% of the CO2e of a round trip between San Francisco and New York, while T5 is ~26%, Meena is 53%, Gshard-600B is ~2%, and Switch Transformer is 32%.","The context provides CO2e calculations for T5, Meena, GShard, and Switch Transformer, but does not directly compare their energy consumption. However, it mentions that GPT-3 has approximately 305% of the CO2e of a round trip between San Francisco and New York, while T5 has 26%, Meena 53%, Gshard-600B 2%, and Switch Transformer 32%. Given these percentages, GPT-3 likely has the highest energy consumption among the mentioned models."
"q170","How many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?","The context states that training BERT on GPU is roughly equivalent to a trans-American flight. A typical trans-American flight emits about 2,200 kg or 4,850 lbs of CO‚ÇÇ.","4850","days","[""strubell2019""]","is_blank","training BERT on GPU is roughly equivalent to a trans-American Ô¨Çight.","The context states that training BERT on GPU is roughly equivalent to a trans-American flight. A typical trans-American flight emits about 2,200 kg or 4,850 lbs of CO‚ÇÇ."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context discusses the Evolved Transformer and its performance compared to the vanilla Transformer on the WMT'24 EN-DE BLUE task. However, it does not provide specific information about the Transformer architecture eventually outperforming the Evolved Transformers as model sizes grow.","is_blank","is_blank","[""patterson2021"", ""so19""]","[""https://www.nature.com/articles/s41467-018-04068-0"", ""https://arxiv.org/pdf/1910.09055.pdf""]","Figure 4 shows that the Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation.","The context discusses the Evolved Transformer and its performance compared to the vanilla Transformer on the WMT'24 EN-DE BLUE task. However, it does not provide specific information about the Transformer architecture eventually outperforming the Evolved Transformers as model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context mentions that using eight T4 instances can be more cost-efficient than a DGX-2 node for distributed training, especially considering the lower spot instance prices for older GPUs.","1","is_blank","[""erben2023""]","is_blank","While we used low spot prices as a cost-saving argument in our experiments, we did not elaborate on the most significant drawback - the possibility of being terminated by the cloud provider at any time. There is already some research on how different cloud providers track the interruption frequency and can be used for varying workloads to achieve a positive $-per-throughput effect [24, 42, 43].","The context mentions that using eight T4 instances can be more cost-efficient than a DGX-2 node for distributed training, especially considering the lower spot instance prices for older GPUs."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage, which directly contradicts the statement in the question.","0","is_blank","[""luccioni2025b""]","is_blank","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI‚Äôs greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage, which directly contradicts the statement in the question."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context mentions that the German Energy Efficiency Act of 2023 requires data centers to run on 50% renewable energy by January 1, 2027, but it does not specify that this must be 100% renewable energy by that date.","0","is_blank","[""ebert2024""]","is_blank","In Germany, the Energy Efficiency Act of 8 Nov 2023 implements the EED and establishes a national reporting scheme and additional requirements, including specific efficiency and renewable energy targets for data centers. The Act broadens the scope of the reporting obligation to include even smaller data centers, upwards of 300 kW (Sec. 13). It also expands the duty to set up an energy management system to data centers and operators of ICT‚Äîi.e., customers of colocation data centers‚Äîof more than 50 kW (Sec. 12). Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The context mentions that the German Energy Efficiency Act of 2023 requires data centers to run on 50% renewable energy by January 1, 2027, but it does not specify that this must be 100% renewable energy by that date."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of 60 papers, 90% of ACL papers target accuracy, which means 10% target both accuracy and efficiency.","6","papers","[""schwartz2019""]","is_blank","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efÔ¨Åciency result.","The context states that out of 60 papers, 90% of ACL papers target accuracy, which means 10% target both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context clearly states that inference can account for up to 90% of a model‚Äôs total lifecycle energy use.","90","percent","[""jegham2025"", ""luccioni2024"", ""samsi2024""]","[""https://arxiv.org/pdf/2505.09598.pdf"", ""https://arxiv.org/pdf/2406.13225.pdf"", ""https://arxiv.org/pdf/2310.03003.pdf""]","Recent estimates suggest inference can account for up to 90% of a model‚Äôs total lifecycle energy use [14, 15]. Inference energy costs already receive less attention than the energy costs of training LLMs‚Äîdespite how often these large models are called on to conduct inference in reality (e.g., ChatGPT).","The context clearly states that inference can account for up to 90% of a model‚Äôs total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context mentions that the AI Act does not require providers to report energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications. However, it does not explicitly state that the AI Act requires providers to report both training and inference energy consumption.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context mentions that the AI Act does not require providers to report energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications. However, it does not explicitly state that the AI Act requires providers to report both training and inference energy consumption."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context mentions that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context mentions that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, which contradicts the statement that they often rely on air cooling.","0","is_blank","[""li2025b""]","is_blank","In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","The context states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, which contradicts the statement that they often rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that platform-level caching reduces the operational energy footprint by more than 800√ó for the cross-lingual ML task (LM).","800","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800√ó using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization .","The context states that platform-level caching reduces the operational energy footprint by more than 800√ó for the cross-lingual ML task (LM)."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table listing CO2 emissions for training various models, including BERT. The row for BERT base model trained on 64 V100 GPUs for 79 hours shows 1507 kWh and 1438 lbs of CO2 emissions.","1438","lbs","[""strubell2019""]","is_blank","BERTbase V100x64 12,041.51 79 1507 1438 $3751‚Äì$12,571","The context provides a table listing CO2 emissions for training various models, including BERT. The row for BERT base model trained on 64 V100 GPUs for 79 hours shows 1507 kWh and 1438 lbs of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context mentions that ML inference reportedly accounts for 80‚Äì90% of the total compute demand, which directly answers the question.","80-90%","percent","[""chung2025"", ""patterson2021"", ""luccioni2024"", ""fernandez2025""]","[""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2107.00049"", ""https://arxiv.org/abs/2406.13765"", ""https://arxiv.org/abs/2504.17674""]","serving real-world services as ML inference reportedly accounts for 80‚Äì90% of the total compute demand [12, 32, 58, 60].
According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by Meta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the remainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training [40].","The context mentions that ML inference reportedly accounts for 80‚Äì90% of the total compute demand, which directly answers the question."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training a 6.1 billion parameter transformer model consumed 13.8 MWh of electricity. Since 1 MWh is equivalent to 1,000 kWh, 13.8 MWh is equal to 13,800 kWh. The question asks for the equivalent in 'household-years' of electricity consumption, which means how many households would consume this much electricity in a year. Assuming an average U.S. household consumes 10,000 kWh per year, the calculation is 13,800 kWh / 10,000 kWh/year = 1.38 household-years.","1.38","household-years","[""dodge2022""]","is_blank","The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ‚àó 13.8 = 103.5 MWh, or 103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!","The context states that training a 6.1 billion parameter transformer model consumed 13.8 MWh of electricity. Since 1 MWh is equivalent to 1,000 kWh, 13.8 MWh is equal to 13,800 kWh. The question asks for the equivalent in 'household-years' of electricity consumption, which means how many households would consume this much electricity in a year. Assuming an average U.S. household consumes 10,000 kWh per year, the calculation is 13,800 kWh / 10,000 kWh/year = 1.38 household-years."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly states that 'For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).'","1","is_blank","[""erben2023""]","is_blank","For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","The context explicitly states that 'For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).'"
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. To estimate the wall-clock time, we need to know the GPU utilization and speed, which are not provided.","is_blank","days","[""shen2024""]","is_blank","However, it serves as a good starting point for training JetMoE-8B and can be further optimized in future iterations.

The detailed data mixture can be found in Figure 2 and Table 2. It is important to note that given the limited computing budget available, our data mixture might not be ideal. However, it serves as a good starting point for training JetMoE-8B and can be further optimized in future iterations.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. To estimate the wall-clock time, we need to know the GPU utilization and speed, which are not provided."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explicitly states that water consumption is defined as 'water withdrawal minus water discharge', and it matches the description of the term in the question.","Water consumption","is_blank","[""li2025b""]","is_blank","‚Ä¢ Water consumption:It is defined as ‚Äúwater withdrawal minus water discharge‚Äù, and means the amount of water ‚Äúevaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment‚Äù [13].","The context explicitly states that water consumption is defined as 'water withdrawal minus water discharge', and it matches the description of the term in the question."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt depending on the number of GPUs used, ranging from 8 to 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt depending on the number of GPUs used, ranging from 8 to 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","is_blank","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context shows that Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference, which is a reduction of 0.005 kg. This represents a 55.56% decrease in carbon emissions.","55.56","percent","[""khan2025""]","is_blank","After optimization, Qwen's carbon emissions were 0.004 kg, compared to 0.009 kg before optimization, representing a 55.56% reduction.","The context shows that Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference, which is a reduction of 0.005 kg. This represents a 55.56% decrease in carbon emissions."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion. However, it does not specify the exact number of widely used model architectures included.","is_blank","models","[""chung2025""]","[""2https://github.com/ml-energy/benchmark""]","Model.The ML.ENERGY Benchmark already supports various popular architectures like Llama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A for a full list).","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion. However, it does not specify the exact number of widely used model architectures included."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context mentions that training an AI model of the Llama-3.1 scale in Altoona, Iowa can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car, resulting in a health cost that exceeds 120% of the training electricity cost. Table 2 provides the health cost for Altoona, Iowa as $2.5 million.","2.5 million","USD","[""han2024""]","is_blank","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context mentions that training an AI model of the Llama-3.1 scale in Altoona, Iowa can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car, resulting in a health cost that exceeds 120% of the training electricity cost. Table 2 provides the health cost for Altoona, Iowa as $2.5 million."
