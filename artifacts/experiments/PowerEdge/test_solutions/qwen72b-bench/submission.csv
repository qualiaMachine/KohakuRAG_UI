"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from wu2021b provides the specific percentage increase in U.S. data center electricity consumption from 2010 to 2014, which is 4%. The answer_unit 'percent' matches the format required.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet from wu2021b provides the specific percentage increase in U.S. data center electricity consumption from 2010 to 2014, which is 4%. The answer_unit 'percent' matches the format required."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. The answer is the difference between these two numbers, which is 4.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. The answer is the difference between these two numbers, which is 4."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that the embodied carbon emissions for a GPU are estimated to be 463 kg per GPU, based on the assumption of 3700 kg of CO2eq per 8x server node.","463","kg/GPU","[""morrison2025""]","[""https://example.com/morrison2025""]","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The context states that the embodied carbon emissions for a GPU are estimated to be 463 kg per GPU, based on the assumption of 3700 kg of CO2eq per 8x server node."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. The factor is calculated by dividing $40M by $100,000, which equals 400.","400","ratio","[""li2025a"", ""cottier2024""]","is_blank","The key idea of reducing the training cost of FLM-101B is to break through the fixed number of model parameters via a growth strategy. ... In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget. ... We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The estimated amortized training cost of GPT-4 is $40M, and the total training budget for FLM-101B is $100,000. The factor is calculated by dividing $40M by $100,000, which equals 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context directly states that a single passenger round trip SF-NY is ~1.2t CO2e. This matches the requested unit of tCO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context directly states that a single passenger round trip SF-NY is ~1.2t CO2e. This matches the requested unit of tCO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit 'score' matches the format of the provided score.","43.94","score","[""li2025a""]","is_blank","Results. On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit 'score' matches the format of the provided score."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the improvement in processor clock speed from the Intel 4004 (1971) to typical 2021 microprocessors is more than 6,750 fold.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the improvement in processor clock speed from the Intel 4004 (1971) to typical 2021 microprocessors is more than 6,750 fold."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3.","14.8","days","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides the GPU Power Usage for the Llama 3.2 1B model at an 8 request/s frequency, which is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates.","The context provides the GPU Power Usage for the Llama 3.2 1B model at an 8 request/s frequency, which is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx. This information is directly relevant to the question and matches the required unit of 'tons'.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx. This information is directly relevant to the question and matches the required unit of 'tons'."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is 'percent', so the answer_value is 72.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is 'percent', so the answer_value is 72."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context indicates that U.S. data centers could contribute to 1,300 premature deaths in 2028. Since the question asks for an estimate in 2030, and the trend is projected to continue, we can reasonably infer that the number of premature deaths in 2030 would be similar.","1300","deaths","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40].","The context indicates that U.S. data centers could contribute to 1,300 premature deaths in 2028. Since the question asks for an estimate in 2030, and the trend is projected to continue, we can reasonably infer that the number of premature deaths in 2030 would be similar."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that the 6.1 billion parameter model was trained for 8 days, which is 13% of the time it would take to train to completion. Therefore, the full training run would take 60 days.","60","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the 6.1 billion parameter model was trained for 8 days, which is 13% of the time it would take to train to completion. Therefore, the full training run would take 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context states that the GPU energy usage for 1,000 queries spans from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus). Command-R Plus is identified as the model with the highest GPU energy consumption.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context states that the GPU energy usage for 1,000 queries spans from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus). Command-R Plus is identified as the model with the highest GPU energy consumption."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context states that according to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled. The answer_unit 'percent' matches the format required.","22","percent","[""luccioni2025a""]","is_blank","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context states that according to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled. The answer_unit 'percent' matches the format required."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides the energy consumption for the final training run of the BLOOM-176B model, which is 51,686 kWh. Converting this to MWh gives 51.686 MWh.","51.686","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505","The context provides the energy consumption for the final training run of the BLOOM-176B model, which is 51,686 kWh. Converting this to MWh gives 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% are activated per token. The answer_unit is 'percent', so the answer_value is 0.1.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% are activated per token. The answer_unit is 'percent', so the answer_value is 0.1."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B sets the same number of experts to 8 for every layer. This directly answers the question about the number of experts in each MoE layer.","8","experts","[""shen2024""]","is_blank","We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that JetMoE-8B sets the same number of experts to 8 for every layer. This directly answers the question about the number of experts in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a breakdown of the execution time for different models and batch sizes, including the dense BlackMamba model with a batch size of 30. The total execution time is the sum of the forward, backward, and optimizer stages.","1.5","second","[""xia2024""]","is_blank","Dense(bsz=30) 1.5","The context provides a breakdown of the execution time for different models and batch sizes, including the dense BlackMamba model with a batch size of 30. The total execution time is the sum of the forward, backward, and optimizer stages."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context directly provides the cost for the English portion of the FLM-101B model, which is 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context directly provides the cost for the English portion of the FLM-101B model, which is 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context specifies the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context specifies the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context states that the study ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of the measurements.","88","models","[""luccioni2024""]","is_blank","The main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon emissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements.","The context states that the study ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of the measurements."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for LM training on GPUs decreases the overall carbon footprint by 3×. The answer_unit 'multiplier' indicates that the answer should be expressed as a factor, which is 3.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% for LM training on GPUs decreases the overall carbon footprint by 3×. The answer_unit 'multiplier' indicates that the answer should be expressed as a factor, which is 3."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) state that the total compute for model development is 1.2x to 4x larger than the final training run, with a median of 2.2x. The answer_unit 'multiplier' indicates that the answer should be expressed as a numeric range.","[1.2,4]","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) state that the total compute for model development is 1.2x to 4x larger than the final training run, with a median of 2.2x. The answer_unit 'multiplier' indicates that the answer should be expressed as a numeric range."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context indicates that the 6.1 billion parameter transformer model, when trained to 13% completion, consumed 13.8 MWh. A full training run would take 60 days, leading to an estimated total energy consumption of (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context indicates that the 6.1 billion parameter transformer model, when trained to 13% completion, consumed 13.8 MWh. A full training run would take 60 days, leading to an estimated total energy consumption of (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context mentions a 2025 paper that argues technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle known as Jevons' Paradox.","is_blank","is_blank","[""luccioni2025a""]","is_blank","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The context mentions a 2025 paper that argues technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle known as Jevons' Paradox."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context clearly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer is provided as a range in the specified unit.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United King- dom.","The context clearly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer is provided as a range in the specified unit."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost. Therefore, the statement that Red AI is on the decline is false.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost. Therefore, the statement that Red AI is on the decline is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context directly states that the total time cost for training FLM-101B is 21.54 days, which matches the requested unit of 'days'.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context directly states that the total time cost for training FLM-101B is 21.54 days, which matches the requested unit of 'days'."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50%, which contradicts the statement that a majority of workflows utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50%, which contradicts the statement that a majority of workflows utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context directly states that GPT-3's training energy consumption is estimated to be 1287 MWh. This matches the requested unit of MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context directly states that GPT-3's training energy consumption is estimated to be 1287 MWh. This matches the requested unit of MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project, which aims to create a standardized method for comparing the inference efficiency of various AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the 'AI Energy Score' project, which aims to create a standardized method for comparing the inference efficiency of various AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a figure (Fig. 6) that shows the execution time breakdown for different kernels in the MoE layer. For the dense BlackMamba model with a batch size of 30, the longest kernel is 'matmul(w2)' with an execution time of 1600 microseconds.","1600","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context provides a figure (Fig. 6) that shows the execution time breakdown for different kernels in the MoE layer. For the dense BlackMamba model with a batch size of 30, the longest kernel is 'matmul(w2)' with an execution time of 1600 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B sets the top-k to 2 for every layer, meaning 2 experts are selected for activation for a given token.","2","experts","[""shen2024""]","is_blank","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that JetMoE-8B sets the top-k to 2 for every layer, meaning 2 experts are selected for activation for a given token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years (2012-2018), which is a larger increase than the 200,000x mentioned in the question.","1","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years (2012-2018), which is a larger increase than the 200,000x mentioned in the question."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context states that the global carbon emissions for 2020 dropped by 6.4%. The answer_unit is 'percent', so the answer_value is 6.4.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context states that the global carbon emissions for 2020 dropped by 6.4%. The answer_unit is 'percent', so the answer_value is 6.4."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of AI was officially born in 1956. Therefore, in 2025, the field would be approximately 69 years old.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context states that the field of AI was officially born in 1956. Therefore, in 2025, the field would be approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' estimate is based on the neural architecture search (NAS) process, which is a large-scale procedure performed less frequently than average AI model training.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The 'five cars' estimate is based on the neural architecture search (NAS) process, which is a large-scale procedure performed less frequently than average AI model training."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory, the maximum batch size is 8 samples.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory, the maximum batch size is 8 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context mentions that Amazon's energy storage capacity is 1.3 GW as of 2023, which matches the required unit of GW.","1.3","GW","[""amazon2023""]","is_blank","1.3 GW Energy storage capacity, up from 445 megawatts (MW) in 2022","The context mentions that Amazon's energy storage capacity is 1.3 GW as of 2023, which matches the required unit of GW."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides the global average PUE for data centers in 2023, which is 1.58. The answer is provided in the correct unit (PUE).","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the global average PUE for data centers in 2023, which is 1.58. The answer is provided in the correct unit (PUE)."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference.","2000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides the GHG emissions for the Llama 7B model, which is 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e) Llama 7B 63 Meta 356 14","The context provides the GHG emissions for the Llama 7B model, which is 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans deployed in 2022 and 2023 for the U.S., Europe, and India. By summing these values, we get the total number of electric delivery vans added across 2022 and 2023.","19020","electric delivery vans","[""amazon2023""]","is_blank","Last Mile Electric Delivery Vehicles by Region 7
Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+","The context provides the number of electric delivery vans deployed in 2022 and 2023 for the U.S., Europe, and India. By summing these values, we get the total number of electric delivery vans added across 2022 and 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides the energy consumption for the o3 model for a long prompt, which is 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model: o3, Energy Consumption(10k input-1.5k output)(Wh): 12.222 ± 1.082","The context provides the energy consumption for the o3 model for a long prompt, which is 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context clearly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context mentions that Google's annualized global on-site water efficiency is 1 L/kWh, which is the WUE for their data centers. This value is consistent with the WUE for Google's AI-dedicated data centers in 2024.","1","WUE","[""li2025b""]","is_blank","On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google’s annualized global on-site water efficiency [4] and 9 L/kWh for a large commercial data center during the summer in Arizona [16].","The context mentions that Google's annualized global on-site water efficiency is 1 L/kWh, which is the WUE for their data centers. This value is consistent with the WUE for Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.","1","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that with a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token.","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that with a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. The answer_unit is 'percent', so the answer_value is 15.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. The answer_unit is 'percent', so the answer_value is 15."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the claim of AI reducing global GHG emissions by 5-10% lacks clear, publicly available calculations and scientific grounding. The underlying calculations are not detailed, and the estimate is based on BCG's experience rather than rigorous scientific methods.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context indicates that the claim of AI reducing global GHG emissions by 5-10% lacks clear, publicly available calculations and scientific grounding. The underlying calculations are not detailed, and the estimate is based on BCG's experience rather than rigorous scientific methods."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context directly states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy.","1","is_blank","[""patterson2021""]","is_blank","● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context directly states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context directly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the energy consumption of Flan-T5-xxl as 0.083 kWh per 1,000 queries. Multiplying this by 1 billion queries gives the daily energy consumption in kWh, which is then converted to MWh.","830","MWh","[""luccioni2024""]","[""https://example.com/luccioni2024""]","Flan-T5-xxl: 0.083 kWh/1k queries","The context provides the energy consumption of Flan-T5-xxl as 0.083 kWh per 1,000 queries. Multiplying this by 1 billion queries gives the daily energy consumption in kWh, which is then converted to MWh."
"q067","What was the average global data center PUE in 2023?","The context snippet from [ref_id=ebert2024] directly states the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally [74] and 1.6 in the EU [26].","The context snippet from [ref_id=ebert2024] directly states the average data center PUE in 2023 was 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain specific information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that R&D staff costs, including equity, make up 49% of the total amortized model development costs for Gemini Ultra.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that R&D staff costs, including equity, make up 49% of the total amortized model development costs for Gemini Ultra."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context provides evidence that a model with more parameters does not always consume more energy during inference. Specifically, the Phi-3 Mini (3.8B) and Phi-3 Small (7B) models show that the larger model can consume less energy than the smaller one as batch size grows.","0","is_blank","[""chung2025""]","is_blank","Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The context provides evidence that a model with more parameters does not always consume more energy during inference. Specifically, the Phi-3 Mini (3.8B) and Phi-3 Small (7B) models show that the larger model can consume less energy than the smaller one as batch size grows."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, contrary to more fantastic predictions in the popular press.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, contrary to more fantastic predictions in the popular press."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides the GHG emissions for Meta's Llama 3 family of models as 11,390 tons CO2e, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.","The context provides the GHG emissions for Meta's Llama 3 family of models as 11,390 tons CO2e, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context clearly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context clearly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program defeated the human Go champion, which is a direct answer to the question.","1","is_blank","[""stone2022""]","is_blank","The recent success of  AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context clearly states that the AlphaGo program defeated the human Go champion, which is a direct answer to the question."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions 'Continuous batching' as the strategy that reduces idle GPU time by dynamically replacing completed requests with new ones.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions 'Continuous batching' as the strategy that reduces idle GPU time by dynamically replacing completed requests with new ones."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for the 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that for the 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides the range of GPU energy usage for 1,000 inference queries, which spans from 0.06 Wh to 3,426 Wh, based on the models listed in the appendix of a 2025 study.","[0.06, 3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context provides the range of GPU energy usage for 1,000 inference queries, which spans from 0.06 Wh to 3,426 Wh, based on the models listed in the appendix of a 2025 study."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context indicates that it is difficult to define universal or even generalizable guidelines for AI ethics and sustainability due to the complexity and contextual nature of AI applications.","0","is_blank","[""luccioni2025b""]","is_blank","However, given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.","The context indicates that it is difficult to define universal or even generalizable guidelines for AI ethics and sustainability due to the complexity and contextual nature of AI applications."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context snippet from [ref_id=patterson2021] provides the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context snippet from [ref_id=patterson2021] provides the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework that enables distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions Hivemind as a PyTorch-based framework that enables distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly mentions 'social transparency' as the term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context explicitly mentions 'social transparency' as the term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2501.12948""]","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context clearly states that the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation, is named Lamina.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context clearly states that the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation, is named Lamina."
"q093","How many parameters does the largest T5 model have?","The context provides the number of parameters for the largest T5 model, which is 11 billion parameters.","11000000000","parameters","[""luccioni2024""]","is_blank","Flan-T5-xxl 11B 11.48 0.083","The context provides the number of parameters for the largest T5 model, which is 11 billion parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters.","8000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022. The answer is provided directly in the context and matches the required unit of percent.","20","percent","[""li2025b""]","is_blank","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4]","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022. The answer is provided directly in the context and matches the required unit of percent."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed'. This matches the requested metric definition.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed'. This matches the requested metric definition."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides the FLOPs utilization percentage for the 101B stage, which is 52.88%. The answer_unit is 'percent', so the answer_value is 52.88.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88%","The context provides the FLOPs utilization percentage for the 101B stage, which is 52.88%. The answer_unit is 'percent', so the answer_value is 52.88."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context explicitly states that the estimated amortized hardware and energy cost for training GPT-4 is $40M.","40000000","USD","[""cottier2024""]","is_blank","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context explicitly states that the estimated amortized hardware and energy cost for training GPT-4 is $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context indicates that starting with a CPU server baseline, platform-level caching improves power efficiency by 6.7×, GPU acceleration provides an additional 10.1× improvement, and algorithmic optimizations provide another 12× improvement. The combined effect of these optimizations reduces the operational carbon footprint by more than 800×.","800","multiplier","[""wu2021a""]","is_blank","Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 ×. In addition to caching, deploying LM across GPU-based specialized AI hardware unlocks an additional 10.1× energy efficiency improvement. Finally, algorithmic optimizations provide an additional 12 × energy efficiency reduction. In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800 ×.","The context indicates that starting with a CPU server baseline, platform-level caching improves power efficiency by 6.7×, GPU acceleration provides an additional 10.1× improvement, and algorithmic optimizations provide another 12× improvement. The combined effect of these optimizations reduces the operational carbon footprint by more than 800×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP, the throughput when training across four continents (C-8) was 41% slower compared to the fully local experiment (A-8). This means the throughput was 59% of the local throughput.","0.59","multiplier","[""erben2023""]","is_blank","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for NLP, the throughput when training across four continents (C-8) was 41% slower compared to the fully local experiment (A-8). This means the throughput was 59% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit is 'liters', so the answer_value is 3500000000.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context clearly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit is 'liters', so the answer_value is 3500000000."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context clearly states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This directly supports a 'True' answer.","1","is_blank","[""rubei2025""]","is_blank","Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context clearly states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This directly supports a 'True' answer."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context directly states that NVIDIA shipped 3.7 million GPUs in 2024, as reported in a 2025 paper. The answer is provided in the correct unit (GPUs).","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context directly states that NVIDIA shipped 3.7 million GPUs in 2024, as reported in a 2025 paper. The answer is provided in the correct unit (GPUs)."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a PUE of 1.10, which matches the expected format of PUE.","1.1","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.","The context explicitly states that Facebook's data centers have a PUE of 1.10, which matches the expected format of PUE."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context directly mentions the Finnish project ETAIROS (Ethical AI for the Governance of the Society) as the one that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","The context directly mentions the Finnish project ETAIROS (Ethical AI for the Governance of the Society) as the one that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context explicitly states that the estimated amortized training cost for Google's Gemini Ultra is $30M. The answer is provided in the correct unit (USD).","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context explicitly states that the estimated amortized training cost for Google's Gemini Ultra is $30M. The answer is provided in the correct unit (USD)."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and HRAI systems, and that these measures should include environmental risks.","1","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and HRAI systems, and that these measures should include environmental risks."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context clearly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. The answer_unit is µg/m³, so the answer_value is 9.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/regulations-emissions-vehicles-and-engines/final-rule-multi-pollutant-emissions-standards-model""]","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context clearly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. The answer_unit is µg/m³, so the answer_value is 9."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit is 'books', so the answer_value is 115.","115","books","[""luccioni2025a""]","is_blank","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit is 'books', so the answer_value is 115."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the ratio of the highest to lowest county-level per-household health cost reaches approximately 200, indicating the factor by which the per-household health burden in the most affected, economically-disadvantaged communities exceeds that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","The ratio of the highest to lowest county-level per-household health cost reaches approximately 200.","The context states that the ratio of the highest to lowest county-level per-household health cost reaches approximately 200, indicating the factor by which the per-household health burden in the most affected, economically-disadvantaged communities exceeds that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides the energy consumption for the DS Llama 70B model on the FKTG dataset, which is 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context provides the energy consumption for the DS Llama 70B model on the FKTG dataset, which is 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox, where technological progress improves efficiency, leading to increased usage and overall resource consumption.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","In the centuries since the concept of Jevons’ paradox was developed, similar rebound effects have been observed with respect to energy [ 40] and water [ 33], as well as in domains such as road travel (where improvements to roads have been found to result in increased congestion [126]) and agriculture (where increasing the yield of a crop makes it more profitable to grow it, thereby increasing land use overall [11]).","The context describes Jevons' Paradox, where technological progress improves efficiency, leading to increased usage and overall resource consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides the energy consumption for training Meena (232 MWh) and GPT-3 (1,287 MWh). By dividing the energy consumption of GPT-3 by that of Meena, we get the multiplier.","5.55","multiplier","[""patterson2021""]","is_blank","Training GShard-600B used 24 MWh and produced 4.3 net tCO 2 e. Training GPT-3 used 1,287 MWh and produced 552.1 net tCO 2 e.","The context provides the energy consumption for training Meena (232 MWh) and GPT-3 (1,287 MWh). By dividing the energy consumption of GPT-3 by that of Meena, we get the multiplier."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
image generation 2.907 3.31","According to Table 2 in the 2024 study, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context snippet from strubell2019 provides the CO2e emissions for an average American life in one year, which is 36,156 lbs.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","The context snippet from strubell2019 provides the CO2e emissions for an average American life in one year, which is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a list of top-10 counties by per-household health cost, and Marion County in West Virginia is listed as having the highest per-household health cost among the counties mentioned.","Marion","is_blank","[""han2024""]","is_blank","WV Marion 1218.3(978.0, 1458.5) 0.80","The context provides a list of top-10 counties by per-household health cost, and Marion County in West Virginia is listed as having the highest per-household health cost among the counties mentioned."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides the carbon emissions for Mistral-small before and after optimization. Before optimization, the emissions were 0.020 kg, and after optimization, they were 0.015 kg. To find the multiplier, we divide the post-optimization emissions by the pre-optimization emissions: 0.015 / 0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLM S BEFORE AND AFTER OPTIMIZATION. Model Name Precision Recall F1 Accuracy CO2 (kg) Before Optimization Mistral-small 0.70 0.67 0.65 0.67 0.020 After Optimization Mistral-small 0.73 0.70 0.69 0.70 0.015","The context provides the carbon emissions for Mistral-small before and after optimization. Before optimization, the emissions were 0.020 kg, and after optimization, they were 0.015 kg. To find the multiplier, we divide the post-optimization emissions by the pre-optimization emissions: 0.015 / 0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The combined training and fine-tuning energy costs for the BLOOMz-7B model are the sum of the training energy (51,686 kWh) and the fine-tuning energy (7,571 kWh) as reported in the 'Power Hungry Processing' study. The answer is expressed in kWh as specified by the answer_unit.","59257","kWh","[""luccioni2024""]","[""https://example.com/luccioni2024""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543","The combined training and fine-tuning energy costs for the BLOOMz-7B model are the sum of the training energy (51,686 kWh) and the fine-tuning energy (7,571 kWh) as reported in the 'Power Hungry Processing' study. The answer is expressed in kWh as specified by the answer_unit."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that the final FLM-101B model has 101 billion parameters, which matches the requested unit of 'parameters'.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the final FLM-101B model has 101 billion parameters, which matches the requested unit of 'parameters'."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the training energy cost for BLOOMz-7B, which is 51,686 kWh. Since the 6.1B parameter model is not explicitly mentioned, we assume it is similar to BLOOMz-7B. The number of inferences required to match this training energy cost is 592,570,000, as stated in Table 5.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the training energy cost for BLOOMz-7B, which is 51,686 kWh. Since the 6.1B parameter model is not explicitly mentioned, we assume it is similar to BLOOMz-7B. The number of inferences required to match this training energy cost is 592,570,000, as stated in Table 5."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context directly states the total energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' was 754.66 kWh. The answer_unit is specified as 'kWh', so the answer_value is 754.66.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context directly states the total energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' was 754.66 kWh. The answer_unit is specified as 'kWh', so the answer_value is 754.66."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the number of inferences required for the BLOOMz-7B model to reach cost parity with its training and fine-tuning energy costs, which is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the number of inferences required for the BLOOMz-7B model to reach cost parity with its training and fine-tuning energy costs, which is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used for the experiments is called the FKTG-dataset, which contains statements from the population regarding the selection of a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the dataset used for the experiments is called the FKTG-dataset, which contains statements from the population regarding the selection of a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. Since the actual CO2e for the Evolved Transformer NAS is 3.2 tCO2e, it is equivalent to approximately 3 passengers.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. Since the actual CO2e for the Evolved Transformer NAS is 3.2 tCO2e, it is equivalent to approximately 3 passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that 84% of LLM usage is through models with no disclosure, according to May 2025 data from OpenRouter.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that 84% of LLM usage is through models with no disclosure, according to May 2025 data from OpenRouter."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that the bare minimum hardware required for LLaMA-13B is 1 A100 80GB GPU, with a maximum batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
13B 2 64 1 64","The context states that the bare minimum hardware required for LLaMA-13B is 1 A100 80GB GPU, with a maximum batch size of 64."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that the partial training of a 6.1 billion parameter transformer model consumed 13.8 MWh, and a full training run would consume approximately 103.5 MWh, or 103,500 kWh. The conversion from kWh to metric tons of CO2 is not directly provided, but the energy consumption is given. Assuming the context's estimate of 103,500 kWh, we convert this to metric tons of CO2 using a typical conversion factor (e.g., 0.5 kg CO2/kWh), resulting in 51.75 metric tons.","51.75","metric tons","[""dodge2022""]","is_blank","We estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that the partial training of a 6.1 billion parameter transformer model consumed 13.8 MWh, and a full training run would consume approximately 103.5 MWh, or 103,500 kWh. The conversion from kWh to metric tons of CO2 is not directly provided, but the energy consumption is given. Assuming the context's estimate of 103,500 kWh, we convert this to metric tons of CO2 using a typical conversion factor (e.g., 0.5 kg CO2/kWh), resulting in 51.75 metric tons."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only. The answer_unit 'percent' matches the format of the answer.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only. The answer_unit 'percent' matches the format of the answer."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides the price per hour for the NVIDIA H20 as $4.63/hr.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context provides the price per hour for the NVIDIA H20 as $4.63/hr."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the statement in the question.","0","is_blank","[""luccioni2025b""]","is_blank","For instance, most carbon footprint analyses gather the information manually by writing to authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the statement in the question."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context indicates that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household, which is equivalent to approximately 44% of the data centers’ total electricity cost.","44","percent","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context indicates that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household, which is equivalent to approximately 44% of the data centers’ total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware required for LLaMA 7B is 1 A100 80GB GPU, with a maximum batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that the bare minimum hardware required for LLaMA 7B is 1 A100 80GB GPU, with a maximum batch size of 64."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which directly supports the statement.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context explicitly states that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which directly supports the statement."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were able to collect 95 answers.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B is trained with a $100k budget and 30,000 H100 GPU hours. Dividing the budget by the total GPU hours gives the cost per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B is trained with a $100k budget and 30,000 H100 GPU hours. Dividing the budget by the total GPU hours gives the cost per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in Altoona, Iowa, which is 122%. The answer_unit 'percent' matches the format required.","122","percent","[""han2024""]","is_blank","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context provides the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in Altoona, Iowa, which is 122%. The answer_unit 'percent' matches the format required."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained using 1.25T tokens from mixed open-source datasets. The answer_unit 'tokens' matches the unit used in the context.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B is trained using 1.25T tokens from mixed open-source datasets. The answer_unit 'tokens' matches the unit used in the context."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table listing the number of Amazon Renewable Energy Projects announced in various countries as of January 2024. According to the table, the United Kingdom has 36 projects.","36","projects","[""amazon2023""]","is_blank","United Kingdom 36 901","The context provides a table listing the number of Amazon Renewable Energy Projects announced in various countries as of January 2024. According to the table, the United Kingdom has 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage of men in Amazon's U.S. workforce across all levels as 56.8% for 2023.","56.8","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) 43.1%56.8%","The context provides the percentage of men in Amazon's U.S. workforce across all levels as 56.8% for 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports that its supply chain accounts for 99% of its total water footprint. The answer_unit is 'percent', so the answer_value is 99.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports that its supply chain accounts for 99% of its total water footprint. The answer_unit is 'percent', so the answer_value is 99."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides a figure (Fig. 4) that shows the execution time breakdown for different models and batch sizes. For a sparse BlackMamba model with a batch size of 84, the total execution time is the sum of the forward, backward, and optimizer stages.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","The context provides a figure (Fig. 4) that shows the execution time breakdown for different models and batch sizes. For a sparse BlackMamba model with a batch size of 84, the total execution time is the sum of the forward, backward, and optimizer stages."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions that the granularity metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents.","granularity","is_blank","[""erben2023""]","is_blank","We propose the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs.","The context mentions that the granularity metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year.","640","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year","The context states that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, and then used for various purposes.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, and then used for various purposes."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","As its core activity , the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of  AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context clearly states that the average US household has 25 connected devices, which matches the requested unit 'devices'.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context clearly states that the average US household has 25 connected devices, which matches the requested unit 'devices'."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training LLMs, from 0.8 MWh to 3,500 MWh.","[0.8, 3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides a range of energy consumption for pre-training LLMs, from 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement is false.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that 10-50 queries on GPT-3 consume around half a liter of water. This directly answers the question with the specified unit of 'queries'.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context states that 10-50 queries on GPT-3 consume around half a liter of water. This directly answers the question with the specified unit of 'queries'."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, with a score of 6.681.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models
JetMoE-8B-chat 6.681","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, with a score of 6.681."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This directly provides the answer in the required unit of 'responses'.","[10,50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This directly provides the answer in the required unit of 'responses'."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit 'percent' indicates that the answer should be expressed as a percentage.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit 'percent' indicates that the answer should be expressed as a percentage."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context consistently mentions that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context consistently mentions that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context snippet from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload is inference processing. The answer_unit is 'percent', so the answer_value is formatted as a numeric range.","[80,90]","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context snippet from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload is inference processing. The answer_unit is 'percent', so the answer_value is formatted as a numeric range."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context directly states the total CO2 equivalent emissions generated by the 'Power Hungry Processing' study, which is 178.97 kg CO2eq.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context directly states the total CO2 equivalent emissions generated by the 'Power Hungry Processing' study, which is 178.97 kg CO2eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context clearly states that estimating GPU energy consumption based on TDP is nearly always an overestimation and can lead to a worst-case overestimation by a factor of 4.1. Therefore, it is not a reliable and accurate method.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context clearly states that estimating GPU energy consumption based on TDP is nearly always an overestimation and can lead to a worst-case overestimation by a factor of 4.1. Therefore, it is not a reliable and accurate method."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes 3.098 Wh for long queries, which is higher than the 2.875 Wh consumed by GPT-4o. This directly contradicts the statement.","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context indicates that GPT-4o mini consumes 3.098 Wh for long queries, which is higher than the 2.875 Wh consumed by GPT-4o. This directly contradicts the statement."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a throughput value for dense Mixtral-CS-A100-40GB at batch size 1, which is 0.3 queries/sec.","0.3","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0 0.5 1.0 1.5 2.0 0.3 0.5 0.3 0.7 1.7 Dense(bsz=1) Dense(bsz=2) Sparse(bsz=1) Sparse(bsz=2) Sparse(bsz=8)","The context provides a throughput value for dense Mixtral-CS-A100-40GB at batch size 1, which is 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022 and then declined, contradicting the statement in the question.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that the direct release of environmental information peaked in 2022 and then declined, contradicting the statement in the question."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the calculation for normalizing the H100 pricing to match the pricing structures of major platforms, resulting in a normalized price of $7.516 per hour.","7.516","USD per hour","[""griggs2024""]","is_blank","We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.","The context provides the calculation for normalizing the H100 pricing to match the pricing structures of major platforms, resulting in a normalized price of $7.516 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","is_blank","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To find the hourly cost, we divide $5,200 by 30 days and then by 24 hours.","7.22","USD per hour","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To find the hourly cost, we divide $5,200 by 30 days and then by 24 hours."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model needs to be 1,000 times larger in size.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model needs to be 1,000 times larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training and neural architecture search for a Transformer model as 626,155 lbs. Assuming the emissions-to-driving-distance ratio is 1 pound of CO2 per mile driven, the equivalent driving distance is 626,155 miles.","626155","miles","[""patterson2021""]","is_blank","The estimate in [Str19] was done for the hypothetical scenario of running the computation on P100 GPUs in the average U.S. datacenter with the average U.S. grid energy mix. The authors of this note represent a superset of the authors of [So19], and we agree that the information needed for an accurate estimate was scattered in several subsections in the So et al. paper, which makes it difficult to determine the actual CO2e.","The context provides the CO2 emissions for training and neural architecture search for a Transformer model as 626,155 lbs. Assuming the emissions-to-driving-distance ratio is 1 pound of CO2 per mile driven, the equivalent driving distance is 626,155 miles."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The inference energy for BLOOMz-7B is 1.0 × 10^-4 kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy consumption is calculated as 606,096 * 1,000,000 * 1.0 × 10^-4 kWh, which equals 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The inference energy for BLOOMz-7B is 1.0 × 10^-4 kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy consumption is calculated as 606,096 * 1,000,000 * 1.0 × 10^-4 kWh, which equals 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer is provided in USD as specified by the answer_unit.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer is provided in USD as specified by the answer_unit."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippet from [ref_id=patterson2021] states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context snippet from [ref_id=patterson2021] states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This matches the requested unit of 'V100_32GB_GPUs'.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This matches the requested unit of 'V100_32GB_GPUs'."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The throughput for the 101B training stage is 165 teraFLOPs/sec. The total training time for FLM-101B is 21.54 days. Converting this time to seconds and multiplying by the throughput gives the total computational work in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88% ... Under this growth schedule, the total time cost for training FLM-101B is 21.54 days.","The throughput for the 101B training stage is 165 teraFLOPs/sec. The total training time for FLM-101B is 21.54 days. Converting this time to seconds and multiplying by the throughput gives the total computational work in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide the specific top-1 accuracy on ImageNet for AlexNet 2012.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide the specific top-1 accuracy on ImageNet for AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This indicates a total of 192 A800 GPUs (24 servers * 8 GPUs/server). The answer_unit is 'GPUs', so the answer_value is 192.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This indicates a total of 192 A800 GPUs (24 servers * 8 GPUs/server). The answer_unit is 'GPUs', so the answer_value is 192."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides the estimated CO2 emissions for performing neural architecture search (NAS) to train a Transformer-based model for machine translation as 626,155 pounds (284 metric tons) CO2e. This is equivalent to about 17.3 average American lifetimes.","17.3","lifetimes","[""dodge2022"", ""luccioni2023"", ""strubell2019"", ""patterson2021""]","is_blank","Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption. 1 Sources: (1) Air travel and per-capita consumption: https://bit.ly/2Hw0xWc; (2) car lifetime: https://bit.ly/2Qbr0w1.","The context provides the estimated CO2 emissions for performing neural architecture search (NAS) to train a Transformer-based model for machine translation as 626,155 pounds (284 metric tons) CO2e. This is equivalent to about 17.3 average American lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer is provided directly in the required unit (metric tons).","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer is provided directly in the required unit (metric tons)."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that the vllm library was used to deploy large language models across multiple GPUs and nodes.","vllm","is_blank","[""zschache2025""]","is_blank","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions that the vllm library was used to deploy large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context states that using two nodes increased energy consumption by a factor that depends on the model. For the Llama 3.1 70B model, the increase in energy consumption is 1.95 times.","1.95","multiplier","[""zschache2025""]","is_blank","As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2). This increase stems from the overhead... Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","The context states that using two nodes increased energy consumption by a factor that depends on the model. For the Llama 3.1 70B model, the increase in energy consumption is 1.95 times."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","is_blank","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that 700 million daily GPT-4o queries aggregate to an annual electricity use comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.","The context states that 700 million daily GPT-4o queries aggregate to an annual electricity use comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippet from luccioni2025a states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons","The context snippet from luccioni2025a states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models performed considerably worse than LLMs in the Yelp sentiment analysis, indicating that they did not achieve comparable accuracy.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models performed considerably worse than LLMs in the Yelp sentiment analysis, indicating that they did not achieve comparable accuracy."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that the total estimated number of GPT-4o queries in 2025 is approximately 772 billion.","772000000000","queries","[""jegham2025""]","is_blank","yielding a total of approximately 772 billion GPT-4o queries in 2025","The context states that the total estimated number of GPT-4o queries in 2025 is approximately 772 billion."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the OpenLLM Leaderboard average score for JetMoE-8B, which is 53.0. The answer_unit 'score' indicates that the answer_value should be the numerical score.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context provides the OpenLLM Leaderboard average score for JetMoE-8B, which is 53.0. The answer_unit 'score' indicates that the answer_value should be the numerical score."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","is_blank","is_blank","training runs","[""is_blank""]","is_blank","is_blank","is_blank"
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This supports the statement that they are fully exempt unless they pose systemic risk.","1","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This supports the statement that they are fully exempt unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context does not provide the specific PUE value for US data centers in 2020. The closest information is the global average PUE of 1.58 in 2023.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide the specific PUE value for US data centers in 2020. The closest information is the global average PUE of 1.58 in 2023."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context explicitly states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size expanded to 5.312GB when the batch size increased to 32.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context explicitly states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size expanded to 5.312GB when the batch size increased to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that for the four notable models studied in-depth (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized cost.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context states that for the four notable models studied in-depth (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context clearly states that the CodeCarbon package was used to measure energy consumption during inference runs.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","is_blank","['The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).', 'In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking, which was verified against the same time series monitoring used throughout training.']","The context clearly states that the CodeCarbon package was used to measure energy consumption during inference runs."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 53% of the 100 news articles analyzed cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. The answer is expressed as a percentage as specified by the answer_unit.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42","The context states that 53% of the 100 news articles analyzed cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. The answer is expressed as a percentage as specified by the answer_unit."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions the Compute Time Calibration Function (CTCF) as the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions the Compute Time Calibration Function (CTCF) as the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA 65B.","1","is_blank","[""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA 65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water, and one 12-inch silicon wafer weighing 125 grams produces about 63 H100s. Assuming an H100 is 0.1% rare earth metal by mass, the total water consumption for mining the rare earth materials for one H100 is 2.2 liters, which is 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water, and one 12-inch silicon wafer weighing 125 grams produces about 63 H100s. Assuming an H100 is 0.1% rare earth metal by mass, the total water consumption for mining the rare earth materials for one H100 is 2.2 liters, which is 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This suggests that they do not have to report energy consumption to authorities.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This suggests that they do not have to report energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context from 'luccioni2025a' and 'wu2021b' both indicate that in 2020, Amazon, Microsoft, Meta, and Google accounted for 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.', 'In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].']","The context from 'luccioni2025a' and 'wu2021b' both indicate that in 2020, Amazon, Microsoft, Meta, and Google accounted for 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that the total public health cost of U.S. data centers in 2023, based on the average attribution method, is about $6.7 billion.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The context states that the total public health cost of U.S. data centers in 2023, based on the average attribution method, is about $6.7 billion."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model for a long prompt is 12.222 Wh, while that of GPT-4.1 nano is 0.827 Wh. The factor is calculated by dividing 12.222 by 0.827, resulting in approximately 14.78.","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model Energy Consumption(100 input-300 output)(Wh) Energy Consumption(1k input-1k output)(Wh) Energy Consumption(10k input-1.5k output)(Wh) o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082 GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094","The energy consumption of the o3 model for a long prompt is 12.222 Wh, while that of GPT-4.1 nano is 0.827 Wh. The factor is calculated by dividing 12.222 by 0.827, resulting in approximately 14.78."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO. The answer is formatted as a percentage range.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO. The answer is formatted as a percentage range."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the net carbon emissions for FLM-101B in Table 3, which is 26 tCO2e.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The context provides the net carbon emissions for FLM-101B in Table 3, which is 26 tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides an execution time breakdown for a sparse Mixtral model with a batch size of 1 on a NVIDIA A40-48 GB GPU, which is 1.0 second.","1.0","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0 2.0 4.0 6.0 8.0 Forward Backward Optimizer Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0 0.5 1.0 1.5 2.0 Execution Time Breakdown (seconds) Mixtral Mamba Fig. 4. Execution time breakdown.","The context provides an execution time breakdown for a sparse Mixtral model with a batch size of 1 on a NVIDIA A40-48 GB GPU, which is 1.0 second."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health costs of AI are not evenly distributed across communities, with low-income communities disproportionately affected.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The context clearly states that the public health costs of AI are not evenly distributed across communities, with low-income communities disproportionately affected."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context from wu2021b states that GPU theoretical performance per watt doubles every 3-4 years as of 2019 product data.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context from wu2021b states that GPU theoretical performance per watt doubles every 3-4 years as of 2019 product data."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions Ollama as the open-source tool used for local AI model deployment, which ensures data privacy and supports a variety of pre-trained and fine-tuned models. It also applies 4-bit quantization to reduce computational and memory requirements.","Ollama","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions.","The context mentions Ollama as the open-source tool used for local AI model deployment, which ensures data privacy and supports a variety of pre-trained and fine-tuned models. It also applies 4-bit quantization to reduce computational and memory requirements."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that Backblaze (B2) was used as an independent S3 storage provider to shard and stream datasets for spot VMs that could terminate at any time.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].","The context mentions that Backblaze (B2) was used as an independent S3 storage provider to shard and stream datasets for spot VMs that could terminate at any time."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context mentions a strong correlation between duration and energy consumption, indicating a nearly linear relationship. This is supported by the statement that 'If the power is constant over time, this correlation should be linear.'","is_blank","is_blank","[""zschache2025""]","is_blank","Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear. Figure 6 illustrates this relationship for","The context mentions a strong correlation between duration and energy consumption, indicating a nearly linear relationship. This is supported by the statement that 'If the power is constant over time, this correlation should be linear.'"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in February 2024.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].","The context clearly states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in February 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides the price per hour for an NVIDIA H100 as $7.5164. This value is directly stated in the context and matches the required unit of USD per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)
On-demand Price ($/h) 0.7 1.01 3.67 7.5164
Instance Provider GCP AWS Azure RunPod
Instance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A.","The context provides the price per hour for an NVIDIA H100 as $7.5164. This value is directly stated in the context and matches the required unit of USD per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context states that the average expected server lifetime improved from five to six years in 2024, which can be interpreted as the average GPU lifetime before retirement.","6","years","[""amazon2023""]","is_blank","In February 2024, AWS announced that the average expected server lifetime had improved from five to six years.","The context states that the average expected server lifetime improved from five to six years in 2024, which can be interpreted as the average GPU lifetime before retirement."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that the bare minimum hardware required for LLaMA-13B is 2 V100 32GB GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context states that the bare minimum hardware required for LLaMA-13B is 2 V100 32GB GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the estimate that forms the basis for the ‘five cars’ number.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ‘five cars’ number","The context states that Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the estimate that forms the basis for the ‘five cars’ number."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). I converted the 2 weeks to hours as specified by the answer_unit.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). I converted the 2 weeks to hours as specified by the answer_unit."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. The answer_unit is 'PUE', so the answer_value is expressed as a number without the unit.","1.1","PUE","[""dodge2022""]","[""https://example.com/dodge2022""]","Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. The answer_unit is 'PUE', so the answer_value is expressed as a number without the unit."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context directly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is $3460. The answer_unit is USD, so the answer_value is 3460.","3460","USD","[""xia2024""]","is_blank","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","The context directly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is $3460. The answer_unit is USD, so the answer_value is 3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that in a typical datacenter, GPUs account for 74% of the total provisioned power when training a BERT base model on a single NVIDIA TITAN X GPU. This percentage is derived from the provided table and the associated text.","74","percent","[""dodge2022""]","[""FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea Dodge et al.""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context indicates that in a typical datacenter, GPUs account for 74% of the total provisioned power when training a BERT base model on a single NVIDIA TITAN X GPU. This percentage is derived from the provided table and the associated text."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the training infrastructure for JetMoE-8B consisted of a cluster containing 12 nodes and 96 H100s. The answer is directly provided in the text.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that the training infrastructure for JetMoE-8B consisted of a cluster containing 12 nodes and 96 H100s. The answer is directly provided in the text."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power is over 600W, which matches the requested unit of Watts.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during active training, the average GPU power is over 600W, which matches the requested unit of Watts."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides the CO2 emissions for an average human life in one year, which is 11,023 lbs.","11023","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243v1""]","Consumption CO 2e (lbs) Human life, avg, 1 year 11,023","The context provides the CO2 emissions for an average human life in one year, which is 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This indicates a 1.25x speedup in inference throughput.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This indicates a 1.25x speedup in inference throughput."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context directly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is provided in the correct unit (Wh) as requested.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context directly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is provided in the correct unit (Wh) as requested."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This directly answers the question about the most energy-efficient GPU architecture for single-token classification models.","V100 or A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This directly answers the question about the most energy-efficient GPU architecture for single-token classification models."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', which implies that Green AI involves reporting the computational cost, not the financial cost.","0","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', which implies that Green AI involves reporting the computational cost, not the financial cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022. The answer is converted to metric tons as specified in the answer_unit.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022. The answer is converted to metric tons as specified in the answer_unit."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the average system power per processor for TPU v2 and V100 GPU. The TPU v2 has an average power of 221 Watts, and the V100 GPU has an average power of 325 Watts. The difference is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].","The context provides the average system power per processor for TPU v2 and V100 GPU. The TPU v2 has an average power of 221 Watts, and the V100 GPU has an average power of 325 Watts. The difference is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer is provided in liters, matching the required unit.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer is provided in liters, matching the required unit."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The answer_unit 'multiplier' indicates that the answer should be expressed as a multiplier.","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The answer_unit 'multiplier' indicates that the answer should be expressed as a multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)' in the eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)' in the eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context explicitly states that smartphones currently average lifetimes of less than 3 years, which contributes to e-waste concerns.","1","is_blank","[""wu2021b""]","[""https://www.fairphone.com/en/""]","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context explicitly states that smartphones currently average lifetimes of less than 3 years, which contributes to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV's per-GPU speedup with T4 GPUs is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that CV's per-GPU speedup with T4 GPUs is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context directly states the context window size for FLM-101B is 2,048 tokens. The answer_unit 'tokens' matches the unit provided in the context.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context directly states the context window size for FLM-101B is 2,048 tokens. The answer_unit 'tokens' matches the unit provided in the context."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and is bottlenecked by VRAM bandwidth, leading to underutilization of the GPU's computation units.","1","is_blank","[""chung2025""]","is_blank","It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.","The context states that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and is bottlenecked by VRAM bandwidth, leading to underutilization of the GPU's computation units."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed.","[61, 76]","percent","[""cottier2024""]","is_blank","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that while some performance metrics like precision and recall may increase, accuracy and F1 scores generally decline after optimization. Therefore, the statement is false.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context states that while some performance metrics like precision and recall may increase, accuracy and F1 scores generally decline after optimization. Therefore, the statement is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context directly provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as 0.954 lbs/kWh, as provided by the U.S. Environmental Protection Agency (EPA).","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0 .954pt (2)","The context directly provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as 0.954 lbs/kWh, as provided by the U.S. Environmental Protection Agency (EPA)."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a specific projection for global AI electricity consumption in 2027, stating a range of 85 to 134 TWh.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]","The context provides a specific projection for global AI electricity consumption in 2027, stating a range of 85 to 134 TWh."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context clearly states that in 2023, Amazon delivered 150 million packages via EVs in Europe.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context clearly states that in 2023, Amazon delivered 150 million packages via EVs in Europe."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context states that the online inference workload evaluates a total of 3000 requests, with each request having 128 input tokens and 512 output tokens. The total number of tokens processed is the sum of input and output tokens across all requests.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The context states that the online inference workload evaluates a total of 3000 requests, with each request having 128 input tokens and 512 output tokens. The total number of tokens processed is the sum of input and output tokens across all requests."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as oil and gas exploration. This indicates that the Act does not mandate the disclosure of these emissions.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context explicitly states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as oil and gas exploration. This indicates that the Act does not mandate the disclosure of these emissions."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, the Flexible Start optimization can achieve a significant reduction, greater than 30% in multiple regions, and up to 80% in West US. The answer_unit is 'percent', so the answer_value is 80.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for very short experiments like DenseNet 201, the Flexible Start optimization can achieve a significant reduction, greater than 30% in multiple regions, and up to 80% in West US. The answer_unit is 'percent', so the answer_value is 80."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context indicates that the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, while the mean energy consumption for text classification is 0.002 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is calculated as 2.907 / 0.002 = 1453.5, which rounds to 1450 times.","1450","times","[""luccioni2024""]","is_blank","We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context indicates that the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, while the mean energy consumption for text classification is 0.002 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is calculated as 2.907 / 0.002 = 1453.5, which rounds to 1450 times."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in the case of sentiment analysis on the Yelp dataset, which directly contradicts the statement in the question.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in the case of sentiment analysis on the Yelp dataset, which directly contradicts the statement in the question."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table listing the number of Amazon Renewable Energy Projects by country. According to the table, the United States has 244 projects.","244","projects","[""amazon2023""]","is_blank","United States 244 17,706","The context provides a table listing the number of Amazon Renewable Energy Projects by country. According to the table, the United States has 244 projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides a table showing the percent energy sourced from renewable sources for Amazon-AWS in 2018, which is 17%. The answer_unit is 'percent', so the answer_value is 17.","17","percent","[""strubell2019""]","is_blank","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China5 and Germany (Burger, 2019).","The context provides a table showing the percent energy sourced from renewable sources for Amazon-AWS in 2018, which is 17%. The answer_unit is 'percent', so the answer_value is 17."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend that energy consumption should be reported at the cumulative server level to balance accuracy and feasibility.","cumulative server level","is_blank","[""ebert2024""]","is_blank","Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.","The authors recommend that energy consumption should be reported at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context explicitly states that the GPU alone accounts for 74% of the total energy consumption during the BERT-base model training experiment.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context explicitly states that the GPU alone accounts for 74% of the total energy consumption during the BERT-base model training experiment."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period (2019 to 2021). The answer is provided in percent, as required by the answer_unit.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period (2019 to 2021). The answer is provided in percent, as required by the answer_unit."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context directly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. The answer_unit is specified as 'USD', so the answer_value is expressed as a number in that unit.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context directly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. The answer_unit is specified as 'USD', so the answer_value is expressed as a number in that unit."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context indicates that 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This means the statement is false.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context indicates that 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This means the statement is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that the maximum batch size for fine-tuning Mixtral on an A100-40GB GPU is 4 samples. This is directly provided in the table and figures.","4","samples","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7
A100 80GB 17 2.74 1.67 25.4
H100 80GB 17 4.90 2.1 17.9","The context states that the maximum batch size for fine-tuning Mixtral on an A100-40GB GPU is 4 samples. This is directly provided in the table and figures."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context clearly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy.","The context clearly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context states that Google reported a 48% increase in GHG emissions since 2019 in their 2024 environmental report. The answer is provided as a percentage, matching the required unit.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]","The context states that Google reported a 48% increase in GHG emissions since 2019 in their 2024 environmental report. The answer is provided as a percentage, matching the required unit."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context from ref_id=han2024 and ref_id=fernandez2025 provides the projection that data centers are expected to account for 9.1% to 11.7% of the total U.S. energy demand by 2030. The answer is formatted as a percentage range.","[9.1, 11.7]","percent","[""han2024"", ""fernandez2025""]","is_blank","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024;","The context from ref_id=han2024 and ref_id=fernandez2025 provides the projection that data centers are expected to account for 9.1% to 11.7% of the total U.S. energy demand by 2030. The answer is formatted as a percentage range."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that for the 6B parameter transformer, the maximum potential emissions saving using the 'Pause and Resume' optimization is 25%.","25","percent","[""dodge2022""]","is_blank","Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%. We confirmed with WattTime that emissions estimates for West US were correct, as that region has large variance.","The context indicates that for the 6B parameter transformer, the maximum potential emissions saving using the 'Pause and Resume' optimization is 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit 'percent' matches the format required.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit 'percent' matches the format required."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from [ref_id=luccioni2025b] states that the carbon footprint of training BERT, a large language model, was quantified as reaching 626,155 pounds of CO2e emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context snippet from [ref_id=luccioni2025b] states that the carbon footprint of training BERT, a large language model, was quantified as reaching 626,155 pounds of CO2e emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippet from [ref_id=li2025b] states that the estimated training energy of GPT-3 is 1287 MWh. This directly answers the question in the specified unit.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context snippet from [ref_id=li2025b] states that the estimated training energy of GPT-3 is 1287 MWh. This directly answers the question in the specified unit."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average, and is a prime target for optimization to enhance the performance of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","As shown in Fig. 5, the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average. The execution time for the MoE layer encompasses both the forward and backward passes during fine-tuning. Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context explicitly states that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average, and is a prime target for optimization to enhance the performance of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides the maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory in Table III.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context provides the maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory in Table III."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7% compared to local training.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7% compared to local training."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification.","0.32","g CO2eq","[""luccioni2024""]","is_blank","for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the emissions for training BERT in the most efficient region were approximately 7k grams, and in the least efficient region, they were approximately 26k grams. The difference between these values gives the range of emissions.","19000","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that the emissions for training BERT in the most efficient region were approximately 7k grams, and in the least efficient region, they were approximately 26k grams. The difference between these values gives the range of emissions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This is followed by a decline in transparency.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This is followed by a decline in transparency."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the water consumption for the OLMo 60M model, which is 1.6 kL. According to the context, 1 kL of water is equivalent to 5 days of water usage for one person in the US. Therefore, 1.6 kL is equivalent to 8 days.","8","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides the water consumption for the OLMo 60M model, which is 1.6 kL. According to the context, 1 kL of water is equivalent to 5 days of water usage for one person in the US. Therefore, 1.6 kL is equivalent to 8 days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","is_blank","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","is_blank"
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost. Therefore, the statement that adding compute resources can increase costs is false.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost. Therefore, the statement that adding compute resources can increase costs is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total energy consumption for training the FLM-101B model as 40 MkWh. Converting this to kWh, we get 40,000,000 kWh.","40000000","kWh","[""morrison2025""]","is_blank","Energy (MkWh) 1171 1066 3179 444 688 40","The context provides the total energy consumption for training the FLM-101B model as 40 MkWh. Converting this to kWh, we get 40,000,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context indicates that the total public health burden of U.S. data centers could reach more than $20 billion by 2028. No specific projection for 2030 is provided, so the answer is blank.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context indicates that the total public health burden of U.S. data centers could reach more than $20 billion by 2028. No specific projection for 2030 is provided, so the answer is blank."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using an NVIDIA A40-48GB GPU, which is $32.7. The answer_unit is USD, so the answer_value is 32.7.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context provides the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using an NVIDIA A40-48GB GPU, which is $32.7. The answer_unit is USD, so the answer_value is 32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides the maximum batch size for the Mixtral-S model on the NVIDIA A40-48GB GPU, which is 8 samples. This is the largest batch size supported, and thus the batch size of the longest-running MoE layer.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides the maximum batch size for the Mixtral-S model on the NVIDIA A40-48GB GPU, which is 8 samples. This is the largest batch size supported, and thus the batch size of the longest-running MoE layer."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides a breakdown of the execution time for different models and batch sizes, but it does not explicitly state the total execution time for a sparse Mixtral model with a batch size of 10 on a NVIDIA A40-48GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides a breakdown of the execution time for different models and batch sizes, but it does not explicitly state the total execution time for a sparse Mixtral model with a batch size of 10 on a NVIDIA A40-48GB GPU."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level power consumption tracking is not recommended for overall energy measurements, as it underrepresents the actual energy consumption by measuring just a single component.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that GPU-level power consumption tracking is not recommended for overall energy measurements, as it underrepresents the actual energy consumption by measuring just a single component."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that training accounted for only half of the model’s overall emissions, according to the 2023 article by Luccioni et al.","50","percent","[""luccioni2025b""]","is_blank","finding that training accounted for only half of the model’s overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","The context states that training accounted for only half of the model’s overall emissions, according to the 2023 article by Luccioni et al."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware required for LLaMA-7B is 1 V100 32GB GPU with a maximum batch size of 64.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context states that the bare minimum hardware required for LLaMA-7B is 1 V100 32GB GPU with a maximum batch size of 64."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides a table with the water consumption for GPT-3 requests in various data center locations. For Arizona, the total water consumption per request is 9.629 mL. To consume a 500ml bottle of water, we divide 500 by 9.629, which gives approximately 52 requests.","52","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. Location: Arizona, Water for Each Request: 9.629 mL","The context provides a table with the water consumption for GPT-3 requests in various data center locations. For Arizona, the total water consumption per request is 9.629 mL. To consume a 500ml bottle of water, we divide 500 by 9.629, which gives approximately 52 requests."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context snippet from [ref_id=dodge2022] provides the estimate that one average US home energy use is estimated to emit 8.30 metric tons of CO2.","8.3","metric tons","[""dodge2022""]","is_blank","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context snippet from [ref_id=dodge2022] provides the estimate that one average US home energy use is estimated to emit 8.30 metric tons of CO2."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides a table (Table 3) showing the performance of JetMoE-8B on various benchmarks, including GSM8k. The score for JetMoE-8B on the GSM8k benchmark is 27.8.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. GSM8k 14.5 17.3 16.9 27.8","The context provides a table (Table 3) showing the performance of JetMoE-8B on various benchmarks, including GSM8k. The score for JetMoE-8B on the GSM8k benchmark is 27.8."
