[
  {
    "id": "q001",
    "question": "What was the average increase in U.S. data center electricity consumption between 2010 and 2014?",
    "gt_value": "4",
    "gt_unit": "percent",
    "gt_ref": "['wu2021b']",
    "pred_value": "4",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4",
      "qwen32b-bench": "4",
      "qwen14b-bench": "4",
      "qwen7b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]",
      "qwen7b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q002",
    "question": "In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",
    "gt_value": "13900",
    "gt_unit": "cars",
    "gt_ref": "['amazon2023']",
    "pred_value": "13900",
    "pred_unit": "cars",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "13900",
      "qwen32b-bench": "13900",
      "qwen14b-bench": "13900",
      "qwen7b-bench": "13900"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q004",
    "question": "How many data centers did AWS begin using recycled water for cooling in 2023?",
    "gt_value": "24",
    "gt_unit": "data centers",
    "gt_ref": "['amazon2023']",
    "pred_value": "4",
    "pred_unit": "data centers",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4",
      "qwen32b-bench": "4",
      "qwen14b-bench": "4",
      "qwen7b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q005",
    "question": "Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",
    "gt_value": "463",
    "gt_unit": "kg/GPU",
    "gt_ref": "['morrison2025']",
    "pred_value": "463",
    "pred_unit": "kg/GPU",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "463",
      "qwen32b-bench": "463",
      "qwen14b-bench": "463",
      "qwen7b-bench": "463"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]",
      "qwen7b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q006",
    "question": "By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?",
    "gt_value": "400",
    "gt_unit": "ratio",
    "gt_ref": "['cottier2025', 'li2025a']",
    "pred_value": "400",
    "pred_unit": "ratio",
    "pred_ref": "[\"cottier2024\", \"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "400",
      "qwen32b-bench": "400",
      "qwen14b-bench": "400",
      "qwen7b-bench": "400"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\", \"cottier2024\"]",
      "qwen32b-bench": "[\"li2025a\", \"cottier2024\"]",
      "qwen14b-bench": "[\"li2025a\", \"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.3333333333333333,
    "na_correct": true
  },
  {
    "id": "q007",
    "question": "What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?",
    "gt_value": "1.2",
    "gt_unit": "tCO2e",
    "gt_ref": "['patterson2021']",
    "pred_value": "1.2",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.2",
      "qwen32b-bench": "1.2",
      "qwen14b-bench": "1.2",
      "qwen7b-bench": "1.2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]",
      "qwen7b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q008",
    "question": "When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",
    "gt_value": "43.94",
    "gt_unit": "score",
    "gt_ref": "['li2025a']",
    "pred_value": "43.94",
    "pred_unit": "score",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "43.94",
      "qwen32b-bench": "43.94",
      "qwen14b-bench": "43.94",
      "qwen7b-bench": "43.94"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]",
      "qwen7b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q010",
    "question": "By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?",
    "gt_value": "6750",
    "gt_unit": "fold",
    "gt_ref": "['wu2021b']",
    "pred_value": "6750",
    "pred_unit": "fold",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6750",
      "qwen32b-bench": "6750",
      "qwen14b-bench": "6750",
      "qwen7b-bench": "6750"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]",
      "qwen7b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q011",
    "question": "How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",
    "gt_value": "14.8",
    "gt_unit": "days",
    "gt_ref": "['patterson2021']",
    "pred_value": "14.8",
    "pred_unit": "days",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "14.8",
      "qwen32b-bench": "14.8",
      "qwen14b-bench": "14.8",
      "qwen7b-bench": "14.8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]",
      "qwen7b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q012",
    "question": "What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?",
    "gt_value": "0.036",
    "gt_unit": "kWh",
    "gt_ref": "['morrison2025']",
    "pred_value": "0.036",
    "pred_unit": "kWh",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.036",
      "qwen32b-bench": "0.036",
      "qwen14b-bench": "0.036",
      "qwen7b-bench": "0.036"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]",
      "qwen7b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q013",
    "question": "What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",
    "gt_value": "13000",
    "gt_unit": "tons",
    "gt_ref": "['han2024']",
    "pred_value": "13000",
    "pred_unit": "tons",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "13000",
      "qwen32b-bench": "13000",
      "qwen14b-bench": "13000",
      "qwen7b-bench": "13000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]",
      "qwen7b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q014",
    "question": "A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?",
    "gt_value": "72",
    "gt_unit": "percent",
    "gt_ref": "['li2025a']",
    "pred_value": "72",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "72",
      "qwen32b-bench": "72",
      "qwen14b-bench": "72",
      "qwen7b-bench": "72"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]",
      "qwen7b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q015",
    "question": "Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?",
    "gt_value": "1300",
    "gt_unit": "deaths",
    "gt_ref": "['han2024']",
    "pred_value": "1300",
    "pred_unit": "deaths",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1300",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "1300",
      "qwen7b-bench": "1300"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"han2024\"]",
      "qwen7b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q016",
    "question": "Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?",
    "gt_value": "60",
    "gt_unit": "days",
    "gt_ref": "['dodge2022']",
    "pred_value": "60",
    "pred_unit": "days",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "60",
      "qwen32b-bench": "60",
      "qwen14b-bench": "60",
      "qwen7b-bench": "8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]",
      "qwen7b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q017",
    "question": "For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",
    "gt_value": "Command-R Plus",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "Command-R Plus",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Command-R Plus",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "Command-R Plus",
      "qwen7b-bench": "Qwen 2.5 72B"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q018",
    "question": "In what year was the One Hundred Year Study on Artificial Intelligence launched?",
    "gt_value": "2014",
    "gt_unit": "year",
    "gt_ref": "['stone2022']",
    "pred_value": "2014",
    "pred_unit": "year",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2014",
      "qwen32b-bench": "2014",
      "qwen14b-bench": "2014",
      "qwen7b-bench": "2014"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q019",
    "question": "According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",
    "gt_value": "22",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "22",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "22",
      "qwen32b-bench": "22",
      "qwen14b-bench": "22",
      "qwen7b-bench": "22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]",
      "qwen7b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q020",
    "question": "What is the energy consumption (in MWh) for pre-training the BLOOM model?",
    "gt_value": "520",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "520",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2024\", \"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "51.686",
      "qwen32b-bench": "520",
      "qwen14b-bench": "520",
      "qwen7b-bench": "520"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2024\"]",
      "qwen7b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q021",
    "question": "What percentage of the Switch Transformer's 1500 billion parameters are activated per token?",
    "gt_value": "0.1",
    "gt_unit": "percent",
    "gt_ref": "['patterson2021']",
    "pred_value": "0.1",
    "pred_unit": "percent",
    "pred_ref": "[\"patterson2021\", \"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.1",
      "qwen32b-bench": "0.1",
      "qwen14b-bench": "0.1",
      "qwen7b-bench": "0.1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"wu2021a\", \"patterson2021\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q022",
    "question": "The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?",
    "gt_value": "8",
    "gt_unit": "experts",
    "gt_ref": "['shen2024']",
    "pred_value": "8",
    "pred_unit": "experts",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8",
      "qwen32b-bench": "8",
      "qwen14b-bench": "8",
      "qwen7b-bench": "2000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]",
      "qwen7b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q023",
    "question": "What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",
    "gt_value": "1",
    "gt_unit": "second",
    "gt_ref": "['xia2024']",
    "pred_value": "1.5",
    "pred_unit": "second",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.5",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q024",
    "question": "According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",
    "gt_value": "28.22",
    "gt_unit": "zettaFLOPs",
    "gt_ref": "['li2025a']",
    "pred_value": "28.22",
    "pred_unit": "zettaFLOPs",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "28.22",
      "qwen32b-bench": "28.22",
      "qwen14b-bench": "28.22",
      "qwen7b-bench": "28.22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]",
      "qwen7b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q025",
    "question": "Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?",
    "gt_value": "Intel Core i7-1165G7",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "11th Gen Intel(R) Core(TM) i7-1165G7",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11th Gen Intel(R) Core(TM) i7-1165G7",
      "qwen32b-bench": "11th Gen Intel(R) Core(TM) i7-1165G7",
      "qwen14b-bench": "Intel(R) Core(TM) i7-1165G7",
      "qwen7b-bench": "Intel(R) Core(TM) i7-1165G7"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"khan2025\"]",
      "qwen7b-bench": "[\"khan2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q026",
    "question": "How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?",
    "gt_value": "88",
    "gt_unit": "models",
    "gt_ref": "['luccioni2024']",
    "pred_value": "88",
    "pred_unit": "models",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "88",
      "qwen32b-bench": "88",
      "qwen14b-bench": "88",
      "qwen7b-bench": "18"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]",
      "qwen7b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q027",
    "question": "By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?",
    "gt_value": "3",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "3",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3",
      "qwen32b-bench": "3",
      "qwen14b-bench": "3",
      "qwen7b-bench": "3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q028",
    "question": "Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?",
    "gt_value": "[1.2,4]",
    "gt_unit": "multiplier",
    "gt_ref": "['cottier2024']",
    "pred_value": "[1.2,4]",
    "pred_unit": "multiplier",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "[1.2,4]",
      "qwen32b-bench": "[1.2,4]",
      "qwen14b-bench": "[1.2,4]",
      "qwen7b-bench": "[1.2, 4]"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q029",
    "question": "What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?",
    "gt_value": "103.5",
    "gt_unit": "MWh",
    "gt_ref": "['dodge2022']",
    "pred_value": "103.5",
    "pred_unit": "MWh",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "103.5",
      "qwen32b-bench": "103.5",
      "qwen14b-bench": "103.5",
      "qwen7b-bench": "103.5"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]",
      "qwen7b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q030",
    "question": "The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?",
    "gt_value": "Jevons' Paradox",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "Jevons' Paradox",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "Jevons' Paradox",
      "qwen14b-bench": "Jevons' Paradox",
      "qwen7b-bench": "Jevons' Paradox"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]",
      "qwen7b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q031",
    "question": "By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",
    "gt_value": "[4.2,6.6]",
    "gt_unit": "billion cubic meters",
    "gt_ref": "['li2025b']",
    "pred_value": "[4.2, 6.6]",
    "pred_unit": "billion cubic meters",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "[4.2, 6.6]",
      "qwen32b-bench": "[4.2,6.6]",
      "qwen14b-bench": "[4.2,6.6]",
      "qwen7b-bench": "[4.2, 6.6]"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]",
      "qwen7b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q032",
    "question": "True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['schwartz2019']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0",
      "qwen7b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]",
      "qwen7b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q033",
    "question": "Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",
    "gt_value": "21.54",
    "gt_unit": "days",
    "gt_ref": "['li2025a']",
    "pred_value": "21.54",
    "pred_unit": "days",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "21.54",
      "qwen32b-bench": "21.54",
      "qwen14b-bench": "21.54",
      "qwen7b-bench": "21.54"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]",
      "qwen7b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q034",
    "question": "True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021a']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q035",
    "question": "How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?",
    "gt_value": "1287",
    "gt_unit": "MWh",
    "gt_ref": "['jegham2025']",
    "pred_value": "1287",
    "pred_unit": "MWh",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1287",
      "qwen32b-bench": "1287",
      "qwen14b-bench": "1287",
      "qwen7b-bench": "1287"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]",
      "qwen7b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q036",
    "question": "What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?",
    "gt_value": "AI Energy Score",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "AI Energy Score",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "AI Energy Score",
      "qwen32b-bench": "AI Energy Score",
      "qwen14b-bench": "AI Energy Score",
      "qwen7b-bench": "AI Energy Score"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q037",
    "question": "For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",
    "gt_value": "1000",
    "gt_unit": "microseconds",
    "gt_ref": "['xia2024']",
    "pred_value": "1600",
    "pred_unit": "microseconds",
    "pred_ref": "[\"x\", \"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1600",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "1600"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"x\"]"
    },
    "value_correct": false,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q038",
    "question": "In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",
    "gt_value": "2",
    "gt_unit": "experts",
    "gt_ref": "['shen2024']",
    "pred_value": "2",
    "pred_unit": "experts",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2",
      "qwen32b-bench": "2",
      "qwen14b-bench": "2",
      "qwen7b-bench": "2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]",
      "qwen7b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q039",
    "question": "True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['schwartz2019']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0",
      "qwen7b-bench": "300000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]",
      "qwen7b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q040",
    "question": "What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?",
    "gt_value": "6.4",
    "gt_unit": "percent",
    "gt_ref": "['wu2021b']",
    "pred_value": "6.4",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6.4",
      "qwen32b-bench": "6.4",
      "qwen14b-bench": "6.4",
      "qwen7b-bench": "6.4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]",
      "qwen7b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q041",
    "question": "In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",
    "gt_value": "22",
    "gt_unit": "data centers",
    "gt_ref": "['amazon2023']",
    "pred_value": "22",
    "pred_unit": "data centers",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "22",
      "qwen32b-bench": "22",
      "qwen14b-bench": "22",
      "qwen7b-bench": "22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q042",
    "question": "What is the approximate age of the field of Artificial Intelligence in 2025?",
    "gt_value": "69",
    "gt_unit": "years",
    "gt_ref": "['stone2022']",
    "pred_value": "69",
    "pred_unit": "years",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "69",
      "qwen32b-bench": "69",
      "qwen14b-bench": "60",
      "qwen7b-bench": "69"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q043",
    "question": "The well-known \"five cars\" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",
    "gt_value": "Neural architecture search (NAS)",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "Neural Architecture Search (NAS)",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "neural architecture search (NAS)",
      "qwen32b-bench": "Neural Architecture Search (NAS)",
      "qwen14b-bench": "Neural Architecture Search (NAS)",
      "qwen7b-bench": "NAS"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q044",
    "question": "For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",
    "gt_value": "44",
    "gt_unit": "percent",
    "gt_ref": "['chung2025']",
    "pred_value": "44",
    "pred_unit": "percent",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "44",
      "qwen32b-bench": "44",
      "qwen14b-bench": "44",
      "qwen7b-bench": "44"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]",
      "qwen7b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q045",
    "question": "What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?",
    "gt_value": "8",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "8",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8",
      "qwen32b-bench": "8",
      "qwen14b-bench": "8",
      "qwen7b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]",
      "qwen7b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q046",
    "question": "As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",
    "gt_value": "1.3",
    "gt_unit": "GW",
    "gt_ref": "['amazon2023']",
    "pred_value": "1.3",
    "pred_unit": "GW",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.3",
      "qwen32b-bench": "1.3",
      "qwen14b-bench": "1.3",
      "qwen7b-bench": "1.3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q047",
    "question": "The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?",
    "gt_value": "2300",
    "gt_unit": "flights",
    "gt_ref": "['jegham2025']",
    "pred_value": "272",
    "pred_unit": "flights",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "272",
      "qwen32b-bench": "272",
      "qwen14b-bench": "272",
      "qwen7b-bench": "272"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]",
      "qwen7b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q048",
    "question": "What percentage of AI inference workloads in Asia were powered by coal in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "percent",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "percent",
    "pred_ref": "[\"is_blank\", \"li2025b\", \"luccioni2024\", \"luccioni2025a\", \"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"li2025b\", \"luccioni2024\", \"luccioni2025a\", \"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 0.2,
    "na_correct": true
  },
  {
    "id": "q049",
    "question": "What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "PUE",
    "gt_ref": "is_blank",
    "pred_value": "1.58",
    "pred_unit": "PUE",
    "pred_ref": "[\"ebert2024\", \"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.58",
      "qwen32b-bench": "1.58",
      "qwen14b-bench": "1.58",
      "qwen7b-bench": "1.58"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]",
      "qwen7b-bench": "[\"ebert2024\", \"strubell2019\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": false
  },
  {
    "id": "q050",
    "question": "During inference, how many of JetMoE-8B's parameters are activated for each input token?",
    "gt_value": "2000000000",
    "gt_unit": "parameters",
    "gt_ref": "['shen2024']",
    "pred_value": "2000000000",
    "pred_unit": "parameters",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2000000000",
      "qwen32b-bench": "2000000000",
      "qwen14b-bench": "2000000000",
      "qwen7b-bench": "2000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]",
      "qwen7b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q051",
    "question": "What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?",
    "gt_value": "14",
    "gt_unit": "tCO2e",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "14",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"luccioni2024\", \"luccioni2025c\", \"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "14",
      "qwen32b-bench": "14",
      "qwen14b-bench": "14",
      "qwen7b-bench": "14"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"samsi2024\", \"luccioni2024\", \"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 0.3333333333333333,
    "na_correct": true
  },
  {
    "id": "q052",
    "question": "How many Amazon electric delivery vans were added in total across 2022 and 2023?",
    "gt_value": "21600",
    "gt_unit": "electric delivery vans",
    "gt_ref": "['amazon2023']",
    "pred_value": "9200",
    "pred_unit": "electric delivery vans",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "19020",
      "qwen32b-bench": "9200",
      "qwen14b-bench": "9200",
      "qwen7b-bench": "19000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  }
]