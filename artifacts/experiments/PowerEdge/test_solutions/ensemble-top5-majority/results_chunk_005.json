[
  {
    "id": "q287",
    "question": "How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "kilometers of fiberoptic cable",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "kilometers of fiberoptic cable",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "is_blank",
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen3-30b-a3b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"is_blank\"]",
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen3-30b-a3b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q288",
    "question": "What is the estimated upfront hardware acquisition cost to train GPT-4?",
    "gt_value": "800000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "800000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "800000000",
      "qwen72b-bench": "800000000",
      "qwen32b-bench": "800000000",
      "qwen3-30b-a3b-bench": "800000000",
      "qwen14b-bench": "800000000"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"cottier2024\"]",
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen3-30b-a3b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q289",
    "question": "True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "luccioni2025b",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "0",
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen3-30b-a3b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2025b\"]",
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q290",
    "question": "What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU",
    "gt_value": "5",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "17",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "17",
      "qwen72b-bench": "4",
      "qwen32b-bench": "17",
      "qwen3-30b-a3b-bench": "17",
      "qwen14b-bench": "28"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q291",
    "question": "When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",
    "gt_value": "Swapping",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "Swapping",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "Swapping",
      "qwen72b-bench": "Swapping",
      "qwen32b-bench": "Swapping",
      "qwen3-30b-a3b-bench": "Swapping",
      "qwen14b-bench": "Swapping"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"chung2025\"]",
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen3-30b-a3b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q292",
    "question": "In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",
    "gt_value": "48",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "48",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "48",
      "qwen72b-bench": "48",
      "qwen32b-bench": "48",
      "qwen3-30b-a3b-bench": "48",
      "qwen14b-bench": "48"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2025a\"]",
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q293",
    "question": "According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",
    "gt_value": "11.7",
    "gt_unit": "percent",
    "gt_ref": "['han2024']",
    "pred_value": "[9.1, 11.7]",
    "pred_unit": "percent",
    "pred_ref": "[\"chung2025\", \"fernandez2025\", \"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "11.7",
      "qwen72b-bench": "[9.1, 11.7]",
      "qwen32b-bench": "[9.1, 11.7]",
      "qwen3-30b-a3b-bench": "[9.1, 11.7]",
      "qwen14b-bench": "11.7"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"chung2025\", \"fernandez2025\", \"morrison2025\"]",
      "qwen72b-bench": "[\"han2024\", \"fernandez2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen3-30b-a3b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": false,
    "ref_score": 0.3333333333333333,
    "na_correct": true
  },
  {
    "id": "q294",
    "question": "When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",
    "gt_value": "25",
    "gt_unit": "percent",
    "gt_ref": "['dodge2022']",
    "pred_value": "25",
    "pred_unit": "percent",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "25",
      "qwen72b-bench": "25",
      "qwen32b-bench": "25",
      "qwen3-30b-a3b-bench": "25",
      "qwen14b-bench": "25"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"dodge2022\"]",
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen3-30b-a3b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q295",
    "question": "By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?",
    "gt_value": "70",
    "gt_unit": "percent",
    "gt_ref": "['shen2024']",
    "pred_value": "70",
    "pred_unit": "percent",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "70",
      "qwen72b-bench": "70",
      "qwen32b-bench": "70",
      "qwen3-30b-a3b-bench": "70",
      "qwen14b-bench": "70"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"shen2024\"]",
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen3-30b-a3b-bench": "[\"shen2024\", \"shen2024\", \"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q298",
    "question": "What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?",
    "gt_value": "626155",
    "gt_unit": "lbs CO2e",
    "gt_ref": "luccioni2025b",
    "pred_value": "626155",
    "pred_unit": "lbs CO2e",
    "pred_ref": "[\"luccioni2025b\", \"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "626155",
      "qwen72b-bench": "626155",
      "qwen32b-bench": "626155",
      "qwen3-30b-a3b-bench": "626155",
      "qwen14b-bench": "626155"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2025b\", \"strubell2019\"]",
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q299",
    "question": "What was the estimated training energy of the full GPT-3 model, in MWh?",
    "gt_value": "1287",
    "gt_unit": "MWh",
    "gt_ref": "['li2025b']",
    "pred_value": "1287",
    "pred_unit": "MWh",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "1287",
      "qwen72b-bench": "1287",
      "qwen32b-bench": "1287",
      "qwen3-30b-a3b-bench": "1287",
      "qwen14b-bench": "1287"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"li2025b\"]",
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen3-30b-a3b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q300",
    "question": "True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['xia2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "1",
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen3-30b-a3b-bench": "1",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q301",
    "question": "What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?",
    "gt_value": "2",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "2",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "2",
      "qwen72b-bench": "2",
      "qwen32b-bench": "2",
      "qwen3-30b-a3b-bench": "2",
      "qwen14b-bench": "2"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q302",
    "question": "True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "1",
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen3-30b-a3b-bench": "1",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"erben2023\"]",
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen3-30b-a3b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q303",
    "question": "How many hectares of land were occupied by new AI data centers globally in 2022?",
    "gt_value": "is_blank",
    "gt_unit": "hectares",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "hectares",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "is_blank",
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen3-30b-a3b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"is_blank\"]",
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen3-30b-a3b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q305",
    "question": "A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",
    "gt_value": "0.32",
    "gt_unit": "g CO2eq",
    "gt_ref": "['luccioni2024']",
    "pred_value": "0.32",
    "pred_unit": "g CO2eq",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "0.32",
      "qwen72b-bench": "0.32",
      "qwen32b-bench": "0.32",
      "qwen3-30b-a3b-bench": "0.32",
      "qwen14b-bench": "0.32"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2024\"]",
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q307",
    "question": "In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",
    "gt_value": "[7000,26000]",
    "gt_unit": "grams",
    "gt_ref": "['dodge2022']",
    "pred_value": "[7000, 26000]",
    "pred_unit": "grams",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "[7000, 26000]",
      "qwen72b-bench": "19000",
      "qwen32b-bench": "[7,26]",
      "qwen3-30b-a3b-bench": "[7000, 26000]",
      "qwen14b-bench": "[7000,26000]"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"dodge2022\"]",
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen3-30b-a3b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q308",
    "question": "In what year did the practice of directly releasing environmental information for notable models peak before declining?",
    "gt_value": "2022",
    "gt_unit": "year",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "2022",
    "pred_unit": "year",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "2022",
      "qwen72b-bench": "2022",
      "qwen32b-bench": "2022",
      "qwen3-30b-a3b-bench": "2022",
      "qwen14b-bench": "2022"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2025c\"]",
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q309",
    "question": "What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",
    "gt_value": "5",
    "gt_unit": "days",
    "gt_ref": "['morrison2025']",
    "pred_value": "5",
    "pred_unit": "days",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "5",
      "qwen72b-bench": "8",
      "qwen32b-bench": "5",
      "qwen3-30b-a3b-bench": "5",
      "qwen14b-bench": "5"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"morrison2025\"]",
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen3-30b-a3b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q310",
    "question": "How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "liters of freshwater",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "liters of freshwater",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "is_blank",
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen3-30b-a3b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"is_blank\"]",
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen3-30b-a3b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q311",
    "question": "True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['xia2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "0",
      "qwen72b-bench": "0",
      "qwen32b-bench": "is_blank",
      "qwen3-30b-a3b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q312",
    "question": "According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",
    "gt_value": "40000",
    "gt_unit": "kWh",
    "gt_ref": "['li2025a']",
    "pred_value": "40000000",
    "pred_unit": "kWh",
    "pred_ref": "[\"li2025a\", \"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "40000000",
      "qwen72b-bench": "40000000",
      "qwen32b-bench": "40000000",
      "qwen3-30b-a3b-bench": "40000000",
      "qwen14b-bench": "40000"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"li2025a\"]",
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen3-30b-a3b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": false,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q313",
    "question": "According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",
    "gt_value": "20000000000",
    "gt_unit": "USD",
    "gt_ref": "['han2024']",
    "pred_value": "20",
    "pred_unit": "USD",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "is_blank",
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "20",
      "qwen3-30b-a3b-bench": "20",
      "qwen14b-bench": "20000000000"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"han2024\"]",
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen3-30b-a3b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q314",
    "question": "What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?",
    "gt_value": "32.7",
    "gt_unit": "USD",
    "gt_ref": "['xia2024']",
    "pred_value": "32.7",
    "pred_unit": "USD",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "32.7",
      "qwen72b-bench": "32.7",
      "qwen32b-bench": "32.7",
      "qwen3-30b-a3b-bench": "32.7",
      "qwen14b-bench": "32.7"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q315",
    "question": "For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",
    "gt_value": "32",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "8",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "8",
      "qwen72b-bench": "8",
      "qwen32b-bench": "4",
      "qwen3-30b-a3b-bench": "8",
      "qwen14b-bench": "8"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q317",
    "question": "What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",
    "gt_value": "4",
    "gt_unit": "seconds",
    "gt_ref": "['xia2024']",
    "pred_value": "6.0",
    "pred_unit": "seconds",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "is_blank",
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen3-30b-a3b-bench": "6.0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"xia2024\"]",
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen3-30b-a3b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q318",
    "question": "True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "0",
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen3-30b-a3b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"ebert2024\"]",
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen3-30b-a3b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q319",
    "question": "In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",
    "gt_value": "50",
    "gt_unit": "percent",
    "gt_ref": "luccioni2025b",
    "pred_value": "50",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "50",
      "qwen72b-bench": "50",
      "qwen32b-bench": "50",
      "qwen3-30b-a3b-bench": "50",
      "qwen14b-bench": "50"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"luccioni2025b\"]",
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen3-30b-a3b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q320",
    "question": "What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?",
    "gt_value": "1",
    "gt_unit": "V100_32GB_GPU",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "V100_32GB_GPU",
    "pred_ref": "[\"rubei2025\", \"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "1",
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen3-30b-a3b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"samsi2024\"]",
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen3-30b-a3b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"rubei2025\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q321",
    "question": "When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",
    "gt_value": "16.7",
    "gt_unit": "requests",
    "gt_ref": "['li2025b']",
    "pred_value": "[10,50]",
    "pred_unit": "requests",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "16.7",
      "qwen72b-bench": "52",
      "qwen32b-bench": "238",
      "qwen3-30b-a3b-bench": "[10,50]",
      "qwen14b-bench": "[10,50]"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"li2025b\"]",
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen3-30b-a3b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q322",
    "question": "What is the estimated CO2 emission in metric tons for one year of average US home energy use?",
    "gt_value": "8.3",
    "gt_unit": "metric tons",
    "gt_ref": "['dodge2022']",
    "pred_value": "8.3",
    "pred_unit": "metric tons",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "8.3",
      "qwen72b-bench": "8.3",
      "qwen32b-bench": "8.3",
      "qwen3-30b-a3b-bench": "8.3",
      "qwen14b-bench": "8.3"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"dodge2022\"]",
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen3-30b-a3b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q323",
    "question": "On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",
    "gt_value": "27.8",
    "gt_unit": "score",
    "gt_ref": "['shen2024']",
    "pred_value": "27.8",
    "pred_unit": "score",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 5 runs: qwen3-next-80b-a3b-bench, qwen72b-bench, qwen32b-bench, qwen3-30b-a3b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen3-next-80b-a3b-bench": "27.8",
      "qwen72b-bench": "27.8",
      "qwen32b-bench": "27.8",
      "qwen3-30b-a3b-bench": "27.8",
      "qwen14b-bench": "27.8"
    },
    "individual_refs": {
      "qwen3-next-80b-a3b-bench": "[\"shen2024\"]",
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen3-30b-a3b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  }
]