[
  {
    "id": "q053",
    "question": "True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['morrison2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"[ref_id=morrison2025]\", \"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]",
      "qwen7b-bench": "[\"[ref_id=morrison2025]\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q055",
    "question": "How much energy (in Wh) does the o3 model consume for a long prompt?",
    "gt_value": "39.223",
    "gt_unit": "Wh",
    "gt_ref": "['jegham2025']",
    "pred_value": "12.222",
    "pred_unit": "Wh",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "12.222",
      "qwen32b-bench": "12.222",
      "qwen14b-bench": "12.222",
      "qwen7b-bench": "0.24"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]",
      "qwen7b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q056",
    "question": "When was the field of Artificial Intelligence officially christened?",
    "gt_value": "1956",
    "gt_unit": "year",
    "gt_ref": "['stone2022']",
    "pred_value": "1956",
    "pred_unit": "year",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1956",
      "qwen32b-bench": "1956",
      "qwen14b-bench": "1956",
      "qwen7b-bench": "1956"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q057",
    "question": "What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?",
    "gt_value": "is_blank",
    "gt_unit": "WUE",
    "gt_ref": "is_blank",
    "pred_value": "1",
    "pred_unit": "WUE",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": false
  },
  {
    "id": "q058",
    "question": "True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021b']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "770 million"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]",
      "qwen7b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q059",
    "question": "How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?",
    "gt_value": "[3,4]",
    "gt_unit": "joules per token",
    "gt_ref": "['samsi2024']",
    "pred_value": "[3,4]",
    "pred_unit": "joules per token",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "[3,4]",
      "qwen32b-bench": "[3,4]",
      "qwen14b-bench": "[3,4]",
      "qwen7b-bench": "3-4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]",
      "qwen7b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q060",
    "question": "By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?",
    "gt_value": "15",
    "gt_unit": "percent",
    "gt_ref": "['wu2021a']",
    "pred_value": "15",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "15",
      "qwen32b-bench": "15",
      "qwen14b-bench": "15",
      "qwen7b-bench": "15"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q061",
    "question": "True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"luccioni2025c\", \"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q063",
    "question": "True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['patterson2021']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q064",
    "question": "What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",
    "gt_value": "25000",
    "gt_unit": "USD",
    "gt_ref": "['schwartz2019']",
    "pred_value": "25000",
    "pred_unit": "USD",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "25000",
      "qwen32b-bench": "25000",
      "qwen14b-bench": "25000",
      "qwen7b-bench": "25000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]",
      "qwen7b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q065",
    "question": "What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?",
    "gt_value": "53",
    "gt_unit": "percent",
    "gt_ref": "['xia2024']",
    "pred_value": "53",
    "pred_unit": "percent",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "53",
      "qwen32b-bench": "53",
      "qwen14b-bench": "53",
      "qwen7b-bench": "53"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]",
      "qwen7b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q066",
    "question": " A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",
    "gt_value": "83",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "83",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "830",
      "qwen32b-bench": "83",
      "qwen14b-bench": "83",
      "qwen7b-bench": "83"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]",
      "qwen7b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q067",
    "question": "What was the average global data center PUE in 2023?",
    "gt_value": "1.58",
    "gt_unit": "PUE",
    "gt_ref": "['ebert2024']",
    "pred_value": "1.58",
    "pred_unit": "PUE",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.58",
      "qwen32b-bench": "1.58",
      "qwen14b-bench": "1.58",
      "qwen7b-bench": "1.58"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]",
      "qwen7b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q068",
    "question": "How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "wind turbines",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "wind turbines",
    "pred_ref": "[\"is_blank\", \"luccioni2025a\", \"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"luccioni2025b\", \"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 0.3333333333333333,
    "na_correct": true
  },
  {
    "id": "q069",
    "question": "In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",
    "gt_value": "49",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "49",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "49",
      "qwen32b-bench": "49",
      "qwen14b-bench": "49",
      "qwen7b-bench": "49"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q070",
    "question": "How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?",
    "gt_value": "17",
    "gt_unit": "people",
    "gt_ref": "['stone2022']",
    "pred_value": "17",
    "pred_unit": "people",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "17",
      "qwen32b-bench": "17",
      "qwen14b-bench": "17",
      "qwen7b-bench": "17"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q071",
    "question": "What percentage of a client device's total carbon footprint is accounted for by its manufacturing?",
    "gt_value": "74",
    "gt_unit": "percent",
    "gt_ref": "['wu2021a']",
    "pred_value": "74",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "74",
      "qwen32b-bench": "74",
      "qwen14b-bench": "74",
      "qwen7b-bench": "74"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q072",
    "question": "True or False: A model with more parameters will always consume more energy during inference.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\", \"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"wu2021a\", \"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]",
      "qwen7b-bench": "[\"chung2025\", \"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q073",
    "question": "True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['stone2022']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q074",
    "question": "How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?",
    "gt_value": "is_blank",
    "gt_unit": "tCO2e",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q076",
    "question": "What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?",
    "gt_value": "11390",
    "gt_unit": "tCO2e",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "11390",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11390",
      "qwen32b-bench": "11390",
      "qwen14b-bench": "11390",
      "qwen7b-bench": "11390"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q077",
    "question": "By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",
    "gt_value": "2.9",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "2.9",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2.9",
      "qwen32b-bench": "2.9",
      "qwen14b-bench": "2.9",
      "qwen7b-bench": "2.9"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q079",
    "question": "How many miles is the Earth from the Sun?",
    "gt_value": "is_blank",
    "gt_unit": "miles",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "miles",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q080",
    "question": "True or False: The AlphaGo program defeated the human Go champion.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['stone2022']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1",
      "qwen7b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]",
      "qwen7b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q081",
    "question": "What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?",
    "gt_value": "Continuous batching",
    "gt_unit": "is_blank",
    "gt_ref": "['fernandez2025']",
    "pred_value": "Continuous batching",
    "pred_unit": "is_blank",
    "pred_ref": "[\"fernandez2025\", \"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Continuous batching",
      "qwen32b-bench": "Continuous Batching",
      "qwen14b-bench": "Continuous Batching",
      "qwen7b-bench": "Continuous batching"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"fernandez2025\"]",
      "qwen32b-bench": "[\"fernandez2025\"]",
      "qwen14b-bench": "[\"fernandez2025\"]",
      "qwen7b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q082",
    "question": "How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",
    "gt_value": "60",
    "gt_unit": "H100 GPU hours",
    "gt_ref": "['shen2024']",
    "pred_value": "60",
    "pred_unit": "H100 GPU hours",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "60",
      "qwen32b-bench": "60",
      "qwen14b-bench": "60",
      "qwen7b-bench": "60"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]",
      "qwen7b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q083",
    "question": "In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",
    "gt_value": "26.7",
    "gt_unit": "percent",
    "gt_ref": "['kim2025']",
    "pred_value": "26.7",
    "pred_unit": "percent",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "26.7",
      "qwen32b-bench": "26.7",
      "qwen14b-bench": "26.7",
      "qwen7b-bench": "26.7"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"kim2025\"]",
      "qwen7b-bench": "[\"kim2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q084",
    "question": "The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",
    "gt_value": "1594",
    "gt_unit": "g CO2eq",
    "gt_ref": "['luccioni2024']",
    "pred_value": "1594",
    "pred_unit": "g CO2eq",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1594",
      "qwen32b-bench": "1594",
      "qwen14b-bench": "1594",
      "qwen7b-bench": "1594"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]",
      "qwen7b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q085",
    "question": "What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?",
    "gt_value": "[0.06,3,426]",
    "gt_unit": "Wh",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "[0.06,3426]",
    "pred_unit": "Wh",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "[0.06, 3426]",
      "qwen32b-bench": "[0.06,3426]",
      "qwen14b-bench": "[0.06,3426]",
      "qwen7b-bench": "[3, 9]"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]",
      "qwen7b-bench": "[\"morrison2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q086",
    "question": "True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]",
      "qwen7b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q087",
    "question": "What was the gross carbon intensity of energy according to the U.S. average mix in 2021?",
    "gt_value": "0.429",
    "gt_unit": "kg of CO2e/KWh",
    "gt_ref": "['patterson2021']",
    "pred_value": "0.429",
    "pred_unit": "kg of CO2e/KWh",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.429",
      "qwen32b-bench": "0.429",
      "qwen14b-bench": "0.429",
      "qwen7b-bench": "0.429"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]",
      "qwen7b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q088",
    "question": "What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?",
    "gt_value": "Hivemind",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "Hivemind",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Hivemind",
      "qwen32b-bench": "Hivemind",
      "qwen14b-bench": "Hivemind",
      "qwen7b-bench": "Hivemind"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]",
      "qwen7b-bench": "[\"erben2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q089",
    "question": "What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?",
    "gt_value": "Social transparency",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "social transparency",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "social transparency",
      "qwen32b-bench": "Social transparency",
      "qwen14b-bench": "social transparency",
      "qwen7b-bench": "social transparency"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]",
      "qwen7b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q090",
    "question": "In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",
    "gt_value": "Linear Embedding",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "traditional linear model using pre-trained sentence embeddings",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "traditional linear model using pre-trained sentence embeddings",
      "qwen14b-bench": "linear model with sentence embeddings",
      "qwen7b-bench": "linear model with pre-trained sentence embeddings"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]",
      "qwen7b-bench": "[\"zschache2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q092",
    "question": "What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",
    "gt_value": "Lamina",
    "gt_unit": "is_blank",
    "gt_ref": "['chen2024']",
    "pred_value": "Lamina",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Lamina",
      "qwen32b-bench": "Lamina",
      "qwen14b-bench": "Lamina",
      "qwen7b-bench": "Lamina"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chen2024\"]",
      "qwen32b-bench": "[\"chen2024\"]",
      "qwen14b-bench": "[\"chen2024\"]",
      "qwen7b-bench": "[\"chen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q093",
    "question": "How many parameters does the largest T5 model have?",
    "gt_value": "11000000000",
    "gt_unit": "parameters",
    "gt_ref": "['patterson2021']",
    "pred_value": "11000000000",
    "pred_unit": "parameters",
    "pred_ref": "[\"luccioni2024\", \"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11000000000",
      "qwen32b-bench": "11000000000",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "11000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"is_blank\"]",
      "qwen7b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q094",
    "question": "What is the total number of parameters in the JetMoE-8B model?",
    "gt_value": "8000000000",
    "gt_unit": "parameters",
    "gt_ref": "['shen2024']",
    "pred_value": "8000000000",
    "pred_unit": "parameters",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8000000000",
      "qwen32b-bench": "8000000000",
      "qwen14b-bench": "8000000000",
      "qwen7b-bench": "8000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]",
      "qwen7b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q095",
    "question": "By what percentage did Google's data center water consumption increase from 2021 to 2022?",
    "gt_value": "20",
    "gt_unit": "percent",
    "gt_ref": "['li2025b']",
    "pred_value": "20",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025b\", \"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "20",
      "qwen32b-bench": "20",
      "qwen14b-bench": "20",
      "qwen7b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]",
      "qwen7b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q096",
    "question": "What is the name of the emissions metric defined as 'CO\u2082 emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?",
    "gt_value": "Carbon Intensity",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "Carbon Intensity",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Carbon Intensity",
      "qwen32b-bench": "Carbon Intensity",
      "qwen14b-bench": "Carbon Intensity",
      "qwen7b-bench": "gCO2/kWh"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"khan2025\"]",
      "qwen7b-bench": "[\"khan2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q097",
    "question": "In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",
    "gt_value": "52.88",
    "gt_unit": "percent",
    "gt_ref": "['li2025a']",
    "pred_value": "52.88",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "52.88",
      "qwen32b-bench": "52.88",
      "qwen14b-bench": "52.88",
      "qwen7b-bench": "52.88"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]",
      "qwen7b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q098",
    "question": "What were the estimated amortized training costs for OpenAI's GPT-4?",
    "gt_value": "40000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "40000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "40000000",
      "qwen32b-bench": "40000000",
      "qwen14b-bench": "40000000",
      "qwen7b-bench": "40000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q099",
    "question": "Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",
    "gt_value": "810",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "800",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "800",
      "qwen32b-bench": "800",
      "qwen14b-bench": "810",
      "qwen7b-bench": "810"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q100",
    "question": "What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?",
    "gt_value": "0.59",
    "gt_unit": "multiplier",
    "gt_ref": "['erben2023']",
    "pred_value": "0.59",
    "pred_unit": "multiplier",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.59",
      "qwen32b-bench": "0.59",
      "qwen14b-bench": "0.59",
      "qwen7b-bench": "0.59"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]",
      "qwen7b-bench": "[\"erben2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q101",
    "question": "How many liters of water were returned to communities from Amazon's replenishment projects in 2023?",
    "gt_value": "3500000000",
    "gt_unit": "liters",
    "gt_ref": "['amazon2023']",
    "pred_value": "3500000000",
    "pred_unit": "liters",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3500000000",
      "qwen32b-bench": "3500000000",
      "qwen14b-bench": "3500000000",
      "qwen7b-bench": "3500000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]",
      "qwen7b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q103",
    "question": "True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['rubei2025']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"rubei2025\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "is_blank",
      "qwen7b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"rubei2025\"]",
      "qwen32b-bench": "[\"rubei2025\"]",
      "qwen14b-bench": "[\"rubei2025\"]",
      "qwen7b-bench": "[\"rubei2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q104",
    "question": "As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",
    "gt_value": "3700000",
    "gt_unit": "GPUs",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "3700000",
    "pred_unit": "GPUs",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3700000",
      "qwen32b-bench": "3700000",
      "qwen14b-bench": "3700000",
      "qwen7b-bench": "3700000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]",
      "qwen7b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q107",
    "question": "What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",
    "gt_value": "44",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "44",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "44",
      "qwen32b-bench": "44",
      "qwen14b-bench": "44",
      "qwen7b-bench": "44"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q108",
    "question": "What is the Power Usage Effectiveness (PUE) for Facebook's data centers?",
    "gt_value": "1.1",
    "gt_unit": "PUE",
    "gt_ref": "['wu2021a']",
    "pred_value": "1.1",
    "pred_unit": "PUE",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.1",
      "qwen32b-bench": "1.10",
      "qwen14b-bench": "1.1",
      "qwen7b-bench": "1.1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]",
      "qwen7b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q109",
    "question": "What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",
    "gt_value": "ETAIROS",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "ETAIROS",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "ETAIROS",
      "qwen32b-bench": "ETAIROS",
      "qwen14b-bench": "ETAIROS",
      "qwen7b-bench": "ETAIROS"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]",
      "qwen7b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q110",
    "question": "What were the estimated amortized training costs for Google's Gemini Ultra?",
    "gt_value": "30000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "30000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 4 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench, qwen7b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "30000000",
      "qwen32b-bench": "30000000",
      "qwen14b-bench": "30000000",
      "qwen7b-bench": "30000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]",
      "qwen7b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  }
]