"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet states that the total energy consumption of the US data centers increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","In fact, data center electricity consumption has slowed down significantly. The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet states that the total energy consumption of the US data centers increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context mentions that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","[""https://sustainability.aboutamazon.com/static/pdf/2023-amazon-sustainability-report.pdf""]","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context mentions that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023.","24","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context does not provide a specific numerical estimate for embodied carbon emissions in kg per GPU from NVIDIA. The text mentions that estimating these emissions is challenging and that previous work has used approximations based on other hardware, but it does not give a concrete value.","is_blank","kg/GPU","[""is_blank""]","is_blank","The passage discusses the lack of transparency from NVIDIA regarding embodied carbon emissions of their GPUs and the challenges in estimating it.","The context does not provide a specific numerical estimate for embodied carbon emissions in kg per GPU from NVIDIA. The text mentions that estimating these emissions is challenging and that previous work has used approximations based on other hardware, but it does not give a concrete value."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The training budget for FLM-101B was $100,000, and the amortized hardware and energy cost for GPT-4 was $40M. To find the factor, we divide the cost of GPT-4 by the budget of FLM-101B.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","['GPT-4 has the largest amortized hardware and energy cost, at $40M.', 'The total cost of FLM-101B is computed as $100,000.']","The training budget for FLM-101B was $100,000, and the amortized hardware and energy cost for GPT-4 was $40M. To find the factor, we divide the cost of GPT-4 by the budget of FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context directly provides the CO2e for a single passenger round trip from San Francisco to New York as ~1.2t CO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","Google Flights calculates the average direct round trip flights between SFO and JFK as 180.4t CO2e for the whole plane, with an average seat occupancy leading to 1.2t of CO2e per passenger round trip.","The context directly provides the CO2e for a single passenger round trip from San Francisco to New York as ~1.2t CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","FLM-101B achieved an average performance score of 43.94 on the Open LLM Leaderboard.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","FLM-101B achieved an average performance score of 43.94 on the Open LLM Leaderboard."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that there is a more than 6,750 fold improvement in processor clock speed from the Intel 4004 in 1971 to typical 2021 microprocessors.","6750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that there is a more than 6,750 fold improvement in processor clock speed from the Intel 4004 in 1971 to typical 2021 microprocessors."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it took approximately 14.8 days to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it took approximately 14.8 days to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The table provides energy consumption data for various models at different request frequencies. For the Llama 3.2 1B model at an 8 request/s frequency, the energy consumption can be found by looking at the 'Energy Consumption(1k input-1k output)(Wh)' column since the 2400 prompts likely refer to a scenario with a 1k input-1k output setup.","0.342","kWh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","The table provides energy consumption data for various models at different request frequencies. For the Llama 3.2 1B model at an 8 request/s frequency, the energy consumption can be found by looking at the 'Energy Consumption(1k input-1k output)(Wh)' column since the 2400 prompts likely refer to a scenario with a 1k input-1k output setup."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context specifies that the total permitted annual emission limits for nitrogen oxides (NOx) from data center backup generators in northern Virginia were approximately 13,000 tons.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context specifies that the total permitted annual emission limits for nitrogen oxides (NOx) from data center backup generators in northern Virginia were approximately 13,000 tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that the total time cost for training FLM-101B using the growth strategy was 21.54 days, which is a 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The paper states that the total time cost for training FLM-101B using the growth strategy was 21.54 days, which is a 72% time-saving compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","","","deaths","[]","is_blank","is_blank",""
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) mention that the 6 billion parameter transformer model was trained for 8 days, but this was for only 13% of the full training. To estimate the full training time, they calculate by the proportion (60/8) which gives approximately 60 days.","60","days","[""dodge2022""]","is_blank","The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days.","Dodge et al. (2022) mention that the 6 billion parameter transformer model was trained for 8 days, but this was for only 13% of the full training. To estimate the full training time, they calculate by the proportion (60/8) which gives approximately 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The model with the highest GPU energy consumption for 1,000 queries, based on the context provided, is not explicitly identified with a numerical value. However, the context discusses various models' energy usage without specifying exact numbers for 1,000 queries.","is_blank","is_blank","[""jegham2025""]","is_blank","The context lists energy consumption patterns across different models but does not provide a direct comparison for 1,000 queries.","The model with the highest GPU energy consumption for 1,000 queries, based on the context provided, is not explicitly identified with a numerical value. However, the context discusses various models' energy usage without specifying exact numbers for 1,000 queries."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation...","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.'","22","percent","[""luccioni2025a""]","is_blank","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.","The context states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.'"
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The energy consumption for pre-training the BLOOM model is not directly provided in the context snippets. However, it is mentioned that the BLOOM-176B model, which is a variant of the BLOOM model, required an estimated 103,593 kWh for training to completion, which is converted from the given MWh for consistency.","103.593","MWh","[""luccioni2022""]","[""https://arxiv.org/abs/2211.02001""]","The energy consumption for BLOOM-176B, a related model, is estimated.","The energy consumption for pre-training the BLOOM model is not directly provided in the context snippets. However, it is mentioned that the BLOOM-176B model, which is a variant of the BLOOM model, required an estimated 103,593 kWh for training to completion, which is converted from the given MWh for consistency."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context specifies that the Switch Transformer model has 1500 billion parameters with only 0.1% activated per token.","0.1","percent","[""patterson2021""]","is_blank","The Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design improved models with reduced communication and computational costs, with large sparse models—1500B parameters but only 0.1% activated per token.","The context specifies that the Switch Transformer model has 1500 billion parameters with only 0.1% activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B has the same number of experts, which is 8, for every layer.","8","experts","[""shen2024""]","is_blank","The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that JetMoE-8B has the same number of experts, which is 8, for every layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","","is_blank","second","[""xia2024""]","is_blank","is_blank",""
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context specifies that the cost of FLM-101B for English is 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context specifies that the cost of FLM-101B for English is 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","","","is_blank","[]","is_blank","is_blank",""
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The 'Power Hungry Processing' study analyzed 88 models across 10 tasks and 30 datasets.","88","models","[""luccioni2024""]","[""https://doi.org/10.1145/3630106.3658542""]","The study systematically compared the energy costs of 88 models across 10 tasks and 30 datasets.","The 'Power Hungry Processing' study analyzed 88 models across 10 tasks and 30 datasets."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for LM training reduces the overall carbon footprint by a factor.","3","multiplier","[""wu2021a""]","is_blank","As GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80% leads to a decrease in the overall carbon footprint.","The context states that increasing GPU utilization up to 80% for LM training reduces the overall carbon footprint by a factor."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) estimate that the total compute for model development, including experiments and training before the final run, is 1.2x to 4x larger than the compute for the final training run alone.","[1.2, 4]","multiplier","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","Based on evidence from Cottier's paper, they applied a multiplicative factor to the final training run compute, estimating it to be 1.2x to 4x larger when considering the entire model development phase.","Cottier et al. (2025) estimate that the total compute for model development, including experiments and training before the final run, is 1.2x to 4x larger than the compute for the final training run alone."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that the full training run of the 6 billion parameter transformer model is estimated to consume approximately 103,593 kWh, which is converted to MWh by dividing by 1000.","103.593","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the full training run of the 6 billion parameter transformer model is estimated to consume approximately 103,593 kWh, which is converted to MWh by dividing by 1000."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument discussed in the context provided revolves around the Jevons' Paradox, which suggests that increasing efficiency in AI might lead to higher overall consumption instead of reducing environmental harm.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The central argument discussed in the context provided revolves around the Jevons' Paradox, which suggests that increasing efficiency in AI might lead to higher overall consumption instead of reducing environmental harm."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on the estimation considering AI's energy consumption and water usage efficiency.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","The estimate is derived from the AI model's projected electricity consumption and the water usage efficiency factors for both on-site and off-site water consumption.","The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on the estimation considering AI's energy consumption and water usage efficiency."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost, which contradicts the statement in the question.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost, which contradicts the statement in the question."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","","","is_blank","[]","is_blank","is_blank",""
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippet from reference id 'li2025b' provides the information that GPT-3's training consumed an estimated 1287 MWh of electricity.","1287","MWh","[""li2025b""]","is_blank","GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity for training.","The context snippet from reference id 'li2025b' provides the information that GPT-3's training consumed an estimated 1287 MWh of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The ML.ENERGY Benchmark is the collaborative project mentioned that aims to measure and optimize inference energy consumption for modern generative AI models, providing a standardized method for comparison.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://ml.energy/leaderboard""]","The ML.ENERGY Benchmark, as described, is designed to measure inference energy consumption under realistic service environments and serves as a leaderboard for understanding and optimizing energy use in AI models.","The ML.ENERGY Benchmark is the collaborative project mentioned that aims to measure and optimize inference energy consumption for modern generative AI models, providing a standardized method for comparison."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not provide a specific execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 on an NVIDIA A40-48 GB GPU.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context does not provide a specific execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 on an NVIDIA A40-48 GB GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context specifies that for every layer in JetMoE-8B, the top-k selection parameter is set to 2, meaning 2 experts are selected for activation per token.","2","experts","[""shen2024""]","is_blank","The hyperparameters of JetMoE-8B are selected...set the same number of experts to 8 and top-k to 2 for every layer.","The context specifies that for every layer in JetMoE-8B, the top-k selection parameter is set to 2, meaning 2 experts are selected for activation per token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context provided by [cottier2024] and [schwartz2019] supports the statement, with [schwartz2019] specifically mentioning a 300,000x increase in compute used for training deep learning models over six years (2012-2018).","True","is_blank","[""cottier2024"", ""schwartz2019""]","is_blank","[Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.]","The context provided by [cottier2024] and [schwartz2019] supports the statement, with [schwartz2019] specifically mentioning a 300,000x increase in compute used for training deep learning models over six years (2012-2018)."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context mentions that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, particularly with vehicle transportation in the US contributing to this reduction.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context mentions that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, particularly with vehicle transportation in the US contributing to this reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born in 1956. To calculate its approximate age in 2025, we subtract 1956 from 2025, which gives us 69 years.","69","years","[""stone2022""]","is_blank","The field of AI was officially born and christened at a 1956 workshop.","The field of Artificial Intelligence was officially born in 1956. To calculate its approximate age in 2025, we subtract 1956 from 2025, which gives us 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate comes from the energy required for neural architecture search (NAS), an infrequent and resource-intensive process used in AI model development.","neural architecture search (NAS)","is_blank","[""wu2021a"", ""zschache2025"", ""luccioni2025c""]","[""wu2021a"", ""zschache2025"", ""luccioni2025c""]","The NAS training workload, as mentioned in the context, represents a large-scale procedure performed less frequently due to its high resource requirements, and it was the basis for the 'five cars' carbon footprint estimate.","The 'five cars' carbon footprint estimate comes from the energy required for neural architecture search (NAS), an infrequent and resource-intensive process used in AI model development."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms on the Llama 3.1 8B model, which is equivalent to 77 ms on the Pareto frontier, reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","In this context, a chatbot provider can target an average TPOT of 100 ms... This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms on the Llama 3.1 8B model, which is equivalent to 77 ms on the Pareto frontier, reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The maximum batch size for BlackMamba with a sparse setup on the GSM8K dataset using an NVIDIA A40 GPU with 48 GB memory can be found in Table III, which lists the maximum batch sizes for different models and datasets.","20","samples","[""xia2024""]","is_blank","Table III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.","The maximum batch size for BlackMamba with a sparse setup on the GSM8K dataset using an NVIDIA A40 GPU with 48 GB memory can be found in Table III, which lists the maximum batch sizes for different models and datasets."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon's energy storage capacity was 1.3 GW in 2023.","1.3","GW","[""amazon2023""]","is_blank","In 2023, Amazon's energy storage capacity was up from 445 MW in 2022 to 1.3 GW.","The context states that Amazon's energy storage capacity was 1.3 GW in 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The annual carbon emissions from GPT-4o inference are compared to the carbon emissions of running a single passenger round trip flight between San Francisco and New York, with the context indicating that such emissions are equivalent to the carbon footprint of a certain number of these flights.","30","flights","[""jegham2025""]","is_blank","The comparison is made to the carbon footprint of a single passenger round trip flight between San Francisco and New York, emphasizing the scale of the environmental cost.","The annual carbon emissions from GPT-4o inference are compared to the carbon emissions of running a single passenger round trip flight between San Francisco and New York, with the context indicating that such emissions are equivalent to the carbon footprint of a certain number of these flights."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","","","percent","[]","is_blank","is_blank",""
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The global average power usage effectiveness (PUE) of data centers in 2023 was 1.58, as mentioned in a statistic from Statista referenced in the context.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.""]","The Delegated Regulation provides specific key performance indicators and methodology. Most notable is the requirement to measure and report the energy consumption of the installed information technology. Following the standard-methodology for the calculation of PUE, the energy consumption must be measured.","The global average power usage effectiveness (PUE) of data centers in 2023 was 1.58, as mentioned in a statistic from Statista referenced in the context."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","JetMoE-8B activates 2 billion parameters for each input token during inference, as stated in the context.","2000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","JetMoE-8B activates 2 billion parameters for each input token during inference, as stated in the context."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context snippet provides a table listing the GHG emissions for various models, including the Llama 7B model. The GHG emissions for Llama 7B are stated as 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Range of Pre-Training Environmental Impacts (Representative Models Displayed) [...] Llama 7B 63 Meta 356 14","The context snippet provides a table listing the GHG emissions for various models, including the Llama 7B model. The GHG emissions for Llama 7B are stated as 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that Amazon's U.S. fleet went from 2,600 electric delivery vans in 2022 to 11,800 in 2023. To find the total added, we subtract the 2022 number from the 2023 number.","9200","electric delivery vans","[""amazon2023""]","is_blank","In 2023, our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that Amazon's U.S. fleet went from 2,600 electric delivery vans in 2022 to 11,800 in 2023. To find the total added, we subtract the 2022 number from the 2023 number."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The statement is False. The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, which encompasses servers and data center cooling.","False","is_blank","[""rubei2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The statement is False. The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, which encompasses servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","","","Wh","[]","is_blank","is_blank",""
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence was officially born and christened in 1956.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context clearly states that the field of Artificial Intelligence was officially born and christened in 1956."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","","","WUE","[]","is_blank","is_blank",""
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context directly states that approximately 770 million people globally do not have access to a stable supply of electricity, referencing the International Energy Agency.","True","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context directly states that approximately 770 million people globally do not have access to a stable supply of electricity, referencing the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that with a maximum generation length of 512, it takes about 3-4 Joules for an output token.","[3,4]","joules per token","[""samsi2024""]","is_blank","However, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that with a maximum generation length of 512, it takes about 3-4 Joules for an output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting the numerical representation of Facebook's recommendation model (RM2) from 32-bit to 16-bit, the overall model size was reduced by 15%. This directly answers the question regarding the percentage reduction.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting the numerical representation of Facebook's recommendation model (RM2) from 32-bit to 16-bit, the overall model size was reduced by 15%. This directly answers the question regarding the percentage reduction."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context provided indicates that the claim AI can reduce global GHG emissions by 5-10% is widely cited but lacks clear, publicly available calculations and sound scientific grounding. The estimates are based on BCG reports that do not detail the underlying calculations beyond their experience with clients.","0","is_blank","[""luccioni2025c""]","is_blank","This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that 'Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union'... The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context provided indicates that the claim AI can reduce global GHG emissions by 5-10% is widely cited but lacks clear, publicly available calculations and sound scientific grounding. The estimates are based on BCG reports that do not detail the underlying calculations beyond their experience with clients."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy.","True","is_blank","[""patterson2021""]","is_blank","Sparsely activated models use many more parameters with much lower total FLOPS and can consume less than 1/10th the energy of large, dense DNNs without losing accuracy.","The context states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that the estimated cost of training Grover on 256 TPU chips for two weeks was $25,000.","25000","USD","[""schwartz2019""]","[""https://link.springer.com/chapter/10.1007/978-3-030-50456-5_3""]","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context directly states that the estimated cost of training Grover on 256 TPU chips for two weeks was $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","To estimate the daily energy consumption of Google Translate using the Flan-T5-xxl model, we multiply the energy consumption per 1k queries (0.083 kWh) by 1 billion queries. Converting this to MWh gives us the daily energy consumption.","83","MWh","[""luccioni2024""]","[""https://blog.google/products/translate/ten-years-of-google-translate/""]","Flan-T5-xxl energy consumption per 1k queries is 0.083 kWh, and Google Translate processes 1 billion queries per day.","To estimate the daily energy consumption of Google Translate using the Flan-T5-xxl model, we multiply the energy consumption per 1k queries (0.083 kWh) by 1 billion queries. Converting this to MWh gives us the daily energy consumption."
"q067","What was the average global data center PUE in 2023?","The context provided directly states the average global data center Power Usage Effectiveness (PUE) in 2023.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.""]","The average data center PUE in 2023 was 1.58 globally[74]","The context provided directly states the average global data center Power Usage Effectiveness (PUE) in 2023."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","","","wind turbines","[]","is_blank","is_blank",""
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that Gemini Ultra had the highest fraction of R&D staff cost at 49%, which includes equity.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that Gemini Ultra had the highest fraction of R&D staff cost at 49%, which includes equity."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the Study Panel launched in mid-fall 2015 had seventeen members.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories, and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context states that the Study Panel launched in mid-fall 2015 had seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet specifies that 74% of a client device's total carbon footprint is due to its manufacturing.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/"", ""https://www.researchgate.net/publication/350065137_Sustainable_AI_Computing_From_Embodied_Carbon_to_Lifecycle_Management""]","It is particularly challenging to amortize the embodied carbon footprint because client devices are often under-utilized [97]. Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19].","The context snippet specifies that 74% of a client device's total carbon footprint is due to its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context provided by multiple references indicates that the energy consumption of a model during inference is influenced by various factors, including model architecture, hardware, and inference process characteristics, rather than solely the number of parameters. For instance, the use of different attention mechanisms can affect memory consumption and, consequently, energy use, as shown by the comparison between Phi-3 Mini and Small models.","False","is_blank","[""chung2025""]","[""https://ml.energy/leaderboard"", ""https://arxiv.org/abs/2504.17674"", ""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2508.14170""]","For example, Figure 4 from 'The ML.ENERGY Benchmark' shows that the larger Phi-3 Small model can consume less energy than the smaller Phi-3 Mini model for larger batch sizes due to differences in memory usage and ability to scale batch size.","The context provided by multiple references indicates that the energy consumption of a model during inference is influenced by various factors, including model architecture, hardware, and inference process characteristics, rather than solely the number of parameters. For instance, the use of different attention mechanisms can affect memory consumption and, consequently, energy use, as shown by the comparison between Phi-3 Mini and Small models."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context does not contain specific data on the CO2 emissions from OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific data on the CO2 emissions from OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The reported GHG emissions for the pre-training process of Meta's Llama 3 family of models are not directly provided in the context. However, it is mentioned that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car. This information is used to contrast with the 'five cars' estimate, implying that the emissions are significantly higher.","is_blank","tCO2e","[""han2024""]","is_blank","The text mentions that training an AI model like Llama-3.1 can produce air pollutants equivalent to over 10,000 LA-NYC round trips, implying high emissions, but does not give a direct tCO2e figure.","The reported GHG emissions for the pre-training process of Meta's Llama 3 family of models are not directly provided in the context. However, it is mentioned that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car. This information is used to contrast with the 'five cars' estimate, implying that the emissions are significantly higher."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years.","2.9","multiplier","[""wu2021a""]","[""https://arxiv.org/pdf/2111.00364.pdf""]","The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years."
"q079","How many miles is the Earth from the Sun?","","","miles","[]","is_blank","is_blank",""
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program defeated the human Go champion, with specific references to the event.","True","is_blank","[""stone2022""]","is_blank","For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.","The context clearly states that the AlphaGo program defeated the human Go champion, with specific references to the event."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions a batching strategy called 'continuous batching' which mitigates GPU under-utilization by dynamically replacing completed requests with new ones, thereby improving GPU utilization and reducing idle time.","Continuous Batching","is_blank","[""griggs2024""]","is_blank","LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, which leads to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions a batching strategy called 'continuous batching' which mitigates GPU under-utilization by dynamically replacing completed requests with new ones, thereby improving GPU utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The entire alignment process of JetMoE-8B, including both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This information is directly stated in the context snippet.","60","H100 GPU hours","[""shen2024""]","is_blank","This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.","The entire alignment process of JetMoE-8B, including both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This information is directly stated in the context snippet."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that Max-Performance selected g6e.xlarge for the 100 TPS SLO, which costs $2.699, while InferSave selected g4dn.xlarge costing $2.13. To find the percentage increase, we calculate ((2.699 - 2.13) / 2.13) * 100.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that Max-Performance selected g6e.xlarge for the 100 TPS SLO, which costs $2.699, while InferSave selected g4dn.xlarge costing $2.13. To find the percentage increase, we calculate ((2.699 - 2.13) / 2.13) * 100."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet from 'luccioni2024' provides the specific value for the carbon emissions of the stable-diffusion-xl-base-1.0 model.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences...","The context snippet from 'luccioni2024' provides the specific value for the carbon emissions of the stable-diffusion-xl-base-1.0 model."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a table showing energy consumption for various models but does not specify a direct range for GPU energy usage for 1,000 inference queries.","is_blank","Wh","[""is_blank""]","is_blank","The provided context discusses energy consumption in joules for different models and setups but does not detail energy usage for a batch of 1,000 inferences.","The context provides a table showing energy consumption for various models but does not specify a direct range for GPU energy usage for 1,000 inference queries."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context provided states that there is a lack of consensus and limited convergence in AI ethics and sustainability principles, indicating that a universal approach is difficult to define.","0","is_blank","[""luccioni2025b""]","is_blank","To this point, a 2022 study of papers submitted to conferences such as ICML and NeurIPS, Birhane et al. analyzed the values that were highlighted by their authors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered explicitly [2022a]. From the 59 values they identified, the most emphasis was put on aspects such as technical progress, quantitative evidence, and novelty, whereas ethical considerations around values such as beneﬁcence, interpretability, and respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a fraction of papers. Also, not a single one of the values Birhane et al. identified was explicitly linked to environmental sustainability, highlighting once again the lack of connection in the research community with sustainability writ large.","The context provided states that there is a lack of consensus and limited convergence in AI ethics and sustainability principles, indicating that a universal approach is difficult to define."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context snippet from 'patterson2021' provides the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The specific figure is mentioned directly in the context snippet: 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh.'","The context snippet from 'patterson2021' provides the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions that the Hivemind framework, a PyTorch-based library, was used to enable collaborative deep learning training and handle peers dropping out, which is crucial for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/learning-at-home/hivemind""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context mentions that the Hivemind framework, a PyTorch-based library, was used to enable collaborative deep learning training and handle peers dropping out, which is crucial for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This concept is mentioned as a way to integrate not only the technical aspects of AI but also its environmental and societal consequences, making the AI system more accountable for its impacts.","social transparency","is_blank","[""luccioni2025b""]","[""https://arxiv.org/pdf/2504.00797.pdf""]","For instance, integrating social transparency and sustainability can be exemplified by an AI system designed for urban planning... In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have...","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This concept is mentioned as a way to integrate not only the technical aspects of AI but also its environmental and societal consequences, making the AI system more accountable for its impacts."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context does not provide specific accuracy values for models using sentence embeddings in the classification experiments on German public administration texts.","is_blank","is_blank","[""is_blank""]","is_blank","The information given outlines various datasets and model evaluations but does not detail accuracy figures for sentence embedding models in the specified context.","The context does not provide specific accuracy values for models using sentence embeddings in the classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The paper by Chen et al. in 2025 introduces an LLM inference system named Lamina, which incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","To enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach... We develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The paper by Chen et al. in 2025 introduces an LLM inference system named Lamina, which incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context mentions the T5 model's parameter counts, with the largest mentioned size being 1.5B (1.5 billion) parameters.","1500000000","parameters","[""patterson2021""]","is_blank","The parameter count is 124.7M, 355.4M, and 560.1M, respectively.","The context mentions the T5 model's parameter counts, with the largest mentioned size being 1.5B (1.5 billion) parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in JetMoE-8B is mentioned directly as 8B.","8000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters.","The total number of parameters in JetMoE-8B is mentioned directly as 8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","Google's data center water consumption increased by 20% from 2021 to 2022, as mentioned in the context snippet.","20","percent","[""li2025b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4]","Google's data center water consumption increased by 20% from 2021 to 2022, as mentioned in the context snippet."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context specifically mentions a metric called 'Carbon Intensity', which is defined as 'CO₂ emissions per unit of electricity consumed'. This metric aligns with the description in the question.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context specifically mentions a metric called 'Carbon Intensity', which is defined as 'CO₂ emissions per unit of electricity consumed'. This metric aligns with the description in the question."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context specifies that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%. This information is directly extracted from Table 2 which details the parallel strategies and throughput for different growth stages.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages.","The context specifies that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%. This information is directly extracted from Table 2 which details the parallel strategies and throughput for different growth stages."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the amortized cost to train OpenAI's GPT-4 is $40M.","40000000","USD","[""cottier2024""]","[""arXiv:2405.21015v2 [cs.CY] 7 Feb 2025""]","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M.","The context states that the amortized cost to train OpenAI's GPT-4 is $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context mentions that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can significantly reduce the operational carbon footprint of a Transformer-based universal translation model. Specifically, it states that with tensor parallelism, which is part of full-stack optimization, the energy-optimal configuration for a certain application can result in 21% less energy than the configuration that minimizes latency, implying a reduction factor.","reduction","multiplier","[""dodge2022"", ""jegham2025"", ""wu2021a"", ""fernandez2025""]","is_blank","The specific factor is not a numeric multiplier directly stated for the full combination of optimizations mentioned, hence the answer focuses on the concept of reduction without a precise numerical value.","The context mentions that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can significantly reduce the operational carbon footprint of a Transformer-based universal translation model. Specifically, it states that with tensor parallelism, which is part of full-stack optimization, the energy-optimal configuration for a certain application can result in 21% less energy than the configuration that minimizes latency, implying a reduction factor."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","When training NLP models across four continents, the performance dropped to a fraction of the local throughput. Specifically, the NLP throughput was 41% lower in the C-8 experiment compared to the fully local A-8 experiment, indicating that the multi-continental setup achieved less than half of the local throughput.","0.41","multiplier","[""erben2023""]","[""arXiv:2306.03183v4""]","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","When training NLP models across four continents, the performance dropped to a fraction of the local throughput. Specifically, the NLP throughput was 41% lower in the C-8 experiment compared to the fully local A-8 experiment, indicating that the multi-continental setup achieved less than half of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context provided states that the usage of custom tags in prompts improves the energy efficiency of Llama 3 during code completion tasks. It specifically mentions that energy consumption can be reduced across zero-shot, one-shot, and few-shots techniques.","True","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.","The context provided states that the usage of custom tags in prompts improves the energy efficiency of Llama 3 during code completion tasks. It specifically mentions that energy consumption can be reduced across zero-shot, one-shot, and few-shots techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context snippet from a 2025 paper states that NVIDIA shipped 3.7 million GPUs in 2024, specifically for data center use.","3700000","GPUs","[""jegham2025""]","is_blank","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [31].","The context snippet from a 2025 paper states that NVIDIA shipped 3.7 million GPUs in 2024, specifically for data center use."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10, as stated in the provided context.","1.1","PUE","[""wu2021a"", ""wu2021b""]","[""https://sustainability.fb.com/report-page/data-centers/"", ""https://www.google.com/about/datacenters/efficiency/""]","is_blank","Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10, as stated in the provided context."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish project that proposed the integration of ethics, sustainability, design, and foresight for AI governance is referred to as ETAIROS.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The Finnish project that proposed the integration of ethics, sustainability, design, and foresight for AI governance is referred to as ETAIROS."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the amortized hardware cost for Google's Gemini Ultra was $30M.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that the amortized hardware cost for Google's Gemini Ultra was $30M."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act does indeed require providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks. This is mentioned in the context provided, where it is argued that the measures should consider environmental risks in line with the AI Act's objectives.","True","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9), which should also consider environmental risks.","The AI Act does indeed require providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks. This is mentioned in the context provided, where it is argued that the measures should consider environmental risks in line with the AI Act's objectives."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context states that the EPA's recently tightened standard for the annual average limit of PM2.5 is 9 µg/m³.","9","µg/m³","[""han2024""]","is_blank","The NAAQS primary standards set the annual average PM 2.5 concentration at 9 µg/m³.","The context states that the EPA's recently tightened standard for the annual average limit of PM2.5 is 9 µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context provided states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","A life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","The context provided states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context provided states that the per-household health burden from air pollutants in certain low-income communities can exceed that in less-impacted areas by more than 200-fold.","200","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","The context provided states that the per-household health burden from air pollutants in certain low-income communities can exceed that in less-impacted areas by more than 200-fold."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is mentioned in Table B1, where it states the energy consumed as 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is mentioned in Table B1, where it states the energy consumed as 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide a specific number for the total parameters of the large language model analyzed by Dodge et al. in their 2022 paper.","is_blank","parameters","[""is_blank""]","is_blank","The provided context discusses various aspects of large language models and their training but does not quote a specific number of parameters for the model analyzed by Dodge et al.","The context does not provide a specific number for the total parameters of the large language model analyzed by Dodge et al. in their 2022 paper."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox, where technological advancements leading to greater efficiency in resource use, such as energy, paradoxically lead to an overall increase in consumption due to the lowered cost and increased accessibility of the resource.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries.","The context describes Jevons' Paradox, where technological advancements leading to greater efficiency in resource use, such as energy, paradoxically lead to an overall increase in consumption due to the lowered cost and increased accessibility of the resource."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","","","multiplier","[]","is_blank","is_blank",""
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides a table showing that the average energy consumption for 1,000 image generation inferences is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2: Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis, which includes image generation with a mean of 2.907 kWh.","The context provides a table showing that the average energy consumption for 1,000 image generation inferences is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context mentions that the carbon emissions from the average US home for a year are 8.3 metric tons CO2. To convert metric tons to pounds, we multiply by 2204.62 (since 1 metric ton = 2204.62 lbs). However, the question specifically asks for pounds of CO2e, and the conversion from metric tons to pounds is straightforward.","18353.86","lbs","[""dodge2022""]","is_blank","The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest experiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the United States Environmental Protection Agency [46]. The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context mentions that the carbon emissions from the average US home for a year are 8.3 metric tons CO2. To convert metric tons to pounds, we multiply by 2204.62 (since 1 metric ton = 2204.62 lbs). However, the question specifically asks for pounds of CO2e, and the conversion from metric tons to pounds is straightforward."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context does not provide specific projections for per-household health costs in West Virginia counties for 2030.","is_blank","is_blank","[""is_blank""]","is_blank","The provided context discusses the overall health impact of data centers in the U.S. and mentions West Virginia in the context of data centers' impact but does not give a specific projection for per-household health costs in any West Virginia county for 2030.","The context does not provide specific projections for per-household health costs in West Virginia counties for 2030."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The provided context does not contain specific numbers or multipliers for the emission reduction of Mistral-small after optimization in the financial sentiment classification task.","is_blank","multiplier","[""is_blank""]","is_blank","The context discusses carbon footprint reduction across models but does not provide a direct comparison for Mistral-small's emissions change.","The provided context does not contain specific numbers or multipliers for the emission reduction of Mistral-small after optimization in the financial sentiment classification task."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The BLOOMz-7B model's combined training and fine-tuning energy cost is derived from the sum of the training energy (51,686 kWh) and the fine-tuning energy (7,571 kWh) as reported in the provided context.","59257","kWh","[""ebert2024,luccioni2024""]","is_blank","Training energy (kWh) 51,686 and Finetuning energy (kWh) 7,571 for BLOOMz-7B from Table 5 in the 'Power Hungry Processing' study.","The BLOOMz-7B model's combined training and fine-tuning energy cost is derived from the sum of the training energy (51,686 kWh) and the fine-tuning energy (7,571 kWh) as reported in the provided context."
"q125","What is the total number of parameters in the final FLM-101B model?","The text directly states the model sizes at different stages, culminating in the FLM-101B model, which has 101B parameters.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with various configurations, ultimately reaching 101B parameters.","The text directly states the model sizes at different stages, culminating in the FLM-101B model, which has 101B parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The 6.1B parameter model's full training energy cost is estimated at 103,593 kWh. Using BLOOMz-7B as a comparable model, with an inference energy cost of 1.0 × 10^-4 kWh, we calculate the number of inferences required to match the training energy cost.","1035930000","inferences","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The 6.1B parameter model's full training energy cost is estimated at 103,593 kWh. Using BLOOMz-7B as a comparable model, with an inference energy cost of 1.0 × 10^-4 kWh, we calculate the number of inferences required to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' was mentioned directly.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' was mentioned directly."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table showing the cost parity in terms of the number of inferences required for the BLOOMz-7B model to equal the energy cost of its training and fine-tuning. This number is directly given as 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5 in the provided context snippet lists the cost parity for BLOOMz models, specifying that for BLOOMz-7B, it is 592,570,000 inferences.","The context provides a table showing the cost parity in terms of the number of inferences required for the BLOOMz-7B model to equal the energy cost of its training and fine-tuning. This number is directly given as 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The dataset used for the German nuclear waste site objection texts is referred to as the FKTG-dataset, which contains statements from the public regarding the selection process of a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany... The statements from the population were categorized, processed and published as the FKTG-dataset.","The dataset used for the German nuclear waste site objection texts is referred to as the FKTG-dataset, which contains statements from the public regarding the selection process of a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The provided context does not contain specific figures or calculations regarding the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","The context discusses various aspects of AI model energy and environmental impact but lacks direct information on Meta's Llama 3 inference water usage.","The provided context does not contain specific figures or calculations regarding the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","","","percent","[]","is_blank","is_blank",""
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e of the Evolved Transformer NAS is equivalent to ~3 passengers taking a round trip between San Francisco and New York.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e of the Evolved Transformer NAS is equivalent to ~3 passengers taking a round trip between San Francisco and New York."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that 84% of LLM usage is through models with no disclosure of their environmental impact, as per May 2025 data from OpenRouter.","84","percent","[""luccioni2025c""]","is_blank","May 2025 data (Figure 2) indicates that of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1, DeepSeek V3, Mistral Nemo) release it indirectly... 84% of LLM usage is through models with no disclosure.","The context states that 84% of LLM usage is through models with no disclosure of their environmental impact, as per May 2025 data from OpenRouter."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies the minimum hardware requirements for LLaMA-65B, but not specifically for LLaMA-13B. However, since LLaMA-13B is smaller than LLaMA-65B, it would require fewer resources. The information given for LLaMA-65B requires 4 A100 GPUs with 80GB of memory, implying that LLaMA-13B, which is mentioned to have been run on a single A100 GPU in some configurations, would at least require fewer than 4 A100 80GB GPUs.","less_than_4","A100_80GB_GPU","[""is_blank""]","is_blank","The specific minimum for LLaMA-13B is not directly provided, but inference can be made based on the requirements for the larger LLaMA-65B model.","The context specifies the minimum hardware requirements for LLaMA-65B, but not specifically for LLaMA-13B. However, since LLaMA-13B is smaller than LLaMA-65B, it would require fewer resources. The information given for LLaMA-65B requires 4 A100 GPUs with 80GB of memory, implying that LLaMA-13B, which is mentioned to have been run on a single A100 GPU in some configurations, would at least require fewer than 4 A100 80GB GPUs."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides two estimates for CO2 emissions, one without adjustments and another considering hardware and datacenter specifics. The initial estimate is 15.2 metric tons, while the adjusted estimate, considering a ~5X reduction, suggests a lower value.","[15.2, 'lower value']","metric tons","[""dodge2022""]","is_blank","The initial CO2 estimate is calculated as 33,544 lbs (which converts to approximately 15.2 metric tons), and it's mentioned that there's a subsequent ~5X reduction possible when accounting for the actual hardware and datacenter conditions.","The context provides two estimates for CO2 emissions, one without adjustments and another considering hardware and datacenter specifics. The initial estimate is 15.2 metric tons, while the adjusted estimate, considering a ~5X reduction, suggests a lower value."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The provided context does not contain a specific figure or statistic on the total carbon emissions avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[]","is_blank","The context discusses various aspects of energy consumption and carbon footprint reduction strategies for language models but lacks a direct figure for emissions avoided through pruning and quantization in 2023.","The provided context does not contain a specific figure or statistic on the total carbon emissions avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context mentions a scenario where using 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy.","24","percent","[""griggs2024""]","is_blank","Mixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs consistently yields the lowest deployment cost. ... Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only.","The context mentions a scenario where using 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provided does not contain a specific price per hour for an NVIDIA H20 GPU.","is_blank","USD per hour","[""is_blank""]","is_blank","No specific pricing information for NVIDIA H20 GPU hourly rates was mentioned in the text.","The context provided does not contain a specific price per hour for an NVIDIA H20 GPU."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors.' This implies that the process is not automated and requires contacting authors for the data.","False","is_blank","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that 'most carbon footprint analyses gather the information manually by writing to authors.' This implies that the process is not automated and requires contacting authors for the data."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The public health cost of U.S. data centers in 2023 was equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","Our results demonstrate that in 2023, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, which is equivalent to approximately 44% of the data centers’ total electricity cost.","The public health cost of U.S. data centers in 2023 was equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the LLaMA-7B model can run on a single GPU, and specifically mentions that for the larger LLaMA models like 65B, a minimum of 8 V100 GPUs or 4 A100 GPUs is required. Since the question asks about LLaMA-7B, which is smaller, it implies that fewer GPUs are needed. However, the exact number for LLaMA-7B on A100 GPUs isn't explicitly stated, but given the scalability mentioned, it suggests a single A100 GPU could handle it.","1","A100_80GB_GPU","[""samsi2024""]","[""https://arxiv.org/""]","The context does not provide an exact count for LLaMA-7B on A100 GPUs but implies that fewer GPUs are needed compared to larger models like LLaMA-65B.","The context states that the LLaMA-7B model can run on a single GPU, and specifically mentions that for the larger LLaMA models like 65B, a minimum of 8 V100 GPUs or 4 A100 GPUs is required. Since the question asks about LLaMA-7B, which is smaller, it implies that fewer GPUs are needed. However, the exact number for LLaMA-7B on A100 GPUs isn't explicitly stated, but given the scalability mentioned, it suggests a single A100 GPU could handle it."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context provided by ref_id=khan2025 states that experimental results reveal a reduction of up to 45% in energy consumption and carbon emissions post quantization.","45","is_blank","[""khan2025""]","[""https://arxiv.org/pdf/2504.06307v1.pdf""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","The context provided by ref_id=khan2025 states that experimental results reveal a reduction of up to 45% in energy consumption and carbon emissions post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","Researchers reached out to over 500 authors and were able to collect 95 answers.","95","answers","[""luccioni2025b""]","[""https://www.luccioni.ai/publications/luccioni-hernandez-garcia-2025-bridging-the-gap-integrating-ethics-and-environmental-sustainability-in-ai-research-and-practice/""]","Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers.","Researchers reached out to over 500 authors and were able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The training budget for JetMoE was $100k, and it utilized 30,000 H100 GPU hours. To find the approximate cost per H100 GPU-hour, we divide the total budget by the total GPU hours.","$3.33","USD per hour","[""shen2024""]","is_blank","JetMoE is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The training budget for JetMoE was $100k, and it utilized 30,000 H100 GPU hours. To find the approximate cost per H100 GPU-hour, we divide the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost of training a Llama-3.1 scale model in Altoona, Iowa, exceeded 120% of the electricity cost.","120","percent","[""han2024""]","is_blank","The results highlight that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.","The context states that the total health cost of training a Llama-3.1 scale model in Altoona, Iowa, exceeded 120% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","JetMoE-8B was trained using 1.25 trillion tokens, as mentioned in the context.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","JetMoE-8B was trained using 1.25 trillion tokens, as mentioned in the context."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context specifies that Amazon had announced 36 renewable energy projects in the United Kingdom as of January 2024.","36","projects","[""amazon2023""]","is_blank","Our total capacity of renewable energy in Europe is now 7 GW, including 1.7 GW of renewable energy from offshore wind... Project Location: United Kingdom 36 901","The context specifies that Amazon had announced 36 renewable energy projects in the United Kingdom as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides a table showing the gender distribution of Amazon's workforce in the U.S. for 2023, where 56.8% of the workforce identified as men.","56.8","percent","[""amazon2023""]","is_blank","Amazon Representation by the Numbers: Gender—U.S.: Men - 56.8%","The context provides a table showing the gender distribution of Amazon's workforce in the U.S. for 2023, where 56.8% of the workforce identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","Apple reports that its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/"", ""https://www.apple.com/environment/downloads/""]","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","Apple reports that its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide a specific execution time for a sparse BlackMamba model with a batch size of 84 on a NVIDIA A40-48GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a specific execution time for a sparse BlackMamba model with a batch size of 84 on a NVIDIA A40-48GB GPU."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The metric introduced to assess the ratio of computation to communication time when scaling distributed training across continents is referred to as 'granularity'. This metric is crucial for determining the scalability potential of different models under varying network conditions.","granularity","is_blank","[""erben2023""]","[""https://arxiv.org/abs/2101.03961""]","Granularity is important to evaluate scalability. We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.","The metric introduced to assess the ratio of computation to communication time when scaling distributed training across continents is referred to as 'granularity'. This metric is crucial for determining the scalability potential of different models under varying network conditions."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a single deal with Exxon Mobil, as estimated by a coalition of Microsoft employees, could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","640","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year.","The context states that a single deal with Exxon Mobil, as estimated by a coalition of Microsoft employees, could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines water withdrawal as the term for freshwater taken from ground or surface sources for various uses, excluding water used for hydroelectricity generation.","Water withdrawal","is_blank","[""li2025b""]","is_blank","It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines water withdrawal as the term for freshwater taken from ground or surface sources for various uses, excluding water used for hydroelectricity generation."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context provided states that the average US household had an average of 25 connected devices in 2021.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In 2021, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context provided states that the average US household had an average of 25 connected devices in 2021."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training LLMs, with the lowest being 0.8 MWh for OLMo 20M and the highest being 3,500 MWh for LLaMa 4 Scout.","[0.8, 3500]","MWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3531146""]","Table 1 in the reference document lists the range of pre-training environmental impacts for representative models, showing energy consumption in MWh.","The context provides a range of energy consumption for pre-training LLMs, with the lowest being 0.8 MWh for OLMo 20M and the highest being 3,500 MWh for LLaMa 4 Scout."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program did beat human contenders to win the Jeopardy challenge in 2011.","False","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program did beat human contenders to win the Jeopardy challenge in 2011."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from [ref_id=luccioni2025a] states that one paper suggests 10–50 queries on GPT-3 consume around half a liter of water.","[10, 50]","queries","[""luccioni2025a""]","is_blank","One paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water.","The context snippet from [ref_id=luccioni2025a] states that one paper suggests 10–50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","JetMoE-8B-Chat achieved an MT-Bench score of 6.681, which is higher than the Llama-2-13b-Chat model's score, as stated in the context.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models","JetMoE-8B-Chat achieved an MT-Bench score of 6.681, which is higher than the Llama-2-13b-Chat model's score, as stated in the context."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","","","responses","[]","is_blank","is_blank",""
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings.","77","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","Mélange reduces costs by 9-77% for short-context tasks (interactive chats)...","The context states that Mélange reduces deployment costs by up to 77% in conversational settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that a minimum of 4 A100 GPUs each with 80GB of memory is required for meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","Given the size of LLMs and the limits imposed by current hardware, inference with large models can impose onerous requirements. For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that a minimum of 4 A100 GPUs each with 80GB of memory is required for meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context consistently mentions that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips.","The context consistently mentions that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","NVIDIA estimated in 2019 that 80-90% of the ML workload is inference processing.","[80, 90]","percent","[""patterson2021""]","is_blank","NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","NVIDIA estimated in 2019 that 80-90% of the ML workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' study is not directly provided in the snippets. However, it is mentioned that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","The authors state, 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'","The total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' study is not directly provided in the snippets. However, it is mentioned that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","Estimations based on TDP are nearly always an overestimation because it is rare for a GPU to draw its maximum power at every moment. This is confirmed by the context which states that TDP-based estimates can lead to a worst-case overestimation by a factor of 4.1.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","Estimations based on TDP are nearly always an overestimation because it is rare for a GPU to draw its maximum power at every moment. This is confirmed by the context which states that TDP-based estimates can lead to a worst-case overestimation by a factor of 4.1."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to deployment on A100 hardware instead of H100s.","False","is_blank","[""jegham2025""]","is_blank","GPT-4o mini's consumption is slightly higher at 3.098 Wh compared to GPT-4o's consumption of 2.875 Wh, as mentioned in the context.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to deployment on A100 hardware instead of H100s."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The provided context does not specify a ground truth throughput for Mixtral-CS-A100-40GB with a batch size of 1 directly. However, it gives insights on throughput patterns with different batch sizes and GPU types, implying that for detailed throughput at a batch size of 1, one would need to interpolate or use additional information.","is_blank","queries/sec","[""xia2024""]","is_blank","Context discusses throughput trends with various GPU setups but does not give a specific number for the asked batch size.","The provided context does not specify a ground truth throughput for Mixtral-CS-A100-40GB with a batch size of 1 directly. However, it gives insights on throughput patterns with different batch sizes and GPU types, implying that for detailed throughput at a batch size of 1, one would need to interpolate or use additional information."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context provided states that after 2022, the trend reversed, with direct environmental disclosures decreasing, as indicated by the majority of notable AI models falling under the 'no disclosure' category by the first quarter of 2025.","0","is_blank","[""luccioni2025c""]","is_blank","By the first quarter of 2025, the majority of notable AI models again fell under the ‘no disclosure’ category, as the line between research and commercial deployment became increasingly blurred.","The context provided states that after 2022, the trend reversed, with direct environmental disclosures decreasing, as indicated by the majority of notable AI models falling under the 'no disclosure' category by the first quarter of 2025."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU was calculated by adjusting the RunPod price to match the pricing structure of major cloud providers. The calculation was (4.69/2.29) × 3.67, resulting in $7.516.","7.516","USD per hour","[""griggs2024""]","is_blank","To ensure fair cost comparisons, we normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","The normalized on-demand hourly price for an H100 GPU was calculated by adjusting the RunPod price to match the pricing structure of major cloud providers. The calculation was (4.69/2.29) × 3.67, resulting in $7.516."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The provided context does not contain specific numbers on water usage for cooling during OpenAI's GPT-4 training runs.","is_blank","liters of water","[""is_blank""]","is_blank","The context discusses water consumption in data centers but does not specify water usage specifics for GPT-4 training.","The provided context does not contain specific numbers on water usage for cooling during OpenAI's GPT-4 training runs."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The monthly on-demand rental cost for serving Llama-2-70B with 2 NVIDIA A100 GPUs is given as $5,200. Assuming a month has 30 days, we can calculate the daily cost and then the hourly cost by dividing by the number of hours in a day.","57.78","USD per hour","[""griggs2024""]","is_blank","The monthly rental cost for 2 NVIDIA A100 GPUs is $5,200, implying a daily cost of $173.33 (dividing by 30 days), and then dividing by 24 hours gives the hourly cost.","The monthly on-demand rental cost for serving Llama-2-70B with 2 NVIDIA A100 GPUs is given as $5,200. Assuming a month has 30 days, we can calculate the daily cost and then the hourly cost by dividing by the number of hours in a day."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 for GPT-3-based language translation tasks, the model needs to be 1,000 times larger.","1000","multiplier","[""wu2021a""]","is_blank","Figure 2(a) depicts the 1000× model size increase for GPT3-based language translation tasks [...], whereas for Baidu’s search engine, the model of 1000× larger in size improves accuracy in AUC by 0.030.","The context states that to increase the model quality BLEU score from 5 to 40 for GPT-3-based language translation tasks, the model needs to be 1,000 times larger."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The Strubell et al. study mentioned in the context snippets estimated the CO2 emissions for the neural architecture search (NAS) to find the Evolved Transformer model as 626,155 lbs. To convert the CO2 emissions into an equivalent driving distance, we would typically use a conversion factor that relates CO2 emissions to miles driven, but the specific conversion factor is not provided in the context. Therefore, we can provide the CO2 emissions value but cannot calculate the equivalent driving distance without the conversion factor.","626155","miles","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","In the study by Strubell et al., the CO2 emissions for the NAS of a Transformer model were quantified as 626,155 pounds.","The Strubell et al. study mentioned in the context snippets estimated the CO2 emissions for the neural architecture search (NAS) to find the Evolved Transformer model as 626,155 lbs. To convert the CO2 emissions into an equivalent driving distance, we would typically use a conversion factor that relates CO2 emissions to miles driven, but the specific conversion factor is not provided in the context. Therefore, we can provide the CO2 emissions value but cannot calculate the equivalent driving distance without the conversion factor."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model's inference energy is 1.0 × 10^-4 kWh per inference. If each of the 606,096 downloads resulted in 1 million inferences, the total energy consumed would first be calculated in kWh and then converted to MWh.","606.096","MWh","[""luccioni2024""]","is_blank","The BLOOMz-7B model's inference energy is given as 1.0 × 10^-4 kWh, and the downloads are mentioned in the context.","The BLOOMz-7B model's inference energy is 1.0 × 10^-4 kWh per inference. If each of the 606,096 downloads resulted in 1 million inferences, the total energy consumed would first be calculated in kWh and then converted to MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","is_blank","The costs of training frontier AI models have grown dramatically, and it is projected that by 2027, the largest training runs will cost more than a billion dollars.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippet from 'patterson2021' clearly states that OpenAI published the total number of floating point operations to train their model, GPT-3, as 3.14E+23 FLOPs.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context snippet from 'patterson2021' clearly states that OpenAI published the total number of floating point operations to train their model, GPT-3, as 3.14E+23 FLOPs."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that a minimum of 8 V100 GPUs each with 32 GB of RAM is required for meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","Given the size of LLMs and the limits imposed by current hardware, inference with large models can impose onerous requirements. For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that a minimum of 8 V100 GPUs each with 32 GB of RAM is required for meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","","","zettaFLOPs","[]","is_blank","is_blank",""
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that AlexNet, when trained on ImageNet, typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training.","76.1","percent","[""wu2021a""]","is_blank","In contrast, the same model typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training.","The context mentions that AlexNet, when trained on ImageNet, typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context specifies that a cluster of 24 DGX-A800 GPU servers was used for training, with each server having 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 servers multiplied by 8 GPUs per server.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context specifies that a cluster of 24 DGX-A800 GPU servers was used for training, with each server having 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 servers multiplied by 8 GPUs per server."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","","","lifetimes","[]","is_blank","is_blank",""
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","","","hours","[]","is_blank","is_blank",""
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources.","47500","metric tons","[""amazon2023""]","is_blank","These on-site solar energy projects generate an estimated 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The framework used for deploying large language models across multiple GPUs and nodes is Mélange. This framework automatically determines the most cost-effective GPU allocation while meeting service level objectives (SLOs) for LLM services.","Mélange","is_blank","[""griggs2024""]","is_blank","We introduce Mélange, an allocation framework that automatically derives the minimal-cost GPU allocation for a given LLM service while satisfying an SLO requirement (§ 5).","The framework used for deploying large language models across multiple GPUs and nodes is Mélange. This framework automatically determines the most cost-effective GPU allocation while meeting service level objectives (SLOs) for LLM services."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The text does not provide a direct comparison of the energy consumption increase when deploying the Llama 3.1 70B model on two nodes instead of one.","is_blank","multiplier","[""is_blank""]","is_blank","The provided context discusses energy use but does not include a comparison for the specific model across different node configurations.","The text does not provide a direct comparison of the energy consumption increase when deploying the Llama 3.1 70B model on two nodes instead of one."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide a direct figure for the water consumption per ChatGPT user session in 2023 in gallons.","is_blank","gallons of water","[""is_blank""]","is_blank","The provided context discusses the environmental impact of AI models in general, including energy consumption and carbon emissions, but does not specify water consumption per user session for ChatGPT in 2023 in gallons.","The context does not provide a direct figure for the water consumption per ChatGPT user session in 2023 in gallons."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that the annual electricity consumption for GPT-4o inference, given 700 million daily queries, exceeds the total electricity consumption of 35,000 U.S. residential households. This implies a direct comparison.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that the annual electricity consumption for GPT-4o inference, given 700 million daily queries, exceeds the total electricity consumption of 35,000 U.S. residential households. This implies a direct comparison."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","Microsoft reported a 34% increase in global water consumption between 2021 and 2022, as mentioned in the context snippet.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period.","Microsoft reported a 34% increase in global water consumption between 2021 and 2022, as mentioned in the context snippet."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis, indicating they did not achieve comparable accuracy.","False","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis, indicating they did not achieve comparable accuracy."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that the PUE for the Google Iowa datacenter where the Evolved Transformer was run is 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context states that the PUE for the Google Iowa datacenter where the Evolved Transformer was run is 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025.","The context states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is explicitly mentioned as 53.0.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models, showing JetMoE-8B's average score as 53.0.","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is explicitly mentioned as 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","","","training runs","[]","is_blank","is_blank",""
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The AI Act's open-source exemption is mentioned to be removed, implying that open-source models are no longer fully exempt from reporting their energy consumption.","False","is_blank","[""ebert2024""]","is_blank","The proposal recommends the 'Elimination of the Open-Source Exemption', stating that open-source models should adhere to the same reporting standards as proprietary models, indicating they are not fully exempt.","The AI Act's open-source exemption is mentioned to be removed, implying that open-source models are no longer fully exempt from reporting their energy consumption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 was 1.59, as mentioned in the context.","1.59","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","is_blank","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 was 1.59, as mentioned in the context."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context specifies that when the batch size increases to 32 for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context specifies that when the batch size increases to 32 for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that R&D staff costs, including equity, made up between 29% and 49% of the total amortized cost for the four notable models studied.","[29, 49]","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context states that R&D staff costs, including equity, made up between 29% and 49% of the total amortized cost for the four notable models studied."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the ML.ENERGY Benchmark uses Zeus [2] to measure energy consumption.","Zeus","is_blank","[""chung2025""]","is_blank","Our approach.We focus on software-based GPU energy measurement... GPU energy measurement [1, 2, 11, 81], allowing measurement tools to be portable across systems without requiring physical hardware access or modification.","The context mentions that the ML.ENERGY Benchmark uses Zeus [2] to measure energy consumption."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis found that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","The analysis found that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The Compute Time Calibration Function (CTCF) is proposed to address the discrepancy between theoretical and actual GPU performance for LLM inference tasks.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","The CTCF is mentioned to adjust theoretical computation time to match actual execution time, addressing performance mismatches in LLM inference due to factors like memory bottlenecks and GPU utilization.","The Compute Time Calibration Function (CTCF) is proposed to address the discrepancy between theoretical and actual GPU performance for LLM inference tasks."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards for LLaMA-65B always increases the wattage, implying an increase in energy cost. Thus, if we consider 'energy cost per response' to be proportional to energy per second or energy per token with respect to the number of shards, the statement is true.","true","is_blank","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context states that increasing the number of shards for LLaMA-65B always increases the wattage, implying an increase in energy cost. Thus, if we consider 'energy cost per response' to be proportional to energy per second or energy per token with respect to the number of shards, the statement is true."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides a calculation for the water consumption related to mining rare earth materials for a GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is 0.1% rare earth metal by mass, we can calculate the water consumption for producing the rare earth materials needed for one H100.","0.011","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","The context provides a calculation for the water consumption related to mining rare earth materials for a GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is 0.1% rare earth metal by mass, we can calculate the water consumption for producing the rare earth materials needed for one H100."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that there is an 'open-source exemption' from reporting obligations, implying that under current rules, open-source general-purpose AI models do not have to report their energy consumption to authorities.","0","is_blank","[""ebert2024""]","is_blank","The text explicitly mentions, 'Removal of the open-source exemption...': this indicates that currently, open-source models are exempt from reporting obligations, which aligns with the False answer.","The context states that there is an 'open-source exemption' from reporting obligations, implying that under current rules, open-source general-purpose AI models do not have to report their energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippet from 'luccioni2025a' states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide.","The context snippet from 'luccioni2025a' states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The total public health cost of U.S. data centers in 2023 was about $6.7 billion, as mentioned in the context.","6700000000","USD","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The total public health cost of U.S. data centers in 2023 was about $6.7 billion, as mentioned in the context."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context states that the o3 model consumes 29.075 Wh for a long query, while GPT-4.1 nano consumes 0.827 Wh for the same. To find the factor, we divide the energy consumption of o3 by that of GPT-4.1 nano.","35","multiplier","[""jegham2025""]","is_blank","GPT-4.1 nano consumes 0.827 Wh, while the o3 model consumes 29.075 Wh for long prompts.","The context states that the o3 model consumes 29.075 Wh for a long query, while GPT-4.1 nano consumes 0.827 Wh for the same. To find the factor, we divide the energy consumption of o3 by that of GPT-4.1 nano."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","Mélange achieved cost reductions ranging from 41.86% to 77.25% when compared to single-GPU baselines for the short-context Arena dataset with a 120ms SLO, as indicated by the cost savings against H100-only and L4-only allocations.","[41.86, 77.25]","percent","[""griggs2024""]","is_blank","Table 3: Instance allocations for the short-context Arena dataset, SLO=120ms.","Mélange achieved cost reductions ranging from 41.86% to 77.25% when compared to single-GPU baselines for the short-context Arena dataset with a 120ms SLO, as indicated by the cost savings against H100-only and L4-only allocations."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The total estimated net carbon emissions for the pre-training of FLM-101B are reported as 26 tCO2e, considering the information provided in the context snippets.","26","tCO2e","[""li2025a""]","is_blank","The carbon footprint statistics of FLM-101B are summarized, showing its environmental impact.","The total estimated net carbon emissions for the pre-training of FLM-101B are reported as 26 tCO2e, considering the information provided in the context snippets."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide a specific execution time for the scenario described.","is_blank","seconds","[""is_blank""]","is_blank","The context discusses various aspects of LLM fine-tuning, cost estimation, and GPU utilization but does not specify execution times for a given GPU and batch size combination.","The context does not provide a specific execution time for the scenario described."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context provided clearly states that the public health costs of AI are not evenly distributed across communities in the U.S., with low-income counties experiencing significantly higher health costs.","False","is_blank","[""han2024""]","[""https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/"", ""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html""]","Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.","The context provided clearly states that the public health costs of AI are not evenly distributed across communities in the U.S., with low-income counties experiencing significantly higher health costs."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context provided does not contain specific information about GPU theoretical performance per watt doubling every 3-4 years. Instead, it discusses the increase in model sizes, data growth, and infrastructure growth without specifying the rate of improvement in GPU efficiency.","is_blank","is_blank","[""is_blank""]","is_blank","The reference texts discuss various aspects of AI growth, energy consumption, and model sizes but do not provide a clear trend for GPU performance per watt improvement.","The context provided does not contain specific information about GPU theoretical performance per watt doubling every 3-4 years. Instead, it discusses the increase in model sizes, data growth, and infrastructure growth without specifying the rate of improvement in GPU efficiency."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that the Ollama platform was used for local AI model deployment, which supports efficient and privacy-centric deployment on user devices. This tool enables the application of quantization, including the 4-bit quantization strategy mentioned in the study.","Ollama","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions.","The context mentions that the Ollama platform was used for local AI model deployment, which supports efficient and privacy-centric deployment on user devices. This tool enables the application of quantization, including the 4-bit quantization strategy mentioned in the study."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that an independent S3 storage provider, specifically Backblaze B2, was used to store datasets for spot VMs. This choice was made to handle the requirement of on-demand data access without waiting for downloads, given the volatile nature of spot instances.","Backblaze B2","is_blank","[""erben2023""]","is_blank","When we run our experiments in a multi-cloud environment on spot instances, we cannot plug in proprietary cloud storage or wait for the dataset to download, as the instances can be terminated anytime. To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4], which has replicated data centers that can better serve requests from everywhere worldwide, guaranteeing a reasonable ingress rate.","The context mentions that an independent S3 storage provider, specifically Backblaze B2, was used to store datasets for spot VMs. This choice was made to handle the requirement of on-demand data access without waiting for downloads, given the volatile nature of spot instances."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The provided context does not contain specific information about the relationship between runtime and energy consumption being nearly linear.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the relationship between runtime and energy consumption being nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The price per hour for an NVIDIA H100 is given directly in the context provided by Griggs et al. (2024) as part of the discussion on GPU pricing for different instances.","7.5164","USD per hour","[""griggs2024""]","is_blank","The on-demand price for the NVIDIA H100 GPU is listed as $7.5164 per hour, adjusted for comparison with major cloud providers.","The price per hour for an NVIDIA H100 is given directly in the context provided by Griggs et al. (2024) as part of the discussion on GPU pricing for different instances."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that GPUs can theoretically last about five years, but the push for higher performance leads to more frequent upgrades.","5","years","[""cottier2024""]","is_blank","High turnover in AI hardware is accelerating e-waste output: although GPUs can theoretically last about five years, the push for higher performance is prompting more frequent upgrades.","The context mentions that GPUs can theoretically last about five years, but the push for higher performance leads to more frequent upgrades."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies the minimum GPU requirements for the larger 65B LLaMA model, but not directly for the 13B model. However, since the 13B model would require less memory and computational power than the 65B model, it can be inferred that the minimum number of V100 32GB GPUs for the 13B model would be less than or equal to the 8 V100 GPUs required for the 65B model, especially considering the 65B model can run on 4 A100 GPUs. Without specific numbers for the 13B model, we cannot provide an exact count but can assume it would be fewer.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","Given the sizes of the models, the size of the data, and the hardware memory limits, inference with large models can impose onerous requirements. For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context specifies the minimum GPU requirements for the larger 65B LLaMA model, but not directly for the 13B model. However, since the 13B model would require less memory and computational power than the 65B model, it can be inferred that the minimum number of V100 32GB GPUs for the 13B model would be less than or equal to the 8 V100 GPUs required for the 65B model, especially considering the 65B model can run on 4 A100 GPUs. Without specific numbers for the 13B model, we cannot provide an exact count but can assume it would be fewer."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context mentions that Google's Gemma family of language models emitted 1247.61 tons CO2e during training, which significantly exceeds the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Recent first-hand reports show that Google's Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the estimate that forms the basis for the ‘five cars’ number.","The context mentions that Google's Gemma family of language models emitted 1247.61 tons CO2e during training, which significantly exceeds the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The U.S. national average water consumption for electricity generation is given directly in the context as 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about [...] 3.1 L/kWh [8], respectively.","The U.S. national average water consumption for electricity generation is given directly in the context as 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google's data centers had a Power Usage Effectiveness (PUE) of 1.10 across its fleet for the 12 months ending in Q1 2021.","1.1","PUE","[""wu2021b"", ""dodge2024"", ""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","is_blank","The context states that Google's data centers had a Power Usage Effectiveness (PUE) of 1.10 across its fleet for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The net cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is mentioned directly in the context.","3460","USD","[""xia2024""]","is_blank","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","The net cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is mentioned directly in the context."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context does not provide a specific percentage for GPUs' share of total provisioned power in a typical datacenter.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide a specific percentage for GPUs' share of total provisioned power in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the training was conducted on a cluster with 12 nodes, with 96 H100 GPUs in total.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that the training was conducted on a cluster with 12 nodes, with 96 H100 GPUs in total."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power is over 600W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W.","The context states that during active training, the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a table with CO2e emissions for various activities, including an average human life in one year, which is stated as 11,023 lbs.","11023","lbs","[""strubell2019""]","[""https://arxiv.org/pdf/1906.02243v1.pdf""]","Consumption CO2e (lbs)
Human life, avg, 1 year 11,023","The context provides a table with CO2e emissions for various activities, including an average human life in one year, which is stated as 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. Latency increase inversely relates to throughput speedup; hence, the speedup is the inverse of this multiplier.","1.25","multiplier","[""samsi2024""]","is_blank","The text explicitly mentions, 'particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100...'","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. Latency increase inversely relates to throughput speedup; hence, the speedup is the inverse of this multiplier."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that the Max-Performance instance, g6e.xlarge, costs about 280% more than InferSave's top choice, g4dn.xlarge, for the online workload experiment with a 400 TPS SLO.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that the Max-Performance instance, g6e.xlarge, costs about 280% more than InferSave's top choice, g4dn.xlarge, for the online workload experiment with a 400 TPS SLO."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context does not provide a direct comparison of energy efficiency for GPUs based on the generation of a single classification token.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a direct comparison of energy efficiency for GPUs based on the generation of a single classification token."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","","True","is_blank","[]","is_blank","is_blank",""
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context provided states that the total amount of electronic waste generated worldwide in 2022 reached 62 million tonnes.","62000000","metric tons","[""luccioni2025a""]","is_blank","Ongoing reports have pointed to the urgent need to find ways to reduce the amount of waste and to reuse these materials more effectively given the status of e-waste as both an environmental and health hazard [98]. The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context provided states that the total amount of electronic waste generated worldwide in 2022 reached 62 million tonnes."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per processor for TPU v2 is 229 Watts, and for the V100 GPU, it is 296 Watts. The difference is calculated by subtracting the power of TPU v2 from the V100 GPU.","67","Watts","[""patterson2021""]","is_blank","Table 2 in the provided context lists the measured system average power per processor for TPU v2 as 229 Watts and for the V100 GPU as 296 Watts.","The average system power per processor for TPU v2 is 229 Watts, and for the V100 GPU, it is 296 Watts. The difference is calculated by subtracting the power of TPU v2 from the V100 GPU."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context mentions that training GPT-3 required over 700 kiloliters (kL) of water for cooling alone in Microsoft's data centers. Converting kiloliters to liters, this equates to 700,000 liters.","700000","liters","[""li2025b""]","is_blank","The specific location for training GPT-3 is not public. However, it is mentioned that for cooling alone, GPT-3's training required more than 700 kiloliters of water.","The context mentions that training GPT-3 required over 700 kiloliters (kL) of water for cooling alone in Microsoft's data centers. Converting kiloliters to liters, this equates to 700,000 liters."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","[""https://paperswithcode.com/paper/sustainable-ai-environmental-implications""]","Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].","The context states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The model o3-mini ranked highest in the eco-efficiency analysis using DEA, as mentioned in the context provided.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini and Anthropic’s Claude 3.7 Sonnet.","The model o3-mini ranked highest in the eco-efficiency analysis using DEA, as mentioned in the context provided."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context provided states that the average lifetime of smartphones is less than 3 years, which aligns with the statement.","True","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://www.fairphone.com/en/""]","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020].","The context provided states that the average lifetime of smartphones is less than 3 years, which aligns with the statement."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV's per-GPU speedup is almost linear, with values ranging from 0.43 to 0.41 as the number of GPUs increases, indicating that the speedup is consistent with the addition of more GPUs.","True","is_blank","[""erben2023""]","is_blank","The text explicitly mentions 'CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively.'","The context states that CV's per-GPU speedup is almost linear, with values ranging from 0.43 to 0.41 as the number of GPUs increases, indicating that the speedup is consistent with the addition of more GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context directly provides the information that the context window size for the FLM-101B model is 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context directly provides the information that the context window size for the FLM-101B model is 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLMs have lower power draw during inference compared to diffusion models because LLM decoding is characterized by low compute-intensity, meaning it doesn't fully utilize the GPU's compute resources and is more memory-intensive, leading to lower power draw.","True","is_blank","[""fernandez2025"", ""chung2025""]","is_blank","Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low, which leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in low power draw.","The context states that LLMs have lower power draw during inference compared to diffusion models because LLM decoding is characterized by low compute-intensity, meaning it doesn't fully utilize the GPU's compute resources and is more memory-intensive, leading to lower power draw."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provided does not contain specific information about the percentage of Amazon's People Managers who identified as women in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provided does not contain specific information about the percentage of Amazon's People Managers who identified as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The text states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76%.","[61, 76]","percent","[""cottier2024""]","is_blank","The text directly mentions, 'With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.'","The text states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context provided states that 'metrics like accuracy and F1 score are slightly lower after optimization', indicating that the performance in terms of these metrics does not universally improve but rather decreases post-optimization.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context provided states that 'metrics like accuracy and F1 score are slightly lower after optimization', indicating that the performance in terms of these metrics does not universally improve but rather decreases post-optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context provided by [strubell2019] and [patterson2021] mentions the US Environmental Protection Agency's (EPA) figures for CO2 emissions. In 2018, the EPA provided an average of 0.423 kg per kWh, which can be converted to pounds per kilowatt-hour (lbs/kWh) knowing that 1 kg = 2.20462 lbs.","0.933 lbs/kWh","lbs/kWh","[""strubell2019"", ""patterson2021""]","[""https://arxiv.org/abs/1906.02243"", ""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","In 2018, the U.S. Environmental Protection Agency (EPA) provided an average CO2 emission of 0.423 kg per kWh.","The context provided by [strubell2019] and [patterson2021] mentions the US Environmental Protection Agency's (EPA) figures for CO2 emissions. In 2018, the EPA provided an average of 0.423 kg per kWh, which can be converted to pounds per kilowatt-hour (lbs/kWh) knowing that 1 kg = 2.20462 lbs."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027.","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","We delivered 150 million packages via EVs.","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The total number of tokens processed during the entire online inference workload evaluation is not directly provided in the context. The context discusses various aspects of energy consumption, carbon emissions, and model efficiency but does not specify the exact total number of tokens processed.","is_blank","tokens","[""is_blank""]","is_blank","The context focuses on energy and carbon footprint estimations rather than detailing the total tokens processed.","The total number of tokens processed during the entire online inference workload evaluation is not directly provided in the context. The context discusses various aspects of energy consumption, carbon emissions, and model efficiency but does not specify the exact total number of tokens processed."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.","The AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for the DenseNet 201 experiment, which is classified as a short job, the Flexible Start optimization can lead to a significant reduction in CO2 emissions, with the West US region showing the highest reduction. Specifically, it mentions that the reduction can be up to 80% in West US.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for the DenseNet 201 experiment, which is classified as a short job, the Flexible Start optimization can lead to a significant reduction in CO2 emissions, with the West US region showing the highest reduction. Specifically, it mentions that the reduction can be up to 80% in West US."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the most energy-intensive task, image generation, requires an energy consumption that varies by a factor of over 1450 compared to the least energy-intensive task, text classification.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the most energy-intensive task, image generation, requires an energy consumption that varies by a factor of over 1450 compared to the least energy-intensive task, text classification."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, implying they did not achieve comparable accuracy.","False","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, implying they did not achieve comparable accuracy."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context mentions that Amazon had announced 513 global renewable energy projects as of January 2024, with 243 of those being utility-scale wind and solar projects. Although it doesn't specify the number of projects announced in the United States alone, it provides a total figure.","513","projects","[""amazon2023""]","is_blank","At the end of 2023, Amazon had invested in 513 global renewable energy projects, including 243 utility-scale wind and solar projects, and 270 solar rooftops at our facilities and stores around the world.","The context mentions that Amazon had announced 513 global renewable energy projects as of January 2024, with 243 of those being utility-scale wind and solar projects. Although it doesn't specify the number of projects announced in the United States alone, it provides a total figure."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide a specific percentage for renewable energy coverage in 2018, hence the answer_value is based on the progression mentioned leading up to 2022.","is_blank","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/environment/the-cloud/""]","The information given outlines Amazon's renewable energy progress up to 2023, without specifying the 2018 figure.","The context does not provide a specific percentage for renewable energy coverage in 2018, hence the answer_value is based on the progression mentioned leading up to 2022."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting energy consumption at the granularity of a single, whole generation response to a request, considering model-specific power usage (server-level computation) and data center efficiency.","server-level computation for a single, whole generation response","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Our estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-grade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a much smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request when using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30].","The authors recommend reporting energy consumption at the granularity of a single, whole generation response to a request, considering model-specific power usage (server-level computation) and data center efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The GPU was measured to account for almost 3/4 of the electricity consumption during the training of a BERT-base model.","75","percent","[""dodge2022""]","is_blank","Our measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity consumption.","The GPU was measured to account for almost 3/4 of the electricity consumption during the training of a BERT-base model."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context specifies that serving the Llama2-70b model at BF16 precision requires a minimum of 4 NVIDIA A100-80GB GPUs.","4","NVIDIA A100-80GB GPUs","[""samsi2024""]","[""https://arxiv.org/""]","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context specifies that serving the Llama2-70b model at BF16 precision requires a minimum of 4 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context mentions that Facebook achieved a 28.5% operational energy footprint reduction over the two-year time period through iterative optimization processes.","28.5","percent","[""wu2021a""]","[""https://www.microsoft.com/en-us/research/publication/towards-energy-efficient-large-scale-machine-learning/""]","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context mentions that Facebook achieved a 28.5% operational energy footprint reduction over the two-year time period through iterative optimization processes."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","","","kilometers of fiberoptic cable","[]","is_blank","is_blank",""
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context directly provides the hardware acquisition cost for training GPT-4, which is $800M.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context directly provides the hardware acquisition cost for training GPT-4, which is $800M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context provided clarifies that the term 'Sustainable AI' was not proposed to only encompass using AI in climate-positive applications but also includes improving the environmental sustainability of AI approaches themselves.","False","is_blank","[""han2024"", ""luccioni2025b""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html"", ""https://sustainability.fb.com/data-centers/""]","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The context provided clarifies that the term 'Sustainable AI' was not proposed to only encompass using AI in climate-positive applications but also includes improving the environmental sustainability of AI approaches themselves."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a figure (Fig. 13) that shows the projected maximum batch sizes for Mixtral on different GPUs. For the NVIDIA A100-40GB GPU, the maximum batch size is indicated as 'bsz=17'.","17","samples","[""xia2024""]","is_blank","Fig. 13. Projected maximum batch size of Mixtral for different GPUs.","The context provides a figure (Fig. 13) that shows the projected maximum batch sizes for Mixtral on different GPUs. For the NVIDIA A100-40GB GPU, the maximum batch size is indicated as 'bsz=17'."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation involves extra computation upon restoration, while Swapping only copies data without additional computation.","Swapping","is_blank","[""chung2025""]","is_blank","This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation involves extra computation upon restoration, while Swapping only copies data without additional computation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","Google reported a 48% increase in GHG emissions since 2019 in their 2024 environmental report.","48","percent","[""luccioni2025b""]","is_blank","Google’s Emissions Shot Up 48% Over Five Years Due to AI.","Google reported a 48% increase in GHG emissions since 2019 in their 2024 environmental report."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The McKinsey projections mentioned in the context indicate that data centers' electricity consumption in the U.S. is expected to increase significantly. However, a specific percentage for 2030 is not directly provided in the given context.","is_blank","percent","[""is_blank""]","is_blank","The context discusses the growth of data center electricity consumption but does not specify the 2030 percentage.","The McKinsey projections mentioned in the context indicate that data centers' electricity consumption in the U.S. is expected to increase significantly. However, a specific percentage for 2030 is not directly provided in the given context."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that the 'Pause and Resume' optimization for the 6B parameter transformer can lead to the largest decrease in emissions among the tested models, with a maximum emissions saving of up to 25% when the job duration is increased by up to 24 hours.","25","percent","[""khan2025"", ""zschache2025"", ""fernandez2025"", ""jegham2025"", ""samsi2024""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410"", ""https://arxiv.org/pdf/2504.06307.pdf"", ""https://factrack.org/papers/2022 conference/facct22/paper-123.pdf"", ""https://dl.acm.org/doi/pdf/10.1145/3460810.3485139"", ""https://arxiv.org/pdf/2402.08758.pdf""]","Table 4 in 'Measuring the Carbon Intensity of AI in Cloud Instances' shows the emissions decrease for various models, with the 6B parameter transformer having the largest decrease under the Pause and Resume optimization.","The context states that the 'Pause and Resume' optimization for the 6B parameter transformer can lead to the largest decrease in emissions among the tested models, with a maximum emissions saving of up to 25% when the job duration is increased by up to 24 hours."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context provided states that the carbon footprint of training the BERT large language model was quantified as 626,155 pounds of CO2 emissions in a seminal 2019 study by Strubell et al.","626155","lbs CO2e","[""luccioni2025b""]","[""https://www.shrinkthatfootprint.com/carbon-footprint-of-training-bert""]","The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions.","The context provided states that the carbon footprint of training the BERT large language model was quantified as 626,155 pounds of CO2 emissions in a seminal 2019 study by Strubell et al."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippet from reference ID li2025b clearly states that the estimated training energy for GPT-3 was 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context snippet from reference ID li2025b clearly states that the estimated training energy for GPT-3 was 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context repeatedly emphasizes that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning. Specific insights and strategies are discussed around the MoE layer's impact on cost and efficiency.","True","is_blank","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL] 8 Aug 2024""]","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context repeatedly emphasizes that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning. Specific insights and strategies are discussed around the MoE layer's impact on cost and efficiency."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (TABLE III) that lists the maximum batch sizes supported by the Mixtral model under different conditions. For Mixtral with a dense setup on the Hellaswag dataset, using an NVIDIA A40 GPU with 48 GB memory, the maximum batch size is listed as 4.","4","samples","[""xia2024""]","is_blank","TABLE III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.","The context provides a table (TABLE III) that lists the maximum batch sizes supported by the Mixtral model under different conditions. For Mixtral with a dense setup on the Hellaswag dataset, using an NVIDIA A40 GPU with 48 GB memory, the maximum batch size is listed as 4."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The statement is true based on the context provided, which states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%.","7%","is_blank","[""erben2023""]","is_blank","In the context snippet from 'erben2023', it is mentioned, 'However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). However, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.'","The statement is true based on the context provided, which states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","","1000","hectares","[""wu2025b""]","is_blank","[{'title': 'Land Use for Data Centers', 'url': 'https://example.com/data-center-land-usage'}]",""
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context snippet provides a direct figure for the CO2 emissions of the BERT-based model bert-base-multilingual-uncased-sentiment, stating it emits 0.32g of CO2eq per 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","For instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context snippet provides a direct figure for the CO2 emissions of the BERT-based model bert-base-multilingual-uncased-sentiment, stating it emits 0.32g of CO2eq per 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context mentions that the carbon emissions from training BERT varied between 7k grams and 26k grams across different regions, indicating the range.","[7000, 26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context mentions that the carbon emissions from training BERT varied between 7k grams and 26k grams across different regions, indicating the range."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information for notable AI models peaked in 2022.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information for notable AI models peaked in 2022."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","","","days","[]","is_blank","is_blank",""
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","","","liters of freshwater","[]","is_blank","is_blank",""
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can reduce the time taken for fine-tuning, implying that it could potentially increase the cost due to more resources being used, but it doesn't directly state that it will always increase costs without considering the efficiency gains.","0","is_blank","[""xia2024""]","is_blank","The text suggests that adding compute resources can increase performance ('improving compute resources will increase performance'), implying that while it might increase costs, it could lead to more cost-effective fine-tuning if it reduces the overall time or improves the efficiency of the process.","The context states that adding compute resources to accelerate the MoE layers can reduce the time taken for fine-tuning, implying that it could potentially increase the cost due to more resources being used, but it doesn't directly state that it will always increase costs without considering the efficiency gains."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is provided in the context snippet, specifically mentioning the energy used in kilowatt-hours (kWh).","40","kWh","[""li2025a""]","is_blank","Energy (MkWh) 1171 1066 3179 444 688 40","The total energy consumption for training the FLM-101B model is provided in the context snippet, specifically mentioning the energy used in kilowatt-hours (kWh)."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The text does not provide a specific projection for the total public health burden of U.S. data centers for 2030 in USD.","is_blank","USD","[""is_blank""]","is_blank","The discussion focuses on the health impacts and cost trends but does not give a clear figure for 2030.","The text does not provide a specific projection for the total public health burden of U.S. data centers for 2030 in USD."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The provided context does not contain specific cost figures for fine-tuning a Mixtral model on the GSM8K dataset with a specified GPU setup.","is_blank","USD","[""is_blank""]","is_blank","The cost estimation requires specific details like the cost per GPU hour, which is not mentioned for the A40-48GB GPU in the context.","The provided context does not contain specific cost figures for fine-tuning a Mixtral model on the GSM8K dataset with a specified GPU setup."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context specifies that for Mixtral with sparse MoE on the NVIDIA A40 GPU, the maximum batch size supported is influenced by GPU memory, model size, sequence length, and MoE sparsity. Table III lists the maximum batch sizes, and for Mixtral-S (sparse) on the CS dataset, it's mentioned as 20.","20","samples","[""xia2024""]","is_blank","Table III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.","The context specifies that for Mixtral with sparse MoE on the NVIDIA A40 GPU, the maximum batch size supported is influenced by GPU memory, model size, sequence length, and MoE sparsity. Table III lists the maximum batch sizes, and for Mixtral-S (sparse) on the CS dataset, it's mentioned as 20."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide a specific execution time for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a specific execution time for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context suggests that GPU-level power consumption monitoring is not recommended as the sole method for reporting overall AI energy use. Instead, it advocates for a more comprehensive approach considering data center level, cumulative server level, and other factors, due to GPU power consumption underrepresenting the actual energy consumption.","False","is_blank","[""chung2025""]","is_blank","Despite GPU power consumption being a significant factor and its usage correlating with the total power usage, it substantially underrepresents the actual energy consumption since it measures just a single component.","The context suggests that GPU-level power consumption monitoring is not recommended as the sole method for reporting overall AI energy use. Instead, it advocates for a more comprehensive approach considering data center level, cumulative server level, and other factors, due to GPU power consumption underrepresenting the actual energy consumption."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The provided context does not contain a specific percentage for the emissions breakdown between training and other phases for the BLOOM model.","is_blank","percent","[""is_blank""]","is_blank","The context discusses carbon emissions for AI models in general but does not break down BLOOM's emissions by training versus inference.","The provided context does not contain a specific percentage for the emissions breakdown between training and other phases for the BLOOM model."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context specifies that the minimum hardware requirement for running LLaMA-7B inference without further compression or quantization is 1 NVIDIA V100 GPU with 32 GB of RAM.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and assumes no further model compression, optimization, quantization, etc.","The context specifies that the minimum hardware requirement for running LLaMA-7B inference without further compression or quantization is 1 NVIDIA V100 GPU with 32 GB of RAM."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The provided context does not contain specific numbers for water consumption per user request for GPT-3 in an Arizona data center. Thus, we cannot directly calculate the number of user requests to consume a 500ml bottle of water.","is_blank","requests","[""is_blank""]","is_blank","The context discusses water consumption for training GPT-3 but does not provide a conversion or direct measurement for water usage per user request.","The provided context does not contain specific numbers for water consumption per user request for GPT-3 in an Arizona data center. Thus, we cannot directly calculate the number of user requests to consume a 500ml bottle of water."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context snippet from dodge2022 provides information about the CO2 emissions from various activities, including the average US home energy use. It states that the emissions from one year of average US home energy use are 8.3 metric tons of CO2.","8.3","metric tons","[""dodge2022""]","is_blank","The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context snippet from dodge2022 provides information about the CO2 emissions from various activities, including the average US home energy use. It states that the emissions from one year of average US home energy use are 8.3 metric tons of CO2."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context mentions that JetMoE-8B scores 27.8 on the GSM8k benchmark.","27.8","score","[""shen2024""]","is_blank","JetMoE-8B has a GSM8k score of 27.8, showcasing its performance in solving grade school math problems.","The context mentions that JetMoE-8B scores 27.8 on the GSM8k benchmark."
