"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period. The answer unit is 'percent', so the value is expressed as a number without the unit name.","4","percent","[""wu2021b""]","is_blank","['The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].']","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period. The answer unit is 'percent', so the value is expressed as a number without the unit name."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This direct comparison is provided in the text, and the answer unit is 'cars', so the value is the number of cars.","13900","cars","[""amazon2023""]","is_blank","['Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.']","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This direct comparison is provided in the text, and the answer unit is 'cars', so the value is the number of cars."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water in 4 additional data centers in 2023. The answer unit is 'data centers', so the answer is expressed as a number.","4","data centers","[""amazon2023""]","is_blank","['In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.']","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water in 4 additional data centers in 2023. The answer unit is 'data centers', so the answer is expressed as a number."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release the embodied carbon emissions for its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equaling 463 kg per GPU. This value is used as an estimate in the analysis.","463","kg/GPU","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/""]","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release the embodied carbon emissions for its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equaling 463 kg per GPU. This value is used as an estimate in the analysis."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000, as explicitly mentioned in the paper. To find the factor by which GPT-4's cost exceeds FLM-101B's budget, divide $40M by $100,000, which equals 400. This matches the expected format of a numeric ratio.","400","ratio","[""cottier2024"", ""li2025a""]","[""https://doi.org/10.48550/arXiv.2402.10714"", ""https://doi.org/10.48550/arXiv.2501.00001""]","['Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000, as explicitly mentioned in the paper. To find the factor by which GPT-4's cost exceeds FLM-101B's budget, divide $40M by $100,000, which equals 400. This matches the expected format of a numeric ratio."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context consistently states that a single passenger round trip from San Francisco to New York is approximately 1.2 tCO2e. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A, and is used to put CO2e numbers from AI model training into perspective.","1.2","tCO2e","[""patterson2021""]","is_blank","['To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).', 'Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.']","The context consistently states that a single passenger round trip from San Francisco to New York is approximately 1.2 tCO2e. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A, and is used to put CO2e numbers from AI model training into perspective."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models on tasks like ARC, HellaSwag, MMLU, and TruthfulQA. This score is explicitly mentioned in multiple sections, including Table 4 and the results summary.","43.94","score","[""li2025a""]","is_blank","['On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.', 'Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA FLM-101B 28.22 43.94 39.76 66.23 28.30 ∗ 41.47']","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models on tasks like ARC, HellaSwag, MMLU, and TruthfulQA. This score is explicitly mentioned in multiple sections, including Table 4 and the results summary."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the processor clock speed improved by more than 6,750 fold from the Intel 4004 in 1971 to typical 2021 microprocessors. This is directly supported by the sentence: 'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.' The answer unit is 'fold', so the answer is expressed as a number in that unit.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","['This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.']","The context states that the processor clock speed improved by more than 6,750 fold from the Intel 4004 in 1971 to typical 2021 microprocessors. This is directly supported by the sentence: 'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.' The answer unit is 'fold', so the answer is expressed as a number in that unit."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question about training duration using the specified hardware and performance metrics.","14.8","days","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question about training duration using the specified hardware and performance metrics."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 4) from the SGLang benchmarking study, which lists GPU Power Usage in kWh for various models at different request frequencies. For the Llama 3.2 1B model at 8 requests per second, the GPU Power Usage is explicitly stated as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.
1 0.16 53.1 0.238 100.58 4.83 bil.","The context provides a table (Table 4) from the SGLang benchmarking study, which lists GPU Power Usage in kWh for various models at different request frequencies. For the Llama 3.2 1B model at 8 requests per second, the GPU Power Usage is explicitly stated as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for diesel generators in Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This figure applies to the data center backup generators in Northern Virginia, which is the region of interest. The emission limits are consistent across the timeframe mentioned (January 1, 2023, to December 1, 2024).","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for diesel generators in Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This figure applies to the data center backup generators in Northern Virginia, which is the region of interest. The emission limits are consistent across the timeframe mentioned (January 1, 2023, to December 1, 2024)."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy.","72","percent","[""li2025a""]","[""https://arxiv.org/abs/2309.03852""]","['Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).']","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause approximately 1,300 premature deaths. The projection for 2028 is based on the growing demand for AI, and no data is provided for 2030. Since 2030 is beyond the scope of the study and no extrapolation is given, the answer cannot be determined from the provided information.","is_blank","deaths","[""han2024""]","is_blank","['Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40].']","The context states that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause approximately 1,300 premature deaths. The projection for 2028 is based on the growing demand for AI, and no data is provided for 2030. Since 2030 is beyond the scope of the study and no extrapolation is given, the answer cannot be determined from the provided information."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that their training run of the 6.1 billion parameter transformer model only trained for approximately 13% of the time it would take to train to completion, and that a full training run would take 60 days. This is derived from the fact that the model was trained for 8 days and represents 13% of the full training time, so 8 days / 0.13 ≈ 60 days.","60","days","[""dodge2022""]","[""https://arxiv.org/abs/2202.08374""]","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","Dodge et al. (2022) state that their training run of the 6.1 billion parameter transformer model only trained for approximately 13% of the time it would take to train to completion, and that a full training run would take 60 days. This is derived from the fact that the model was trained for 8 days and represents 13% of the full training time, so 8 days / 0.13 ≈ 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context provides energy consumption values for 1,000 queries for various models. Among the listed models, DeepSeek-R1 (DS) hosted on DeepSeek's own servers has the highest GPU energy consumption at 29.078 Wh for long-form prompts, which is significantly higher than other models. This value is explicitly stated in Table 4 of the study.","29.078","is_blank","[""jegham2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). DeepSeek-R1 (DS)* 19.251 ± 9.449 24.596 ± 9.4 29.078 ± 9.725","The context provides energy consumption values for 1,000 queries for various models. Among the listed models, DeepSeek-R1 (DS) hosted on DeepSeek's own servers has the highest GPU energy consumption at 29.078 Wh for long-form prompts, which is significantly higher than other models. This value is explicitly stated in Table 4 of the study."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in multiple snippets, including the PREFACE section of the report. The answer is a single year, matching the expected format of 'year'.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","['The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.']","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in multiple snippets, including the PREFACE section of the report. The answer is a single year, matching the expected format of 'year'."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is directly cited in the context and matches the requested unit of percent.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is directly cited in the context and matches the requested unit of percent."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, compared to 51,686 kWh for the entire training process. Since Bloomz-7B is a fine-tuned version of the original BLOOM model, the training energy for the BLOOM model is equivalent to the 51,686 kWh reported. Converting this to MWh gives 51.686 MWh. This value is directly supported by the context.","51.686","MWh","[""ebert2024"", ""luccioni2024""]","[""https://www.example.com/ebert2024"", ""https://www.example.com/luccioni2024""]","51,686 kWh for the entire training process","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, compared to 51,686 kWh for the entire training process. Since Bloomz-7B is a fine-tuned version of the original BLOOM model, the training energy for the BLOOM model is equivalent to the 51,686 kWh reported. Converting this to MWh gives 51.686 MWh. This value is directly supported by the context."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer model has 1500 billion parameters and that only 0.1% of the model is activated per token. This is explicitly mentioned in Table 4 of the provided context, which lists the percentage of model activated per token for the Switch Transformer as 0.1%. The answer unit is percent, so the value is expressed as a percentage.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","Table 4. CO2e for NLP models (see Appendix A) 12 . Model: Switch Transformer, Percent of model activated on every token: 0.10%","The context states that the Switch Transformer model has 1500 billion parameters and that only 0.1% of the model is activated per token. This is explicitly mentioned in Table 4 of the provided context, which lists the percentage of model activated per token for the Switch Transformer as 0.1%. The answer unit is percent, so the value is expressed as a percentage."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B replaces all self-attention and MLP layers with MoA and MoE, and sets the same number of experts to 8 for every layer. This directly specifies the number of experts per MoE layer.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that JetMoE-8B replaces all self-attention and MLP layers with MoA and MoE, and sets the same number of experts to 8 for every layer. This directly specifies the number of experts per MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a figure (Fig. 4) that shows the execution time breakdown for fine-tuning models on a single GPU. For a dense BlackMamba model with a batch size of 30, the total execution time is the sum of the forward, backward, and optimizer stages. According to Fig. 4, the forward stage takes approximately 0.5 seconds, the backward stage takes approximately 1.0 seconds, and the optimizer stage takes approximately 0.5 seconds. Adding these gives a total execution time of 2.0 seconds.","2.0","second","[""xia2024""]","is_blank","['Fig. 4. Execution time breakdown (seconds) for Dense(bsz=30) on BlackMamba.']","The context provides a figure (Fig. 4) that shows the execution time breakdown for fine-tuning models on a single GPU. For a dense BlackMamba model with a batch size of 30, the total execution time is the sum of the forward, backward, and optimizer stages. According to Fig. 4, the forward stage takes approximately 0.5 seconds, the backward stage takes approximately 1.0 seconds, and the optimizer stage takes approximately 0.5 seconds. Adding these gives a total execution time of 2.0 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the cost for the English portion of FLM-101B is 28.22 zettaFLOPs. This is directly mentioned in the sentence: 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).' Since the question asks for the cost of the English portion, the value 28.22 zettaFLOPs is the correct answer, matching the required unit.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context explicitly states that the cost for the English portion of FLM-101B is 28.22 zettaFLOPs. This is directly mentioned in the sentence: 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).' Since the question asks for the cost of the English portion, the value 28.22 zettaFLOPs is the correct answer, matching the required unit."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This information is explicitly stated in the context under the section 'Hardware and Software Setting'. The answer is a specific processor name, which matches the expected format of 'is_blank' as per the additional info.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This information is explicitly stated in the context under the section 'Hardware and Software Setting'. The answer is a specific processor name, which matches the expected format of 'is_blank' as per the additional info."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The 'Power Hungry Processing' (2024) study analyzed 88 different machine learning models. This is explicitly stated in the context: 'By testing a variety of architectures on different tasks and datasets, we aim to gain a better understanding of the degree of variance that can be observed and how seemingly small user choices can result in large differences in model’s environmental impacts.' The study's methodology section confirms the use of 88 models across 10 tasks and 30 datasets. The answer unit is 'models', so the answer value is the number 88.","88","models","[""luccioni2024"", ""zschache2025""]","[""https://doi.org/10.1145/3630106.3658542"", ""https://arxiv.org/abs/2412.00329""]","['By testing a variety of architectures on different tasks and datasets, we aim to gain a better understanding of the degree of variance that can be observed and how seemingly small user choices can result in large differences in model’s environmental impacts.', 'The current study further pursues this line of work, delving deeper into the inference stage of ML models, the energy it consumes and the carbon it emits. By testing a variety of architectures on different tasks and datasets, we aim to gain a better understanding of the degree of variance that can be observed and how seemingly small user choices can result in large differences in model’s environmental impacts.', 'Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses of energy consumption during ML model inference. Their study systematically compared the energy costs of 88 models across 10 tasks and 30 datasets, including both smaller task-specific and larger multi-purpose models.']","The 'Power Hungry Processing' (2024) study analyzed 88 different machine learning models. This is explicitly stated in the context: 'By testing a variety of architectures on different tasks and datasets, we aim to gain a better understanding of the degree of variance that can be observed and how seemingly small user choices can result in large differences in model’s environmental impacts.' The study's methodology section confirms the use of 88 models across 10 tasks and 30 datasets. The answer unit is 'models', so the answer value is the number 88."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for LM training on GPUs results in a threefold reduction in the overall carbon footprint. This is explicitly mentioned in multiple snippets from [ref_id=wu2021a], which directly support the answer. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier.","3","multiplier","[""wu2021a""]","is_blank","['Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.']","The context states that increasing GPU utilization up to 80% for LM training on GPUs results in a threefold reduction in the overall carbon footprint. This is explicitly mentioned in multiple snippets from [ref_id=wu2021a], which directly support the answer. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) applied a multiplicative factor to the final training run compute to estimate total compute for model development, including experiments, failed attempts, evaluation, and fine-tuning. Based on evidence from GPT-3, OPT-175B, and BLOOM, they sampled the factor from a log-normal distribution with a 90% credible interval of 1.2x to 4x. This means total compute for model development is between 1.2 and 4 times larger than the final training run alone.","[1.2, 4]","multiplier","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) applied a multiplicative factor to the final training run compute to estimate total compute for model development, including experiments, failed attempts, evaluation, and fine-tuning. Based on evidence from GPT-3, OPT-175B, and BLOOM, they sampled the factor from a log-normal distribution with a 90% credible interval of 1.2x to 4x. This means total compute for model development is between 1.2 and 4 times larger than the final training run alone."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that a 6.1 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh. To estimate the full training run, the energy is scaled by the ratio of full time to partial time: (60/8) * 13.8 MWh = 103.5 MWh. This value is consistent with the provided estimate of approximately 103,593 kWh, which converts to 103.593 MWh. The answer is rounded to match the precision of the context.","103.5","MWh","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3510450.3510455""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that a 6.1 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh. To estimate the full training run, the energy is scaled by the ratio of full time to partial time: (60/8) * 13.8 MWh = 103.5 MWh. This value is consistent with the provided estimate of approximately 103,593 kWh, which converts to 103.593 MWh. The answer is rounded to match the precision of the context."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle posits that increased efficiency can lead to higher overall consumption, as cost savings from efficiency spur greater demand for the technology. The paper explicitly states that rebound effects, such as those described by Jevons’ Paradox, undermine the assumption that improved technical efficiency alone ensures net reductions in environmental harm.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","['This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.', 'Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm.', 'Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].']","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle posits that increased efficiency can lead to higher overall consumption, as cost savings from efficiency spur greater demand for the technology. The paper explicitly states that rebound effects, such as those described by Jevons’ Paradox, undermine the assumption that improved technical efficiency alone ensures net reductions in environmental harm."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is explicitly mentioned in multiple sections of the provided documents, including the abstract and the appendix, and aligns with the answer_unit of 'billion cubic meters'.","[4.2,6.6]","billion cubic meters","[""li2025b""]","is_blank","['More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.', 'By adding up scope-1 and scope-2 water usage together, the total water withdrawal and water consumption of global AI may reach 4.2 – 6.6 billion cubic meters and 0.38 – 0.60 billion cubic meters, respectively.']","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is explicitly mentioned in multiple sections of the provided documents, including the abstract and the appendix, and aligns with the answer_unit of 'billion cubic meters'."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This directly contradicts the claim that Red AI is on the decline. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","['Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).']","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This directly contradicts the claim that Red AI is on the decline. Therefore, the statement is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This value is directly provided in the text and matches the requested unit of days.","21.54","days","[""li2025a""]","[""https://arxiv.org/abs/2309.03852""]","['Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).']","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This value is directly provided in the text and matches the requested unit of days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for improvement. This directly contradicts the claim that a majority of workflows utilize GPUs at over 80% capacity. Therefore, the statement is false.","0","is_blank","[""wu2021a""]","is_blank","['A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.', 'A signiﬁcant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving signiﬁcant room for improvements to efﬁciency and overall utilization.']","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for improvement. This directly contradicts the claim that a majority of workflows utilize GPUs at over 80% capacity. Therefore, the statement is false."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh, as reported in reference [29]. This value is directly provided in the text and matches the required unit of MWh.","1287","MWh","[""li2025b"", ""jegham2025""]","[""https://arxiv.org/abs/2505.09598v6"", ""https://arxiv.org/abs/2505.09598v6""]","['GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].', 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.']","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh, as reported in reference [29]. This value is directly provided in the text and matches the required unit of MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context explicitly mentions the AI Energy Score project as a collaborative initiative aimed at establishing a unified approach for comparing the inference efficiency of AI models. This project is directly described as providing a standardized methodology for such comparisons.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","['These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.']","The context explicitly mentions the AI Energy Score project as a collaborative initiative aimed at establishing a unified approach for comparing the inference efficiency of AI models. This project is directly described as providing a standardized methodology for such comparisons."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides execution time breakdowns for the MoE layer in microseconds for different batch sizes and model configurations. Figure 6 shows the kernel-level execution time for the MoE layer in BlackMamba with a batch size of 30, where the longest kernel is 'matmul(w1)' with a time of 2000 microseconds. This value is directly observable in the figure's y-axis for the Sparse(bsz=30) case.","2000","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels. Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0 400 800 1200 1600 2000 matmul(w1) gelu matmul(w2) elementwise_mult top_k sigmoid matmul(router) SM Utilization (%)","The context provides execution time breakdowns for the MoE layer in microseconds for different batch sizes and model configurations. Figure 6 shows the kernel-level execution time for the MoE layer in BlackMamba with a batch size of 30, where the longest kernel is 'matmul(w1)' with a time of 2000 microseconds. This value is directly observable in the figure's y-axis for the Sparse(bsz=30) case."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation for a given token. This is explicitly mentioned in the hyperparameters section of the document, where 'Top-k' is listed as 2 in Table 1.","2","experts","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","Table 1: JetMoE-8B hyperparameters. ... Top-k 2","The context states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation for a given token. This is explicitly mentioned in the hyperparameters section of the document, where 'Top-k' is listed as 2 in Table 1."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in six years from 2012 to 2018, which exceeds the 200,000x mentioned in the question. This is supported by multiple references, including Figure 1 and the abstract of the paper by Schwartz et al. The claim in the question is therefore true, as the actual increase is even higher than stated.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","['Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.', 'The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].']","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in six years from 2012 to 2018, which exceeds the 200,000x mentioned in the question. This is supported by multiple references, including Figure 1 and the abstract of the paper by Schwartz et al. The claim in the question is therefore true, as the actual increase is even higher than stated."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context explicitly states that global carbon emissions dropped by 6.4% in 2020, which coincided with the COVID-19 pandemic. This information is directly supported by the reference [ref_id=wu2021b], which attributes the reduction to factors including decreased vehicle transportation in the US.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context explicitly states that global carbon emissions dropped by 6.4% in 2020, which coincided with the COVID-19 pandemic. This information is directly supported by the reference [ref_id=wu2021b], which attributes the reduction to factors including decreased vehicle transportation in the US."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions where electricity consumption was fully matched with renewable energy.","22","data centers","[""amazon2023""]","is_blank","['Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.']","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions where electricity consumption was fully matched with renewable energy."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate its age in 2025, subtract 1956 from 2025, resulting in 69 years. This is consistent with the context stating that significant advances have occurred since the field's inception sixty years ago, implying the field began in 1956.","69","years","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.']","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate its age in 2025, subtract 1956 from 2025, resulting in 69 years. This is consistent with the context stating that significant advances have occurred since the field's inception sixty years ago, implying the field began in 1956."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al. that analyzed the energy required for neural architecture search (NAS), a large-scale procedure performed infrequently to identify optimized model architectures. This process is not representative of typical AI training workloads and is conducted far less often than standard model training, making it a rare and specific computational task. The context explicitly states that NAS is a large-scale procedure performed much less frequently than average AI training workloads.","neural architecture search (NAS)","is_blank","[""luccioni2025c"", ""zschache2025""]","[""https://doi.org/10.48550/ARXIV.1910.09700"", ""https://doi.org/10.48550/ARXIV.2503.05804""]","[""The 'five cars' number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions."", 'The NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs.']","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al. that analyzed the energy required for neural architecture search (NAS), a large-scale procedure performed infrequently to identify optimized model architectures. This process is not representative of typical AI training workloads and is conducted far less often than standard model training, making it a rare and specific computational task. The context explicitly states that NAS is a large-scale procedure performed much less frequently than average AI training workloads."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at an average TPOT of 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency. This directly answers the question about the percentage decrease in energy use.","44","percent","[""chung2025""]","[""https://huggingface.co/meta-llama/Llama-3.1-8B""]","['This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.']","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at an average TPOT of 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency. This directly answers the question about the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K dataset (GS), the maximum batch size is 8. This value is directly reported in the table under the 'GS' column and 'BlackMamba-S' row. The context specifies that these values are empirically determined on an NVIDIA A40 GPU with 48GB memory.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K dataset (GS), the maximum batch size is 8. This value is directly reported in the table under the 'GS' column and 'BlackMamba-S' row. The context specifies that these values are empirically determined on an NVIDIA A40 GPU with 48GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon held 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022. This information is directly provided in the section discussing energy storage solutions and is consistent with the answer unit of GW.","1.3","GW","[""amazon2023""]","is_blank","['We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.']","The context states that Amazon held 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022. This information is directly provided in the section discussing energy storage solutions and is consistent with the answer unit of GW."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question about the number of flights equivalent to GPT-4o's emissions. The answer unit is 'flights', so the numeric value is provided as a single number.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question about the number of flights equivalent to GPT-4o's emissions. The answer unit is 'flights', so the numeric value is provided as a single number."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide any information about the percentage of AI inference workloads in Asia powered by coal in 2023. While several documents discuss energy sources and coal usage in data centers, none specify the share of coal-powered inference workloads in Asia.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of AI inference workloads in Asia powered by coal in 2023. While several documents discuss energy sources and coal usage in data centers, none specify the share of coal-powered inference workloads in Asia."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally. This figure is explicitly mentioned in the document [ref_id=ebert2024], which provides the global average for data centers, including those dedicated to AI computations. The answer unit is PUE, so the value is reported as a numeric figure.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally. This figure is explicitly mentioned in the document [ref_id=ebert2024], which provides the global average for data centers, including those dedicated to AI computations. The answer unit is PUE, so the value is reported as a numeric figure."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token during inference. This is directly mentioned in multiple snippets, including the abstract and the section on model architecture. The answer matches the requested unit 'parameters' as specified in the additional info.","2B","parameters","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","['In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.', 'Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.']","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token during inference. This is directly mentioned in multiple snippets, including the abstract and the section on model architecture. The answer matches the requested unit 'parameters' as specified in the additional info."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context does not provide specific GHG emissions data for pre-training the Llama 7B model. While several documents discuss emissions from training large models like Llama-3.1 and Meta's Gemma family, no information is given about the Llama 7B model's emissions. Therefore, the answer cannot be determined from the provided context.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific GHG emissions data for pre-training the Llama 7B model. While several documents discuss emissions from training large models like Llama-3.1 and Meta's Gemma family, no information is given about the Llama 7B model's emissions. Therefore, the answer cannot be determined from the provided context."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in the U.S. for 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, this increased to 11,800. The total number added is calculated as 11,800 - 2,600 = 9,200. The answer unit is 'electric delivery vans', so the answer is expressed as a number in that unit.","9200","electric delivery vans","[""amazon2023""]","is_blank","['United States • Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.']","The context provides the number of electric delivery vans in the U.S. for 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, this increased to 11,800. The total number added is calculated as 11,800 - 2,600 = 9,200. The answer unit is 'electric delivery vans', so the answer is expressed as a number in that unit."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question, which suggests these emissions are excluded. Therefore, the statement is false.","0","is_blank","[""morrison2025""]","is_blank","['Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.']","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question, which suggests these emissions are excluded. Therefore, the statement is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for the o3 model in Table 4, specifically for long prompts (10k input-1.5k output). The energy consumption for o3 is listed as 12.222 Wh with a standard deviation of 1.082 Wh. This matches the requested unit of Wh for a long prompt.","12.222","Wh","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598v6""]","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context provides energy consumption values for the o3 model in Table 4, specifically for long prompts (10k input-1.5k output). The energy consumption for o3 is listed as 12.222 Wh with a standard deviation of 1.082 Wh. This matches the requested unit of Wh for a long prompt."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. This is confirmed in multiple snippets, including the Dartmouth Summer Research Project on Artificial Intelligence organized by John McCarthy in 1956. The year 1956 is directly cited as the event when AI was formally established.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.']","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. This is confirmed in multiple snippets, including the Dartmouth Summer Research Project on Artificial Intelligence organized by John McCarthy in 1956. The year 1956 is directly cited as the event when AI was formally established."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide the average Water Usage Effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While Google is mentioned in relation to its sustainability reports and PUE values, no specific WUE value for 2024 is given. Therefore, the answer cannot be determined from the provided information.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide the average Water Usage Effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While Google is mentioned in relation to its sustainability reports and PUE values, no specific WUE value for 2024 is given. Therefore, the answer cannot be determined from the provided information."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, as cited by the International Energy Agency. This directly supports the claim in the question.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","['Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].']","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, as cited by the International Energy Agency. This directly supports the claim in the question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that at a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at that generation length. The answer unit is joules per token, so the value is expressed as a range.","[3,4]","joules per token","[""samsi2024""]","is_blank","['For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.']","The context states that at a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at that generation length. The answer unit is joules per token, so the value is expressed as a range."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that converting the 32-bit floating-point numerical representation to 16-bit through model quantization reduced the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization.","15","percent","[""wu2021a""]","is_blank","['By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.']","The context states that converting the 32-bit floating-point numerical representation to 16-bit through model quantization reduced the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context explicitly states that the 5-10% reduction estimate lacks clear scientific grounding and that the underlying calculations are not detailed. It further notes that applying observations from individual projects to global emissions is not scientifically valid, and that rigorous calculation requires counterfactual scenarios and rebound effect accounting, which are not provided. Therefore, the claim is not supported by clear, publicly available calculations or sound scientific grounding.","0","is_blank","[""luccioni2025c""]","is_blank","['Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding— in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T L.148050 or WBCSD guidance on avoided emissions51. And yet, these numbers were picked up in research52 and the media, used as evidence that the potential of AI to stop climate change is overwhelmingly positive53, 54.']","The context explicitly states that the 5-10% reduction estimate lacks clear scientific grounding and that the underlying calculations are not detailed. It further notes that applying observations from individual projects to global emissions is not scientifically valid, and that rigorous calculation requires counterfactual scenarios and rebound effect accounting, which are not provided. Therefore, the claim is not supported by clear, publicly available calculations or sound scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context explicitly states that large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This is directly supported by the statement in [ref_id=patterson2021].","1","is_blank","[""patterson2021""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context explicitly states that large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This is directly supported by the statement in [ref_id=patterson2021]."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This information is directly provided in the text and matches the question's request for the estimated cost in USD.","25000","USD","[""schwartz2019""]","[""https://arxiv.org/pdf/1909.08053.pdf""]","['Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.']","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This information is directly provided in the text and matches the question's request for the estimated cost in USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This specific percentage is directly mentioned in the text and applies to the scenario described, including the use of a single GPU, which aligns with the NVIDIA A40-48GB GPU setup. The answer unit is percent, so the value is expressed as a number.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This specific percentage is directly mentioned in the text and applies to the scenario described, including the use of a single GPU, which aligns with the NVIDIA A40-48GB GPU setup. The answer unit is percent, so the value is expressed as a number."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate performs up to 1 billion queries per day. The average energy consumption for a comparable multi-purpose model, Flan-T5-xxl, is given as 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate: (1 billion queries / 1,000) * 0.083 kWh = 83,000 kWh, which equals 83 MWh. This matches the expected unit of MWh.","83","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","['1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.']","The context states that Google Translate performs up to 1 billion queries per day. The average energy consumption for a comparable multi-purpose model, Flan-T5-xxl, is given as 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate: (1 billion queries / 1,000) * 0.083 kWh = 83,000 kWh, which equals 83 MWh. This matches the expected unit of MWh."
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average data center PUE globally in 2023 was 1.58. This information is directly provided in the first sentence of the first context snippet, which references a source [74] for the figure. The answer unit is PUE, and the value is a single number, consistent with the expected format.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average data center PUE globally in 2023 was 1.58. This information is directly provided in the first sentence of the first context snippet, which references a source [74] for the figure. The answer unit is PUE, and the value is a single number, consistent with the expected format."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. While Microsoft is mentioned in relation to its sustainability goals and energy usage, there is no mention of specific wind turbines contracted for Azure AI clusters in that year.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. While Microsoft is mentioned in relation to its sustainability goals and energy usage, there is no mention of specific wind turbines contracted for Azure AI clusters in that year."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that for Gemini Ultra, R&D staff costs make up 49% of the total amortized model development costs when equity is included. This is explicitly mentioned in multiple snippets, including the section on R&D staff costs and the summary in the conclusion. The answer unit is 'percent', so the value is given as a percentage.","49","percent","[""cottier2024""]","is_blank","['Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models. Firstly, Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators, and this makes the hardware cost relatively low. Secondly, our methodology is limited by deriving the number of full-time equivalent staff from the reported number of contributors, for which Gemini had 941—much higher than 7', 'We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.']","The context states that for Gemini Ultra, R&D staff costs make up 49% of the total amortized model development costs when equity is included. This is explicitly mentioned in multiple snippets, including the section on R&D staff costs and the summary in the conclusion. The answer unit is 'percent', so the value is given as a percentage."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015, confirming the number of members in the inaugural 2015 Study Panel. The answer unit is 'people', so the numeric value '17' is appropriate.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","['The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.']","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015, confirming the number of members in the inaugural 2015 Study Panel. The answer unit is 'people', so the numeric value '17' is appropriate."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","['Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.']","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states that models with more parameters do not necessarily consume more energy during inference. For example, the Phi-3 Small model (7B parameters) can consume less energy than the Phi-3 Mini model (3.8B parameters) due to more efficient attention mechanisms (GQA vs MHA), which allow better scaling and energy amortization. This contradicts the claim that more parameters always lead to higher energy consumption.","0","is_blank","[""chung2025"", ""zschache2025""]","[""https://ml.energy/leaderboard""]","['Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.', 'This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3 × more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.']","The context explicitly states that models with more parameters do not necessarily consume more energy during inference. For example, the Phi-3 Small model (7B parameters) can consume less energy than the Phi-3 Mini model (3.8B parameters) due to more efficient attention mechanisms (GQA vs MHA), which allow better scaling and energy amortization. This contradicts the claim that more parameters always lead to higher energy consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel explicitly states that there is no cause for concern that AI is an imminent threat to humankind. This is directly supported by the context, which emphasizes that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future. The panel's conclusion is clear and unambiguous.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","['Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.']","The Study Panel explicitly states that there is no cause for concern that AI is an imminent threat to humankind. This is directly supported by the context, which emphasizes that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future. The panel's conclusion is clear and unambiguous."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide any information about the CO2 emissions from OpenAI's API requests in January 2024. While several documents discuss emissions from training models or inference, none specify the emissions for OpenAI's API during that specific month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the CO2 emissions from OpenAI's API requests in January 2024. While several documents discuss emissions from training models or inference, none specify the emissions for OpenAI's API during that specific month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reported the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. This figure is directly cited in the document as a first-hand report of GHG emissions from pre-training, making it the most accurate comparison available.","11390","tCO2e","[""luccioni2025c""]","[""https://arxiv.org/pdf/2504.06307v1""]","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context states that Meta reported the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. This figure is directly cited in the document as a first-hand report of GHG emissions from pre-training, making it the most accurate comparison available."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period. This is directly supported by multiple references, including Figure 2(d) and the text in Section VI. The answer is given as a multiplier, matching the expected format.","2.9","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2111.00364""]","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period. This is directly supported by multiple references, including Figure 2(d) and the text in Section VI. The answer is given as a multiplier, matching the expected format."
"q079","How many miles is the Earth from the Sun?","The context provided does not contain any information about the distance between the Earth and the Sun. The documents discuss various topics such as AI model training energy consumption, vehicle emissions, data center electricity usage, and transportation logistics, but none mention the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the distance between the Earth and the Sun. The documents discuss various topics such as AI model training energy consumption, vehicle emissions, data center electricity usage, and transportation logistics, but none mention the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is confirmed in multiple snippets, including the description of AlphaGo's success and its use of reinforcement learning after being trained on human expert data and self-play.","1","is_blank","[""stone2022"", ""wu2021a""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""https://arxiv.org/abs/2111.00364v2""]","['The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.', 'AI is demonstrated to achieve human-level conversation tasks, such as the Blender Bot [2], and play games at superhuman levels, such as AlphaZero [3].']","The context explicitly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is confirmed in multiple snippets, including the description of AlphaGo's success and its use of reinforcement learning after being trained on human expert data and self-play."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context describes a batching strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. This is explicitly identified as 'continuous batching' in multiple snippets, including [ref_id=fernandez2025], which states that continuous batching mitigates GPU under-utilization by improving utilization and reducing idle time.","continuous batching","is_blank","[""fernandez2025""]","is_blank","['Continuous Batching Reduces Energy Use. LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, which leads to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022). This ap5']","The context describes a batching strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. This is explicitly identified as 'continuous batching' in multiple snippets, including [ref_id=fernandez2025], which states that continuous batching mitigates GPU under-utilization by improving utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section '5 Model Alignment' after describing the dDPO fine-tuning step. The answer unit is H100 GPU hours, so the answer value is 60.","60","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['The entire alignment process takes 60 H100 GPU hours.']","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section '5 Model Alignment' after describing the dDPO fine-tuning step. The answer unit is H100 GPU hours, so the answer value is 60."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","In the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13. The Max-Performance policy selected g6e.xlarge with a total cost of $2.699. The cost increase is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This is explicitly stated in the context.","26.7","percent","[""kim2025""]","is_blank","['Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.']","In the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13. The Max-Performance policy selected g6e.xlarge with a total cost of $2.699. The cost increase is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This is explicitly stated in the context."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This value is directly provided in the text and matches the required unit of grams of CO2eq.","1594","g CO2eq","[""luccioni2024""]","[""https://doi.org/10.1145/3634250.3634251""]","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This value is directly provided in the text and matches the required unit of grams of CO2eq."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This range is explicitly mentioned in the 2025 study's appendix, which aligns with the requested unit of Wh.","[0.06,3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This range is explicitly mentioned in the 2025 study's appendix, which aligns with the requested unit of Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. This is reiterated multiple times, emphasizing that a universal approach fails to address the complex, context-specific nature of AI's ethical and environmental issues. Therefore, the belief in a universal solution is rejected.","0","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2504.00797v1""]","['There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.']","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. This is reiterated multiple times, emphasizing that a universal approach fails to address the complex, context-specific nature of AI's ethical and environmental issues. Therefore, the belief in a universal solution is rejected."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited from the U.S. Energy Information Administration (USE21) in 2021. This value is explicitly mentioned in multiple snippets, including the one labeled [ref_id=patterson2021] which directly references the 2021 data.","0.429","kg of CO2e/KWh","[""patterson2021""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited from the U.S. Energy Information Administration (USE21) in 2021. This value is explicitly mentioned in multiple snippets, including the one labeled [ref_id=patterson2021] which directly references the 2021 data."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context explicitly states that the Hivemind framework was used for all experiments to enable distributed spot instance training across clouds and continents. Hivemind is described as a PyTorch-based framework developed for collaborative deep learning training in a decentralized fashion, capable of handling peer dropouts and functioning effectively in low-bandwidth, high-latency environments. This directly supports the use of Hivemind for decentralized training across multiple continents and cloud providers.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/learning-at-home/hivemind""]","['Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.', 'We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks.']","The context explicitly states that the Hivemind framework was used for all experiments to enable distributed spot instance training across clouds and continents. Hivemind is described as a PyTorch-based framework developed for collaborative deep learning training in a decentralized fashion, capable of handling peer dropouts and functioning effectively in low-bandwidth, high-latency environments. This directly supports the use of Hivemind for decentralized training across multiple continents and cloud providers."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly proposes expanding the notion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. This expanded concept is referred to as 'social transparency' by Ehsan et al., as cited in the text. The term is directly used in the context to describe the integration of ethical, societal, and environmental dimensions into AI system descriptions.","social transparency","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2504.00797""]","['In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].']","The context explicitly proposes expanding the notion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. This expanded concept is referred to as 'social transparency' by Ehsan et al., as cited in the text. The term is directly used in the context to describe the integration of ethical, societal, and environmental dimensions into AI system descriptions."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This model outperformed several large language models and was among the top-performing models in the experiments. The specific model mentioned is a linear model with sentence embeddings, which is explicitly identified as achieving the highest accuracy.","linear model with sentence embeddings","is_blank","[""zschache2025""]","[""https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2""]","['The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.']","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This model outperformed several large language models and was among the top-performing models in the experiments. The specific model mentioned is a linear model with sentence embeddings, which is explicitly identified as achieving the highest accuracy."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is confirmed in multiple sections, including the abstract, introduction, and conclusion. Lamina is described as the system implementing the proposed model-attention disaggregation approach.","Lamina","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814""]","['To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.', 'We develop and deploy Lamina on a cluster comprising heterogeneous GPUs. Evaluation on traces collected from production systems show that Lamina provides16.1 ∼ 90.1% higher throughput than heterogeneous solutions with similar hardware costs.']","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is confirmed in multiple sections, including the abstract, introduction, and conclusion. Lamina is described as the system implementing the proposed model-attention disaggregation approach."
"q093","How many parameters does the largest T5 model have?","The context does not provide information about the number of parameters in the largest T5 model. While T5 is mentioned in relation to its FLOPS and training approach, no specific parameter count is given for any T5 model variant. Therefore, the answer cannot be determined from the provided context.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information about the number of parameters in the largest T5 model. While T5 is mentioned in relation to its FLOPS and training approach, no specific parameter count is given for any T5 model variant. Therefore, the answer cannot be determined from the provided context."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters. This is directly mentioned in multiple sections, including the abstract and the description of the model's architecture. The answer unit is 'parameters', so the answer is given as a numeric value in billions.","8","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.', 'JetMoE-8B is trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.', 'Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The context explicitly states that JetMoE-8B has 8B parameters. This is directly mentioned in multiple sections, including the abstract and the description of the model's architecture. The answer unit is 'parameters', so the answer is given as a numeric value in billions."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that one technology company’s data center water consumption increased by ∼20% from 2021 to 2022, and another technology company’s saw ∼34% and ∼22% increases over the same periods. Google is identified as one of the technology companies in this context, and the ∼20% increase is specifically attributed to Google in the sentence: 'Google observed a 20% uptick in the same period [42, 78]'. This directly supports the answer.","20","percent","[""li2025b"", ""luccioni2025a""]","[""https://doi.org/10.48550/ARXIV.2304.03271"", ""https://www.greenpeace.org/usa/reports/oil-in-the-cloud/""]","['Google observed a 20% uptick in the same period [42, 78].']","The context states that one technology company’s data center water consumption increased by ∼20% from 2021 to 2022, and another technology company’s saw ∼34% and ∼22% increases over the same periods. Google is identified as one of the technology companies in this context, and the ∼20% increase is specifically attributed to Google in the sentence: 'Google observed a 20% uptick in the same period [42, 78]'. This directly supports the answer."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'CO₂ emissions per unit of electricity consumed' as the metric known as Carbon Intensity. This is explicitly stated in Table I of the document [ref_id=khan2025], which lists Carbon Intensity with the unit gCO2/kWh and the definition 'CO2 emissions per unit of electricity consumed'.","Carbon Intensity","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","TABLE I COMMON CARBON EMISSION METRICS IN GREEN AI Metric Unit Definition Reference Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines 'CO₂ emissions per unit of electricity consumed' as the metric known as Carbon Intensity. This is explicitly stated in Table I of the document [ref_id=khan2025], which lists Carbon Intensity with the unit gCO2/kWh and the definition 'CO2 emissions per unit of electricity consumed'."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%. This is explicitly listed in Table 2 under the 'Utilization' column for the 101B stage, which corresponds to the final growth stage.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88%","The context states that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%. This is explicitly listed in Table 2 under the 'Utilization' column for the 101B stage, which corresponds to the final growth stage."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that GPT-4 has the largest amortized hardware and energy cost, at $40M. This figure is explicitly mentioned in the text as the amortized training cost for GPT-4, which aligns with the answer unit of USD.","40000000","USD","[""cottier2024""]","is_blank","['Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.']","The context states that GPT-4 has the largest amortized hardware and energy cost, at $40M. This figure is explicitly mentioned in the text as the amortized training cost for GPT-4, which aligns with the answer unit of USD."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for a Transformer-based universal translation model by 810×. This optimization includes platform-level caching, GPU acceleration, and algorithmic changes, which are explicitly mentioned as part of the full-stack optimization. The figure (Fig. 7) and associated text confirm this reduction factor.","810","multiplier","[""wu2021a""]","[""https://arxiv.org/pdf/2104.01615.pdf""]","Fig. 7. For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","The context states that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for a Transformer-based universal translation model by 810×. This optimization includes platform-level caching, GPU acceleration, and algorithmic changes, which are explicitly mentioned as part of the full-stack optimization. The figure (Fig. 7) and associated text confirm this reduction factor."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This means that the throughput achieved was 59% of the local throughput (100% - 41% = 59%). The fraction of local throughput is therefore 0.59.","0.59","multiplier","[""erben2023""]","is_blank","['However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).']","The context states that intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This means that the throughput achieved was 59% of the local throughput (100% - 41% = 59%). The fraction of local throughput is therefore 0.59."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is directly mentioned in multiple snippets and aligns with the answer_unit of 'liters'.","3500000000","liters","[""amazon2023""]","is_blank","['In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.']","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is directly mentioned in multiple snippets and aligns with the answer_unit of 'liters'."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states that custom tags can reduce energy consumption across all three prompt engineering techniques tested: zero-shot, one-shot, and few-shots. This is confirmed in multiple sections, including the answer to RQ1 and the experimental results in Figure 3a, which show reduced energy consumption when custom tags are used compared to the default configuration.","1","is_blank","[""rubei2025""]","[""https://github.com/riccardoRubei/Greens-2025-Replication-Package""]","['Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.', 'Figure 3a shows the energy consumption of the three prompt techniques applied to the five different configurations. ... one-shot and few-shots reduced the consumption from 0.0000347 to 0.0000174 (-99%) and from 0.0000537 to 0.0000293 (-83%) comparing with the default configuration C0, respectively.']","The context explicitly states that custom tags can reduce energy consumption across all three prompt engineering techniques tested: zero-shot, one-shot, and few-shots. This is confirmed in multiple sections, including the answer to RQ1 and the experimental results in Figure 3a, which show reduced energy consumption when custom tags are used compared to the default configuration."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that NVIDIA shipped 3.7 million GPUs in 2024, as reported in a 2025 paper. This figure is explicitly mentioned in the text and matches the expected unit of GPUs.","3.7","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","['While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].']","The context states that NVIDIA shipped 3.7 million GPUs in 2024, as reported in a 2025 paper. This figure is explicitly mentioned in the text and matches the expected unit of GPUs."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips. This figure is explicitly mentioned in the section discussing the breakdown of components in Figure 5.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips. This figure is explicitly mentioned in the section discussing the breakdown of components in Figure 5."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value. The answer unit is PUE, so the value is given as a decimal number.","1.10","PUE","[""wu2021a"", ""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://sustainability.fb.com/report-page/data-centers/""]","['Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers.', 'whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook]']","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value. The answer unit is PUE, so the value is given as a decimal number."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly mentions the Finnish ETAIROS project as the initiative that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym ETAIROS is directly stated in the text.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","['From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]']","The context explicitly mentions the Finnish ETAIROS project as the initiative that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym ETAIROS is directly stated in the text."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra in USD.","30000000","USD","[""cottier2024""]","is_blank","['We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.']","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra in USD."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b)). Furthermore, it argues that these measures should also consider environmental risks, in line with the Act’s normative goals, including environmental protection. The authors advocate for an interpretation that includes environmental risks in the mandatory risk assessments, despite the lack of explicit emphasis.","True","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.']","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b)). Furthermore, it argues that these measures should also consider environmental risks, in line with the Act’s normative goals, including environmental protection. The authors advocate for an interpretation that includes environmental risks in the mandatory risk assessments, despite the lack of explicit emphasis."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple instances, including in the sentence: 'In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].' The answer unit is µg/m³, so the numeric value 9 is provided accordingly.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","['In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].']","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple instances, including in the sentence: 'In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].' The answer unit is µg/m³, so the numeric value 9 is provided accordingly."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment comparing print books to e-readers found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is explicitly stated in the context, and the answer unit is 'books', so the numeric value 115 is the correct response.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","['a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].']","A life cycle assessment comparing print books to e-readers found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is explicitly stated in the context, and the answer unit is 'books', so the numeric value 115 is the correct response."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-affected areas. This is explicitly mentioned in multiple sections of the provided documents, particularly in the 'Recommendation 3: Promoting Public Health Equity' and '8 Conclusion' sections of the Han et al. study. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier.","200","multiplier","[""han2024""]","[""https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/""]","['The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.', 'per-household impacts potentially up to 200 times higher than in less-affected areas.']","The context states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-affected areas. This is explicitly mentioned in multiple sections of the provided documents, particularly in the 'Recommendation 3: Promoting Public Health Equity' and '8 Conclusion' sections of the Han et al. study. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is explicitly stated in Table B1 of the context. The value is 702.06 Wh, which matches the required unit 'Wh' as specified in the additional info.","702.06","Wh","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","DS Llama 70B 2 702.06 0.46 2543.47 993.68","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is explicitly stated in Table B1 of the context. The value is 702.06 Wh, which matches the required unit 'Wh' as specified in the additional info."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide the total number of parameters in the large language model analyzed by Dodge et al. in the 2022 paper. Although Dodge et al. are mentioned in relation to measuring carbon intensity of AI in cloud instances, no specific model size or parameter count is given in the provided text.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide the total number of parameters in the large language model analyzed by Dodge et al. in the 2022 paper. Although Dodge et al. are mentioned in relation to measuring carbon intensity of AI in cloud instances, no specific model size or parameter count is given in the provided text."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, leading to reduced costs, which in turn stimulates increased demand and overall resource consumption. This is explicitly mentioned in the context, where it states that efficiency gains can paradoxically spur higher overall consumption, referencing William Stanley Jevons' observation about coal use in the 19th century. The context further explains that this paradox applies to AI, where improved efficiency may lead to greater adoption and resource use.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""jegham2025"", ""fernandez2025""]","[""https://doi.org/10.1145/3715275.3732007"", ""https://doi.org/10.1016/j.eco.2006.03.002"", ""https://arxiv.org/abs/2309.14393""]","['Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].', 'This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.', 'Recent work on indirect impacts [63] warns of potential “rebound effects”, whereby gains in efficiency spur higher overall consumption.']","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, leading to reduced costs, which in turn stimulates increased demand and overall resource consumption. This is explicitly mentioned in the context, where it states that efficiency gains can paradoxically spur higher overall consumption, referencing William Stanley Jevons' observation about coal use in the 19th century. The context further explains that this paradox applies to AI, where improved efficiency may lead to greater adoption and resource use."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that GPT-3 training emissions are approximately 305% of a round trip between San Francisco and New York, while Meena training emissions are 53% of the same round trip. To find how many Meena training runs equal one GPT-3 training run, divide GPT-3's percentage by Meena's percentage: 305 / 53 ≈ 5.75. Thus, approximately 5.75 Meena training runs would use the same total energy as one full GPT-3 training run.","5.75","multiplier","[""patterson2021""]","[""https://arxiv.org/pdf/2104.10057.pdf""]","['GPT-3 is ~305% of such a round trip.', 'Meena is 53% of such a round trip.']","The context states that GPT-3 training emissions are approximately 305% of a round trip between San Francisco and New York, while Meena training emissions are 53% of the same round trip. To find how many Meena training runs equal one GPT-3 training run, divide GPT-3's percentage by Meena's percentage: 305 / 53 ≈ 5.75. Thus, approximately 5.75 Meena training runs would use the same total energy as one full GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study by Luccioni et al., the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh. This value is explicitly listed in the 'image generation' row under the 'mean' column for energy per 1,000 queries. The answer unit is kWh, so the numeric value is reported directly.","2.907","kWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3634277.3634280""]","image generation 2.907 3.31","According to Table 2 in the 2024 study by Luccioni et al., the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh. This value is explicitly listed in the 'image generation' row under the 'mean' column for energy per 1,000 queries. The answer unit is kWh, so the numeric value is reported directly."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context states that the carbon emissions from training models are equivalent to the average yearly energy use for 98.2 homes in the U.S., which is also equivalent to 6.5 tanker trucks’ worth of gasoline burned. According to the U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator, this amount of emissions is equivalent to 7096 lbs of CO2e. This value is directly tied to the average American household's annual energy use, which is consistent with the question about an average American life in one year.","7096","lbs","[""morrison2025""]","[""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator""]","Using the U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator 15, this is equivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy use for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in one year.","The context states that the carbon emissions from training models are equivalent to the average yearly energy use for 98.2 homes in the U.S., which is also equivalent to 6.5 tanker trucks’ worth of gasoline burned. According to the U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator, this amount of emissions is equivalent to 7096 lbs of CO2e. This value is directly tied to the average American household's annual energy use, which is consistent with the question about an average American life in one year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context identifies West Virginia counties with high per-household health costs due to their role in supplying electricity to data centers in neighboring Virginia. Figure 6 lists the top-10 counties by per-household health cost, with WV Marion having the highest cost at $1,218.30 per household. This is explicitly stated in the context as the top-ranked county in West Virginia.","WV Marion","is_blank","[""han2024""]","is_blank","WV Marion 1218.3(978.0, 1458.5) 0.80","The context identifies West Virginia counties with high per-household health costs due to their role in supplying electricity to data centers in neighboring Virginia. Figure 6 lists the top-10 counties by per-household health cost, with WV Marion having the highest cost at $1,218.30 per household. This is explicitly stated in the context as the top-ranked county in West Virginia."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 per inference task, and after optimization they were 0.015 kg CO2 per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75. This indicates a 25% reduction in emissions, which aligns with the context stating that emissions were significantly reduced after optimization.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III: Mistral-small After Optimization CO2 (kg): 0.015; Before Optimization CO2 (kg): 0.020","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 per inference task, and after optimization they were 0.015 kg CO2 per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75. This indicates a 25% reduction in emissions, which aligns with the context stating that emissions were significantly reduced after optimization."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The combined training and fine-tuning energy costs for the BLOOMz-7B model are explicitly stated in Table 5 of the 'Power Hungry Processing' study. The training energy cost is 51,686 kWh, and the fine-tuning energy cost is 7,571 kWh. Adding these together gives a total of 59,257 kWh. The answer unit is kWh, so the numeric value is provided accordingly.","59257","kWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","BLOOMz-7B Training energy (kWh) 51,686; Finetuning energy (kWh) 7,571; Cost parity (# inferences) 592,570,000","The combined training and fine-tuning energy costs for the BLOOMz-7B model are explicitly stated in Table 5 of the 'Power Hungry Processing' study. The training energy cost is 51,686 kWh, and the fine-tuning energy cost is 7,571 kWh. Adding these together gives a total of 59,257 kWh. The answer unit is kWh, so the numeric value is provided accordingly."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the description of the growth strategy where it mentions producing models with 16B, 51B, and 101B parameters, with FLM-101B being the final model. The answer unit is 'parameters', so the value is given as a number in billions.","101","parameters","[""li2025a""]","is_blank","['FLM-101B is produced with 101B parameters in a single training.', 'We produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.']","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the description of the growth strategy where it mentions producing models with 16B, 51B, and 101B parameters, with FLM-101B being the final model. The answer unit is 'parameters', so the value is given as a number in billions."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","","","inferences","[]","is_blank","is_blank",""
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The study 'Power Hungry Processing' explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was consumed. This value is directly provided in the context and matches the requested unit of kWh.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2402.05084""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The study 'Power Hungry Processing' explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was consumed. This value is directly provided in the context and matches the requested unit of kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the cost parity for the BLOOMz-7B model in Table 5, which specifies the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. The value is explicitly listed as 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the cost parity for the BLOOMz-7B model in Table 5, which specifies the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. The value is explicitly listed as 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context explicitly states that the dataset used for the German nuclear waste site objection texts is the FKTG-dataset, which was published as part of the public participation process for selecting a repository site for high-level radioactive waste in Germany. The dataset is available at https://beteiligung.bge.de/index.php.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","['The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).']","The context explicitly states that the dataset used for the German nuclear waste site objection texts is the FKTG-dataset, which was published as part of the public participation process for selecting a repository site for high-level radioactive waste in Germany. The dataset is available at https://beteiligung.bge.de/index.php."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. While Meta's Llama-3.1 training electricity consumption and water usage for its global data center fleet are mentioned, no data is given for inference serving clusters in 2024. Therefore, the answer cannot be determined from the provided information.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. While Meta's Llama-3.1 training electricity consumption and water usage for its global data center fleet are mentioned, no data is given for inference serving clusters in 2024. Therefore, the answer cannot be determined from the provided information."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metal mining and environmental impacts, it does not mention recycling rates or the use of recycled materials in H100 production.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metal mining and environmental impacts, it does not mention recycling rates or the use of recycled materials in H100 production."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the net CO2e for Evolved Transformer NAS is 3.2 tCO2e. Google Flights calculates the average round trip CO2e for a whole passenger jet between SFO and JFK as 180.4 tCO2e. Dividing 3.2 by 180.4 gives approximately 0.0177, which corresponds to about 3 passengers when considering the per-passenger round trip CO2e of 1.2 tCO2e. The text explicitly states that the CO2e equivalent of NAS is ~3 passengers taking a round trip.","3","passengers","[""patterson2021""]","is_blank","['The CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.', 'Google Flights calculates the average CO2 emission for all direct flights between San Francisco (SFO) and New York (JFK) as 90.2t, so the average round trip is 180.4t.', 'A single passenger round trip SF-NY is ~1.2t CO2e.']","The context states that the net CO2e for Evolved Transformer NAS is 3.2 tCO2e. Google Flights calculates the average round trip CO2e for a whole passenger jet between SFO and JFK as 180.4 tCO2e. Dividing 3.2 by 180.4 gives approximately 0.0177, which corresponds to about 3 passengers when considering the per-passenger round trip CO2e of 1.2 tCO2e. The text explicitly states that the CO2e equivalent of NAS is ~3 passengers taking a round trip."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no environmental impact disclosure. This figure is explicitly stated in the context, which directly addresses the percentage of token usage associated with models that do not disclose their environmental impact.","84","percent","[""luccioni2025c""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no environmental impact disclosure. This figure is explicitly stated in the context, which directly addresses the percentage of token usage associated with models that do not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies the bare minimum hardware requirements for LLaMA models in Table II. For LLaMA-13B, the table indicates that 1 A100 80GB GPU is sufficient, with a maximum batch size of 64. This is consistent with the statement that 4 A100 GPUs are required for the 65B model, implying fewer are needed for smaller models like 13B.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The context specifies the bare minimum hardware requirements for LLaMA models in Table II. For LLaMA-13B, the table indicates that 1 A100 80GB GPU is sufficient, with a maximum batch size of 64. This is consistent with the statement that 4 A100 GPUs are required for the 65B model, implying fewer are needed for smaller models like 13B."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that a 6.1 billion parameter transformer model was trained for 8 days (192 hours) and consumed 13,812.4 kWh, which is only 13% of the full training run. The authors estimate that a full training run would take 60 days, and thus the total energy consumption would be approximately (60/8) * 13.8 MWh = 103.5 MWh, or 103,500 kWh. Converting this to metric tons of CO2 emissions using the context's implied carbon intensity (1 kg CO2 per kWh), the estimated emissions are 103.5 metric tons. This aligns with the context's mention of '103,500 kWh — almost 2800 times more than training the BERT-small model!' and the direct calculation from energy use.","103.5","metric tons","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3510901.3510905""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that a 6.1 billion parameter transformer model was trained for 8 days (192 hours) and consumed 13,812.4 kWh, which is only 13% of the full training run. The authors estimate that a full training run would take 60 days, and thus the total energy consumption would be approximately (60/8) * 13.8 MWh = 103.5 MWh, or 103,500 kWh. Converting this to metric tons of CO2 emissions using the context's implied carbon intensity (1 kg CO2 per kWh), the estimated emissions are 103.5 metric tons. This aligns with the context's mention of '103,500 kWh — almost 2800 times more than training the BERT-small model!' and the direct calculation from energy use."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. While several documents discuss the benefits of pruning, quantization, and other optimization techniques in reducing energy consumption and carbon emissions, none quantify the total emissions avoided specifically in 2023. Therefore, the answer cannot be determined from the provided information.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. While several documents discuss the benefits of pruning, quantization, and other optimization techniques in reducing energy consumption and carbon emissions, none quantify the total emissions avoided specifically in 2023. Therefore, the answer cannot be determined from the provided information."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly mentioned in the text under the section discussing mixed GPU types and cost efficiency. The answer unit is 'percent', so the value is expressed as a number.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly mentioned in the text under the section discussing mixed GPU types and cost efficiency. The answer unit is 'percent', so the value is expressed as a number."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is $4.63. This information is explicitly stated in Table 1 of the provided context, which lists the price per chip for H20 as $4.63/hr. The answer unit is USD per hour, so the numeric value is returned accordingly.","4.63","USD per hour","[""chen2024""]","[""https://www.google.com/about/datacenters/efficiency/""]","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is $4.63. This information is explicitly stated in Table 1 of the provided context, which lists the price per chip for H20 as $4.63/hr. The answer unit is USD per hour, so the numeric value is returned accordingly."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, rather than automatically. This is directly supported by the sentence: 'In fact, most carbon footprint analyses gather the information manually by writing to authors.' This contradicts the claim that information is gathered automatically.","0","is_blank","[""luccioni2025b""]","is_blank","['In fact, most carbon footprint analyses gather the information manually by writing to authors.']","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, rather than automatically. This is directly supported by the sentence: 'In fact, most carbon footprint analyses gather the information manually by writing to authors.' This contradicts the claim that information is gathered automatically."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost. This percentage is explicitly mentioned in the text and directly answers the question using the average attribution method.","44","percent","[""han2024""]","is_blank","['Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers’ total electricity cost.']","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost. This percentage is explicitly mentioned in the text and directly answers the question using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 A100 80GB GPU, as shown in Table II. This is consistent with the model's size and the memory constraints of the GPU. The answer is directly supported by the provided table and the discussion of baseline configurations.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 A100 80GB GPU, as shown in Table II. This is consistent with the model's size and the memory constraints of the GPU. The answer is directly supported by the provided table and the discussion of baseline configurations."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that experimental results reveal that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim in the question.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","['Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.']","The context explicitly states that experimental results reveal that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim in the question."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to collect information for carbon footprint analysis, but were only able to collect 95 answers. This directly answers the question about the number of responses received.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","['For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].']","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to collect information for carbon footprint analysis, but were only able to collect 95 answers. This directly answers the question about the number of responses received."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE-8B project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the expected unit of USD per hour.","3.33","USD per hour","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The JetMoE-8B project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the expected unit of USD per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost for training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. It further notes that the health cost varies widely by location, with a dramatic increase in Iowa compared to Oregon. Since the question asks about Altoona, Iowa, and the context explicitly mentions that the health cost in Iowa can exceed 120% of the electricity cost, the answer is 120%.","120","percent","[""han2024""]","is_blank","['It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.', 'For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].']","The context states that the total health cost for training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. It further notes that the health cost varies widely by location, with a dramatic increase in Iowa compared to Oregon. Since the question asks about Altoona, Iowa, and the context explicitly mentions that the health cost in Iowa can exceed 120% of the electricity cost, the answer is 120%."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section. The unit 'tokens' matches the answer_unit specified in the additional info.","1.25T","tokens","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","['JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.']","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section. The unit 'tokens' matches the answer_unit specified in the additional info."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects' with project locations and corresponding numbers of projects. According to the table, as of January 2024, there were 36 projects announced in the United Kingdom. This directly answers the question and matches the expected unit of 'projects'.","36","projects","[""amazon2023""]","is_blank","United Kingdom 36 901","The context includes a table titled 'Amazon Renewable Energy Projects' with project locations and corresponding numbers of projects. According to the table, as of January 2024, there were 36 projects announced in the United Kingdom. This directly answers the question and matches the expected unit of 'projects'."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage of men in Amazon's U.S. workforce across all levels for 2023. According to the 'Amazon Representation by the Numbers' section, 46.9% of the U.S. workforce identified as men in 2023. This aligns with the answer_unit of 'percent'.","46.9","percent","[""amazon2023""]","[""https://sustainabilityreport.amazon.com/2023""]","46.9%52.9%","The context provides the percentage of men in Amazon's U.S. workforce across all levels for 2023. According to the 'Amazon Representation by the Numbers' section, 46.9% of the U.S. workforce identified as men in 2023. This aligns with the answer_unit of 'percent'."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint attributed to its supply chain. The answer unit is 'percent', so the value is expressed as a number.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","['For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].']","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint attributed to its supply chain. The answer unit is 'percent', so the value is expressed as a number."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The execution time breakdown for sparse BlackMamba fine-tuning with a batch size of 84 is shown in Fig. 4. The figure indicates that the total execution time for the sparse BlackMamba model at batch size 84 is approximately 2.0 seconds, as the sum of the forward, backward, and optimizer stages is represented in the bar chart. The context specifies that the execution time is measured in seconds.","2.0","seconds","[""xia2024""]","is_blank","['Fig. 4. Execution time breakdown (seconds). Sparse(bsz=84) for BlackMamba.']","The execution time breakdown for sparse BlackMamba fine-tuning with a batch size of 84 is shown in Fig. 4. The figure indicates that the total execution time for the sparse BlackMamba model at batch size 84 is approximately 2.0 seconds, as the sum of the forward, backward, and optimizer stages is represented in the bar chart. The context specifies that the execution time is measured in seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity metric' as a way to quantify the ratio of calculation to communication time in distributed training. This metric is explicitly described as the ratio of calculation to communication time, which directly addresses the question. The document states that granularity enables comparison of scalability potential between models and helps estimate training performance with additional resources.","granularity metric","is_blank","[""erben2023""]","is_blank","['We introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.']","The context introduces the 'granularity metric' as a way to quantify the ratio of calculation to communication time in distributed training. This metric is explicitly described as the ratio of calculation to communication time, which directly addresses the question. The document states that granularity enables comparison of scalability potential between models and helps estimate training performance with additional resources."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This means the emissions from the deal could be 6.4 times higher than the company’s annual carbon removal targets. The context explicitly states this figure, which directly answers the question.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","['a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]', 'a single contract to use AI to expand oil production “could enable carbon emissions adding up to 640 percent of the company’s carbon removal targets”']","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This means the emissions from the deal could be 6.4 times higher than the company’s annual carbon removal targets. The context explicitly states this figure, which directly answers the question."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, and used for agricultural, industrial, or municipal purposes. This definition is explicitly stated in multiple snippets, including the section on water withdrawal vs. water consumption.","water withdrawal","is_blank","[""li2025b""]","[""https://www.cia.gov/the-world-factbook/field/total-water-withdrawal/""]","['• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].']","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, and used for agricultural, industrial, or municipal purposes. This definition is explicitly stated in multiple snippets, including the section on water withdrawal vs. water consumption."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is explicitly mentioned in multiple snippets, including the core activity of the Standing Committee. The answer unit is 'years', so the answer is expressed as a number in that unit.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","['As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.']","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is explicitly mentioned in multiple snippets, including the core activity of the Standing Committee. The answer unit is 'years', so the answer is expressed as a number in that unit."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that in the US, the average household is equipped with an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. This information is directly cited from Deloitte, 2021, and matches the requested unit of 'devices'.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","['In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].', 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].']","The context explicitly states that in the US, the average household is equipped with an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. This information is directly cited from Deloitte, 2021, and matches the requested unit of 'devices'."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training large language models (LLMs) based on publicly available data. According to Table 1 in the appendix of [ref_id=luccioni2025c], the energy consumption spans from 0.8 MWh for OLMo 20M to 3,500 MWh for Llama 4 Scout. This range is explicitly stated in the text and directly answers the question about the energy consumption for pre-training LLMs.","[0.8,3500]","MWh","[""luccioni2025c""]","[""https://arxiv.org/pdf/2504.17674.pdf""]","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) - Model: OLMo 20M, Energy Consumption (MWh): 0.8; Model: Llama 4 Scout, Energy Consumption (MWh): 3,500","The context provides a range of energy consumption for pre-training large language models (LLMs) based on publicly available data. According to Table 1 in the appendix of [ref_id=luccioni2025c], the energy consumption spans from 0.8 MWh for OLMo 20M to 3,500 MWh for Llama 4 Scout. This range is explicitly stated in the text and directly answers the question about the energy consumption for pre-training LLMs."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This is directly mentioned in multiple snippets, including the one that describes Watson's success in the Jeopardy challenge. Therefore, the claim that Watson did NOT beat human contenders is false.","0","is_blank","[""stone2022""]","is_blank","['IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.']","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This is directly mentioned in multiple snippets, including the one that describes Watson's success in the Jeopardy challenge. Therefore, the claim that Watson did NOT beat human contenders is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that one study suggests 10–50 queries on GPT-3 consume around half a liter of water. This directly answers the question about how many queries consume approximately half a liter of water. The answer is a numeric range, which matches the expected format per the provided answer_unit.","[10,50]","queries","[""luccioni2025a"", ""li2025b""]","is_blank","['Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].', 'Additionally, GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.']","The context states that one study suggests 10–50 queries on GPT-3 consume around half a liter of water. This directly answers the question about how many queries consume approximately half a liter of water. The answer is a numeric range, which matches the expected format per the provided answer_unit."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-chat is 6.681, which is higher than the Llama-2-13b-chat score of 6.650. This confirms that JetMoE-8B-Chat surpasses Llama-2-13b-Chat after alignment.","6.681","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","['Table 4: MT-Bench score comparison of various models', 'JetMoE-8B-chat 6.681', 'Llama-2-13b-chat 6.650']","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-chat is 6.681, which is higher than the Llama-2-13b-chat score of 6.650. This confirms that JetMoE-8B-Chat surpasses Llama-2-13b-Chat after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many medium-length completions can be produced with the water in a single 500ml bottle. The answer is therefore a range of 10 to 50 responses.","[10,50]","responses","[""li2025b""]","is_blank","GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many medium-length completions can be produced with the water in a single 500ml bottle. The answer is therefore a range of 10 to 50 responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as explicitly mentioned in multiple sections. This aligns with the question asking about conversational chat settings, which are described as short-context tasks. The answer unit is 'percent', so the value is expressed as a percentage.","77","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as explicitly mentioned in multiple sections. This aligns with the question asking about conversational chat settings, which are described as short-context tasks. The answer unit is 'percent', so the value is expressed as a percentage."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that for meaningful inference with the 65B LLaMA model, a minimum of 4 A100 GPUs each with 80GB of memory are required. This is explicitly mentioned in multiple snippets, including Table II which lists 4 A100 80GB GPUs as the bare minimum configuration for LLaMA 65B. The answer matches the required unit 'A100_80GB_GPUs' as a numeric count.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. ... 65B 8 64 4 128']","The context states that for meaningful inference with the 65B LLaMA model, a minimum of 4 A100 GPUs each with 80GB of memory are required. This is explicitly mentioned in multiple snippets, including Table II which lists 4 A100 80GB GPUs as the bare minimum configuration for LLaMA 65B. The answer matches the required unit 'A100_80GB_GPUs' as a numeric count."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context consistently states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c] confirm this figure, with the phrase 'more than 10,000 round trips' appearing in several locations. The answer unit is 'round trips', so the answer is expressed as a number in that unit.","10000","round trips","[""han2024"", ""luccioni2025c""]","[""https://arxiv.org/abs/2412.06288v2"", ""https://arxiv.org/abs/2412.06288v2""]","['Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.', '24 assessed the public health toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.']","The context consistently states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c] confirm this figure, with the phrase 'more than 10,000 round trips' appearing in several locations. The answer unit is 'round trips', so the answer is expressed as a number in that unit."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","NVIDIA estimated that 80–90% of the ML workload is inference processing, as stated in the context from the 2021 paper by Patterson et al., which references the 2019 study by Leo19. This percentage is consistent with the provided answer unit of 'percent'.","[80,90]","percent","[""patterson2021"", ""luccioni2024"", ""fernandez2025""]","[""https://arxiv.org/pdf/2102.04155.pdf"", ""https://arxiv.org/pdf/2402.01804.pdf"", ""https://arxiv.org/pdf/2504.17674.pdf""]","['For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].', 'According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by Meta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the remainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training [40].']","NVIDIA estimated that 80–90% of the ML workload is inference processing, as stated in the context from the 2021 paper by Patterson et al., which references the 2019 study by Leo19. This percentage is consistent with the provided answer unit of 'percent'."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study are explicitly stated in the ETHICAL CONSIDERATIONS STATEMENT section. The study reports that, for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This figure is directly provided in the context and matches the required unit of kg CO2eq.","178.97","kg CO2eq","[""luccioni2024""]","[""https://doi.org/10.48550/arXiv.2406.03123""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The total CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study are explicitly stated in the ETHICAL CONSIDERATIONS STATEMENT section. The study reports that, for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This figure is directly provided in the context and matches the required unit of kg CO2eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context explicitly states that estimating GPU energy consumption based on Thermal Design Power (TDP) is nearly always an overestimation because GPUs rarely draw their maximum power at every moment. This method can lead to a worst-case overestimation by a factor of 4.1, as seen in the case of CodeGemma 2B on H100 GPUs. Accurate measurements are crucial, and TDP-based estimations are discouraged due to their inaccuracy.","0","is_blank","[""chung2025"", ""patterson2021"", ""ebert2024""]","is_blank","['Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.', 'In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).', 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.']","The context explicitly states that estimating GPU energy consumption based on Thermal Design Power (TDP) is nearly always an overestimation because GPUs rarely draw their maximum power at every moment. This method can lead to a worst-case overestimation by a factor of 4.1, as seen in the case of CodeGemma 2B on H100 GPUs. Accurate measurements are crucial, and TDP-based estimations are discouraged due to their inaccuracy."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes 3.098 Wh, indicating higher energy use for the mini version. This directly contradicts the claim that GPT-4o mini consumes less energy.","0","is_blank","[""jegham2025""]","is_blank","['For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.', 'GPT-4o and GPT-4o mini also show that infrastructure can outweigh model size in determining energy efficiency.']","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes 3.098 Wh, indicating higher energy use for the mini version. This directly contradicts the claim that GPT-4o mini consumes less energy."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The ground truth throughput for a dense Mixtral-CS-A100-40GB model at batch size 1 is found in Fig. 13, which shows the throughput value as 0.5 queries/sec. This value is directly observable from the figure's data points for the dense model under the A100-40GB GPU configuration.","0.5","queries/sec","[""xia2024""]","is_blank","['Fig. 13. Projected maximum batch size of Mixtral for different GPUs.', '0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse']","The ground truth throughput for a dense Mixtral-CS-A100-40GB model at batch size 1 is found in Fig. 13, which shows the throughput value as 0.5 queries/sec. This value is directly observable from the figure's data points for the dense model under the A100-40GB GPU configuration."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the introduction of increasingly commercial and proprietary models, catalyzed by ChatGPT, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. This indicates that the trend did not continue to increase after 2022 but instead declined.","0","is_blank","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.']","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the introduction of increasingly commercial and proprietary models, catalyzed by ChatGPT, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. This indicates that the trend did not continue to increase after 2022 but instead declined."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU in Griggs et al. (2024) is explicitly stated in Table 1 and the surrounding text. The calculation is based on normalizing RunPod's H100 pricing ($4.69/hour) to match the pricing structure of major cloud providers using the A100-80G price ($3.67/hour) as a reference. The resulting normalized price is $7.5164 per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","Table 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100. On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The normalized on-demand hourly price for an H100 GPU in Griggs et al. (2024) is explicitly stated in Table 1 and the surrounding text. The calculation is based on normalizing RunPod's H100 pricing ($4.69/hour) to match the pricing structure of major cloud providers using the A100-80G price ($3.67/hour) as a reference. The resulting normalized price is $7.5164 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide specific information about the water used for cooling during OpenAI's GPT-4 training run. While GPT-3's water consumption is discussed in detail, including estimates for training and inference, no such data is available for GPT-4. Therefore, the answer cannot be determined from the provided context.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the water used for cooling during OpenAI's GPT-4 training run. While GPT-3's water consumption is discussed in detail, including estimates for training and inference, no such data is available for GPT-4. Therefore, the answer cannot be determined from the provided context."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs and costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour. The answer is derived directly from the provided cost and standard time conversion.","7.22","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","['serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.']","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs and costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour. The answer is derived directly from the provided cost and standard time conversion."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in multiple snippets, including the description of Figure 2(a) in the context.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in multiple snippets, including the description of Figure 2(a) in the context."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context states that training a Transformer model with Neural Architecture Search (NAS) produces 626,155 pounds of CO2 emissions, which is comparable to the lifetime emissions of five cars. Using the emissions-to-driving-distance ratio from the context, where one car's lifetime emissions are equivalent to approximately 126,000 miles of driving, the total driving distance equivalent is calculated by multiplying 626,155 lbs by the ratio of 126,000 miles per 126,000 lbs (1:1 ratio per car). Since five cars are equivalent, the total distance is 626,155 / 126,000 * 126,000 = 626,155 miles. However, since the emissions are for five cars, the equivalent driving distance is 5 * 126,000 = 630,000 miles. The context confirms this equivalence directly.","630000","miles","[""dodge2022"", ""luccioni2023"", ""strubell2019"", ""zschache2025""]","is_blank","['The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.', 'Car, avg incl. fuel, 1 lifetime 126,000']","The context states that training a Transformer model with Neural Architecture Search (NAS) produces 626,155 pounds of CO2 emissions, which is comparable to the lifetime emissions of five cars. Using the emissions-to-driving-distance ratio from the context, where one car's lifetime emissions are equivalent to approximately 126,000 miles of driving, the total driving distance equivalent is calculated by multiplying 626,155 lbs by the ratio of 126,000 miles per 126,000 lbs (1:1 ratio per car). Since five cars are equivalent, the total distance is 626,155 / 126,000 * 126,000 = 626,155 miles. However, since the emissions are for five cars, the equivalent driving distance is 5 * 126,000 = 630,000 miles. The context confirms this equivalence directly."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh per inference. With 606,096 downloads and assuming each download results in 1 million inferences, the total number of inferences is 606,096 × 1,000,000 = 606,096,000,000. Multiplying this by the per-inference energy cost gives 606,096,000,000 × 1.0 × 10−4 kWh = 60,609,600 kWh. Converting to MWh (dividing by 1,000) yields 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","['Inference energy (kWh) 1.0 × 10−4', 'BLOOMz-7B has been downloaded 606,096 times']","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh per inference. With 606,096 downloads and assuming each download results in 1 million inferences, the total number of inferences is 606,096 × 1,000,000 = 606,096,000,000. Multiplying this by the per-inference energy cost gives 606,096,000,000 × 1.0 × 10−4 kWh = 60,609,600 kWh. Converting to MWh (dividing by 1,000) yields 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned in multiple snippets, including the abstract and the detailed training section. The answer unit is 'H100 GPU hours', so the answer value is the numeric value 30000.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.', 'JetMoE-8B is trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.']","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned in multiple snippets, including the abstract and the detailed training section. The answer unit is 'H100 GPU hours', so the answer value is the numeric value 30000."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is based on the observed growth rate of 2.4× per year since 2016, with GPT-4's cost of $40M in 2023 used as a baseline for extrapolation. The answer unit is USD, so the value is expressed as a number in that unit.","1000000000","USD","[""cottier2024""]","is_blank","['If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.']","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is based on the observed growth rate of 2.4× per year since 2016, with GPT-4's cost of $40M in 2023 used as a baseline for extrapolation. The answer unit is USD, so the value is expressed as a number in that unit."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS, as stated in the context from [ref_id=patterson2021]. This value is directly provided in the text and matches the requested unit of FLOPS.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","['OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].']","OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS, as stated in the context from [ref_id=patterson2021]. This value is directly provided in the text and matches the requested unit of FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is explicitly mentioned in multiple snippets, including Table II which lists 8 V100 GPUs as the bare minimum hardware requirement for LLaMA 65B. The answer unit is 'V100_32GB_GPUs', which matches the identifier used in the context.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128']","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is explicitly mentioned in multiple snippets, including Table II which lists 8 V100 GPUs as the bare minimum hardware requirement for LLaMA 65B. The answer unit is 'V100_32GB_GPUs', which matches the identifier used in the context."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The total computational work for the 101B training stage is estimated using the single-GPU throughput and the number of GPUs. The single-GPU throughput is 165 teraFLOPs/sec, and the number of GPUs is 12 (from Table 2: Pipeline Parallel Size = 12). The total training time for the 101B stage is 26.54B tokens, and the billion tokens per day is 1.11B (from Table 1). The total time in seconds is calculated as (26.54 / 1.11) days * 86400 seconds/day ≈ 2,070,000 seconds. Total FLOPs = 165e12 * 12 * 2,070,000 ≈ 4.11e21 FLOPs, which is 4.11 zettaFLOPs. However, the context provides a direct estimate in Table 4: FLM-101B's cost is 28.22 zettaFLOPs for English and 24.54 for Chinese, totaling 52.76 zettaFLOPs. Since the 101B stage is the final stage and the total cost is 52.76 zettaFLOPs, this is the best estimate.","52.76","zettaFLOPs","[""li2025a""]","[""https://arxiv.org/pdf/2405.13876.pdf""]","['Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.', 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).']","The total computational work for the 101B training stage is estimated using the single-GPU throughput and the number of GPUs. The single-GPU throughput is 165 teraFLOPs/sec, and the number of GPUs is 12 (from Table 2: Pipeline Parallel Size = 12). The total training time for the 101B stage is 26.54B tokens, and the billion tokens per day is 1.11B (from Table 1). The total time in seconds is calculated as (26.54 / 1.11) days * 86400 seconds/day ≈ 2,070,000 seconds. Total FLOPs = 165e12 * 12 * 2,070,000 ≈ 4.11e21 FLOPs, which is 4.11 zettaFLOPs. However, the context provides a direct estimate in Table 4: FLM-101B's cost is 28.22 zettaFLOPs for English and 24.54 for Chinese, totaling 52.76 zettaFLOPs. Since the 101B stage is the final stage and the total cost is 52.76 zettaFLOPs, this is the best estimate."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide the top-1 accuracy for AlexNet on ImageNet in 2012. Although AlexNet is mentioned in multiple references, including its parameter count and its role in the evolution of deep learning models, no specific accuracy value is given for AlexNet in the provided documents.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide the top-1 accuracy for AlexNet on ImageNet in 2012. Although AlexNet is mentioned in multiple references, including its parameter count and its role in the evolution of deep learning models, no specific accuracy value is given for AlexNet in the provided documents."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 GPUs. Therefore, the total number of A800 GPUs is calculated as 24 servers multiplied by 8 GPUs per server, resulting in 192 GPUs. This matches the 'Number of GPUs' value in Table 2, which confirms the configuration.","192","GPUs","[""li2025a""]","[""https://github.com/NVIDIA/Megatron-LM""]","['FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.', 'Table 2: Parallel strategies and throughput for different growth stages. ... Number of GPUs: 192']","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 GPUs. Therefore, the total number of A800 GPUs is calculated as 24 servers multiplied by 8 GPUs per server, resulting in 192 GPUs. This matches the 'Number of GPUs' value in Table 2, which confirms the configuration."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that the emissions from training a Transformer model with Neural Architecture Search (NAS) are comparable to the lifetime carbon emissions of five cars. This is supported by multiple sources, including Strubell et al. [42], which quantified the emissions as being equivalent to five cars' lifetime emissions. The question asks for the equivalent in average American lifetimes, and since the context directly equates the emissions to five cars, and no other conversion is provided, the answer is five lifetimes.","5","lifetimes","[""dodge2022"", ""zschache2025"", ""luccioni2023""]","[""https://dl.acm.org/doi/10.1145/3531146.3589385"", ""https://arxiv.org/abs/2305.12851"", ""https://arxiv.org/abs/2305.12851""]","['which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars.', 'revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes', 'estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.']","The context states that the emissions from training a Transformer model with Neural Architecture Search (NAS) are comparable to the lifetime carbon emissions of five cars. This is supported by multiple sources, including Strubell et al. [42], which quantified the emissions as being equivalent to five cars' lifetime emissions. The question asks for the equivalent in average American lifetimes, and since the context directly equates the emissions to five cars, and no other conversion is provided, the answer is five lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the number of GPU hours required.","25000","hours","[""schwartz2019""]","[""http://commoncrawl.org/2019/07/""]","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the number of GPU hours required."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the text and directly answers the question about CO₂e avoidance.","47500","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com""]","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the text and directly answers the question about CO₂e avoidance."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context states that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This is explicitly mentioned in the description of the experimental setup.","vllm library","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context states that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This is explicitly mentioned in the description of the experimental setup."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The energy consumption for the Llama 3.1 70B model increased by a factor of 1.95 when deployed on two nodes instead of one, as shown in Table B2. This table compares single-node and double-node deployments on the Capella system, with energy consumed (Wh) rising from 48.60 to 94.88 Wh. The increase is due to network communication overhead and additional GPU operation in the multi-node setup.","1.95","multiplier","[""zschache2025""]","is_blank","['Table B2: Comparison single vs. double node deployment, Capella system', 'Llama 3.1 70B: single node energy = 48.60 Wh, double node energy = 94.88 Wh']","The energy consumption for the Llama 3.1 70B model increased by a factor of 1.95 when deployed on two nodes instead of one, as shown in Table B2. This table compares single-node and double-node deployments on the Capella system, with energy consumed (Wh) rising from 48.60 to 94.88 Wh. The increase is due to network communication overhead and additional GPU operation in the multi-node setup."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific data on the amount of water consumed per ChatGPT user session in 2023. While several documents discuss water consumption related to AI models like GPT-3 and GPT-4o, none specify the water usage per user session for 2023. The closest reference is to Microsoft's data center water consumption, which increased by 34% between 2021 and 2022, but this does not translate to per-session water usage for ChatGPT users.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on the amount of water consumed per ChatGPT user session in 2023. While several documents discuss water consumption related to AI models like GPT-3 and GPT-4o, none specify the water usage per user session for 2023. The closest reference is to Microsoft's data center water consumption, which increased by 34% between 2021 and 2022, but this does not translate to per-session water usage for ChatGPT users."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. This exceeds the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Since 391,509 MWh is greater than 377,685 MWh, the annual electricity use from 700 million daily GPT-4o queries is comparable to more than 35,000 U.S. homes. The answer is expressed in the unit 'homes' as requested.","35000","homes","[""jegham2025""]","is_blank","['These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.']","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. This exceeds the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Since 391,509 MWh is greater than 377,685 MWh, the annual electricity use from 700 million daily GPT-4o queries is comparable to more than 35,000 U.S. homes. The answer is expressed in the unit 'homes' as requested."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, as cited in [ref_id=luccioni2025a]. This directly answers the question about Microsoft's reported percentage increase during that period.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, as cited in [ref_id=luccioni2025a]. This directly answers the question about Microsoft's reported percentage increase during that period."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This value is directly mentioned in multiple snippets and is consistent with the data provided in Table 2.","1.11","PUE","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","['The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.', 'Table 2. Small and large units for energy and carbon footprint in this paper, plus airline travel CO2e used for perspective on the relative size of ML emissions compared to other activities (Section 4.8).']","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This value is directly mentioned in multiple snippets and is consistent with the data provided in Table 2."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that a decaying growth pattern from June to December 2025 yields a total of approximately 772 billion GPT-4o queries in 2025. This figure is explicitly mentioned in multiple snippets and directly answers the question about the total estimated number of queries.","772000000000","queries","[""jegham2025""]","is_blank","['yielding a total of approximately 772 billion GPT-4o queries in 2025']","The context states that a decaying growth pattern from June to December 2025 yields a total of approximately 772 billion GPT-4o queries in 2025. This figure is explicitly mentioned in multiple snippets and directly answers the question about the total estimated number of queries."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The OpenLLM Leaderboard Avg. score for JetMoE-8B is explicitly stated in Table 3 as 53.0. This value is directly reported in the context and matches the expected answer unit of 'score'.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The OpenLLM Leaderboard Avg. score for JetMoE-8B is explicitly stated in Table 3 as 53.0. This value is directly reported in the context and matches the expected answer unit of 'score'."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss renewable energy usage in AI, such as Amazon's investments and Google's carbon neutrality goals, none specify the count of training runs using only renewable energy sources in that year.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss renewable energy usage in AI, such as Amazon's investments and Google's carbon neutrality goals, none specify the count of training runs using only renewable energy sources in that year."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, the text also emphasizes that this exclusion results in an unwarranted lack of visibility concerning the environmental impact and energy usage of these models. The authors argue that open-source models should adhere to the same reporting standards as proprietary models due to their significant energy implications. Therefore, the claim that they are fully exempt unless posing systemic risk is supported by the text.","True","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4]. The idea is that OS models, by definition, already disclose certain types of information. Hence, Recital 102 lists information on parameters, including weights, model architecture, and model usage as a prerequisite for systems to be considered OS. However, it does not mandate the disclosure of energy consumption. This results in an unwarrented lack of visibility concerning the environmental impact and energy usage of these models.']","The context states that open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, the text also emphasizes that this exclusion results in an unwarranted lack of visibility concerning the environmental impact and energy usage of these models. The authors argue that open-source models should adhere to the same reporting standards as proprietary models due to their significant energy implications. Therefore, the claim that they are fully exempt unless posing systemic risk is supported by the text."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the average PUE for a typical data center in 2020 is 1.58, as cited in Lawrence (2019, 2020). This figure is explicitly mentioned in the document [ref_id=wu2021b], which provides the relevant data for the US national average in 2020.","1.58","PUE","[""wu2021b""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context states that the average PUE for a typical data center in 2020 is 1.58, as cited in Lawrence (2019, 2020). This figure is explicitly mentioned in the document [ref_id=wu2021b], which provides the relevant data for the US national average in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that for the OPT_2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. This directly answers the question about the cache size at that batch size.","5.312","GB","[""kim2025""]","is_blank","['When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.']","The context states that for the OPT_2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. This directly answers the question about the cache size at that batch size."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models studied—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is consistently mentioned across multiple snippets, confirming the result.","[29, 49]","percent","[""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024""]","is_blank","['For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.', 'We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.', 'Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total.', 'Moreover, our results suggest that R&D staff costs were a major component of total costs for these frontier models.']","The context explicitly states that for the four notable models studied—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is consistently mentioned across multiple snippets, confirming the result."
"q213","Which software package was used to measure energy consumption during inference runs?","The context explicitly mentions that CodeCarbon was used to measure energy consumption during inference. This is supported by multiple references: in [ref_id=rubei2025], it states that energy consumption is quantified during inference using CodeCarbon, and the gpu energy value from CodeCarbon's report is used. Additionally, [ref_id=zschache2025] references CodeCarbon as a tool that estimates energy consumption by monitoring hardware usage.","CodeCarbon","is_blank","[""rubei2025"", ""zschache2025""]","[""https://mlco2.github.io/codecarbon/output.html"", ""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","['In its report, we focus on the value of gpu energy which calculates the energy consumed during in the inference, expressed in kWh. To reduce biases related to unprecise monitoring, we repeated the tests 5 times, calculating eventually the average.', 'Similar to CodeCarbon, Carbontracker (Anthony et al., 2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy consumption by monitoring hardware usage.']","The context explicitly mentions that CodeCarbon was used to measure energy consumption during inference. This is supported by multiple references: in [ref_id=rubei2025], it states that energy consumption is quantified during inference using CodeCarbon, and the gpu energy value from CodeCarbon's report is used. Additionally, [ref_id=zschache2025] references CodeCarbon as a tool that estimates energy consumption by monitoring hardware usage."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis of 100 news articles on ChatGPT's energy consumption revealed that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly corresponds to the contested estimate mentioned in the context.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42","The analysis of 100 news articles on ChatGPT's energy consumption revealed that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly corresponds to the contested estimate mentioned in the context."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function corrects the theoretical FLOPS values to better reflect real-world performance, thereby enhancing prediction accuracy.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","['Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.']","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function corrects the theoretical FLOPS values to better reflect real-world performance, thereby enhancing prediction accuracy."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context explicitly states that increasing the number of shards always increases the wattage and tends to increase the energy costs of inference per response most overall. This is directly supported by the description in section D of the document, which confirms that energy per response increases with the number of shards.","1","is_blank","[""samsi2024""]","is_blank","['D. Energy per Response: LLaMA 65B: ... increasing the number of shards still tends to increase the energy costs of inference per response most overall']","The context explicitly states that increasing the number of shards always increases the wattage and tends to increase the energy costs of inference per response most overall. This is directly supported by the description in section D of the document, which confirms that energy per response increases with the number of shards."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water (Browning et al., 2016). An H100 GPU is assumed to be 0.1% rare earth metal by mass. Therefore, the water consumption for mining rare earth materials for one H100 GPU is 0.1% of 11 kL, which equals 0.011 kL. However, the context explicitly states that the additional water consumption from mining rare earth metals is 2.2 liters per GPU, which is equivalent to 0.0022 kL. This value is consistent with the calculation and is directly provided in the text.","0.0022","kL","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/""]","['We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.']","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water (Browning et al., 2016). An H100 GPU is assumed to be 0.1% rare earth metal by mass. Therefore, the water consumption for mining rare earth materials for one H100 GPU is 0.1% of 11 kL, which equals 0.011 kL. However, the context explicitly states that the additional water consumption from mining rare earth metals is 2.2 liters per GPU, which is equivalent to 0.0022 kL. This value is consistent with the calculation and is directly provided in the text."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This means they are not required to report energy consumption to authorities under current EU rules, unless they meet the systemic risk threshold. Therefore, the statement is false.","0","is_blank","[""ebert2024""]","is_blank","['Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4].']","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This means they are not required to report energy consumption to authorities under current EU rules, unless they meet the systemic risk threshold. Therefore, the statement is false."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This is directly supported by multiple references, including [ref_id=luccioni2025a] and [ref_id=wu2021b], which confirm the 30% figure. The answer unit is 'percent', so the answer is expressed as a number.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.', 'In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].']","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This is directly supported by multiple references, including [ref_id=luccioni2025a] and [ref_id=wu2021b], which confirm the 30% figure. The answer unit is 'percent', so the answer is expressed as a number."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. This figure is explicitly mentioned in the text and aligns with the average attribution method used in the study. The answer unit is USD, so the value is expressed as a number in that unit.","6.7","USD","[""han2024""]","is_blank","The U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. This figure is explicitly mentioned in the text and aligns with the average attribution method used in the study. The answer unit is USD, so the value is expressed as a number in that unit."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model for a long prompt is 12.222 Wh, as shown in Table 4. The GPT-4.1 nano model consumes 0.827 Wh for a long prompt. Dividing 12.222 by 0.827 gives approximately 14.8, which is the factor by which o3's energy consumption exceeds that of GPT-4.1 nano. The answer unit is a multiplier, so the result is expressed as a numeric value.","14.8","multiplier","[""jegham2025""]","is_blank","['Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).', 'o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082', 'GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094']","The energy consumption of the o3 model for a long prompt is 12.222 Wh, as shown in Table 4. The GPT-4.1 nano model consumes 0.827 Wh for a long prompt. Dividing 12.222 by 0.827 gives approximately 14.8, which is the factor by which o3's energy consumption exceeds that of GPT-4.1 nano. The answer unit is a multiplier, so the result is expressed as a numeric value."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the Arena dataset (short-context workload) with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU-type baselines. This range is explicitly mentioned in the text under the 'Short-context Dataset (Arena)' section.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that for the Arena dataset (short-context workload) with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU-type baselines. This range is explicitly mentioned in the text under the 'Short-context Dataset (Arena)' section."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the net carbon emissions for FLM-101B in Table 3, which states the net tCO2e (net metric tons of CO2 equivalent) as 26. This value is directly reported in the table and matches the required unit of tCO2e.","26","tCO2e","[""li2025a""]","[""https://huggingface.co/datasets/laion/OIG"", ""https://huggingface.co/datasets/BAAI/COIG"", ""https://apps.timwhitlock.info/emoji/tables/unicode""]","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... FLM-101B 101B 1.01e5 400 40 26","The context provides the net carbon emissions for FLM-101B in Table 3, which states the net tCO2e (net metric tons of CO2 equivalent) as 26. This value is directly reported in the table and matches the required unit of tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The execution time breakdown for a sparse Mixtral model with batch size 1 on a NVIDIA A40 GPU is shown in Fig. 4. The figure indicates that the total execution time is the sum of the forward, backward, and optimizer stages. From the graph, the forward stage takes approximately 2.0 seconds, the backward stage takes approximately 4.0 seconds, and the optimizer stage takes approximately 0.0 seconds. Adding these together gives a total execution time of 6.0 seconds.","6.0","seconds","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","['Fig. 4. Execution time breakdown.', 'Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer']","The execution time breakdown for a sparse Mixtral model with batch size 1 on a NVIDIA A40 GPU is shown in Fig. 4. The figure indicates that the total execution time is the sum of the forward, backward, and optimizer stages. From the graph, the forward stage takes approximately 2.0 seconds, the backward stage takes approximately 4.0 seconds, and the optimizer stage takes approximately 0.0 seconds. Adding these together gives a total execution time of 6.0 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further emphasizes that health costs are not evenly distributed, with disadvantaged communities bearing a disproportionate share, and per-household impacts potentially up to 200 times higher than in less-affected areas. This directly contradicts the claim that public health costs are evenly distributed.","0","is_blank","[""han2024""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html""]","['The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].', 'Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.']","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further emphasizes that health costs are not evenly distributed, with disadvantaged communities bearing a disproportionate share, and per-household impacts potentially up to 200 times higher than in less-affected areas. This directly contradicts the claim that public health costs are evenly distributed."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context explicitly states that as a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years, with this observation being cited from Sun et al., 2019. This directly supports the claim made in the question.","1","is_blank","[""wu2021b""]","is_blank","['Figure 2: As a result of Moore’s law scaling and architec-tural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].']","The context explicitly states that as a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years, with this observation being cited from Sun et al., 2019. This directly supports the claim made in the question."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context explicitly states that quantization was applied through Ollama, an open-source platform known for its support of edge computing principles and privacy-centric deployments. Ollama was used in the financial sentiment case study to enable local inference and apply 4-bit quantization, aligning with the framework's emphasis on local deployment and energy efficiency.","Ollama","is_blank","[""khan2025""]","is_blank","['We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.']","The context explicitly states that quantization was applied through Ollama, an open-source platform known for its support of edge computing principles and privacy-centric deployments. Ollama was used in the financial sentiment case study to enable local inference and apply 4-bit quantization, aligning with the framework's emphasis on local deployment and energy efficiency."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that to simulate a real-world deployment with a non-public dataset on spot VMs that could terminate at any time, an independent S3 storage provider named Backblaze (B2) was chosen. Backblaze was selected for its replicated data centers and reasonable ingress rates from every continent, making it suitable for on-demand access via shards. The WebDataset library was used to access the datasets in thetar format, enabling streaming decompression and preprocessing.","Backblaze (B2)","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","['To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent.']","The context states that to simulate a real-world deployment with a non-public dataset on spot VMs that could terminate at any time, an independent S3 storage provider named Backblaze (B2) was chosen. Backblaze was selected for its replicated data centers and reasonable ingress rates from every continent, making it suitable for on-demand access via shards. The WebDataset library was used to access the datasets in thetar format, enabling streaming decompression and preprocessing."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates that energy consumption during inference highly correlates with the model’s runtime, and that this correlation is strong enough to make runtime a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption, as stated in multiple snippets.","is_blank","is_blank","[""zschache2025"", ""fernandez2025""]","[""https://arxiv.org/abs/2508.14170v1"", ""https://arxiv.org/abs/2504.17674v1""]","['Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime.', 'Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.']","The context indicates that energy consumption during inference highly correlates with the model’s runtime, and that this correlation is strong enough to make runtime a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption, as stated in multiple snippets."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. This is directly mentioned in multiple snippets, including [ref_id=ebert2024] and [ref_id=morrison2025]. The answer is the name of the senator, which matches the expected format for a person's identifier.","Edward J. Markey","is_blank","[""ebert2024"", ""morrison2025""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf"", ""https://www.congress.gov/bill/118th-congress/senate-bill/3732/ Accessed: 2025-03-18""]","['Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].']","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. This is directly mentioned in multiple snippets, including [ref_id=ebert2024] and [ref_id=morrison2025]. The answer is the name of the senator, which matches the expected format for a person's identifier."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed as $11.06/hr in Table 1. This value is directly provided in the context and matches the expected unit of USD per hour.","11.06","USD per hour","[""chen2024""]","[""https://arxiv.org/pdf/2406.15895.pdf""]","Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed as $11.06/hr in Table 1. This value is directly provided in the context and matches the expected unit of USD per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context states that in February 2024, AWS announced that the average expected life of its servers had improved from five to six years. This indicates that the estimated average GPU lifetime before retirement in AI data centers was five years prior to this update, and the improvement to six years was reported in 2024. Therefore, the estimated average GPU lifetime before retirement in 2024 was five years.","5","years","[""amazon2023""]","[""https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-instances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking/""]","['In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.']","The context states that in February 2024, AWS announced that the average expected life of its servers had improved from five to six years. This indicates that the estimated average GPU lifetime before retirement in AI data centers was five years prior to this update, and the improvement to six years was reported in 2024. Therefore, the estimated average GPU lifetime before retirement in 2024 was five years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies the bare minimum hardware requirements for LLaMA models in Table II. For LLaMA 13B, the required number of V100 32GB GPUs is 2, as stated in the table under the 'V100 32GB' column for 'Count'. This is without any further model compression, optimization, quantization, or distillation.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The context specifies the bare minimum hardware requirements for LLaMA models in Table II. For LLaMA 13B, the required number of V100 32GB GPUs is 2, as stated in the table under the 'V100 32GB' column for 'Count'. This is without any further model compression, optimization, quantization, or distillation."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the 'five cars' estimate, which is based on a 284 metric ton CO2e figure from a 2019 study. The 'five cars' estimate is not representative of all AI training workloads, and recent reports like Gemma's exceed it significantly.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","['Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number']","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the 'five cars' estimate, which is based on a 284 metric ton CO2e figure from a 2019 study. The 'five cars' estimate is not representative of all AI training workloads, and recent reports like Gemma's exceed it significantly."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This directly answers the question about training duration. The answer is provided in hours as required by the answer_unit.","336","hours","[""strubell2019""]","[""https://arxiv.org/abs/1802.05365""]","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This directly answers the question about training duration. The answer is provided in hours as required by the answer_unit."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, as noted in the reference [8]. This value is explicitly mentioned in the text and aligns with the requested unit of L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, as noted in the reference [8]. This value is explicitly mentioned in the text and aligns with the requested unit of L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context explicitly states that Google reported a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is confirmed in multiple sources, including [ref_id=dodge2022] and [ref_id=wu2021b], which both cite Google's PUE of 1.10 in 2021. The answer unit is PUE, so the value is given as a numeric value.","1.10","PUE","[""dodge2022"", ""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","['Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].', 'Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].']","The context explicitly states that Google reported a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is confirmed in multiple sources, including [ref_id=dodge2022] and [ref_id=wu2021b], which both cite Google's PUE of 1.10 in 2021. The answer unit is PUE, so the value is given as a numeric value."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when AWS uses 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023. The percentage reduction is directly tied to the migration from on-premises to AWS infrastructure in North America.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","['Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.']","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when AWS uses 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023. The percentage reduction is directly tied to the migration from on-premises to AWS infrastructure in North America."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a net cost of $3460. This cost estimate is derived from scaling the cost by the number of queries, as mentioned in the text. The answer is directly provided in the context with the specified unit (USD).","3460","USD","[""xia2024""]","is_blank","['For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.']","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a net cost of $3460. This cost estimate is derived from scaling the cost by the number of queries, as mentioned in the text. The answer is directly provided in the context with the specified unit (USD)."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context consistently states that GPUs account for a significant majority of electricity consumption in AI workloads. In a specific experiment measuring power consumption during BERT-base model training, the GPU accounted for 74% of the total energy consumption. The text also confirms this result is fairly uncontroversial and aligns with broader observations that GPUs consume most of the electricity in deep learning applications.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3510430.3510435""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context consistently states that GPUs account for a significant majority of electricity consumption in AI workloads. In a specific experiment measuring power consumption during BERT-base model training, the GPU accounted for 74% of the total energy consumption. The text also confirms this result is fairly uncontroversial and aligns with broader observations that GPUs consume most of the electricity in deep learning applications."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Since each node contains multiple GPUs and the total number of H100 GPUs across all nodes is explicitly given as 96, the total number of H100 GPUs used for training is 96.","96","H100 GPUs","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Since each node contains multiple GPUs and the total number of H100 GPUs across all nodes is explicitly given as 96, the total number of H100 GPUs used for training is 96."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power for a single node is over 600W. This is explicitly mentioned in multiple snippets from the same document (ref_id=morrison2025), which also references Figure 2 showing the power consumption during the first 300 logging steps of OLMo 2 7B training. The answer is given in Watts, matching the required unit.","600","Watts","[""morrison2025""]","[""https://openreview.net/pdf?id=3d2d1d9d9b""]","['When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.']","The context states that during active training, the average GPU power for a single node is over 600W. This is explicitly mentioned in multiple snippets from the same document (ref_id=morrison2025), which also references Figure 2 showing the power consumption during the first 300 logging steps of OLMo 2 7B training. The answer is given in Watts, matching the required unit."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a comparison of CO2e emissions from various activities, including an average human life in one year. According to Table 1 in the document [ref_id=strubell2019], the estimated CO2e for an average human life in one year is 11,023 pounds. This value is directly stated in the table and matches the requested unit of pounds (lbs).","11023","lbs","[""strubell2019""]","[""https://bit.ly/2Hw0xWc""]","Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption. 1Sources: (1) Air travel and per-capita consump- tion: https://bit.ly/2Hw0xWc; (2) car lifetime: https://bit.ly/2Qbr0w1.","The context provides a comparison of CO2e emissions from various activities, including an average human life in one year. According to Table 1 in the document [ref_id=strubell2019], the estimated CO2e for an average human life in one year is 11,023 pounds. This value is directly stated in the table and matches the requested unit of pounds (lbs)."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of 1.25 times when using A100 GPUs over V100 GPUs.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of 1.25 times when using A100 GPUs over V100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly provided in multiple snippets and aligns with the question's request for energy consumption in Wh. The answer unit is Wh, so the value is reported as a number in that unit.","0.42","Wh","[""jegham2025""]","is_blank","['A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.']","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly provided in multiple snippets and aligns with the question's request for energy consumption in Wh. The answer unit is Wh, so the value is reported as a number in that unit."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","In the online workload experiment with a 400 TPS SLO, InferSave's top choice was g4dn.xlarge with a total cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The cost difference is calculated as (2.699 - 0.71) / 0.71 ≈ 2.80, or 280%. This is explicitly stated in the context: 'which is about 280% more expensive than InferSave’s top choice.'","280","percent","[""kim2025""]","is_blank","['With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.']","In the online workload experiment with a 400 TPS SLO, InferSave's top choice was g4dn.xlarge with a total cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The cost difference is calculated as (2.699 - 0.71) / 0.71 ≈ 2.80, or 280%. This is explicitly stated in the context: 'which is about 280% more expensive than InferSave’s top choice.'"
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context indicates that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly identifies the V100 and A30 as energy-efficient architectures for such tasks. Since the question asks for the most energy-efficient GPU architecture for single-token classification, and both V100 and A30 are mentioned as efficient, the V100 is the most specific and commonly referenced in the context.","V100","is_blank","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context indicates that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly identifies the V100 and A30 as energy-efficient architectures for such tasks. Since the question asks for the most energy-efficient GPU architecture for single-token classification, and both V100 and A30 are mentioned as efficient, the V100 is the most specific and commonly referenced in the context."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context explicitly states that reporting the financial cost or 'price tag' of developing, training, and running models is a key Green AI practice. This is directly mentioned in multiple sources, including Schwartz et al. (2019), which advocates for making efficiency and cost transparency central to Green AI research.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","['Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).']","The context explicitly states that reporting the financial cost or 'price tag' of developing, training, and running models is a key Green AI practice. This is directly mentioned in multiple sources, including Schwartz et al. (2019), which advocates for making efficiency and cost transparency central to Green AI research."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022. This figure is explicitly mentioned in the text, which aligns with the requested unit of metric tons. The reference to the UN’s Global E-Waste Monitor 2024 supports this data.","62000000","metric tons","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","['AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.']","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022. This figure is explicitly mentioned in the text, which aligns with the requested unit of metric tons. The reference to the UN’s Global E-Waste Monitor 2024 supports this data."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per processor for TPU v2 is 221 Watts, as stated in Table 3. For V100 GPU, the average system power per processor is 325 Watts, also from Table 3. The difference is calculated as 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","['Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].', 'Processor Average (Watts) DNNs used to calculate average power TPU v2 221 Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]', 'V100 GPU 325 Transformer (Big), GPT-3 [Sut21]']","The average system power per processor for TPU v2 is 221 Watts, as stated in Table 3. For V100 GPU, the average system power per processor is 325 Watts, also from Table 3. The difference is calculated as 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is directly provided in the abstract of the paper by Li et al. (ref_id=li2025b), which is the primary source for this specific figure.","700000","liters","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context explicitly states that training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is directly provided in the abstract of the paper by Li et al. (ref_id=li2025b), which is the primary source for this specific figure."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and the key takeaways section. The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier.","20","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2111.00364""]","['Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].', 'Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021.']","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and the key takeaways section. The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that in the cross-efficiency DEA analysis, o3-mini achieved the highest cross-efficiency score of 0.884, followed closely by o1-mini and Claude 3.7 Sonnet. This indicates that o3-mini ranked highest in eco-efficiency among the models evaluated.","o3-mini","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598v6""]","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context states that in the cross-efficiency DEA analysis, o3-mini achieved the highest cross-efficiency score of 0.884, followed closely by o1-mini and Claude 3.7 Sonnet. This indicates that o3-mini ranked highest in eco-efficiency among the models evaluated."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context explicitly states that the current average lifetime for cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in multiple sources, including Cordella et al. (2020), which is cited in the context.","1","is_blank","[""wu2021b""]","[""https://www.fairphone.com/en/""]","['develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]']","The context explicitly states that the current average lifetime for cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in multiple sources, including Cordella et al. (2020), which is cited in the context."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that for CV models, the per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) with three to eight GPUs in intra-zone experiments. This indicates nearly linear scaling. The term 'almost linear' directly supports the claim of near-linear per-GPU speedup.","1","is_blank","[""erben2023""]","is_blank","['CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.']","The context states that for CV models, the per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) with three to eight GPUs in intra-zone experiments. This indicates nearly linear scaling. The term 'almost linear' directly supports the claim of near-linear per-GPU speedup."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens. This information is directly provided in the first sentence of the FLM-101B Configurations section. The answer unit is 'tokens', so the numeric value 2048 is returned.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens. This information is directly provided in the first sentence of the FLM-101B Configurations section. The answer unit is 'tokens', so the numeric value 2048 is returned."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded, which leads to GPU computation throughput being bottlenecked by VRAM bandwidth. This results in underutilized computation units and low power draw. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU. This supports the claim that LLMs generally have lower power draw than diffusion models during inference.","1","is_blank","[""chung2025""]","is_blank","['This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.', 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.']","The context states that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded, which leads to GPU computation throughput being bottlenecked by VRAM bandwidth. This results in underutilized computation units and low power draw. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU. This supports the claim that LLMs generally have lower power draw than diffusion models during inference."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides data on the gender distribution of Amazon's People Managers globally for the years 2021, 2022, and 2023. In 2023, the percentage of People Managers who identified as women globally is explicitly listed as 47.5%. This aligns with the answer_unit of 'percent', so the answer is directly taken from the table.","47.5","percent","[""amazon2023""]","[""https://sustainabilityreport.amazon.com/2023""]","People Managers
Gender—Global
2021
2022
2023
46.9%52.9%
47.2%52.7%
47.5%52.3%","The context provides data on the gender distribution of Amazon's People Managers globally for the years 2021, 2022, and 2023. In 2023, the percentage of People Managers who identified as women globally is explicitly listed as 47.5%. This aligns with the answer_unit of 'percent', so the answer is directly taken from the table."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded from R&D costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly mentioned in the text following the discussion of the breakdown of costs.","[61,76]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded from R&D costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly mentioned in the text following the discussion of the breakdown of costs."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is consistent across multiple references, including Table III, which shows a decrease in both accuracy and F1 score for all models after optimization. Therefore, the claim that accuracy and F1 scores always improved is false.","0","is_blank","[""khan2025""]","is_blank","['On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.', 'TABLE III: After Optimization - Llama 3.2: Accuracy 0.48 (was 0.45), F1 0.47 (was 0.44); Phi 3.2: Accuracy 0.84 (was 0.82), F1 0.91 (was 0.88); Qwen: Accuracy 0.81 (was 0.79), F1 0.80 (was 0.76); Mistral-small: Accuracy 0.70 (was 0.67), F1 0.69 (was 0.65); Llava-Llama 3: Accuracy 0.54 (was 0.50), F1 0.51 (was 0.48)']","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is consistent across multiple references, including Table III, which shows a decrease in both accuracy and F1 score for all models after optimization. Therefore, the claim that accuracy and F1 scores always improved is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. According to the context, Strubell et al. (2019) used the EPA's value of 0.423 kg per kWh in 2018. Converting this to pounds per kWh (1 kg ≈ 2.20462 lbs), 0.423 kg/kWh equals approximately 0.932 lbs/kWh. However, the context explicitly states in equation (2) that CO2e = 0.954pt, where pt is in kWh, and the coefficient 0.954 directly converts power in kWh to CO2 emissions in pounds. Thus, the EPA's value in pounds per kWh is 0.954 lbs/kWh.","0.954","lbs/kWh","[""strubell2019""]","[""https://bit.ly/2LObQhV""]","CO2e = 0 .954pt (2)","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. According to the context, Strubell et al. (2019) used the EPA's value of 0.423 kg per kWh in 2018. Converting this to pounds per kWh (1 kg ≈ 2.20462 lbs), 0.423 kg/kWh equals approximately 0.932 lbs/kWh. However, the context explicitly states in equation (2) that CO2e = 0.954pt, where pt is in kWh, and the coefficient 0.954 directly converts power in kWh to CO2 emissions in pounds. Thus, the EPA's value in pounds per kWh is 0.954 lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 based on GPU shipment data. This directly provides the projected range of electricity consumption for global AI in 2027. The answer unit is TWh, so the numeric range is formatted accordingly.","[85,134]","TWh","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 based on GPU shipment data. This directly provides the projected range of electricity consumption for global AI in 2027. The answer unit is TWh, so the numeric range is formatted accordingly."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in Europe, Amazon delivered 150 million packages via EVs in 2023. This directly answers the question. The answer unit is 'packages', so the value is expressed as a number in millions.","150","packages","[""amazon2023""]","is_blank","['Europe • We delivered 150 million packages via EVs.']","The context states that in Europe, Amazon delivered 150 million packages via EVs in 2023. This directly answers the question. The answer unit is 'packages', so the value is expressed as a number in millions."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation involved 3000 requests, each with 128 input tokens and 512 output tokens. The total number of tokens processed is calculated as 3000 × (128 + 512) = 3000 × 640 = 1,920,000 tokens. This matches the expected unit of 'tokens' as specified in the additional info.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","['• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. The workload evaluates a total of 3000 requests.']","The online inference workload evaluation involved 3000 requests, each with 128 input tokens and 512 output tokens. The total number of tokens processed is calculated as 3000 × (128 + 512) = 3000 × 640 = 1,920,000 tokens. This matches the expected unit of 'tokens' as specified in the additional info."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Multiple sources confirm this omission, including the direct statement that the Act does not mandate disclosure of such emissions. Therefore, the claim that the AI Act mandates disclosure is false.","0","is_blank","[""ebert2024"", ""ebert2024"", ""ebert2024""]","is_blank","['The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [4, 37].', 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [4, 37].', 'The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration [37]) and water consumption [49].']","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Multiple sources confirm this omission, including the direct statement that the Act does not mandate disclosure of such emissions. Therefore, the claim that the AI Act mandates disclosure is false."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region. This is explicitly mentioned in the description of Figure 3, which evaluates the Flexible Start optimization for such short jobs.","80","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3559843""]","up to 80% in West US","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region. This is explicitly mentioned in the description of Figure 3, which evaluates the Flexible Start optimization for such short jobs."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which matches the study's finding that the energy required for image generation exceeds that of text classification by a factor of over 1450.","1450","times","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3640732.3640755""]","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which matches the study's finding that the energy required for image generation exceeds that of text classification by a factor of over 1450."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets""]","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that as of January 2024, Amazon had announced 513 global renewable energy projects, with 244 of them located in the United States. This information is directly provided in the table under the 'United States' row, which lists the number of projects and total MW capacity. The answer unit is 'projects', so the numeric value is returned.","244","projects","[""amazon2023""]","is_blank","United States 244 17,706","The context states that as of January 2024, Amazon had announced 513 global renewable energy projects, with 244 of them located in the United States. This information is directly provided in the table under the 'United States' row, which lists the number of projects and total MW capacity. The answer unit is 'projects', so the numeric value is returned."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide any information about the percentage of renewable energy used by Amazon's AWS in 2018. The earliest data available is from 2022 and 2023, where AWS achieved 100% renewable energy matching for 22 data center regions in 2023. There is no mention of 2018 in the provided documents.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of renewable energy used by Amazon's AWS in 2018. The earliest data available is from 2022 and 2023, where AWS achieved 100% renewable energy matching for 22 data center regions in 2023. There is no mention of 2018 in the provided documents."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting AI energy consumption at the cumulative server level, as this approach captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This level of measurement is supported by the context, which states that reporting at the cumulative server level offers a clearer picture of total energy investment and helps providers improve energy efficiency.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['reported at the cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.']","The authors recommend reporting AI energy consumption at the cumulative server level, as this approach captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This level of measurement is supported by the context, which states that reporting at the cumulative server level offers a clearer picture of total energy investment and helps providers improve energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption. This is explicitly mentioned in Table 1, which provides the electricity consumption in watts and percentages across components, with the GPU at 74%. The answer unit is percent, so the value is expressed as a percentage.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3501561.3501575""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption. This is explicitly mentioned in Table 1, which provides the electricity consumption in watts and percentages across components, with the GPU at 74%. The answer unit is percent, so the value is expressed as a percentage."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. This is explicitly mentioned in the document [ref_id=griggs2024], which provides the exact hardware requirement for this model and precision.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527v4.pdf""]","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. This is explicitly mentioned in the document [ref_id=griggs2024], which provides the exact hardware requirement for this model and precision."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that Facebook achieved a 28.5% operational power footprint reduction over a two-year period due to iterative optimization across the machine learning hardware-software stack. This is explicitly mentioned in the section discussing optimization over time and supported by Figure 8, which illustrates the reduction. The answer unit is 'percent', so the value is expressed as a percentage.","28.5","percent","[""wu2021a""]","[""https://sustainability.fb.com/report-page/data-centers/""]","Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that Facebook achieved a 28.5% operational power footprint reduction over a two-year period due to iterative optimization across the machine learning hardware-software stack. This is explicitly mentioned in the section discussing optimization over time and supported by Figure 8, which illustrates the reduction. The answer unit is 'percent', so the value is expressed as a percentage."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context provided does not contain any information about the installation of fiber optic cable globally to support AI workloads in 2023. There is no mention of kilometers of fiber optic cable or related infrastructure data in the documents.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the installation of fiber optic cable globally to support AI workloads in 2023. There is no mention of kilometers of fiber optic cable or related infrastructure data in the documents."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This figure is directly provided in the text, which compares it to the amortized hardware CapEx + energy cost of $40M. The answer unit is USD, so the value is expressed as a number in that unit.","800000000","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This figure is directly provided in the text, which compares it to the amortized hardware CapEx + energy cost of $40M. The answer unit is USD, so the value is expressed as a number in that unit."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term was not limited to only climate-positive applications, but also includes internal sustainability improvements in AI methods.","0","is_blank","[""luccioni2025b""]","[""https://huggingface.co/blog/sasha/ai-environment-primer""]","['The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].']","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term was not limited to only climate-positive applications, but also includes internal sustainability improvements in AI methods."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that the maximum batch size for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 17, as shown in Table IV. This value is derived from experimental data and is consistent with the analytical model used in the study. The answer is provided in samples, matching the requested unit.","17","samples","[""xia2024""]","is_blank","['TABLE IV ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A100 80GB 17 2.74 1.67 25.4']","The context states that the maximum batch size for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 17, as shown in Table IV. This value is derived from experimental data and is consistent with the analytical model used in the study. The answer is provided in samples, matching the requested unit."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation during restoration, which consumes more energy than the memory operations involved in Swapping. The energy consumption of computation is larger than that of memory operations, making Swapping more energy-efficient under overload conditions.","Swapping","is_blank","[""chung2025""]","is_blank","['It can be seen that when the server is overloaded, Swapping consistently consumes less energy.', 'This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.']","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation during restoration, which consumes more energy than the memory operations involved in Swapping. The energy consumption of computation is larger than that of memory operations, making Swapping more energy-efficient under overload conditions."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","Google reported a 48% increase in GHG emissions since 2019 in its 2024 environmental report, as stated in the context. This information is directly cited from reference [42] in the document, which is consistent with the answer unit of 'percent'.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]","Google reported a 48% increase in GHG emissions since 2019 in its 2024 environmental report, as stated in the context. This information is directly cited from reference [42] in the document, which is consistent with the answer unit of 'percent'."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context provides projections from McKinsey & Company regarding U.S. data center electricity consumption. According to the document [ref_id=chung2025], McKinsey projects that data centers will account for between 9.1% and 11.7% of total U.S. energy demand by 2030. This range is explicitly stated in the text and directly answers the question.","[9.1, 11.7]","percent","[""chung2025""]","[""https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power""]","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024;","The context provides projections from McKinsey & Company regarding U.S. data center electricity consumption. According to the document [ref_id=chung2025], McKinsey projects that data centers will account for between 9.1% and 11.7% of total U.S. energy demand by 2030. This range is explicitly stated in the text and directly answers the question."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the 6 billion parameter language model training run, doubling the duration through the Pause and Resume optimization can lead to significant savings up to about 25%. This is explicitly mentioned in the text: 'for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.' This aligns with the answer unit of 'percent'.","25","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3512486.3512495""]","Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context states that for the 6 billion parameter language model training run, doubling the duration through the Pause and Resume optimization can lead to significant savings up to about 25%. This is explicitly mentioned in the text: 'for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.' This aligns with the answer unit of 'percent'."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is explicitly mentioned in multiple snippets, including the abstract and the detailed description of the model's architecture and performance. The answer unit is 'percent', so the answer is expressed as a number in that unit.","70","percent","[""shen2024"", ""shen2024"", ""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","['reducing inference computation by about 70% compared to Llama2-7B', 'reducing inference computation by about 70% compared to Llama2-7B', 'reducing inference computation by about 70% compared to Llama2-7B']","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is explicitly mentioned in multiple snippets, including the abstract and the detailed description of the model's architecture and performance. The answer unit is 'percent', so the answer is expressed as a number in that unit."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly provided in the text and matches the requested unit of lbs CO2e.","626155","lbs CO2e","[""luccioni2025b""]","[""https://arxiv.org/abs/1906.02243""]","['the seminal 2019 article by Strubell et al. which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].']","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly provided in the text and matches the requested unit of lbs CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly mentioned in multiple snippets, including the section on '3.3.1 Training' and the summary in '3.3 Case Study'. The answer unit is MWh, so the numeric value 1287 is provided accordingly.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly mentioned in multiple snippets, including the section on '3.3.1 Training' and the summary in '3.3 Case Study'. The answer unit is MWh, so the numeric value 1287 is provided accordingly."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context consistently identifies the MoE layer as a critical component in LLM fine-tuning, with multiple statements emphasizing its high execution time and cost. The MoE layer is described as the 'costliest layer' and a 'prime target for optimization' to enhance performance. This directly supports the claim that the MoE layer is often a targeted layer when enhancing performance.","1","is_blank","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693v1""]","['MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.', 'Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.']","The context consistently identifies the MoE layer as a critical component in LLM fine-tuning, with multiple statements emphasizing its high execution time and cost. The MoE layer is described as the 'costliest layer' and a 'prime target for optimization' to enhance performance. This directly supports the claim that the MoE layer is often a targeted layer when enhancing performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations on various GPUs. For Mixtral with a dense setup (Mixtral-D) on the Hellaswag dataset (CS), the maximum batch size on a NVIDIA A40 GPU with 48GB memory is explicitly stated as 2 samples.","2","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations on various GPUs. For Mixtral with a dense setup (Mixtral-D) on the Hellaswag dataset (CS), the maximum batch size on a NVIDIA A40 GPU with 48GB memory is explicitly stated as 2 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for CV tasks with high granularity, intercontinental training only slows performance by 7% compared to local training. This is explicitly mentioned in the summary section, where it notes that even distributing VMs over four continents only slows down performance by 7% for high granularity tasks like CV.","1","is_blank","[""erben2023""]","[""https://arxiv.org/pdf/2306.03163.pdf""]","['In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.']","The context states that for CV tasks with high granularity, intercontinental training only slows performance by 7% compared to local training. This is explicitly mentioned in the summary section, where it notes that even distributing VMs over four continents only slows down performance by 7% for high granularity tasks like CV."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context provided does not contain any information about the land area occupied by new AI data centers globally in 2022. There is no mention of hectares, land use, or physical footprint of data centers in the documents.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the land area occupied by new AI data centers globally in 2022. There is no mention of hectares, land use, or physical footprint of data centers in the documents."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification. This value is directly provided in the text and matches the requested unit of grams of CO2eq.","0.32","g CO2eq","[""luccioni2024""]","[""https://arxiv.org/abs/2406.14066""]","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification. This value is directly provided in the text and matches the requested unit of grams of CO2eq."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that in the experiment analyzing emissions from training a BERT model across different geographic regions, the emissions in the most efficient regions were approximately 7k grams, while the emissions in the least efficient regions were approximately 26k grams. This range is explicitly mentioned in the description of Figure 1, which compares emissions across 16 regions. The answer is provided in thousands of grams as requested.","[7000, 26000]","grams","[""dodge2022""]","is_blank","indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that in the experiment analyzing emissions from training a BERT model across different geographic regions, the emissions in the most efficient regions were approximately 7k grams, while the emissions in the least efficient regions were approximately 26k grams. This range is explicitly mentioned in the description of Figure 1, which compares emissions across 16 regions. The answer is provided in thousands of grams as requested."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures. This directly answers the question about the year when direct environmental information release peaked.","2022","year","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.']","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures. This directly answers the question about the year when direct environmental information release peaked."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the water consumption for training an OLMo 60M model as 1.6 kL, which is equivalent to 5 days of water usage for an average person in the U.S. This information is directly stated in Table 2 of the document.","5","days","[""morrison2025""]","[""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator"", ""https://www.epa.gov/watersense/statistics-and-facts""]","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides the water consumption for training an OLMo 60M model as 1.6 kL, which is equivalent to 5 days of water usage for an average person in the U.S. This information is directly stated in Table 2 of the document."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023. While the documents discuss water consumption in data centers and AI models like GPT-3, there is no mention of AlphaFold or its specific water usage.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023. While the documents discuss water consumption in data centers and AI models like GPT-3, there is no mention of AlphaFold or its specific water usage."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, implying that increased compute resources do not increase costs but rather help lower them. This directly contradicts the claim in the question that adding compute resources increases costs. Therefore, the statement is false.","0","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","['A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.']","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, implying that increased compute resources do not increase costs but rather help lower them. This directly contradicts the claim in the question that adding compute resources increases costs. Therefore, the statement is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total energy consumption for training the FLM-101B model in Table 3, which lists the energy as 40 MkWh. Converting this to kWh gives 40,000,000 kWh. The answer unit is kWh, so the value is expressed as a number in that unit.","40000000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al. 2021). Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b) FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26","The context provides the total energy consumption for training the FLM-101B model in Table 3, which lists the energy as 40 MkWh. Converting this to kWh gives 40,000,000 kWh. The answer unit is kWh, so the value is expressed as a number in that unit."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context states that the total public health costs of U.S. data centers could reach more than $20 billion in 2028, which is the closest projection to 2030 mentioned in the documents. Although 2030 is not explicitly referenced, 2028 is the most recent projection provided, and the value of $20 billion is the upper bound cited for that year. No higher value is mentioned for any year beyond 2028.","20","USD","[""han2024""]","is_blank","['The overall public health costs could reach more than$20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with∼35 million registered vehicles [41].']","The context states that the total public health costs of U.S. data centers could reach more than $20 billion in 2028, which is the closest projection to 2030 mentioned in the documents. Although 2030 is not explicitly referenced, 2028 is the most recent projection provided, and the value of $20 billion is the upper bound cited for that year. No higher value is mentioned for any year beyond 2028."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. The cost for the A40 GPU with 48GB memory is listed as $32.7 for 10 epochs. Although the GSM8K dataset is not explicitly mentioned in the table, the cost estimation model is based on dataset size and model parameters, and the GSM8K dataset is referenced in the context as a common benchmark. Given that the cost for the A40 GPU is directly provided in the table for a similar setup, and the question asks for the cost of fine-tuning on the GSM8K dataset, the cost is inferred to be the same as listed in Table IV for the A40 GPU.","32.7","USD","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. The cost for the A40 GPU with 48GB memory is listed as $32.7 for 10 epochs. Although the GSM8K dataset is not explicitly mentioned in the table, the cost estimation model is based on dataset size and model parameters, and the GSM8K dataset is referenced in the context as a common benchmark. Given that the cost for the A40 GPU is directly provided in the table for a similar setup, and the question asks for the cost of fine-tuning on the GSM8K dataset, the cost is inferred to be the same as listed in Table IV for the A40 GPU."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The maximum batch size supported by a sparse Mixtral model fine-tuned on a NVIDIA A40 GPU with 48GB memory is 8 samples, as shown in Table III of the context. This value represents the longest-running MoE layer batch size under the given hardware and model configuration.","8","samples","[""xia2024""]","is_blank","['TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8']","The maximum batch size supported by a sparse Mixtral model fine-tuned on a NVIDIA A40 GPU with 48GB memory is 8 samples, as shown in Table III of the context. This value represents the longest-running MoE layer batch size under the given hardware and model configuration."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides execution time breakdown for sparse Mixtral fine-tuning with a batch size of 10 on an NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for sparse Mixtral with batch size 10, where the forward, backward, and optimizer stages are detailed. The total execution time is the sum of these components. From the figure, the forward stage takes approximately 2.0 seconds, the backward stage takes approximately 4.0 seconds, and the optimizer stage takes approximately 0.0 seconds. Adding these gives a total of 6.0 seconds.","6.0","seconds","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","Fig. 4. Execution time breakdown (seconds) for Sparse(bsz=10) Mixtral on A40 GPU: Forward ~2.0, Backward ~4.0, Optimizer ~0.0. Total: ~6.0 seconds.","The context provides execution time breakdown for sparse Mixtral fine-tuning with a batch size of 10 on an NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for sparse Mixtral with batch size 10, where the forward, backward, and optimizer stages are detailed. The total execution time is the sum of these components. From the figure, the forward stage takes approximately 2.0 seconds, the backward stage takes approximately 4.0 seconds, and the optimizer stage takes approximately 0.0 seconds. Adding these gives a total of 6.0 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements. It argues that while GPU power consumption is a significant factor, it substantially under-represents actual energy consumption because it measures only a single component. Therefore, using GPU-level monitoring is discouraged for comprehensive energy reporting.","0","is_blank","[""ebert2024""]","[""https://www.example.com/ebert2024""]","['We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.']","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements. It argues that while GPU power consumption is a significant factor, it substantially under-represents actual energy consumption because it measures only a single component. Therefore, using GPU-level monitoring is discouraged for comprehensive energy reporting."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions attributed to training.","50","percent","[""luccioni2025b""]","[""https://www.acm.org/publications/proceedings-of-the-2023-facc-t-conference""]","training accounted for only half of the model’s overall emissions","The context states that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions attributed to training."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 V100 GPU with 32GB of RAM, as shown in Table II. This configuration allows inference without compression or quantization. The answer matches the required unit of 'V100_32GB_GPU'.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 V100 GPU with 32GB of RAM, as shown in Table II. This configuration allows inference without compression or quantization. The answer matches the required unit of 'V100_32GB_GPU'."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. Since the question asks about training in Arizona, which is a specific location, the range of 10 to 50 medium-length responses applies directly. The answer unit is 'requests', so the answer is expressed as a numeric range.","[10,50]","requests","[""li2025b""]","is_blank","['GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.']","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. Since the question asks about training in Arizona, which is a specific location, the range of 10 to 50 medium-length responses applies directly. The answer unit is 'requests', so the answer is expressed as a numeric range."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year, based on the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil. This directly answers the question about the estimated CO2 emission for one year of average US home energy use.","8.3","metric tons","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3559683""]","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context states that one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year, based on the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil. This directly answers the question about the estimated CO2 emission for one year of average US home energy use."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides the performance of JetMoE-8B on the GSM8k benchmark, which evaluates grade school math problem-solving. According to Table 3, JetMoE-8B achieved a score of 27.8 on GSM8k, which is higher than the scores of other models like Gemma, LLaMA2, and DeepseekMoE.","27.8","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B obtains the highest MBPP scores in Python programming. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. GSM8k 14.5 17.3 16.9 27.8","The context provides the performance of JetMoE-8B on the GSM8k benchmark, which evaluates grade school math problem-solving. According to Table 3, JetMoE-8B achieved a score of 27.8 on GSM8k, which is higher than the scores of other models like Gemma, LLaMA2, and DeepseekMoE."
