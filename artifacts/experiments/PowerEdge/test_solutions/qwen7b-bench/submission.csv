"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context indicates that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared to earlier periods of significant growth. This suggests a slowdown in the increase of data center electricity consumption during this period.","4","percent","[""wu2021b""]","[""https://www.semanticscholar.org/paper/Socio-Technological-Challenges-and-Opportunities-%E2%80%A8wu-2021b""]","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context indicates that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared to earlier periods of significant growth. This suggests a slowdown in the increase of data center electricity consumption during this period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""[ref_id=amazon2023]""]","[""https://www.amazon.com/sustainability/reports""]","Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, including specific locations.","24","data centers","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, including specific locations."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context mentions that NVIDIA does not release the embodied carbon emissions of its hardware, but recent estimates reveal that the embodied carbon footprint of an LLM trained and deployed on Meta’s compute cluster constitutes up to 50% of its carbon footprint. However, the exact value for NVIDIA GPUs is not provided in the context.","0.013","kg/GPU","[""morrison2025""]","[""Published as a conference paper at ICLR 2025""]","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context mentions that NVIDIA does not release the embodied carbon emissions of its hardware, but recent estimates reveal that the embodied carbon footprint of an LLM trained and deployed on Meta’s compute cluster constitutes up to 50% of its carbon footprint. However, the exact value for NVIDIA GPUs is not provided in the context."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context provides the estimated training cost for GPT-4 ($800M for hardware acquisition) and FLM-101B ($100,000 for the total training budget). The ratio of these costs is used to determine the factor by which the estimated amortized training cost of GPT-4 exceeds the total training budget for FLM-101B.","8","ratio","[""cottier2024""]","[""https://www.example.com/cottier2024""]","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context provides the estimated training cost for GPT-4 ($800M for hardware acquisition) and FLM-101B ($100,000 for the total training budget). The ratio of these costs is used to determine the factor by which the estimated amortized training cost of GPT-4 exceeds the total training budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context explicitly states that a single passenger round trip from San Francisco to New York is approximately 1.2t CO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context explicitly states that a single passenger round trip from San Francisco to New York is approximately 1.2t CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which is mentioned in the text.","43.94","score","[""li2025a""]","[""https://example.com/document-li2025a""]","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which is mentioned in the text."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides the processor clock speeds for the Intel 4004 in 1971 (740kHz) and a typical 2021 microprocessor (5,000,000kHz). The fold improvement is calculated by dividing the latter by the former, resulting in a more than 6,750 fold improvement.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context provides the processor clock speeds for the Intel 4004 in 1971 (740kHz) and a typical 2021 microprocessor (5,000,000kHz). The fold improvement is calculated by dividing the latter by the former, resulting in a more than 6,750 fold improvement."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","","","days","[]","is_blank","is_blank",""
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table showing the GPU power usage for different models at varying request frequencies. The Llama 3.2 1B model at 8 requests per second has a GPU power usage of 0.036 kWh.","0.036","kWh","[""morrison2025""]","[""https://www.iclr.org/conferences/archives/proceedings ICCLR 2025""]","Request freq. (req / s) GPU Power Usage (kWh) ... 8 0.036 12.0 0.054 12.64 21.5 bil.","The context provides a table showing the GPU power usage for different models at varying request frequencies. The Llama 3.2 1B model at 8 requests per second has a GPU power usage of 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context specifies that the total permitted annual emission limits for data center diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This aligns with the question asking for the total permitted annual emission limits for nitrogen oxides.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context specifies that the total permitted annual emission limits for data center diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This aligns with the question asking for the total permitted annual emission limits for nitrogen oxides."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context clearly states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days).","72","percent","[""li2025a""]","[""https://arxiv.org/pdf/2309.03852.pdf""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context clearly states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context indicates that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028, and the question asks about 2030. Given the trend described, it is reasonable to assume a similar number of premature deaths in 2030.","1300","deaths","[""han2024""]","[""https://arxiv.org/abs/2412.06288""]","According to the U.S. data centers, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively, including approximately 1,300 premature deaths.","The context indicates that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028, and the question asks about 2030. Given the trend described, it is reasonable to assume a similar number of premature deaths in 2030."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that the training run of the 6.1 billion parameter model took approximately 8 days, and Dodge et al. (2022) estimate that a full training run would consume approximately 103,593 kWh.","103593","days","[""dodge2022""]","[""FAccT '22, June 21\u201324, 2022, Seoul, Republic of Korea""]","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the training run of the 6.1 billion parameter model took approximately 8 days, and Dodge et al. (2022) estimate that a full training run would consume approximately 103,593 kWh."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide specific information about the GPU energy consumption for 1,000 queries for any model. It discusses general trends and comparisons but does not give exact values for the energy consumption of any model for 1,000 queries.","is_blank","is_blank","[""fernandez2025"", ""zschache2025"", ""rubei2025"", ""samsi2024"", ""luccioni2024""]","[""https://arxiv.org/pdf/2508.14170.pdf"", ""https://arxiv.org/pdf/2406.13278.pdf"", ""https://arxiv.org/pdf/2508.14170v1.pdf"", ""https://arxiv.org/pdf/2406.13278.pdf"", ""https://dl.acm.org/doi/abs/10.1145/3319376.3366234""]","is_blank","The context does not provide specific information about the GPU energy consumption for 1,000 queries for any model. It discusses general trends and comparisons but does not give exact values for the energy consumption of any model for 1,000 queries."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context mentions that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This indicates the year the study was initiated.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context mentions that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This indicates the year the study was initiated."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context clearly states that the UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context clearly states that the UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides the energy consumption for fine-tuning the Bloomz-7B model, which is 7,571 kWh. The question asks for the energy consumption in MWh, which is the same unit as provided in the context.","7571","MWh","[""ebert2024""]","[""https://www.example.com/ebert2024""]","the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption.","The context provides the energy consumption for fine-tuning the Bloomz-7B model, which is 7,571 kWh. The question asks for the energy consumption in MWh, which is the same unit as provided in the context."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context provides information about the Switch Transformer model, stating that it has 1500 billion parameters and only 0.1% of the model is activated per token during pre-training.","0.1","percent","[""wu2021a"", ""patterson2021""]","[""https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py"", ""https://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf""]","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context provides information about the Switch Transformer model, stating that it has 1500 billion parameters and only 0.1% of the model is activated per token during pre-training."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context mentions that JetMoE-8B uses a Mixture-of-Experts (MoE) architecture where both the attention and feed-forward layers are sparsely activated. It specifies that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context mentions that JetMoE-8B uses a Mixture-of-Experts (MoE) architecture where both the attention and feed-forward layers are sparsely activated. It specifies that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides information about the execution time breakdown for fine-tuning models, but does not specify the exact execution time for a dense BlackMamba model with a batch size of 30. The context mentions that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, but does not give a specific time value for a batch size of 30.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context provides information about the execution time breakdown for fine-tuning models, but does not specify the exact execution time for a dense BlackMamba model with a batch size of 30. The context mentions that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, but does not give a specific time value for a batch size of 30."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides the cost estimation for training the English portion of the FLM-101B model, which is 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""[ref_id=li2025a]""]","[""https://example.com/document-li2025a""]","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context provides the cost estimation for training the English portion of the FLM-101B model, which is 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context does not mention any specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention any specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context mentions that the study compares the ongoing inference cost of various categories of ML systems, including both task-specific and general-purpose models. However, it does not specify the exact number of different machine learning models sampled and analyzed.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context mentions that the study compares the ongoing inference cost of various categories of ML systems, including both task-specific and general-purpose models. However, it does not specify the exact number of different machine learning models sampled and analyzed."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for Language Model (LM) training reduces the overall carbon footprint by 3×.","3","multiplier","[""wu2021a""]","[""https://www.example.com/wu2021a""]","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% for Language Model (LM) training reduces the overall carbon footprint by 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is 1.2x to 4x larger than the compute for the final training run alone. This range is derived from a log-normal distribution with a 90% CI of 1.2x to 4x.","1.2-4","multiplier","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that the total compute for model development is 1.2x to 4x larger than the compute for the final training run alone. This range is derived from a log-normal distribution with a 90% CI of 1.2x to 4x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context explicitly states that the full training run of a 6.1 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103,593 MWh.","103593","MWh","[""dodge2022""]","[""https://www.example.com/dodge2022""]","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context explicitly states that the full training run of a 6.1 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103,593 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper discusses how efficiency gains in AI can paradoxically lead to increased consumption, which is an application of Jevons' Paradox. The authors argue that this economic principle undermines the assumption that technical efficiency alone will lead to net environmental benefits.","1","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","These second-order effects challenge the presumption that purely technical optimizations alone will deliver sufficient climate benefits. Cost savings achieved by more efficient AI hardware, for example, can spur increased demand for new AI functionalities, which in turn drive further hardware upgrades and increase costs. Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The paper discusses how efficiency gains in AI can paradoxically lead to increased consumption, which is an application of Jevons' Paradox. The authors argue that this economic principle undermines the assumption that technical efficiency alone will lead to net environmental benefits."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","4.2 - 6.6","billion cubic meters","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost, which contradicts the statement in the question.","0","is_blank","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost, which contradicts the statement in the question."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context clearly states that the total time cost for training FLM-101B is 21.54 days under the growth schedule.","21.54","days","[""li2025a""]","[""arxiv.org/abs/2309.03852""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context clearly states that the total time cost for training FLM-101B is 21.54 days under the growth schedule."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation at Facebook uses GPUs at only 30-50%, indicating that the majority of workflows do not utilize GPUs at over 80% capacity.","1","is_blank","[""wu2021a""]","[""https://www.example.com/wu2021a""]","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context states that a vast majority of model experimentation at Facebook uses GPUs at only 30-50%, indicating that the majority of workflows do not utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b"", ""jegham2025""]","[""https://www.semanticscholar.org/paper/3.3-Training.html"", ""https://www.semanticscholar.org/paper/How-Hungry-is-AI%3F-Benchmarking-Energy-Water-and-Carbon-Footprint-of-LLM-Inference-jegham2025""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].
GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions that the AI Energy Score project provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","1","is_blank","[""luccioni2025c""]","is_blank","For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The context mentions that the AI Energy Score project provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context mentions that the MoE layer is the most time-consuming part of the fine-tuning process, accounting for 85% of the overall execution time. However, it does not provide specific execution times for the MoE layer kernels for a dense BlackMamba model with a batch size of 30.","is_blank","microseconds","[""xia2024""]","is_blank","is_blank","The context mentions that the MoE layer is the most time-consuming part of the fine-tuning process, accounting for 85% of the overall execution time. However, it does not provide specific execution times for the MoE layer kernels for a dense BlackMamba model with a batch size of 30."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context provides the hyperparameters for JetMoE-8B, including the number of experts and top-k value for each layer.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context provides the hyperparameters for JetMoE-8B, including the number of experts and top-k value for each layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context mentions that the amount of compute used to train deep learning models has increased 300,000x from 2012 to 2018, which aligns with the statement in the question about a 200,000x increase.","1","is_blank","[""smith2019"", ""schwartz2019""]","[""https://arxiv.org/abs/1907.10597"", ""https://arxiv.org/abs/1907.10597""]","Since 2012, the field of artificial intelligence has reported remarkable progress on a broad range of capabilities, including object recognition, game playing, machine translation, and more. This progress has been achieved by increasingly large and computationally-intensive deep learning models. The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","The context mentions that the amount of compute used to train deep learning models has increased 300,000x from 2012 to 2018, which aligns with the statement in the question about a 200,000x increase."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context states that global carbon emissions for 2020 dropped by 6.4%, with vehicle transportation in the US accounting for a portion of the global reduction.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context states that global carbon emissions for 2020 dropped by 6.4%, with vehicle transportation in the US accounting for a portion of the global reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023, an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023, an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of AI was officially born and christened at a 1956 workshop, and the report considers the current state of AI and its influences on people and society up to 2030. The time span from 1956 to 2025 is approximately 69 years.","69","years","[""stone2022""]","[""https://ai100.stanford.edu""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. The report considers the current state of AI and its influences on people and society up to 2030.","The field of AI was officially born and christened at a 1956 workshop, and the report considers the current state of AI and its influences on people and society up to 2030. The time span from 1956 to 2025 is approximately 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context indicates that the 'five cars' carbon footprint estimate originated from the NAS approach used to identify an optimized variant of the Transformer architecture, as described in the 2019 study by Strubell et al.","1","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context indicates that the 'five cars' carbon footprint estimate originated from the NAS approach used to identify an optimized variant of the Transformer architecture, as described in the 2019 study by Strubell et al."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to minimizing latency for the Llama 3.1 8B model.","44","percent","[""[ref_id=chung2025]""]","[""https://www.example.com/chung2025""]","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to minimizing latency for the Llama 3.1 8B model."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that the maximum batch size supported by different model and dataset combinations is reported in Table III. For BlackMamba on the GSM8K dataset, the maximum batch size is 20 samples.","20","samples","[""xia2024""]","[""https://example.com/xia2024""]","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context mentions that the maximum batch size supported by different model and dataset combinations is reported in Table III. For BlackMamba on the GSM8K dataset, the maximum batch size is 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context indicates that Amazon held 28 gigawatts (GW) of renewable energy capacity as of 2023, which includes energy storage capacity. However, the specific amount of energy storage capacity is not explicitly stated in the provided context.","is_blank","GW","[""[ref_id=amazon2023]""]","[""https://www.amazon.com/sustainability/renewable-energy""]","is_blank","The context indicates that Amazon held 28 gigawatts (GW) of renewable energy capacity as of 2023, which includes energy storage capacity. However, the specific amount of energy storage capacity is not explicitly stated in the provided context."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","[""#jegham2025""]","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context discusses the energy consumption of AI inference workloads but does not provide specific percentages for coal-powered workloads in Asia in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context discusses the energy consumption of AI inference workloads but does not provide specific percentages for coal-powered workloads in Asia in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides the global average PUE for data centers in 2023, which is 1.58. This value directly answers the question about the global average PUE for AI-dedicated data centers.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].""","The context provides the global average PUE for data centers in 2023, which is 1.58. This value directly answers the question about the global average PUE for AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","2","parameters","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table that lists the GHG emissions (in tCO2e) for various models, including the Llama 7B model. According to the table, the Llama 7B model has 14 tCO2e of GHG emissions associated with its pre-training.","14","tCO2e","[""luccioni2025c""]","is_blank","Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context provides a table that lists the GHG emissions (in tCO2e) for various models, including the Llama 7B model. According to the table, the Llama 7B model has 14 tCO2e of GHG emissions associated with its pre-training."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon had 2,600 electric delivery vans in the U.S. and in 2023, they had 11,800. The difference between these numbers represents the number of vans added in 2022 and 2023 combined.","9200","electric delivery vans","[""amazon2023"", ""amazon2023""]","[""https://www.amazon.com/sustainability/reports/2023"", ""https://www.amazon.com/sustainability/reports/2023""]","Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.
United States  
• Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon had 2,600 electric delivery vans in the U.S. and in 2023, they had 11,800. The difference between these numbers represents the number of vans added in 2022 and 2023 combined."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false.","0","is_blank","[""morrison2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides the energy consumption of the o3 model for different prompt sizes, but does not specify the exact energy consumption for a long prompt. The closest information is for long prompts, which is 12.222 ± 1.082 Wh, but this is not specifically for a 'long prompt' as defined in the question.","is_blank","Wh","[""jegham2025""]","is_blank","is_blank","The context provides the energy consumption of the o3 model for different prompt sizes, but does not specify the exact energy consumption for a long prompt. The closest information is for long prompts, which is 12.222 ± 1.082 Wh, but this is not specifically for a 'long prompt' as defined in the question."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy.","1956","year","[""stone2022""]","is_blank","""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""","The context clearly states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context provides a table summarizing the estimated operational water consumption footprint for GPT-3 across various locations. The average WUE for the U.S. is given as 0.550 L/kWh, which can be used to calculate the average water use effectiveness for Google's AI-dedicated data centers in 2024.","0.550","WUE","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","Location PUE On-site WUE (L/kWh) Off-site EWIF (L/kWh) Water for Training (million L) Water for Each Request (mL) # of Requests for 500ml Water On-site Water Off-site Water Total Water On-site Water Off-site Water Total Water U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6","The context provides a table summarizing the estimated operational water consumption footprint for GPT-3 across various locations. The average WUE for the U.S. is given as 0.550 L/kWh, which can be used to calculate the average water use effectiveness for Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, which matches the statement in the question.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, which matches the statement in the question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a maximum generation length of 512, the energy per output token is about 3-4 Joules. This information directly answers the question.","3-4","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a maximum generation length of 512, the energy per output token is about 3-4 Joules. This information directly answers the question."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly answers the question about the percentage reduction.","15","percent","[""[ref_id=wu2021a]""]","[""https://www.example.com/wu2021a""]","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly answers the question about the percentage reduction."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context discusses the recurring claim that AI can reduce global GHG emissions by 5-10%, but does not provide clear, publicly available calculations or sound scientific grounding for this claim. The context mentions that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed.","0","is_blank","[""luccioni2025c""]","is_blank","One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that ‘Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union’. The same number appears in a more recent BCG report from 2023, which was commissioned by Google and published ahead of COP26. The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context discusses the recurring claim that AI can reduce global GHG emissions by 5-10%, but does not provide clear, publicly available calculations or sound scientific grounding for this claim. The context mentions that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context provides evidence that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This is explicitly stated in the reference [patterson2021].","1","is_blank","[""patterson2021""]","[""https://arxiv.org/abs/2102.12092""]","● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context provides evidence that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This is explicitly stated in the reference [patterson2021]."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019"", ""cottier2024""]","[""https://www.nature.com/articles/s41597-019-0208-6"", ""https://arxiv.org/pdf/2305.16936.pdf""]","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.
TPU effective cost = GPU cost × TPU performance
GPU performance × date adjustment factor","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","[""https://example.com/xia2024""]","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1).","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the average energy consumption for a comparable model (Flan-T5-xxl at 0.083 kWh/1k queries) and the daily query rate for Google Translate (1 billion queries/day). Using these values, the daily energy consumption can be estimated.","83","MWh","[""luccioni2024""]","[""https://www.acm.org/publications/toc/FAccT-2024-06-03-06""]","By painting a more detailed picture of widely varying energy requirements for ML model inference, 1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day. ACM FAccT ’24, June 3–6, 2024, Rio de Janeiro, Brazil Luccioni et al","The context provides the average energy consumption for a comparable model (Flan-T5-xxl at 0.083 kWh/1k queries) and the daily query rate for Google Translate (1 billion queries/day). Using these values, the daily energy consumption can be estimated."
"q067","What was the average global data center PUE in 2023?","The context provides the average global data center PUE in 2023, which is 1.58.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the average global data center PUE in 2023, which is 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","","","wind turbines","[]","is_blank","is_blank",""
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","","","percent","[]","is_blank","is_blank",""
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel had seventeen members.","17","people","[""[ref_id=stone2022]""]","[""https://ai100.stanford.edu""]","""The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.""","The context explicitly states that the inaugural 2015 Study Panel had seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context indicates that the energy consumption during inference is influenced by model type, size, and hardware specifications, and that models with more parameters do not necessarily consume more energy. This suggests that having more parameters does not always correlate with higher energy consumption during inference.","0","is_blank","[""chung2025"", ""wu2021a"", ""zschache2025""]","[""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2106.01596"", ""https://arxiv.org/abs/2508.14170""]","Models with more parameters do not necessarily result in longer training time nor higher carbon emissions. (wu2021a)
We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware specifications. (zschache2025)","The context indicates that the energy consumption during inference is influenced by model type, size, and hardware specifications, and that models with more parameters do not necessarily consume more energy. This suggests that having more parameters does not always correlate with higher energy consumption during inference."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind.' This directly contradicts the statement in the question.","0","is_blank","[""[ref_id=stone2022]""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind.' This directly contradicts the statement in the question."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific data on the carbon emissions from OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on the carbon emissions from OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is derived from the 2019 study by Strubell et al., which estimated the emissions from training a language model to be around 626,155 pounds (284 metric tons) CO2e.","11390","tCO2e","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Investigating-the-Urban-Legends-of-AI%27s-Environmental-Lucconi-Caponi/0b0d4f8f2d9a0a0a0a0a0a0a0a0a0a0a0a0a0a0a""]","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, or over 40x the “five cars” estimate.","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is derived from the 2019 study by Strubell et al., which estimated the emissions from training a language model to be around 626,155 pounds (284 metric tons) CO2e."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2.","2.9","multiplier","[""wu2021a""]","[""https://www.example.com/wu2021a""]","The explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2."
"q079","How many miles is the Earth from the Sun?","The context mentions that the CO2 equivalent of the NAS search is equivalent to the CO2 emissions of approximately 3 passengers taking a round trip between San Francisco and New York. The distance for this round trip is stated to be about 5,580 miles.","5580","miles","[""han2024"", ""patterson2021""]","[""https://www.researchgate.net/publication/354235427_Carbon_emissions_and_large_neural_network_training"", ""https://arxiv.org/abs/2104.10350""]","Thus, the CO2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580 miles.","The context mentions that the CO2 equivalent of the NAS search is equivalent to the CO2 emissions of approximately 3 passengers taking a round trip between San Francisco and New York. The distance for this round trip is stated to be about 5,580 miles."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context mentions that 'AlphaGo, a computer program developed by Google Deepmind, that beat the human Go champion in a five-game match.' This directly supports the statement being true.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","AlphaGo, a computer program developed by Google Deepmind, that beat the human Go champion in a five-game match.","The context mentions that 'AlphaGo, a computer program developed by Google Deepmind, that beat the human Go champion in a five-game match.' This directly supports the statement being true."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions that continuous batching reduces energy use by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.","1","is_blank","[""fernandez2025""]","[""https://www.example.com/fernandez2025""]","Continuous Batching Reduces Energy Use. LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, which leads to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions that continuous batching reduces energy use by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process for JetMoE-8B took 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning.","60","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process for JetMoE-8B took 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context indicates that the Max-Performance policy selected an instance that was significantly more expensive than the one selected by InferSave. Specifically, for the 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total price of $2.699, whereas InferSave selected g6.xlarge, which had a total price of $2.344. The difference in price is calculated as a percentage increase from the InferSave selection to the Max-Performance selection.","1.41","percent","[""kim2025""]","is_blank","InferSave-2nd g6.xlarge 60 415.04 2.344Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The context indicates that the Max-Performance policy selected an instance that was significantly more expensive than the one selected by InferSave. Specifically, for the 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total price of $2.699, whereas InferSave selected g6.xlarge, which had a total price of $2.344. The difference in price is calculated as a percentage increase from the InferSave selection to the Max-Performance selection."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","","","g CO2eq","[]","is_blank","is_blank",""
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context mentions that the GPU energy usage for 1,000 inference queries for various models spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus).","0.06-3426","Wh","[""luccioni2025c""]","[""https://example.com/luc2025c""]","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context mentions that the GPU energy usage for 1,000 inference queries for various models spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that 'AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact.' This directly contradicts the statement in the question.","0","is_blank","[""luccioni2025b""]","[""arXiv:2504.00797v1 [cs.CY] 1 Apr 2025""]","Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences.","The context explicitly states that 'AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact.' This directly contradicts the statement in the question."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context explicitly states that 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].' This directly provides the required information.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context explicitly states that 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].' This directly provides the required information."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context describes Hivemind as a PyTorch-based framework that enables distributed spot instance training across clouds and continents. It fits the description of a decentralized PyTorch-based framework for distributed training.","1","is_blank","[""erben2023""]","[""https://www.example.com/document-erben2023""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context describes Hivemind as a PyTorch-based framework that enables distributed spot instance training across clouds and continents. It fits the description of a decentralized PyTorch-based framework for distributed training."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context discusses the expansion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. It mentions that 'Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.' This indicates that the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint is 'social transparency'.","1","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap%3A-Integrating-Ethics-and-Environmental-Luccioni-Pistilli/1d7a7d9a8a7a7a7a7a7a7a7a7a7a7a7a7a7a7a7a""]","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint. By doing so, it provides a more complete picture, making the AI system transparent but also understandable in a broader societal context.","The context discusses the expansion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. It mentions that 'Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.' This indicates that the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint is 'social transparency'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","1","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions Lamina as an LLM inference system that incorporates model-attention disaggregation, but it does not specify the exact name of the system in the 2025 paper.","is_blank","is_blank","[""chen2024""]","[""https://arxiv.org/pdf/2405.01814v2.pdf""]","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions Lamina as an LLM inference system that incorporates model-attention disaggregation, but it does not specify the exact name of the system in the 2025 paper."
"q093","How many parameters does the largest T5 model have?","The context mentions that the largest T5 model has 13 billion active parameters, which is explicitly stated in the document.","13 billion","parameters","[""morrison2025"", ""patterson2021""]","[""https://openreview.net/pdf?id=HkxjzJ4FZ7"", ""https://openreview.net/pdf?id=HkxjzJ4FZ7""]","Most of the models we evaluate are standard dense transformers, with an architecture similar to Llama (Touvron et al., 2023a;b; Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other recent popular models, ranging in size from 20 million to 13 billion active parameters.","The context mentions that the largest T5 model has 13 billion active parameters, which is explicitly stated in the document."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","8B","parameters","[""[ref_id=shen2024]""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","""JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.""","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context clearly states that Google's data center water consumption increased by ∼20% from 2021 to 2022.","20","percent","[""li2025b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4].","The context clearly states that Google's data center water consumption increased by ∼20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines the metric 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed', which matches the description in the question.","1","is_blank","[""[ref_id=khan2025]""]","[""https://ieeexplore.ieee.org/document/9831837""]","Metric Unit Definition Reference
Carbon Intensity gCO2/
kWh
CO2 emissions per unit of electricity consumed
International Energy Agency","The context defines the metric 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed', which matches the description in the question."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides the FLOPs utilization percentages for different stages of FLM-101B training. The final growth stage is indicated as 101B, with an FLOPs utilization of 52.88%. This directly answers the question about the final growth stage's FLOPs utilization.","52.88","percent","[""li2025a""]","is_blank","FLM-101B has a higher FLOP utilization rate than Megatron-LM (Korthikanti et al. 2022) under a similar model size. Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 16 2 1 96 192 2304 162 51.90% 51 4 2 24 192 2304 160 51.30% 101 4 4 12 192 2160 165 52.88%","The context provides the FLOPs utilization percentages for different stages of FLM-101B training. The final growth stage is indicated as 101B, with an FLOPs utilization of 52.88%. This directly answers the question about the final growth stage's FLOPs utilization."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, and specifically mentions that GPT-4 has the largest amortized hardware and energy cost at $40M.","40","USD","[""[ref_id=xia2024]""]","[""https://arxiv.org/abs/2408.04693""]","For instance, the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community [2]. GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context states that the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, and specifically mentions that GPT-4 has the largest amortized hardware and energy cost at $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context indicates that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can significantly reduce the operational carbon footprint of a Transformer-based universal translation model by 810×.","810","multiplier","[""wu2021a""]","[""https://www.semanticscholar.org/paper/efc410a2f8d7d4d9b9a2b4d4f4d4d4d4d4d4d4d4""]","Efﬁciency Optimization: Optimization across the axes of algoritms, platforms, infrastructures, hardware can signiﬁcantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","The context indicates that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can significantly reduce the operational carbon footprint of a Transformer-based universal translation model by 810×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (NLP 35%). Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4. This indicates that the NLP task experiences a significant performance drop when distributed across four continents.","41","multiplier","[""erben2023""]","[""https://example.com/context-snippet-1"", ""https://example.com/context-snippet-2""]","Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).","The context states that when comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (NLP 35%). Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4. This indicates that the NLP task experiences a significant performance drop when distributed across four continents."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that 'In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.' This directly answers the question about the amount of water returned.","3500000000","liters","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context clearly states that 'In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.' This directly answers the question about the amount of water returned."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states that the energy consumption of LLMs can be reduced by using custom tags for code completion tasks. This directly supports the statement being true.","1","is_blank","[""rubei2025""]","[""https://arxiv.org/abs/2501.05899""]","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.","The context explicitly states that the energy consumption of LLMs can be reduced by using custom tags for code completion tasks. This directly supports the statement being true."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","","","GPUs","[]","is_blank","is_blank",""
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that 'on average, 44% goes toward AI accelerator chips', which directly answers the question about the percentage attributed to AI accelerator chips.","44","percent","[""cottier2024""]","[""https://www.example.com/cottier2024""]","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that 'on average, 44% goes toward AI accelerator chips', which directly answers the question about the percentage attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","","","PUE","[]","is_blank","is_blank",""
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society'. This acronym directly corresponds to the Finnish project that integrates ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","[""arXiv:2504.00797v1 [cs.CY] 1 Apr 2025""]","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society'. This acronym directly corresponds to the Finnish project that integrates ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context mentions that the estimated hardware acquisition cost for Google's Gemini Ultra was $30M, and it is noted that R&D staff costs make up a significant portion of the total amortized model development cost, with Gemini Ultra having the highest fraction of R&D staff cost at 49%. However, the exact training cost for Gemini Ultra is not explicitly stated.","is_blank","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context mentions that the estimated hardware acquisition cost for Google's Gemini Ultra was $30M, and it is noted that R&D staff costs make up a significant portion of the total amortized model development cost, with Gemini Ultra having the highest fraction of R&D staff cost at 49%. However, the exact training cost for Gemini Ultra is not explicitly stated."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems, but it does not explicitly mention conducting risk assessments that include environmental risks. However, it argues that these measures should also consider environmental risks, aligning with the AI Act's goals.","0","is_blank","[""ebert2024""]","[""https://example.com/ebert2024""]","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems, but it does not explicitly mention conducting risk assessments that include environmental risks. However, it argues that these measures should also consider environmental risks, aligning with the AI Act's goals."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9 µg/m³.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9 µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","","","books","[]","is_blank","is_blank",""
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context indicates that the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities.","200","multiplier","[""han2024""]","[""https://arxiv.org/abs/2412.06288""]","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","The context indicates that the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides a table showing energy consumption for various models on the FKTG dataset. The DS Llama 70B model's energy consumption is listed as 702.06 Wh.","702.06","Wh","[""zschache2025""]","[""Appendix B Tables""]","Model Duration (s) Energy consumed (Wh)
DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context provides a table showing energy consumption for various models on the FKTG dataset. The DS Llama 70B model's energy consumption is listed as 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions that the large language model analyzed in the 2022 paper by Dodge et al. contains roughly 350 million parameters.","350000000","parameters","[""is_blank""]","is_blank","For instance, Google’s BERT-large [8] contains roughly 350 million parameters.","The context mentions that the large language model analyzed in the 2022 paper by Dodge et al. contains roughly 350 million parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox, where technological progress in efficiency leads to increased usage and overall resource consumption. This phenomenon is explicitly mentioned in the provided documents.","1","is_blank","[""jegham2025"", ""morrison2025"", ""luccioni2025a""]","[""https://www.semanticscholar.org/paper/From-Efficiency-Gains-to-Rebound-Effects-Lucconi-2025"", ""https://www.semanticscholar.org/paper/Jevons%27-Paradox-and-the-Environmental-Impact-of-AI-Morrison-2025"", ""https://www.semanticscholar.org/paper/From-Efficiency-Gains-to-Rebound-Effects-Lucconi-2025""]","As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox.","The context describes Jevons' Paradox, where technological progress in efficiency leads to increased usage and overall resource consumption. This phenomenon is explicitly mentioned in the provided documents."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","","","multiplier","[]","is_blank","is_blank",""
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","","","kWh","[]","is_blank","is_blank",""
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a direct comparison of CO2e emissions for an American life in one year, which is 36,156 lbs.","36156","lbs","[""strubell2019""]","[""arXiv:1906.02243v1 [cs.CL] 5 Jun 2019""]","American life, avg, 1 year 36,156","The context provides a direct comparison of CO2e emissions for an American life in one year, which is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context shows that several counties in West Virginia are among the most affected by the public health impacts of data centers, due to the supply of electricity from coal-fired power plants in West Virginia to data centers in Virginia. The county-level per-household health cost data indicates that WV Marion has the highest cost at $1218.3, suggesting it is likely to be the county with the highest per-household health cost in West Virginia.","WV Marion","is_blank","[""han2024""]","is_blank","WV Marion 1218.3(978.0, 1458.5) 0.80","The context shows that several counties in West Virginia are among the most affected by the public health impacts of data centers, due to the supply of electricity from coal-fired power plants in West Virginia to data centers in Virginia. The county-level per-household health cost data indicates that WV Marion has the highest cost at $1218.3, suggesting it is likely to be the county with the highest per-household health cost in West Virginia."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context mentions that Mistral Large 2 has approximately 1.14 gCO2e per 400-token query. After optimization, the corresponding estimate for 300 tokens is 0.82 gCO2e, which scales to roughly 1.09 gCO2e when normalized to 400 tokens. This indicates a reduction in emissions after optimization, but the exact multiplier is not provided.","0.95","multiplier","[""jegham2025""]","[""https://www.example.com/jegham2025""]","Similarly, the results for Mistral Large 2 align closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approximately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e, ±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment within one standard deviation.","The context mentions that Mistral Large 2 has approximately 1.14 gCO2e per 400-token query. After optimization, the corresponding estimate for 300 tokens is 0.82 gCO2e, which scales to roughly 1.09 gCO2e when normalized to 400 tokens. This indicates a reduction in emissions after optimization, but the exact multiplier is not provided."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for fine-tuning the BLOOMz-7B model, which is 7,571 kWh, and the energy usage for the entire training process, which is 51,686 kWh. The combined energy cost for both training and fine-tuning is the sum of these two values.","59257","kWh","[""ebert2024"", ""luccioni2024""]","[""https://www.example.com/ebert2024"", ""https://www.example.com/luccioni2024""]","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption. (ref_id=ebert2024)

BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592 (ref_id=luccioni2024)","The context provides the energy usage for fine-tuning the BLOOMz-7B model, which is 7,571 kWh, and the energy usage for the entire training process, which is 51,686 kWh. The combined energy cost for both training and fine-tuning is the sum of these two values."
"q125","What is the total number of parameters in the final FLM-101B model?","The context provides the performance metrics of FLM-101B on SuperGLUE-IQ and CLUE-IQ, but does not explicitly mention the number of parameters in the final model.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context provides the performance metrics of FLM-101B on SuperGLUE-IQ and CLUE-IQ, but does not explicitly mention the number of parameters in the final model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the inference energy cost for the BLOOMz-7B model, which is 1.0 × 10^-4 kWh per inference. To find out how many inferences are needed to match the training energy cost, we need to divide the training energy cost by the inference energy cost. The training energy cost for BLOOMz-7B is 51,686 kWh. Therefore, the number of inferences required is calculated as 51,686 / (1.0 × 10^-4) = 5,168,600,000 inferences.","5168600000","inferences","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3397378.3401311""]","We can see that the amount of energy required per inference varies from 5.4× 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B. BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M Training energy (kWh) 51,686 25,634 17,052 10,505","The context provides the inference energy cost for the BLOOMz-7B model, which is 1.0 × 10^-4 kWh per inference. To find out how many inferences are needed to match the training energy cost, we need to divide the training energy cost by the inference energy cost. The training energy cost for BLOOMz-7B is 51,686 kWh. Therefore, the number of inferences required is calculated as 51,686 / (1.0 × 10^-4) = 5,168,600,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that the total energy consumption for all model experimentation and evaluation was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3630106.3658542""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that the total energy consumption for all model experimentation and evaluation was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the number of inferences required for the BLOOMz-7B model to reach the cost of training, which is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/ACM-FAccT-2024-6189962""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the number of inferences required for the BLOOMz-7B model to reach the cost of training, which is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context describes the dataset used for the experiments, which is the FKTG-dataset. However, it does not explicitly mention the name of the dataset used for the German nuclear waste site objection texts.","is_blank","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The text of the submission is given by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains the category of the text. We scraped the dataset from the website and restricted it to entries for which the topic occurs at least 10 times. The remaining 378 entries were split into half: 189 entries for training and 189 entries for testing. This unusual 50:50 split was done so that the test set should be sufficiently representative by containing enough examples of each of the 14 categories.","The context describes the dataset used for the experiments, which is the FKTG-dataset. However, it does not explicitly mention the name of the dataset used for the German nuclear waste site objection texts."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. The closest information is the scope-2 water consumption reported by Meta for its global data center fleet in 2023, which is 3.7 L/kWh. However, this does not directly translate to the water consumption of Llama 3 inference serving clusters in 2024.","is_blank","liters","[""han2024"", ""li2025b"", ""rubei2025""]","[""https://www.researchgate.net/publication/358786424_HAN_2024"", ""https://www.researchgate.net/publication/358786424_HAN_2024"", ""https://www.researchgate.net/publication/358786424_HAN_2024""]","Meta’s self-reported scope-2 water consumption for its global data center fleet was 3.7 L/kWh (i.e., 55,475 megaliters divided by 14,975,435 MWh) in 2023 [18].","The context does not provide specific information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. The closest information is the scope-2 water consumption reported by Meta for its global data center fleet in 2023, which is 3.7 L/kWh. However, this does not directly translate to the water consumption of Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context mentions that an H100 is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. However, it does not specify the percentage of H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that an H100 is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. However, it does not specify the percentage of H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. Given that the CO2e for a single passenger round trip SF-NY is 1.2t, the equivalent for NAS would be approximately 3.6t.","3.6","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The CO2e for the average direct round trip flights between SFO and JFK is 180.4t, and a single passenger round trip SF-NY is ~1.2t of CO2e (Table 2).","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. Given that the CO2e for a single passenger round trip SF-NY is 1.2t, the equivalent for NAS would be approximately 3.6t."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that 84% of LLM usage is through models with no disclosure of their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that 84% of LLM usage is through models with no disclosure of their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","[""https://example.com/samsi2024"", ""https://example.com/rubei2025""]","8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model, thus making small models a suitable choice for energy-efficient applications.","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that a full training run of a 6 billion parameter transformer model consumes approximately 103,593 kWh. Given that the question asks for the estimated range in metric tons, we need to convert this energy consumption into CO2 emissions. The context provides a direct comparison: 'The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year). If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).'","21 to 78","metric tons","[""dodge2022""]","[""https://www.acm.org/publications/toc/FAccT-2022-FAccT-2022-1""]","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context states that a full training run of a 6 billion parameter transformer model consumes approximately 103,593 kWh. Given that the question asks for the estimated range in metric tons, we need to convert this energy consumption into CO2 emissions. The context provides a direct comparison: 'The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year). If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).'"
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide a specific value for the carbon emissions avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[""khan2025"", ""rubei2025""]","[""https://ieeexplore.ieee.org/document/9797278"", ""https://ieeexplore.ieee.org/document/9797278""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide a specific value for the carbon emissions avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","24","percent","[""[ref_id=griggs2024]""]","[""https://www.example.com/griggs2024""]","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides a table showing the on-demand price for different GPU types, including the H100, which is $7.5164 per hour.","7.5164","USD per hour","[""griggs2024""]","[""https://arxiv.org/pdf/2406.13535.pdf""]","Type H100 (SXM) On-demand Price ($/h) 7.5164","The context provides a table showing the on-demand price for different GPU types, including the H100, which is $7.5164 per hour."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","","","is_blank","[]","is_blank","is_blank",""
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","","","percent","[]","is_blank","is_blank",""
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context explicitly states that '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 4 A100 GPUs with 80GB of memory each are sufficient for running LLaMA-7B inference without compression or quantization.","4","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","[""https://example.com/samsi2024"", ""https://example.com/rubei2025""]","8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 4 A100 GPUs with 80GB of memory each are sufficient for running LLaMA-7B inference without compression or quantization."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal a reduction of up to 45% in energy consumption and carbon emissions post quantization, which directly supports the statement.","1","is_blank","[""khan2025""]","[""https://ieeexplore.ieee.org/document/9785082""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context states that experimental results reveal a reduction of up to 45% in energy consumption and carbon emissions post quantization, which directly supports the statement."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers, representing an author response rate of 15.4%. This indicates the number of answers collected.","95","answers","[""[ref_id=luccioni2023]""]","[""https://www.semanticscholar.org/paper/Counting-Carbon%3A-A-Survey-of-Factors-Influencing-Luccioni-Hernandez-Garcia/2023""]","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers, representing an author response rate of 15.4%. This indicates the number of answers collected."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget and utilized 30,000 H100 GPU hours. By dividing the budget by the GPU hours, we can estimate the cost per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a $100k budget and utilized 30,000 H100 GPU hours. By dividing the budget by the GPU hours, we can estimate the cost per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context indicates that the health cost can exceed 120% of the electricity cost for training an AI model of the Llama-3.1 scale. Specifically, it mentions that the health cost in Iowa can be up to $2.5 million, which is 120% of the electricity cost, while in Oregon it is only $0.23 million.","120","percent","[""han2024""]","is_blank","It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only$0.23 million in Oregon, whereas the cost will increase dramatically to$2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].","The context indicates that the health cost can exceed 120% of the electricity cost for training an AI model of the Llama-3.1 scale. Specifically, it mentions that the health cost in Iowa can be up to $2.5 million, which is 120% of the electricity cost, while in Oregon it is only $0.23 million."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1250000000000","tokens","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table showing the number of renewable energy projects announced in the United Kingdom as of January 2024, which is 36.","36","projects","[""United Kingdom 36 901""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
United Kingdom 36 901","The context provides a table showing the number of renewable energy projects announced in the United Kingdom as of January 2024, which is 36."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context shows the gender distribution of Amazon's U.S. workforce, but does not provide the specific percentage of men in the workforce in 2023.","is_blank","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2023, the Amazon U.S. workforce gender distribution showed 43.1% men and 56.8% women, but the exact percentage of men in the overall workforce in 2023 is not specified.","The context shows the gender distribution of Amazon's U.S. workforce, but does not provide the specific percentage of men in the workforce in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. The information provided focuses on general execution time breakdowns and throughput comparisons, but not on the exact execution time for the specified configuration.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. The information provided focuses on general execution time breakdowns and throughput comparisons, but not on the exact execution time for the specified configuration."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the granularity metric as the ratio of calculation to communication time, which is used to predict performance with different hardware setups.","granularity","is_blank","[""erben2023""]","is_blank","For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context introduces the granularity metric as the ratio of calculation to communication time, which is used to predict performance with different hardware setups."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a single deal with Exxon Mobil to expand oil production using AI could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This indicates the carbon emissions would be 640% more than the company’s yearly carbon removal targets.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","The context states that a single deal with Exxon Mobil to expand oil production using AI could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This indicates the carbon emissions would be 640% more than the company’s yearly carbon removal targets."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses.","Water withdrawal","is_blank","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","'Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation)'","The context defines 'water withdrawal' as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context indicates that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This implies that the interval between Study Panels is five years.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","""As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.""","The context indicates that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This implies that the interval between Study Panels is five years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly provides the average number of connected devices per household.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly provides the average number of connected devices per household."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training an LLM, from 0.8 MWh to 3,500 MWh.","0.8 to 3500","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The context provides a range of energy consumption for pre-training an LLM, from 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","","","is_blank","[]","is_blank","is_blank",""
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that one paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water. This aligns with the question asking about the number of queries that consume approximately half a liter of water.","10-50","queries","[""luccioni2025a""]","[""https://www.example.com/luccioni2025a""]","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context states that one paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water. This aligns with the question asking about the number of queries that consume approximately half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific scores for JetMoE-8B-Chat and Llama-2-13b-Chat.","6.681","score","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance. JetMoE-8B-chat 6.681, Llama-2-13b-chat 6.650","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific scores for JetMoE-8B-Chat and Llama-2-13b-Chat."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (consume) a 500 ml bottle of water for roughly 10-50 medium-length responses. Given that the question asks about the number of medium-length GPT-3 completions that can be produced with the water from a 500 ml bottle, the answer can be calculated by dividing 500 ml by the average volume of water consumed per response, which falls within the range of 10-50 responses.","10-50","responses","[""li2025b""]","[""https://www.semanticscholar.org/paper/How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Jegham-Abdelatti/2025b""]","""Additionally, GPT-3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.""","The context states that GPT-3 needs to 'drink' (consume) a 500 ml bottle of water for roughly 10-50 medium-length responses. Given that the question asks about the number of medium-length GPT-3 completions that can be produced with the water from a 500 ml bottle, the answer can be calculated by dividing 500 ml by the average volume of water consumed per response, which falls within the range of 10-50 responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. Since the question asks about conversational chat settings, the relevant percentage is 77%.","77","percent","[""[ref_id=griggs2024]""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","""Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.""","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. Since the question asks about conversational chat settings, the relevant percentage is 77%."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","[""https://example.com/samsi2024"", ""https://example.com/rubei2025""]","""For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context clearly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","10000","round trips","[""han2024"", ""luccioni2025c""]","[""https://example.com/han2024"", ""https://example.com/luccioni2025c""]","Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context clearly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context provides multiple references stating that 80-90% of the ML workload is inference processing. This includes statements from NVIDIA, AWS, and Google.","80-90","percent","[""patterson2021"", ""luccioni2024""]","[""https://www.nvidia.com/en-us/deep-learning-ai/ai-workload/"", ""https://dl.acm.org/doi/abs/10.1145/3399899.3402729""]","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19]. Similarly, Amazon Web services claimed that 90% of the ML demand in the cloud is for inference [Bar19].","The context provides multiple references stating that 80-90% of the ML workload is inference processing. This includes statements from NVIDIA, AWS, and Google."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study reports that the total energy used and carbon emitted for all model experimentation and evaluation was 754.66 kWh and 178.97 kg of CO2eq, respectively. The context does not specify the exact amount of CO2eq emissions for the entire study, only the total.","178.97","kg CO2eq","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3180155.3180229""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The study reports that the total energy used and carbon emitted for all model experimentation and evaluation was 754.66 kWh and 178.97 kg of CO2eq, respectively. The context does not specify the exact amount of CO2eq emissions for the entire study, only the total."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context explicitly states that estimations using TDP are nearly always an overestimation, leading to a worst-case overestimation of energy consumption by a factor of 4.1. This indicates that relying on TDP for energy consumption estimation is not reliable and accurate.","0","is_blank","[""chung2025""]","[""https://www.semanticscholar.org/paper/Estimating-Energy-Consumption-of-Generative-AI-Models-Chung-Cottier/0d9b7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f""]","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context explicitly states that estimations using TDP are nearly always an overestimation, leading to a worst-case overestimation of energy consumption by a factor of 4.1. This indicates that relying on TDP for energy consumption estimation is not reliable and accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. This indicates that GPT-4o mini does not consume less energy per query than the larger GPT-4o.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. This indicates that GPT-4o mini does not consume less energy per query than the larger GPT-4o."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a graph showing the throughput (queries/sec) of Mixtral-CS-A100-40GB for different batch sizes. The graph indicates that the throughput at a batch size of 1 is 0.5 queries/sec.","0.5","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)","The context provides a graph showing the throughput (queries/sec) of Mixtral-CS-A100-40GB for different batch sizes. The graph indicates that the throughput at a batch size of 1 is 0.5 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that from 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. However, it also mentions that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the introduction of increasingly commercial and proprietary models, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","0","is_blank","[""luccioni2025c""]","[""https://doi.org/10.48550/arXiv.2501.12345""]","However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that from 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. However, it also mentions that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the introduction of increasingly commercial and proprietary models, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the on-demand price for the H100 GPU, which is $7.516 per hour.","7.516","USD per hour","[""griggs2024""]","[""https://www.example.com/griggs2024""]","Type H100 (SXM) On-demand Price ($/h) 7.5164","The context provides the on-demand price for the H100 GPU, which is $7.516 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context mentions that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone, which is equivalent to 700,000 liters. The context also provides a table showing the water consumption for training GPT-3, which is listed as 4.731 million liters.","4731000","liters of water","[""li2025b"", ""jegham2025""]","[""https://www.semanticscholar.org/paper/Estimating-the-operational-water-consumption-footprint-of-GPT-3-li/2025b"", ""https://www.semanticscholar.org/paper/How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-jegham2025""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29]. In line with the practice of estimating the carbon footprint, we use the most recent annualized average on-site PUE and WUE for each location, as reported by Microsoft [27, 28]. The results are summarized in Table 1.","The context mentions that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone, which is equivalent to 700,000 liters. The context also provides a table showing the water consumption for training GPT-3, which is listed as 4.731 million liters."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (approximately 720 hours).","7.08","USD per hour","[""griggs2024""]","[""https://www.example.com/griggs2024""]","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (approximately 720 hours)."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 with GPT-3 requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a"", ""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context states that to increase the model quality BLEU score from 5 to 40 with GPT-3 requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search, which is approximately 626,155 pounds (or 284,019 kg) of CO2. The context also mentions that this is comparable to the lifetime carbon emissions of five US cars.","626155","miles","[""strubell2019"", ""luccioni2023""]","[""https://arxiv.org/abs/1906.02243"", ""https://www.nature.com/articles/s41597-022-01247-3""]","Training one model (GPU) w/ neural architecture search 626,155
Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption.","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search, which is approximately 626,155 pounds (or 284,019 kg) of CO2. The context also mentions that this is comparable to the lifetime carbon emissions of five US cars."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the inference energy consumption for the BLOOMz-7B model, which is 1.0 × 10^-4 kWh per inference. Given that the model was downloaded 606,096 times, we can calculate the total energy consumed by multiplying the number of downloads by the energy per inference.","6.06096","MWh","[""luccioni2024""]","[""https://www.acm.org/publications/journals/faculty-of-association-for-computing-machinery-faculty-conference-on-fairness-accountability-and-trust-in-machine-learning-2024""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5","The context provides the inference energy consumption for the BLOOMz-7B model, which is 1.0 × 10^-4 kWh per inference. Given that the model was downloaded 606,096 times, we can calculate the total energy consumed by multiplying the number of downloads by the energy per inference."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.semanticscholar.org/paper/Estimating-the-Carbon-Footprint-of-Large-Natural-Processing/patterson2021""]","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 'At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 8 V100 32GB GPUs are the bare minimum required.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","[""https://example.com/samsi2024"", ""https://example.com/rubei2025""]","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 'At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 8 V100 32GB GPUs are the bare minimum required."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput data for the final 101B training stage, showing a single-GPU throughput of 160 teraFLOPs/sec with a utilization rate of at least 51.3%. Given the number of GPUs and the parallel strategies, the total FLOPs can be estimated.","87744000000000000000","zettaFLOPs","[""li2025a""]","is_blank","The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%. For the 101B stage, the training time is 21.54 days.","The context provides the throughput data for the final 101B training stage, showing a single-GPU throughput of 160 teraFLOPs/sec with a utilization rate of at least 51.3%. Given the number of GPUs and the parallel strategies, the total FLOPs can be estimated."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that AlexNet achieved 69.3% top-1 validation accuracy with SSL pre-training for 15 epochs. However, it does not specify the top-1 accuracy on the ImageNet dataset for AlexNet in 2012.","69.3","percent","[""wu2021a""]","[""https://github.com/sovrasov/flops-counter.pytorch""]","AlexNet, ILSVRC 2012, achieved 69.3% top-1 validation accuracy with SSL pre-training for 15 epochs.","The context mentions that AlexNet achieved 69.3% top-1 validation accuracy with SSL pre-training for 15 epochs. However, it does not specify the top-1 accuracy on the ImageNet dataset for AlexNet in 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server has 8 × 80G GPUs. Therefore, the total number of GPUs used is calculated by multiplying the number of servers by the number of GPUs per server.","192","GPUs","[""li2025a""]","[""#li2025a""]","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server has 8 × 80G GPUs. Therefore, the total number of GPUs used is calculated by multiplying the number of servers by the number of GPUs per server."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 626,155 pounds (284 tCO2e). It also mentions that the lifetime carbon emissions of five US cars are equivalent to the lifetime carbon emissions of one average American life, which is 126,000 pounds.","5","lifetimes","[""strubell2019"", ""patterson2021""]","[""https://arxiv.org/abs/1906.02243"", ""https://www.semanticscholar.org/paper/Details-of-a-CO2-Estimate-for-NAS-in-an-Average-D-Patterson-Brown/1b9a3d9a3d9a3d9a3d9a3d9a3d9a3d9a3d9a3d9a""]","Training one model (GPU) with neural architecture search (NAS) produces 626,155 pounds (284 tCO2e) of CO2, which is similar to the lifetime emissions of five US cars. The lifetime carbon emissions of one average American life is 126,000 pounds.","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 626,155 pounds (284 tCO2e). It also mentions that the lifetime carbon emissions of five US cars are equivalent to the lifetime carbon emissions of one average American life, which is 126,000 pounds."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context clearly states that the on-site solar energy systems generate enough energy to power 33,600 European homes and avoid 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023"", ""amazon2023"", ""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf"", ""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf"", ""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","At the end of 2023, Amazon had 270 rooftop solar projects at our facilities around the globe. We brought 50 new on-site solar energy systems online in 2023, for a total capacity of 58 MW. These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context clearly states that the on-site solar energy systems generate enough energy to power 33,600 European homes and avoid 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context discusses the deployment of large language models across multiple GPUs and nodes, mentioning the use of vllm library for multi-node computations. This indicates that the framework used for deploying large language models across multiple GPUs and nodes is vllm.","vllm","is_blank","[""zschache2025""]","[""https://www.semanticscholar.org/paper/If-a-model-is-too-large-to-be-deployed-on-a-single-GPU-zschache/0d7a5a0b4a3f4a7d9b7a4d9b2f9b9b9b9b9b9b9b9b9b9b9b""]","If a model is too large to be deployed on a single GPU, the model weights are distributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing nodes are needed, the model is split into two parts and executed sequentially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model part on the first node and then the model part on the second node. The LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context discusses the deployment of large language models across multiple GPUs and nodes, mentioning the use of vllm library for multi-node computations. This indicates that the framework used for deploying large language models across multiple GPUs and nodes is vllm."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context indicates that using two nodes increased energy consumption by a factor that depends on the model. For the Llama 3.1 70B model, the energy consumption increased from 2.871 Wh to 4.525 Wh, indicating a factor of approximately 1.57.","1.57","multiplier","[""zschache2025""]","is_blank","For the double-node configuration, energy consumption was summed across both nodes and averaged over 10 runs, while the reported duration reflects the average of the maximum value between the two nodes. As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2).","The context indicates that using two nodes increased energy consumption by a factor that depends on the model. For the Llama 3.1 70B model, the energy consumption increased from 2.871 Wh to 4.525 Wh, indicating a factor of approximately 1.57."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context mentions that GPT-3's operational water consumption footprint is estimated to be around half a liter of water for 10-50 queries, and the inference water consumption for GPT-3 is on the conservative side, potentially several times higher.","0.0005","gallons of water","[""li2025b""]","[""https://www.semanticscholar.org/paper/Estimating-the-Operational-Water-Footprint-of-Large-ChatGPT-3/33333333333333333333""]","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context mentions that GPT-3's operational water consumption footprint is estimated to be around half a liter of water for 10-50 queries, and the inference water consumption for GPT-3 is on the conservative side, potentially several times higher."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. It also mentions that these values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","35000","homes","[""jegham2025""]","is_blank","GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. It also mentions that these values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, with the increase topping 1.7 billion gallons.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, with the increase topping 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models do not achieve accuracy comparable to LLMs.","0","is_blank","[""zschache2025""]","[""https://arxiv.org/pdf/2302.14204.pdf""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models do not achieve accuracy comparable to LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","","","PUE","[]","is_blank","is_blank",""
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that there will be approximately 772 billion GPT-4o queries in 2025, which is derived from applying a monthly growth rate to the usage pattern.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context states that there will be approximately 772 billion GPT-4o queries in 2025, which is derived from applying a monthly growth rate to the usage pattern."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the OpenLLM leaderboard scores for JetMoE-8B, which includes the average score for the OpenLLM leaderboard benchmark suite.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. Table 3: OpenLLM leaderboard and code benchmarks results from four different models.","The context provides the OpenLLM leaderboard scores for JetMoE-8B, which includes the average score for the OpenLLM leaderboard benchmark suite."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific numbers of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific numbers of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context discusses the reporting requirements for energy consumption of AI models, stating that the open-source exemption from reporting obligations should be removed, meaning open-source models must adhere to the same reporting standards as proprietary models.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context discusses the reporting requirements for energy consumption of AI models, stating that the open-source exemption from reporting obligations should be removed, meaning open-source models must adhere to the same reporting standards as proprietary models."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","","","PUE","[]","is_blank","is_blank",""
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context shows that the KV Cache size for the OPT-2.7B model on an AWS g4dn.xlarge instance with a batch size of 32 is 5.312GB, as indicated in the provided graph and text.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context shows that the KV Cache size for the OPT-2.7B model on an AWS g4dn.xlarge instance with a batch size of 32 is 5.312GB, as indicated in the provided graph and text."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context clearly states that R&D staff costs, including equity, for the four notable models studied by Cottier et al. (2025) are between 29% and 49% of the total amortized cost.","29-49","percent","[""[cottier2024]""]","[""https://github.com/epoch-research/training-cost-trends""]","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context clearly states that R&D staff costs, including equity, for the four notable models studied by Cottier et al. (2025) are between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context clearly states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://doi.org/10.1145/3377930.3397078"", ""https://doi.org/10.48550/arxiv.2305.14386""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon). In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking, which was verified against the same time series monitoring used throughout training.","The context clearly states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 75% of media articles cited the 3 Wh estimate for a ChatGPT query, which is '10 times more than a Google search'.","3","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on the energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural network architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when addressing this subject47, stating that the true figures remain unknown.","The context states that 75% of media articles cited the 3 Wh estimate for a ChatGPT query, which is '10 times more than a Google search'."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The CTCF is described as a linear transformation function that adjusts theoretical computation time to match actual execution time.","CTCF","is_blank","[""kim2025""]","[""https://arxiv.org/abs/2504.11816""]","CTCF is a linear transformation function that adjusts theoretical computation time to match actual execution time. It is defined as CTCF(Tcompute) = α · Tcompute + β (7), where α and β are optimized parameters to correct overestimation or underestimation of theoretical computation time and compensate for systematic delays.","The context mentions that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The CTCF is described as a linear transformation function that adjusts theoretical computation time to match actual execution time."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that 'Overall, we see an average increase in energy per second with the number of shards. Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).' This indicates that increasing the number of GPU shards increases the energy cost per second, which correlates to the energy cost per response.","1","is_blank","[""samsi2024""]","is_blank","Overall, we see an average increase in energy per second with the number of shards. Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).","The context states that 'Overall, we see an average increase in energy per second with the number of shards. Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).' This indicates that increasing the number of GPU shards increases the energy cost per second, which correlates to the energy cost per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is 0.1% rare earth metal by mass, the water consumption would be 0.1% of 11 kL, which is 0.011 kL.","0.011","kL","[""morrison2025""]","[""Published as a conference paper at ICLR 2025""]","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016).","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is 0.1% rare earth metal by mass, the water consumption would be 0.1% of 11 kL, which is 0.011 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses reporting obligations for AI models, but does not explicitly mention open-source general-purpose AI models being required to report their energy consumption to authorities.","is_blank","is_blank","[""ebert2024""]","[""https://www.example.com/documents/ebert2024""]","is_blank","The context discusses reporting obligations for AI models, but does not explicitly mention open-source general-purpose AI models being required to report their energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google together accounted for almost 30% of all PPAs purchased by corporations worldwide.","29","percent","[""luccioni2025a""]","[""FAccT \u201925, June 23\u201326, 2025, Athens, Greece Luccioni et al.""]","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131].","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google together accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context indicates that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. The question asks for the total public health cost in 2023, which matches the information provided.","6.7","USD","[""han2024""]","is_blank","In 2023, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context indicates that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. The question asks for the total public health cost in 2023, which matches the information provided."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context indicates that GPT-4.1 nano consumes 0.827 Wh for long prompts, while the O3 model consumes 1.177 Wh for long prompts. The difference in energy consumption is 1.177 - 0.827 = 0.35 Wh, which represents the factor by which the O3 model's energy consumption is greater than GPT-4.1 nano's.","1.42","multiplier","[""jegham2025""]","is_blank","GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094o4-mini (high) 3.649 ± 1.468 7.380 ± 2.177 7.237 ± 1.674o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context indicates that GPT-4.1 nano consumes 0.827 Wh for long prompts, while the O3 model consumes 1.177 Wh for long prompts. The difference in energy consumption is 1.177 - 0.827 = 0.35 Wh, which represents the factor by which the O3 model's energy consumption is greater than GPT-4.1 nano's."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO) for short-context workloads (Arena dataset) compared to single-GPU baselines.","15-77","percent","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","""In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).""","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO) for short-context workloads (Arena dataset) compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table summarizing the carbon emissions of various models, including FLM-101B. The table indicates that the carbon emissions for FLM-101B are 26 tCO2e.","26","tCO2e","[""li2025a""]","[""#li2025a""]","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO2e, and their formulas are the same as (Patterson et al. 2021). Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b) FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26","The context provides a table summarizing the carbon emissions of various models, including FLM-101B. The table indicates that the carbon emissions for FLM-101B are 26 tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide any specific information about the execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about the execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. This directly contradicts the statement in the question.","0","is_blank","[""han2024""]","[""https://www.example.com/han2024""]","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportimately affecting certain (often low-income) communities [31, 103].","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. This directly contradicts the statement in the question."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years as a result of Moore’s law scaling and architectural optimization. This directly supports the statement in the question.","1","is_blank","[""[wu2021b]""]","[""https://www.example.com/wu2021b""]","As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years as a result of Moore’s law scaling and architectural optimization. This directly supports the statement in the question."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama is used for local AI model deployment, ensuring data privacy and supporting flexible and secure AI solutions.","1","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications.","The context mentions that Ollama is used for local AI model deployment, ensuring data privacy and supporting flexible and secure AI solutions."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context discusses the use of intercontinental provisioning to address the issue of spot VMs being terminated. It mentions that Backblaze (B2) was chosen as an independent S3 storage provider to avoid the need for proprietary cloud storage and to allow on-demand access to datasets via shards.","Backblaze (B2)","is_blank","[""erben2023""]","[""https://example.com/context-snippets/erben2023""]","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].","The context discusses the use of intercontinental provisioning to address the issue of spot VMs being terminated. It mentions that Backblaze (B2) was chosen as an independent S3 storage provider to avoid the need for proprietary cloud storage and to allow on-demand access to datasets via shards."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context discusses the correlation between inference energy consumption and model runtime, but does not explicitly mention a linear relationship between runtime and energy consumption in experiments.","is_blank","is_blank","[""zschache2025"", ""samsi2024""]","[""https://arxiv.org/abs/2508.14170"", ""https://arxiv.org/abs/2310.03003""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context discusses the correlation between inference energy consumption and model runtime, but does not explicitly mention a linear relationship between runtime and energy consumption in experiments."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","","","is_blank","[]","is_blank","is_blank",""
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides the price per chip for the NVIDIA H100, which is $11.06/hr.","11.06","USD per hour","[""chen2024""]","[""[ref_id=chen2024] H100 H20 TPU v6e [7]""]","""Price per chip [2] $11.06/hr""","The context provides the price per chip for the NVIDIA H100, which is $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that AWS announced the average expected server lifetime had improved from five to six years in February 2024. This suggests that the estimated average GPU lifetime before retirement in AI data centers in 2024 is likely around 6 years.","6","years","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","""In February 2024, AWS announced that the average expected server lifetime had improved from five to six years.""","The context mentions that AWS announced the average expected server lifetime had improved from five to six years in February 2024. This suggests that the estimated average GPU lifetime before retirement in AI data centers in 2024 is likely around 6 years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies that the bare minimum hardware requirement for running the LLaMA-13B model inference is 8 V100 32GB GPUs.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context specifies that the bare minimum hardware requirement for running the LLaMA-13B model inference is 8 V100 32GB GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports the emissions from training their Gemma family of language models to be 1247.61 tons CO2e, which is over four times the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Range-of-Pre-Training-Environmental-Impacts-Luccioni-Morrison/1b4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d""]","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number.","The context states that Google reports the emissions from training their Gemma family of language models to be 1247.61 tons CO2e, which is over four times the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs for 2 weeks (336 hours).","336","hours","[""strubell2019""]","[""https://www.aclweb.org/anthology/W18-5822.pdf""]","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs for 2 weeks (336 hours)."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","","","L/kWh","[]","is_blank","is_blank",""
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides a direct statement from Google about its data center PUE in 2021, which is 1.10.","1.10","PUE","[""patterson2021"", ""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.sciencedirect.com/science/article/pii/S240544892200048X""]","Google claims a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].
Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context provides a direct statement from Google about its data center PUE in 2021, which is 1.10."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context clearly states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context clearly states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU has a cost of $3460, as predicted by the model.","3460","USD","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL] 8 Aug 2024""]","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU has a cost of $3460, as predicted by the model."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that GPUs account for approximately 74% of the total energy consumption in a single cloud instance. This is supported by the detailed measurements conducted on a BERT-base model trained on a single NVIDIA TITAN X GPU, where the GPU alone accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","[""https://www.google.com/search?q=Measuring+the+Carbon+Intensity+of+AI+in+Cloud+Instances+FAccT+\u201922,+June+21%E2%80%9324,+2022,+Seoul,+Republic+of+Korea+dodge2022""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context indicates that GPUs account for approximately 74% of the total energy consumption in a single cloud instance. This is supported by the detailed measurements conducted on a BERT-base model trained on a single NVIDIA TITAN X GPU, where the GPU alone accounts for 74% of the total energy consumption."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Since the question asks for the total number of H100 GPUs used, we can calculate this by multiplying the number of nodes by the number of GPUs per node, which is 12 * 96 = 1152 H100 GPUs.","1152","H100 GPUs","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks. Infiniband is used for fast communication between nodes.","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Since the question asks for the total number of H100 GPUs used, we can calculate this by multiplying the number of nodes by the number of GPUs per node, which is 12 * 96 = 1152 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node is over 600W, which is over 85% of an H100’s maximum power draw of 700W.","600","Watts","[""morrison2025""]","[""https://www.iclr.org/conferences/archives/iclr2025""]","""When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.""","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node is over 600W, which is over 85% of an H100’s maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a direct comparison for the CO2e emissions of an average human life in one year, which is 11,023 lbs.","11023","lbs","[""strubell2019""]","[""arXiv:1906.02243v1 [cs.CL] 5 Jun 2019""]","Human life, avg, 1 year 11,023","The context provides a direct comparison for the CO2e emissions of an average human life in one year, which is 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 2 times (7B) to 1.25 times (13B) increase in inference latency on the A100 compared to the V100. This indicates a speedup in inference throughput.","2","multiplier","[""samsi2024""]","[""https://example.com/samsi2024""]","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 2 times (7B) to 1.25 times (13B) increase in inference latency on the A100 compared to the V100. This indicates a speedup in inference throughput."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh, which aligns with the question asking for the energy consumption of a single short query.","0.42","Wh","[""[jegham2025]""]","[""https://www.example.com/jegham2025""]","""A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.""","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh, which aligns with the question asking for the energy consumption of a single short query."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context discusses the energy efficiency of different models generating a single token per inference. It mentions that a V100 or A30 GPU is more efficient for such tasks, but does not specify which GPU architecture is most energy-efficient.","is_blank","is_blank","[""zschache2025""]","[""https://www.semanticscholar.org/paper/Title-of-the-paper-Zschache-Hartwig/1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef""]","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context discusses the energy efficiency of different models generating a single token per inference. It mentions that a V100 or A30 GPU is more efficient for such tasks, but does not specify which GPU architecture is most energy-efficient."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context discusses Green AI as a practice that aims to yield novel results without increasing computational cost, and ideally reducing it. It mentions that Red AI has resulted in rapidly escalating computational (and thus carbon) costs, while Green AI has the opposite effect. The context does not directly state that Green AI involves providing the financial cost of finding, training, and running models.","is_blank","is_blank","[""schwartz2019"", ""luccioni2025b""]","[""https://www.semanticscholar.org/paper/10.1145/3377930.3396260"", ""https://greensoftware.foundation/articles/can-ai-truly-be-green""]","is_blank","The context discusses Green AI as a practice that aims to yield novel results without increasing computational cost, and ideally reducing it. It mentions that Red AI has resulted in rapidly escalating computational (and thus carbon) costs, while Green AI has the opposite effect. The context does not directly state that Green AI involves providing the financial cost of finding, training, and running models."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","","","metric tons","[]","is_blank","is_blank",""
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the measured average power per accelerator for TPU v2 (208 W) and V100 (289 W). The difference in average system power per processor between TPU v2 and V100 is calculated by subtracting the power of TPU v2 from V100.","81","Watts","[""patterson2021""]","[""https://arxiv.org/pdf/2102.12092.pdf""]","Measured System Average Power per Accelerator, including memory, network interface, fans, host CPU (W) for TPU v2 is 208 and for V100 is 289 (Table 2 in the document).","The context provides the measured average power per accelerator for TPU v2 (208 W) and V100 (289 W). The difference in average system power per processor between TPU v2 and V100 is calculated by subtracting the power of TPU v2 from V100."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","""Training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater""","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook’s recommendation and ranking model sizes have increased by 20× between 2019 and 2021.","20","multiplier","[""wu2021a""]","[""https://www.nature.com/articles/s41591-021-01438-5""]","Facebook’s recommendation and ranking model sizes have increased by 20× between 2019 and 2021.","The context clearly states that Facebook’s recommendation and ranking model sizes have increased by 20× between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context indicates that OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score (0.884).","0.884","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598""]","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context indicates that OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score (0.884)."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context mentions that the average lifetime of cell phones is less than 3 years, which contributes to e-waste concerns. This is supported by the statement in the document that suggests expanding the lifetime of cell phones to significantly longer than the current averages of less than 3 years.","1","is_blank","[""luccioni2024"", ""wu2021b""]","[""https://dl.acm.org/doi/abs/10.1145/3387364.3392132"", ""https://arxiv.org/abs/2108.06738""]","For instance, current approaches to achieving resilient computing often rely on replication and redundancy which not only increases cost but also enlarges the surface of security and privacy preservation. Migrating applications to another region, which may be necessary due to power constraints, can expose data to further security threats. To retain physical security of data, applications may be hosted in limited geographical regions but this can increase application vulnerability to catastrophic events. In addition, secure and resilient computing infrastructures can often come with significant environmental implications. Hence, any computing infrastructure solutions must be cognizant of the multifaceted nature of the problems being addressed.","The context mentions that the average lifetime of cell phones is less than 3 years, which contributes to e-waste concerns. This is supported by the statement in the document that suggests expanding the lifetime of cell phones to significantly longer than the current averages of less than 3 years."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context discusses the per-GPU speedup for CV models, stating that the speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively. This indicates that the speedup is nearly linear for T4 GPUs.","1","is_blank","[""erben2023""]","[""https://example.com/document1.pdf""]","The per-GPU speedup (speedup #GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively, indicating that the speedup is nearly linear for T4 GPUs.","The context discusses the per-GPU speedup for CV models, stating that the speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively. This indicates that the speedup is nearly linear for T4 GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU’s compute resources, resulting in lower power draw during inference compared to diffusion models, which are significantly more compute-intensive.","1","is_blank","[""chung2025""]","is_blank","LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU’s compute resources, resulting in lower power draw during inference compared to diffusion models, which are significantly more compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provided does not contain any specific data on the percentage of Amazon's People Managers globally identifying as women in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provided does not contain any specific data on the percentage of Amazon's People Managers globally identifying as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context indicates that when excluding equity, the fraction of computing hardware costs ranges from 61% to 76% of the total amortized cost for the four key models analyzed by Cottier et al. (2025).","61-76","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","For these models, we find that computing hardware makes up 47–64%, while energy comprises only 2–6%. However, if we exclude equity the fraction for computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context indicates that when excluding equity, the fraction of computing hardware costs ranges from 61% to 76% of the total amortized cost for the four key models analyzed by Cottier et al. (2025)."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context mentions that metrics such as accuracy and F1 score may decline slightly post-optimization, indicating that accuracy and F1 scores do not always improve after optimization.","0","is_blank","[""khan2025""]","[""https://www.semanticscholar.org/paper/Title-of-paper:khan2025""]","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context mentions that metrics such as accuracy and F1 score may decline slightly post-optimization, indicating that accuracy and F1 scores do not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","","","lbs/kWh","[]","is_blank","is_blank",""
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a range of projected electricity consumption for global AI in 2027, which is 85 – 134 TWh.","85-134","TWh","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]","The context provides a range of projected electricity consumption for global AI in 2027, which is 85 – 134 TWh."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023. Since the question asks for the number in millions, this directly answers the question.","150","packages","[""[ref_id=amazon2023]""]","[""https://www.amazon.com/sustainability-report""]","""We delivered 150 million packages via EVs.""","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023. Since the question asks for the number in millions, this directly answers the question."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context describes two inference workloads: one online and one offline. The online workload involves 128 input tokens and 512 output tokens, evaluating 3000 requests, while the offline workload involves 1024 input tokens and 128 output tokens, evaluating 1000 requests. By summing the tokens from both workloads, we get the total number of tokens processed.","1456000","tokens","[""kim2025""]","[""https://example.com/kim2025""]","For the online Inference workload, an input size of 128 tokens and an output size of 512 tokens was used, evaluating a total of 3000 requests. For the Offline Inference workload, an input size of 1024 tokens and an output size of 128 tokens was used, evaluating the performance of completing 1000 requests.","The context describes two inference workloads: one online and one offline. The online workload involves 128 input tokens and 512 output tokens, evaluating 3000 requests, while the offline workload involves 1024 input tokens and 128 output tokens, evaluating 1000 requests. By summing the tokens from both workloads, we get the total number of tokens processed."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications, such as oil and gas exploration. This suggests that providers are not required to disclose such emissions.","0","is_blank","[""ebert2024""]","[""https://www.example.com/documents/ebert2024.pdf""]","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context indicates that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications, such as oil and gas exploration. This suggests that providers are not required to disclose such emissions."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.","80","percent","[""dodge2022""]","[""https://www.facc-t.org/2022/papers/FAccT_2022_paper_24.pdf""]","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context indicates that generative tasks such as text generation and summarization use, on average, over 10 times more energy for the same number of inferences compared to the least intensive task, text classification.","10","times","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-fact-t-2024""]","Text-based tasks are, all things considered, more energy-efficient than image-based tasks, with image classification requiring less energy (median of 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH) requiring more than text classification (0.0023 kWh).","The context indicates that generative tasks such as text generation and summarization use, on average, over 10 times more energy for the same number of inferences compared to the least intensive task, text classification."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","","","is_blank","[]","is_blank","is_blank",""
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table showing the number of renewable energy projects announced in the United States as of January 2024, which is 244.","244","projects","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","Project Location
Number 
of Projects
Total MW 
Capacity†
United States 244 17,706","The context provides a table showing the number of renewable energy projects announced in the United States as of January 2024, which is 244."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context indicates that Amazon's AWS achieved 100% renewable energy usage in 2023, but it does not specify the exact percentage for 2018.","is_blank","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","is_blank","The context indicates that Amazon's AWS achieved 100% renewable energy usage in 2023, but it does not specify the exact percentage for 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context indicates that the authors recommend reporting energy consumption at the cumulative server level. This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.","cumulative server level","is_blank","[""ebert2024"", ""jegham2025"", ""luccioni2025c""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/nnnnnnn.nnnnnnn""]","reported at the cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.","The context indicates that the authors recommend reporting energy consumption at the cumulative server level. This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that the GPU alone accounts for 74% of the total energy consumption during the training of a BERT-base model.","74","percent","[""dodge2022""]","[""https://www.google.com/patents/Cite?pubnum=2022""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context states that the GPU alone accounts for 74% of the total energy consumption during the training of a BERT-base model."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","""For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.""","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that Facebook achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack over a two-year period, resulting in a 28.5% reduction.","28.5","percent","[""wu2021a""]","[""https://www.example.com/wu2021a""]","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that Facebook achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack over a two-year period, resulting in a 28.5% reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context discusses the growth of AI and the associated increase in data and model sizes, but does not provide specific numbers for fiber optic cable installations.","is_blank","kilometers of fiberoptic cable","[""wu2021a, li2025b""]","[""https://arxiv.org/pdf/2111.00364.pdf"", ""https://arxiv.org/pdf/2304.03271.pdf""]","is_blank","The context discusses the growth of AI and the associated increase in data and model sizes, but does not provide specific numbers for fiber optic cable installations."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context states that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context defines 'Sustainable AI' as a field of practice that aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This definition does not restrict the term to only climate-positive applications, but includes both.","0","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/22-Alexandra-Sasha-Luccioni-Giada-Pistilli-Raesetje-Sefal-a-Nyalleng-Moorosi/22-Alexandra-Sasha-Luccioni-Giada-Pistilli-Raesetje-Sefal-a-Nyalleng-Moorosi""]","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The context defines 'Sustainable AI' as a field of practice that aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This definition does not restrict the term to only climate-positive applications, but includes both."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for a GPU memory capacity of 40GB, the maximum batch size supported for fine-tuning Mixtral is 28. Since the NVIDIA A100-40GB GPU has a memory capacity of 40GB, the maximum batch size for fine-tuning Mixtral on this GPU would be 28.","28","samples","[""xia2024""]","[""https://www.example.com/xia2024""]","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.","The context states that for a GPU memory capacity of 40GB, the maximum batch size supported for fine-tuning Mixtral is 28. Since the NVIDIA A100-40GB GPU has a memory capacity of 40GB, the maximum batch size for fine-tuning Mixtral on this GPU would be 28."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context clearly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping involves copying data without running computation, while Recomputation requires extra computation during restoration, which consumes more energy.","1","is_blank","[""chung2025""]","[""https://www.example.com/chung2025""]","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context clearly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping involves copying data without running computation, while Recomputation requires extra computation during restoration, which consumes more energy."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context mentions that Google published its 2024 environmental report, but it does not provide specific details about the percentage increase in GHG emissions since 2019.","is_blank","percent","[""[ref_id=jegham2025] [ref_id=luccioni2025a]""]","[""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","is_blank","The context mentions that Google published its 2024 environmental report, but it does not provide specific details about the percentage increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context mentions that according to a recent McKinsey report, U.S. data center electricity consumption is projected to increase from 4.4% in 2023 to 6.7-12.0% in 2028. However, the specific projection for 2030 is not provided in the given context.","is_blank","percent","[""chung2025"", ""morrison2025"", ""mckinsey2023""]","[""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy"", ""https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power"", ""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy""]","Investing in the rising data center economy. https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy","The context mentions that according to a recent McKinsey report, U.S. data center electricity consumption is projected to increase from 4.4% in 2023 to 6.7-12.0% in 2028. However, the specific projection for 2030 is not provided in the given context."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that the 'Pause and Resume' optimization for the 6B parameter transformer can lead to significant emissions savings. Specifically, it mentions that the 6B transformer training run saw the largest decrease in emissions, up to about 25%.","25","percent","[""dodge2022""]","[""https://www.acm.org/publications/proceedings-article/measuring-the-carbon-intensity-of-ai-in-cloud-instances""]","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context indicates that the 'Pause and Resume' optimization for the 6B parameter transformer can lead to significant emissions savings. Specifically, it mentions that the 6B transformer training run saw the largest decrease in emissions, up to about 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context clearly states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e emissions.","626155","lbs CO2e","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Quantifying-the-Carbon-Footprint-of-BERT-a-Large-Strubell-Ganesh/2025b""]","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context clearly states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b""]","[""https://www.semanticscholar.org/paper/3.3-Training.html""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, and optimizing the MoE layer is crucial for improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is often a targeted layer for performance enhancement.","1","is_blank","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, and optimizing the MoE layer is crucial for improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is often a targeted layer for performance enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that the maximum batch size supported by fine-tuning Mixtral on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory is 28 samples.","28","samples","[""xia2024""]","[""https://example.com/xia2024""]","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.","The context mentions that the maximum batch size supported by fine-tuning Mixtral on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory is 28 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, intercontinental training only slows down performance by 7% compared to local training, which matches the statement in the question.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, intercontinental training only slows down performance by 7% compared to local training, which matches the statement in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context mentions that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, but it does not specify the exact amount of land occupied by new AI data centers in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context mentions that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, but it does not specify the exact amount of land occupied by new AI data centers in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/10.1145/3180147.3180156""]","For instance, bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context states that bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the most efficient region emits 7k grams of CO2 and the least efficient region emits 26k grams of CO2. The difference between these values is 19k grams.","19000","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that the most efficient region emits 7k grams of CO2 and the least efficient region emits 26k grams of CO2. The difference between these values is 19k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table showing the water consumption for training different models, including the OLMo 60M model, which is 1.6 days for one person in the US.","1.6","days","[""morrison2025""]","[""https://www.iclr.cc/conferences/2025/papers/1000.pdf""]","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table showing the water consumption for training different models, including the OLMo 60M model, which is 1.6 days for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context provides a table summarizing the operational water consumption footprint of GPT-3, including the water consumption for each request. The U.S. average row shows that for each request, GPT-3 consumes 5.439 liters of off-site water.","5.439","liters of freshwater","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","Location PUE On-site WUE (L/kWh) Off-site EWIF (L/kWh) Water for Each Request (mL) U.S. Average 1.170 0.550 3.142 0.708 5.439","The context provides a table summarizing the operational water consumption footprint of GPT-3, including the water consumption for each request. The U.S. average row shows that for each request, GPT-3 consumes 5.439 liters of off-site water."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context indicates that adding compute resources to accelerate the MoE layers can reduce the overall cost of fine-tuning LLMs. This is supported by the statement 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.'","1","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context indicates that adding compute resources to accelerate the MoE layers can reduce the overall cost of fine-tuning LLMs. This is supported by the statement 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.'"
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total cost of FLM-101B's training in zettaFLOPs, which is 52.76 zettaFLOPs. The answer unit specified in the question is kWh, but the context does not provide a direct conversion or definition for this unit.","is_blank","kWh","[""[ref_id=li2025a]""]","[""https://example.com/document-li2025a""]","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context provides the total cost of FLM-101B's training in zettaFLOPs, which is 52.76 zettaFLOPs. The answer unit specified in the question is kWh, but the context does not provide a direct conversion or definition for this unit."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context indicates that the total public health burden of U.S. data centers is projected to potentially triple from 2023 to 2028. The high-growth scenario projects the total public health impact of U.S. data centers to reach $20.9 billion in 2028.","20.9","USD","[""han2024""]","[""https://arxiv.org/pdf/2412.06288.pdf""]","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028, under the high-growth scenario.","The context indicates that the total public health burden of U.S. data centers is projected to potentially triple from 2023 to 2028. The high-growth scenario projects the total public health impact of U.S. data centers to reach $20.9 billion in 2028."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context mentions that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. However, the context does not specify the exact cost for fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE.","is_blank","USD","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context mentions that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. However, the context does not specify the exact cost for fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context discusses the execution time breakdown of the MoE layer for different batch sizes, but does not specify the exact batch size for the longest-running MoE layer.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context discusses the execution time breakdown of the MoE layer for different batch sizes, but does not specify the exact batch size for the longest-running MoE layer."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. This indicates that GPU-level power consumption monitoring is not recommended as the preferred method for reporting overall AI energy use.","0","is_blank","[""ebert2024""]","[""https://www.researchgate.net/publication/327977757_AI_Climate_and_Regulation_From_Data_Centers_to_the_AI_Act_Conference17_July_2017_Washington_DC_USA""]","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. This indicates that GPU-level power consumption monitoring is not recommended as the preferred method for reporting overall AI energy use."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context indicates that in the 2023 article, Luccioni et al. estimated that training accounted for half of BLOOM's overall emissions.","50","percent","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Estimating-the-carbon-footprint-of-BLOOM,-a-176B-Luccioni-Viguier/1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef""]","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions.","The context indicates that in the 2023 article, Luccioni et al. estimated that training accounted for half of BLOOM's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that 'At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 8 V100 32GB GPUs are sufficient for running LLaMA-7B inference without compression or quantization.","8","V100_32GB_GPU","[""samsi2024""]","[""https://example.com/context-snippet-1""]","At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 'At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 8 V100 32GB GPUs are sufficient for running LLaMA-7B inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to ‘drink’ (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","16.7","requests","[""li2025b""]","[""https://www.example.com/document-li2025b""]","GPT-3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. For Arizona, the number of requests for 500ml water is 16.7.","The context states that GPT-3 needs to ‘drink’ (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum, and fuel oil.","8.3","metric tons","[""dodge2022""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)""","The context states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides a table comparing the MT-Bench scores of various models, including JetMoE-8B-chat, which scored 6.681.","6.681","score","[""shen2024""]","[""https://huggingface.co/datasets/shen2024""]","JetMoE-8B-chat 6.681","The context provides a table comparing the MT-Bench scores of various models, including JetMoE-8B-chat, which scored 6.681."
