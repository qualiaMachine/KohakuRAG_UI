"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context from ref_id=wu2021b states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the value is reported as a number without the unit name.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context from ref_id=wu2021b states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the value is reported as a number without the unit name."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This directly provides the requested equivalent in cars.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This directly provides the requested equivalent in cars."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This means 4 new data centers began using recycled water in 2023 (24 - 20 = 4). The answer_unit is 'data centers', so the value is expressed as a number.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This means 4 new data centers began using recycled water in 2023 (24 - 20 = 4). The answer_unit is 'data centers', so the value is expressed as a number."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release embodied carbon emissions data for its hardware, but Morrison et al. (2025) assume the same embodied carbon emissions as Luccioni et al. (2023), estimating 463 kg of CO2eq per GPU. This value is explicitly stated as the assumed embodied emissions per GPU based on available estimates.","463","kg/GPU","[""morrison2025"", ""luccioni2024""]","is_blank","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release embodied carbon emissions data for its hardware, but Morrison et al. (2025) assume the same embodied carbon emissions as Luccioni et al. (2023), estimating 463 kg of CO2eq per GPU. This value is explicitly stated as the assumed embodied emissions per GPU based on available estimates."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the amortized training cost of GPT-4 is $40M, while FLM-101B was trained within a $100,000 budget. To find the factor by which GPT-4's cost is greater, we divide $40,000,000 by $100,000, yielding a ratio of 400. The answer_unit is 'ratio', so the value is expressed as a pure number.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","['Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The context states that the amortized training cost of GPT-4 is $40M, while FLM-101B was trained within a $100,000 budget. To find the factor by which GPT-4's cost is greater, we divide $40,000,000 by $100,000, yielding a ratio of 400. The answer_unit is 'ratio', so the value is expressed as a pure number."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context repeatedly states that a single passenger round trip from San Francisco to New York produces approximately 1.2 tCO2e, as calculated by Google Flights using historical seat occupancy rates. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context repeatedly states that a single passenger round trip from San Francisco to New York produces approximately 1.2 tCO2e, as calculated by Google Flights using historical seat occupancy rates. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context explicitly states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which uses the average score as its metric. This value is directly reported in multiple sections, including the results summary and Table 4.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context explicitly states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which uses the average score as its metric. This value is directly reported in multiple sections, including the results summary and Table 4."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the Intel 4004 from 1971 ran at 740 kHz, and typical 2021 microprocessors ran at 5,000,000 kHz, resulting in a more than 6,750-fold improvement in processor clock speed. This matches the required unit 'fold'.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","The first commercially-produced microprocessor – Intel 4004 – was manufactured in 10,000 nm process technology in 1971, and ran at 740kHZ with 2,250 transistors [Intel]. Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the Intel 4004 from 1971 ran at 740 kHz, and typical 2021 microprocessors ran at 5,000,000 kHz, resulting in a more than 6,750-fold improvement in processor clock speed. This matches the required unit 'fold'."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context from [ref_id=patterson2021] explicitly states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to train GPT-3, based on the total floating-point operations (3.14E+23) and the measured performance of the V100 GPUs. The answer_unit is 'days', so the numeric value 14.8 is used directly.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context from [ref_id=patterson2021] explicitly states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to train GPT-3, based on the total floating-point operations (3.14E+23) and the measured performance of the V100 GPUs. The answer_unit is 'days', so the numeric value 14.8 is used directly."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context from ref_id=morrison2025 provides a table with GPU Power Usage values for different models at varying request rates. For the Llama 3.2 1B model at 8 request/s, the GPU Power Usage is explicitly listed as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: ... Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.","The context from ref_id=morrison2025 provides a table with GPU Power Usage values for different models at varying request rates. For the Llama 3.2 1B model at 8 request/s, the GPU Power Usage is explicitly listed as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia (which includes Loudoun, Prince William, and Fairfax counties) are approximately 13,000 tons of NOx, and this information is tied to permits issued between January 1, 2023, and December 1, 2024, as referenced in [60].","13000","tons","[""han2024""]","[""https://www.deq.virginia.gov/permits/air/issued-air-permits-for-data-centers""]","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia (which includes Loudoun, Prince William, and Fairfax counties) are approximately 13,000 tons of NOx, and this information is tied to permits issued between January 1, 2023, and December 1, 2024, as referenced in [60]."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days). The answer_unit is 'percent', so the saved time percentage is directly extracted as 72.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days). The answer_unit is 'percent', so the saved time percentage is directly extracted as 72."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides projections for premature deaths from U.S. data center scope-2 pollutants in 2028 (1,300), but no specific estimate is given for 2030. While scope-2 air pollution is projected to remain high into the future, no numerical projection for 2030 is available in the provided snippets.","is_blank","deaths","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause... approximately 600,000 asthma symptom cases and 1,300 premature deaths... The overall public health costs could reach more than $20 billion...","The context provides projections for premature deaths from U.S. data center scope-2 pollutants in 2028 (1,300), but no specific estimate is given for 2030. While scope-2 air pollution is projected to remain high into the future, no numerical projection for 2030 is available in the provided snippets."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that the 6.1 billion parameter model was trained for 8 days until 13% completion, and estimate that a full training run would take 60 days. This is calculated as (60/8) * 13% = 100%, confirming the 60-day estimate.","60","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","Dodge et al. (2022) state that the 6.1 billion parameter model was trained for 8 days until 13% completion, and estimate that a full training run would take 60 days. This is calculated as (60/8) * 13% = 100%, confirming the 60-day estimate."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context from ref_id=jegham2025 shows that DeepSeek-R1 (DS) consumes 29.078 Wh per long query, and for 1,000 queries, this would scale proportionally. Among all listed models, DeepSeek-R1 (DS) has the highest energy consumption per query, making it the model with the highest GPU energy consumption for 1,000 queries. The answer_unit is 'is_blank', so the answer_value is the model identifier.","DeepSeek-R1 (DS)","is_blank","[""jegham2025""]","is_blank","DeepSeek-R1 (DS)* consumes 29.078 ± 9.725 Wh for the 10k input-1.5k output prompt size (Table 4).","The context from ref_id=jegham2025 shows that DeepSeek-R1 (DS) consumes 29.078 Wh per long query, and for 1,000 queries, this would scale proportionally. Among all listed models, DeepSeek-R1 (DS) has the highest energy consumption per query, making it the model with the highest GPU energy consumption for 1,000 queries. The answer_unit is 'is_blank', so the answer_value is the model identifier."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This directly answers the question about the launch year.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This directly answers the question about the launch year."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context from ref_id=luccioni2025a states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.' This directly answers the question with the percentage value, and the answer_unit is 'percent', so the value is returned as a number.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context from ref_id=luccioni2025a states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.' This directly answers the question with the percentage value, and the answer_unit is 'percent', so the value is returned as a number."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the BLOOM model, a 176B parameter language model, consumed 520 MWh during pre-training, as listed in Table 1 of the Luccioni et al. (2025c) appendix. This value is explicitly provided and matches the required unit of MWh.","520","MWh","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel Organization Energy Consumption (MWh) GHG Emissions (tCO2e)\n...\nBLOOM 11 Big Science 520 30\n...","The context states that the BLOOM model, a 176B parameter language model, consumed 520 MWh during pre-training, as listed in Table 1 of the Luccioni et al. (2025c) appendix. This value is explicitly provided and matches the required unit of MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context from [ref_id=patterson2021] explicitly states that the Switch Transformer has 1500 billion parameters with only 0.1% activated per token. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number without the unit name.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","Sparse models can have many model parameters while requiring much less computation than dense models. The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context from [ref_id=patterson2021] explicitly states that the Switch Transformer has 1500 billion parameters with only 0.1% activated per token. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number without the unit name."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context explicitly states in Table 1 and the surrounding text that JetMoE-8B has 8 experts per layer, as indicated by the 'Nexperts' value of 8 in the hyperparameters table. This is further reinforced by the statement: 'we set the same number of experts to 8 and top-k to 2 for every layer.'","8","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context explicitly states in Table 1 and the surrounding text that JetMoE-8B has 8 experts per layer, as indicated by the 'Nexperts' value of 8 in the hyperparameters table. This is further reinforced by the statement: 'we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdowns for BlackMamba under different batch sizes and sparsity settings, but it does not explicitly state the total execution time for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. While Fig. 4 and Table III show execution time components and batch sizes, no total time value is given for this specific configuration.","is_blank","second","[""xia2024""]","is_blank","Fig. 4 illustrates execution time breakdown for Mixtral and BlackMamba, but no total time value is given for dense BlackMamba with batch size 30.","The context provides execution time breakdowns for BlackMamba under different batch sizes and sparsity settings, but it does not explicitly state the total execution time for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. While Fig. 4 and Table III show execution time components and batch sizes, no total time value is given for this specific configuration."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated for the English portion and 24.54 zettaFLOPs for Chinese. This directly answers the question about the English portion's cost in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated for the English portion and 24.54 zettaFLOPs for Chinese. This directly answers the question about the English portion's cost in zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context from ref_id=khan2025 describes an experimental setup for energy-efficient local inference in financial sentiment classification, specifying the use of an '11th Gen Intel(R) Core(TM) i7-1165G7 processor'. This matches the question's focus on the hardware processor used.","11th Gen Intel(R) Core(TM) i7-1165G7 processor","is_blank","[""khan2025""]","[""https://arxiv.org/abs/2412.00329""]","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context from ref_id=khan2025 describes an experimental setup for energy-efficient local inference in financial sentiment classification, specifying the use of an '11th Gen Intel(R) Core(TM) i7-1165G7 processor'. This matches the question's focus on the hardware processor used."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context from ref_id=luccioni2024 states that the study measured the energy requirements of 88 models across 10 tasks and 30 datasets, focusing on inference costs. This directly answers the question about the number of models sampled and analyzed.","88","models","[""luccioni2024""]","[""https://doi.org/10.1145/3630106.3658542""]","We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency.","The context from ref_id=luccioni2024 states that the study measured the energy requirements of 88 models across 10 tasks and 30 datasets, focusing on inference costs. This directly answers the question about the number of models sampled and analyzed."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context from ref_id=wu2021a explicitly states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This directly answers the question about the factor of decrease for LM training at 80% GPU utilization, and the answer_unit is 'multiplier', so the value is 3.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context from ref_id=wu2021a explicitly states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This directly answers the question about the factor of decrease for LM training at 80% GPU utilization, and the answer_unit is 'multiplier', so the value is 3."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) state that the total compute for model development, including experiments, fine-tuning, and evaluation, is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x. This is explicitly stated in multiple sections of the context, including Appendix A.6 and Section 2.5.","[1.2, 4]","multiplier","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/prices.py#L210-L294""]","we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) state that the total compute for model development, including experiments, fine-tuning, and evaluation, is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x. This is explicitly stated in multiple sections of the context, including Appendix A.6 and Section 2.5."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context from ref_id=dodge2022 states that the 6.1 billion parameter transformer model was trained for 8 days consuming 13.8 MWh, and that a full training run would take 60 days. The estimated total energy consumption for a full training run is calculated as (60/8) * 13.8 = 103.5 MWh, which is explicitly stated in the text.","103.5","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. ... Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh","The context from ref_id=dodge2022 states that the 6.1 billion parameter transformer model was trained for 8 days consuming 13.8 MWh, and that a full training run would take 60 days. The estimated total energy consumption for a full training run is calculated as (60/8) * 13.8 = 103.5 MWh, which is explicitly stated in the text."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context clearly states that technical efficiency gains in AI may lead to increased overall consumption due to Jevons’ Paradox, where efficiency improvements paradoxically spur higher resource use. This is explicitly described as an economic principle proposed by William Stanley Jevons in the 19th century.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context clearly states that technical efficiency gains in AI may lead to increased overall consumption due to Jevons’ Paradox, where efficiency improvements paradoxically spur higher resource use. This is explicitly described as an economic principle proposed by William Stanley Jevons in the 19th century."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context repeatedly states that global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on conservative estimates from scope-1 and scope-2 water usage calculations. This value is explicitly mentioned in multiple sections of the provided documents.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","Even considering the lower estimate, the combined scope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 – 6.6 billion cubic meters in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context repeatedly states that global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on conservative estimates from scope-1 and scope-2 water usage calculations. This value is explicitly mentioned in multiple sections of the provided documents."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3), which directly contradicts the claim in the question that Red AI is on the decline. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3), which directly contradicts the claim in the question that Red AI is on the decline. Therefore, the statement is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is compared to 76.74 days for training a 101B model from scratch. This value is directly provided in the text and matches the required unit 'days'.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is compared to 76.74 days for training a 101B model from scratch. This value is directly provided in the text and matches the required unit 'days'."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context repeatedly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for improvement. This directly contradicts the claim that a majority utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context repeatedly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for improvement. This directly contradicts the claim that a majority utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context from [ref_id=li2025b] and [ref_id=jegham2025] both state that GPT-3's training consumed an estimated 1287 MWh of electricity. This value is consistently reported across multiple sources and directly answers the question in the required unit (MWh).","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context from [ref_id=li2025b] and [ref_id=jegham2025] both state that GPT-3's training consumed an estimated 1287 MWh of electricity. This value is consistently reported across multiple sources and directly answers the question in the required unit (MWh)."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing the inference efficiency of AI models, as stated in [ref_id=luccioni2025c]. This matches the question's requirement for a collaborative project aiming to create a standardized method for comparing inference efficiency.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing the inference efficiency of AI models, as stated in [ref_id=luccioni2025c]. This matches the question's requirement for a collaborative project aiming to create a standardized method for comparing inference efficiency."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides kernel-level execution time breakdown for the MoE layer in Fig. 6, showing execution times in microseconds for different kernels under various batch sizes. For dense BlackMamba with batch size=30, the longest kernel is 'matmul(w1)' with a value of 2000 microseconds, as shown in the figure data.","2000","microseconds","[""xia2024""]","is_blank","Fig. 6 shows the kernel-level MoE time breakdown: matmul(w1) for BlackMamba at Dense(bsz=30) has an execution time breakdown of 2000 μs.","The context provides kernel-level execution time breakdown for the MoE layer in Fig. 6, showing execution times in microseconds for different kernels under various batch sizes. For dense BlackMamba with batch size=30, the longest kernel is 'matmul(w1)' with a value of 2000 microseconds, as shown in the figure data."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B sets 'the same number of experts to 8 and top-k to 2 for every layer', indicating that for each input token, 2 experts are selected for activation in every layer. This directly answers the question with the required unit 'experts'.","2","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context states that JetMoE-8B sets 'the same number of experts to 8 and top-k to 2 for every layer', indicating that for each input token, 2 experts are selected for activation in every layer. This directly answers the question with the required unit 'experts'."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models increased by 300,000x from 2012 to 2018, not 200,000x, as explicitly mentioned in multiple snippets from [ref_id=schwartz2019]. Therefore, the claim of a 200,000x increase is incorrect.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the amount of compute used to train deep learning models increased by 300,000x from 2012 to 2018, not 200,000x, as explicitly mentioned in multiple snippets from [ref_id=schwartz2019]. Therefore, the claim of a 200,000x increase is incorrect."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context from ref_id=wu2021b states that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction. The answer_unit is 'percent', so the value is returned as a number without the unit name.","6.4","percent","[""wu2021b""]","is_blank","the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context from ref_id=wu2021b states that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This directly answers the question with the number of regions, and the answer_unit is 'data centers', so the numeric value 22 is used.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This directly answers the question with the number of regions, and the answer_unit is 'data centers', so the numeric value 22 is used."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project, as stated in the context. In 2025, the age of the field would be 2025 - 1956 = 69 years. The context confirms this founding date multiple times, including in Appendix I and the executive summary.","69","years","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project, as stated in the context. In 2025, the age of the field would be 2025 - 1956 = 69 years. The context confirms this founding date multiple times, including in Appendix I and the executive summary."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context clearly states that the 'five cars' estimate originates from a 2019 study by Strubell et al., which quantified the energy use and emissions of neural architecture search (NAS) — a large-scale procedure used to identify optimized model architectures. This process is explicitly described as infrequently performed due to its high resource demands and because its results are reused to reduce emissions in subsequent training workloads.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context clearly states that the 'five cars' estimate originates from a 2019 study by Strubell et al., which quantified the energy use and emissions of neural architecture search (NAS) — a large-scale procedure used to identify optimized model architectures. This process is explicitly described as infrequently performed due to its high resource demands and because its results are reused to reduce emissions in subsequent training workloads."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question with the percentage decrease in energy use.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question with the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states in Table III that for BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K dataset (denoted as GS), the maximum batch size supported on an NVIDIA A40 GPU with 48GB memory is 20 samples. This value is explicitly listed in the table under the 'CS' and 'MATH' datasets, where 'CS' refers to Hellaswag (HE) and 'GS' refers to GSM8K.","20","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context states in Table III that for BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K dataset (denoted as GS), the maximum batch size supported on an NVIDIA A40 GPU with 48GB memory is 20 samples. This value is explicitly listed in the table under the 'CS' and 'MATH' datasets, where 'CS' refers to Hellaswag (HE) and 'GS' refers to GSM8K."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that in 2023, Amazon added seven solar projects with battery energy storage and now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This directly provides the energy storage capacity as of 2023 in gigawatts (GW), matching the required answer_unit.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context states that in 2023, Amazon added seven solar projects with battery energy storage and now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This directly provides the energy storage capacity as of 2023 in gigawatts (GW), matching the required answer_unit."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question using the unit 'flights' as specified.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question using the unit 'flights' as specified."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context provides data on AI energy consumption and carbon intensity globally and in specific regions like the U.S., China, and Germany, but it does not contain any specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. While coal usage is mentioned in relation to training models and global grid carbon intensity, no regional breakdown for AI inference in Asia is provided.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides data on AI energy consumption and carbon intensity globally and in specific regions like the U.S., China, and Germany, but it does not contain any specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. While coal usage is mentioned in relation to training models and global grid carbon intensity, no regional breakdown for AI inference in Asia is provided."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally, but it does not specify a separate PUE value for AI-dedicated data centers. While AI workloads are increasingly hosted in data centers, no distinct PUE figure is provided for AI-dedicated facilities.","is_blank","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally, but it does not specify a separate PUE value for AI-dedicated data centers. While AI workloads are increasingly hosted in data centers, no distinct PUE figure is provided for AI-dedicated facilities."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference, as noted in multiple snippets including Table 1 and the abstract. The answer_unit is 'parameters', so the numeric value 2B (2 billion) is returned as the activated parameter count.","2000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference, as noted in multiple snippets including Table 1 and the abstract. The answer_unit is 'parameters', so the numeric value 2B (2 billion) is returned as the activated parameter count."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","","","tCO2e","[]","is_blank","is_blank",""
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon had 2,600 electric delivery vans in the U.S., and in 2023, this increased to 11,800. The total number added is the difference: 11,800 - 2,600 = 9,200 electric delivery vans. This is supported by the U.S. fleet data in the 'Last Mile Electric Delivery Vehicles by Region' table and the corresponding text.","9200","electric delivery vans","[""amazon2023""]","is_blank","United States\n• Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon had 2,600 electric delivery vans in the U.S., and in 2023, this increased to 11,800. The total number added is the difference: 11,800 - 2,600 = 9,200 electric delivery vans. This is supported by the U.S. fleet data in the 'Last Mile Electric Delivery Vehicles by Region' table and the corresponding text."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling (ref_id=morrison2025). Therefore, the statement that they do not include such emissions is false.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling (ref_id=morrison2025). Therefore, the statement that they do not include such emissions is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for the 'o3' model under the '10k input-1.5k output' long prompt scenario in Table 4, which lists the value as 12.222 Wh. This matches the required unit (Wh) and corresponds to the longest prompt length evaluated.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). ... o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context provides energy consumption values for the 'o3' model under the '10k input-1.5k output' long prompt scenario in Table 4, which lists the value as 12.222 Wh. This matches the required unit (Wh) and corresponds to the longest prompt length evaluated."
"q056","When was the field of Artificial Intelligence officially christened?","The context repeatedly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, specifically the Dartmouth Summer Research Project on Artificial Intelligence. This directly answers the question with the year as the required format.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context repeatedly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, specifically the Dartmouth Summer Research Project on Artificial Intelligence. This directly answers the question with the year as the required format."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide the specific water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While it mentions Google's sustainability reports and a global scope-1 water efficiency assumption of 1.2 L/kWh for withdrawal and 1.0 L/kWh for consumption based on Google and Equinix data, these values are not specified as 2024 figures nor explicitly tied to AI-dedicated centers. No exact WUE value for Google in 2024 is stated.","is_blank","WUE","[""li2025b""]","is_blank","is_blank","The context does not provide the specific water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While it mentions Google's sustainability reports and a global scope-1 water efficiency assumption of 1.2 L/kWh for withdrawal and 1.0 L/kWh for consumption based on Google and Equinix data, these values are not specified as 2024 figures nor explicitly tied to AI-dedicated centers. No exact WUE value for Google in 2024 is stated."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context from ref_id=wu2021b explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, citing the International Energy Agency. This directly supports the truth of the statement.","1","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context from ref_id=wu2021b explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, citing the International Energy Agency. This directly supports the truth of the statement."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a maximum generation length of 512, it takes about 3-4 Joules per output token for LLaMA 65B, as explicitly mentioned in multiple snippets. This matches the required unit of 'joules per token'.","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a maximum generation length of 512, it takes about 3-4 Joules per output token for LLaMA 65B, as explicitly mentioned in multiple snippets. This matches the required unit of 'joules per token'."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context from [ref_id=wu2021a] explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduced the overall RM2 model size by 15%. The answer_unit is 'percent', so the value is returned as a number without the unit symbol.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context from [ref_id=wu2021a] explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduced the overall RM2 model size by 15%. The answer_unit is 'percent', so the value is returned as a number without the unit symbol."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the claim AI can reduce global GHG emissions by 5-10% is traced to a 2021 BCG report and a 2023 Google-commissioned BCG report, but the reasoning and underlying calculations are unclear and lack scientific grounding. It explicitly notes that applying individual project observations to global emissions lacks scientific validity and that rigorous methodologies for avoided emissions are not followed.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding","The context states that the claim AI can reduce global GHG emissions by 5-10% is traced to a 2021 BCG report and a 2023 Google-commissioned BCG report, but the reasoning and underlying calculations are unclear and lack scientific grounding. It explicitly notes that applying individual project observations to global emissions lacks scientific validity and that rigorous methodologies for avoided emissions are not followed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from [ref_id=patterson2021] explicitly states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the truth of the statement.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context from [ref_id=patterson2021] explicitly states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the truth of the statement."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context from [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly provides the requested cost in USD.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context from [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly provides the requested cost in USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This is explicitly mentioned in the execution time breakdown section, and the GPU used in the experiments is the NVIDIA A40 with 48GB memory, which matches the question's specification.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This is explicitly mentioned in the execution time breakdown section, and the GPU used in the experiments is the NVIDIA A40 with 48GB memory, which matches the question's specification."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate performs 1 billion queries per day, and the average energy for a comparable multi-purpose model, Flan-T5-xxl, is 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate to 1 billion queries: (0.083 kWh / 1,000 queries) * 1,000,000,000 queries = 83,000 kWh, which equals 83 MWh. The unit requested is MWh, so the value is expressed accordingly.","83","MWh","[""luccioni2024""]","[""https://blog.google/products/translate/ten-years-of-google-translate/""]","Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.","The context states that Google Translate performs 1 billion queries per day, and the average energy for a comparable multi-purpose model, Flan-T5-xxl, is 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate to 1 billion queries: (0.083 kWh / 1,000 queries) * 1,000,000,000 queries = 83,000 kWh, which equals 83 MWh. The unit requested is MWh, so the value is expressed accordingly."
"q067","What was the average global data center PUE in 2023?","The context from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. This value is repeated in multiple sections of the context, including direct quotes and references to Statista data, confirming consistency. The answer_unit is 'PUE', so the value is returned as a numeric value without units.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. This value is repeated in multiple sections of the context, including direct quotes and references to Statista data, confirming consistency. The answer_unit is 'PUE', so the value is returned as a numeric value without units."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context provides detailed information about Amazon's renewable energy projects, including wind farms and power purchase agreements, but there is no mention of Microsoft contracting wind turbines to power Azure AI clusters in 2023. While Microsoft's sustainability goals and energy use are referenced, no specific number of wind turbines contracted by Microsoft is provided.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context provides detailed information about Amazon's renewable energy projects, including wind farms and power purchase agreements, but there is no mention of Microsoft contracting wind turbines to power Azure AI clusters in 2023. While Microsoft's sustainability goals and energy use are referenced, no specific number of wind turbines contracted by Microsoft is provided."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that for Gemini Ultra, R&D staff costs make up 49% of total amortized model development costs when equity is included. This is explicitly mentioned as the highest fraction among the frontier models analyzed.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that for Gemini Ultra, R&D staff costs make up 49% of total amortized model development costs when equity is included. This is explicitly mentioned as the highest fraction among the frontier models analyzed."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the inaugural panel had 17 members. The answer_unit is 'people', so the numeric value is returned as a number.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context states that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the inaugural panel had 17 members. The answer_unit is 'people', so the numeric value is returned as a number."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context from [ref_id=wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question in terms of percentage, matching the required answer_unit.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context from [ref_id=wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question in terms of percentage, matching the required answer_unit."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states that models with more parameters do not necessarily consume more energy during inference, as shown by the case of Phi-3 Mini (3.8B) and Phi-3 Small (7B), where the larger model consumes less energy due to architectural differences like Grouped Query Attention (GQA). This directly contradicts the claim that more parameters always mean higher energy consumption.","0","is_blank","[""chung2025""]","is_blank","Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The context explicitly states that models with more parameters do not necessarily consume more energy during inference, as shown by the case of Phi-3 Mini (3.8B) and Phi-3 Small (7B), where the larger model consumes less energy due to architectural differences like Grouped Query Attention (GQA). This directly contradicts the claim that more parameters always mean higher energy consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind,' which directly contradicts the claim in the question. Therefore, the statement is false.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind,' which directly contradicts the claim in the question. Therefore, the statement is false."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context provides estimates for GPT-4o's annual carbon emissions in 2025 (138,125 to 163,441 tCO2e) and mentions OpenAI's API usage trends, but no data is available for January 2024 specifically. There is no direct or indirect estimate for emissions from OpenAI's API requests during that month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context provides estimates for GPT-4o's annual carbon emissions in 2025 (138,125 to 163,441 tCO2e) and mentions OpenAI's API usage trends, but no data is available for January 2024 specifically. There is no direct or indirect estimate for emissions from OpenAI's API requests during that month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context from [ref_id=luccioni2025c] states that Meta reports their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is based on a 2019 study and is not representative of modern LLM training emissions.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context from [ref_id=luccioni2025c] states that Meta reports their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is based on a 2019 study and is not representative of modern LLM training emissions."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context from ref_id=wu2021a explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021, which corresponds to the timeframe from Yr1-Q1 to Yr2-Q2. This is confirmed in multiple sections, including Figure 2(d) and the key takeaways.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context from ref_id=wu2021a explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021, which corresponds to the timeframe from Yr1-Q1 to Yr2-Q2. This is confirmed in multiple sections, including Figure 2(d) and the key takeaways."
"q079","How many miles is the Earth from the Sun?","The provided context contains information about distances such as the round-trip between Los Angeles and New York City, and emissions from AI training and car travel, but it does not mention the distance from the Earth to the Sun. Therefore, the context does not support an answer.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context contains information about distances such as the round-trip between Los Angeles and New York City, and emissions from AI training and car travel, but it does not mention the distance from the Earth to the Sun. Therefore, the context does not support an answer."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match, as noted in multiple snippets including those from ref_id=stone2022 and ref_id=schwartz2019. The question asks for a True/False answer, so the answer_value must be '1' for True.","1","is_blank","[""stone2022"", ""schwartz2019""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""https://www.nature.com/articles/nature24270""]","The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context clearly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match, as noted in multiple snippets including those from ref_id=stone2022 and ref_id=schwartz2019. The question asks for a True/False answer, so the answer_value must be '1' for True."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly states that 'Continuous batching mitigates [GPU under-utilization] by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time' (ref_id=fernandez2025). This matches the question exactly, and since the answer_unit is 'is_blank', the answer_value must be the exact name of the strategy as stated.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context explicitly states that 'Continuous batching mitigates [GPU under-utilization] by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time' (ref_id=fernandez2025). This matches the question exactly, and since the answer_unit is 'is_blank', the answer_value must be the exact name of the strategy as stated."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process (dSFT and dDPO) takes 60 H100 GPU hours. This is explicitly mentioned in the section on alignment details, where dSFT and dDPO fine-tuning are described as part of the alignment process.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process (dSFT and dDPO) takes 60 H100 GPU hours. This is explicitly mentioned in the section on alignment details, where dSFT and dDPO fine-tuning are described as part of the alignment process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for the 100 TPS SLO in the offline workload, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%. The answer_unit is 'percent', so the value is reported as 26.7.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that for the 100 TPS SLO in the offline workload, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%. The answer_unit is 'percent', so the value is reported as 26.7."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from ref_id=luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This matches the required unit of 'g CO2eq' for answer_value.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context from ref_id=luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This matches the required unit of 'g CO2eq' for answer_value."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context from [ref_id=luccioni2025c] states that GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on model size and task complexity. This range directly answers the question, and the answer_unit is specified as 'Wh', so the numeric range is reported without unit names.","[0.06, 3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context from [ref_id=luccioni2025c] states that GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on model size and task complexity. This range directly answers the question, and the answer_unit is specified as 'Wh', so the numeric range is reported without unit names."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that it is difficult to define universal or generalizable guidelines for AI ethics due to the diversity of AI approaches and contextual factors, and that there is no one-size-fits-all solution for either ethics or sustainability. This directly contradicts the idea that a universal approach can be developed.","0","is_blank","[""luccioni2025b""]","is_blank","‘However, given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.’ and 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.'","The context explicitly states that it is difficult to define universal or generalizable guidelines for AI ethics due to the diversity of AI approaches and contextual factors, and that there is no one-size-fits-all solution for either ethics or sustainability. This directly contradicts the idea that a universal approach can be developed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context from [ref_id=patterson2021] explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh in 2021, as cited from the U.S. Energy Information Administration [USE21]. This matches the required answer_unit of kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021"", ""USE21""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context from [ref_id=patterson2021] explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh in 2021, as cited from the U.S. Energy Information Administration [USE21]. This matches the required answer_unit of kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context explicitly states that Hivemind [39] is a PyTorch-based framework developed to enable collaborative deep learning training and is used in the paper to enable distributed spot training across clouds and continents. It handles peer failures and is central to the study's approach.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context explicitly states that Hivemind [39] is a PyTorch-based framework developed to enable collaborative deep learning training and is used in the paper to enable distributed spot training across clouds and continents. It handles peer failures and is central to the study's approach."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly states that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and includes the portrayal of an AI system’s societal impacts and environmental footprint. This matches the question’s request for the proposed term.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.","The context explicitly states that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and includes the portrayal of an AI system’s societal impacts and environmental footprint. This matches the question’s request for the proposed term."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on German public administration texts (FKTG dataset). This directly identifies the model type that achieved the highest accuracy.","linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on German public administration texts (FKTG dataset). This directly identifies the model type that achieved the highest accuracy."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context explicitly states that Chen et al. developed and deployed 'Lamina' as an LLM inference system that incorporates model-attention disaggregation. This is mentioned in both the abstract and conclusion sections of the paper, directly answering the question.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context explicitly states that Chen et al. developed and deployed 'Lamina' as an LLM inference system that incorporates model-attention disaggregation. This is mentioned in both the abstract and conclusion sections of the paper, directly answering the question."
"q093","How many parameters does the largest T5 model have?","The context mentions Flan-T5-XXL as the biggest model in the Flan-T5 family with 11 billion parameters, as stated in Table 3 from ref_id=luccioni2024. This is the largest T5 model explicitly mentioned in the provided snippets.","11000000000","parameters","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3593013.3593047""]","Flan-T5-xxl 11B 11.48 0.083","The context mentions Flan-T5-XXL as the biggest model in the Flan-T5 family with 11 billion parameters, as stated in Table 3 from ref_id=luccioni2024. This is the largest T5 model explicitly mentioned in the provided snippets."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters, as noted in multiple sections including the abstract, introduction, and hyperparameters table. The '8B' in the model name directly refers to the total number of parameters, and this is further confirmed by the line 'JetMoE-8B has 8B parameters while only activating 2B for each input token'.","8000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters, as noted in multiple sections including the abstract, introduction, and hyperparameters table. The '8B' in the model name directly refers to the total number of parameters, and this is further confirmed by the line 'JetMoE-8B has 8B parameters while only activating 2B for each input token'."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, as noted in both [ref_id=li2025b] and [ref_id=luccioni2025a]. The answer_unit is 'percent', so the value is reported as a number without the unit name.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, as noted in both [ref_id=li2025b] and [ref_id=luccioni2025a]. The answer_unit is 'percent', so the value is reported as a number without the unit name."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context from [ref_id=khan2025] explicitly defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I under 'COMMON CARBON EMISSION METRICS IN GREEN AI'. This matches the exact definition asked in the question, and since answer_unit is 'is_blank', the answer_value must be the metric name as identified in the context.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context from [ref_id=khan2025] explicitly defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I under 'COMMON CARBON EMISSION METRICS IN GREEN AI'. This matches the exact definition asked in the question, and since answer_unit is 'is_blank', the answer_value must be the metric name as identified in the context."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states in Table 2 that for the 101B stage (final growth stage) of FLM-101B training, the FLOPs utilization rate is 52.88%. The answer_unit is 'percent', so the value is returned as a number without the unit name.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages... 101 4 4 12 192 2160 165 52.88%","The context states in Table 2 that for the 101B stage (final growth stage) of FLM-101B training, the FLOPs utilization rate is 52.88%. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context from [ref_id=cottier2024] states that the amortized hardware and energy cost for GPT-4 is $40M, and this is consistently referenced as the most expensive publicly announced training run at the time of publication. The unit is USD, so the value is expressed as a number.","40000000","USD","[""cottier2024""]","is_blank","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context from [ref_id=cottier2024] states that the amortized hardware and energy cost for GPT-4 is $40M, and this is consistently referenced as the most expensive publicly announced training run at the time of publication. The unit is USD, so the value is expressed as a number."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for a Transformer-based universal translation model by 810×. This is further detailed in Figure 7 and associated text, where platform-level caching, GPU acceleration, and algorithmic optimizations collectively achieve over 800× reduction, with the aggregate optimization reaching 810×. The answer_unit is 'multiplier', so the value is reported as a number.","810","multiplier","[""wu2021a""]","is_blank","Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for a Transformer-based universal translation model by 810×. This is further detailed in Figure 7 and associated text, where platform-level caching, GPU acceleration, and algorithmic optimizations collectively achieve over 800× reduction, with the aggregate optimization reaching 810×. The answer_unit is 'multiplier', so the value is reported as a number."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP, training across four continents (C-8) resulted in a 41% performance drop compared to the fully local experiment (A-8), meaning the throughput achieved was 59% of the local throughput (100% - 41% = 59%). This is expressed as a multiplier of 0.59.","0.59","multiplier","[""erben2023""]","is_blank","intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for NLP, training across four continents (C-8) resulted in a 41% performance drop compared to the fully local experiment (A-8), meaning the throughput achieved was 59% of the local throughput (100% - 41% = 59%). This is expressed as a multiplier of 0.59."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This is repeated multiple times across the snippets, including in the 'Delivering Water Replenishment' section and the summary metrics.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This is repeated multiple times across the snippets, including in the 'Delivering Water Replenishment' section and the summary metrics."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context clearly states that custom tags reduce energy consumption across zero-shot, one-shot, and few-shots techniques during code completion tasks, as shown in the findings of RQ1 and multiple sections including the abstract and experimental results.","1","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.","The context clearly states that custom tags reduce energy consumption across zero-shot, one-shot, and few-shots techniques during code completion tasks, as shown in the findings of RQ1 and multiple sections including the abstract and experimental results."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more than in 2023. The question asks for the number of data center GPUs shipped in 2024, and the unit is specified as 'GPUs', so the numeric value is directly extracted.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more than in 2023. The question asks for the number of data center GPUs shipped in 2024, and the unit is specified as 'GPUs', so the numeric value is directly extracted."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy cost. This directly answers the question with the required unit 'percent'.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy cost. This directly answers the question with the required unit 'percent'."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context repeatedly states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, as cited in references wu2021a, wu2021b, and patterson2021. This value is explicitly mentioned as Facebook's PUE in 2020 and is consistent across multiple sources.","1.1","PUE","[""wu2021a"", ""wu2021b"", ""patterson2021""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers.","The context repeatedly states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, as cited in references wu2021a, wu2021b, and patterson2021. This value is explicitly mentioned as Facebook's PUE in 2020 and is consistent across multiple sources."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly mentions the Finnish project 'ETAIROS (Ethical AI for the Governance of the Society)' as the one that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. Since the answer_unit is 'is_blank', the answer_value must be the exact identifier/name from the context, which is 'ETAIROS'.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context explicitly mentions the Finnish project 'ETAIROS (Ethical AI for the Governance of the Society)' as the one that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. Since the answer_unit is 'is_blank', the answer_value must be the exact identifier/name from the context, which is 'ETAIROS'."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context explicitly states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, referring to amortized hardware CapEx + energy costs. This matches the required unit (USD) and specifies the exact value for Gemini Ultra.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context explicitly states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, referring to amortized hardware CapEx + energy costs. This matches the required unit (USD) and specifies the exact value for Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that while the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, it lacks sufficient emphasis on environmental factors. However, the authors argue that environmental risks must be interpreted as included within the fundamental rights protected by the AI Act (Art. 1 and Recitals 1, 2, 8), and that risk assessments should include environmental risks to meet the Act’s objectives. This is a proposed interpretation, not a current requirement. Therefore, as written, the AI Act does not explicitly require environmental risks in assessments.","0","is_blank","[""ebert2024""]","is_blank","Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act’s objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.","The context states that while the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, it lacks sufficient emphasis on environmental factors. However, the authors argue that environmental risks must be interpreted as included within the fundamental rights protected by the AI Act (Art. 1 and Recitals 1, 2, 8), and that risk assessments should include environmental risks to meet the Act’s objectives. This is a proposed interpretation, not a current requirement. Therefore, as written, the AI Act does not explicitly require environmental risks in assessments."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as specified in the NAAQS primary standards [ref_id=han2024]. The answer_unit is µg/m³, so the value is returned as a number without the unit name.","9","µg/m³","[""han2024""]","is_blank","For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9µg/m 3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48].","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as specified in the NAAQS primary standards [ref_id=han2024]. The answer_unit is µg/m³, so the value is returned as a number without the unit name."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context from ref_id=luccioni2025a states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. Therefore, one Kindle produces the same CO2 as 115 physical print books, matching the required answer_unit of 'books'.","115","books","[""luccioni2025a""]","is_blank","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context from ref_id=luccioni2025a states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. Therefore, one Kindle produces the same CO2 as 115 physical print books, matching the required answer_unit of 'books'."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context from ref_id=han2024 explicitly states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-impacted communities, with multiple mentions of the 200-fold difference in health costs. The answer_unit is 'multiplier', so the numeric value 200 is returned.","200","multiplier","[""han2024""]","is_blank","The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context from ref_id=han2024 explicitly states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-impacted communities, with multiple mentions of the 200-fold difference in health costs. The answer_unit is 'multiplier', so the numeric value 200 is returned."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context from [ref_id=zschache2025] Table B1 shows that the DS Llama 70B model consumed 702.06 Wh of energy during inference on the FKTG dataset, measured over 10 runs on a single node. The answer_unit is 'Wh', so the value is reported as a number without unit name.","702.06","Wh","[""zschache2025""]","is_blank","Table B1: DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context from [ref_id=zschache2025] Table B1 shows that the DS Llama 70B model consumed 702.06 Wh of energy during inference on the FKTG dataset, measured over 10 runs on a single node. The answer_unit is 'Wh', so the value is reported as a number without unit name."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions Dodge et al. (2022) in relation to measuring electricity consumption and carbon emissions for training language models, but it does not specify the total number of parameters in any particular model analyzed by them. Therefore, the required numeric value cannot be determined from the provided snippets.","is_blank","parameters","[""luccioni2024"", ""luccioni2023""]","is_blank","is_blank","The context mentions Dodge et al. (2022) in relation to measuring electricity consumption and carbon emissions for training language models, but it does not specify the total number of parameters in any particular model analyzed by them. Therefore, the required numeric value cannot be determined from the provided snippets."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context repeatedly describes the Jevons Paradox as the phenomenon where improvements in efficiency lead to increased consumption and overall resource use, particularly in the context of AI and energy. This is explicitly stated in multiple snippets, including Luccioni et al. (2025), which notes that efficiency gains in AI paradoxically spur increased consumption.","Jevons Paradox","is_blank","[""luccioni2025a"", ""morrison2025"", ""fernandez2025"", ""jegham2025""]","is_blank","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The context repeatedly describes the Jevons Paradox as the phenomenon where improvements in efficiency lead to increased consumption and overall resource use, particularly in the context of AI and energy. This is explicitly stated in multiple snippets, including Luccioni et al. (2025), which notes that efficiency gains in AI paradoxically spur increased consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that GPT-3 training consumed 1,287 MWh of electricity, while Meena's training consumed 232 MWh. To find how many Meena training runs equal one GPT-3 run, we divide 1,287 by 232, yielding approximately 5.55. This matches the multiplier format specified in answer_unit.","5.55","multiplier","[""patterson2021""]","is_blank","Energy Consumption (MWh): GPT-3 = 1,287; Meena = 232","The context states that GPT-3 training consumed 1,287 MWh of electricity, while Meena's training consumed 232 MWh. To find how many Meena training runs equal one GPT-3 run, we divide 1,287 by 232, yielding approximately 5.55. This matches the multiplier format specified in answer_unit."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","Table 2 in the Luccioni et al. (2024) study explicitly lists the mean energy consumption for 1,000 image generation inferences as 2.907 kWh. This value is directly stated in the table and corroborated in the surrounding text, which emphasizes image generation as the most energy-intensive task.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
... image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","Table 2 in the Luccioni et al. (2024) study explicitly lists the mean energy consumption for 1,000 image generation inferences as 2.907 kWh. This value is directly stated in the table and corroborated in the surrounding text, which emphasizes image generation as the most energy-intensive task."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for an American life, average, in one year is 36,156 lbs. This matches the required unit (lbs) in the question.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for an American life, average, in one year is 36,156 lbs. This matches the required unit (lbs) in the question."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides per-household health cost data for counties in West Virginia for the period 2019–2023, with WV Marion having the highest value at $1218.3. However, there are no projections for 2030 in the provided context; the only future projection is for 2028. Since the question asks for 2030 projections and no such data exists in the context, the answer cannot be determined.","is_blank","is_blank","[""han2024""]","is_blank","Figure 6 and Table in context show per-household health costs for 2019–2023; no 2030 projections are mentioned.","The context provides per-household health cost data for counties in West Virginia for the period 2019–2023, with WV Marion having the highest value at $1218.3. However, there are no projections for 2030 in the provided context; the only future projection is for 2028. Since the question asks for 2030 projections and no such data exists in the context, the answer cannot be determined."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context shows that Mistral-small's carbon emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization, which is a reduction by a factor of 0.75 (0.015 / 0.020). The question asks for the multiplier, which is the ratio of post-optimization to pre-optimization emissions.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III: Before Optimization CO2 (kg): 0.020; After Optimization CO2 (kg): 0.015","The context shows that Mistral-small's carbon emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization, which is a reduction by a factor of 0.75 (0.015 / 0.020). The question asks for the multiplier, which is the ratio of post-optimization to pre-optimization emissions."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context from [ref_id=luccioni2024] provides the training energy for BLOOMz-7B as 51,686 kWh and the fine-tuning energy as 7,571 kWh. Adding these values gives the combined energy cost. The answer_unit is 'kWh', so the result is expressed as a numeric value without the unit name.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","The context from [ref_id=luccioni2024] provides the training energy for BLOOMz-7B as 51,686 kWh and the fine-tuning energy as 7,571 kWh. Adding these values gives the combined energy cost. The answer_unit is 'kWh', so the result is expressed as a numeric value without the unit name."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that FLM-101B is the final model in a growth strategy that progresses from 16B to 51B to 101B parameters, and it is referred to as the '101B (i.e., FLM-101B) parameters' model. This confirms the final model has 101 billion parameters.","101000000000","parameters","[""li2025a""]","is_blank","Model Sizes. Benefiting from our growth strategy, we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.","The context explicitly states that FLM-101B is the final model in a growth strategy that progresses from 16B to 51B to 101B parameters, and it is referred to as the '101B (i.e., FLM-101B) parameters' model. This confirms the final model has 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context states that training a 6.1B parameter model for 13% of completion consumed 13,812.4 kWh, and a full training run is estimated at 103,593 kWh (Dodge2022). For BLOOMz-7B, training energy is 51,686 kWh and cost parity is 592,570,000 inferences (Luccioni2024). Since 103,593 kWh is roughly double 51,686 kWh, the number of inferences needed is approximately double 592,570,000, yielding about 1.185 billion inferences.","1185140000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://doi.org/10.1145/3531146.3533133"", ""https://doi.org/10.1145/3592927.3592932""]","['We estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!', 'BLOOMz-7B Training energy (kWh) 51,686 ... Cost parity (# inferences) 592,570,000']","The context states that training a 6.1B parameter model for 13% of completion consumed 13,812.4 kWh, and a full training run is estimated at 103,593 kWh (Dodge2022). For BLOOMz-7B, training energy is 51,686 kWh and cost parity is 592,570,000 inferences (Luccioni2024). Since 103,593 kWh is roughly double 51,686 kWh, the number of inferences needed is approximately double 592,570,000, yielding about 1.185 billion inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that in the 2024 study 'Power Hungry Processing', a total of 754.66 kWh of energy was used for all model experimentation and evaluation. This value is repeated multiple times in the provided snippets, confirming its accuracy and relevance to the question.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that in the 2024 study 'Power Hungry Processing', a total of 754.66 kWh of energy was used for all model experimentation and evaluation. This value is repeated multiple times in the provided snippets, confirming its accuracy and relevance to the question."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context explicitly states in Table 5 and the surrounding text that for the BLOOMz-7B model, the cost parity — the number of inferences required for the cumulative inference energy cost to equal the combined training and fine-tuning energy cost — is 592,570,000 inferences. This value is directly provided and matches the required unit 'inferences'.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5: Cost parity (# inferences) for BLOOMz-7B is 592,570,000.","The context explicitly states in Table 5 and the surrounding text that for the BLOOMz-7B model, the cost parity — the number of inferences required for the cumulative inference energy cost to equal the combined training and fine-tuning energy cost — is 592,570,000 inferences. This value is directly provided and matches the required unit 'inferences'."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context from ref_id=zschache2025 states that the data used in the experiments originates from the FKTG-dataset, which contains population objections related to the selection of a repository site for high-level radioactive waste in Germany. The dataset is explicitly named and linked, and the text categorization experiments are based on this dataset.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context from ref_id=zschache2025 states that the data used in the experiments originates from the FKTG-dataset, which contains population objections related to the selection of a repository site for high-level radioactive waste in Germany. The dataset is explicitly named and linked, and the text categorization experiments are based on this dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context provides no specific data on freshwater consumption by Meta's Llama 3 inference serving clusters in 2024. While water usage for GPT-3 and GPT-4o inference is estimated, and Meta's scope-2 water consumption for its global data center fleet in 2023 is mentioned (3.7 L/kWh), there is no figure for Llama 3 inference in 2024.","is_blank","liters","[]","is_blank","is_blank","The context provides no specific data on freshwater consumption by Meta's Llama 3 inference serving clusters in 2024. While water usage for GPT-3 and GPT-4o inference is estimated, and Meta's scope-2 water consumption for its global data center fleet in 2023 is mentioned (3.7 L/kWh), there is no figure for Llama 3 inference in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context provides no information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While it mentions rare earth metal usage in H100 manufacturing (0.1% by mass) and environmental impacts of mining, there is no data on recycling rates or use of recycled materials.","is_blank","percent","[""morrison2025"", ""luccioni2025a""]","[""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/""]","assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016)","The context provides no information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While it mentions rare earth metal usage in H100 manufacturing (0.1% by mass) and environmental impacts of mining, there is no data on recycling rates or use of recycled materials."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that 3.2 net tCO2e from Evolved Transformer training is equivalent to ~3 passengers taking a round trip between San Francisco and New York, as each passenger round trip emits ~1.2t CO2e (3.2 / 1.2 ≈ 2.67, rounded to ~3). This is explicitly stated in the context using the same metric for perspective.","3","passengers","[""patterson2021""]","is_blank","To put 3.2 net tCO2e into perspective, Table 1 and Appendix A use Google Flights to calculate the CO2e for the average direct round trip flights between SFO and JFK as 180.4t. The Boeing 767 that United Airlines flies on that route has 175 seats. Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip. Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that 3.2 net tCO2e from Evolved Transformer training is equivalent to ~3 passengers taking a round trip between San Francisco and New York, as each passenger round trip emits ~1.2t CO2e (3.2 / 1.2 ≈ 2.67, rounded to ~3). This is explicitly stated in the context using the same metric for perspective."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context from ref_id=luccioni2025c states that in May 2025, OpenRouter data showed that 84% of LLM token usage was through models with no environmental disclosure. This directly answers the question with the required unit 'percent'.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context from ref_id=luccioni2025c states that in May 2025, OpenRouter data showed that 84% of LLM token usage was through models with no environmental disclosure. This directly answers the question with the required unit 'percent'."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as shown in Table II, which lists the minimum GPU count for each model size without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 13B 2 64 1 64","The context states that for LLaMA-13B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as shown in Table II, which lists the minimum GPU count for each model size without compression or quantization."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context from [ref_id=dodge2022] states that if the 6.1 billion parameter transformer model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region. This range is explicitly provided in the text and matches the required answer_unit of 'metric tons'.","[21, 78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context from [ref_id=dodge2022] states that if the 6.1 billion parameter transformer model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region. This range is explicitly provided in the text and matches the required answer_unit of 'metric tons'."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any specific data on total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. While quantization is discussed as reducing emissions by up to 45% in one study, no aggregate or total avoided emissions figure is reported for 2023.","is_blank","tCO2e","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization...","The context does not provide any specific data on total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. While quantization is discussed as reducing emissions by up to 45% in one study, no aggregate or total avoided emissions figure is reported for 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy, as highlighted in Fig. 9 and repeated in multiple sections. The answer_unit is 'percent', so the value is returned as a number.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy, as highlighted in Fig. 9 and repeated in multiple sections. The answer_unit is 'percent', so the value is returned as a number."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context from Chen et al. (2024) lists the price per chip for NVIDIA H20 as $4.63/hr in Table 1. Although the question references 'Chen et al. (2025)', the only available context is from Chen et al. (2024), which provides the required value. The answer_unit is 'USD per hour', so the value is returned as a number without the unit name.","4.63","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context from Chen et al. (2024) lists the price per chip for NVIDIA H20 as $4.63/hr in Table 1. Although the question references 'Chen et al. (2025)', the only available context is from Chen et al. (2024), which provides the required value. The answer_unit is 'USD per hour', so the value is returned as a number without the unit name."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, and that many authors refuse to provide the data due to privacy concerns and lack of experimental logs. This contradicts the idea of automatic gathering without contacting authors.","0","is_blank","[""luccioni2025b"", ""luccioni2025c""]","is_blank","In fact, most carbon footprint analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, and that many authors refuse to provide the data due to privacy concerns and lack of experimental logs. This contradicts the idea of automatic gathering without contacting authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as explicitly mentioned in multiple sections including Table 1 and the text. This value is derived using the midrange (average) attribution method, as the context uses midrange values by default for ratios.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as explicitly mentioned in multiple sections including Table 1 and the text. This value is derived using the midrange (average) attribution method, as the context uses midrange values by default for ratios."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for LLaMA-7B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that for LLaMA-7B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context from ref_id=khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', directly supporting the claim in the question. Since the question asks for a True/False answer, the answer_value must be '1' for True or '0' for False.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context from ref_id=khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', directly supporting the claim in the question. Since the question asks for a True/False answer, the answer_value must be '1' for True or '0' for False."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers, as noted in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is 'answers', so the numeric value 95 is returned.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers, as noted in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is 'answers', so the numeric value 95 is returned."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the answer_unit requirement of USD per hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the answer_unit requirement of USD per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context from ref_id=han2024, Table 2, shows that training a Llama-3.1 scale model in Altoona, Iowa, resulted in a health cost of 122% of the electricity cost. This directly answers the question with the required unit of percent.","122","percent","[""han2024""]","is_blank","Table 2: Altoona, IA ... Health Cost (million$) ... % of Electricity Cost: 122%","The context from ref_id=han2024, Table 2, shows that training a Llama-3.1 scale model in Altoona, Iowa, resulted in a health cost of 122% of the electricity cost. This directly answers the question with the required unit of percent."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This directly answers the question about the number of tokens used for pre-training, and the answer_unit is 'tokens', so the value is expressed as a number in tokens.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This directly answers the question about the number of tokens used for pre-training, and the answer_unit is 'tokens', so the value is expressed as a number in tokens."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom, as shown in the table under 'Project Location' with 'United Kingdom' listed with a total of 36 projects.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
... United Kingdom 36 901 ...","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom, as shown in the table under 'Project Location' with 'United Kingdom' listed with a total of 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides U.S. gender representation data for Amazon's workforce across all levels in 2023, showing that 43.1% identified as men and 56.8% as women. This is explicitly stated in the 'Amazon Representation by the Numbers' section under 'Amazon Workforce (All Levels)' for 2023.","43.1","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels)
43.1%56.8%
2023","The context provides U.S. gender representation data for Amazon's workforce across all levels in 2023, showing that 43.1% identified as men and 56.8% as women. This is explicitly stated in the 'Amazon Representation by the Numbers' section under 'Amazon Workforce (All Levels)' for 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context from ref_id=li2025b states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].' This directly answers the question, and since the answer_unit is 'percent', the value is reported as a number without the unit name.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context from ref_id=li2025b states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].' This directly answers the question, and since the answer_unit is 'percent', the value is reported as a number without the unit name."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides execution time breakdowns for sparse BlackMamba fine-tuning with batch sizes up to 84, but does not specify the total execution time in seconds for a batch size of 84 on an NVIDIA A40-48GB GPU. Figure 4 and related tables show execution time breakdowns for sparse BlackMamba at batch sizes of 1, 10, and 32, but not 84. The highest batch size shown for BlackMamba-S on CS is 20, and while batch size 84 is mentioned in the figure caption, no corresponding total execution time value is provided.","is_blank","seconds","[""xia2024""]","is_blank","Fig. 4 illustrates execution time breakdown for Mixtral and BlackMamba; sparse BlackMamba batch sizes shown are 1, 10, and 32. Batch size 84 is mentioned in the caption but no total time value is given.","The context provides execution time breakdowns for sparse BlackMamba fine-tuning with batch sizes up to 84, but does not specify the total execution time in seconds for a batch size of 84 on an NVIDIA A40-48GB GPU. Figure 4 and related tables show execution time breakdowns for sparse BlackMamba at batch sizes of 1, 10, and 32, but not 84. The highest batch size shown for BlackMamba-S on CS is 20, and while batch size 84 is mentioned in the figure caption, no corresponding total execution time value is provided."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context explicitly states that the 'granularity metric' was introduced as the ratio of calculation to communication time to assess scalability in distributed training, including across continents. This is directly stated in multiple sections, including the contributions and lessons learned.","granularity metric","is_blank","[""erben2023""]","is_blank","We introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context explicitly states that the 'granularity metric' was introduced as the ratio of calculation to communication time to assess scalability in distributed training, including across continents. This is directly stated in multiple sections, including the contributions and lessons learned."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. Since 640 percent equals 6.4 times, the answer_value is 6.4, matching the required unit 'times'.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. Since 640 percent equals 6.4 times, the answer_value is 6.4, matching the required unit 'times'."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This matches the question exactly. Since the answer_unit is 'is_blank', the answer_value must be the exact term from the context.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This matches the question exactly. Since the answer_unit is 'is_blank', the answer_value must be the exact term from the context."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that 'the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.' This directly answers the question with the frequency in years, matching the required answer_unit.","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that 'the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.' This directly answers the question with the frequency in years, matching the required answer_unit."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context from [ref_id=wu2021b] explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly answers the question with the exact number of devices.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context from [ref_id=wu2021b] explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly answers the question with the exact number of devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context from ref_id=luccioni2025c states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (Llama 4 Scout). This range is explicitly provided in the text and corroborated by Table 1 in the appendix, which lists these same values.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context from ref_id=luccioni2025c states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (Llama 4 Scout). This range is explicitly provided in the text and corroborated by Table 1 in the appendix, which lists these same values."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the claim in the question. Therefore, the statement is false.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the claim in the question. Therefore, the statement is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context from ref_id=luccioni2025a explicitly states that '10–50 queries on GPT-3 consumes around half a liter of water', which directly answers the question. The answer_unit is 'queries', so the numeric range [10,50] is provided as the answer_value.","[10, 50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context from ref_id=luccioni2025a explicitly states that '10–50 queries on GPT-3 consumes around half a liter of water', which directly answers the question. The answer_unit is 'queries', so the numeric range [10,50] is provided as the answer_value."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context explicitly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which surpasses Llama-2-13b-Chat's score of 6.650, as shown in Table 4. The answer_unit is 'score', so the numeric value is returned directly.","6.681","score","[""shen2024""]","is_blank","JetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nTable 4: MT-Bench score comparison of various models\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.","The context explicitly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which surpasses Llama-2-13b-Chat's score of 6.650, as shown in Table 4. The answer_unit is 'score', so the numeric value is returned directly."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that 10–50 queries on GPT-3 consume around half a liter of water, which is equivalent to a 500 mL bottle. Since each query corresponds to a completion, the number of medium-length GPT-3 completions that can be produced with the water in a 500 mL bottle is therefore between 10 and 50 responses.","[10, 50]","responses","[""luccioni2025a"", ""li2025b""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context states that 10–50 queries on GPT-3 consume around half a liter of water, which is equivalent to a 500 mL bottle. Since each query corresponds to a completion, the number of medium-length GPT-3 completions that can be produced with the water in a 500 mL bottle is therefore between 10 and 50 responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context from ref_id=griggs2024 explicitly states that Mélange reduces deployment costs by up to 77% in conversational settings, which are synonymous with chat-based interactions. The answer_unit is 'percent', so the value is returned as a number without the unit name.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context from ref_id=griggs2024 explicitly states that Mélange reduces deployment costs by up to 77% in conversational settings, which are synonymous with chat-based interactions. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model, as confirmed in Table II and multiple sections of the paper. The answer_unit specifies 'A100_80GB_GPUs', so the numeric value 4 is returned.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 65B 8 64 4 128 ... the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model, as confirmed in Table II and multiple sections of the paper. The answer_unit specifies 'A100_80GB_GPUs', so the numeric value 4 is returned."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context repeatedly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is explicitly mentioned in multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c], with consistent phrasing and no conflicting values.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context repeatedly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is explicitly mentioned in multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c], with consistent phrasing and no conflicting values."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from [ref_id=patterson2021] states that NVIDIA estimated that 80–90% of the ML workload is inference processing, citing [Leo19]. This matches the question's focus on NVIDIA's 2019 estimate. The answer_unit is 'percent', so the value is provided as a numeric range.","[80, 90]","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context from [ref_id=patterson2021] states that NVIDIA estimated that 80–90% of the ML workload is inference processing, citing [Leo19]. This matches the question's focus on NVIDIA's 2019 estimate. The answer_unit is 'percent', so the value is provided as a numeric range."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context from ref_id=luccioni2024 explicitly states that for all model experimentation and evaluation in the 'Power Hungry Processing' study, a total of 178.97 kg of CO2eq was emitted. This matches the required answer_unit of 'kg CO2eq'.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context from ref_id=luccioni2024 explicitly states that for all model experimentation and evaluation in the 'Power Hungry Processing' study, a total of 178.97 kg of CO2eq was emitted. This matches the required answer_unit of 'kg CO2eq'."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context repeatedly states that estimating GPU energy consumption using TDP is nearly always an overestimation because GPUs rarely operate at maximum power continuously. For example, Chung2025 explicitly says TDP-based estimations can overestimate by a factor of up to 4.1, and Patterson2021 notes TDP is a worst-case scenario. Thus, it is not reliable or accurate.","0","is_blank","[""chung2025"", ""patterson2021""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context repeatedly states that estimating GPU energy consumption using TDP is nearly always an overestimation because GPUs rarely operate at maximum power continuously. For example, Chung2025 explicitly says TDP-based estimations can overestimate by a factor of up to 4.1, and Patterson2021 notes TDP is a worst-case scenario. Thus, it is not reliable or accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o on long queries due to deployment on older A100 GPU nodes, with specific values: GPT-4o consumes 2.875 Wh and GPT-4o mini consumes 3.098 Wh. This indicates GPT-4o mini uses more energy, not less, per query. Therefore, the statement is false.","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o on long queries due to deployment on older A100 GPU nodes, with specific values: GPT-4o consumes 2.875 Wh and GPT-4o mini consumes 3.098 Wh. This indicates GPT-4o mini uses more energy, not less, per query. Therefore, the statement is false."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context from [ref_id=xia2024] shows in Fig. 15 and associated data that for Mixtral-CS-A100-40GB with dense model and batch size 1, the throughput is 0.3 queries/sec. This is explicitly listed in the throughput values for Dense(bsz=1) under Mixtral-CS.","0.3","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)","The context from [ref_id=xia2024] shows in Fig. 15 and associated data that for Mixtral-CS-A100-40GB with dense model and batch size 1, the throughput is 0.3 queries/sec. This is explicitly listed in the throughput values for Dense(bsz=1) under Mixtral-CS."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that direct environmental disclosure peaked in 2022 and then experienced a notable reversal after 2022 due to the rise of commercial and proprietary models, leading to dramatically reduced direct disclosures by the first quarter of 2025. This contradicts the claim that the trend continued to increase.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that direct environmental disclosure peaked in 2022 and then experienced a notable reversal after 2022 due to the rise of commercial and proprietary models, leading to dramatically reduced direct disclosures by the first quarter of 2025. This contradicts the claim that the trend continued to increase."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context from Griggs et al. (2024) states that on-demand H100 is not offered by major cloud providers, so pricing from RunPod ($4.69/h) was used and normalized relative to A100-80G prices. The normalized price was calculated as (4.69/2.29) × 3.67 = $7.516 for H100, which is explicitly listed in Table 1 as the on-demand price per hour.","7.516","USD per hour","[""griggs2024""]","is_blank","We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100. Table 1: On-demand Price ($/h) ... H100 (SXM): 7.5164","The context from Griggs et al. (2024) states that on-demand H100 is not offered by major cloud providers, so pricing from RunPod ($4.69/h) was used and normalized relative to A100-80G prices. The normalized price was calculated as (4.69/2.29) × 3.67 = $7.516 for H100, which is explicitly listed in Table 1 as the on-demand price per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides water consumption estimates for GPT-3 training (e.g., millions of liters) and inference, but no specific data is given for GPT-4's training water usage. While GPT-4's training energy and cost are mentioned, water consumption for its training run is not quantified in any of the provided snippets.","is_blank","liters of water","[""li2025b"", ""jegham2025"", ""cottier2024"", ""patterson2021""]","is_blank","is_blank","The context provides water consumption estimates for GPT-3 training (e.g., millions of liters) and inference, but no specific data is given for GPT-4's training water usage. While GPT-4's training energy and cost are mentioned, water consumption for its training run is not quantified in any of the provided snippets."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context from ref_id=griggs2024 states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs. To convert this to a per-hour cost, assuming 30 days per month (720 hours), the hourly cost is $5,200 / 720 ≈ $7.22. This matches the required answer_unit of 'USD per hour'.","7.22","USD per hour","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from ref_id=griggs2024 states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs. To convert this to a per-hour cost, assuming 30 days per month (720 hours), the hourly cost is $5,200 / 720 ≈ $7.22. This matches the required answer_unit of 'USD per hour'."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context from [ref_id=wu2021a] explicitly states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation, a model 1,000× larger in size is required. This directly answers the question with the multiplier format specified in answer_unit.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context from [ref_id=wu2021a] explicitly states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation, a model 1,000× larger in size is required. This directly answers the question with the multiplier format specified in answer_unit."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context states that training a Transformer model with neural architecture search emitted 626,155 lbs of CO2 (Strubell et al., 2019, ref_id=strubell2019). It also states this is comparable to the lifetime carbon emissions of five cars (ref_id=dodge2022, ref_id=zschache2025). The lifetime emissions of a typical car are approximately 126,000 lbs (ref_id=strubell2019), so five cars equal 630,000 lbs. Assuming an average car emits 0.404 lbs of CO2 per mile (EPA standard), the equivalent driving distance is 626,155 / 0.404 ≈ 1,550,000 miles.","1550000","miles","[""strubell2019"", ""dodge2022"", ""zschache2025""]","[""https://arxiv.org/abs/1906.02243"", ""https://dl.acm.org/doi/10.1145/3531146.3533135""]","Table 1: Estimated CO2 emissions from training common NLP models... w/ neural architecture search 626,155

[ref_id=dodge2022] ...finding it to be comparable to the lifetime carbon emissions of five cars.

[ref_id=strubell2019] Car, avg incl. fuel, 1 lifetime 126,000","The context states that training a Transformer model with neural architecture search emitted 626,155 lbs of CO2 (Strubell et al., 2019, ref_id=strubell2019). It also states this is comparable to the lifetime carbon emissions of five cars (ref_id=dodge2022, ref_id=zschache2025). The lifetime emissions of a typical car are approximately 126,000 lbs (ref_id=strubell2019), so five cars equal 630,000 lbs. Assuming an average car emits 0.404 lbs of CO2 per mile (EPA standard), the equivalent driving distance is 626,155 / 0.404 ≈ 1,550,000 miles."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10⁻⁴ kWh per inference (from Table 5 in luccioni2024). With 606,096 downloads and 1 million inferences per download, total inferences = 606,096 × 1,000,000 = 6.06096 × 10¹¹. Total energy = 6.06096 × 10¹¹ × 1.0 × 10⁻⁴ kWh = 60,609,600 kWh = 60,609.6 MWh. The answer_unit is MWh, so the value is expressed in that unit.","60609.6","MWh","[""luccioni2024""]","is_blank","Inference energy (kWh) 1.0 × 10−4 for BLOOMz-7B (Table 5); BLOOMz-7B has been downloaded 606,096 times (as of November 2023).","The BLOOMz-7B model has an inference energy cost of 1.0 × 10⁻⁴ kWh per inference (from Table 5 in luccioni2024). With 606,096 downloads and 1 million inferences per download, total inferences = 606,096 × 1,000,000 = 6.06096 × 10¹¹. Total energy = 6.06096 × 10¹¹ × 1.0 × 10⁻⁴ kWh = 60,609,600 kWh = 60,609.6 MWh. The answer_unit is MWh, so the value is expressed in that unit."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections including the abstract, introduction, and model pretraining sections.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections including the abstract, introduction, and model pretraining sections."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context repeatedly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, based on a growth rate of 2.4× per year since 2016. This is explicitly stated in multiple sections, including the abstract and section 3.2, and corroborated by extrapolation from GPT-4's $40M cost in 2023.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context repeatedly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, based on a growth rate of 2.4× per year since 2016. This is explicitly stated in multiple sections, including the abstract and section 3.2, and corroborated by extrapolation from GPT-4's $40M cost in 2023."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context from [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS. This matches the required unit 'FLOPS' for answer_value.","314000000000000000000000","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context from [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS. This matches the required unit 'FLOPS' for answer_value."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inference with the LLaMA-65B model, as confirmed in Table II and multiple sections of the paper. This is the bare minimum without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inference with the LLaMA-65B model, as confirmed in Table II and multiple sections of the paper. This is the bare minimum without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The final 101B training stage of FLM-101B used a single-GPU throughput of 165 teraFLOPs/sec with a utilization rate of 52.88%, and the stage processed 26.54 billion tokens. Using the FLOPs estimation method from Narayanan et al. (2021), as referenced in the context, and the total training time of 21.54 days for the entire FLM-101B model, the computational work for the 101B stage can be estimated. The total FLOPs for FLM-101B is reported as 52.76 zettaFLOPs, with the 101B stage contributing the majority. The context directly states the FLOPs for the 101B stage as 28.22 zettaFLOPs in Table 4.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.We list the estimated floating-point operations (zetta = 10^21) of the training process for reference. FLM-101B 28.22","The final 101B training stage of FLM-101B used a single-GPU throughput of 165 teraFLOPs/sec with a utilization rate of 52.88%, and the stage processed 26.54 billion tokens. Using the FLOPs estimation method from Narayanan et al. (2021), as referenced in the context, and the total training time of 21.54 days for the entire FLM-101B model, the computational work for the 101B stage can be estimated. The total FLOPs for FLM-101B is reported as 52.76 zettaFLOPs, with the 101B stage contributing the majority. The context directly states the FLOPs for the 101B stage as 28.22 zettaFLOPs in Table 4."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions AlexNet in relation to ImageNet classification and performance, but does not provide the specific top-1 accuracy value achieved by AlexNet in 2012. While it references the ImageNet dataset and AlexNet's role in its classification, no numeric accuracy value is stated.","is_blank","percent","[""schwartz2019"", ""stone2022"", ""luccioni2023"", ""luccioni2024"", ""luccioni2025b""]","is_blank","is_blank","The context mentions AlexNet in relation to ImageNet classification and performance, but does not provide the specific top-1 accuracy value achieved by AlexNet in 2012. While it references the ImageNet dataset and AlexNet's role in its classification, no numeric accuracy value is stated."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server contains 8 × 80G GPUs. Multiplying 24 servers by 8 GPUs per server gives 192 GPUs total. This is further confirmed in Table 2, which lists the 'Number of GPUs' as 192 for all training stages.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server contains 8 × 80G GPUs. Multiplying 24 servers by 8 GPUs per server gives 192 GPUs total. This is further confirmed in Table 2, which lists the 'Number of GPUs' as 192 for all training stages."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that training a Transformer model with Neural Architecture Search (NAS) produced emissions comparable to the lifetime carbon emissions of five cars, as reported by Strubell et al. [42]. However, no direct equivalence to 'average American lifetimes' is provided. The context mentions that an average American life emits 36,156 lbs of CO2, and the NAS emissions were 626,155 lbs, which would equate to approximately 17.3 American lifetimes (626,155 / 36,156). But since the question asks for the equivalence as stated in the context and the context only references 'five cars' — not American lifetimes — the exact requested equivalence is not provided.","is_blank","lifetimes","[""dodge2022"", ""strubell2019"", ""luccioni2023"", ""patterson2021""]","is_blank","Strubell et al. estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","The context states that training a Transformer model with Neural Architecture Search (NAS) produced emissions comparable to the lifetime carbon emissions of five cars, as reported by Strubell et al. [42]. However, no direct equivalence to 'average American lifetimes' is provided. The context mentions that an average American life emits 36,156 lbs of CO2, and the NAS emissions were 626,155 lbs, which would equate to approximately 17.3 American lifetimes (626,155 / 36,156). But since the question asks for the equivalence as stated in the context and the context only references 'five cars' — not American lifetimes — the exact requested equivalence is not provided."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context from [ref_id=schwartz2019] explicitly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question with the required unit of hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context from [ref_id=schwartz2019] explicitly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question with the required unit of hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This value is explicitly mentioned in two places in the context, confirming the figure.","47500","metric tons","[""amazon2023""]","is_blank","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This value is explicitly mentioned in two places in the context, confirming the figure."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations. This directly answers the question about the framework used for deploying LLMs across multiple GPUs and nodes.","vllm","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations. This directly answers the question about the framework used for deploying LLMs across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context from [ref_id=zschache2025] Table B2 shows that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes, resulting in a ratio of 1.95. This is explicitly stated as the factor of increase due to double-node deployment.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2: Llama 3.1 70B single vs. double node deployment: Energy consumed (Wh) single=48.60, double=94.88, ratio=1.95","The context from [ref_id=zschache2025] Table B2 shows that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes, resulting in a ratio of 1.95. This is explicitly stated as the factor of increase due to double-node deployment."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While it mentions that GPT-3 (the core of ChatGPT) consumes 5.4 million liters of water during training and approximately 500 mL per 10–50 queries, it does not break down water usage per user session in 2023 or convert it to gallons. Therefore, the exact value cannot be determined.","is_blank","gallons of water","[""li2025b"", ""luccioni2025a"", ""jegham2025""]","is_blank","is_blank","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While it mentions that GPT-3 (the core of ChatGPT) consumes 5.4 million liters of water during training and approximately 500 mL per 10–50 queries, it does not break down water usage per user session in 2023 or convert it to gallons. Therefore, the exact value cannot be determined."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o's estimated annual energy consumption from 700 million daily queries is approximately 391,509 MWh to 463,269 MWh, which exceeds the annual electricity consumption of 35,000 U.S. residential households (377,685 MWh). This comparison is explicitly made in Section 6.2, confirming that the annual electricity use is comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o's estimated annual energy consumption from 700 million daily queries is approximately 391,509 MWh to 463,269 MWh, which exceeds the annual electricity consumption of 35,000 U.S. residential households (377,685 MWh). This comparison is explicitly made in Section 6.2, confirming that the annual electricity use is comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context from ref_id=luccioni2025a and ref_id=li2025b both state that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer_unit is 'percent', so the numeric value 34 is used directly.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons...","The context from ref_id=luccioni2025a and ref_id=li2025b both state that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer_unit is 'percent', so the numeric value 34 is used directly."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs', which directly contradicts the claim that traditional models achieved comparable accuracy. Therefore, the statement is false.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs', which directly contradicts the claim that traditional models achieved comparable accuracy. Therefore, the statement is false."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, as noted in multiple sections including 2.3 and Table 1. The answer_unit is 'PUE', so the numeric value 1.11 is returned directly.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, as noted in multiple sections including 2.3 and Table 1. The answer_unit is 'PUE', so the numeric value 1.11 is returned directly."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that GPT-4o is projected to have approximately 772 billion queries in 2025, based on a monthly growth rate from January to May and a decaying growth pattern through December. This figure is explicitly mentioned in multiple snippets and aligns with the question's request for the total estimated number of queries.","772000000000","queries","[""jegham2025""]","is_blank","yielding a total of approximately 772 billion GPT-4o queries in 2025","The context states that GPT-4o is projected to have approximately 772 billion queries in 2025, based on a monthly growth rate from January to May and a decaying growth pattern through December. This figure is explicitly mentioned in multiple snippets and aligns with the question's request for the total estimated number of queries."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context explicitly states that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, as shown in Table 3. This value is the final average score for the JetMoE-8B model on the benchmark suite, and the answer_unit is 'score', so the numeric value is returned directly.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context explicitly states that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, as shown in Table 3. This value is the final average score for the JetMoE-8B model on the benchmark suite, and the answer_unit is 'score', so the numeric value is returned directly."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context does not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022. While there are references to renewable energy usage and carbon intensity, no specific count or metric of training runs under renewable-only power is reported.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022. While there are references to renewable energy usage and carbon intensity, no specific count or metric of training runs under renewable-only power is reported."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that open-source models are currently exempt from reporting obligations under the AI Act unless they present a systemic risk, but the paper argues this exemption should be removed because open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under the current AI Act, open-source general-purpose AI models are exempt from energy consumption reporting unless they pose systemic risk.","1","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4]. The idea is that OS models, by definition, already disclose certain types of information. Hence, Recital 102 lists information on parameters, including weights, model architecture, and model usage as a prerequisite for systems to be considered OS. However, it does not mandate the disclosure of energy consumption.","The context states that open-source models are currently exempt from reporting obligations under the AI Act unless they present a systemic risk, but the paper argues this exemption should be removed because open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under the current AI Act, open-source general-purpose AI models are exempt from energy consumption reporting unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the US national datacenter average PUE in 2020 was 1.59, as cited from [Lawrence, 2019, 2020] and corroborated by multiple references including Patterson et al. (2021) and Dodge et al. (2022). The answer_unit is 'PUE', so the value is returned as a numeric value without units.","1.59","PUE","[""patterson2021"", ""dodge2022""]","[""https://arxiv.org/abs/2104.10350"", ""https://dl.acm.org/doi/10.1145/3531156.3533168""]","The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.","The context states that the US national datacenter average PUE in 2020 was 1.59, as cited from [Lawrence, 2019, 2020] and corroborated by multiple references including Patterson et al. (2021) and Dodge et al. (2022). The answer_unit is 'PUE', so the value is returned as a numeric value without units."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that for the OPT-2.7B model with a batch size of 32, the KV Cache expands to 5.312GB, as explicitly mentioned in multiple sections including the analysis of KV Cache size growth.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context states that for the OPT-2.7B model with a batch size of 32, the KV Cache expands to 5.312GB, as explicitly mentioned in multiple sections including the analysis of KV Cache size growth."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context repeatedly states that for the four notable models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity make up between 29% and 49% of the total amortized cost. This is explicitly mentioned in multiple sections, including the methodology and results.","[29, 49]","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context repeatedly states that for the four notable models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity make up between 29% and 49% of the total amortized cost. This is explicitly mentioned in multiple sections, including the methodology and results."
"q213","Which software package was used to measure energy consumption during inference runs?","The context repeatedly states that CodeCarbon was used to measure energy consumption during inference runs, specifically mentioning its use in tracking power usage via NVIDIA's NVML and Intel RAPL, and that it was verified against physical measurements. The answer_unit is 'is_blank', so the exact identifier name 'CodeCarbon' is returned as the answer_value.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://github.com/mlco2/codecarbon"", ""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context repeatedly states that CodeCarbon was used to measure energy consumption during inference runs, specifically mentioning its use in tracking power usage via NVIDIA's NVML and Intel RAPL, and that it was verified against physical measurements. The answer_unit is 'is_blank', so the exact identifier name 'CodeCarbon' is returned as the answer_value."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles on ChatGPT energy consumption, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles on ChatGPT energy consumption, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This is mentioned in multiple sections, including the abstract and Section IV-F, confirming CTCF as the named function.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This is mentioned in multiple sections, including the abstract and Section IV-F, confirming CTCF as the named function."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall' (Section D, Energy per Response: LLaMA 65B), directly supporting that more GPU shards increase energy cost per response.","1","is_blank","[""samsi2024""]","is_blank","increasing the number of shards still tends to increase the energy costs of inference per response most overall","The context states that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall' (Section D, Energy per Response: LLaMA 65B), directly supporting that more GPU shards increase energy cost per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. A 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Therefore, the mass of rare earth metal per H100 is 0.1% of (125g / 63) ≈ 0.1984 grams. The water consumption for mining this amount is (0.0001984 kg) × (11 kL/kg) ≈ 0.00218 kL. However, the context directly states that mining rare earth metals adds an additional 2.2 liters (i.e., 2.2 kL) consumed per GPU, which is the value used in their analysis. Thus, the estimated water consumption from mining rare earth metals per H100 is 2.2 kL.","2.2","kL","[""morrison2025""]","is_blank","We additionally estimate the environmental impact from mining rare earth metals used during man-ufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. A 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Therefore, the mass of rare earth metal per H100 is 0.1% of (125g / 63) ≈ 0.1984 grams. The water consumption for mining this amount is (0.0001984 kg) × (11 kL/kg) ≈ 0.00218 kL. However, the context directly states that mining rare earth metals adds an additional 2.2 liters (i.e., 2.2 kL) consumed per GPU, which is the value used in their analysis. Thus, the estimated water consumption from mining rare earth metals per H100 is 2.2 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that the open-source exemption from reporting obligations should be removed, as open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt and do not have to report energy consumption, but the paper proposes eliminating this exemption. Therefore, the statement is false.","0","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify ex- clusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary mod- els.","The context states that the open-source exemption from reporting obligations should be removed, as open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt and do not have to report energy consumption, but the paper proposes eliminating this exemption. Therefore, the statement is false."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context from ref_id=luccioni2025a and ref_id=wu2021b both state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. The answer_unit is 'percent', so the value is reported as a number without the unit name.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The context from ref_id=luccioni2025a and ref_id=wu2021b both state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. The answer_unit is 'percent', so the value is reported as a number without the unit name."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, based on the average attribution method, as shown in Table 1 under the 'Total' row for 2023. This value is derived from scope-1 and scope-2 health costs combined, using the standard average attribution method for air pollutant accounting.","47.48","USD","[""han2024""]","is_blank","Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028 — 2023 Total Health Cost: $47.48 billion (35.77, 59.19)","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, based on the average attribution method, as shown in Table 1 under the 'Total' row for 2023. This value is derived from scope-1 and scope-2 health costs combined, using the standard average attribution method for air pollutant accounting."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption values for o3 and GPT-4.1 nano for long prompts (10k input-1.5k output). o3 consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. The factor is calculated as 12.222 / 0.827 ≈ 14.78, which is the multiplier by which o3's energy consumption exceeds GPT-4.1 nano's.","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). o3: 12.222 ± 1.082 Wh (10k input-1.5k output); GPT-4.1 nano: 0.827 ± 0.094 Wh (10k input-1.5k output).","The context provides energy consumption values for o3 and GPT-4.1 nano for long prompts (10k input-1.5k output). o3 consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. The factor is calculated as 12.222 / 0.827 ≈ 14.78, which is the multiplier by which o3's energy consumption exceeds GPT-4.1 nano's."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the Arena dataset with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU baselines. This range directly answers the question and matches the required format of 'percent'.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO)","The context states that for the Arena dataset with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU baselines. This range directly answers the question and matches the required format of 'percent'."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context states in Table 3 that the net carbon emissions (nettCO2e) for FLM-101B are 26 tCO2e, which matches the required unit. This value is explicitly listed alongside other models' emissions, confirming it as the total estimated net carbon footprint for FLM-101B's pre-training.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... FLM-101B ... net tCO2e 26","The context states in Table 3 that the net carbon emissions (nettCO2e) for FLM-101B are 26 tCO2e, which matches the required unit. This value is explicitly listed alongside other models' emissions, confirming it as the total estimated net carbon footprint for FLM-101B's pre-training."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdowns for Mixtral fine-tuning, including for sparse models with batch size = 1, but does not give a specific total execution time in seconds for a single run on an NVIDIA A40-48GB GPU. While Figure 4 and related text describe time shares for forward, backward, and optimizer stages, no aggregate total time is reported.","is_blank","seconds","[""xia2024""]","is_blank","Fig. 4 illustrates the following insights. (1) The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time... while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context provides execution time breakdowns for Mixtral fine-tuning, including for sparse models with batch size = 1, but does not give a specific total execution time in seconds for a single run on an NVIDIA A40-48GB GPU. While Figure 4 and related text describe time shares for forward, backward, and optimizer stages, no aggregate total time is reported."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health costs of AI are not evenly distributed, with disadvantaged and low-income communities bearing disproportionately higher health impacts—up to 200 times higher than in less-affected areas. This is supported by multiple references in the Han2024 document.","0","is_blank","[""han2024""]","is_blank","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context explicitly states that the public health costs of AI are not evenly distributed, with disadvantaged and low-income communities bearing disproportionately higher health impacts—up to 200 times higher than in less-affected areas. This is supported by multiple references in the Han2024 document."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context from ref_id=wu2021b states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' as a result of Moore’s law scaling and architectural optimization, and this is cited with [Sun et al., 2019], which aligns with the 2019 product data timeframe. The question asks for a True/False evaluation, so the answer_value must be '1' for True or '0' for False.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context from ref_id=wu2021b states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' as a result of Moore’s law scaling and architectural optimization, and this is cited with [Sun et al., 2019], which aligns with the 2019 product data timeframe. The question asks for a True/False evaluation, so the answer_value must be '1' for True or '0' for False."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context from [ref_id=khan2025] explicitly states that quantization was applied through Ollama, an open-source platform that supports edge computing and privacy-centric deployments, specifically in the context of the financial sentiment analysis case study. This matches the question's requirement for the tool used to apply 4-bit quantization and support local deployment.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context from [ref_id=khan2025] explicitly states that quantization was applied through Ollama, an open-source platform that supports edge computing and privacy-centric deployments, specifically in the context of the financial sentiment analysis case study. This matches the question's requirement for the tool used to apply 4-bit quantization and support local deployment."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that the datasets were accessed on-demand via shards in the tar format using the WebDataset library, which supports streaming decompression and preprocessing. This was necessary because spot instances can terminate at any time, making traditional storage methods unsuitable.","WebDataset","is_blank","[""erben2023""]","is_blank","We access the datasets on-demand via shards in thetar format with the WebDataset library [10]. We chose WebDataset due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format.","The context states that the datasets were accessed on-demand via shards in the tar format using the WebDataset library, which supports streaming decompression and preprocessing. This was necessary because spot instances can terminate at any time, making traditional storage methods unsuitable."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context explicitly states that 'energy is the integral of power over time, these two measures exhibit a strong correlation' and 'if the power is constant over time, this correlation should be linear.' Figure 6 illustrates this linear relationship between duration and energy consumption in LLM inference experiments. This directly supports that the relationship is nearly linear under constant power conditions.","1","is_blank","[""zschache2025""]","is_blank","Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear. Figure 6 illustrates this relationship for 10","The context explicitly states that 'energy is the integral of power over time, these two measures exhibit a strong correlation' and 'if the power is constant over time, this correlation should be linear.' Figure 6 illustrates this linear relationship between duration and energy consumption in LLM inference experiments. This directly supports that the relationship is nearly linear under constant power conditions."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, as cited in ref_id=ebert2024. Since the answer_unit is 'is_blank', the answer_value must be the exact name of the senator as the identifier.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","The bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context explicitly states that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, as cited in ref_id=ebert2024. Since the answer_unit is 'is_blank', the answer_value must be the exact name of the senator as the identifier."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context references Chen et al. (2024), not 2025, and lists the price per chip for NVIDIA H100 as $11.06/hr in Table 1. Although the question incorrectly cites 'Chen et al. (2025)', the only available price for H100 per hour in the context is from Chen2024, which matches the required unit of USD per hour.","11.06","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context references Chen et al. (2024), not 2025, and lists the price per chip for NVIDIA H100 as $11.06/hr in Table 1. Although the question incorrectly cites 'Chen et al. (2025)', the only available price for H100 per hour in the context is from Chen2024, which matches the required unit of USD per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context states that in February 2024, AWS announced that the average expected life of its servers had improved from five to six years. While this refers to servers generally, GPUs are a core component of AI data center hardware, and no specific GPU lifetime is provided. The context also mentions that GPUs can theoretically last about five years, but frequent upgrades due to performance demands are accelerating turnover. Thus, the best-supported estimate for GPU lifetime in AI data centers in 2024 is six years, based on AWS server lifespan extension.","6","years","[""amazon2023""]","[""https://d1.awsstatic.com/sustainability/Amazon-Sustainability-Report-2023.pdf""]","In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.","The context states that in February 2024, AWS announced that the average expected life of its servers had improved from five to six years. While this refers to servers generally, GPUs are a core component of AI data center hardware, and no specific GPU lifetime is provided. The context also mentions that GPUs can theoretically last about five years, but frequent upgrades due to performance demands are accelerating turnover. Thus, the best-supported estimate for GPU lifetime in AI data centers in 2024 is six years, based on AWS server lifespan extension."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B inference, the bare minimum hardware requires 2 V100 32GB GPUs, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. ... 13B 2 64 1 64","The context states that for LLaMA-13B inference, the bare minimum hardware requires 2 V100 32GB GPUs, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. (2019) study. The answer_unit is tCO2e, so the value is reported in that unit.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ""five cars"" number","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. (2019) study. The answer_unit is tCO2e, so the value is reported in that unit."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although one snippet mentions GTX 1080 Ti GPUs, the training time of 336 hours is consistently reported for ELMo using GTX 1080 GPUs, and no different duration is provided for GTX 1080 Ti GPUs. Thus, the same training time is applied.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although one snippet mentions GTX 1080 Ti GPUs, the training time of 336 hours is consistently reported for ELMo using GTX 1080 GPUs, and no different duration is provided for GTX 1080 Ti GPUs. Thus, the same training time is applied."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context from [ref_id=li2025b] states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, as cited from reference [8]. This value is explicitly mentioned multiple times in the context, including in the section on scope-2 water usage and in the appendix for global AI water estimates.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context from [ref_id=li2025b] states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, as cited from reference [8]. This value is explicitly mentioned multiple times in the context, including in the section on scope-2 water usage and in the appendix for global AI water estimates."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google's PUE improved to 1.10 in 2021, as reported in multiple sources including [dodge2022], [wu2021b], and [patterson2021]. The PUE is a unitless efficiency metric, so the answer_value is the numeric value 1.10 as specified by answer_unit.","1.1","PUE","[""dodge2022"", ""wu2021b"", ""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://journal.uptimeinstitute.com/is-pue-actually-going-up/"", ""https://journal.uptimeinstitute.com/data-center-pues-flat-since-2013/""]","Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021 [dodge2022]; PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [wu2021b]; Google publishes its datacenter PUE online every quarter [patterson2021].","The context states that Google's PUE improved to 1.10 in 2021, as reported in multiple sources including [dodge2022], [wu2021b], and [patterson2021]. The PUE is a unitless efficiency metric, so the answer_value is the numeric value 1.10 as specified by answer_unit."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context from ref_id=amazon2023 states that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS uses 100% renewable energy. This directly answers the question about the typical percent reduction customers can expect.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context from ref_id=amazon2023 states that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS uses 100% renewable energy. This directly answers the question about the typical percent reduction customers can expect."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on an NVIDIA H100 GPU has a net cost of $3460. This value is repeated multiple times across the context, including in the abstract, introduction, and conclusions, and is directly tied to the H100 GPU and 2M queries.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on an NVIDIA H100 GPU has a net cost of $3460. This value is repeated multiple times across the context, including in the abstract, introduction, and conclusions, and is directly tied to the H100 GPU and 2M queries."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context from dodge2022 shows that in a specific experiment training BERT base on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of total energy consumption. Additional statements confirm that GPUs typically consume the majority of electricity in AI workloads, and in many cases, account for almost 3/4 of electricity consumption. This percentage is consistently supported across multiple excerpts.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB)... The GPU alone accounts for 74% of the total energy consumption due to these components.","The context from dodge2022 shows that in a specific experiment training BERT base on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of total energy consumption. Additional statements confirm that GPUs typically consume the majority of electricity in AI workloads, and in many cases, account for almost 3/4 of electricity consumption. This percentage is consistently supported across multiple excerpts."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s, directly specifying the total number of H100 GPUs used.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s, directly specifying the total number of H100 GPUs used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during actively training, the average GPU power for a single node is over 600W, as specified in multiple snippets referencing Figure 2 of the OLMo 2 7B training. The answer_unit is Watts, so the value is reported as a number in that unit.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during actively training, the average GPU power for a single node is over 600W, as specified in multiple snippets referencing Figure 2 of the OLMo 2 7B training. The answer_unit is Watts, so the value is reported as a number in that unit."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for 'Human life, avg, 1 year' is 11,023 lbs. This matches the requested unit (lbs) and refers to a global average, as no specific country is indicated in the phrase 'Human life, avg'.","11023","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs)
Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl.","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for 'Human life, avg, 1 year' is 11,023 lbs. This matches the requested unit (lbs) and refers to a global average, as no specific country is indicated in the phrase 'Human life, avg'."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the LLaMA 13B model, there is a 1.25 times increase in inference performance (throughput) on the A100 compared to the V100 across metrics like words per second and tokens per second. This directly corresponds to the speedup multiplier requested.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the LLaMA 13B model, there is a 1.25 times increase in inference performance (throughput) on the A100 compared to the V100 across metrics like words per second and tokens per second. This directly corresponds to the speedup multiplier requested."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context repeatedly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as reported in multiple sections including Section 6.1 and validation against public disclosures. This value is explicitly tied to GPT-4o and is consistent across references.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context repeatedly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as reported in multiple sections including Section 6.1 and validation against public disclosures. This value is explicitly tied to GPT-4o and is consistent across references."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for a 400 TPS SLO, InferSave's top choice was g4dn.xlarge at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice. This directly provides the percentage difference in cost.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that for a 400 TPS SLO, InferSave's top choice was g4dn.xlarge at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice. This directly provides the percentage difference in cost."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context from [ref_id=zschache2025] states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This implies that among the architectures mentioned, V100 and A30 are the most energy-efficient for single-token classification tasks, with no evidence suggesting H100 is more efficient in this specific case.","V100 or A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context from [ref_id=zschache2025] states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This implies that among the architectures mentioned, V100 and A30 are the most energy-efficient for single-token classification tasks, with no evidence suggesting H100 is more efficient in this specific case."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context from [ref_id=schwartz2019] states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', where 'computational price tag' refers to computational cost, not financial cost. While computational cost may correlate with financial cost, the context does not equate the two. Therefore, Green AI involves reporting computational cost, not explicitly financial cost.","0","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context from [ref_id=schwartz2019] states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', where 'computational price tag' refers to computational cost, not financial cost. While computational cost may correlate with financial cost, the context does not equate the two. Therefore, Green AI involves reporting computational cost, not explicitly financial cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context from ref_id=luccioni2025a states that electronic waste (e-waste) reached 62 million tonnes in 2022, which is equivalent to 62 million metric tons. The answer_unit specifies 'metric tons', so the numeric value is reported as 62000000.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context from ref_id=luccioni2025a states that electronic waste (e-waste) reached 62 million tonnes in 2022, which is equivalent to 62 million metric tons. The answer_unit specifies 'metric tons', so the numeric value is reported as 62000000."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the measured system average power per processor for TPU v2 as 208 Watts and for V100 GPU as 289 Watts (Table 4, row 12). The difference is calculated as 289 - 208 = 81 Watts, which matches the required unit.","81","Watts","[""patterson2021""]","is_blank","Table 4: Measured System Average Power per Accelerator... (W) 208 [TPU v2] 289 [V100]","The context provides the measured system average power per processor for TPU v2 as 208 Watts and for V100 GPU as 289 Watts (Table 4, row 12). The difference is calculated as 289 - 208 = 81 Watts, which matches the required unit."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This is stated in the abstract and reiterated in the main text, and the answer_unit is specified as 'liters', so the numeric value is directly extracted.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context explicitly states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This is stated in the abstract and reiterated in the main text, and the answer_unit is specified as 'liters', so the numeric value is directly extracted."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context repeatedly states that Facebook's recommendation and ranking model sizes increased by 20× between 2019 and 2021, as seen in multiple snippets from [ref_id=wu2021a]. The answer_unit is 'multiplier', so the value is expressed as a numeric multiplier.","20","multiplier","[""wu2021a""]","is_blank","Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].","The context repeatedly states that Facebook's recommendation and ranking model sizes increased by 20× between 2019 and 2021, as seen in multiple snippets from [ref_id=wu2021a]. The answer_unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that o3-mini achieved the highest cross-efficiency score (0.884) in the DEA-based eco-efficiency analysis, outperforming other models like o1-mini and Claude 3.7 Sonnet. This identifies o3-mini as the top-ranked model according to the DEA methodology described.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context states that o3-mini achieved the highest cross-efficiency score (0.884) in the DEA-based eco-efficiency analysis, outperforming other models like o1-mini and Claude 3.7 Sonnet. This identifies o3-mini as the top-ranked model according to the DEA methodology described."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from ref_id=wu2021b states that current average lifetimes for cell phones are less than 3 years, which contributes to e-waste concerns as highlighted in discussions on sustainability and electronic waste. This directly supports the truth of the statement.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context from ref_id=wu2021b states that current average lifetimes for cell phones are less than 3 years, which contributes to e-waste concerns as highlighted in discussions on sustainability and electronic waste. This directly supports the truth of the statement."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs in intra-zone experiments with T4 GPUs, indicating nearly linear scaling. This directly supports the statement.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs in intra-zone experiments with T4 GPUs, indicating nearly linear scaling. This directly supports the statement."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens, as mentioned in two identical sentences describing its architecture. This directly answers the question in the required unit of 'tokens'.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens, as mentioned in two identical sentences describing its architecture. This directly answers the question in the required unit of 'tokens'."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity and is bottlenecked by VRAM bandwidth, leading to underutilized GPU computation units and lower power draw compared to diffusion models, which are significantly more compute-intensive. This directly supports the truth of the statement.","1","is_blank","[""chung2025""]","is_blank","This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.","The context states that LLM decoding is characterized by low compute-intensity and is bottlenecked by VRAM bandwidth, leading to underutilized GPU computation units and lower power draw compared to diffusion models, which are significantly more compute-intensive. This directly supports the truth of the statement."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides workforce representation data for Amazon in 2023, including gender breakdowns for 'People Managers'. Under the 'People Managers' section, the global gender breakdown for 2023 shows 31.9% women and 68.1% men (derived from 31.9% women and the implied remainder). This matches the format requested in 'answer_unit' as a percentage.","31.9","percent","[""amazon2023""]","is_blank","People Managers
Gender Race/Ethnicity
2021
2022
2023
2021
2022
2023
0.3% 1.7% 2.6%
2021
2022
2023
2021
2022
2023
16.6% 31.9%26.1%23.5%","The context provides workforce representation data for Amazon in 2023, including gender breakdowns for 'People Managers'. Under the 'People Managers' section, the global gender breakdown for 2023 shows 31.9% women and 68.1% men (derived from 31.9% women and the implied remainder). This matches the format requested in 'answer_unit' as a percentage."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This is explicitly mentioned in multiple sections, including the breakdown of costs in Section 3.5 and Figure 6(b). The answer_unit is 'percent', so the range is returned as a numeric interval.","[61, 76]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This is explicitly mentioned in multiple sections, including the breakdown of costs in Section 3.5 and Figure 6(b). The answer_unit is 'percent', so the range is returned as a numeric interval."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that after optimization, metrics like accuracy and F1 score slightly declined for most models in the financial sentiment analysis case study, indicating a trade-off between energy efficiency and performance. For example, Qwen's F1 score decreased from 0.76 to 0.80 (slight increase), but Llama 3.2's F1 score dropped from 0.44 to 0.47, and Phi 3.2's increased from 0.88 to 0.91 — showing mixed results. Thus, accuracy and F1 scores did not always improve.","0","is_blank","[""khan2025""]","is_blank","Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context states that after optimization, metrics like accuracy and F1 score slightly declined for most models in the financial sentiment analysis case study, indicating a trade-off between energy efficiency and performance. For example, Qwen's F1 score decreased from 0.76 to 0.80 (slight increase), but Llama 3.2's F1 score dropped from 0.44 to 0.47, and Phi 3.2's increased from 0.88 to 0.91 — showing mixed results. Thus, accuracy and F1 scores did not always improve."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context from [ref_id=strubell2019] and [ref_id=patterson2021] states that the U.S. Environmental Protection Agency (EPA) provides average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., and specifically cites a value of 0.954 lbs/kWh derived from 0.423 kg/kWh (since 0.423 kg = 0.932 lbs, and 0.954 lbs/kWh is the direct conversion used in Equation 2). This matches the reported conversion factor in the context.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt","The context from [ref_id=strubell2019] and [ref_id=patterson2021] states that the U.S. Environmental Protection Agency (EPA) provides average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., and specifically cites a value of 0.954 lbs/kWh derived from 0.423 kg/kWh (since 0.423 kg = 0.932 lbs, and 0.954 lbs/kWh is the direct conversion used in Equation 2). This matches the reported conversion factor in the context."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context from ref_id=li2025b states that a recent study suggests global AI could consume 85 – 134 TWh of electricity in 2027. This matches the required unit (TWh) and provides a clear numeric range.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]","The context from ref_id=li2025b states that a recent study suggests global AI could consume 85 – 134 TWh of electricity in 2027. This matches the required unit (TWh) and provides a clear numeric range."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe, as explicitly mentioned in the Europe section under 'Last Mile Electric Delivery Vehicles by Region'. The answer_unit is 'packages', so the numeric value is reported as 150.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe, as explicitly mentioned in the Europe section under 'Last Mile Electric Delivery Vehicles by Region'. The answer_unit is 'packages', so the numeric value is reported as 150."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluated 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request is 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 total tokens. The answer_unit is 'tokens', so the value is expressed as a number.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. ... The workload evaluates a total of 3000 requests.","The online inference workload evaluated 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request is 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 total tokens. The answer_unit is 'tokens', so the value is expressed as a number."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas (GHG) emissions generated by AI applications, including those used in oil and gas exploration. Multiple snippets from ref_id=ebert2024 confirm this omission, noting it creates a 'notable reporting gap'. Therefore, the AI Act does not mandate such disclosure.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context explicitly states that the AI Act fails to address greenhouse gas (GHG) emissions generated by AI applications, including those used in oil and gas exploration. Multiple snippets from ref_id=ebert2024 confirm this omission, noting it creates a 'notable reporting gap'. Therefore, the AI Act does not mandate such disclosure."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for the DenseNet 201 short job, the Flexible Start optimization led to up to 80% CO2 emissions reduction in the West US region. This is explicitly mentioned in the description of Figure 3, which compares emissions reductions across regions and job durations.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for the DenseNet 201 short job, the Flexible Start optimization led to up to 80% CO2 emissions reduction in the West US region. This is explicitly mentioned in the description of Figure 3, which compares emissions reductions across regions and job durations."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task is text classification with a mean energy consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with a mean of 2.9 kWh per 1,000 inferences. The ratio is calculated as 2.9 / 0.002 = 1450, and the text explicitly confirms that the variation is 'over 1450 in terms of the energy required'. The answer_unit is 'times', so the numeric factor is returned.","1450","times","[""luccioni2024""]","is_blank","Table 2: inference energy (kWh) for text classification (mean: 0.002) and image generation (mean: 2.907). We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the least energy-intensive task is text classification with a mean energy consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with a mean of 2.9 kWh per 1,000 inferences. The ratio is calculated as 2.9 / 0.002 = 1450, and the text explicitly confirms that the variation is 'over 1450 in terms of the energy required'. The answer_unit is 'times', so the numeric factor is returned."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that as of January 2024, Amazon had announced 244 renewable energy projects in the United States, as listed in the 'Project Location' table with 'United States' having 244 projects and total MW capacity of 17,706. This is the only explicit count of projects announced in the U.S. as of that date.","244","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
... United States 244 17,706","The context states that as of January 2024, Amazon had announced 244 renewable energy projects in the United States, as listed in the 'Project Location' table with 'United States' having 244 projects and total MW capacity of 17,706. This is the only explicit count of projects announced in the U.S. as of that date."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides renewable energy usage percentages for Amazon AWS in 2019 (42%), 2020 (65%), 2021 (85%), 2022 (90%), and 2023 (100%), but there is no data for 2018. Therefore, the percentage for 2018 cannot be determined from the provided context.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides renewable energy usage percentages for Amazon AWS in 2019 (42%), 2020 (65%), 2021 (85%), 2022 (90%), and 2023 (100%), but there is no data for 2018. Therefore, the percentage for 2018 cannot be determined from the provided context."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context from ref_id=ebert2024 explicitly recommends that energy consumption should be reported at the cumulative server level to balance accuracy and feasibility, as it states: 'energy consumption should be reported at the cumulative server level... estimations may be used only when direct measurements are unavailable.' This level is presented as the optimal compromise between precision and practical implementation.","cumulative server level","is_blank","[""ebert2024""]","is_blank","• Energy Reporting at the Cumulative Server Level : Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.","The context from ref_id=ebert2024 explicitly recommends that energy consumption should be reported at the cumulative server level to balance accuracy and feasibility, as it states: 'energy consumption should be reported at the cumulative server level... estimations may be used only when direct measurements are unavailable.' This level is presented as the optimal compromise between precision and practical implementation."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context from [ref_id=dodge2022] states that during the training of a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption, as shown in Table 1 with measured power draws for GPU, CPU, and DRAM components.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB)... The GPU alone accounts for 74% of the total energy consumption due to these components.","The context from [ref_id=dodge2022] states that during the training of a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption, as shown in Table 1 with measured power draws for GPU, CPU, and DRAM components."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from [ref_id=griggs2024] explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the unit as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric count.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from [ref_id=griggs2024] explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the unit as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric count."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context from [ref_id=wu2021a] explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period, which aligns with the timeframe of 2019 to 2021 mentioned in the question. The answer_unit is 'percent', so the value is returned as a number without the unit.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context from [ref_id=wu2021a] explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period, which aligns with the timeframe of 2019 to 2021 mentioned in the question. The answer_unit is 'percent', so the value is returned as a number without the unit."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context discusses energy consumption, water usage, carbon emissions, and public health impacts related to AI and data centers, but it does not mention any information about fiber optic cable installations or infrastructure length globally in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context discusses energy consumption, water usage, carbon emissions, and public health impacts related to AI and data centers, but it does not mention any information about fiber optic cable installations or infrastructure length globally in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context from [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, distinguishing it from the amortized cost of $40M. This value is directly provided in multiple sections, including the comparison between acquisition and amortized costs.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context from [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, distinguishing it from the amortized cost of $40M. This value is directly provided in multiple sections, including the comparison between acquisition and amortized costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, the claim that it was proposed to only encompass climate-positive applications is false.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, the claim that it was proposed to only encompass climate-positive applications is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for an NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is 17, as shown in Table IV. This value is derived from experimental ground truth and is not a projected value for future GPU capacities.","17","samples","[""xia2024""]","is_blank","TABLE IV: ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A100 80GB 17 2.74 1.67 25.4
H100 80GB 17 4.90 2.1 17.9","The context states that for an NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is 17, as shown in Table IV. This value is derived from experimental ground truth and is not a projected value for future GPU capacities."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Recomputation requires extra computation upon restoration, while Swapping only involves data movement, which consumes less energy.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Recomputation requires extra computation upon restoration, while Swapping only involves data movement, which consumes less energy."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context from ref_id=luccioni2025a explicitly states that Google reported a 48% increase in GHG emissions since 2019, attributing it to increases in data center energy consumption. The answer_unit is 'percent', so the value is returned as a number without the unit name.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ""increases in data center energy consumption"" [42]","The context from ref_id=luccioni2025a explicitly states that Google reported a 48% increase in GHG emissions since 2019, attributing it to increases in data center energy consumption. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context cites McKinsey & Company (ref_id=chung2025 and ref_id=luccioni2025a) and other sources (e.g., Shehabi et al., 2024; Green et al., 2024) projecting that data centers may consume upwards of 11.7% of total U.S. energy demand by 2030. This is corroborated by multiple sources including Fernandez et al. (ref_id=fernandez2025) and Morrison et al. (ref_id=morrison2025), which also cite the same 11.7% upper bound. The answer_unit is 'percent', so the value is returned as a number.","11.7","percent","[""chung2025"", ""fernandez2025"", ""morrison2025""]","[""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy"", ""https://arxiv.org/abs/2504.17674"", ""https://arxiv.org/abs/2503.05804""]","projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).","The context cites McKinsey & Company (ref_id=chung2025 and ref_id=luccioni2025a) and other sources (e.g., Shehabi et al., 2024; Green et al., 2024) projecting that data centers may consume upwards of 11.7% of total U.S. energy demand by 2030. This is corroborated by multiple sources including Fernandez et al. (ref_id=fernandez2025) and Morrison et al. (ref_id=morrison2025), which also cite the same 11.7% upper bound. The answer_unit is 'percent', so the value is returned as a number."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the 6 billion parameter transformer training run, which lasted 8 days, the Pause and Resume optimization can lead to significant emissions savings of up to about 25% when the duration is doubled. This is explicitly mentioned in Figure 4 and Table 5, where the maximum emissions reduction for this model under 100% duration increase is 11.4%, but the text clarifies that doubling the duration (i.e., 100% increase) can lead to savings up to 25%.","25","percent","[""dodge2022""]","is_blank","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context states that for the 6 billion parameter transformer training run, which lasted 8 days, the Pause and Resume optimization can lead to significant emissions savings of up to about 25% when the duration is doubled. This is explicitly mentioned in Figure 4 and Table 5, where the maximum emissions reduction for this model under 100% duration increase is 11.4%, but the text clarifies that doubling the duration (i.e., 100% increase) can lead to savings up to 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context repeatedly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, directly answering the question. The answer_unit is 'percent', so the value is returned as a number without the unit name.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context repeatedly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, directly answering the question. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context from [ref_id=luccioni2025b] and [ref_id=strubell2019] both reference the seminal 2019 study by Strubell et al., which quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions. The unit specified in the question (lbs CO2e) matches this value exactly.","626155","lbs CO2e","[""luccioni2025b"", ""strubell2019""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context from [ref_id=luccioni2025b] and [ref_id=strubell2019] both reference the seminal 2019 study by Strubell et al., which quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions. The unit specified in the question (lbs CO2e) matches this value exactly."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context from ref_id=li2025b explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly provided and matches the required unit (MWh).","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context from ref_id=li2025b explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly provided and matches the required unit (MWh)."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and is identified as the costliest component, making it a prime target for optimization to enhance performance. This directly supports that the MoE layer is often targeted when enhancing LLM fine-tuning performance.","1","is_blank","[""xia2024""]","is_blank","MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and is identified as the costliest component, making it a prime target for optimization to enhance performance. This directly supports that the MoE layer is often targeted when enhancing LLM fine-tuning performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states in Table III that the maximum batch size supported by Mixtral-D (dense) on the Hellaswag (CS) dataset using an NVIDIA A40 GPU is 2. This value is explicitly listed under the 'Mixtral-D' column for the 'CS' dataset, which corresponds to Hellaswag.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","The context states in Table III that the maximum batch size supported by Mixtral-D (dense) on the Hellaswag (CS) dataset using an NVIDIA A40 GPU is 2. This value is explicitly listed under the 'Mixtral-D' column for the 'CS' dataset, which corresponds to Hellaswag."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment (A-8). This directly supports the truth of the statement.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment (A-8). This directly supports the truth of the statement."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context discusses the energy consumption, water footprint, air pollution, and public health impacts of AI and data centers, but does not mention any data regarding the land area (in hectares) occupied by new AI data centers globally in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context discusses the energy consumption, water footprint, air pollution, and public health impacts of AI and data centers, but does not mention any data regarding the land area (in hectares) occupied by new AI data centers globally in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context from ref_id=luccioni2024 explicitly states that bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks. This matches the required unit of g CO2eq.","0.32","g CO2eq","[""luccioni2024""]","is_blank","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context from ref_id=luccioni2024 explicitly states that bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks. This matches the required unit of g CO2eq."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context from ref_id=dodge2022 states that training BERT on 8 V100s for 36 hours resulted in emissions ranging from 7k grams in the most efficient region to 26k grams in the least efficient region. This range is explicitly given in grams, matching the required answer_unit.","[7000, 26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context from ref_id=dodge2022 states that training BERT on 8 V100s for 36 hours resulted in emissions ranging from 7k grams in the most efficient region to 26k grams in the least efficient region. This range is explicitly given in grams, matching the required answer_unit."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context explicitly states that 'the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information,' and that a decline followed after 2022 due to the rise of commercial and proprietary models. This directly identifies 2022 as the peak year.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context explicitly states that 'the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information,' and that a decline followed after 2022 due to the rise of commercial and proprietary models. This directly identifies 2022 as the peak year."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context shows that training the OLMo 60M† model consumed 1.6 kL of water, equivalent to 5 days of water usage by an average person in the U.S. This is explicitly stated in Table 2 under the 'Equiv. to... (water usage, 1 person, U.S.)' column for OLMo 60M†.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context shows that training the OLMo 60M† model consumed 1.6 kL of water, equivalent to 5 days of water usage by an average person in the U.S. This is explicitly stated in Table 2 under the 'Equiv. to... (water usage, 1 person, U.S.)' column for OLMo 60M†."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The provided context does not mention Google's DeepMind AlphaFold servers or any specific freshwater consumption data for them in 2023. While it discusses AI water footprints and mentions DeepMind reducing Google's data center cooling bills by 40%, no quantitative data is given for AlphaFold servers.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The provided context does not mention Google's DeepMind AlphaFold servers or any specific freshwater consumption data for them in 2023. While it discusses AI water footprints and mentions DeepMind reducing Google's data center cooling bills by 40%, no quantitative data is given for AlphaFold servers."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that 'a way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers,' indicating that adding compute resources is proposed as a cost-reduction strategy, not a cost-increasing one. Therefore, the statement that it 'can increase costs' is false.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that 'a way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers,' indicating that adding compute resources is proposed as a cost-reduction strategy, not a cost-increasing one. Therefore, the statement that it 'can increase costs' is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context states that the energy consumption for training FLM-101B is 40 MkWh (mega-kilowatt-hours), which equals 40,000,000 kWh. This is found in Table 3 under the 'Energy (MkWh)' row for FLM-101B.","40000000","kWh","[""li2025a""]","is_blank","Energy (MkWh) ... FLM-101B 40","The context states that the energy consumption for training FLM-101B is 40 MkWh (mega-kilowatt-hours), which equals 40,000,000 kWh. This is found in Table 3 under the 'Energy (MkWh)' row for FLM-101B."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides projections for the total public health burden of U.S. data centers in 2028, not 2030. The highest projected value is $20.9 billion under the high-growth scenario for 2028. There is no mention of 2030 projections in the provided context, so the requested year cannot be addressed.","is_blank","USD","[""han2024""]","is_blank","The overall public health costs could reach more than $20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with∼35 million registered vehicles [41].","The context provides projections for the total public health burden of U.S. data centers in 2028, not 2030. The highest projected value is $20.9 billion under the high-growth scenario for 2028. There is no mention of 2030 projections in the provided context, so the requested year cannot be addressed."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides the estimated cost of fine-tuning Mixtral on the GSM8K (GS) dataset with sparse MoE using an NVIDIA A40-48GB GPU in Table IV, which lists the cost as $32.7. This value is explicitly stated under the 'Cost ($)' column for the A40 GPU.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context provides the estimated cost of fine-tuning Mixtral on the GSM8K (GS) dataset with sparse MoE using an NVIDIA A40-48GB GPU in Table IV, which lists the cost as $32.7. This value is explicitly stated under the 'Cost ($)' column for the A40 GPU."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context states that for Mixtral fine-tuned on the CS dataset with sparse MoE and an NVIDIA A40 GPU (48GB), the maximum batch size supported is 8 (see Table III). The MoE layer is the most time-consuming component, and the longest-running MoE layer would occur at the maximum batch size. Thus, the batch size for the longest-running MoE layer is 8 samples.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","The context states that for Mixtral fine-tuned on the CS dataset with sparse MoE and an NVIDIA A40 GPU (48GB), the maximum batch size supported is 8 (see Table III). The MoE layer is the most time-consuming component, and the longest-running MoE layer would occur at the maximum batch size. Thus, the batch size for the longest-running MoE layer is 8 samples."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides execution time breakdowns for Mixtral fine-tuning on NVIDIA A40 GPU, but does not specify the total execution time for a sparse Mixtral model with batch size 10. Figure 4 shows execution time breakdowns for batch sizes of 1 and maximum supported batch sizes, but no exact value for batch size 10 is given. The context also does not provide a direct numeric value for total execution time under these exact conditions.","is_blank","seconds","[""xia2024""]","is_blank","Figure 4 illustrates execution time breakdown for Mixtral with batch sizes of 1 and maximum supported batch sizes, but no value is given for batch size 10.","The context provides execution time breakdowns for Mixtral fine-tuning on NVIDIA A40 GPU, but does not specify the total execution time for a sparse Mixtral model with batch size 10. Figure 4 shows execution time breakdowns for batch sizes of 1 and maximum supported batch sizes, but no exact value for batch size 10 is given. The context also does not provide a direct numeric value for total execution time under these exact conditions."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and the authors advocate against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and the authors advocate against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model’s overall emissions. This directly provides the percentage as 50%.","50","percent","[""luccioni2025b""]","is_blank","finding that training accounted for only half of the model’s overall emissions [121]","The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model’s overall emissions. This directly provides the percentage as 50%."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context from ref_id=samsi2024 states that the LLaMA 7B model was run on a single GPU using the bare minimum hardware settings, and Table II explicitly lists that 1 V100 32GB GPU is sufficient for LLaMA 7B inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 7B 1 64 1 64","The context from ref_id=samsi2024 states that the LLaMA 7B model was run on a single GPU using the bare minimum hardware settings, and Table II explicitly lists that 1 V100 32GB GPU is sufficient for LLaMA 7B inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment location. Table 1 in [li2025b] shows that for Arizona, the total water consumption per request is 29.926 mL. Using this value, 500 mL divided by 29.926 mL per request yields approximately 16.7 requests. This matches the '# of Requests for 500ml Water' value listed for Arizona in Table 1.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. ... Arizona ... Water for Each Request(mL): 29.926 ... # of Requests for 500ml Water: 16.7","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment location. Table 1 in [li2025b] shows that for Arizona, the total water consumption per request is 29.926 mL. Using this value, 500 mL divided by 29.926 mL per request yields approximately 16.7 requests. This matches the '# of Requests for 500ml Water' value listed for Arizona in Table 1."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context from [ref_id=dodge2022] explicitly states that the emissions from one average US home energy use per year is 8.3 metric tons of CO2, which includes electricity generation, natural gas, liquid petroleum gas, and fuel oil. This directly answers the question with the required unit of metric tons.","8.3","metric tons","[""dodge2022""]","is_blank","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context from [ref_id=dodge2022] explicitly states that the emissions from one average US home energy use per year is 8.3 metric tons of CO2, which includes electricity generation, natural gas, liquid petroleum gas, and fuel oil. This directly answers the question with the required unit of metric tons."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context from ref_id=shen2024 shows that JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark, which evaluates grade school math problem-solving. This value is explicitly listed in Table 3 under the GSM8k row for JetMoE.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8","The context from ref_id=shen2024 shows that JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark, which evaluates grade school math problem-solving. This value is explicitly listed in Table 3 under the GSM8k row for JetMoE."
