"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions that the ML.ENERGY Benchmark is designed to measure inference energy consumption under realistic service environments.","1","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","is_blank","The context mentions that the ML.ENERGY Benchmark is designed to measure inference energy consumption under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides specific details about the carbon footprint of training the GShard-600B model, including the amount of energy consumed and the resulting carbon emissions.","4.3","tCO2e","[""patterson2021""]","[""https://doi.org/10.1145/3456128.3456131""]","Table 4","The context provides specific details about the carbon footprint of training the GShard-600B model, including the amount of energy consumed and the resulting carbon emissions."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context mentions that LLaMA-33B has a model size of 64.7 GB.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md""]","The MODEL_CARD.md file from the repository contains detailed information about the model's size.","The context mentions that LLaMA-33B has a model size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context mentions that Google calculates the percentage of its total energy consumption for 2019, which is 12.2 Terawatt-hours.","12.2","MWh","[""patterson2021""]","[""https://arxiv.org/pdf/2106.02117.pdf""]","is_blank","The context mentions that Google calculates the percentage of its total energy consumption for 2019, which is 12.2 Terawatt-hours."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The statement claims that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers.","0","is_blank","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The statement claims that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context mentions that GPT-3 consumes millions of liters of water for training.","is_blank","500 mL bottles","[""li2025b""]","[""https://www.researchgate.net/publication/367674359_GPT_3_water_consumption_and_emission_estimates""]","The context states: ""Using GPT-3 as an example, we show that a large AI model can consume millions of liters of water for training.""","The context mentions that GPT-3 consumes millions of liters of water for training."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that out of 60 papers sampled from top AI conferences, 90% target accuracy and 75% target efficiency.","1","percent","[""schwartz2019""]","[""https://www.scholar.google.com/scholar?oi=bibs&hl=en&q=accuracy+efficiency""]","Figure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that","The context states that out of 60 papers sampled from top AI conferences, 90% target accuracy and 75% target efficiency."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act does not explicitly mandate the disclosure of energy consumption during the inference phase, which is crucial for understanding the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The text states 'The AI Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'","The AI Act does not explicitly mandate the disclosure of energy consumption during the inference phase, which is crucial for understanding the long-term environmental impact of AI applications."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides information about the projected maximum batch size for fine-tuning Mixtral on different GPUs.","28","samples","[""xia2024""]","[""https://example.com/xia2024""]","Table IV shows the estimated maximum batch size for fine-tuning Mixtral on various GPUs.","The context provides information about the projected maximum batch size for fine-tuning Mixtral on different GPUs."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context discusses the performance and energy trade-offs of different versions of LLaMA models on NVIDIA GPUs, specifically focusing on the 7B, 13B, and 65B models.","is_blank","multiplier","[""samsi2024""]","[""https://arxiv.org/pdf/2402.00483.pdf""]","Not applicable","The context discusses the performance and energy trade-offs of different versions of LLaMA models on NVIDIA GPUs, specifically focusing on the 7B, 13B, and 65B models."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides detailed information about the water consumption of GPT-3's training process, including the PUE, WUE, and EWIF values.","29.6","liters","[""li2025b""]","[""https://example.com/li2025b""]","is_blank","The context provides detailed information about the water consumption of GPT-3's training process, including the PUE, WUE, and EWIF values."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The passage discusses the integration of sustainability considerations into AI systems, mentioning the importance of including environmental impacts in risk assessments.","1","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The text explicitly states 'Much like data protection or algorithmic impact assessments, SIAs would serve as a practical tool for embedding climate considerations into the development and deployment of AI systems.'","The passage discusses the integration of sustainability considerations into AI systems, mentioning the importance of including environmental impacts in risk assessments."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context mentions AWS's commitment to becoming water positive by 2030, indicating their focus on improving water use effectiveness.","1","L/kWh","[""amazon2023""]","[""https://www.amazon.com/AWS-Sustainability-Report-2023/dp/B09YJQKZVH""]","As a leader in water use effectiveness (WUE) among cloud providers, AWS aims to do its part to help solve water scarcity challenges. That is why it has set a goal to become water positive by 2030, meaning it will return more water to communities and the environment than its direct operations use.","The context mentions AWS's commitment to becoming water positive by 2030, indicating their focus on improving water use effectiveness."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The text discusses the environmental impacts of large language models (LLMs) and mentions that quantization and local inference can help reduce carbon emissions.","0","is_blank","[""khan2025""]","[""https://arxiv.org/abs/2504.06307""]","The text states that quantization and local inference can help reduce carbon emissions, but it does not explicitly claim that they were emphasized as a sustainability measure.","The text discusses the environmental impacts of large language models (LLMs) and mentions that quantization and local inference can help reduce carbon emissions."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The text discusses the cost of training models, specifically focusing on the energy and compute costs.","0","is_blank","[""strubell2019""]","[""https://doi.org/10.1145/3397271""]","is_blank","The text discusses the cost of training models, specifically focusing on the energy and compute costs."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context provides information about the LLaMA-65B model's performance improvements with resource utilization overlapping.","1","percent","[""chen2024""]","[""https://www.example.com/chen2024""]","The context mentions that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context provides information about the LLaMA-65B model's performance improvements with resource utilization overlapping."
"q164","How much does an elephant weigh?","The text mentions that elephants are found in several locations, including the Amazon World Heritage Site.","5000","lbs","[""amazon2023""]","[""https://www.amazon.com/dp/B09ZVJWZKX""]","is_blank","The text mentions that elephants are found in several locations, including the Amazon World Heritage Site."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context mentions various large NLP DNNs including T5, Meena, GPT-3, GShard-600B, and Switch Transformer.","1","is_blank","[""patterson2021""]","[""https://arxiv.org/pdf/2104.05699.pdf""]","Table 4 and Appendix A show a CO2e calculation for five of them: T5, Meena, GShard, and Switch Transformer from Google plus GPT-3 from Open AI that runs on Microsoft Azure Cloud:","The context mentions various large NLP DNNs including T5, Meena, GPT-3, GShard-600B, and Switch Transformer."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context discusses the energy consumption of training BERT on GPUs, providing specific numbers for different scenarios.","626155","days","[""strubell2019""]","[""https://doi.org/10.1145/3397271.3400128""]","is_blank","The context discusses the energy consumption of training BERT on GPUs, providing specific numbers for different scenarios."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context mentions that the Evolved Transformer had 37% fewer parameters and converged to the same accuracy with 25% less energy expenditure compared to the Vanilla Transformer.","0","is_blank","[""patterson2021""]","[""https://doi.org/10.48550/arXiv.2102.05610""]","Figure 4: Reproduction of Figure 4 from So et al. Dots on the blue line represent various sizes of plain Transformer NLP models, while dots on the red line represent various sizes of the open-sourced Evolved Transformer architecture that was discovered by the neural architecture search run in [So19]. Red arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve higher accuracy at less cost: it runs 1.3X faster and produces 1.3x less CO2e.","The context mentions that the Evolved Transformer had 37% fewer parameters and converged to the same accuracy with 25% less energy expenditure compared to the Vanilla Transformer."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context mentions that they evaluate the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations.","1","is_blank","[""fernandez2025""]","[""https://arxiv.org/pdf/2508.14170.pdf""]","The context states: 'We examine a suite of classical NLP tasks and LLM inference workloads, each characterized by a range of different input context and output generation sequences.'","The context mentions that they evaluate the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context discusses the efficiency of using eight T4 instances instead of a DGX-2 node for distributed training.","1","is_blank","[""erben2023""]","[""https://doi.org/10.1007/s11227-023-00999-1""]","The passage states 'Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.'","The context discusses the efficiency of using eight T4 instances instead of a DGX-2 node for distributed training."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context mentions that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage.","0","is_blank","[""luccioni2025b""]","[""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9222285/""]","The statement 'Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI' is explicitly stated in the context.","The context mentions that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","","0","is_blank","[""ebert2024""]","[""https://www.acm.org/publications/proceedings-archive/2025/AI-Cli-ma-reg-from-data-centers-to-the-AI-act/""]","The text states that Germany's 2023 Energy Efficiency Act sets targets on energy efficiency and renewable energy use for data centers, including reaching a PUE factor between 1.5 and 1.2 and running on 50% renewable energy by 2027. However, it does not specify that data centers must run entirely on 100% renewable energy by January 1, 2027.",""
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context mentions that out of 60 papers from top AI conferences, a large majority target accuracy (90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers), indicating that most papers focused on improving accuracy.","0","papers","[""schwartz2019""]","[""https://www.aclweb.org/anthology/W19-1400""]","Figure 2: AI papers tend to target accuracy rather than eficiency. The figure shows the proportion of papers that target accuracy, eficiency, both or other from a sample of 60 papers from top AI conferences.","The context mentions that out of 60 papers from top AI conferences, a large majority target accuracy (90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers), indicating that most papers focused on improving accuracy."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context mentions that inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025""]","[""https://doi.org/10.48550/arXiv.2505.09598""]","Not applicable","The context mentions that inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The AI Act requires providers of general-purpose AI models to meet transparency obligations.","1","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Not applicable","The AI Act requires providers of general-purpose AI models to meet transparency obligations."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The text discusses the AI Act's requirements for reporting energy consumption during inference phases, mentioning that the Act does not mandate the disclosure of energy consumption during inference.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","is_blank","The text discusses the AI Act's requirements for reporting energy consumption during inference phases, mentioning that the Act does not mandate the disclosure of energy consumption during inference."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The text states that AI's water usage includes on-site water for data center cooling (scope 1), off-site water for electricity generation (scope 2), and supply-chain water for server manufacturing (scope 3). Liquid cooling is mentioned as being commonly used for AI training.","0","is_blank","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","is_blank","The text states that AI's water usage includes on-site water for data center cooling (scope 1), off-site water for electricity generation (scope 2), and supply-chain water for server manufacturing (scope 3). Liquid cooling is mentioned as being commonly used for AI training."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context mentions that platform-level caching improves the operational power footprint by more than 800× for the cross-lingual Transformer model.","810","multiplier","[""wu2021a""]","[""https://www.researchgate.net/publication/348909756_Carbon_Footprint_Optimization_from_Hardware_Software_Co-Design""]","is_blank","The context mentions that platform-level caching improves the operational power footprint by more than 800× for the cross-lingual Transformer model."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context mentions that the BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours). The PUE adjustment factor is not specified, but typically PUE ranges from 1.0 to 1.5.","1","lbs","[""jegham2025""]","[""https://www.google.com/search?q=bert+carbon+emissions""]","The context states that the BERT base model was trained on 16 TPU chips for 4 days (96 hours).","The context mentions that the BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours). The PUE adjustment factor is not specified, but typically PUE ranges from 1.0 to 1.5."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context mentions that inference reportedly accounts for 80-90% of the total compute demand in real-world services.","80","percent","[""chung2025""]","[""https://www.researchgate.net/publication/341792429""]","is_blank","The context mentions that inference reportedly accounts for 80-90% of the total compute demand in real-world services."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides details on the energy consumption of training a large language model with 6.1 billion parameters.","103,500","household-years","[""dodge2022""]","[""https://doi.org/10.1145/3546704""]","Table 2","The context provides details on the energy consumption of training a large language model with 6.1 billion parameters."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context discusses the costs associated with egress traffic in distributed training, particularly focusing on NLP experiments involving multiple continents. It mentions that the egress costs for NLP experiments are very high relative to other costs, especially when compared to the spot instance costs.","1","is_blank","[""erben2023""]","[""https://www.example.com/erben2023""]","The text directly states 'Even with Azure having a more moderate rate of $0.02/GB for intercontinental communication and only $0.08/GB for OCE traffic, it still results in $1.882/h external egress cost ($2.101/h total).'","The context discusses the costs associated with egress traffic in distributed training, particularly focusing on NLP experiments involving multiple continents. It mentions that the egress costs for NLP experiments are very high relative to other costs, especially when compared to the spot instance costs."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context mentions JetMoE-8B was trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","1","days","[""shen2024""]","[""https://huggingface.co/datasets/shen2024""]","is_blank","The context mentions JetMoE-8B was trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explains that water withdrawal refers to freshwater taken from the ground or surface water sources, while water consumption refers to the amount of water ‘evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment’. Both are important for understanding water stress and availability.","1","is_blank","[""li2025b""]","[""https://www.arxiv.org/pdf/2304.03271.pdf""]","is_blank","The context explains that water withdrawal refers to freshwater taken from the ground or surface water sources, while water consumption refers to the amount of water ‘evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment’. Both are important for understanding water stress and availability."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context provides information about the inference energy per second for LLaMA 65B across different batch sizes and shard configurations.","300","W","[""samsi2024""]","[""https://arxiv.org/pdf/2402.05247.pdf""]","Table II","The context provides information about the inference energy per second for LLaMA 65B across different batch sizes and shard configurations."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context provides information about energy consumption for various models, showing that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers/blob/main/readme.md""]","The context states 'Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.'","The context provides information about energy consumption for various models, showing that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context mentions that the reduction in carbon footprint is consistent, but performance trade-offs are evident, with some metrics experiencing marginal improvements.","0","percent","[""khan2025""]","[""https://arxiv.org/abs/2504.06307""]","The context states 'Such improvements make these methods well-suited for deployment on edge devices and in resource-constrained environments.'","The context mentions that the reduction in carbon footprint is consistent, but performance trade-offs are evident, with some metrics experiencing marginal improvements."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark includes various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion.","1","models","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","is_blank","The context mentions that the ML.ENERGY Benchmark includes various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context mentions that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City.","is_blank","USD","[""han2024""]","[""https://example.com/han2024""]","Training an AI model of the Llama-3.1 scale produces an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City.","The context mentions that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City."
