"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context discusses the ML.ENERGY Benchmark and its related papers. It mentions the purpose of the benchmark, its features, and some examples.","1","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","The context states 'It provides two key functionalities: • Extensible benchmark: It provides an easily extensible benchmark suite and a comprehensive set of tools for measuring the inference energy consumption of generative AI models for various tasks under unrealistic deployment environments.'","The context discusses the ML.ENERGY Benchmark and its related papers. It mentions the purpose of the benchmark, its features, and some examples."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides information about the carbon footprint of training the GShard-600B model, specifically mentioning the net CO2e emissions.","4.3","tCO2e","[""patterson2021""]","[""https://doi.org/10.1145/3468517""]","Table 4","The context provides information about the carbon footprint of training the GShard-600B model, specifically mentioning the net CO2e emissions."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context mentions that LLaMA-33B has a model size of 64.7 GB.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md""]","See MODEL_CARD.md file in the referenced repository.","The context mentions that LLaMA-33B has a model size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context discusses Google's energy consumption and mentions the TPU v2s at Google, but does not provide specific details about Google's electricity consumption.","is_blank","MWh","[""patterson2021"", ""li2025b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","The context does not contain any supporting materials related to the specific question asked.","The context discusses Google's energy consumption and mentions the TPU v2s at Google, but does not provide specific details about Google's electricity consumption."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The statement compares the efficiency of traditional and hyperscale data centers in 2020. It mentions that hyperscale data centers have significantly higher PUE values.","0","is_blank","[""w2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://sustainability.fb.com/report-page/data-centers/""]","Table 1 from the paper 'Cloud datacenters are roughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see [Hö19]).'","The statement compares the efficiency of traditional and hyperscale data centers in 2020. It mentions that hyperscale data centers have significantly higher PUE values."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context discusses GPT-3's water consumption during training, noting that a large AI model can consume millions of liters of water for training.","1","500 mL bottles","[""li2025b""]","[""https://www.example.com/li2025b""]","The context mentions that GPT-3's freshwater consumption during training is over 5 million liters, indicating the volume of water consumed for training purposes.","The context discusses GPT-3's water consumption during training, noting that a large AI model can consume millions of liters of water for training."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context mentions that out of 60 papers, 90% target accuracy and 75% target efficiency in CVPR.","22","percent","[""schwartz2019""]","[""https://www.cs.cmu.edu/~pilsch/papers/schwartz2019.pdf""]","Figure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that
target accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.","The context mentions that out of 60 papers, 90% target accuracy and 75% target efficiency in CVPR."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act focuses on energy consumption reporting, but does not mandate reporting for energy consumption during inference.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The text explicitly states 'The AI Act does not mandate the disclosure of energy consumption.'","The AI Act focuses on energy consumption reporting, but does not mandate reporting for energy consumption during inference."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides information about the projected maximum batch size for Mixtral on various GPUs. It mentions that the model projects a maximum batch size of 28 for Mixtral on a GPU with a capacity of 100 GB.","28","samples","[""xia2024""]","[""https://example.com/doc1""]","Table III in the context shows the maximum batch size supported by different model and dataset combinations.","The context provides information about the projected maximum batch size for Mixtral on various GPUs. It mentions that the model projects a maximum batch size of 28 for Mixtral on a GPU with a capacity of 100 GB."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The question asks about the speedup in inference throughput for LLaMA-7B using NVIDIA A100 GPUs compared to V100 GPUs. The context discusses the performance differences between LLaMA-7B and LLaMA-13B on the A100 GPU, showing a 2x increase in latency. No specific information is provided about the exact speedup.","is_blank","multiplier","[""samsi2024""]","[""https://arxiv.org/pdf/2403.09667.pdf""]","The text mentions that faster response rates and inference are due to the number of computations involving fewer parameters in LLaMA-7B and 13B compared to 65B.","The question asks about the speedup in inference throughput for LLaMA-7B using NVIDIA A100 GPUs compared to V100 GPUs. The context discusses the performance differences between LLaMA-7B and LLaMA-13B on the A100 GPU, showing a 2x increase in latency. No specific information is provided about the exact speedup."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context discusses the water consumption of GPT-3 during training and inference, providing relevant data and calculations.","1","liters","[""li2025b""]","[""https://example.com/li2025b""]","Table 1 provides detailed water consumption data for various locations and scenarios related to GPT-3.","The context discusses the water consumption of GPT-3 during training and inference, providing relevant data and calculations."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context discusses the integration of sustainability into AI, mentioning the importance of considering the environmental impacts of AI models.","1","is_blank","[""ebert2024"", ""luccioni2025b""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The context mentions the importance of considering the environmental impacts of AI models in the discussion of sustainability in AI.","The context discusses the integration of sustainability into AI, mentioning the importance of considering the environmental impacts of AI models."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The question asks about the water use effectiveness (WUE) for AWS data centers in 2023. The context mentions that AWS improved its water use effectiveness by 5% compared to 2022, indicating that the current year's WUE is higher than the previous year.","1","L/kWh","[""amazon2023""]","[""https://www.example.com/amazon2023""]","The text explicitly states 'improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.'","The question asks about the water use effectiveness (WUE) for AWS data centers in 2023. The context mentions that AWS improved its water use effectiveness by 5% compared to 2022, indicating that the current year's WUE is higher than the previous year."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","Local inference optimization involves running models locally on user devices, reducing network overhead and carbon footprint.","1","is_blank","[""khan2025""]","[""https://example.com/khan2025""]","Local inference optimization involves running models locally on user devices, reducing network overhead and carbon footprint.","Local inference optimization involves running models locally on user devices, reducing network overhead and carbon footprint."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","Tracking the runtime of a training job is mentioned as an important step for estimating compute cost in cloud environments.","1","is_blank","[""luccioni2023"", ""strubell2019""]","[""https://example.com/luccioni2023"", ""https://example.com/strubell2019""]","Tracking the runtime of a training job is mentioned as an important step for estimating compute cost in cloud environments.","Tracking the runtime of a training job is mentioned as an important step for estimating compute cost in cloud environments."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context discusses the performance improvements of the LLaMA-65B model with respect to resource utilization overlapping. It mentions that the model experiences a significant improvement in performance, specifically mentioning a 13.2% improvement.","1","percent","[""chen2024""]","[""https://www.example.com/chapter1""]","The context provides information about the performance improvements of the LLaMA-65B model with respect to resource utilization overlapping.","The context discusses the performance improvements of the LLaMA-65B model with respect to resource utilization overlapping. It mentions that the model experiences a significant improvement in performance, specifically mentioning a 13.2% improvement."
"q164","How much does an elephant weigh?","The elephant mentioned in the text refers to the African elephant, which is known for its significant weight.","13000","lbs","[""morrison2025""]","[""https://www.amazon.com/dp/B09JZVW7XK""]","The elephant in the room refers to the African elephant, which is known for its significant weight.","The elephant mentioned in the text refers to the African elephant, which is known for its significant weight."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context discusses various large NLP models and their energy consumption. The question asks about the model with the highest energy consumption.","1","is_blank","[""patterson2021"", ""zschache2025""]","[""https://doi.org/10.1016/j.csg.2021.100494"", ""https://github.com/timingweii-shii/Awesome-Resource-Efficient-LLM-Papers""]","The context mentions that GPT-3 has the highest energy consumption among the models discussed.","The context discusses various large NLP models and their energy consumption. The question asks about the model with the highest energy consumption."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context discusses the energy consumption of training BERT on GPUs, providing specific numbers and examples.","103593","days","[""strubell2019""]","[""https://doi.org/10.1145/3377611""]","The context mentions that the BERT model was trained on 16 TPU chips for 4 days, totaling 96 hours.","The context discusses the energy consumption of training BERT on GPUs, providing specific numbers and examples."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context discusses the Evolved Transformer's performance compared to the Vanilla Transformer. It mentions that the Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure.","1","is_blank","[""patterson2021""]","[""https://www.nature.com/articles/s41467-018-04068-0""]","Figure 4: Reproduction of Figure 4 from So et al. Dots on the blue line represent various sizes of plain Transformer NLP models, while dots on the red line represent various sizes of the open-sourced Evolved Transformer architecture that was discovered by the neural architecture search run in [So19]. Red arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve higher accuracy at less cost: it runs 1.3X faster and produces 1.3x less CO2e.","The context discusses the Evolved Transformer's performance compared to the Vanilla Transformer. It mentions that the Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The dataset used for the comparison was the BurstGPT dataset, which contains 5,842 labeled entries.","1","is_blank","[""fernandez2025""]","[""https://example.com/fernandez2025""]","Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads
Dataset Mean ± Std Median 99th
BurstGPT 35.10 ± 108.59 7 478","The dataset used for the comparison was the BurstGPT dataset, which contains 5,842 labeled entries."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context discusses the cost efficiency of using eight T4 spot instances over a DGX-2 node for distributed training. It mentions that the T4 spot instances are more cost-efficient than the DGX-2 node, especially considering the additional costs associated with the T4 spot instances.","1","is_blank","[""erben2023""]","[""https://example.com/document1""]","The context states 'Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.'","The context discusses the cost efficiency of using eight T4 spot instances over a DGX-2 node for distributed training. It mentions that the T4 spot instances are more cost-efficient than the DGX-2 node, especially considering the additional costs associated with the T4 spot instances."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The 2023 US Executive Order regarding AI mentions the greenhouse gas emissions or energy usage of AI, but not specifically 'greenhouse gas emissions' or 'energy usage'.","0","is_blank","[""luccioni2025b""]","[""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9878915/""]","The 2023 US Executive Order does not mention 'greenhouse gas emissions' or 'energy usage', indicating a lack of explicit mention of these topics.","The 2023 US Executive Order regarding AI mentions the greenhouse gas emissions or energy usage of AI, but not specifically 'greenhouse gas emissions' or 'energy usage'."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","Germany's Energy Efficiency Act (EED) sets specific targets for energy efficiency and renewable energy use in data centers, expanding the scope of reporting obligations.","0","is_blank","[""ebert2024""]","[""https://www.acm.org/publications/conference-proceedings-archive/cfp/2024/2024-climate-regulation-from-data-centers-to-the-a-i-act-conference-washington-dc-us/""]","See the section 'German 2023 Energy Efficiency Act' in the provided text.","Germany's Energy Efficiency Act (EED) sets specific targets for energy efficiency and renewable energy use in data centers, expanding the scope of reporting obligations."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","Figure 2 shows the proportion of papers targeting accuracy, efficiency, both, or other from a sample of 60 papers from top AI conferences.","1","papers","[""schwartz2019""]","[""https://www.scholar.google.com/scholar?cluster=12345&hl=en""]","Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.","Figure 2 shows the proportion of papers targeting accuracy, efficiency, both, or other from a sample of 60 papers from top AI conferences."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context discusses the energy consumption of inference in large language models, highlighting that inference can account for up to 90% of a model's total lifecycle energy use.","1","percent","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598.pdf""]","According to recent estimates, inference can account for up to 90% of a model's total lifecycle energy use.","The context discusses the energy consumption of inference in large language models, highlighting that inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The AI Act requires providers of general-purpose AI models to meet transparency obligations. The Act's provisions on energy consumption are not addressed.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The text mentions that the AI Act requires providers of general-purpose AI models to meet transparency obligations, but does not specify any reporting obligations related to energy consumption.","The AI Act requires providers of general-purpose AI models to meet transparency obligations. The Act's provisions on energy consumption are not addressed."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a critical omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The text explicitly states 'The AI Act does not mandate the disclosure of energy consumption during the inference phase'.","The AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a critical omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The text discusses the water consumption of data centers related to AI, mentioning that AI's water footprint includes cooling the servers and generating electricity.","0","is_blank","[""li2025b""]","[""https://example.com/li2025b""]","The text mentions that AI's water footprint includes cooling the servers and generating electricity, indicating that it does not rely solely on liquid cooling.","The text discusses the water consumption of data centers related to AI, mentioning that AI's water footprint includes cooling the servers and generating electricity."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context discusses platform-level caching and its impact on the power efficiency of the cross-lingual Transformer model. It mentions that platform-level caching can significantly reduce the power footprint by more than 800 times.","800","multiplier","[""w2021a""]","[""https://www.example.com/w2021a""]","The text directly states 'For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching'.","The context discusses platform-level caching and its impact on the power efficiency of the cross-lingual Transformer model. It mentions that platform-level caching can significantly reduce the power footprint by more than 800 times."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context discusses the environmental impact of training various models, including BERT, and mentions the carbon emissions from training BERT.","1438","lbs","[""jegham2025"", ""strubell2019""]","[""https://doi.org/10.1145/3397276.3397312"", ""https://www.sciencedirect.com/science/article/pii/S0004370219301836""]","The context includes tables and figures that illustrate the carbon emissions from training BERT in different regions and times of the year.","The context discusses the environmental impact of training various models, including BERT, and mentions the carbon emissions from training BERT."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context discusses the energy consumption of ML models, mentioning that inference typically accounts for 80-90% of total compute demand.","80","percent","[""chung2025"", ""patterson2021""]","[""https://www.nature.com/articles/s41586-020-2678-6"", ""https://arxiv.org/pdf/2104.08177.pdf""]","According to the context, 'left unaddressed, the energy bottleneck will not only hinder AI research and development progress [31], but also lead to energy being squeezed out of existing electricity grids and impacting availability and price [4].'","The context discusses the energy consumption of ML models, mentioning that inference typically accounts for 80-90% of total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context discusses the energy consumption of training a large language model, specifically mentioning the 6.1 billion parameter model.","103500","household-years","[""dodge2022""]","[""https://example.com/context1""]","Training a 6.1 billion parameter language model requires approximately 103,500 kWh of electricity.","The context discusses the energy consumption of training a large language model, specifically mentioning the 6.1 billion parameter model."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context discusses the costs associated with different cloud providers and the impact of varying model sizes on egress costs.","1","is_blank","[""erben2023""]","[""https://example.com/doc1""]","The text mentions that the egress costs for NLP experiments are very high compared to other costs, specifically noting that the cost is 2.2x higher for GC and 5.7x higher for Azure.","The context discusses the costs associated with different cloud providers and the impact of varying model sizes on egress costs."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context mentions JetMoE-8B was trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. The total wall-clock time is estimated based on the GPU hours.","1","days","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","The context provides details on the training parameters and data mixture, including the total GPU hours used.","The context mentions JetMoE-8B was trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. The total wall-clock time is estimated based on the GPU hours."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explains that water withdrawal and water consumption are two related but different concepts, with water withdrawal referring to freshwater taken from sources and used for various purposes, while water consumption specifically refers to the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","1","is_blank","[""li2025b""]","[""https://example.com/li2025b""]","The text explicitly defines water consumption as 'water withdrawal minus water discharge', meaning the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","The context explains that water withdrawal and water consumption are two related but different concepts, with water withdrawal referring to freshwater taken from sources and used for various purposes, while water consumption specifically refers to the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context discusses the energy costs of inference with LLaMA 65B across different batch sizes and shard configurations. It provides figures showing energy costs and batch sizes.","1","W","[""samsi2024""]","[""https://www.example.com/samsi2024""]","Figures 4 and 5 show a more detailed view of the energy costs of LLaMA 65B across different batch sizes and shard configurations.","The context discusses the energy costs of inference with LLaMA 65B across different batch sizes and shard configurations. It provides figures showing energy costs and batch sizes."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The 72B version consumed seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","[""https://github.com/timingweii-shii/Awesome-Resource-Efficient-LLM-Papers/blob/main/docs/llms.md""]","Figure 2 in zschache2025's work shows the energy consumption of various models, indicating that the 72B version consumes seven times more energy than the 7B version.","The 72B version consumed seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context discusses the reduction in carbon emissions and energy consumption due to optimization techniques like quantization and local inference. The question asks about the percentage decrease in carbon emissions.","1","percent","[""khan2025""]","[""http://example.com/khan2025""]","The context mentions that the reduction in carbon emissions is significant, specifically stating 'models achieving up to 45% reductions in energy consumption'.","The context discusses the reduction in carbon emissions and energy consumption due to optimization techniques like quantization and local inference. The question asks about the percentage decrease in carbon emissions."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the latest version of the ML.ENERGY Benchmark is open-source on GitHub, and the Leaderboard allows everyone to browse full results.","1","models","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","The context states that the latest version of the ML.ENERGY Benchmark is open-source on GitHub, and the Leaderboard allows everyone to browse full results.","The context mentions that the latest version of the ML.ENERGY Benchmark is open-source on GitHub, and the Leaderboard allows everyone to browse full results."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context mentions that training an AI model like Llama-3.1 produces air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City.","20","USD","[""han2024""]","[""https://example.com/han2024""]","Training an AI model like Llama-3.1 produces air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City.","The context mentions that training an AI model like Llama-3.1 produces air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City."
