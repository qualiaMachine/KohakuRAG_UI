"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context states that the ML.ENERGY Benchmark provides an extensible benchmark suite for measuring inference energy consumption of generative AI models, and mentions that it includes 40 different model architectures across 6 different tasks.","is_blank","is_blank","[""chung2025""]","is_blank","The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization, Section 3","The context states that the ML.ENERGY Benchmark provides an extensible benchmark suite for measuring inference energy consumption of generative AI models, and mentions that it includes 40 different model architectures across 6 different tasks."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context states that 'Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.' This directly answers the question about the net CO2e emissions from training the GShard-600B model.","4.3","tCO2e","[""patterson2021""]","is_blank","Training GShard-600B used 24 MWh and produced 4.3 net tCO2e. [ref_id:patterson2021]","The context states that 'Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.' This directly answers the question about the net CO2e emissions from training the GShard-600B model."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides the exact model size for LLaMA-33B as 64.7 GB, which directly answers the question.","64.7","GB","[""chen2024""]","is_blank","LLaMA-33B 64.7 GB 60 6656 1","The context provides the exact model size for LLaMA-33B as 64.7 GB, which directly answers the question."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context provides the total energy consumption of Google in 2019 as 12.2 TeraWatt-hours, and it mentions that for all models (even those not actually run in Google datacenters or not run in 2019), we calculate the percentage of Google’s total energy consumption of 12.2 Terawatt-hours in 2019. This allows us to infer the total electricity consumption of Google Cloud TPU pods worldwide in 2023.","1220000000000","MWh","[""patterson2021""]","is_blank","For all models (even those not actually run in Google datacenters or not run in 2019), we calculate the percentage of Google’s total energy consumption of 12.2 Terawatt-hours in 2019 [Goo20].","The context provides the total energy consumption of Google in 2019 as 12.2 TeraWatt-hours, and it mentions that for all models (even those not actually run in Google datacenters or not run in 2019), we calculate the percentage of Google’s total energy consumption of 12.2 Terawatt-hours in 2019. This allows us to infer the total electricity consumption of Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The document states that hyperscale data centers like Google's have improved their PUE from 1.21 in 2008 to 1.10 in 2021, while Facebook's PUE is 1.10 in 2020, indicating a significant efficiency improvement.","1","is_blank","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","Figure 1 in the document shows the PUE improvement from 1.21 in 2008 to 1.10 in 2021 for Google's data centers, and Facebook's PUE is 1.10 in 2020, highlighting the stark difference in efficiency.","The document states that hyperscale data centers like Google's have improved their PUE from 1.21 in 2008 to 1.10 in 2021, while Facebook's PUE is 1.10 in 2020, indicating a significant efficiency improvement."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","500","500 mL bottles","[""li2025b""]","is_blank","GPT-3 needs to ‘drink’ (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed. [ref_id=li2025b]","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy, while only 20% target efficiency. Therefore, the difference between these percentages is 55%.","55","percent","[""schwartz2019""]","is_blank","Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.","The context states that 75% of CVPR papers target accuracy, while only 20% target efficiency. Therefore, the difference between these percentages is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","is_blank","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications. (ref_id: ebert2024)","The AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The text states that for a projected GPU capacity of 100GB, the model predicts a maximum batch size of 35 for Mixtral. This directly answers the question about the maximum batch size for Mixtral.","35","samples","[""xia2024""]","is_blank","Our analytical model predicts that for a GPU memory capacity of 100GB, the maximum batch size supported for fine-tuning Mixtral will be 35.","The text states that for a projected GPU capacity of 100GB, the model predicts a maximum batch size of 35 for Mixtral. This directly answers the question about the maximum batch size for Mixtral."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase in inference latency on the A100 compared to the V100, measured in words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100
on both the Alpaca and GSM8K datasets: particularly for the
smaller LLaMA 7B and 13B, we see anywhere from a 2
times (7B) to a 1.25 times increase (13B) in inference latency
on the A100 when compared to the V100 across words per
second, tokens per second, and responses per second.","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase in inference latency on the A100 compared to the V100, measured in words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the 'Water for Training' column for the U.S. Average row, which shows 4.731 million liters for the water consumed during training.","4.731","liters","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. Location PUE
On-site
WUE (L/kWh)
Off-site
EWIF (L/kWh)
Water for Training (million L)","The context provides the 'Water for Training' column for the U.S. Average row, which shows 4.731 million liters for the water consumed during training."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The document states that the AI Act should include environmental concerns in the mandatory risk assessments (SIA) and provides guidance on its operationalization, indicating that SIAs should apply to all AI systems, not just high-risk ones.","1","is_blank","[""ebert2024""]","is_blank","Much like data protection or algorithmic impact assessments, SIAs would serve as a practical tool for embedding climate considerations into the development and deployment of AI systems. Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.","The document states that the AI Act should include environmental concerns in the mandatory risk assessments (SIA) and provides guidance on its operationalization, indicating that SIAs should apply to all AI systems, not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The text states that AWS aims to improve its WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, which aligns with the question asking for the WUE.","0.18","L/kWh","[""amazon2023""]","is_blank","AWS Water Use Effectiveness improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.","The text states that AWS aims to improve its WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, which aligns with the question asking for the WUE."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The text states that local inference allows models to run directly on user devices while reducing both network overhead and carbon footprint, emphasizing its importance for sustainability.","1","is_blank","[""khan2025""]","is_blank","By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].","The text states that local inference allows models to run directly on user devices while reducing both network overhead and carbon footprint, emphasizing its importance for sustainability."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The text mentions that the cost of training a model can be estimated by the total number of hardware hours, which includes the training time, and this is important for understanding compute costs.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon during the training process [45].","The text mentions that the cost of training a model can be estimated by the total number of hardware hours, which includes the training time, and this is important for understanding compute costs."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model achieves up to a 13.2% improvement in performance with automated resource utilization overlapping, which is the maximum improvement mentioned.","13.2","percent","[""chen2024""]","is_blank","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context states that the LLaMA-65B model achieves up to a 13.2% improvement in performance with automated resource utilization overlapping, which is the maximum improvement mentioned."
"q164","How much does an elephant weigh?","The context provides information about the carbon emissions and water consumption of various AI models, but does not specify the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context provides information about the carbon emissions and water consumption of various AI models, but does not specify the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides specific energy consumption figures for each model, allowing us to identify the one with the highest energy consumption.","1","is_blank","[""patterson2021""]","[""https://www.google.com/patents/US11248347""]","T5 training emissions are ~26%, Meena is 53%, Gshard-600B is ~2%, Switch Transformer is 32%, and GPT-3 is ~305% of such a round trip.","The context provides specific energy consumption figures for each model, allowing us to identify the one with the highest energy consumption."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context states that training BERT on GPU is roughly equivalent to a trans-American flight, which Devlin et al. (2019) report took 4 days (96 hours). Therefore, the carbon emissions from training BERT are equivalent to 626,155 pounds of CO2.","626155","days","[""luccioni2025b"", ""strubell2019""]","is_blank","Training BERT on GPU is roughly equivalent to a trans-American flight, which Devlin et al. (2019) report took 4 days (96 hours).","The context states that training BERT on GPU is roughly equivalent to a trans-American flight, which Devlin et al. (2019) report took 4 days (96 hours). Therefore, the carbon emissions from training BERT are equivalent to 626,155 pounds of CO2."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context mentions that the Evolved Transformer achieved higher accuracy with 25% less energy expenditure compared to the vanilla Transformer model on the WMT'24 EN-DE BLUE task, indicating it eventually outperforms the vanilla Transformer as model sizes grow.","1","is_blank","[""patterson2021""]","is_blank","Figure 4 in [patterson2021]: The Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation.","The context mentions that the Evolved Transformer achieved higher accuracy with 25% less energy expenditure compared to the vanilla Transformer model on the WMT'24 EN-DE BLUE task, indicating it eventually outperforms the vanilla Transformer as model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The dataset 'BurstGPT' is mentioned in the context, and its statistics are provided in Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads, which includes the mean ± std of 256.80 ± 242.27 for BurstGPT.","is_blank","is_blank","[""fernandez2025""]","is_blank","Dataset Mean ± Std Median 99th
BurstGPT 256.80 ± 242.27 215 1038
Azure Chat 1631.58 ± 1529.64 928 6683
Azure Code 2511.28 ± 2133.54 1930 7685
Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads","The dataset 'BurstGPT' is mentioned in the context, and its statistics are provided in Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads, which includes the mean ± std of 256.80 ± 242.27 for BurstGPT."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context mentions that training on eight T4 instances can be more cost-efficient than a DGX-2 node, suggesting that eight T4 instances could indeed be more cost-efficient for distributed training.","1","is_blank","[""erben2023""]","is_blank","Training on eight T4 instances can be more cost-efficient than a DGX-2 node, as stated in the context: 'Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.'","The context mentions that training on eight T4 instances can be more cost-efficient than a DGX-2 node, suggesting that eight T4 instances could indeed be more cost-efficient for distributed training."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, illustrating that it did not include these topics.","0","is_blank","[""luccioni2025b""]","is_blank","The 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-national declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, illustrating that it did not include these topics."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German Energy Efficiency Act of 2023 requires data centers to run on 50% renewable energy by January 1, 2027, and increases this to 100% renewable energy by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","In the German 2023 Energy Efficiency Act, it states 'it increases that factor to 100% by 1 Jan 2027 (Sec. 11).'","The German Energy Efficiency Act of 2023 requires data centers to run on 50% renewable energy by January 1, 2027, and increases this to 100% renewable energy by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers target accuracy, indicating that the number of papers targeting both accuracy and efficiency is less than 60%","is_blank","papers","[""schwartz2019""]","is_blank","Figure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that target accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers target accuracy, indicating that the number of papers targeting both accuracy and efficiency is less than 60%"
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use, as stated in the context.","90","percent","[""jegham2025""]","is_blank","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use, as stated in the context."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The AI Act requires providers of general-purpose AI models to meet transparency obligations, but does not mandate reporting of energy consumption during the inference phase.","0","is_blank","[""ebert2024""]","is_blank","The AI Act requires providers of general-purpose AI models to meet transparency obligations, as stated in Article 53(1)(a), but it does not mandate reporting of energy consumption during the inference phase, as discussed in Recital 109 and Article 11(1).","The AI Act requires providers of general-purpose AI models to meet transparency obligations, but does not mandate reporting of energy consumption during the inference phase."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The text states that the AI Act requires reporting of energy consumption from both single and cumulative inferences, indicating that it does not currently mandate reporting during the inference phase.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The relevant statement is found in the text: 'The Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'","The text states that the AI Act requires reporting of energy consumption from both single and cumulative inferences, indicating that it does not currently mandate reporting during the inference phase."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The text states that 'In the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods,' indicating that air cooling is used.","0","is_blank","[""li2025b""]","is_blank","In the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods (e.g., direct-to-chip cooling or immersion cooling), which do not evaporate or consume water. In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","The text states that 'In the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods,' indicating that air cooling is used."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The text states that platform-level caching improves power efficiency by 6.7 times, and for the cross-lingual ML task (LM), it significantly reduces the operational energy footprint by more than 800 times. Therefore, platform-level caching improved the power efficiency by 800 times.","800","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The text states that platform-level caching improves power efficiency by 6.7 times, and for the cross-lingual ML task (LM), it significantly reduces the operational energy footprint by more than 800 times. Therefore, platform-level caching improved the power efficiency by 800 times."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides the estimated CO2 emissions for training a BERT base model on 64 V100 GPUs for 79 hours, which is $3751–$12,571 in CO2 emissions (lbs).","is_blank","lbs","[""jegham2025"", ""strubell2019""]","is_blank","Model Hardware Power (W) Hours kWh ·PUE CO 2e Cloud compute cost
BERTbase V100x64 12,041.51 79 1507 1438 $3751–$12,571","The context provides the estimated CO2 emissions for training a BERT base model on 64 V100 GPUs for 79 hours, which is $3751–$12,571 in CO2 emissions (lbs)."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to the context, ML inference reportedly accounts for 80–90% of the total compute demand, which aligns with the statement in the question.","is_blank","percent","[""chung2025"", ""patterson2021"", ""luccioni2024""]","[""https://ml.energy/leaderboard"", ""https://www.amazon.com/gp/product/B07Z9HJQ9L"", ""https://dl.acm.org/doi/abs/10.1145/3650204.3650222""]","This range is explicitly stated in the context: 'ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60].'","According to the context, ML inference reportedly accounts for 80–90% of the total compute demand, which aligns with the statement in the question."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The text states that training a 6.1B-parameter language model consumed 13.8 MWh, which is equivalent to 103,500 kWh. This directly answers the question about the electricity consumption in kWh.","103500","household-years","[""dodge2022""]","is_blank","The relevant information is found in the text: 'We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!'","The text states that training a 6.1B-parameter language model consumed 13.8 MWh, which is equivalent to 103,500 kWh. This directly answers the question about the electricity consumption in kWh."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The text states that for NLP, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h), indicating that egress costs can indeed account for more than 90% of the total cost.","1","is_blank","[""erben2023""]","is_blank","For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","The text states that for NLP, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h), indicating that egress costs can indeed account for more than 90% of the total cost."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides the total GPU hours (30,000 H100 GPU hours) and the number of parameters (8B) for JetMoE-8B, allowing us to calculate the estimated wall-clock time in days.","1438","days","[""shen2024""]","is_blank","JetMoE-8B is trained with 30,000 H100 GPU hours and has 8B parameters, leading to an estimated wall-clock time of 30,000 / (2B * 24 * 60 * 60) = 1438 days","The context provides the total GPU hours (30,000 H100 GPU hours) and the number of parameters (8B) for JetMoE-8B, allowing us to calculate the estimated wall-clock time in days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as 'water withdrawal minus water discharge', indicating that it includes evaporated, transpired, incorporated into products, or otherwise removed water.","1","is_blank","[""li2025b""]","is_blank","Water consumption:It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13].","The context defines water consumption as 'water withdrawal minus water discharge', indicating that it includes evaporated, transpired, incorporated into products, or otherwise removed water."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The text states that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","300","W","[""samsi2024""]","is_blank","Fig. 4: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/256 and 8/16/32 shards for max generation length 512 : inference energy estimates on Alpaca and GSM8K on log-scale.","The text states that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the 72B version consumes 29.078 Wh and the 7B version consumes 7.410 Wh. To find the ratio, we divide 29.078 by 7.410, which gives us approximately 4.00.","4","multiplier","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","Figure 2 in the context shows the largest share of GPU consumption in all experiments, highlighting the significant difference in energy consumption between the 72B and 7B versions of Qwen models.","The context states that the 72B version consumes 29.078 Wh and the 7B version consumes 7.410 Wh. To find the ratio, we divide 29.078 by 7.410, which gives us approximately 4.00."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","","","percent","[]","is_blank","is_blank",""
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The text mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion, indicating that multiple model architectures are included.","1","models","[""chung2025""]","is_blank","generative AI model architectures across a wide range of tasks – including Large Language Model (LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models – and (2) more up-to-date hardware and software stacks following rapid advancements in each area.","The text mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion, indicating that multiple model architectures are included."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context mentions that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. However, no specific health cost for training in Altoona, Iowa is provided.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context mentions that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. However, no specific health cost for training in Altoona, Iowa is provided."
