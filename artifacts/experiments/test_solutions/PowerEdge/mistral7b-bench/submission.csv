"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context provides information about the increase in U.S. data center electricity consumption from 2010 to 2014, which was about 4%. However, it does not directly provide the percentage increase in data center electricity consumption from 2005 to 2010 or from 2000 to 2005.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context provides information about the increase in U.S. data center electricity consumption from 2010 to 2014, which was about 4%. However, it does not directly provide the percentage increase in data center electricity consumption from 2005 to 2010 or from 2000 to 2005."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context provides information about Amazon's investments in renewable energy projects, including solar farms. The Amazon Solar Farm Maryland‚ÄìCPV Backbone project is mentioned, which is expected to avoid 64,000 metric tons of CO2e each year. To estimate the equivalent number of cars taken off the road, the context states that 13,900 cars would be equivalent to 64,000 metric tons of CO2e. Therefore, the number of cars that could be taken off the road equivalent to the amount of CO2e avoided by the Amazon Solar Farm Maryland‚ÄìCPV Backbone project is 13,900 cars.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels,¬†Amazon Solar Farm Maryland‚ÄìCPV Backbone will avoid more than 64,000 metric tons of CO2e each year‚Äîthe equivalent of taking more than 13,900 cars off the road.","The context provides information about Amazon's investments in renewable energy projects, including solar farms. The Amazon Solar Farm Maryland‚ÄìCPV Backbone project is mentioned, which is expected to avoid 64,000 metric tons of CO2e each year. To estimate the equivalent number of cars taken off the road, the context states that 13,900 cars would be equivalent to 64,000 metric tons of CO2e. Therefore, the number of cars that could be taken off the road equivalent to the amount of CO2e avoided by the Amazon Solar Farm Maryland‚ÄìCPV Backbone project is 13,900 cars."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context mentions that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, but it does not specify the number of data centers that used recycled water for cooling in 2023.","is_blank","data centers","[""amazon2023""]","is_blank","Using recycled water for data center cooling preserves valuable drinking water for communities.","The context mentions that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, but it does not specify the number of data centers that used recycled water for cooling in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context does not provide a specific estimate for the embodied carbon emissions in kg per GPU. However, it is mentioned that there is currently no information available about the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the overall carbon footprint this represents (ref_id=luccioni2025b).","is_blank","kg/GPU","[""luccioni2025b""]","[""https://www.sciencedirect.com/science/article/abs/pii/S2542435623001432""]","Furthermore, the authors themsel ves note that there is currently no information available about the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the overall carbon footprint this represents.","The context does not provide a specific estimate for the embodied carbon emissions in kg per GPU. However, it is mentioned that there is currently no information available about the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the overall carbon footprint this represents (ref_id=luccioni2025b)."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context does not provide a direct comparison between the estimated amortized training cost of GPT-4 and the total training budget for FLM-101B. However, it does mention that the cost of training GPT-4 is $40M, which can be used as a reference to estimate the cost of training other models. The growth rate in training cost is suggested to be 2.4x per year, implying that the most expensive publicly announced model by the start of 2027 will cost about $1 billion. Since FLM-101B was trained within a $100,000 budget, it is clear that the amortized training cost of GPT-4 is significantly higher than the budget for FLM-101B.","1","ratio","[""is_blank""]","is_blank","The context does not provide a direct comparison between the estimated amortized training cost of GPT-4 and the total training budget for FLM-101B. However, it does mention that the cost of training GPT-4 is $40M, which can be used as a reference to estimate the cost of training other models.","The context does not provide a direct comparison between the estimated amortized training cost of GPT-4 and the total training budget for FLM-101B. However, it does mention that the cost of training GPT-4 is $40M, which can be used as a reference to estimate the cost of training other models. The growth rate in training cost is suggested to be 2.4x per year, implying that the most expensive publicly announced model by the start of 2027 will cost about $1 billion. Since FLM-101B was trained within a $100,000 budget, it is clear that the amortized training cost of GPT-4 is significantly higher than the budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides an estimate of the CO2e for a single passenger round trip from San Francisco (SF) to New York (NY) as approximately 1.2t CO2e (Table 2).","1.2","tCO2e","[""patterson2021""]","is_blank","Table 1 in Patterson et al. (2021)","The context provides an estimate of the CO2e for a single passenger round trip from San Francisco (SF) to New York (NY) as approximately 1.2t CO2e (Table 2)."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context does not provide a direct answer to the question about the average performance score achieved by the FLM-101B model on the Open LLM Leaderboard. However, it does mention that FLM-101B achieves over 90% of the performance of GLM-130B, which has 7 times more FLOPs. It also states that FLM-101B outperforms GLM-130B on CLUE-IQ, but does not provide an average score for FLM-101B on the Open LLM Leaderboard.","is_blank","score","[""li2025a""]","is_blank","is_blank","The context does not provide a direct answer to the question about the average performance score achieved by the FLM-101B model on the Open LLM Leaderboard. However, it does mention that FLM-101B achieves over 90% of the performance of GLM-130B, which has 7 times more FLOPs. It also states that FLM-101B outperforms GLM-130B on CLUE-IQ, but does not provide an average score for FLM-101B on the Open LLM Leaderboard."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides information about the improvement in processor clock speed and transistor count for microprocessors over the past 50 years. It states that the typical microprocessor in 2021 is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","1","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html"", ""https://www.amd.com/en/products/cpu/amd-ryzen-9-5950x""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context provides information about the improvement in processor clock speed and transistor count for microprocessors over the past 50 years. It states that the typical microprocessor in 2021 is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides the number of days it takes for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3. It then calculates the training time by dividing the total FLOPS by the speed of the GPUs, resulting in 14.8 days.","14.8","days","[""patterson2021""]","is_blank","The calculation is based on the information provided in the 'patterson2021' document.","The context provides the number of days it takes for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3. It then calculates the training time by dividing the total FLOPS by the speed of the GPUs, resulting in 14.8 days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 4) that lists the GPU Power Usage (kWh) for various models, including Llama 3.2 1B, when serving 2400 prompts at an 8 request/s frequency. The table shows that the GPU Power Usage for Llama 3.2 1B is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: Full version of Table 3 in ¬ß4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served on machines from the same cluster that our models were trained on, so we use the same WUE and PUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO 2e / kWh. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. We do not report ‚Äòbreak-even‚Äô points for Qwen models since the training costs are not public.","The context provides a table (Table 4) that lists the GPU Power Usage (kWh) for various models, including Llama 3.2 1B, when serving 2400 prompts at an 8 request/s frequency. The table shows that the GPU Power Usage for Llama 3.2 1B is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides information about the permitted annual emission limits for diesel generators in Northern Virginia. The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. However, the question asks for the limits in tons, not short tons. To convert short tons to tons, we multiply by 0.90718474. Therefore, the total permitted annual emission limits for NOx, VOCs, SO2, and PM2.5 are approximately 11,702 tons, 1,259 tons, 45 tons, and 540 tons, respectively.","11702","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context provides information about the permitted annual emission limits for diesel generators in Northern Virginia. The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. However, the question asks for the limits in tons, not short tons. To convert short tons to tons, we multiply by 0.90718474. Therefore, the total permitted annual emission limits for NOx, VOCs, SO2, and PM2.5 are approximately 11,702 tons, 1,259 tons, 45 tons, and 540 tons, respectively."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated). The time cost for training a 101B model from scratch is given as 76.74 days. Therefore, the percentage of time saved by using the growth strategy for the 101B model is 72%.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The paper states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated). The time cost for training a 101B model from scratch is given as 76.74 days. Therefore, the percentage of time saved by using the growth strategy for the 101B model is 72%."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context discusses the public health impacts of data centers, specifically focusing on criteria air pollutants such as PM2.5, SO2, and NO2. It is mentioned that the server manufacturing process, electricity generation from fossil fuels to power data centers, and the maintenance and usage of diesel backup generators to ensure continuous data center operation all produce significant amounts of criteria air pollutants. The context also states that exposure to these pollutants is directly and causally linked to various adverse health outcomes, including premature mortality. However, the context does not provide specific numbers for premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030.","is_blank","deaths","[""han2024""]","[""https://www.sciencedirect.com/science/article/pii/S2542519623003135""]","is_blank","The context discusses the public health impacts of data centers, specifically focusing on criteria air pollutants such as PM2.5, SO2, and NO2. It is mentioned that the server manufacturing process, electricity generation from fossil fuels to power data centers, and the maintenance and usage of diesel backup generators to ensure continuous data center operation all produce significant amounts of criteria air pollutants. The context also states that exposure to these pollutants is directly and causally linked to various adverse health outcomes, including premature mortality. However, the context does not provide specific numbers for premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context provides information about the training time for a 6 billion parameter model, which is estimated to take approximately 13% of the time it would take to train to completion. Given this information, we can estimate a full training run would take 13/0.13 * 8 days = 103.84 days. To convert this to days, we multiply by 24 hours/day, resulting in 103.84 * 24 = 2452.16 days. To convert this to years, we divide by the number of days in a year (365), resulting in 2452.16 / 365 = approximately 6.72 years.","6.72","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context provides information about the training time for a 6 billion parameter model, which is estimated to take approximately 13% of the time it would take to train to completion. Given this information, we can estimate a full training run would take 13/0.13 * 8 days = 103.84 days. To convert this to days, we multiply by 24 hours/day, resulting in 103.84 * 24 = 2452.16 days. To convert this to years, we divide by the number of days in a year (365), resulting in 2452.16 / 365 = approximately 6.72 years."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide specific information about the energy consumption of different models for 1,000 queries in the 2025 study. However, it does discuss energy consumption in various studies and models, and mentions that energy consumption can vary significantly based on model type, model size, and hardware specifications.","is_blank","is_blank","[""fernandez2025"", ""zschache2025""]","is_blank","is_blank","The context does not provide specific information about the energy consumption of different models for 1,000 queries in the 2025 study. However, it does discuss energy consumption in various studies and models, and mentions that energy consumption can vary significantly based on model type, model size, and hardware specifications."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context does not provide a specific year for the launch of the One Hundred Year Study on Artificial Intelligence. The study is mentioned as being launched in the fall of 2014, but it is not explicitly stated when the study was first initiated.","is_blank","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context does not provide a specific year for the launch of the One Hundred Year Study on Artificial Intelligence. The study is mentioned as being launched in the fall of 2014, but it is not explicitly stated when the study was first initiated."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context discusses the global e-waste monitor 2024 report, which states that about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The global e-waste monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context discusses the global e-waste monitor 2024 report, which states that about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10]."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information about the energy usage for fine-tuning the Bloomz-7B model, which required 7,571 kWh. However, it does not specify the energy consumption (in MWh) for pre-training the BLOOM model. Therefore, the energy consumption for pre-training the BLOOM model is is_blank.","is_blank","MWh","[""ebert2024""]","is_blank","The energy usage for fine-tuning the Bloomz-7B model is mentioned in [ebert2024].","The context provides information about the energy usage for fine-tuning the Bloomz-7B model, which required 7,571 kWh. However, it does not specify the energy consumption (in MWh) for pre-training the BLOOM model. Therefore, the energy consumption for pre-training the BLOOM model is is_blank."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context provides information about the carbon footprint of various machine learning models, including the Switch Transformer model with 1.5 trillion parameters. It is stated that training the Switch Transformer model produces significantly less carbon emission than that of GPT-3 (750 billion parameters). However, the context does not provide specific information about the percentage of parameters activated per token for the Switch Transformer model.","is_blank","percent","[""wu2021a""]","is_blank","Training the Switch Transformer model equipped with 1.5 trillion parameters [ 29] produces signiÔ¨Åcantly less carbon emission than that of GPT-3 (750 billion parameters) [13].","The context provides information about the carbon footprint of various machine learning models, including the Switch Transformer model with 1.5 trillion parameters. It is stated that training the Switch Transformer model produces significantly less carbon emission than that of GPT-3 (750 billion parameters). However, the context does not provide specific information about the percentage of parameters activated per token for the Switch Transformer model."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The JetMoE-8B architecture is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. However, the context does not provide specific information about the number of experts included in each MoE layer.","is_blank","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts.","The JetMoE-8B architecture is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. However, the context does not provide specific information about the number of experts included in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide explicit information about the execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the maximum batch size supported by the model-dataset combinations is reported in Table III. The table shows that for the CS dataset, the maximum batch size for BlackMamba-D is 6, and for the MATH dataset, it is 2. Since the question asks for the execution time in seconds, we cannot directly convert the batch size to execution time. However, we can infer that the execution time would depend on the number of epochs, the sequence length, and the specifics of the model and GPU. Therefore, the answer is blank.","is_blank","second","[""xia2024""]","is_blank","Table III: Maximum Batch Size Supported by LLM Fine-Tuning; D: Dense and S: Sparse.","The context does not provide explicit information about the execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the maximum batch size supported by the model-dataset combinations is reported in Table III. The table shows that for the CS dataset, the maximum batch size for BlackMamba-D is 6, and for the MATH dataset, it is 2. Since the question asks for the execution time in seconds, we cannot directly convert the batch size to execution time. However, we can infer that the execution time would depend on the number of epochs, the sequence length, and the specifics of the model and GPU. Therefore, the answer is blank."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese), as stated in the context.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese), as stated in the context."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context does not provide specific information about the hardware processor used in the experimental setup for energy-efficient local inference in financial sentiment classification. However, it does mention the use of NVIDIA GPUs in the experimental setup.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170""]","NVIDIA GPUs were used in the experimental setup.","The context does not provide specific information about the hardware processor used in the experimental setup for energy-efficient local inference in financial sentiment classification. However, it does mention the use of NVIDIA GPUs in the experimental setup."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The study in the provided context compares the energy and carbon emissions of various machine learning models. It is stated that the study measures the deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark datasets using these models. The study finds that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. However, the specific number of models sampled and analyzed is not explicitly mentioned.","is_blank","models","[""luccioni2024"", ""luccioni2025c""]","is_blank","The tasks examined in our study and the average quantity of carbon emissions they produced (in g of ùê∂ùëÇ 2ùëíùëû) for 1,000 queries.","The study in the provided context compares the energy and carbon emissions of various machine learning models. It is stated that the study measures the deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark datasets using these models. The study finds that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. However, the specific number of models sampled and analyzed is not explicitly mentioned."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context provides a table (Table IV) that shows the GPU utilization for 80GB A100 GPUs and LLaMA 65B with 4 shards, a batch size of 64, and varying maximum output lengths. It is stated that increasing the maximum output length from 256 to 2048 results in an average SM utilization of 98% for the A100 GPUs. Since the question asks about the factor by which the carbon footprint decreases when GPU utilization is increased to 80%, and the context shows that a 98% utilization corresponds to a 3x decrease in carbon footprint, the answer is derived.","3","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2106.09656""]","Table IV: A100 Utilization","The context provides a table (Table IV) that shows the GPU utilization for 80GB A100 GPUs and LLaMA 65B with 4 shards, a batch size of 64, and varying maximum output lengths. It is stated that increasing the maximum output length from 256 to 2048 results in an average SM utilization of 98% for the A100 GPUs. Since the question asks about the factor by which the carbon footprint decreases when GPU utilization is increased to 80%, and the context shows that a 98% utilization corresponds to a 3x decrease in carbon footprint, the answer is derived."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context provides information about the total compute for model development being larger than the compute for the final training run alone. It is stated that the total compute for model development is 1.2x to 4x larger than the final training run, based on a log-normal distribution with a 90% CI.","is_blank","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context provides information about the total compute for model development being larger than the compute for the final training run alone. It is stated that the total compute for model development is 1.2x to 4x larger than the final training run, based on a log-normal distribution with a 90% CI."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption for a 6 billion parameter transformer model during an 8-day training run on 256 NVIDIA A100s, which amounts to 13.8 MWh. To estimate the energy consumption for a full training run, the context multiplies the energy used during the partial run by the ratio of the time it would take to train to completion (60 days) over the time it actually took (8 days). This gives us an estimated energy consumption of 103.5 MWh, which is then converted to kWh to match the question's unit.","103500","MWh","[""dodge2022""]","is_blank","The energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ‚àó 13.8 = 103.5 MWh, or 103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!","The context provides the energy consumption for a 6 billion parameter transformer model during an 8-day training run on 256 NVIDIA A100s, which amounts to 13.8 MWh. To estimate the energy consumption for a full training run, the context multiplies the energy used during the partial run by the ratio of the time it would take to train to completion (60 days) over the time it actually took (8 days). This gives us an estimated energy consumption of 103.5 MWh, which is then converted to kWh to match the question's unit."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper argues that the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. It suggests that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. The paper contends that a narrow focus on direct emissions misrepresents AI‚Äôs true climate footprint, limiting the scope for meaningful interventions.","1","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","From Efficiency Gains to Rebound Effects: The Problem of Jevons‚Äô Paradox in AI‚Äôs Polarized Environmental Debate","The paper argues that the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. It suggests that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. The paper contends that a narrow focus on direct emissions misrepresents AI‚Äôs true climate footprint, limiting the scope for meaningful interventions."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a detailed analysis of the water footprint of AI, specifically focusing on the water withdrawal by AI demand in 2027. It is stated that the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom.","4.2 - 6.6","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom.","The context provides a detailed analysis of the water footprint of AI, specifically focusing on the water withdrawal by AI demand in 2027. It is stated that the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context discusses the increasing computational cost of AI research, which is illustrated in Figure 1. It is stated that the computational cost of research is increasing exponentially, at a pace that far exceeds Moore‚Äôs Law. However, it is not mentioned that Red AI is on the decline.","1","is_blank","[""schwartz2019""]","is_blank","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years. Figure taken from [2].","The context discusses the increasing computational cost of AI research, which is illustrated in Figure 1. It is stated that the computational cost of research is increasing exponentially, at a pace that far exceeds Moore‚Äôs Law. However, it is not mentioned that Red AI is on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context provides information about the training time for the FLM-101B model using a growth strategy. It is stated that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context provides information about the training time for the FLM-101B model using a growth strategy. It is stated that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context discusses the utilization of GPU accelerators in Facebook's research training infrastructure. It is stated that a significant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving room for improvements to efficiency and overall utilization.","30-50%","is_blank","[""wu2021a""]","is_blank","A signiÔ¨Åcant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving signiÔ¨Åcant room for improvements to efÔ¨Åciency and overall utilization.","The context discusses the utilization of GPU accelerators in Facebook's research training infrastructure. It is stated that a significant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving room for improvements to efficiency and overall utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context provides an estimate of the operational water consumption footprint for GPT-3, which includes both on-site and off-site water consumption. The water consumption for training is not explicitly stated, but it can be inferred from the total water consumption and the fact that training is one of the operational phases mentioned.","is_blank","MWh","[""li2025b""]","[""https://www.researchgate.net/publication/363780280_Estimating_GPT-3_s_Operational_Water_Consumption_Footprint""]","Table 1: Estimate of GPT-3‚Äôs operational water consumption footprint","The context provides an estimate of the operational water consumption footprint for GPT-3, which includes both on-site and off-site water consumption. The water consumption for training is not explicitly stated, but it can be inferred from the total water consumption and the fact that training is one of the operational phases mentioned."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The AI Energy Score project is mentioned as a standardized methodology for comparing the inference efficiency of various AI models.","is_blank","is_blank","[""luccioni2025c""]","is_blank","For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The AI Energy Score project is mentioned as a standardized methodology for comparing the inference efficiency of various AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not provide specific information about the execution time of the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the MoE layer is the most time-consuming layer, accounting for 85% of the overall execution time on average. This suggests that the longest kernel of the MoE layer would likely have a high execution time, but the exact microseconds are not provided.","is_blank","microseconds","[""xia2024""]","is_blank","Fig. 5 and the text describing the execution time breakdown in terms of different model layers.","The context does not provide specific information about the execution time of the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the MoE layer is the most time-consuming layer, accounting for 85% of the overall execution time on average. This suggests that the longest kernel of the MoE layer would likely have a high execution time, but the exact microseconds are not provided."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context explains that JetMoE-8B is a language model with 8B parameters, and it mentions that the model only activates 2B for each input token. This implies that for each token, two out of eight experts are activated, which means that 2B experts are activated for each token.","2","experts","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explains that JetMoE-8B is a language model with 8B parameters, and it mentions that the model only activates 2B for each input token. This implies that for each token, two out of eight experts are activated, which means that 2B experts are activated for each token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context discusses the exponential growth in the amount of compute used to train deep learning models over a six-year span (2012 - 2018). It is stated that the amount of compute used to train deep learning models has increased 300,000x in 6 years, according to Figure 1 in the Schwartz et al. paper.","1","is_blank","[""schwartz2019""]","is_blank","Figure 1 in the Schwartz et al. paper","The context discusses the exponential growth in the amount of compute used to train deep learning models over a six-year span (2012 - 2018). It is stated that the amount of compute used to train deep learning models has increased 300,000x in 6 years, according to Figure 1 in the Schwartz et al. paper."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context mentions that global carbon emissions dropped by 6.4% in 2020, and it is stated that vehicle transportation in the US accounted for a portion of the global reduction. However, the exact percentage of the reduction attributed to vehicle transportation in the US is not specified.","is_blank","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context mentions that global carbon emissions dropped by 6.4% in 2020, and it is stated that vehicle transportation in the US accounted for a portion of the global reduction. However, the exact percentage of the reduction attributed to vehicle transportation in the US is not specified."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context provides information about Amazon's renewable energy projects, including the number of AWS data center regions that are matched with 100% renewable energy sources. However, the specific number of regions that consume 100% renewable energy and match that consumption with renewable energy sources in a particular year is not explicitly stated.","is_blank","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy section","The context provides information about Amazon's renewable energy projects, including the number of AWS data center regions that are matched with 100% renewable energy sources. However, the specific number of regions that consume 100% renewable energy and match that consumption with renewable energy sources in a particular year is not explicitly stated."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context does not provide a direct answer to the question about the age of AI in 2025. However, it does mention that the field of AI was officially born and christened at a 1956 workshop, and that AI is a continual endeavor to push forward the frontier of machine intelligence. From this, it can be inferred that AI has been around for at least 65 years (from 1956 to 2025), but the age of AI in 2025 is not explicitly stated.","is_blank","years","[""stone2022""]","[""https://www.researchgate.net/publication/339618021_Artificial_Intelligence_and_Life_in_2030_One_Hundred_Year_Study_on_Artificial_Intelligence_Report_of_the_2015_Study_Panel_September_2016""]","The field of AI was officially born and christened at a 1956 workshop.","The context does not provide a direct answer to the question about the age of AI in 2025. However, it does mention that the field of AI was officially born and christened at a 1956 workshop, and that AI is a continual endeavor to push forward the frontier of machine intelligence. From this, it can be inferred that AI has been around for at least 65 years (from 1956 to 2025), but the age of AI in 2025 is not explicitly stated."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context discusses a 2019 study by Strubell et al. which estimated the carbon footprint of training AI models. The study included the 'five cars' estimate, which is a misinterpretation of the carbon footprint of training AI models at large. The estimate was based on a NAS (Neural Architecture Search) approach, which is a large-scale procedure that is not representative of the average AI model training workload.","is_blank","is_blank","[""luccioni2025c""]","is_blank","The 'five cars' estimate is based on the NAS (Neural Architecture Search) approach, which is a large-scale procedure that is not representative of the average AI model training workload.","The context discusses a 2019 study by Strubell et al. which estimated the carbon footprint of training AI models. The study included the 'five cars' estimate, which is a misinterpretation of the carbon footprint of training AI models at large. The estimate was based on a NAS (Neural Architecture Search) approach, which is a large-scale procedure that is not representative of the average AI model training workload."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context provides energy consumption measurements for various models, including Llama 3.1 8B. It is stated that when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency, energy use decreases by 44% for Llama 3.1 8B. This information is found in the '4.3 Automated Energy Optimization Recommendation' section.","44%","percent","[""chung2025""]","is_blank","In this context, a chatbot provider can target an average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]), which is sufficient for most reading or listening speeds. This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context provides energy consumption measurements for various models, including Llama 3.1 8B. It is stated that when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency, energy use decreases by 44% for Llama 3.1 8B. This information is found in the '4.3 Automated Energy Optimization Recommendation' section."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context does not provide a clear answer to the question about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory. However, it does mention that the maximum batch size is affected by GPU memory size, model size, sequence length, and MoE sparsity. It also mentions that the maximum batch size for both Mixtral and BlackMamba on NVIDIA A40 GPU with 48GB memory is reported in Table III, but the table does not specify the batch size for BlackMamba with a sparse setup on the GSM8K dataset.","is_blank","samples","[""xia2024""]","is_blank","Table III","The context does not provide a clear answer to the question about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory. However, it does mention that the maximum batch size is affected by GPU memory size, model size, sequence length, and MoE sparsity. It also mentions that the maximum batch size for both Mixtral and BlackMamba on NVIDIA A40 GPU with 48GB memory is reported in Table III, but the table does not specify the batch size for BlackMamba with a sparse setup on the GSM8K dataset."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context provides information about Amazon's renewable energy investments in 2023, including utility-scale wind and solar projects, solar rooftops at their facilities and stores, and the total capacity of these projects. However, the context does not specify the exact energy storage capacity in gigawatts (GW) that Amazon held as of 2023.","is_blank","GW","[""amazon2023""]","is_blank","At the end of 2023, Amazon had invested in 513 global renewable energy projects, including 243 utility-scale wind and solar projects, and 270¬†solar rooftops at our facilities and stores around the world.","The context provides information about Amazon's renewable energy investments in 2023, including utility-scale wind and solar projects, solar rooftops at their facilities and stores, and the total capacity of these projects. However, the context does not specify the exact energy storage capacity in gigawatts (GW) that Amazon held as of 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context provides information about the estimated annual carbon emissions of GPT-4o inference, which are projected to be comparable to the emissions from thousands of transatlantic flights. Specifically, it mentions that GPT-4o inference would require approximately 391,509 MWh annually at minimum, which is more than the total electricity consumption of 35,000 U.S. residential households. This is comparable to the emissions from approximately 272 transatlantic flights between Boston and London.","is_blank","flights","[""jegham2025""]","is_blank","GPT-4o inference would require approximately 391,509 MWh annually at minimum","The context provides information about the estimated annual carbon emissions of GPT-4o inference, which are projected to be comparable to the emissions from thousands of transatlantic flights. Specifically, it mentions that GPT-4o inference would require approximately 391,509 MWh annually at minimum, which is more than the total electricity consumption of 35,000 U.S. residential households. This is comparable to the emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific information about AI inference workloads in Asia and their energy source in 2023. However, it does mention that AI workloads constitute a significant portion of energy consumption for companies like Meta and Google, and that data centers in general are projected to consume a growing percentage of the total US energy demand by 2030. This suggests that a large portion of this energy is likely to be powered by non-renewable sources, but it does not specify the geographical location of these data centers.","is_blank","percent","[""fernandez2025"", ""morrison2025"", ""luccioni2025a"", ""li2025b"", ""luccioni2024""]","is_blank","is_blank","The context does not provide specific information about AI inference workloads in Asia and their energy source in 2023. However, it does mention that AI workloads constitute a significant portion of energy consumption for companies like Meta and Google, and that data centers in general are projected to consume a growing percentage of the total US energy demand by 2030. This suggests that a large portion of this energy is likely to be powered by non-renewable sources, but it does not specify the geographical location of these data centers."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. However, it does mention the average PUE of data centers globally in 2023 was 1.58, but it does not specify if these are AI-dedicated data centers or not.","is_blank","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The power usage effectiveness (PUE) of a data center is mentioned, but it is not specified if this is for AI-dedicated data centers.","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. However, it does mention the average PUE of data centers globally in 2023 was 1.58, but it does not specify if these are AI-dedicated data centers or not."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","2B","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table (ref_id=luccioni2025c) that lists the GHG emissions (in tCO2e) for various models, including Llama 7B. The emissions for Llama 7B are listed as 14 (in thousands of tons).","14","tCO2e","[""luccioni2025c""]","is_blank","Range of Pre-Training Environmental Impacts (Representative Models Displayed)
Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context provides a table (ref_id=luccioni2025c) that lists the GHG emissions (in tCO2e) for various models, including Llama 7B. The emissions for Llama 7B are listed as 14 (in thousands of tons)."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides information about the number of electric delivery vans added by Amazon in 2022 and 2023. However, it does not specify the total number of electric delivery vans added across both years.","is_blank","electric delivery vans","[""amazon2023""]","is_blank","In the provided context, the number of electric delivery vans added in 2022 is 2,600 and in 2023 is 11,800. However, there is no information about the total number of electric delivery vans added across both years.","The context provides information about the number of electric delivery vans added by Amazon in 2022 and 2023. However, it does not specify the total number of electric delivery vans added across both years."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context discusses the operational environmental impacts of LLMs, which include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. The context specifically mentions that these emissions are included in the operational impacts of LLMs.","1","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context discusses the operational environmental impacts of LLMs, which include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. The context specifically mentions that these emissions are included in the operational impacts of LLMs."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for various models, including GPT-4o, LLaMA-3, and DeepSeek. However, the specific energy consumption for a long prompt for the o3 model is not explicitly stated. The closest we can get is the energy consumption for the 10k input-1.5k output prompt size for the o3 model, which is 12.222 Wh. Since the question asks for energy consumption for a long prompt, and the given value is the closest match, we can assume that the energy consumption for a long prompt for the o3 model is approximately 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ¬± std dev) per model across three prompt sizes (Wh).","The context provides energy consumption values for various models, including GPT-4o, LLaMA-3, and DeepSeek. However, the specific energy consumption for a long prompt for the o3 model is not explicitly stated. The closest we can get is the energy consumption for the 10k input-1.5k output prompt size for the o3 model, which is 12.222 Wh. Since the question asks for energy consumption for a long prompt, and the given value is the closest match, we can assume that the energy consumption for a long prompt for the o3 model is approximately 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context provides a detailed history of the field of Artificial Intelligence (AI), including the 1956 Dartmouth Summer Research Project on AI, which is identified as the event where the field was officially christened. The proposal for this project, co-authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, is specifically mentioned as the document where the term 'artificial intelligence' was first used.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","J. McCarthy , Marvin L. Minsky , Nathaniel Rochester, and Claude E. Shannon, ‚Äò A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence,‚Äô August 31, 1955","The context provides a detailed history of the field of Artificial Intelligence (AI), including the 1956 Dartmouth Summer Research Project on AI, which is identified as the event where the field was officially christened. The proposal for this project, co-authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, is specifically mentioned as the document where the term 'artificial intelligence' was first used."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context provides information about the water consumption of different models, including GPT-3, in various data centers. However, the specific water consumption for Google's AI-dedicated data centers in 2024 is not mentioned.","is_blank","WUE","[""li2025b""]","is_blank","is_blank","The context provides information about the water consumption of different models, including GPT-3, in various data centers. However, the specific water consumption for Google's AI-dedicated data centers in 2024 is not mentioned."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context discusses the need for technology to be economically accessible to most of the world's population. It mentions that even a basic cell phone is prohibitively expensive for much of the world, and that making the internet available to everyone will not solve the problem if the devices commonly used to access the internet are a luxury item for the world's poor.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","According to the Alliance for Affordable Internet, nearly 2.5 billion people live in countries where a basic cell phone would cost nearly a quarter or more of the average monthly income.","The context discusses the need for technology to be economically accessible to most of the world's population. It mentions that even a basic cell phone is prohibitively expensive for much of the world, and that making the internet available to everyone will not solve the problem if the devices commonly used to access the internet are a luxury item for the world's poor."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides energy per token estimates for LLaMA 65B at a maximum generation length of 512 tokens. It is stated that with a length of 512, it takes about 3-4 Joules for a output token. Therefore, the energy per token can be calculated by dividing the total energy by the number of output tokens, which is 3-4 Joules per token.","3-4","joules per token","[""samsi2024""]","is_blank","Figures 6 and 7","The context provides energy per token estimates for LLaMA 65B at a maximum generation length of 512 tokens. It is stated that with a length of 512, it takes about 3-4 Joules for a output token. Therefore, the energy per token can be calculated by dividing the total energy by the number of output tokens, which is 3-4 Joules per token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit Ô¨Çoating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit Ô¨Çoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit Ô¨Çoating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context provides information about a claim that AI can reduce global GHG emissions by 5-10%. However, it is noted that this claim is based on a 2021 Boston Consulting Group (BCG) report, and the reasoning behind the 5-10% reduction estimate is unclear. The underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes. The context also mentions that a more recent BCG report from 2023, which was commissioned by Google, provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.","is_blank","is_blank","[""luccioni2025c""]","is_blank","The claim can be traced back to a 2021 Boston Consulting Group (BCG) report which states that ‚ÄòResearch shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030‚Äìthe equivalent of the total annual emissions of the European Union‚Äô48.","The context provides information about a claim that AI can reduce global GHG emissions by 5-10%. However, it is noted that this claim is based on a 2021 Boston Consulting Group (BCG) report, and the reasoning behind the 5-10% reduction estimate is unclear. The underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes. The context also mentions that a more recent BCG report from 2023, which was commissioned by Google, provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context does not provide a clear answer to the question about the energy consumption of sparsely activated DNNs compared to large, dense DNNs. However, it does mention that sparsely activated models can consume less energy than large, dense DNNs without sacrificing accuracy (Patterson 2021).","1","is_blank","[""patterson2021""]","[""https://arxiv.org/abs/2105.10052""]","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context does not provide a clear answer to the question about the energy consumption of sparsely activated DNNs compared to large, dense DNNs. However, it does mention that sparsely activated models can consume less energy than large, dense DNNs without sacrificing accuracy (Patterson 2021)."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context provides the cost of training Grover on 256 TPU chips for two weeks as $25,000. To estimate the cost for 256 TPU chips for two weeks, we multiply the cost per TPU chip by the number of TPU chips and the number of weeks. Since the cost per TPU chip is given as $25,000 for two weeks, we can infer that the cost for 256 TPU chips for two weeks is 256 * 2 * $25,000 = $1,500,000.","1500000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context provides the cost of training Grover on 256 TPU chips for two weeks as $25,000. To estimate the cost for 256 TPU chips for two weeks, we multiply the cost per TPU chip by the number of TPU chips and the number of weeks. Since the cost per TPU chip is given as $25,000 for two weeks, we can infer that the cost for 256 TPU chips for two weeks is 256 * 2 * $25,000 = $1,500,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context provides information about the execution time breakdown for fine-tuning BlackMamba and Mixtral models. It is stated that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time when conducting sparse fine-tuning with a batch size = 1, and the optimizer stage in Mixtral fine-tuning is negligible. However, the specific percentage of the running time for the optimizer stage in BlackMamba sparse fine-tuning is not explicitly mentioned.","is_blank","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context provides information about the execution time breakdown for fine-tuning BlackMamba and Mixtral models. It is stated that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time when conducting sparse fine-tuning with a batch size = 1, and the optimizer stage in Mixtral fine-tuning is negligible. However, the specific percentage of the running time for the optimizer stage in BlackMamba sparse fine-tuning is not explicitly mentioned."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides information about the energy consumption of various ML models, including a large-scale service like Google Translate. It is stated that Google Translate can perform 1 billion queries per day, and a comparable multi-purpose model (Flan-T5-xxl) consumes 0.083 kWh/1k queries. To estimate the daily energy consumption in MWh, we multiply the number of queries by the energy consumption per query and convert the units.","83","MWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3530418.3530424""]","According to the context, Google Translate can perform 1 billion queries per day, and a comparable multi-purpose model consumes 0.083 kWh/1k queries. Therefore, the daily energy consumption of a large-scale service like Google Translate is approximately 83 MWh.","The context provides information about the energy consumption of various ML models, including a large-scale service like Google Translate. It is stated that Google Translate can perform 1 billion queries per day, and a comparable multi-purpose model (Flan-T5-xxl) consumes 0.083 kWh/1k queries. To estimate the daily energy consumption in MWh, we multiply the number of queries by the energy consumption per query and convert the units."
"q067","What was the average global data center PUE in 2023?","The context provides information about the average global data center PUE in 2023, which was 1.58 according to the document 'Conference‚Äô17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker'.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","Average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide.","The context provides information about the average global data center PUE in 2023, which was 1.58 according to the document 'Conference‚Äô17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker'."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide specific information about the number of wind turbines contracted by Microsoft to power Azure AI clusters in 2023. However, it does mention that Amazon invested in renewable energy projects in various countries, including wind farms, which suggests that similar investments by Microsoft could potentially involve wind turbines.","is_blank","wind turbines","[""luccioni2025a"", ""amazon2023""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://sustainability.aboutamazon.com/environment/renewable-energy""]","The context mentions Amazon's investments in renewable energy projects, but does not provide specific information about Microsoft's investments.","The context does not provide specific information about the number of wind turbines contracted by Microsoft to power Azure AI clusters in 2023. However, it does mention that Amazon invested in renewable energy projects in various countries, including wind farms, which suggests that similar investments by Microsoft could potentially involve wind turbines."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a breakdown of the total amortized model development cost for several models, including Gemini Ultra. It is stated that R&D staff costs make up between 29% and 49% of the total cost, depending on the model. Gemini Ultra has the highest fraction of R&D staff cost at 49%.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context provides a breakdown of the total amortized model development cost for several models, including Gemini Ultra. It is stated that R&D staff costs make up between 29% and 49% of the total cost, depending on the model. Gemini Ultra has the highest fraction of R&D staff cost at 49%."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context provides information about the inaugural 2015 Study Panel of the One Hundred Year Study on AI, which was comprised of 17 members. The members were experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics. The members were launched in mid-fall 2015.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context provides information about the inaugural 2015 Study Panel of the One Hundred Year Study on AI, which was comprised of 17 members. The members were experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics. The members were launched in mid-fall 2015."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context discusses the carbon footprint of various system hardware choices for AI, including general-purpose processors, general-purpose accelerators, field-programmable gate arrays, and application-specific integrated circuits. It is mentioned that the embodied carbon footprint associated with these hardware choices can have an impact on the overall carbon footprint of AI computing. However, the context does not provide a specific percentage for the manufacturing carbon footprint of a client device's total carbon footprint.","is_blank","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","From the environmental sustainability perspective, the optimal point depends on the compounding factor of operational efÔ¨Åciency improvement over generations of ML algorithms/models, deployment lifetime and embodied carbon footprint of the system hardware.","The context discusses the carbon footprint of various system hardware choices for AI, including general-purpose processors, general-purpose accelerators, field-programmable gate arrays, and application-specific integrated circuits. It is mentioned that the embodied carbon footprint associated with these hardware choices can have an impact on the overall carbon footprint of AI computing. However, the context does not provide a specific percentage for the manufacturing carbon footprint of a client device's total carbon footprint."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not directly address the question about energy consumption during inference for models with more parameters. However, it does mention that energy consumption can vary based on factors such as model architecture, batch size, and compute-intensity of operations (ref_id=chung2025). It is not clear from the context whether models with more parameters necessarily consume more energy during inference.","is_blank","is_blank","[""chung2025""]","[""https://arxiv.org/abs/2505.06371""]","The context provides information about energy consumption in the context of the ML.ENERGY Benchmark, but does not directly address the question about energy consumption for models with more parameters.","The context does not directly address the question about energy consumption during inference for models with more parameters. However, it does mention that energy consumption can vary based on factors such as model architecture, batch size, and compute-intensity of operations (ref_id=chung2025). It is not clear from the context whether models with more parameters necessarily consume more energy during inference."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context provides detailed information about the environmental impact of training various language models, including carbon emissions, water consumption, and electricity consumption. However, the context does not specify the carbon emissions for OpenAI's API requests in January 2024.","is_blank","tCO2e","[""morrison2025""]","is_blank","is_blank","The context provides detailed information about the environmental impact of training various language models, including carbon emissions, water consumption, and electricity consumption. However, the context does not specify the carbon emissions for OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides information about the estimated GHG emissions from training AI models, specifically language models. It mentions that the emissions from training Meta's Llama 3 family of models exceed the 'five cars' estimate, with Llama 3 emitting 11,390 tons CO2e, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","The emissions from training Meta's Llama 3 family of models exceed the ‚Äòfive cars‚Äô estimate, with Llama 3 emitting 11,390 tons CO2e, or over 40x the ‚Äòfive cars‚Äô estimate.","The context provides information about the estimated GHG emissions from training AI models, specifically language models. It mentions that the emissions from training Meta's Llama 3 family of models exceed the 'five cars' estimate, with Llama 3 emitting 11,390 tons CO2e, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context provides a figure (Figure 2(d)) that illustrates that the explosive growth in AI use cases at Facebook has driven a 2.9√ó increase in AI training infrastructure capacity over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021.","2.9","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2105.08546""]","Figure 2(d)","The context provides a figure (Figure 2(d)) that illustrates that the explosive growth in AI use cases at Facebook has driven a 2.9√ó increase in AI training infrastructure capacity over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","The context does not provide information about the distance between the Earth and the Sun. However, it does provide information about the distance between Los Angeles, California, and New York City, New York, which is 5,580 miles.","is_blank","miles","[""han2024""]","is_blank","is_blank","The context does not provide information about the distance between the Earth and the Sun. However, it does provide information about the distance between Los Angeles, California, and New York City, New York, which is 5,580 miles."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context discusses various sub-areas of AI, including Machine Learning, which is described as a paradigm that enables systems to automatically improve their performance at a task by observing relevant data. The AlphaGo program, which defeated the human Go champion, is mentioned as an example of a system that used machine learning. However, the context does not explicitly state whether AlphaGo was a machine learning program before or after it defeated the human champion.","1","is_blank","[""stone2022""]","is_blank","The AlphaGo program160 161 that recently defeated the current human champion at the game of  Go used multiple  machine learning algorithms for training itself, and also used a sophisticated search  procedure while playing the game.","The context discusses various sub-areas of AI, including Machine Learning, which is described as a paradigm that enables systems to automatically improve their performance at a task by observing relevant data. The AlphaGo program, which defeated the human Go champion, is mentioned as an example of a system that used machine learning. However, the context does not explicitly state whether AlphaGo was a machine learning program before or after it defeated the human champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context discusses various optimizations for language model inference, including continuous batching, which dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time. Continuous batching is mentioned as a strategy that mitigates GPU under-utilization during generation.","is_blank","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed re-quests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context discusses various optimizations for language model inference, including continuous batching, which dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time. Continuous batching is mentioned as a strategy that mitigates GPU under-utilization during generation."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning, is mentioned to take 60 H100 GPU hours in the context.","60","H100 GPU hours","[""shen2024""]","is_blank","This fine-tuning process takes 60 H100 GPU hours.","The entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning, is mentioned to take 60 H100 GPU hours in the context."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The table (Table VI) in the context provides the comparison of instance selection results by SLO constraints for both 100 TPS and 200 TPS workloads. The Max-Performance policy selected the g6e.xlarge instance, which is 280% more expensive than the instance selected by InferSave for the 100 TPS SLO.","280","percent","[""kim2025""]","is_blank","Table VI","The table (Table VI) in the context provides the comparison of instance selection results by SLO constraints for both 100 TPS and 200 TPS workloads. The Max-Performance policy selected the g6e.xlarge instance, which is 280% more expensive than the instance selected by InferSave for the 100 TPS SLO."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The most carbon-intensive model identified in the 2024 study, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This information is provided in the context snippet.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51].","The most carbon-intensive model identified in the 2024 study, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This information is provided in the context snippet."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The energy consumption for performing 1,000 inference queries for various models is provided in the study's appendix (rubei2025). However, the specific energy consumption for each model for 1,000 queries is not explicitly stated in the provided context. To find this information, one would need to refer to the appendix of the study. Therefore, the energy consumption for each model is is_blank.","is_blank","Wh","[""rubei2025""]","is_blank","is_blank","The energy consumption for performing 1,000 inference queries for various models is provided in the study's appendix (rubei2025). However, the specific energy consumption for each model for 1,000 queries is not explicitly stated in the provided context. To find this information, one would need to refer to the appendix of the study. Therefore, the energy consumption for each model is is_blank."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context discusses the importance of integrating AI ethics and sustainability to ensure a holistic societal impact. It mentions that the absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is necessary to maximize the potential of AI advancements to engender sustainable development.","1","is_blank","[""luccioni2025b""]","is_blank","Therefore, inte grating AI ethics with sustainability is not just beneÔ¨Åcial but necessary, ensuring that AI advancements are not only te chnologically innovative and ethically sound but also maximizing their potential to engender sustainable advanc ement.","The context discusses the importance of integrating AI ethics and sustainability to ensure a holistic societal impact. It mentions that the absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is necessary to maximize the potential of AI advancements to engender sustainable development."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context provides the average carbon intensity of energy according to the U.S. average mix in 2021, which is 0.429 kg of CO2e/KWh. This information is found in the context snippet with the reference id 'patterson2021'.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [patterson2021].","The context provides the average carbon intensity of energy according to the U.S. average mix in 2021, which is 0.429 kg of CO2e/KWh. This information is found in the context snippet with the reference id 'patterson2021'."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context discusses a PyTorch-based framework called Hivemind, which is designed for decentralized, data-parallel training. It mentions that Hivemind can handle peers that drop out at any stage of the training and reduces time and frequency of communication rounds, making it possible to train on heterogeneous devices and low-bandwidth networks. However, the context does not explicitly state that Hivemind was used for geo-distributed spot training across clouds and continents.","is_blank","is_blank","[""erben2023""]","is_blank","The context discusses the Hivemind framework, its features, and its ability to handle peers that drop out at any stage of the training.","The context discusses a PyTorch-based framework called Hivemind, which is designed for decentralized, data-parallel training. It mentions that Hivemind can handle peers that drop out at any stage of the training and reduces time and frequency of communication rounds, making it possible to train on heterogeneous devices and low-bandwidth networks. However, the context does not explicitly state that Hivemind was used for geo-distributed spot training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context discusses the need to expand transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. It provides examples of how this expanded transparency can be achieved, such as an AI system designed for urban planning that includes information on the environmental footprint of running the AI system and potential environmental benefits of the proposed urban layouts.","Sustainable Transparency","is_blank","[""luccioni2025b""]","is_blank","In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have and making more informed decisions based on the inf ormation at our disposal [51].","The context discusses the need to expand transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. It provides examples of how this expanded transparency can be achieved, such as an AI system designed for urban planning that includes information on the environmental footprint of running the AI system and potential environmental benefits of the proposed urban layouts."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context discusses various models for text classification, including linear models with sentence embeddings. It is mentioned that for emotion classification, the linear model with sentence embeddings is among the top-performing models. No specific accuracy value is provided for this model, but it is compared to other models and found to be among the top-performing ones.","1","is_blank","[""zschache2025""]","is_blank","Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","The context discusses various models for text classification, including linear models with sentence embeddings. It is mentioned that for emotion classification, the linear model with sentence embeddings is among the top-performing models. No specific accuracy value is provided for this model, but it is compared to other models and found to be among the top-performing ones."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The paper 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation' by Chen et al. introduces a system called Lamina, which is an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. The system is developed and deployed to validate the theory presented in the paper.","is_blank","is_blank","[""chen2024""]","is_blank","Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation","The paper 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation' by Chen et al. introduces a system called Lamina, which is an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. The system is developed and deployed to validate the theory presented in the paper."
"q093","How many parameters does the largest T5 model have?","The context provides information about the environmental impact of training various transformer language models, including their water consumption. It is stated that the OLMo series of transformer language models, which range in size from 20 million to 13 billion active parameters, were trained on 1.7 to 5.6 trillion tokens. The water consumption for these models is reported in a table, where it is shown that the OLMo 1B model was trained on 3 trillion tokens and had a water consumption of 103 kL.","103","parameters","[""morrison2025""]","is_blank","OLMo 1B was trained on 3 trillion, OLMo 1B was trained on 3 trillion, OLMo 1B was trained on 3 trillion.","The context provides information about the environmental impact of training various transformer language models, including their water consumption. It is stated that the OLMo series of transformer language models, which range in size from 20 million to 13 billion active parameters, were trained on 1.7 to 5.6 trillion tokens. The water consumption for these models is reported in a table, where it is shown that the OLMo 1B model was trained on 3 trillion tokens and had a water consumption of 103 kL."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in the JetMoE-8B model is provided in the context snippet [ref_id=shen2024] under the 'Ptotal' column in Table 1, which shows the JetMoE-8B hyperparameters.","8B","parameters","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters.","The total number of parameters in the JetMoE-8B model is provided in the context snippet [ref_id=shen2024] under the 'Ptotal' column in Table 1, which shows the JetMoE-8B hyperparameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context provides information about the water consumption of a technology company's data centers, stating that the company's data center water consumption increased by approximately 20% from 2021 to 2022. This increase is mentioned in the context snippet [ref_id=li2025b].","20","percent","[""li2025b""]","is_blank","Increased by‚àº20% from 2021 to 2022 and by ‚àº17% from 2022 to 2023 [4]","The context provides information about the water consumption of a technology company's data centers, stating that the company's data center water consumption increased by approximately 20% from 2021 to 2022. This increase is mentioned in the context snippet [ref_id=li2025b]."
"q096","What is the name of the emissions metric defined as 'CO‚ÇÇ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context does not explicitly define a metric for 'CO‚ÇÇ emissions per unit of electricity consumed'. However, it does mention several metrics such as average power draw, throughput per Watt, monetary cost, and operational carbon emissions. These metrics can be derived from the energy consumption, but they are not the same as the specific metric requested.","is_blank","is_blank","[""chung2025""]","is_blank","is_blank","The context does not explicitly define a metric for 'CO‚ÇÇ emissions per unit of electricity consumed'. However, it does mention several metrics such as average power draw, throughput per Watt, monetary cost, and operational carbon emissions. These metrics can be derived from the energy consumption, but they are not the same as the specific metric requested."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The FLOPs utilization percentage for the final growth stage of the FLM-101B training is provided in Table 2, which shows the parallel strategies and throughput for different growth stages. For the 101B stage, the FLOPs utilization is 52.88%.","52.88%","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages.","The FLOPs utilization percentage for the final growth stage of the FLM-101B training is provided in Table 2, which shows the parallel strategies and throughput for different growth stages. For the 101B stage, the FLOPs utilization is 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context discusses the cost of training various AI models, including OpenAI's GPT-4. It is mentioned that the estimated cost of training GPT-4 exceeds $100 million, but no specific amount is given for the amortized training costs.","is_blank","USD","[""xia2024""]","is_blank","For instance, the estimated cost of training OpenAI‚Äôs GPT-4 model exceeds $100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community [2].","The context discusses the cost of training various AI models, including OpenAI's GPT-4. It is mentioned that the estimated cost of training GPT-4 exceeds $100 million, but no specific amount is given for the amortized training costs."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context discusses the reduction in operational carbon footprint of a Transformer-based universal translation model through full-stack optimization, which includes platform-level caching, GPU acceleration, and algorithmic changes. The reduction factor is stated to be 810x in the context.","810","multiplier","[""wu2021a""]","is_blank","EfÔ¨Åciency Optimization: Optimization across the axes of algorithms, platforms, infrastructures, hardware can signiÔ¨Åcantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810√ó.","The context discusses the reduction in operational carbon footprint of a Transformer-based universal translation model through full-stack optimization, which includes platform-level caching, GPU acceleration, and algorithmic changes. The reduction factor is stated to be 810x in the context."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information about the multi-cloud performance of CV and NLP tasks when training is spread across four continents instead of remaining local. It is stated that for NLP, the throughput slowdown is 41% when training is done in four continents compared to the fully local experiment.","41","multiplier","[""erben2023""]","is_blank","When comparing the C-8 experiment with two GPUs in four continents to the local A-8 experiments, the throughput slowdown is almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).","The context provides information about the multi-cloud performance of CV and NLP tasks when training is spread across four continents instead of remaining local. It is stated that for NLP, the throughput slowdown is 41% when training is done in four continents compared to the fully local experiment."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context provides information about AWS's water replenishment activities in 2023, stating that AWS's water replenishment portfolio returned 3.5 billion liters of water to local communities. However, the question asks about a specific year, 2023, and the context does not specify how much water was returned to communities in that year.","is_blank","liters","[""amazon2023""]","is_blank","In 2023, AWS‚Äôs water replenishment portfolio returned 3.5 billion liters to local communities.","The context provides information about AWS's water replenishment activities in 2023, stating that AWS's water replenishment portfolio returned 3.5 billion liters of water to local communities. However, the question asks about a specific year, 2023, and the context does not specify how much water was returned to communities in that year."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The paper 'Prompt engineering and its implications on the energy consumption of Large Language Models' by Rubei et al. investigates the impact of prompt engineering techniques on the energy consumption of LLMs during the inference phase, specifically for code completion tasks. The authors use the CodeXGLUE benchmark and the CodeCarbon tool to measure energy consumption. They find that using custom tags can reduce the energy consumption of LLMs.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The paper 'Prompt engineering and its implications on the energy consumption of Large Language Models' by Rubei et al. investigates the impact of prompt engineering techniques on the energy consumption of LLMs during the inference phase, specifically for code completion tasks. The authors use the CodeXGLUE benchmark and the CodeCarbon tool to measure energy consumption. They find that using custom tags can reduce the energy consumption of LLMs."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The 2025 paper mentioned in the context does not provide information about the number of data center GPUs shipped by NVIDIA in the year 2024. However, it is stated that NVIDIA shipped 3.76 million GPUs in 2023, and the text indicates that they shipped more than a million more units in 2024. Therefore, it can be inferred that NVIDIA shipped more than 4.76 million GPUs in 2024.","is_blank","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study.","The 2025 paper mentioned in the context does not provide information about the number of data center GPUs shipped by NVIDIA in the year 2024. However, it is stated that NVIDIA shipped 3.76 million GPUs in 2023, and the text indicates that they shipped more than a million more units in 2024. Therefore, it can be inferred that NVIDIA shipped more than 4.76 million GPUs in 2024."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides a breakdown of the components of amortized hardware CapEx + energy in Figure 5, stating that on average, 44% goes towards AI accelerator chips.","44","percent","[""cottier2024""]","[""https://arxiv.org/abs/2303.14013""]","3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips","The context provides a breakdown of the components of amortized hardware CapEx + energy in Figure 5, stating that on average, 44% goes towards AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context provides the Power Usage Effectiveness (PUE) of Facebook's data centers, which is stated to be about 1.10. This PUE is compared to small-scale, typical data centers and is mentioned to be 40% more efficient.","1.10","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook‚Äôs data centers are about 40% more efÔ¨Åcient than small-scale, typical data centers.","The context provides the Power Usage Effectiveness (PUE) of Facebook's data centers, which is stated to be about 1.10. This PUE is compared to small-scale, typical data centers and is mentioned to be 40% more efficient."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The provided context discusses various projects and initiatives that aim to integrate ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The Finnish ETAIROS project is mentioned as an example, but no specific acronym is provided for this project.","is_blank","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","The provided context discusses various projects and initiatives that aim to integrate ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The Finnish ETAIROS project is mentioned as an example, but no specific acronym is provided for this project."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context provides detailed cost breakdowns for various AI models, including GPT-3, OPT-175B, GPT-4, and Gemini Ultra. It is mentioned that R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. However, the context does not provide specific cost estimates for Gemini Ultra's training costs.","is_blank","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29‚Äì49% of the total.","The context provides detailed cost breakdowns for various AI models, including GPT-3, OPT-175B, GPT-4, and Gemini Ultra. It is mentioned that R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. However, the context does not provide specific cost estimates for Gemini Ultra's training costs."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","","","is_blank","[]","is_blank","is_blank",""
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m¬≥. The question asks for the annual average limit of PM2.5, which is the same as the standard set by the EPA.","9","¬µg/m¬≥","[""han2024""]","is_blank","For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9¬µg/m 3, both counted over three years [48].","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m¬≥. The question asks for the annual average limit of PM2.5, which is the same as the standard set by the EPA."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context provides a comparison of the environmental impact of training various models, including Llama 2 7B, which is found to consume equivalent water to the average person in the US for about 17 years. The question asks for the water consumption of one Llama 2 7B model, which is equivalent to the water consumption of 17 years of an average US person.","17","books","[""luccioni2025a""]","is_blank","Llama 2 7B - 162 kL - Equiv. to... (water usage, 1 person, U.S.)","The context provides a comparison of the environmental impact of training various models, including Llama 2 7B, which is found to consume equivalent water to the average person in the US for about 17 years. The question asks for the water consumption of one Llama 2 7B model, which is equivalent to the water consumption of 17 years of an average US person."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context discusses the public health impact of data centers, specifically focusing on the emissions of criteria air pollutants such as fine particulate matter (PM2.5) that can significantly degrade air quality and impact public health. It is mentioned that the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","is_blank","multiplier","[""han2024""]","is_blank","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","The context discusses the public health impact of data centers, specifically focusing on the emissions of criteria air pollutants such as fine particulate matter (PM2.5) that can significantly degrade air quality and impact public health. It is mentioned that the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is not explicitly stated in the provided context. However, the context does mention that energy consumption is a metric considered in the study, and it is defined as the energy consumed during the inference phase of Llama excluding the model loading. It is also mentioned that energy consumption is calculated by CodeCarbon, but no specific values are given for the DS Llama 70B model on the FKTG dataset.","is_blank","Wh","[""rubei2025""]","is_blank","Energy Consumption: This metric quantifies the energy consumed during the inference phase of Llama excluding the model loading.","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is not explicitly stated in the provided context. However, the context does mention that energy consumption is a metric considered in the study, and it is defined as the energy consumed during the inference phase of Llama excluding the model loading. It is also mentioned that energy consumption is calculated by CodeCarbon, but no specific values are given for the DS Llama 70B model on the FKTG dataset."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context provides information about the number of parameters in various large language models, including Google's BERT-large, openAI's openGPT2-XL model, and AI2's Grover, all of which are stated to contain 350 million, 1.5 billion, and 1.5 billion parameters respectively. However, the specific paper by Dodge et al. is not mentioned in the context, so we cannot confirm the total number of parameters in the large language model they analyzed.","is_blank","parameters","[""schwartz2019""]","is_blank","is_blank","The context provides information about the number of parameters in various large language models, including Google's BERT-large, openAI's openGPT2-XL model, and AI2's Grover, all of which are stated to contain 350 million, 1.5 billion, and 1.5 billion parameters respectively. However, the specific paper by Dodge et al. is not mentioned in the context, so we cannot confirm the total number of parameters in the large language model they analyzed."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context discusses the concept of Jevons' Paradox, which states that when a resource's efficiency increases, overall consumption of that resource tends to increase rather than decrease. It provides examples of this phenomenon in the context of AI, such as the decrease in cost of training models leading to an increase in their deployment and total usage.","1","is_blank","[""luccioni2025a"", ""jegham2025""]","is_blank","From Efficiency Gains to Rebound Effects FAccT ‚Äô25, June 23‚Äì26, 2025, Athens, Greece; As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76].","The context discusses the concept of Jevons' Paradox, which states that when a resource's efficiency increases, overall consumption of that resource tends to increase rather than decrease. It provides examples of this phenomenon in the context of AI, such as the decrease in cost of training models leading to an increase in their deployment and total usage."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides information about the operational water consumption footprint of various locations for GPT-3, but it does not specify the water consumption for Meena training runs. However, since GPT-3 and Meena are both large language models, it can be inferred that Meena training runs would likely consume a similar amount of water as GPT-3, given the similarities in their training processes.","is_blank","multiplier","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3‚Äôs operational water consumption footprint.","The context provides information about the operational water consumption footprint of various locations for GPT-3, but it does not specify the water consumption for Meena training runs. However, since GPT-3 and Meena are both large language models, it can be inferred that Meena training runs would likely consume a similar amount of water as GPT-3, given the similarities in their training processes."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides information about the energy consumption of different AI tasks, with image generation using, on average, over 60 times more energy than text generation. It is also mentioned that image generation consumes 2.9 kWh for 1,000 inferences, while text generation consumes 0.047 kWh for the same number of inferences.","2.9","kWh","[""luccioni2024""]","is_blank","Inference energy (kWh) task mean std image generation 2.907 3.31","The context provides information about the energy consumption of different AI tasks, with image generation using, on average, over 60 times more energy than text generation. It is also mentioned that image generation consumes 2.9 kWh for 1,000 inferences, while text generation consumes 0.047 kWh for the same number of inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides data on the estimated CO2e emissions for various consumption items, including an average American life in one year, which is 36,156 lbs. This is higher than the emissions for air travel, car fuel, and other items listed.","36156","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Human life, avg, 1 year 36,156","The context provides data on the estimated CO2e emissions for various consumption items, including an average American life in one year, which is 36,156 lbs. This is higher than the emissions for air travel, car fuel, and other items listed."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a table (ref_id=han2024) that lists the per-household health cost for various counties in West Virginia. The county of Marion is listed with a per-household health cost of 1218.3, which is the highest among the counties listed.","1","is_blank","[""han2024""]","is_blank","Table showing per-household health cost for various counties in West Virginia (ref_id=han2024)","The context provides a table (ref_id=han2024) that lists the per-household health cost for various counties in West Virginia. The county of Marion is listed with a per-household health cost of 1218.3, which is the highest among the counties listed."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides data on the carbon emissions of various models, including Mistral-small. The emissions are given in grams of CO2e per 400-token query. However, the question asks for the emissions change after optimization, and the context does not specify any optimization for Mistral-small.","is_blank","multiplier","[""jegham2025""]","is_blank","is_blank","The context provides data on the carbon emissions of various models, including Mistral-small. The emissions are given in grams of CO2e per 400-token query. However, the question asks for the emissions change after optimization, and the context does not specify any optimization for Mistral-small."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for fine-tuning the Bloomz-7B model, which was 7,571 kWh. It also states that the energy usage for the entire training process was 51,686 kWh. To find the combined training and fine-tuning energy costs, we add these two values together, resulting in 59,258 kWh.","59258","kWh","[""ebert2024""]","is_blank","7,571 kWh for fine-tuning the Bloomz-7B ([ref_id=ebert2024]) + 51,686 kWh for the entire training process ([ref_id=ebert2024])","The context provides the energy usage for fine-tuning the Bloomz-7B model, which was 7,571 kWh. It also states that the energy usage for the entire training process was 51,686 kWh. To find the combined training and fine-tuning energy costs, we add these two values together, resulting in 59,258 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The total number of parameters in the final FLM-101B model is not explicitly stated in the provided context. However, the context does mention the cost of the model in zettaFLOPs, which can be used to infer the number of parameters. The cost of FLM-101B is 28.22 zettaFLOPs for English and 24.54 zettaFLOPs for Chinese, which, when combined, is approximately 52.76 zettaFLOPs. The number of parameters in a model is often proportional to the computational cost, so we can infer that the total number of parameters in FLM-101B is approximately 52.76 zettaFLOPs / (average cost per parameter in zettaFLOPs), where the average cost per parameter is a value that would need to be determined experimentally.","is_blank","parameters","[""li2025a""]","is_blank","The cost of FLM-101B is 28.22 zettaFLOPs for English and 24.54 zettaFLOPs for Chinese.","The total number of parameters in the final FLM-101B model is not explicitly stated in the provided context. However, the context does mention the cost of the model in zettaFLOPs, which can be used to infer the number of parameters. The cost of FLM-101B is 28.22 zettaFLOPs for English and 24.54 zettaFLOPs for Chinese, which, when combined, is approximately 52.76 zettaFLOPs. The number of parameters in a model is often proportional to the computational cost, so we can infer that the total number of parameters in FLM-101B is approximately 52.76 zettaFLOPs / (average cost per parameter in zettaFLOPs), where the average cost per parameter is a value that would need to be determined experimentally."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the energy cost for training a 6.1B parameter model (51,686 kWh) and the energy cost per inference for the BLOOMz-7B model (1.0 √ó 10‚àí4 kWh). To find the number of inferences needed to match the training energy cost, we divide the training energy cost by the inference energy cost. This gives us 51,686,000 / 1.0 √ó 10‚àí4 = 516,860,000,000 inferences.","516860000000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B training energy (kWh) 51,686","The context provides the energy cost for training a 6.1B parameter model (51,686 kWh) and the energy cost per inference for the BLOOMz-7B model (1.0 √ó 10‚àí4 kWh). To find the number of inferences needed to match the training energy cost, we divide the training energy cost by the inference energy cost. This gives us 51,686,000 / 1.0 √ó 10‚àí4 = 516,860,000,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The total amount of energy consumed for all model experimentation and evaluation is explicitly stated in the context as 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.","The total amount of energy consumed for all model experimentation and evaluation is explicitly stated in the context as 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the energy cost of training, fine-tuning, and inference for different sizes of the BLOOMz models. It also presents the number of inferences required for the cost of inference to reach the cost of training for each model. The number of inferences required for BLOOMz-7B to reach the cost parity is 592,570,000.","592,570,000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 √ó 10‚àí4 7.3 √ó 10‚àí5 6.2 √ó 10‚àí5 5.4 √ó 10‚àí5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the energy cost of training, fine-tuning, and inference for different sizes of the BLOOMz models. It also presents the number of inferences required for the cost of inference to reach the cost of training for each model. The number of inferences required for BLOOMz-7B to reach the cost parity is 592,570,000."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context describes the dataset used in the experiments as originating from the process of selecting a repository site for high-level radioactive waste in Germany. The dataset is published as the FKTG-dataset and contains text submissions from the population, which are categorized and published. The dataset is used for text categorization experiments, but no specific dataset name is mentioned.","is_blank","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The text of the submission is given by the column ‚ÄòBeitrag‚Äô (input). The column ‚ÄòThemenkomplex‚Äô (topic) contains the category of the text.","The context describes the dataset used in the experiments as originating from the process of selecting a repository site for high-level radioactive waste in Germany. The dataset is published as the FKTG-dataset and contains text submissions from the population, which are categorized and published. The dataset is used for text categorization experiments, but no specific dataset name is mentioned."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. However, it does discuss the water consumption of GPT-3, which is another large language model. The water consumption of GPT-3 is estimated in the context, but it is not the same model as Meta's Llama 3.","is_blank","liters","[""han2024"", ""li2025b""]","is_blank","is_blank","The context does not provide specific information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. However, it does discuss the water consumption of GPT-3, which is another large language model. The water consumption of GPT-3 is estimated in the context, but it is not the same model as Meta's Llama 3."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context provides information about the environmental impact of developing and training AI models, but it does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""morrison2025""]","is_blank","is_blank","The context provides information about the environmental impact of developing and training AI models, but it does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a comparison between the CO2e emissions of the Evolved Transformer NAS (3.2 tCO2e) and the CO2e emissions of a round trip between San Francisco and New York (180.4 tCO2e). It is stated that the CO2e equivalent of NAS is approximately 3 passengers taking a round trip between San Francisco and New York.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context provides a comparison between the CO2e emissions of the Evolved Transformer NAS (3.2 tCO2e) and the CO2e emissions of a round trip between San Francisco and New York (180.4 tCO2e). It is stated that the CO2e equivalent of NAS is approximately 3 passengers taking a round trip between San Francisco and New York."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context provides data on the environmental impact transparency of LLM usage from the OpenRouter API platform. It states that in May 2025, of the top 20 used models, only one directly released environmental data and three released it indirectly. The majority of users who interact with LLMs have no information about their environmental impacts, and 84% of LLM usage is through models with no disclosure. Therefore, it can be inferred that the percentage of LLM token usage occurring through models that did not disclose their environmental impact is 84%.","84","percent","[""luccioni2025c""]","is_blank","May 2025 data (Figure 2) indicates that of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1, DeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by releasing their model weights to enable efficiency analysis). In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context provides data on the environmental impact transparency of LLM usage from the OpenRouter API platform. It states that in May 2025, of the top 20 used models, only one directly released environmental data and three released it indirectly. The majority of users who interact with LLMs have no information about their environmental impacts, and 84% of LLM usage is through models with no disclosure. Therefore, it can be inferred that the percentage of LLM token usage occurring through models that did not disclose their environmental impact is 84%."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context discusses the inference requirements for the 65B LLaMA model, stating that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of A100 80GB GPUs needed is not explicitly mentioned for the 65B LLaMA model inference without compression or quantization.","is_blank","A100_80GB_GPU","[""samsi2024""]","is_blank","The context mentions that 4 A100 GPUs each with 80GB of memory are required for the 65B LLaMA model, but it does not specify the minimum number for inference without compression or quantization.","The context discusses the inference requirements for the 65B LLaMA model, stating that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of A100 80GB GPUs needed is not explicitly mentioned for the 65B LLaMA model inference without compression or quantization."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption for a 6 billion parameter transformer model during an 8-day training run on 256 NVIDIA A100s, which amounts to 13.8 MWh. To convert this to metric tons, we divide by 1000 and then by 3600 (hours to seconds), and then multiply by the carbon intensity of electricity, which is approximately 0.2855296 kg CO2 per kWh. This calculation gives us an estimate of 103,500 kWh, which when converted to metric tons gives us the answer.","103.5","metric tons","[""dodge2022""]","is_blank","13.8 MWh (dodge2022)","The context provides the energy consumption for a 6 billion parameter transformer model during an 8-day training run on 256 NVIDIA A100s, which amounts to 13.8 MWh. To convert this to metric tons, we divide by 1000 and then by 3600 (hours to seconds), and then multiply by the carbon intensity of electricity, which is approximately 0.2855296 kg CO2 per kWh. This calculation gives us an estimate of 103,500 kWh, which when converted to metric tons gives us the answer."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context discusses the environmental impact of large language models (LLMs) and the need for energy-efficient optimization techniques to address these concerns. However, it does not provide specific data on the carbon emissions avoided by pruning and quantizing LLMs in 2023.","is_blank","tCO2e","[""khan2025""]","[""https://ieeexplore.ieee.org/document/9818036""]","is_blank","The context discusses the environmental impact of large language models (LLMs) and the need for energy-efficient optimization techniques to address these concerns. However, it does not provide specific data on the carbon emissions avoided by pruning and quantizing LLMs in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context provides a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only. This is highlighted in the document 'griggs2024'.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context provides a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only. This is highlighted in the document 'griggs2024'."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context does not provide specific information about the price per hour for an NVIDIA H20. However, it does mention that the price per chip for an NVIDIA H20 is estimated using the relative complete system cost against H100, which is $2.70/hr. Since the H20 is not readily available on cloud service providers, the listed price is an estimate.","2.70","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $2.70/hr","The context does not provide specific information about the price per hour for an NVIDIA H20. However, it does mention that the price per chip for an NVIDIA H20 is estimated using the relative complete system cost against H100, which is $2.70/hr. Since the H20 is not readily available on cloud service providers, the listed price is an estimate."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context discusses the process of gathering information for carbon footprint analyses of AI models. It is stated that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers. This suggests that most authors do not automatically provide this information, implying that most carbon footprint analyses gather the information manually by writing to authors.","1","is_blank","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs.","The context discusses the process of gathering information for carbon footprint analyses of AI models. It is stated that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers. This suggests that most authors do not automatically provide this information, implying that most carbon footprint analyses gather the information manually by writing to authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context provides information about the public health costs of data centers in 2028, which are projected to potentially triple from 2023 to 2028. The total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively, under the low- and high-growth scenarios considered in the report. The percentage of the data centers' total electricity cost that is equivalent to their public health cost is stated to be approximately 44%.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers‚Äô total electricity cost.","The context provides information about the public health costs of data centers in 2028, which are projected to potentially triple from 2023 to 2028. The total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively, under the low- and high-growth scenarios considered in the report. The percentage of the data centers' total electricity cost that is equivalent to their public health cost is stated to be approximately 44%."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context discusses the inference requirements for the LLaMA model on different GPU configurations. It is stated that for the 65B LLaMA model, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. Therefore, the minimum number of A100 80GB GPUs required is 4.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context discusses the inference requirements for the LLaMA model on different GPU configurations. It is stated that for the 65B LLaMA model, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. Therefore, the minimum number of A100 80GB GPUs required is 4."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context discusses a case study that demonstrates a methodology for optimizing large language models (LLMs) to reduce their carbon footprint. The study presents a framework that uses strategic quantization and local inference techniques, which can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","The context discusses a case study that demonstrates a methodology for optimizing large language models (LLMs) to reduce their carbon footprint. The study presents a framework that uses strategic quantization and local inference techniques, which can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context provides information that researchers reached out to over 500 authors for their carbon footprint analysis, and were only able to collect 95 answers. This implies that 95 answers were collected after reaching out to over 500 authors.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context provides information that researchers reached out to over 500 authors for their carbon footprint analysis, and were only able to collect 95 answers. This implies that 95 answers were collected after reaching out to over 500 authors."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE project is trained with 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total training budget by the number of GPU hours used. The total budget is $100k, so the cost per H100 GPU-hour is $100,000 / 30,000 = $3.33.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The JetMoE project is trained with 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total training budget by the number of GPU hours used. The total budget is $100k, so the cost per H100 GPU-hour is $100,000 / 30,000 = $3.33."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context discusses the public health impact of training a LLM, using Llama-3.1 as an example. It is mentioned that the health cost can vary widely depending on the training data center locations, and Iowa is specifically mentioned as having a high health cost. However, the exact health cost in Iowa for training Llama-3.1 is not explicitly stated in the context. Therefore, we cannot provide a specific numeric value for the health cost in Iowa. However, it is clear from the context that the health cost can exceed 120% of the electricity cost in certain locations, and Iowa is one such location.","is_blank","percent","[""han2024""]","is_blank","It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.","The context discusses the public health impact of training a LLM, using Llama-3.1 as an example. It is mentioned that the health cost can vary widely depending on the training data center locations, and Iowa is specifically mentioned as having a high health cost. However, the exact health cost in Iowa for training Llama-3.1 is not explicitly stated in the context. Therefore, we cannot provide a specific numeric value for the health cost in Iowa. However, it is clear from the context that the health cost can exceed 120% of the electricity cost in certain locations, and Iowa is one such location."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The number of tokens used to pre-train the JetMoE-8B model is not explicitly stated in the provided context. However, the context does mention the hyperparameters of JetMoE-8B, including the number of layers, hidden state size, number of experts, top-k, number of heads, and hidden layer size. The number of tokens used to pre-train the model can be inferred from these hyperparameters, but it is not explicitly stated.","is_blank","tokens","[""shen2024""]","is_blank","is_blank","The number of tokens used to pre-train the JetMoE-8B model is not explicitly stated in the provided context. However, the context does mention the hyperparameters of JetMoE-8B, including the number of layers, hidden state size, number of experts, top-k, number of heads, and hidden layer size. The number of tokens used to pre-train the model can be inferred from these hyperparameters, but it is not explicitly stated."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides information about Amazon's renewable energy projects in the United Kingdom, but it does not specify the exact number of Amazon Renewable Energy Projects announced as of January 2024 in the United Kingdom.","is_blank","projects","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's renewable energy projects in the United Kingdom, but it does not specify the exact number of Amazon Renewable Energy Projects announced as of January 2024 in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage distribution of Amazon's workforce in the United States across all levels, including men and women. However, the context does not specify the year for the percentage of men, so we cannot find an exact match for the year 2023.","46.9","percent","[""amazon2023""]","is_blank","46.9%","The context provides the percentage distribution of Amazon's workforce in the United States across all levels, including men and women. However, the context does not specify the year for the percentage of men, so we cannot find an exact match for the year 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","","","percent","[]","is_blank","is_blank",""
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific information about the execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, which could affect the total execution time. It also mentions that the maximum batch size supported by a model-dataset combination is determined by GPU memory size, model size, sequence length, and MoE sparsity, but does not provide a specific value for the batch size of 84.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context does not provide specific information about the execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, which could affect the total execution time. It also mentions that the maximum batch size supported by a model-dataset combination is determined by GPU memory size, model size, sequence length, and MoE sparsity, but does not provide a specific value for the batch size of 84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The granularity metric, introduced in the context, is the ratio of calculation to communication time. It is used to quantify the advantages and disadvantages of distributed spot training and predict performance with different hardware setups.","1","is_blank","[""erben2023""]","is_blank","The granularity metric is introduced in the following paragraph: 'To quantify total training cost, we assess cost-effectiveness and evaluate a hybrid or multi-cloud approach with popular cloud providers through training on up to four continents. For comparison of the models‚Äô scalability and to show which of them can be trained in a distributed fashion, we introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.'","The granularity metric, introduced in the context, is the ratio of calculation to communication time. It is used to quantify the advantages and disadvantages of distributed spot training and predict performance with different hardware setups."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context mentions that a single deal between Microsoft and Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets (ref_id: [luccioni2025a]).","640","times","[""luccioni2025a""]","is_blank","From Efficiency Gains to Rebound Effects FAccT ‚Äô25, June 23‚Äì26, 2025, Athens, Greece","The context mentions that a single deal between Microsoft and Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets (ref_id: [luccioni2025a])."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context discusses the water footprint of AI, specifically focusing on water consumption. It mentions that water consumption is defined as 'water withdrawal minus water discharge', and means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment' [13]. It also mentions that water consumption is crucial for assessing watershed-level scarcity [12].","is_blank","is_blank","[""li2025b""]","is_blank","‚Ä¢ Water consumption:It is defined as ‚Äòwater withdrawal minus water discharge‚Äô, and means the amount of water ‚Äòevaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment‚Äô [13].
‚Ä¢ Water consumption reflects the impact on downstream water availability and is crucial for assessing watershed-level scarcity [12].","The context discusses the water footprint of AI, specifically focusing on water consumption. It mentions that water consumption is defined as 'water withdrawal minus water discharge', and means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment' [13]. It also mentions that water consumption is crucial for assessing watershed-level scarcity [12]."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, it can be inferred that the frequency of formation of the Study Panel is every five years.","1","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity , the Standing Committee that oversees the One Hundred  Year Study forms a Study Panel every five years to assess the current state of  AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, it can be inferred that the frequency of formation of the Study Panel is every five years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context mentions that every US household has an average of 25 connected devices, which includes smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc. However, the specific number of each type of device is not provided.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context mentions that every US household has an average of 25 connected devices, which includes smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc. However, the specific number of each type of device is not provided."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides data on the energy consumption of training a large language model (LLM) in kWh for various models. It mentions that the energy consumption for training an LLM spans from as little as 0.8 MWh to 3,500 MWh.","is_blank","MWh","[""luccioni2025c""]","is_blank","From the limited data that is publicly available, we can observe significant disparities in energy use and emissions across models. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides data on the energy consumption of training a large language model (LLM) in kWh for various models. It mentions that the energy consumption for training an LLM spans from as little as 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context discusses the success of IBM's Watson program in beating human contenders in the Jeopardy challenge in 2011. It is mentioned that Watson's efficiency in organizing, indexing, and retrieving large amounts of information was a key factor in its victory.","1","is_blank","[""stone2022""]","is_blank","IBM‚Äôs Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context discusses the success of IBM's Watson program in beating human contenders in the Jeopardy challenge in 2011. It is mentioned that Watson's efficiency in organizing, indexing, and retrieving large amounts of information was a key factor in its victory."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context provides information about the water consumption of AI models, specifically mentioning that one paper suggests that 10-50 queries on GPT-3 consume around half a liter of water.","0.5","queries","[""luccioni2025a""]","is_blank","The context states that one paper suggests that 10-50 queries on GPT-3 consume around half a liter of water.","The context provides information about the water consumption of AI models, specifically mentioning that one paper suggests that 10-50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, as shown in Table 4.","is_blank","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, as shown in Table 4."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context mentions that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.","0.2","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ‚Äòdrink‚Äô (i.e., consume) a500ml bottle of waterfor roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.","The context mentions that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed."
"q168","The 2024 Griggs et al. paper reports that M√©lange can reduce deployment costs by up to what percentage in conversational chat settings?","The Griggs et al. paper reports that M√©lange can reduce deployment costs by up to 77% in conversational chat settings. The specific percentage reduction in the 2024 paper is not mentioned for document-based or mixed settings, but it is mentioned that M√©lange reduces costs by 77% in conversational settings.","77","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","Compared to using only a single GPU type, M√©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The Griggs et al. paper reports that M√©lange can reduce deployment costs by up to 77% in conversational chat settings. The specific percentage reduction in the 2024 paper is not mentioned for document-based or mixed settings, but it is mentioned that M√©lange reduces costs by 77% in conversational settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context provides information about the minimum number of GPUs required for running LLaMA-65B inference without compression or quantization. It states that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of A100 80GB GPUs needed is not explicitly mentioned.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","We find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context provides information about the minimum number of GPUs required for running LLaMA-65B inference without compression or quantization. It states that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of A100 80GB GPUs needed is not explicitly mentioned."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context discusses the public health impact of training an AI model of the Llama-3.1 scale. It is stated that the public health impact can exceed 120% of the electricity cost and can be equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","is_blank","round trips","[""han2024""]","is_blank","Depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context discusses the public health impact of training an AI model of the Llama-3.1 scale. It is stated that the public health impact can exceed 120% of the electricity cost and can be equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context provides several references stating that a significant portion of the machine learning workload is inference processing. For example, NVIDIA estimated that 80-90% of the ML workload is inference processing ([Leo19]), and Amazon Web Services claimed that 90% of the ML demand in the cloud is for inference ([Bar19]). Similarly, a 2022 study from Google attributed 60% of its ML energy use to inference ([40]).","80-90","percent","[""patterson2021"", ""bar19"", ""40""]","is_blank","For example, NVIDIA estimated that 80-90% of the ML workload is inference processing ([Leo19]), and Amazon Web Services claimed that 90% of the ML demand in the cloud is for inference ([Bar19]).","The context provides several references stating that a significant portion of the machine learning workload is inference processing. For example, NVIDIA estimated that 80-90% of the ML workload is inference processing ([Leo19]), and Amazon Web Services claimed that 90% of the ML demand in the cloud is for inference ([Bar19]). Similarly, a 2022 study from Google attributed 60% of its ML energy use to inference ([40])."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is not explicitly stated in the provided context. However, it is mentioned that for all of model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2. Since the context does not specify the total number of inferences performed, we cannot calculate the total CO2 equivalent emissions generated per inference.","is_blank","kg CO2eq","[""luccioni2024""]","is_blank","754.66 kWh of energy and 178.97 kg of CO2 emitted","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is not explicitly stated in the provided context. However, it is mentioned that for all of model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2. Since the context does not specify the total number of inferences performed, we cannot calculate the total CO2 equivalent emissions generated per inference."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context discusses the overestimation of energy consumption when estimating based on Thermal Design Power (TDP) of a GPU. It mentions that a GPU rarely draws its maximum power at every moment in time, and such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time.","The context discusses the overestimation of energy consumption when estimating based on Thermal Design Power (TDP) of a GPU. It mentions that a GPU rarely draws its maximum power at every moment in time, and such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs)."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes (jegham2025).","1","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes (jegham2025)."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides throughput data for different batch sizes of Mixtral-CS for both dense and sparse models. However, the data does not specify the throughput for a batch size of 1 for the dense Mixtral-CS-A100-40GB model.","is_blank","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0, Mixtral-MATH0.0, Blackmamba-CS0, Blackmamba-MATH0","The context provides throughput data for different batch sizes of Mixtral-CS for both dense and sparse models. However, the data does not specify the throughput for a batch size of 1 for the dense Mixtral-CS-A100-40GB model."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context discusses a trend of decreasing environmental transparency in the AI industry over time. It provides an analysis of AI models' level of environmental impact transparency based on key information from Epoch AI's Notable AI Models dataset. The analysis shows that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, with no direct environmental impact data released during this period.","0","is_blank","[""luccioni2025c""]","is_blank","From 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, with no direct environmental impact data released during this period.","The context discusses a trend of decreasing environmental transparency in the AI industry over time. It provides an analysis of AI models' level of environmental impact transparency based on key information from Epoch AI's Notable AI Models dataset. The analysis shows that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, with no direct environmental impact data released during this period."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context does not provide a direct numerical value for the normalized on-demand hourly price for an H100 GPU. However, it does provide the normalized price for H100 in comparison to other GPUs. By calculating the normalized price of H100 as (4.69/2.29) √ó 3.67 = $7.516, and comparing this to the on-demand price of other GPUs (0.7 for L4, 1.01 for A10G, and 3.67 for A100), we can infer that the normalized on-demand hourly price for an H100 GPU is greater than that of L4 and A10G, and equal to that of A100.","7.516","USD per hour","[""griggs2024""]","is_blank","The normalized price of H100 is calculated as (4.69/2.29) √ó 3.67 = $7.516.","The context does not provide a direct numerical value for the normalized on-demand hourly price for an H100 GPU. However, it does provide the normalized price for H100 in comparison to other GPUs. By calculating the normalized price of H100 as (4.69/2.29) √ó 3.67 = $7.516, and comparing this to the on-demand price of other GPUs (0.7 for L4, 1.01 for A10G, and 3.67 for A100), we can infer that the normalized on-demand hourly price for an H100 GPU is greater than that of L4 and A10G, and equal to that of A100."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides a table (Table 1) that estimates the operational water consumption footprint of GPT-3. The table lists the water consumption for each request for different locations, and the total water consumption for each location. However, the table does not specify the total water consumption for GPT-3 as a whole.","is_blank","liters of water","[""li2025b""]","is_blank","Table 1","The context provides a table (Table 1) that estimates the operational water consumption footprint of GPT-3. The table lists the water consumption for each request for different locations, and the total water consumption for each location. However, the table does not specify the total water consumption for GPT-3 as a whole."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the cost of training different models in terms of CO2 emissions (lbs) in Table 3. Llama-2-70B is not explicitly mentioned in the table, but it can be inferred that the cost of training Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs would be higher than the cost of training models like Transformerbase, Transformerbig, ELMo, BERTbase, NAS, and GPT-2, as these models are listed with their costs in the table.","is_blank","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","Table 3: Estimated cost of training a model in terms of CO 2 emissions (lbs) and cloud compute cost (USD)","The context provides the cost of training different models in terms of CO2 emissions (lbs) in Table 3. Llama-2-70B is not explicitly mentioned in the table, but it can be inferred that the cost of training Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs would be higher than the cost of training models like Transformerbase, Transformerbig, ELMo, BERTbase, NAS, and GPT-2, as these models are listed with their costs in the table."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context discusses the growth trend of model size in deep learning, specifically mentioning that a 1000x model size increase for GPT3-based language translation tasks leads to a higher BLEU score. The specific increase from 5 to 40 requires a model 1000x larger in size.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000√ó larger in size.","The context discusses the growth trend of model size in deep learning, specifically mentioning that a 1000x model size increase for GPT3-based language translation tasks leads to a higher BLEU score. The specific increase from 5 to 40 requires a model 1000x larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the emissions produced by training a Transformer model with Neural Architecture search, which is compared to the lifetime carbon emissions of five cars. The emissions for training a Transformer model with Neural Architecture search is estimated to be 626,155 lbs (284 tCO2e) according to [ref_id: dodge2022] and [ref_id: strubell2019].","is_blank","miles","[""dodge2022"", ""strubell2019""]","[""https://arxiv.org/abs/2206.11355"", ""https://arxiv.org/abs/1906.02243""]","626,155 lbs of CO2 emissions from [ref_id: strubell2019] and the emissions-to-driving-distance ratio is not explicitly stated in the context","The context provides the emissions produced by training a Transformer model with Neural Architecture search, which is compared to the lifetime carbon emissions of five cars. The emissions for training a Transformer model with Neural Architecture search is estimated to be 626,155 lbs (284 tCO2e) according to [ref_id: dodge2022] and [ref_id: strubell2019]."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the energy consumption per inference for the BLOOMz-7B model, which is 1.0 √ó 10‚àí4 kWh. To estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences, we multiply the energy per inference by the number of inferences. Since 1 MWh = 1000 kWh, we divide the energy per inference by 1000 to convert to MWh.","10","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B Inference energy (kWh) 1.0 √ó 10‚àí4","The context provides the energy consumption per inference for the BLOOMz-7B model, which is 1.0 √ó 10‚àí4 kWh. To estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences, we multiply the energy per inference by the number of inferences. Since 1 MWh = 1000 kWh, we divide the energy per inference by 1000 to convert to MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The JetMoE-8B model was trained using 30,000 H100 GPU hours, as stated in the context.","30000","H100 GPU hours","[""shen2024""]","is_blank","This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The JetMoE-8B model was trained using 30,000 H100 GPU hours, as stated in the context."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context provides an estimate that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027. This is based on the trend of growing development costs, which have been increasing at a rate of 2.4√ó per year since 2016.","1","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","The trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027","The context provides an estimate that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027. This is based on the trend of growing development costs, which have been increasing at a rate of 2.4√ó per year since 2016."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context provides the total number of floating point operations to train GPT-3, which is 3.14E+23 FLOPS. To find the number of FLOPS per second, we divide the total number of FLOPS by the training time in seconds. OpenAI told us that GPT-3 runs at 24.6 TeraFLOPS/sec. We can find the training time by dividing the total number of FLOPS by the number of GPUs and the FLOPS/sec of each GPU. The context tells us that OpenAI used 10,000 V100 GPUs for GPT-3, and each V100 runs at 330W. We can find the training time in seconds by dividing the total power consumption in watts by the power consumption per second of each GPU. The context tells us that 10,000 GPUs consume 3300 kW, so the training time in seconds is 3300 kW / 330W = 10,000 seconds = 14.8 days.","14.8","FLOPS","[""patterson2021""]","is_blank","OpenAI used 10,000 V100 GPUs for GPT-3. Each V100 runs at 330W. The total power consumption for 10,000 GPUs is 3300 kW.","The context provides the total number of floating point operations to train GPT-3, which is 3.14E+23 FLOPS. To find the number of FLOPS per second, we divide the total number of FLOPS by the training time in seconds. OpenAI told us that GPT-3 runs at 24.6 TeraFLOPS/sec. We can find the training time by dividing the total number of FLOPS by the number of GPUs and the FLOPS/sec of each GPU. The context tells us that OpenAI used 10,000 V100 GPUs for GPT-3, and each V100 runs at 330W. We can find the training time in seconds by dividing the total power consumption in watts by the power consumption per second of each GPU. The context tells us that 10,000 GPUs consume 3300 kW, so the training time in seconds is 3300 kW / 330W = 10,000 seconds = 14.8 days."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context discusses the requirements for running LLaMA-65B inference without compression or quantization. It mentions that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of GPUs needed is not explicitly stated for the V100_32GB_GPUs in the question.","is_blank","V100_32GB_GPUs","[""samsi2024""]","is_blank","In the context, it is mentioned that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, the specific number of GPUs needed is not explicitly stated for the V100_32GB_GPUs in the question.","The context discusses the requirements for running LLaMA-65B inference without compression or quantization. It mentions that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, the specific number of GPUs needed is not explicitly stated for the V100_32GB_GPUs in the question."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The throughput data for the final 101B training stage is provided in Table 2. The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%.","is_blank","zettaFLOPs","[""li2025a""]","is_blank","Table 2","The throughput data for the final 101B training stage is provided in Table 2. The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context provides information about the top-1 accuracy of several models, including AlexNet 2012. Specifically, it mentions a report that achieved 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for 15 epochs. However, the context does not specify the accuracy of AlexNet 2012, so we cannot definitively answer the question. Therefore, the answer is 'is_blank'.","is_blank","percent","[""wu2021a""]","is_blank","The context mentions a report achieving 69.3% top-1 validation accuracy with a ResNet-50 model, but does not specify the accuracy of AlexNet 2012.","The context provides information about the top-1 accuracy of several models, including AlexNet 2012. Specifically, it mentions a report that achieved 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for 15 epochs. However, the context does not specify the accuracy of AlexNet 2012, so we cannot definitively answer the question. Therefore, the answer is 'is_blank'."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 servers, and the number of A800 GPUs used for training is not explicitly mentioned. However, since each server has 8 A800 GPUs, the total number of A800 GPUs used is 24 * 8 = 192.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 servers, and the number of A800 GPUs used for training is not explicitly mentioned. However, since each server has 8 A800 GPUs, the total number of A800 GPUs used is 24 * 8 = 192."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides information about the carbon emissions produced by training a Transformer model with Neural Architecture Search (NAS). It is stated that the emissions produced are comparable to the lifetime carbon emissions of five cars (626,155 lbs).","626155","lifetimes","[""dodge2022"", ""strubell2019""]","[""https://arxiv.org/abs/2206.08455"", ""https://arxiv.org/abs/1906.02243""]","Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption.","The context provides information about the carbon emissions produced by training a Transformer model with Neural Architecture Search (NAS). It is stated that the emissions produced are comparable to the lifetime carbon emissions of five cars (626,155 lbs)."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context provides the information that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR‚Äôs RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context provides the information that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context provides information about Amazon's on-site solar energy systems, specifically stating that 50 new on-site solar energy systems became operational in 2023, adding 58 MW of capacity. It is then mentioned that these systems generate an estimated 123,000 MWh annually, which is enough energy to power over 33,600 European homes. The context does not provide a direct comparison to nonrenewable electricity sources, but it is reasonable to assume that the energy generated by these solar systems avoids the equivalent of CO2e compared to nonrenewable electricity sources, given the context of renewable energy reducing carbon emissions.","47400","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com/amazon-sustainability-report-2023""]","These on-site solar energy systems are estimated to generate 123,000 MWh annually‚Äîenough energy to power over 33,600 European homes‚Äîand avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context provides information about Amazon's on-site solar energy systems, specifically stating that 50 new on-site solar energy systems became operational in 2023, adding 58 MW of capacity. It is then mentioned that these systems generate an estimated 123,000 MWh annually, which is enough energy to power over 33,600 European homes. The context does not provide a direct comparison to nonrenewable electricity sources, but it is reasonable to assume that the energy generated by these solar systems avoids the equivalent of CO2e compared to nonrenewable electricity sources, given the context of renewable energy reducing carbon emissions."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context does not explicitly mention a specific framework used for deploying large language models across multiple GPUs and nodes. However, it does discuss techniques such as local inference optimization, selection of energy-efficient pre-trained LLMs, and the use of the vllm library for multi-node computations. These methods suggest that the deployment of large language models across multiple GPUs and nodes is possible, but the specific framework used is not specified.","is_blank","is_blank","[""khan2025"", ""li2025a"", ""zschache2025"", ""shen2024"", ""fernandez2025"", ""ebert2024"", ""samsi2024""]","is_blank","is_blank","The context does not explicitly mention a specific framework used for deploying large language models across multiple GPUs and nodes. However, it does discuss techniques such as local inference optimization, selection of energy-efficient pre-trained LLMs, and the use of the vllm library for multi-node computations. These methods suggest that the deployment of large language models across multiple GPUs and nodes is possible, but the specific framework used is not specified."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides information about the energy consumption of various models when deployed on multiple nodes. It is shown in Figure 4 that deploying models across multiple nodes increases energy consumption due to network communication overhead. The Llama 3.1 70B model is specifically mentioned as having increased energy consumption when deployed on two nodes instead of one.","1","multiplier","[""zschache2025""]","is_blank","Figure 4","The context provides information about the energy consumption of various models when deployed on multiple nodes. It is shown in Figure 4 that deploying models across multiple nodes increases energy consumption due to network communication overhead. The Llama 3.1 70B model is specifically mentioned as having increased energy consumption when deployed on two nodes instead of one."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context provides information about the energy consumption of a single GPT-4o user session, which is estimated to be 3.73 Wh (¬±0.358 Wh) for a typical daily usage pattern. This energy consumption is then converted to water consumption using the Energy-Water Intensity Factor (EWIF), which is 3.14 L/kWh. Therefore, the water consumption per user session is calculated as 3.73 Wh * 3.14 L/kWh = 11.78 Wh * 3.14 L/kWh = 36.92 L or approximately 37 liters.","37","gallons of water","[""jegham2025""]","is_blank","Based on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of April 2025. Based on this, we quantify the per-user energy impact of GPT-4o interactions against familiar digital activities as presented in Figure 5. A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%. Scaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (¬±0.358 Wh).","The context provides information about the energy consumption of a single GPT-4o user session, which is estimated to be 3.73 Wh (¬±0.358 Wh) for a typical daily usage pattern. This energy consumption is then converted to water consumption using the Energy-Water Intensity Factor (EWIF), which is 3.14 L/kWh. Therefore, the water consumption per user session is calculated as 3.73 Wh * 3.14 L/kWh = 11.78 Wh * 3.14 L/kWh = 36.92 L or approximately 37 liters."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides information about the energy consumption of a single GPT-4o user session and the annual energy consumption of GPT-4o inference. It is stated that a single short GPT-4o query consumes 0.42 Wh, and to estimate the annual energy demand of GPT-4o in 2025, 700 million daily queries are attributed to GPT-4o, which would require approximately 391,509 MWh annually at minimum. This value exceeds the total electricity consumption of 35,000 U.S. residential households annually.","35000","homes","[""jegham2025""]","is_blank","The annual energy demand of GPT-4o in 2025 is estimated to be approximately 391,509 MWh, which exceeds the total electricity consumption of 35,000 U.S. residential households.","The context provides information about the energy consumption of a single GPT-4o user session and the annual energy consumption of GPT-4o inference. It is stated that a single short GPT-4o query consumes 0.42 Wh, and to estimate the annual energy demand of GPT-4o in 2025, 700 million daily queries are attributed to GPT-4o, which would require approximately 391,509 MWh annually at minimum. This value exceeds the total electricity consumption of 35,000 U.S. residential households annually."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context provides information about the water consumption of various companies, including Microsoft, in the period between 2021 and 2022. Microsoft is reported to have a 34% increase in global water consumption, topping 1.7 billion gallons.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context provides information about the water consumption of various companies, including Microsoft, in the period between 2021 and 2022. Microsoft is reported to have a 34% increase in global water consumption, topping 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context discusses the energy consumption of various models in sentiment analysis tasks, specifically focusing on large language models (LLMs) and traditional models. It is mentioned that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","1","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context discusses the energy consumption of various models in sentiment analysis tasks, specifically focusing on large language models (LLMs) and traditional models. It is mentioned that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that the Power Usage Effectiveness (PUE) for Google's Iowa datacenter where the Evolved Transformer was run is 1.11, which is a factor of 1.4X better.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context states that the Power Usage Effectiveness (PUE) for Google's Iowa datacenter where the Evolved Transformer was run is 1.11, which is a factor of 1.4X better."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides a detailed analysis of the estimated energy consumption of GPT-4o in 2025, which is projected to be between 391,509 MWh and 463,269 MWh annually. This information is used to estimate the total number of queries, as the energy consumption is attributed to a baseline of 1 billion queries per day across all ChatGPT deployments, with GPT-4o being the default model.","is_blank","queries","[""jegham2025""]","is_blank","The estimated total annual energy usage of GPT-4o in 2025 is between 391,509 MWh and 463,269 MWh.","The context provides a detailed analysis of the estimated energy consumption of GPT-4o in 2025, which is projected to be between 391,509 MWh and 463,269 MWh annually. This information is used to estimate the total number of queries, as the energy consumption is attributed to a baseline of 1 billion queries per day across all ChatGPT deployments, with GPT-4o being the default model."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides information about JetMoE-8B's performance on the OpenLLM Leaderboard benchmark suite, stating that it outperforms other open-source models. However, the context does not specify the average score for JetMoE-8B.","is_blank","score","[""shen2024""]","is_blank","is_blank","The context provides information about JetMoE-8B's performance on the OpenLLM Leaderboard benchmark suite, stating that it outperforms other open-source models. However, the context does not specify the average score for JetMoE-8B."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. However, it does discuss the carbon footprint of AI training and the potential for reducing this footprint by using renewable energy. It also mentions that the growth of AI outpaces the efficiency improvement at-scale, and that as GPU utilization is improved, both operational and embodied carbon emissions will reduce. However, these discussions do not directly provide a number for AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""wu2021a""]","[""https://www.nature.com/articles/s41467-021-24576-z""]","Fig. 9 illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce.","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. However, it does discuss the carbon footprint of AI training and the potential for reducing this footprint by using renewable energy. It also mentions that the growth of AI outpaces the efficiency improvement at-scale, and that as GPU utilization is improved, both operational and embodied carbon emissions will reduce. However, these discussions do not directly provide a number for AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context discusses policy proposals for AI and environmental impact, including the recommendation to report energy consumption from inferences at the cumulative server level. However, it does not explicitly state whether open-source general-purpose AI models are exempt from reporting their energy consumption under the AI Act. The context does suggest that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models, but it does not specify whether they are fully exempt or not.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Conference‚Äô17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker","The context discusses policy proposals for AI and environmental impact, including the recommendation to report energy consumption from inferences at the cumulative server level. However, it does not explicitly state whether open-source general-purpose AI models are exempt from reporting their energy consumption under the AI Act. The context does suggest that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models, but it does not specify whether they are fully exempt or not."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context provides data on the average global and EU data center PUE in 2023, which is 1.58 globally and 1.6 in the EU. However, the question asks for the PUE in 2020, not 2023. The context does not provide data for 2020, so the answer is blank.","is_blank","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides data on the average global and EU data center PUE in 2023, which is 1.58 globally and 1.6 in the EU. However, the question asks for the PUE in 2020, not 2023. The context does not provide data for 2020, so the answer is blank."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The provided context does not contain information about the KV Cache size for a batch size of 32 when using the OPT-2.7B model on the g4dn.xlarge instance. The context only provides KV Cache size information for batch sizes 2, 4, 8, 16, and 32 for the OPT-1.3B model on the same instance.","is_blank","GB","[""kim2025""]","is_blank","KV Cache Size (GB)
0
1
2
3
4
5
6
Batch Size
2
4
8
16
32
KV Cache Size
0
2
4
6
8
Table I does not provide information about the KV Cache size for a batch size of 32 when using the OPT-2.7B model on the g4dn.xlarge instance.","The provided context does not contain information about the KV Cache size for a batch size of 32 when using the OPT-2.7B model on the g4dn.xlarge instance. The context only provides KV Cache size information for batch sizes 2, 4, 8, 16, and 32 for the OPT-1.3B model on the same instance."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides the specific percentage range for R&D staff costs (including equity) in the four notable models studied in-depth by Cottier et al. (2025). The range is given as 29% to 49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context provides the specific percentage range for R&D staff costs (including equity) in the four notable models studied in-depth by Cottier et al. (2025). The range is given as 29% to 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The energy consumption during inference was measured using the CodeCarbon package, as stated in the context snippet from the document with reference id zschache2025.","is_blank","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package.","The energy consumption during inference was measured using the CodeCarbon package, as stated in the context snippet from the document with reference id zschache2025."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides an estimate of 3 Wh per LLM interaction, which is often quoted in the press and industry reports. This estimate is traced back to a remark from Alphabet's Chairman John Hennessy during a 2023 interview with Reuters, and an estimate published in October 2023 of 'approximately 3 Wh per LLM interaction' was based on this remark. The Google search number taken from a 2009 blog post from Google was used for comparison, stating that 'Queries vary in degree of difficulty, but for the average query [...] this amounts to 0.0003 kWh of energy per search'.","3","percent","[""luccioni2025c""]","is_blank","A request to ChatGPT consumes ten times more energy than a Google search
Another often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours (Wh) of energy, which is 'ten times more than a Google search'.","The context provides an estimate of 3 Wh per LLM interaction, which is often quoted in the press and industry reports. This estimate is traced back to a remark from Alphabet's Chairman John Hennessy during a 2023 interview with Reuters, and an estimate published in October 2023 of 'approximately 3 Wh per LLM interaction' was based on this remark. The Google search number taken from a 2009 blog post from Google was used for comparison, stating that 'Queries vary in degree of difficulty, but for the average query [...] this amounts to 0.0003 kWh of energy per search'."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The Compute Time Calibration Function (CTCF) is proposed in the context to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","is_blank","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The Compute Time Calibration Function (CTCF) is proposed in the context to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context provides data on the energy per second for inference with LLaMA 65B across different shard configurations. It is shown that the energy per second increases with the number of shards, even at the same batch size. For example, the energy per second increases from 300 Watts to 1 Kilowatt when the number of shards increases from 8 GPUs to 32 GPUs.","1","is_blank","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context provides data on the energy per second for inference with LLaMA 65B across different shard configurations. It is shown that the energy per second increases with the number of shards, even at the same batch size. For example, the energy per second increases from 300 Watts to 1 Kilowatt when the number of shards increases from 8 GPUs to 32 GPUs."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information about the water consumption of various GPU models during development and training. It is stated that one 12-inch silicon wafer produces about 63 H100s and mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is assumed to be 0.1% rare earth metal by mass, the water consumption for manufacturing a single H100 GPU can be estimated by multiplying the water consumption per kg of rare earth materials by the mass of rare earth materials in an H100 GPU.","1.3","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water (morrison2025)","The context provides information about the water consumption of various GPU models during development and training. It is stated that one 12-inch silicon wafer produces about 63 H100s and mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 GPU is assumed to be 0.1% rare earth metal by mass, the water consumption for manufacturing a single H100 GPU can be estimated by multiplying the water consumption per kg of rare earth materials by the mass of rare earth materials in an H100 GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses policy proposals for AI and environmental impact, which include mandating the reporting of indirect emissions and water consumption in AI applications. However, it does not explicitly mention energy consumption reporting for open-source general-purpose AI models.","0","is_blank","[""ebert2024""]","[""https://www.ebert2024.com""]","Indirect emissions and water consumption reporting: Mandate reporting of indirect emissions and water use in AI applications.","The context discusses policy proposals for AI and environmental impact, which include mandating the reporting of indirect emissions and water consumption in AI applications. However, it does not explicitly mention energy consumption reporting for open-source general-purpose AI models."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context provides information that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. This is stated in the Luccioni et al. paper (ref_id: luccioni2025a).","30%","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131].","The context provides information that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. This is stated in the Luccioni et al. paper (ref_id: luccioni2025a)."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides a projection for the total public health cost of U.S. data centers in 2028, based on the high-growth scenario considered in a report. According to the report, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario.","20.9","USD","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the total public health impact of U.S. data centers is estimated to reach$11.7 billion and$20.9 billion in 2028, respectively. Under the high-growth scenario, the resulting health burden could rival or exceed that of on-road emissions in the largest U.S. state.","The context provides a projection for the total public health cost of U.S. data centers in 2028, based on the high-growth scenario considered in a report. According to the report, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides information about the energy consumption of various models, including GPT-4.1 nano and o3. While the exact energy consumption of o3 is not explicitly stated, it is mentioned that GPT-4.1 nano consumes approximately 20% more energy than GPT-4.1 on long queries due to reliance on older A100 GPU nodes. Since o3 is also deployed on A100 nodes, it can be inferred that o3 consumes more energy than GPT-4.1 nano on long prompts. However, the exact energy consumption of o3 is not specified in the context.","is_blank","multiplier","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context provides information about the energy consumption of various models, including GPT-4.1 nano and o3. While the exact energy consumption of o3 is not explicitly stated, it is mentioned that GPT-4.1 nano consumes approximately 20% more energy than GPT-4.1 on long queries due to reliance on older A100 GPU nodes. Since o3 is also deployed on A100 nodes, it can be inferred that o3 consumes more energy than GPT-4.1 nano on long prompts. However, the exact energy consumption of o3 is not specified in the context."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M√©lange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context provides a cost savings analysis for M√©lange, a GPU allocation framework, across different datasets and SLOs (Service Level Objectives). For the Arena dataset with a 120ms SLO, M√©lange achieves cost reductions in the range of 15-77% compared to single-GPU baselines.","15-77","percent","[""griggs2024""]","is_blank","6.2 Cost Savings Analysis","The context provides a cost savings analysis for M√©lange, a GPU allocation framework, across different datasets and SLOs (Service Level Objectives). For the Arena dataset with a 120ms SLO, M√©lange achieves cost reductions in the range of 15-77% compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The carbon emissions of the pre-training of FLM-101B are not explicitly stated in the provided context. However, it is mentioned that the training costs only 100,000 US dollars, and carbon emissions are related to costs in the context of model training (ref_id: li2025a).","is_blank","tCO2e","[""li2025a""]","is_blank","is_blank","The carbon emissions of the pre-training of FLM-101B are not explicitly stated in the provided context. However, it is mentioned that the training costs only 100,000 US dollars, and carbon emissions are related to costs in the context of model training (ref_id: li2025a)."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on a NVIDIA A40-48 GB GPU is not explicitly provided in the context. However, the cost for fine-tuning Mixtral on the same dataset with a sparse setup on a NVIDIA H100 GPU is $3460. Since the A40 GPU has less memory capacity than the H100 GPU, it is reasonable to assume that the cost for fine-tuning on the A40 GPU would be less than $3460. Therefore, the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on a NVIDIA A40-48 GB GPU is is_blank.","is_blank","seconds","[""xia2024""]","is_blank","Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate.","The cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on a NVIDIA A40-48 GB GPU is not explicitly provided in the context. However, the cost for fine-tuning Mixtral on the same dataset with a sparse setup on a NVIDIA H100 GPU is $3460. Since the A40 GPU has less memory capacity than the H100 GPU, it is reasonable to assume that the cost for fine-tuning on the A40 GPU would be less than $3460. Therefore, the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on a NVIDIA A40-48 GB GPU is is_blank."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context discusses the public health impact of AI, highlighting that the health costs of training the same AI model can vary significantly across different locations. It also mentions that health costs exhibit a greater temporal variation than carbon emissions in most regions.","0","is_blank","[""han2024""]","[""https://www.sciencedirect.com/science/article/abs/pii/S2468074X22001831""]","6.1 Opportunities for Health-Informed Computing, Table 2","The context discusses the public health impact of AI, highlighting that the health costs of training the same AI model can vary significantly across different locations. It also mentions that health costs exhibit a greater temporal variation than carbon emissions in most regions."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context provides a comparison of the theoretical performance per watt of GPUs and TPUs, with GPUs being 1.5 times better than TPUs on average for large NLP DNNs according to theoretical performance per watt, but TPUs being 2.0 times better than GPUs on average for these large NLP DNNs according to measured performance per watt. However, the question asks about the doubling of theoretical performance per watt every 3-4 years, which is not directly addressed in the context.","is_blank","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore‚Äôs law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019]","The context provides a comparison of the theoretical performance per watt of GPUs and TPUs, with GPUs being 1.5 times better than TPUs on average for large NLP DNNs according to theoretical performance per watt, but TPUs being 2.0 times better than GPUs on average for these large NLP DNNs according to measured performance per watt. However, the question asks about the doubling of theoretical performance per watt every 3-4 years, which is not directly addressed in the context."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context describes the use of Ollama, an open-source tool, for local AI model deployment. This tool ensures data privacy by processing entirely on-device, making it suitable for sensitive applications. The case study specifically mentions the financial sentiment case study, where 4-bit quantization and local deployment of large language models were applied.","is_blank","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications.","The context describes the use of Ollama, an open-source tool, for local AI model deployment. This tool ensures data privacy by processing entirely on-device, making it suitable for sensitive applications. The case study specifically mentions the financial sentiment case study, where 4-bit quantization and local deployment of large language models were applied."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context discusses the use of Hivemind for distributed training, which allows for training on multiple cloud providers and four continents. It is mentioned that training on multiple cloud providers and four continents still scales with additional compute resources. However, the specific storage service used for sharding and streaming datasets for spot VMs that can terminate at any time is not explicitly stated.","is_blank","is_blank","[""erben2023""]","is_blank","is_blank","The context discusses the use of Hivemind for distributed training, which allows for training on multiple cloud providers and four continents. It is mentioned that training on multiple cloud providers and four continents still scales with additional compute resources. However, the specific storage service used for sharding and streaming datasets for spot VMs that can terminate at any time is not explicitly stated."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context discusses the relationship between inference runtime and energy consumption in large language models. It mentions a strong correlation between the two, indicating that execution time can serve as a practical proxy for energy usage.","1","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170v1""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context discusses the relationship between inference runtime and energy consumption in large language models. It mentions a strong correlation between the two, indicating that execution time can serve as a practical proxy for energy usage."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context provides information about the AI Environmental Impacts Act bill, which was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI‚Äôs environmental impacts without imposing significant regulatory obligations.","is_blank","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","S.3732 - Artificial Intelligence Environmental Impacts Act of 2024.","The context provides information about the AI Environmental Impacts Act bill, which was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI‚Äôs environmental impacts without imposing significant regulatory obligations."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context does not provide specific information about the price per hour for an NVIDIA H100. However, it does provide the price per chip for an H100 in the context of [ref_id=chen2024]. To find the price per hour, we would need to know the number of chips in an H100 and the hourly rate for renting these chips.","is_blank","USD per hour","[""chen2024""]","is_blank","Price per chip for an NVIDIA H100 is provided in [ref_id=chen2024].","The context does not provide specific information about the price per hour for an NVIDIA H100. However, it does provide the price per chip for an H100 in the context of [ref_id=chen2024]. To find the price per hour, we would need to know the number of chips in an H100 and the hourly rate for renting these chips."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide specific information about the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024. It discusses the environmental impact of AI systems, the cost of developing AI systems, and the energy consumption cost of hardware during model training, but there is no mention of the average GPU lifetime.","is_blank","years","[""morrison2025"", ""cottier2024""]","is_blank","is_blank","The context does not provide specific information about the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024. It discusses the environmental impact of AI systems, the cost of developing AI systems, and the energy consumption cost of hardware during model training, but there is no mention of the average GPU lifetime."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the inference requirements for the LLaMA model, specifically the V100 32GB GPUs. However, the context does not specify the minimum number of GPUs required for LLaMA-13B inference without compression or quantization.","is_blank","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context provides information about the inference requirements for the LLaMA model, specifically the V100 32GB GPUs. However, the context does not specify the minimum number of GPUs required for LLaMA-13B inference without compression or quantization."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides information about the estimated GHG emissions from training language models, including the Gemma family of models developed by Google. Google reports that training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the 'five cars' estimate. This exceeds the 'five cars' estimate, making the answer '1247.61 tons CO2e'.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their Gemma family of language models emitted 1247.61 tons CO2e.","The context provides information about the estimated GHG emissions from training language models, including the Gemma family of models developed by Google. Google reports that training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the 'five cars' estimate. This exceeds the 'five cars' estimate, making the answer '1247.61 tons CO2e'."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context provides information about the training time for the ELMo model, which was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context provides information about the training time for the ELMo model, which was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours)."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides information about the water consumption intensity factors for electricity generation in the U.S., which are 43.8 L/kWh for water withdrawal and 3.1 L/kWh for consumption. These values are used to estimate the water consumption for global AI in 2027.","3.1","L/kWh","[""li2025b""]","is_blank","43.8 L/kWh for water withdrawal and 3.1 L/kWh for consumption","The context provides information about the water consumption intensity factors for electricity generation in the U.S., which are 43.8 L/kWh for water withdrawal and 3.1 L/kWh for consumption. These values are used to estimate the water consumption for global AI in 2027."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides a comparison of PUEs for various data centers, including Google's data centers. It is stated that Google's PUE across its fleet of data centers for the 12 months ending in Q1 2021 was 1.10.","1.10","PUE","[""dodge2022""]","[""https://ieeexplore.ieee.org/document/9520865""]","Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].","The context provides a comparison of PUEs for various data centers, including Google's data centers. It is stated that Google's PUE across its fleet of data centers for the 12 months ending in Q1 2021 was 1.10."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This implies that moving workloads from on-premises data centers to AWS in North America can result in a significant reduction in carbon footprint.","96","percent","[""amazon2023""]","is_blank","In North America, AWS can lower its customers‚Äô workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","The context states that AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This implies that moving workloads from on-premises data centers to AWS in North America can result in a significant reduction in carbon footprint."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU, which is $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context provides a cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU, which is $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context does not provide a clear percentage for GPU's contribution to the total provisioned power in a typical datacenter. However, it does mention that GPUs are a significant factor in power consumption and that they account for a majority of the electricity consumption in ML applications based on deep learning (as confirmed by an experiment).","is_blank","percent","[""ebert2024""]","[""https://ieeexplore.ieee.org/document/8109155""]","The most accurate and popular AI models today are typically (deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive hardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide significant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming 250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at the core of neural network computations and a high rate of parallelization, GPUs can perform many more of these types of computations in the same amount of time as a CPU, but this increased computation throughput comes at an increased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45]. While this result is fairly uncontroversial, we ran an experiment to confirm it.","The context does not provide a clear percentage for GPU's contribution to the total provisioned power in a typical datacenter. However, it does mention that GPUs are a significant factor in power consumption and that they account for a majority of the electricity consumption in ML applications based on deep learning (as confirmed by an experiment)."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context provides the total number of H100 GPU hours used for training JetMoE-8B, which is 30,000. To find the number of H100 GPUs, we divide the total GPU hours by the number of hours required to train one H100 GPU. The context does not specify the number of hours required for one H100 GPU, so we cannot determine the exact number of GPUs with certainty.","is_blank","H100 GPUs","[""shen2024""]","is_blank","JetMoE-8B is trained with 30,000 H100 GPU hours.","The context provides the total number of H100 GPU hours used for training JetMoE-8B, which is 30,000. To find the number of H100 GPUs, we divide the total GPU hours by the number of hours required to train one H100 GPU. The context does not specify the number of hours required for one H100 GPU, so we cannot determine the exact number of GPUs with certainty."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides a figure showing the average GPU power for a single node during OLMo 2 7B training. It is stated that when actively training, the average GPU power is over 600W, which is more than 85% of an H100‚Äôs maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training.","The context provides a figure showing the average GPU power for a single node during OLMo 2 7B training. It is stated that when actively training, the average GPU power is over 600W, which is more than 85% of an H100‚Äôs maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides data on CO2e emissions for various consumption items, including human life, air travel, and car usage. It is inferred that the CO2e emissions for an average human life in one year (globally) can be estimated by the value given for 'Human life, avg, 1 year' in the table from the Strubell2019 document.","11023","lbs","[""strubell2019""]","is_blank","Human life, avg, 1 year 11,023","The context provides data on CO2e emissions for various consumption items, including human life, air travel, and car usage. It is inferred that the CO2e emissions for an average human life in one year (globally) can be estimated by the value given for 'Human life, avg, 1 year' in the table from the Strubell2019 document."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides a comparison of inference performance between LLaMA models on V100 and A100 GPUs. It is stated that for LLaMA 13B, there is a 1.25 times increase in inference throughput on the A100 when compared to the V100. However, the context does not provide a specific speedup value for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs for the specific inference throughput metric.","is_blank","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context provides a comparison of inference performance between LLaMA models on V100 and A100 GPUs. It is stated that for LLaMA 13B, there is a 1.25 times increase in inference throughput on the A100 when compared to the V100. However, the context does not provide a specific speedup value for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs for the specific inference throughput metric."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides information about the energy consumption of a single short query to GPT-4o, which is stated to be 0.42 Wh (¬±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context provides information about the energy consumption of a single short query to GPT-4o, which is stated to be 0.42 Wh (¬±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699. InferSave's top choice, on the other hand, was g4dn.xlarge, which offers 620.17 TPS at a cost of $0.71. Therefore, the cost of Max-Performance's choice is about 280% more expensive than InferSave‚Äôs top choice.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave‚Äôs top choice.","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699. InferSave's top choice, on the other hand, was g4dn.xlarge, which offers 620.17 TPS at a cost of $0.71. Therefore, the cost of Max-Performance's choice is about 280% more expensive than InferSave‚Äôs top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context provides information about the energy efficiency of different GPU architectures for models generating only a single classification token. It is stated that a V100 or even an A30 GPU is more efficient in inference for models generating a single token per inference.","is_blank","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context provides information about the energy efficiency of different GPU architectures for models generating only a single classification token. It is stated that a V100 or even an A30 GPU is more efficient in inference for models generating a single token per inference."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context discusses Green AI as a practice that involves making efficiency a common evaluation criterion for AI research alongside accuracy and related measures. It is proposed that reporting the financial cost or 'price tag' of developing, training, and running models could provide baselines for the investigation of increasingly efficient methods.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Green AI is an emerging focus at the Allen Institute for AI. The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [40]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.","The context discusses Green AI as a practice that involves making efficiency a common evaluation criterion for AI research alongside accuracy and related measures. It is proposed that reporting the financial cost or 'price tag' of developing, training, and running models could provide baselines for the investigation of increasingly efficient methods."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context mentions that the global e-waste monitor 2024 showed that about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling. It also states that the remainder ends up dumped in landfills, often in developing countries. The total amount of e-waste generated worldwide in the year 2022 is not explicitly stated in the context, but the implication is that e-waste generation is rising faster than recycling, so it is reasonable to infer that the total amount of e-waste in 2022 is more than the amount that has been formally collected and recycled.","is_blank","metric tons","[""luccioni2025a""]","is_blank","The UN‚Äôs Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context mentions that the global e-waste monitor 2024 showed that about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling. It also states that the remainder ends up dumped in landfills, often in developing countries. The total amount of e-waste generated worldwide in the year 2022 is not explicitly stated in the context, but the implication is that e-waste generation is rising faster than recycling, so it is reasonable to infer that the total amount of e-waste in 2022 is more than the amount that has been formally collected and recycled."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides measurements of the average power per processor for various processors, including TPU v2 and V100 GPU. However, it does not provide a direct comparison of the average system power per processor between TPU v2 and V100 GPU. Therefore, we cannot determine the difference in average system power per processor between the TPU v2 and the V100 GPU with certainty based on the provided documents.","is_blank","Watts","[""patterson2021""]","is_blank","is_blank","The context provides measurements of the average power per processor for various processors, including TPU v2 and V100 GPU. However, it does not provide a direct comparison of the average system power per processor between TPU v2 and V100 GPU. Therefore, we cannot determine the difference in average system power per processor between the TPU v2 and the V100 GPU with certainty based on the provided documents."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context discusses the water footprint of AI models, specifically mentioning the water consumption of GPT-3 during training. It is estimated that training GPT-3 in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","Directly evaporated 700,000 liters of clean freshwater","The context discusses the water footprint of AI models, specifically mentioning the water consumption of GPT-3 during training. It is estimated that training GPT-3 in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021, as shown in Figure 2(c).","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c)","The context states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021, as shown in Figure 2(c)."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The document does not provide a clear ranking of models based on eco-efficiency analysis using DEA. However, it does mention that the methodology offers a standardized, scalable framework to quantify eco-efficiency, and it provides a dashboard that visualizes model-level energy, water, and carbon metrics.","is_blank","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598""]","We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes model-level energy, water, and carbon metrics.","The document does not provide a clear ranking of models based on eco-efficiency analysis using DEA. However, it does mention that the methodology offers a standardized, scalable framework to quantify eco-efficiency, and it provides a dashboard that visualizes model-level energy, water, and carbon metrics."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context discusses the energy consumption of various AI tasks, including text generation and image generation. It mentions that text generation uses on average over 10 times more energy for the same number of inferences as compared to text classification, which is the least energy-intensive task. Image generation, on the other hand, uses significantly more energy than text generation. Smartphone charging is compared with the energy consumption of the most efficient text generation model, which uses as much energy as 9% of a full smartphone charge for 1,000 inferences. However, the context does not provide a specific average lifetime for smartphones in terms of years.","0","is_blank","[""luccioni2024""]","[""https://link.springer.com/chapter/10.1007/978-3-031-24523-1_10""]","Inference energy (kWh) task mean std text classification 0.002 0.001 extractive QA 0.003 0.001 masked language modeling 0.003 0.001 token classification 0.004 0.002 image classification 0.007 0.001 object detection 0.038 0.02 text generation 0.047 0.03 summarization 0.049 0.01 image captioning 0.063 0.02 image generation 2.907 3.31","The context discusses the energy consumption of various AI tasks, including text generation and image generation. It mentions that text generation uses on average over 10 times more energy for the same number of inferences as compared to text classification, which is the least energy-intensive task. Image generation, on the other hand, uses significantly more energy than text generation. Smartphone charging is compared with the energy consumption of the most efficient text generation model, which uses as much energy as 9% of a full smartphone charge for 1,000 inferences. However, the context does not provide a specific average lifetime for smartphones in terms of years."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context does not clearly support a definitive answer about the per-GPU speedup for CV models when using T4 GPUs for intra-zone scaling. However, it does show that there is a speedup for CV models when scaling from 1 to 8 GPUs, with a maximum speedup of 3.2x at 8 GPUs. The per-GPU speedup is almost linear, with a speedup of 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively.","3.2","is_blank","[""erben2023""]","[""https://arxiv.org/abs/2303.11336""]","Figure 7: (A) Intra-zone performance for CV and NLP.","The context does not clearly support a definitive answer about the per-GPU speedup for CV models when using T4 GPUs for intra-zone scaling. However, it does show that there is a speedup for CV models when scaling from 1 to 8 GPUs, with a maximum speedup of 3.2x at 8 GPUs. The per-GPU speedup is almost linear, with a speedup of 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context states that the FLM-101B model is structured with a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","In the context, it is stated that the FLM-101B model has a context window of 2,048 tokens.","The context states that the FLM-101B model is structured with a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context discusses the power consumption of various models on A100 and H100 GPUs. It is mentioned that Diffusion models consume nearly the maximum power of the GPU when batch size is not small because they are significantly more compute-intensive compared to LLM decoding.","1","is_blank","[""chung2025""]","is_blank","Generally, Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context discusses the power consumption of various models on A100 and H100 GPUs. It is mentioned that Diffusion models consume nearly the maximum power of the GPU when batch size is not small because they are significantly more compute-intensive compared to LLM decoding."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides data on the gender distribution of Amazon's workforce, but it does not specifically mention the percentage of women who are people managers in 2023.","is_blank","percent","[""amazon2023""]","is_blank","is_blank","The context provides data on the gender distribution of Amazon's workforce, but it does not specifically mention the percentage of women who are people managers in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides information about the breakdown of costs for four key models analyzed by Cottier et al. (2025). It is stated that when equity is excluded, the percentage range of total amortized cost attributed to computing hardware for these models is 61-76%.","61-76","percent","[""cottier2024""]","is_blank","When excluding equity the fraction for computing hardware costs and energy rise to 61‚Äì76% and 2‚Äì7% respectively.","The context provides information about the breakdown of costs for four key models analyzed by Cottier et al. (2025). It is stated that when equity is excluded, the percentage range of total amortized cost attributed to computing hardware for these models is 61-76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context discusses the potential trade-offs between energy efficiency and overall predictive performance when applying optimization techniques. It mentions that while optimization can enhance certain aspects of the models‚Äô ability to correctly identify relevant patterns in the data, metrics like accuracy and F1 score may decline slightly post-optimization.","1","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context discusses the potential trade-offs between energy efficiency and overall predictive performance when applying optimization techniques. It mentions that while optimization can enhance certain aspects of the models‚Äô ability to correctly identify relevant patterns in the data, metrics like accuracy and F1 score may decline slightly post-optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. This information is used to convert power to estimated CO2 emissions. The conversion takes into account the relative proportions of different energy sources (primarily natural gas, coal, nuclear and renewable) consumed to produce energy in the United States.","is_blank","lbs/kWh","[""strubell2019""]","is_blank","CO2e = 0 .954pt (2)","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. This information is used to convert power to estimated CO2 emissions. The conversion takes into account the relative proportions of different energy sources (primarily natural gas, coal, nuclear and renewable) consumed to produce energy in the United States."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a study suggesting that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027. This electricity consumption is associated with water consumption for cooling servers and electricity generation, as indicated in the context.","85-134","TWh","[""li2025b""]","is_blank","For example, a recent study suggests that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027 [7]","The context provides a study suggesting that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027. This electricity consumption is associated with water consumption for cooling servers and electricity generation, as indicated in the context."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context provides information about the number of packages delivered via EVs in different regions in 2023. Specifically, it mentions that Amazon delivered 150 million packages via EVs in Europe. However, the question asks for the number of packages delivered via EVs in Europe in 2023 in millions, and the provided number is in millions. Therefore, the context supports the answer.","150","packages","[""amazon2023""]","is_blank","We delivered 150 million packages via EVs.","The context provides information about the number of packages delivered via EVs in different regions in 2023. Specifically, it mentions that Amazon delivered 150 million packages via EVs in Europe. However, the question asks for the number of packages delivered via EVs in Europe in 2023 in millions, and the provided number is in millions. Therefore, the context supports the answer."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The total number of tokens processed during the entire online inference workload evaluation is not explicitly stated in the provided context. However, the context does mention the input and output sizes for both the online and offline inference workloads. The online inference workload uses a pattern of 128 input tokens and a 512 output tokens, while the offline inference workload uses an input size of 1024 tokens and an output size of 128 tokens. To find the total number of tokens processed during the online inference workload, we can multiply the number of requests (3000) by the sum of the input and output token sizes (128 + 512 = 640).","1920000","tokens","[""kim2025""]","is_blank","Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The total number of tokens processed during the entire online inference workload evaluation is not explicitly stated in the provided context. However, the context does mention the input and output sizes for both the online and offline inference workloads. The online inference workload uses a pattern of 128 input tokens and a 512 output tokens, while the offline inference workload uses an input size of 1024 tokens and an output size of 128 tokens. To find the total number of tokens processed during the online inference workload, we can multiply the number of requests (3000) by the sum of the input and output token sizes (128 + 512 = 640)."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act does not mandate the disclosure of greenhouse gas emissions of AI applications, such as oil and gas exploration.","0","is_blank","[""ebert2024""]","[""https://academic.oup.com/eurpub/article/33/4/473/6214335""]","5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The AI Act does not mandate the disclosure of greenhouse gas emissions of AI applications, such as oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context provides data on the carbon emissions reduction for different job durations and regions, as well as the impact of the region on emissions. It is stated that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction, greater than 30% in multiple regions, and up to 80% in West US can be found. The question asks for the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region.","80","percent","[""dodge2022""]","is_blank","East USWest USWest US2West US3Central US
N. Central USS. Central USW. Central US
CanadaFrance
Germany
West EuropeNorth Europe
NorwayUK SouthAustralia
0
10
20
30
40
50
60
70
80CO2 emissions decrease in %
6h
12h
18h
24h
(a) Flexible Start optimization for Dense 201.
East USWest USWest US2West US3Central US
N. Central USS. Central USW. Central US
CanadaFrance
Germany
West EuropeNorth Europe
NorwayUK SouthAustralia
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4CO2 emissions decrease in %
6h
12h
18h
24h (b) Flexible Start optimization for 6B parameters Transformer.","The context provides data on the carbon emissions reduction for different job durations and regions, as well as the impact of the region on emissions. It is stated that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction, greater than 30% in multiple regions, and up to 80% in West US can be found. The question asks for the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context provides information about the energy consumption of different tasks in machine learning models. It states that generative tasks, such as image generation, are more energy-intensive than discriminative tasks. Image generation is listed as one of the most energy- and carbon-intensive tasks, while text classification is listed as the least energy-intensive task.","is_blank","times","[""luccioni2024""]","is_blank","As shown in Figure 1, the most energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image captioning, and image generation.","The context provides information about the energy consumption of different tasks in machine learning models. It states that generative tasks, such as image generation, are more energy-intensive than discriminative tasks. Image generation is listed as one of the most energy- and carbon-intensive tasks, while text classification is listed as the least energy-intensive task."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context discusses the energy consumption of various models in text classification tasks, including sentiment analysis on the Yelp dataset. It is stated that large language models (LLMs) outperform traditional models in sentiment analysis on the Yelp dataset, justifying the energy costs of deploying LLMs. Traditional models are not mentioned to achieve comparable accuracy to large language models in sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context discusses the energy consumption of various models in text classification tasks, including sentiment analysis on the Yelp dataset. It is stated that large language models (LLMs) outperform traditional models in sentiment analysis on the Yelp dataset, justifying the energy costs of deploying LLMs. Traditional models are not mentioned to achieve comparable accuracy to large language models in sentiment analysis benchmarks."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides information about Amazon's renewable energy projects announced as of January 2024, but it does not specify the number of Amazon Renewable Energy Projects announced in the United States specifically.","is_blank","projects","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's renewable energy projects announced as of January 2024, but it does not specify the number of Amazon Renewable Energy Projects announced in the United States specifically."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides information about Amazon's renewable energy projects and investments, including the fact that Amazon aims to procure renewable electricity in the same grids where it consumes electricity. It also mentions that Amazon's energy supply from utilities, combined with the renewable energy it procures globally, means that 100% of the electricity consumed by Amazon's data centers is matched with renewable energy sources.","1","percent","[""amazon2023""]","is_blank","We are proud to have achieved this goal in 2023, seven years early, with 100% of the electricity consumed by Amazon matched with renewable energy sources, up from 90% in 2022.","The context provides information about Amazon's renewable energy projects and investments, including the fact that Amazon aims to procure renewable electricity in the same grids where it consumes electricity. It also mentions that Amazon's energy supply from utilities, combined with the renewable energy it procures globally, means that 100% of the electricity consumed by Amazon's data centers is matched with renewable energy sources."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context discusses the recommendation for reporting energy consumption of AI at the cumulative server level. While it does not explicitly state the measurement level for water consumption, it is reasonable to infer that the same level of measurement is suggested for water consumption as well, given the focus on energy consumption.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","In particular, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.","The context discusses the recommendation for reporting energy consumption of AI at the cumulative server level. While it does not explicitly state the measurement level for water consumption, it is reasonable to infer that the same level of measurement is suggested for water consumption as well, given the focus on energy consumption."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides the electricity consumption measurements for a BERT-base model trained on a single NVIDIA TITAN X GPU (12 GB) in a commodity server. The electricity consumption is measured in watts and is presented in Table 1. The GPU accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","[""https://arxiv.org/abs/2206.06028""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs).","The context provides the electricity consumption measurements for a BERT-base model trained on a single NVIDIA TITAN X GPU (12 GB) in a commodity server. The electricity consumption is measured in watts and is presented in Table 1. The GPU accounts for 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context provides information about the utilization of different GPU types (V100 and A100) for a 65B LLaMA model. It mentions that 4 A100 GPUs are required for any meaningful inferences, but the number of GPUs needed is not explicitly stated for serving a Llama2-70b model at BF16 precision. However, since the 65B LLaMA model requires 4 A100 GPUs and the Llama2-70b model is larger, it can be inferred that serving a Llama2-70b model at BF16 precision would require more than 4 A100 GPUs.","is_blank","NVIDIA A100-80GB GPUs","[""samsi2024""]","is_blank","is_blank","The context provides information about the utilization of different GPU types (V100 and A100) for a 65B LLaMA model. It mentions that 4 A100 GPUs are required for any meaningful inferences, but the number of GPUs needed is not explicitly stated for serving a Llama2-70b model at BF16 precision. However, since the 65B LLaMA model requires 4 A100 GPUs and the Llama2-70b model is larger, it can be inferred that serving a Llama2-70b model at BF16 precision would require more than 4 A100 GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context provides a specific figure (Figure 8) that states the iterative optimization process has led to a 28.5% operational energy footprint reduction over a two-year period (2019 to 2021). This reduction is attributed to various optimizations, including hardware-software optimization.","28.5","percent","[""wu2021a""]","is_blank","Figure 8","The context provides a specific figure (Figure 8) that states the iterative optimization process has led to a 28.5% operational energy footprint reduction over a two-year period (2019 to 2021). This reduction is attributed to various optimizations, including hardware-software optimization."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide specific information about the number of kilometers of fiberoptic cable installed globally to support AI workloads in 2023. However, it does discuss the growth trends in data, model parameters, and system resources for AI, which could indirectly suggest an increase in infrastructure requirements, including fiberoptic cables.","is_blank","kilometers of fiberoptic cable","[""wu2021a""]","[""https://arxiv.org/abs/2111.00364""]","is_blank","The context does not provide specific information about the number of kilometers of fiberoptic cable installed globally to support AI workloads in 2023. However, it does discuss the growth trends in data, model parameters, and system resources for AI, which could indirectly suggest an increase in infrastructure requirements, including fiberoptic cables."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides an estimate for the amortized hardware and energy cost of training GPT-4, which is $40M. However, it also mentions that the acquisition cost of the hardware used to train GPT-4 is $800M. Since the question asks for the upfront hardware acquisition cost, the context supports the answer.","800","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context provides an estimate for the amortized hardware and energy cost of training GPT-4, which is $40M. However, it also mentions that the acquisition cost of the hardware used to train GPT-4 is $800M. Since the question asks for the upfront hardware acquisition cost, the context supports the answer."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context discusses the concept of 'Sustainable AI' and its definition, which includes the use of AI in climate-positive applications as well as improving the sustainability of AI approaches themselves. However, it does not explicitly state that the umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","1","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‚ÄòSustainable AI‚Äô was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context discusses the concept of 'Sustainable AI' and its definition, which includes the use of AI in climate-positive applications as well as improving the sustainability of AI approaches themselves. However, it does not explicitly state that the umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a table (Fig. 13) that shows the projected maximum batch size for fine-tuning a Mixtral model on different GPUs. For the NVIDIA A100-40GB GPU, the projected maximum batch size is 35.","35","samples","[""xia2024""]","is_blank","Fig. 13","The context provides a table (Fig. 13) that shows the projected maximum batch size for fine-tuning a Mixtral model on different GPUs. For the NVIDIA A100-40GB GPU, the projected maximum batch size is 35."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context provides a comparison of energy consumption for the LLM inference server's preemption mechanisms, Recomputation and Swapping, when the server is overloaded. Figure 8 shows that Swapping consistently consumes less energy than Recomputation when the server is overloaded.","1","is_blank","[""chung2025""]","is_blank","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server‚Äôs preemption mechanism is compared.","The context provides a comparison of energy consumption for the LLM inference server's preemption mechanisms, Recomputation and Swapping, when the server is overloaded. Figure 8 shows that Swapping consistently consumes less energy than Recomputation when the server is overloaded."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The Google 2024 environmental report is mentioned in the context, but it does not provide specific information about the percentage increase in GHG emissions since 2019. However, other documents discuss studies about carbon emissions and energy consumption related to AI models, which could be relevant to the question if we were to make an educated guess.","is_blank","percent","[""jegham2025""]","[""https://sustainability.google/reports/google-2024-environmental-report/""]","is_blank","The Google 2024 environmental report is mentioned in the context, but it does not provide specific information about the percentage increase in GHG emissions since 2019. However, other documents discuss studies about carbon emissions and energy consumption related to AI models, which could be relevant to the question if we were to make an educated guess."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context provides a projection that data center electricity consumption will increase from 4.4% of the national total in 2023 to 6.7-12.0% in 2028. This increase is attributed to the growing demands from AI and large language models.","6.7-12.0","percent","[""han2024""]","is_blank","data center electricity consumption to account for 6.7‚Äì12.0% of the national total in 2028, up from 4.4% in 2023.","The context provides a projection that data center electricity consumption will increase from 4.4% of the national total in 2023 to 6.7-12.0% in 2028. This increase is attributed to the growing demands from AI and large language models."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides optimization results for the training of a 6B Parameter Transformer on 256 A100s, showing a decrease in CO2 emissions for the 'Pause and Resume' optimization. However, the exact maximum potential emissions saving is not explicitly stated.","is_blank","percent","[""dodge2022""]","is_blank","Fig. 7 and Fig. 8 in the document 'Measuring the Carbon Intensity of AI in Cloud Instances FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea'","The context provides optimization results for the training of a 6B Parameter Transformer on 256 A100s, showing a decrease in CO2 emissions for the 'Pause and Resume' optimization. However, the exact maximum potential emissions saving is not explicitly stated."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explains that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explains that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The carbon footprint of training the BERT large language model was quantified in a seminal 2019 study as reaching 626,155 pounds of CO2e emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","626,155 pounds of /u1D436/u1D4422 emissions [192]","The carbon footprint of training the BERT large language model was quantified in a seminal 2019 study as reaching 626,155 pounds of CO2e emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides an estimate of the training energy of the full GPT-3 model, which is 1287 MWh. To convert this to MWh to MWh, no conversion is needed as the units are the same. The water consumption footprint is not directly provided in the context, but it is estimated based on the energy consumption and the water usage effectiveness (WUE) of the data centers where GPT-3 was trained.","1287","MWh","[""li2025b""]","is_blank","3.3.1 Training: GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh [29].","The context provides an estimate of the training energy of the full GPT-3 model, which is 1287 MWh. To convert this to MWh to MWh, no conversion is needed as the units are the same. The water consumption footprint is not directly provided in the context, but it is estimated based on the energy consumption and the water usage effectiveness (WUE) of the data centers where GPT-3 was trained."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context discusses the optimization of the Mixture of Experts (MoE) layer for improving the performance of LLM fine-tuning. It is stated that the MoE layer is the most time-consuming layer and a prime target for optimization (ref_id: xia2024).","1","is_blank","[""xia2024""]","is_blank","Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context discusses the optimization of the Mixture of Experts (MoE) layer for improving the performance of LLM fine-tuning. It is stated that the MoE layer is the most time-consuming layer and a prime target for optimization (ref_id: xia2024)."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides information about the maximum batch sizes supported for fine-tuning Mixtral on different GPUs. It is stated that for a NVIDIA A40 GPU with 48 GB memory, the maximum batch size supported for fine-tuning Mixtral is 28 samples.","28","samples","[""xia2024""]","is_blank","Projected maximum batch size of Mixtral for different GPUs: 0 20 40 60 80 100 1200 5 10 15 20 25 30 35 40 A100-40GB A100-80GB A40 H100 bsz=28 bsz=35","The context provides information about the maximum batch sizes supported for fine-tuning Mixtral on different GPUs. It is stated that for a NVIDIA A40 GPU with 48 GB memory, the maximum batch size supported for fine-tuning Mixtral is 28 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context provides information about the performance of CV models with high granularity when trained intercontinentally. It states that intercontinental training slowed performance by only 7% compared to local training for CV models (erben2023, Figure 10).","1","is_blank","[""erben2023""]","is_blank","Figure 10: Multi-cloud performance for CV and NLP.","The context provides information about the performance of CV models with high granularity when trained intercontinentally. It states that intercontinental training slowed performance by only 7% compared to local training for CV models (erben2023, Figure 10)."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide specific information about the land area occupied by new AI data centers globally in 2022. However, it does mention a study about the public health impact of U.S. data centers, but it does not specify the global impact.","is_blank","hectares","[""han2024"", ""luccioni2024""]","is_blank","is_blank","The context does not provide specific information about the land area occupied by new AI data centers globally in 2022. However, it does mention a study about the public health impact of U.S. data centers, but it does not specify the global impact."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides information about the emissions of different models per 1,000 queries. It is stated that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","For instance bert-base-multilingual-uncased-sentiment emits just 0.32g of ùê∂ùëÇ2ùëíùëû per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context provides information about the emissions of different models per 1,000 queries. It is stated that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context provides information about the carbon emissions from training a BERT model in different geographic regions. It mentions that there is a large variation in emissions between the least and most carbon-intensive regions, with the least efficient region emitting 7k grams and the most efficient region emitting 26k grams. However, the context does not provide specific numbers for the approximate range of emissions in thousands of grams for the most and least efficient regions.","is_blank","grams","[""dodge2022""]","[""https://www.facc.tw/facc2022/papers/dodge2022.pdf""]","Figure 1 in the document","The context provides information about the carbon emissions from training a BERT model in different geographic regions. It mentions that there is a large variation in emissions between the least and most carbon-intensive regions, with the least efficient region emitting 7k grams and the most efficient region emitting 26k grams. However, the context does not provide specific numbers for the approximate range of emissions in thousands of grams for the most and least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context provides information about the trend of environmental impact transparency in AI models over time. It mentions that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, and no direct environmental impact data was released during this period. However, it also mentions that from 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. Therefore, it is not clear from the provided context when the practice of directly releasing environmental information peaked before declining.","is_blank","year","[""luccioni2025c""]","is_blank","From 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, and no direct environmental impact data was released during this period. However, from 2019 to 2022, transparency improved and the direct release of environmental information peaked in 2022.","The context provides information about the trend of environmental impact transparency in AI models over time. It mentions that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, and no direct environmental impact data was released during this period. However, it also mentions that from 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. Therefore, it is not clear from the provided context when the practice of directly releasing environmental information peaked before declining."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table comparing the water consumption of various models, including OLMo 60M. The water consumption of OLMo 60M is given as 1.6 kL, which is equivalent to 1.6 thousand liters. The equivalent water usage for one person in the US, in days, can be calculated by dividing the water consumption by the average daily water usage per person in the US. According to the EPA, the average daily water usage per person in the US is 37,000 liters per year, or approximately 100 liters per day. Therefore, 1.6 thousand liters is equivalent to 16 days.","16","days","[""morrison2025""]","is_blank","Table 2: We list the estimated power usage, carbon emissions, and water consumption from training our dense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and a mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We find that the environmental impact is quite high, even for our relatively small models. Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years. (Table 2 from Morrison et al., 2025)","The context provides a table comparing the water consumption of various models, including OLMo 60M. The water consumption of OLMo 60M is given as 1.6 kL, which is equivalent to 1.6 thousand liters. The equivalent water usage for one person in the US, in days, can be calculated by dividing the water consumption by the average daily water usage per person in the US. According to the EPA, the average daily water usage per person in the US is 37,000 liters per year, or approximately 100 liters per day. Therefore, 1.6 thousand liters is equivalent to 16 days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context discusses the water consumption of AI models, specifically mentioning that training the GPT-3 language model in Microsoft's data centers can directly evaporate 700,000 liters of clean freshwater. However, the question asks about the consumption in 2023, and the context does not provide specific information about the year. Therefore, the answer is not directly supported by the context.","is_blank","liters of freshwater","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft‚Äôs state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context discusses the water consumption of AI models, specifically mentioning that training the GPT-3 language model in Microsoft's data centers can directly evaporate 700,000 liters of clean freshwater. However, the question asks about the consumption in 2023, and the context does not provide specific information about the year. Therefore, the answer is not directly supported by the context."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context discusses the cost of fine-tuning LLMs, stating that adding compute resources to accelerate the MoE layers can potentially increase costs.","1","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context discusses the cost of fine-tuning LLMs, stating that adding compute resources to accelerate the MoE layers can potentially increase costs."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese) according to the provided context.","52.76","kWh","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.We list the estimated floating-point operations (zetta = 10 21) of the training process for reference.","The total energy consumption for training the FLM-101B model is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese) according to the provided context."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides a projection that the total public health burden of U.S. data centers could be valued at up to more than $20.9 billion in 2028. This is based on the high-growth scenario considered in a Lawrence Berkeley National Laboratory (LBNL) report.","20900000000","USD","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7‚Äì12.0% in 2028, depending on the growth trajectory of AI adoption. Under the high-growth scenario, the resulting health burden could rival or exceed that of on-road emissions in the largest U.S. state.","The context provides a projection that the total public health burden of U.S. data centers could be valued at up to more than $20.9 billion in 2028. This is based on the high-growth scenario considered in a Lawrence Berkeley National Laboratory (LBNL) report."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is estimated based on the analytical model provided in the context. The cost is calculated by multiplying the throughput cost ($/hr) by the number of hours required for fine-tuning.","32.7","USD","[""xia2024""]","is_blank","Table IV in the document provides the estimated cost of fine-tuning Mixtral on different GPUs with sparse MoE. The cost for the A40-48GB GPU is 32.7.","The cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is estimated based on the analytical model provided in the context. The cost is calculated by multiplying the throughput cost ($/hr) by the number of hours required for fine-tuning."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The maximum batch size for the longest-running MoE layer in the fine-tuned Mixtral model on the NVIDIA A40-48 GB GPU is not explicitly mentioned in the provided context. However, Table III shows the maximum batch size supported by different models and datasets for both dense and sparse fine-tuning. For Mixtral on the CS dataset, the maximum batch size supported is 8 for sparse fine-tuning. Since the question asks for the longest-running MoE layer, we can infer that the batch size for the longest-running MoE layer is also 8.","8","samples","[""xia2024""]","is_blank","Table III","The maximum batch size for the longest-running MoE layer in the fine-tuned Mixtral model on the NVIDIA A40-48 GB GPU is not explicitly mentioned in the provided context. However, Table III shows the maximum batch size supported by different models and datasets for both dense and sparse fine-tuning. For Mixtral on the CS dataset, the maximum batch size supported is 8 for sparse fine-tuning. Since the question asks for the longest-running MoE layer, we can infer that the batch size for the longest-running MoE layer is also 8."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide specific information about the execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. However, it does mention that the cost of fine-tuning a sparse Mixtral model on a different GPU (NVIDIA H100) is $3460. Since the cost is proportional to the time taken for fine-tuning, we can estimate the time taken on the A40-48GB GPU by dividing the cost of the H100 GPU by the cost of the A40-48GB GPU and multiplying by the time taken on the H100 GPU. The cost of the H100 GPU is not provided in the context, but it can be found in Table IV. The cost of the A40-48GB GPU can be calculated by dividing the cost of the H100 GPU by the ratio of their memory capacities, as mentioned in the context.","is_blank","seconds","[""xia2024""]","is_blank","Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate.","The context does not provide specific information about the execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. However, it does mention that the cost of fine-tuning a sparse Mixtral model on a different GPU (NVIDIA H100) is $3460. Since the cost is proportional to the time taken for fine-tuning, we can estimate the time taken on the A40-48GB GPU by dividing the cost of the H100 GPU by the cost of the A40-48GB GPU and multiplying by the time taken on the H100 GPU. The cost of the H100 GPU is not provided in the context, but it can be found in Table IV. The cost of the A40-48GB GPU can be calculated by dividing the cost of the H100 GPU by the ratio of their memory capacities, as mentioned in the context."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context discusses various levels for measuring and estimating energy consumption in data centers, including the cumulative server level. It is suggested that this level is more suitable for measuring overall energy consumption, as it captures the total computation-related power usage. The data center level is also mentioned, but it is noted that it assumes that the data center is exclusively utilized for pre-training by the model provider, which may not always be the case.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context discusses various levels for measuring and estimating energy consumption in data centers, including the cumulative server level. It is suggested that this level is more suitable for measuring overall energy consumption, as it captures the total computation-related power usage. The data center level is also mentioned, but it is noted that it assumes that the data center is exclusively utilized for pre-training by the model provider, which may not always be the case."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context discusses a study estimating the carbon footprint of the BLOOM model, which found that training accounted for half of the model's overall emissions. However, it does not provide a specific percentage of the emissions that training accounted for in the BLOOM model.","is_blank","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account diÔ¨Äerent stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model‚Äôs overall emissions [121].","The context discusses a study estimating the carbon footprint of the BLOOM model, which found that training accounted for half of the model's overall emissions. However, it does not provide a specific percentage of the emissions that training accounted for in the BLOOM model."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information about the inference requirements for the LLaMA model on different GPUs. It is mentioned that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, the specific question asks about the bare minimum number of V100 32GB GPUs, and the context does not provide a clear answer for this specific configuration.","is_blank","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information about the inference requirements for the LLaMA model on different GPUs. It is mentioned that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, the specific question asks about the bare minimum number of V100 32GB GPUs, and the context does not provide a clear answer for this specific configuration."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context does not provide a direct answer to the number of user requests needed to consume a 500ml bottle of water during GPT-3 training. However, it does mention that GPT-3 consumes millions of liters of water for training, and that water consumption varies depending on the location of the data center. Since a 500ml bottle is a small amount compared to the millions of liters mentioned, it is reasonable to assume that a large number of requests would be needed to consume a 500ml bottle of water during training, but the context does not provide a specific number.","is_blank","requests","[""li2025b""]","is_blank","is_blank","The context does not provide a direct answer to the number of user requests needed to consume a 500ml bottle of water during GPT-3 training. However, it does mention that GPT-3 consumes millions of liters of water for training, and that water consumption varies depending on the location of the data center. Since a 500ml bottle is a small amount compared to the millions of liters mentioned, it is reasonable to assume that a large number of requests would be needed to consume a 500ml bottle of water during training, but the context does not provide a specific number."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides information about the carbon emissions of various models, including a 6 billion parameter Transformer, which is partially trained and emits more CO2 than the average US home for a year. The average US home energy use is estimated to be 8.3 metric tons CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context provides information about the carbon emissions of various models, including a 6 billion parameter Transformer, which is partially trained and emits more CO2 than the average US home for a year. The average US home energy use is estimated to be 8.3 metric tons CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not provide information about the JetMoE-8B model's performance on the GSM8k benchmark for grade school math problem-solving.","is_blank","score","[""shen2024""]","is_blank","is_blank","The context does not provide information about the JetMoE-8B model's performance on the GSM8k benchmark for grade school math problem-solving."
