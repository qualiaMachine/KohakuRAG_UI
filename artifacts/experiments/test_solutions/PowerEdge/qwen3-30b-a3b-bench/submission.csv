"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010 to 2014, as reported in multiple sources including Masanet et al. (2020). This figure is directly relevant to the question about the average increase in electricity consumption during that period.","4","percent","[""wu2021b""]","[""https://doi.org/10.1016/j.energy.2020.107705""]","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010 to 2014, as reported in multiple sources including Masanet et al. (2020). This figure is directly relevant to the question about the average increase in electricity consumption during that period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This directly answers the question about the estimated number of cars that could be taken off the road equivalent to the CO2e avoided by the solar farm.","13900","cars","[""amazon2023""]","is_blank","Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This directly answers the question about the estimated number of cars that could be taken off the road equivalent to the CO2e avoided by the solar farm."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This directly answers the question about how many data centers began using recycled water for cooling in 2023.","24","data centers","[""amazon2023""]","is_blank","['In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.']","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This directly answers the question about how many data centers began using recycled water for cooling in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release embodied carbon emissions for its hardware, making it impossible to estimate the emissions from manufacturing GPUs. However, one study (Morrison et al., 2025) assumes an embodied carbon emission of 463 kg CO2eq per GPU based on prior estimates from Luccioni et al. (2023), which is used for analysis. This value is derived from the embodied emissions of an 8x server node (3700 kg CO2eq) divided by 8 GPUs.","463","kg/GPU","[""morrison2025"", ""luccioni2023""]","[""https://www.iclr.cc/"", ""https://arxiv.org/abs/2305.11410""]","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release embodied carbon emissions for its hardware, making it impossible to estimate the emissions from manufacturing GPUs. However, one study (Morrison et al., 2025) assumes an embodied carbon emission of 463 kg CO2eq per GPU based on prior estimates from Luccioni et al. (2023), which is used for analysis. This value is derived from the embodied emissions of an 8x server node (3700 kg CO2eq) divided by 8 GPUs."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the amortized hardware and energy cost for training GPT-4 is $40M, while the total training budget for FLM-101B is $100,000. Dividing $40M by $100,000 gives a factor of 400. This indicates that the estimated amortized training cost of GPT-4 is 400 times greater than the total training budget for FLM-101B.","400","ratio","[""li2025a"", ""cottier2024""]","[""https://arxiv.org/pdf/2501.00001.pdf"", ""https://arxiv.org/pdf/2405.00001.pdf""]","['Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The context states that the amortized hardware and energy cost for training GPT-4 is $40M, while the total training budget for FLM-101B is $100,000. Dividing $40M by $100,000 gives a factor of 400. This indicates that the estimated amortized training cost of GPT-4 is 400 times greater than the total training budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context consistently states that a single passenger round trip from San Francisco to New York is approximately 1.2 tCO2e. Multiple references, including Table 2 and Appendix A, confirm this figure. The value is explicitly mentioned in several snippets from the Patterson et al. (2021) document.","1.2","tCO2e","[""patterson2021""]","is_blank","['To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).']","The context consistently states that a single passenger round trip from San Francisco to New York is approximately 1.2 tCO2e. Multiple references, including Table 2 and Appendix A, confirm this figure. The value is explicitly mentioned in several snippets from the Patterson et al. (2021) document."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context explicitly states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models using four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. This score is reported directly in the text and is the final average performance score achieved by the model.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context explicitly states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models using four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. This score is reported directly in the text and is the final average performance score achieved by the model."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context explicitly states that there is a 'more than 6,750 fold improvement in processor clock speed' between the Intel 4004 (1971) and typical 2021 microprocessors. This directly answers the question about the fold improvement in clock speed.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context explicitly states that there is a 'more than 6,750 fold improvement in processor clock speed' between the Intel 4004 (1971) and typical 2021 microprocessors. This directly answers the question about the fold improvement in clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context explicitly states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations required to train GPT-3. This directly answers the question about the training time.","14.8","days","[""patterson2021"", ""luccioni2023""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","['It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.', 'the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)']","The context explicitly states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations required to train GPT-3. This directly answers the question about the training time."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table with GPU Power Usage in kWh for various models and request frequencies. For the Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is explicitly listed as 0.036 kWh. This data is directly from the SGLang benchmarking on 2400 prompts from ShareGPT.","0.036","kWh","[""morrison2025""]","is_blank","Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.
1 0.16 53.1 0.238 100.58 4.83 bil.","The context provides a table with GPU Power Usage in kWh for various models and request frequencies. For the Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is explicitly listed as 0.036 kWh. This data is directly from the SGLang benchmarking on 2400 prompts from ShareGPT."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This information directly answers the question about the permitted annual emission limits for nitrogen oxides (NOx). The timeframe of January 1, 2023, to December 1, 2024, falls within the period covered by the data, and the emission limits are presented as current and applicable.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This information directly answers the question about the permitted annual emission limits for nitrogen oxides (NOx). The timeframe of January 1, 2023, to December 1, 2024, falls within the period covered by the data, and the emission limits are presented as current and applicable."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context explicitly states that training the FLM-101B model using the growth strategy saved 72% of the time compared to training a 101B model from scratch. This is directly supported by the sentence: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","72","percent","[""li2025a""]","[""https://arxiv.org/abs/2309.03852v3""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that training the FLM-101B model using the growth strategy saved 72% of the time compared to training a 101B model from scratch. This is directly supported by the sentence: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'"
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario. This figure includes the health burden from scope-2 pollutants, which are emissions from fossil fuel combustion for electricity generation. The study projects that this health burden could rival or exceed that of on-road emissions in the largest U.S. state, indicating a significant public health cost tied to data center operations.","1300","deaths","[""han2024""]","is_blank","The overall public health costs could reach more than$20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California... The U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028","The context states that the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028 under the high-growth scenario. This figure includes the health burden from scope-2 pollutants, which are emissions from fossil fuel combustion for electricity generation. The study projects that this health burden could rival or exceed that of on-road emissions in the largest U.S. state, indicating a significant public health cost tied to data center operations."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that their 6 billion parameter transformer model was trained for 8 days but only reached 13% of completion. They estimate that a full training run would take 60 days, as (60/8) * 13.8 MWh equals the full energy consumption. This directly supports the estimate of 60 days for a full training run.","60","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","Dodge et al. (2022) state that their 6 billion parameter transformer model was trained for 8 days but only reached 13% of completion. They estimate that a full training run would take 60 days, as (60/8) * 13.8 MWh equals the full energy consumption. This directly supports the estimate of 60 days for a full training run."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide a direct comparison of GPU energy consumption for 1,000 queries among the listed models in the appendix of a 2025 study. While several documents mention energy consumption during inference and reference specific models like Qwen 2.5 7B, Qwen 2.5 72B, and others, none explicitly list the models from the appendix or provide a ranked comparison of GPU energy consumption for 1,000 queries. Therefore, the information required to determine which model has the highest GPU energy consumption is not available.","Unable to answer with confidence based on the provided documents.","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a direct comparison of GPU energy consumption for 1,000 queries among the listed models in the appendix of a 2025 study. While several documents mention energy consumption during inference and reference specific models like Qwen 2.5 7B, Qwen 2.5 72B, and others, none explicitly list the models from the appendix or provide a ranked comparison of GPU energy consumption for 1,000 queries. Therefore, the information required to determine which model has the highest GPU energy consumption is not available."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in the PREFACE section of the document.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","['The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.']","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in the PREFACE section of the document."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN’s Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is explicitly mentioned in multiple context snippets, confirming the percentage.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","['The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled']","The UN’s Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is explicitly mentioned in multiple context snippets, confirming the percentage."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides the energy consumption for training the BLOOMz-7B model, which is a fine-tuned version of the original BLOOM model. According to Table 5 from the Luccioni et al. (2024) paper, the training energy for BLOOMz-7B is 51,686 kWh. Since the BLOOMz models are fine-tuned versions of the original BLOOM family, and the training energy for the original BLOOM project is not explicitly stated, the closest available value for the original BLOOM model's training energy is inferred from the BLOOMz-7B training energy. However, the context does not provide the total energy for the original BLOOM model's pre-training, only the fine-tuning energy for BLOOMz-7B. Therefore, the energy consumption for pre-training the BLOOM model cannot be determined with certainty from the provided context.","Unable to answer with confidence based on the provided documents.","MWh","[""luccioni2024""]","is_blank","['BLOOMz-7B Training energy (kWh) 51,686']","The context provides the energy consumption for training the BLOOMz-7B model, which is a fine-tuned version of the original BLOOM model. According to Table 5 from the Luccioni et al. (2024) paper, the training energy for BLOOMz-7B is 51,686 kWh. Since the BLOOMz models are fine-tuned versions of the original BLOOM family, and the training energy for the original BLOOM project is not explicitly stated, the closest available value for the original BLOOM model's training energy is inferred from the BLOOMz-7B training energy. However, the context does not provide the total energy for the original BLOOM model's pre-training, only the fine-tuning energy for BLOOMz-7B. Therefore, the energy consumption for pre-training the BLOOM model cannot be determined with certainty from the provided context."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer model has 1500 billion parameters but only 0.1% of the model is activated per token. This is explicitly mentioned in the table under 'Percent of model activated on every token' for the Switch Transformer row.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context states that the Switch Transformer model has 1500 billion parameters but only 0.1% of the model is activated per token. This is explicitly mentioned in the table under 'Percent of model activated on every token' for the Switch Transformer row."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B uses a Sparsely-gated Mixture-of-Experts (SMoE) architecture with 8 experts and a top-k value of 2 for every layer. This is explicitly mentioned in Table 1 and the surrounding text, which confirms that the number of experts in each MoE layer is 8.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Table 1: JetMoE-8B hyperparameters. ... Nexperts: 8","The context states that JetMoE-8B uses a Sparsely-gated Mixture-of-Experts (SMoE) architecture with 8 experts and a top-k value of 2 for every layer. This is explicitly mentioned in Table 1 and the surrounding text, which confirms that the number of experts in each MoE layer is 8."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdown for BlackMamba fine-tuning on different batch sizes using a NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for BlackMamba with batch sizes of 1, 30, and 84. The total execution time for a batch size of 30 is explicitly shown as 1.5 seconds in the figure, which includes forward, backward, and optimizer stages. This value is derived from the graph's y-axis, which represents execution time in seconds.","1.5","second","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown (seconds) for BlackMamba with batch size = 30: total execution time is 1.5 seconds.","The context provides execution time breakdown for BlackMamba fine-tuning on different batch sizes using a NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for BlackMamba with batch sizes of 1, 30, and 84. The total execution time for a batch size of 30 is explicitly shown as 1.5 seconds in the figure, which includes forward, backward, and optimizer stages. This value is derived from the graph's y-axis, which represents execution time in seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated for the English portion and 24.54 zettaFLOPs for the Chinese portion. This directly answers the question about the cost required to train the English portion of the model.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated for the English portion and 24.54 zettaFLOPs for the Chinese portion. This directly answers the question about the cost required to train the English portion of the model."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context from the paper by Khan et al. (2025) describes an experimental setup for energy-efficient local inference in financial sentiment classification. It explicitly states that the hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz, supported by 16.0 GB of installed memory. This processor is specifically mentioned in the 'Hardware and Software Setting' section of the experimental setup.","11th Gen Intel(R) Core(TM) i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context from the paper by Khan et al. (2025) describes an experimental setup for energy-efficient local inference in financial sentiment classification. It explicitly states that the hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz, supported by 16.0 GB of installed memory. This processor is specifically mentioned in the 'Hardware and Software Setting' section of the experimental setup."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context describes a study titled 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' by Luccioni, Jernite, and Strubell (2024), which systematically compares the ongoing inference cost of various categories of machine learning systems. The study explicitly covers both task-specific models (finetuned for a single task) and general-purpose models (trained for multiple tasks). However, the provided context does not specify the exact number of different machine learning models sampled and analyzed.","Unable to answer with confidence based on the provided documents.","models","[""luccioni2024"", ""luccioni2025b"", ""zschache2025""]","[""https://doi.org/10.1145/3630106.3658542"", ""https://arxiv.org/abs/2311.16863"", ""https://arxiv.org/abs/2412.00329""]","['We propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks).']","The context describes a study titled 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' by Luccioni, Jernite, and Strubell (2024), which systematically compares the ongoing inference cost of various categories of machine learning systems. The study explicitly covers both task-specific models (finetuned for a single task) and general-purpose models (trained for multiple tasks). However, the provided context does not specify the exact number of different machine learning models sampled and analyzed."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context explicitly states that increasing GPU utilization up to 80% for Language Model (LM) training results in the overall carbon footprint decreasing by 3×. This direct statement from the provided documents supports the answer.","3","multiplier","[""wu2021a""]","is_blank","['Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.']","The context explicitly states that increasing GPU utilization up to 80% for Language Model (LM) training results in the overall carbon footprint decreasing by 3×. This direct statement from the provided documents supports the answer."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, based on a log-normal distribution with a 90% credible interval. This range is explicitly mentioned in multiple sections, including the estimation of the multiplicative factor applied to the final training run compute.","1.2x to 4x","multiplier","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, based on a log-normal distribution with a 90% credible interval. This range is explicitly mentioned in multiple sections, including the estimation of the multiplicative factor applied to the final training run compute."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that a 6 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh of energy. A full training run would take 60 days, so the estimated total energy consumption is calculated as (60/8) * 13.8 MWh = 103.5 MWh, which is approximately 103,500 kWh. This estimate is explicitly mentioned in the text.","103.5","MWh","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3559684""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that a 6 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh of energy. A full training run would take 60 days, so the estimated total energy consumption is calculated as (60/8) * 13.8 MWh = 103.5 MWh, which is approximately 103,500 kWh. This estimate is explicitly mentioned in the text."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context explicitly states that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle describes how increased efficiency can paradoxically lead to higher overall consumption, as cost savings from efficiency spur greater demand for AI functionalities, resulting in more resource-intensive upgrades and usage. The paper argues that rebound effects, driven by this paradox, undermine the assumption that efficiency alone ensures environmental gains.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","['This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.', 'Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm.', 'Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].']","The context explicitly states that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle describes how increased efficiency can paradoxically lead to higher overall consumption, as cost savings from efficiency spur greater demand for AI functionalities, resulting in more resource-intensive upgrades and usage. The paper argues that rebound effects, driven by this paradox, undermine the assumption that efficiency alone ensures environmental gains."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is directly provided in multiple sections of the document, including the abstract and the appendix, and is consistent across references.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","['More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.', 'Even considering the lower estimate, the combined scope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 – 6.6 billion cubic meters in 2027.']","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is directly provided in multiple sections of the document, including the abstract and the appendix, and is consistent across references."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states that Red AI is on the rise despite the well-known diminishing returns of increased cost. This directly contradicts the claim that Red AI is on the decline. Therefore, the statement in the question is false.","0","is_blank","[""schwartz2019""]","is_blank","['Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).']","The context explicitly states that Red AI is on the rise despite the well-known diminishing returns of increased cost. This directly contradicts the claim that Red AI is on the decline. Therefore, the statement in the question is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This is directly mentioned in multiple instances within the provided text.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This is directly mentioned in multiple instances within the provided text."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for utilization and efficiency improvements. This directly contradicts the claim that a majority of workflows utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","['A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.']","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for utilization and efficiency improvements. This directly contradicts the claim that a majority of workflows utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This figure is directly mentioned in multiple sources, including the document with ref_id=li2025b and ref_id=jegham2025, both of which confirm the same value.","1287","MWh","[""li2025b"", ""jegham2025""]","[""https://arxiv.org/pdf/2505.09598v6.pdf"", ""https://arxiv.org/pdf/2505.09598v6.pdf""]","['GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].', 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.']","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This figure is directly mentioned in multiple sources, including the document with ref_id=li2025b and ref_id=jegham2025, both of which confirm the same value."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context identifies the AI Energy Score as a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This project provides a standardized methodology for comparing models across different tasks and can be adapted for specific contexts and datasets.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","['These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.', 'For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.']","The context identifies the AI Energy Score as a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This project provides a standardized methodology for comparing models across different tasks and can be adapted for specific contexts and datasets."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a kernel-level execution time breakdown for the MoE layer in BlackMamba, as shown in Fig. 6. The figure displays execution times in microseconds for various kernels, including matmul(w1), gelu, matmul(w2), elementwise_mult, top_k, sigmoid, and matmul(router). Among these, matmul(w1) and matmul(w2) are the largest components, with matmul(w1) reaching approximately 1600 microseconds for a batch size of 30. This indicates that matrix multiplication operations dominate the MoE layer's execution time.","1600","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context provides a kernel-level execution time breakdown for the MoE layer in BlackMamba, as shown in Fig. 6. The figure displays execution times in microseconds for various kernels, including matmul(w1), gelu, matmul(w2), elementwise_mult, top_k, sigmoid, and matmul(router). Among these, matmul(w1) and matmul(w2) are the largest components, with matmul(w1) reaching approximately 1600 microseconds for a batch size of 30. This indicates that matrix multiplication operations dominate the MoE layer's execution time."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context explicitly states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation per input token. This is confirmed in the section discussing hyperparameters, where it says: 'we set the same number of experts to 8 and top-k to 2 for every layer.'","2","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","we set the same number of experts to 8 and top-k to 2 for every layer.","The context explicitly states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation per input token. This is confirmed in the section discussing hyperparameters, where it says: 'we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, from 2012 to 2018. This directly supports the claim in the question about a 200,000x increase over the same period.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, from 2012 to 2018. This directly supports the claim in the question about a 200,000x increase over the same period."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context explicitly states that global carbon emissions dropped by 6.4% in 2020, primarily due to reduced vehicle transportation in the US during the COVID-19 pandemic. This information is directly supported by the cited reference [ref_id=wu2021b].","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context explicitly states that global carbon emissions dropped by 6.4% in 2020, primarily due to reduced vehicle transportation in the US during the COVID-19 pandemic. This information is directly supported by the cited reference [ref_id=wu2021b]."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions powered by 100% renewable energy in 2023.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions powered by 100% renewable energy in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate the age of the field in 2025, subtract 1956 from 2025, resulting in 69 years. This is supported by the explicit mention of the 1956 workshop as the founding event.","69","years","[""stone2022""]","[""https://ai100.stanford.edu""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.']","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate the age of the field in 2025, subtract 1956 from 2025, resulting in 69 years. This is supported by the explicit mention of the 1956 workshop as the founding event."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which estimated emissions from a neural architecture search (NAS) process used to identify optimized model architectures. This NAS process is described as a large-scale, infrequently performed procedure that is not representative of typical AI training workloads. The context explicitly states that NAS is meant to be performed much less frequently than average AI model training due to its high resource demands and the reusability of its results.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","['Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs.']","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which estimated emissions from a neural architecture search (NAS) process used to identify optimized model architectures. This NAS process is described as a large-scale, infrequently performed procedure that is not representative of typical AI training workloads. The context explicitly states that NAS is meant to be performed much less frequently than average AI model training due to its high resource demands and the reusability of its results."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average Time Per Output Token (TPOT) of 100 ms for the Llama 3.1 8B model lands on the Pareto frontier at a point where the average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question about the percentage decrease in energy use.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average Time Per Output Token (TPOT) of 100 ms for the Llama 3.1 8B model lands on the Pareto frontier at a point where the average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question about the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that BlackMamba is inadequate for fine-tuning on the GSM8K dataset, which implies that fine-tuning with BlackMamba on GSM8K is not feasible or effective. The maximum batch size for BlackMamba-S (sparse setup) on GSM8K is listed in Table III as 8. However, since the model is deemed inadequate for this task, the practical maximum batch size for successful fine-tuning is not achievable. Therefore, despite the table indicating a value, the context suggests the model cannot be fine-tuned effectively on this dataset.","8","samples","[""xia2024""]","is_blank","TABLE III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE. BlackMamba-S on GS: 8","The context states that BlackMamba is inadequate for fine-tuning on the GSM8K dataset, which implies that fine-tuning with BlackMamba on GSM8K is not feasible or effective. The maximum batch size for BlackMamba-S (sparse setup) on GSM8K is listed in Table III as 8. However, since the model is deemed inadequate for this task, the practical maximum batch size for successful fine-tuning is not achievable. Therefore, despite the table indicating a value, the context suggests the model cannot be fine-tuned effectively on this dataset."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context explicitly states that Amazon held 1.3 GW of energy storage capacity as of 2023, up from 445 MW in 2022. This information is directly mentioned in multiple sections of the document.","1.3","GW","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","['We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.', 'In 2023, Amazon announced plans to support the repurposing of a previously polluted Maryland coal mine into a solar farm.', 'Energy Storage Solutions: We invest in energy storage to collect and save renewable energy for use when other energy sources may be unavailable—such as at night or during periods of high demand—and to help improve grid stability.']","The context explicitly states that Amazon held 1.3 GW of energy storage capacity as of 2023, up from 445 MW in 2022. This information is directly mentioned in multiple sections of the document."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This direct comparison is provided in the section discussing the estimated 2025 annual carbon footprint of GPT-4o inference.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This direct comparison is provided in the section discussing the estimated 2025 annual carbon footprint of GPT-4o inference."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","","","percent","[]","is_blank","is_blank",""
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides the average global PUE for data centers in 2023, which is 1.58. Although the context mentions AI-dedicated data centers, it does not specify a separate PUE value for such centers. Therefore, the global average PUE of 1.58 is the best available figure for the year 2023.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the average global PUE for data centers in 2023, which is 1.58. Although the context mentions AI-dedicated data centers, it does not specify a separate PUE value for such centers. Therefore, the global average PUE of 1.58 is the best available figure for the year 2023."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is directly mentioned in multiple snippets, including the abstract and the description of the model's architecture.","2B","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","['JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.']","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is directly mentioned in multiple snippets, including the abstract and the description of the model's architecture."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table (ref_id=luccioni2025c) that lists the GHG emissions for pre-training the Llama 7B model as 14 tCO2e. This value is explicitly stated in the table under the 'GHG Emissions (tCO2e)' column for the Llama 7B model.","14","tCO2e","[""luccioni2025c""]","[""https://arxiv.org/pdf/2504.06307v1""]","Llama 7B 63 Meta 356 14","The context provides a table (ref_id=luccioni2025c) that lists the GHG emissions for pre-training the Llama 7B model as 14 tCO2e. This value is explicitly stated in the table under the 'GHG Emissions (tCO2e)' column for the Llama 7B model."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in the U.S. fleet for 2022 and 2023. In 2022, the U.S. fleet had more than 2,600 electric delivery vans, and in 2023, it increased to 11,800. Adding these numbers gives a total increase of 9,200 electric delivery vans across 2022 and 2023.","9200","electric delivery vans","[""amazon2023""]","is_blank","['United States • Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.']","The context provides the number of electric delivery vans in the U.S. fleet for 2022 and 2023. In 2022, the U.S. fleet had more than 2,600 electric delivery vans, and in 2023, it increased to 11,800. Adding these numbers gives a total increase of 9,200 electric delivery vans across 2022 and 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly confirms that cooling in data centers contributes to GHG emissions as part of the operational impacts.","1","is_blank","[""morrison2025""]","is_blank","['Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.']","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly confirms that cooling in data centers contributes to GHG emissions as part of the operational impacts."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for the o3 model across three prompt sizes. For long prompts (10k input-1.5k output), the energy consumption is listed as 12.222 ± 1.082 Wh in Table 4. This directly answers the question about the o3 model's energy consumption for a long prompt.","12.222","Wh","[""jegham2025""]","is_blank","o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context provides energy consumption values for the o3 model across three prompt sizes. For long prompts (10k input-1.5k output), the energy consumption is listed as 12.222 ± 1.082 Wh in Table 4. This directly answers the question about the o3 model's energy consumption for a long prompt."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. This workshop, known as the Dartmouth Summer Research Project on Artificial Intelligence, was organized by John McCarthy and marked the formal beginning of AI as a distinct field of research.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.']","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. This workshop, known as the Dartmouth Summer Research Project on Artificial Intelligence, was organized by John McCarthy and marked the formal beginning of AI as a distinct field of research."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the document discusses WUE in general terms and provides WUE values for Microsoft's data centers, it does not mention Google's WUE or any data specific to Google's AI data centers in 2024.","is_blank","WUE","[""li2025b""]","is_blank","['is_blank']","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the document discusses WUE in general terms and provides WUE values for Microsoft's data centers, it does not mention Google's WUE or any data specific to Google's AI data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, according to the International Energy Agency. This directly supports the claim in the question.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","['Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].']","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, according to the International Energy Agency. This directly supports the claim in the question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at the specified generation length.","3-4","joules per token","[""samsi2024""]","is_blank","['For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.']","The context states that for a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at the specified generation length."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduces the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization.","15","percent","[""wu2021a""]","is_blank","['By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.']","The context explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduces the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the claim that AI can reduce global GHG emissions by 5-10% is based on a 2021 BCG report and a subsequent 2023 Google-commissioned BCG report. However, the reasoning behind the estimate is unclear, and the underlying calculations are not detailed. The context explicitly states that applying observations from individual projects to global emissions lacks scientific grounding and that rigorous calculation of avoided emissions requires counterfactual scenarios and rebound effect accounting, which are not provided. Therefore, the claim is not supported by clear, publicly available calculations or sound scientific grounding.","0","is_blank","[""luccioni2025c""]","is_blank","['The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding— in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T L.148050 or WBCSD guidance on avoided emissions51.']","The context indicates that the claim that AI can reduce global GHG emissions by 5-10% is based on a 2021 BCG report and a subsequent 2023 Google-commissioned BCG report. However, the reasoning behind the estimate is unclear, and the underlying calculations are not detailed. The context explicitly states that applying observations from individual projects to global emissions lacks scientific grounding and that rigorous calculation of avoided emissions requires counterfactual scenarios and rebound effect accounting, which are not provided. Therefore, the claim is not supported by clear, publicly available calculations or sound scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context explicitly states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This is directly supported by multiple references, including [ref_id=patterson2021], which highlights this energy efficiency as a key opportunity.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context explicitly states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This is directly supported by multiple references, including [ref_id=patterson2021], which highlights this energy efficiency as a key opportunity."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly answers the question about the cost of training AI2's Grover on 256 TPU chips for two weeks.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly answers the question about the cost of training AI2's Grover on 256 TPU chips for two weeks."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1, using an NVIDIA A40-48GB GPU. This directly answers the question about the percentage of running time taken by the optimizer stage under the specified conditions.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1, using an NVIDIA A40-48GB GPU. This directly answers the question about the percentage of running time taken by the optimizer stage under the specified conditions."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate handles approximately 1 billion queries per day, based on 100 billion words translated daily with an average query length of 100 words. Additionally, the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) is given as 0.083 kWh per 1,000 queries. Using this, the daily energy consumption is calculated as (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh, which is equivalent to 83 MWh.","83","MWh","[""luccioni2024""]","[""https://blog.google/products/translate/ten-years-of-google-translate/""]","Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day. ... For comparison, charging the average smartphone requires 0.022 kWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full smartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as 522 smartphone charges (11.49 kWh), or around half a charge per image generation 5, although there is also a large variation between image generation models, depending on the size of image that they generate.","The context states that Google Translate handles approximately 1 billion queries per day, based on 100 billion words translated daily with an average query length of 100 words. Additionally, the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) is given as 0.083 kWh per 1,000 queries. Using this, the daily energy consumption is calculated as (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh, which is equivalent to 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average data center PUE globally in 2023 was 1.58, as cited in reference [ref_id=ebert2024]. This information is directly provided in the text and is consistent with the question about the average global data center PUE in 2023.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average data center PUE globally in 2023 was 1.58, as cited in reference [ref_id=ebert2024]. This information is directly provided in the text and is consistent with the question about the average global data center PUE in 2023."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. While there are references to Microsoft's climate goals and oil contracts, and mentions of renewable energy projects by Amazon, no specific number of wind turbines contracted by Microsoft for Azure AI clusters is stated.","is_blank","wind turbines","[""luccioni2025a"", ""luccioni2025b"", ""luccioni2025c"", ""dodge2022"", ""amazon2023""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/"", ""https://npr.org/2024/07/16/1259273343/ai-brings-soaring-emissions-for-google-and-microsoft-a-major-contributor-to-climate-change"", ""https://azure.microsoft.com/en-us/global-infrastructure/sustainability/#overview"", ""https://www.amazon.com/sustainability-report-2023""]","is_blank","The provided context does not contain any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. While there are references to Microsoft's climate goals and oil contracts, and mentions of renewable energy projects by Amazon, no specific number of wind turbines contracted by Microsoft for Azure AI clusters is stated."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs for selected frontier models. Specifically, Gemini Ultra has the highest fraction at 49%, which is noted as unusually high among frontier models. This directly answers the question about the percentage of cost attributed to R&D staff for Gemini Ultra.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs for selected frontier models. Specifically, Gemini Ultra has the highest fraction at 49%, which is noted as unusually high among frontier models. This directly answers the question about the percentage of cost attributed to R&D staff for Gemini Ultra."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the Study Panel launched in mid-fall 2015 was a seventeen-member Study Panel, comprised of experts in AI and AI-savvy scholars from various disciplines. This refers to the inaugural 2015 Study Panel of the One Hundred Year Study on AI.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","['The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.']","The context explicitly states that the Study Panel launched in mid-fall 2015 was a seventeen-member Study Panel, comprised of experts in AI and AI-savvy scholars from various disciplines. This refers to the inaugural 2015 Study Panel of the One Hundred Year Study on AI."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to its manufacturing.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context shows that while models with more parameters generally consume more energy, this is not always the case. For example, the Phi-3 Small model (7B parameters) can consume less energy than the Phi-3 Mini (3.8B parameters) due to more efficient attention mechanisms (GQA vs. MHA), which reduce memory usage and allow larger batch sizes for energy amortization. This demonstrates that energy consumption during inference depends on architectural choices and efficiency, not solely on the number of parameters.","0","is_blank","[""chung2025"", ""zschache2025""]","[""https://ml.energy/leaderboard"", ""https://arxiv.org/abs/2508.14170v1""]","['Memory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.', 'Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.']","The context shows that while models with more parameters generally consume more energy, this is not always the case. For example, the Phi-3 Small model (7B parameters) can consume less energy than the Phi-3 Mini (3.8B parameters) due to more efficient attention mechanisms (GQA vs. MHA), which reduce memory usage and allow larger batch sizes for energy amortization. This demonstrates that energy consumption during inference depends on architectural choices and efficiency, not solely on the number of parameters."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the claim in the question. The panel emphasized that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","['Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.']","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the claim in the question. The panel emphasized that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context discusses the environmental impact of training various language models, including CO2 emissions, but does not provide specific data on OpenAI's API requests in January 2024. While some models like OLMo and Llama are analyzed for emissions, no information is given about OpenAI's emissions during that period.","Unable to answer with confidence based on the provided documents.","tCO2e","[""morrison2025"", ""luccioni2025c"", ""dodge2022""]","[""https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf"", ""https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.""]","is_blank","The context discusses the environmental impact of training various language models, including CO2 emissions, but does not provide specific data on OpenAI's API requests in January 2024. While some models like OLMo and Llama are analyzed for emissions, no information is given about OpenAI's emissions during that period."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context explicitly states that Meta reports the Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This figure is directly provided in the text and compared to the 'five cars' benchmark.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.","The context explicitly states that Meta reports the Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This figure is directly provided in the text and compared to the 'five cars' benchmark."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5-year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. This is directly supported by multiple references in the text, including Figure 2(d) and the key takeaways section.","2.9","multiplier","[""wu2021a""]","is_blank","['Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.', 'The explosive growth in AI use cases has driven 2.9× and 2.5× capacity increases for AI training and inference at Facebook over the recent 18 months, respectively.', 'The explosive growth in AI use cases has driven 2.9× and 2.5× capacity increases for AI training and inference at Facebook over the recent 18 months, respectively.']","The context explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5-year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. This is directly supported by multiple references in the text, including Figure 2(d) and the key takeaways section."
"q079","How many miles is the Earth from the Sun?","The provided context does not contain any information about the distance between the Earth and the Sun. The documents discuss topics such as AI model training energy consumption, data center electricity usage, carbon emissions, and travel distances between cities, but none mention the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the distance between the Earth and the Sun. The documents discuss topics such as AI model training energy consumption, data center electricity usage, carbon emissions, and travel distances between cities, but none mention the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that the AlphaGo program defeated the human Go champion. Multiple snippets confirm this, including direct references to AlphaGo beating the human champion in a five-game match and the program's success being attributed to reinforcement learning and machine learning algorithms.","1","is_blank","[""stone2022"", ""stone2022"", ""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","['AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning.', 'The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.', 'the AlphaGo program that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.']","The context explicitly states that the AlphaGo program defeated the human Go champion. Multiple snippets confirm this, including direct references to AlphaGo beating the human champion in a five-game match and the program's success being attributed to reinforcement learning and machine learning algorithms."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly describes continuous batching as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, thereby improving GPU utilization. This is directly stated in multiple references, including Fernandez et al. (2025) and Jegham et al. (2025), which confirm that continuous batching mitigates GPU under-utilization during inference by replacing finished requests with new ones.","Continuous batching","is_blank","[""fernandez2025"", ""jegham2025""]","is_blank","['Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).', 'AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency [52].']","The context explicitly describes continuous batching as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, thereby improving GPU utilization. This is directly stated in multiple references, including Fernandez et al. (2025) and Jegham et al. (2025), which confirm that continuous batching mitigates GPU under-utilization during inference by replacing finished requests with new ones."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context explicitly states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This directly answers the question about the total H100 GPU hours required for the alignment process.","60","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","The entire alignment process takes 60 H100 GPU hours.","The context explicitly states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This directly answers the question about the total H100 GPU hours required for the alignment process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context shows that for a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total price of $2.699, while InferSave selected g4dn.xlarge with a total price of $2.13. The cost difference is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This percentage is explicitly mentioned in the text as the cost increase for g6e.xlarge compared to g4dn.xlarge under the 100 TPS SLO.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context shows that for a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total price of $2.699, while InferSave selected g4dn.xlarge with a total price of $2.13. The cost difference is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This percentage is explicitly mentioned in the text as the cost increase for g6e.xlarge compared to g4dn.xlarge under the 100 TPS SLO."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This information is directly provided in multiple instances within the text.","1594","g CO2eq","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3637343.3637352""]","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This information is directly provided in multiple instances within the text."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context explicitly states that GPU energy usage for 1,000 inference queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This range is directly supported by the cited document (ref_id=luccioni2025c), which references Tables 1 and 2 in the Appendix for more information.","0.06 Wh to over 3,426 Wh","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context explicitly states that GPU energy usage for 1,000 inference queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This range is directly supported by the cited document (ref_id=luccioni2025c), which references Tables 1 and 2 in the Appendix for more information."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. This directly contradicts the idea that a universal, one-size-fits-all approach can be developed. The authors emphasize the need for context-specific, integrated approaches that consider both ethical and environmental dimensions.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sus tainable.","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. This directly contradicts the idea that a universal, one-size-fits-all approach can be developed. The authors emphasize the need for context-specific, integrated approaches that consider both ethical and environmental dimensions."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited in [USE21]. This value is directly provided in the document and matches the question's request for the U.S. average mix in 2021.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited in [USE21]. This value is directly provided in the document and matches the question's request for the U.S. average mix in 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context explicitly identifies Hivemind as a PyTorch-based framework designed for decentralized, distributed training on spot instances. It enables training across multiple clouds and continents by handling peer dropouts and reducing communication frequency through techniques like aggressive quantization and delayed parameter updates. The framework is specifically highlighted as enabling decentralized, geo-distributed training with resilience to interruptions.","Hivemind","is_blank","[""erben2023""]","is_blank","['Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.', 'The advantage of Hivemind for geo-distributed training comes from cumulating different techniques, such as Delayed Parameter Updates [36], big-batch training [44] and aggressive communication quantization [16]. All of these combined reduce time and frequency of the communication rounds, which in turn makes training on heterogeneous devices and low-bandwidth networks possible.']","The context explicitly identifies Hivemind as a PyTorch-based framework designed for decentralized, distributed training on spot instances. It enables training across multiple clouds and continents by handling peer dropouts and reducing communication frequency through techniques like aggressive quantization and delayed parameter updates. The framework is specifically highlighted as enabling decentralized, geo-distributed training with resilience to interruptions."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly proposes expanding the concept of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. This expanded view is referred to as 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems, including their societal impacts, ethical considerations, and environmental footprint.","social transparency","is_blank","[""luccioni2025b""]","is_blank","['In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].', 'Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.']","The context explicitly proposes expanding the concept of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. This expanded view is referred to as 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems, including their societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This indicates that the linear model with sentence embeddings performed the best in the classification experiments on German public administration texts.","linear model with sentence embeddings","is_blank","[""zschache2025"", ""zschache2025"", ""zschache2025""]","is_blank","['Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.', 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.', 'For clarity, we restricted the visualization to the models with the three highest accuracy scores and included the linear model with sentence embeddings for comparison (see Tables B4 and B5 for details).']","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This indicates that the linear model with sentence embeddings performed the best in the classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This system was designed to validate the theory of model-attention disaggregation, which is the core innovation of the paper.","Lamina","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814v2""]","['To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.']","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This system was designed to validate the theory of model-attention disaggregation, which is the core innovation of the paper."
"q093","How many parameters does the largest T5 model have?","The context states that the OLMo series of transformer language models ranges in size from 20 million to 13 billion active parameters. This indicates that the largest model in this series has 13 billion parameters. Although other models like T5 are mentioned, no specific parameter count for T5 is provided in the context. Therefore, based on the provided information, the largest model referenced has 13 billion parameters.","13 billion","parameters","[""morrison2025""]","[""https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf"", ""https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.html""]","['ranging in size from 20 million to 13 billion active parameters']","The context states that the OLMo series of transformer language models ranges in size from 20 million to 13 billion active parameters. This indicates that the largest model in this series has 13 billion parameters. Although other models like T5 are mentioned, no specific parameter count for T5 is provided in the context. Therefore, based on the provided information, the largest model referenced has 13 billion parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters. This is confirmed in multiple sections, including the abstract and the description of the model's architecture and performance. The parameter count is directly mentioned in the text as 'JetMoE-8B has 8B parameters'.","8B","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters. This is confirmed in multiple sections, including the abstract and the description of the model's architecture and performance. The parameter count is directly mentioned in the text as 'JetMoE-8B has 8B parameters'."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context explicitly states that the company’s data center water consumption increased by ∼20% from 2021 to 2022. This information is directly provided in multiple references, including [ref_id=li2025b] and [ref_id=luccioni2025a], both of which confirm the 20% increase for Google during that period.","20","percent","[""li2025b"", ""luccioni2025a""]","[""https://arxiv.org/pdf/2304.03271.pdf"", ""https://doi.org/10.1145/3690737.3690745""]","['the company’s data center water consumption increased by∼20% from 2021 to 2022', 'Google observed a 20% uptick in the same period']","The context explicitly states that the company’s data center water consumption increased by ∼20% from 2021 to 2022. This information is directly provided in multiple references, including [ref_id=li2025b] and [ref_id=luccioni2025a], both of which confirm the 20% increase for Google during that period."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I of the document [ref_id=khan2025], which directly matches the metric described in the question. This definition is explicitly stated and consistent across the provided references.","Carbon Intensity","is_blank","[""khan2025""]","[""https://ml.energy/leaderboard""]","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I of the document [ref_id=khan2025], which directly matches the metric described in the question. This definition is explicitly stated and consistent across the provided references."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides the FLOPs utilization percentage for the final growth stage of FLM-101B training, which is the 101B stage. According to Table 2, the FLOPs utilization for the 101B stage is 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88%","The context provides the FLOPs utilization percentage for the final growth stage of FLM-101B training, which is the 101B stage. According to Table 2, the FLOPs utilization for the 101B stage is 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context provides multiple estimates for the cost of training GPT-4. The most relevant estimate for amortized hardware CapEx + energy cost is $40 million, as stated in multiple sources including Cottier et al. (2024). This figure is explicitly labeled as the amortized cost, which aligns with the question's focus on amortized training costs.","40000000","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context provides multiple estimates for the cost of training GPT-4. The most relevant estimate for amortized hardware CapEx + energy cost is $40 million, as stated in multiple sources including Cottier et al. (2024). This figure is explicitly labeled as the amortized cost, which aligns with the question's focus on amortized training costs."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800×. This is explicitly mentioned in multiple references, including Figure 7 and the accompanying text, which detail the cumulative effect of these optimizations.","800","multiplier","[""wu2021a""]","is_blank","['For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.', 'In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800 ×.']","The context states that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800×. This is explicitly mentioned in multiple references, including Figure 7 and the accompanying text, which detail the cumulative effect of these optimizations."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when training was spread across four continents (C-8 experiment), the NLP performance dropped by 41% compared to the fully local experiment (A-8). This means the throughput achieved was 59% of the local throughput (100% - 41% = 59%). Therefore, the fraction of local throughput achieved for NLP is 59%.","59","multiplier","[""erben2023""]","is_blank","['However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).']","The context states that when training was spread across four continents (C-8 experiment), the NLP performance dropped by 41% compared to the fully local experiment (A-8). This means the throughput achieved was 59% of the local throughput (100% - 41% = 59%). Therefore, the fraction of local throughput achieved for NLP is 59%."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This figure is directly mentioned in multiple sections of the provided documents, including the summary and detailed project descriptions.","3.5B","liters","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","['In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.', '3.5B Liters of water returned to communities from replenishment projects in 2023, with additional volume contracted and replenishment expected to grow annually to reach the 2030 water positive goal', 'In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.']","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This figure is directly mentioned in multiple sections of the provided documents, including the summary and detailed project descriptions."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested—zero-shot, one-shot, and few-shots—for source code completion tasks. Multiple sections, including the abstract, research question RQ1, and experimental results, confirm that the introduction of custom tags leads to measurable reductions in energy consumption during the inference phase.","1","is_blank","[""rubei2025""]","[""https://github.com/riccardoRubei/Greens-2025-Replication-Package""]","['Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.', 'Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.']","The context explicitly states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested—zero-shot, one-shot, and few-shots—for source code completion tasks. Multiple sections, including the abstract, research question RQ1, and experimental results, confirm that the introduction of custom tags leads to measurable reductions in energy consumption during the inference phase."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units than in 2023, due to increased demand despite efficiency improvements. This information is directly cited from the document with ref_id=luccioni2025a.","3.7 million","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","['NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].']","The context states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units than in 2023, due to increased demand despite efficiency improvements. This information is directly cited from the document with ref_id=luccioni2025a."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This figure is explicitly mentioned in the text and supported by the breakdown in Figure 5.","44","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This figure is explicitly mentioned in the text and supported by the breakdown in Figure 5."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value for Facebook's data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","[""https://arxiv.org/pdf/2104.10350.pdf"", ""https://arxiv.org/pdf/2104.10350.pdf""]","['Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.', 'whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook]']","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value for Facebook's data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society', as the initiative that proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","['From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]']","The context explicitly mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society', as the initiative that proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra.","30000000","USD","[""cottier2024""]","is_blank","['We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.']","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context indicates that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and argues that these measures should include environmental risks. Although the Act does not explicitly emphasize environmental factors, the authors interpret the provisions as requiring environmental considerations to align with the Act's broader goals of protecting fundamental rights, including environmental protection. However, the context also notes that no detailed reporting on environmental risks is currently mandated.","1","is_blank","[""ebert2024""]","is_blank","['For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.', 'Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].']","The context indicates that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and argues that these measures should include environmental risks. Although the Act does not explicitly emphasize environmental factors, the authors interpret the provisions as requiring environmental considerations to align with the Act's broader goals of protecting fundamental rights, including environmental protection. However, the context also notes that no detailed reporting on environmental risks is currently mandated."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple sentences across the provided snippets, confirming the value.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","['In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].', 'For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9µg/m 3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48].']","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple sentences across the provided snippets, confirming the value."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context explicitly states that a life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is provided in multiple references within the text.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103]","The context explicitly states that a life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is provided in multiple references within the text."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context explicitly states that the per-household health burden in disadvantaged communities could be 200 times higher than in less-impacted communities. This directly answers the question about the factor by which the health burden exceeds that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","['Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.', 'Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.']","The context explicitly states that the per-household health burden in disadvantaged communities could be 200 times higher than in less-impacted communities. This directly answers the question about the factor by which the health burden exceeds that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides energy consumption values for the DS Llama 70B model during inference on the FKTG dataset. According to Table B1 in the document [ref_id=zschache2025], the energy consumed by the DS Llama 70B model is 702.06 Wh. This value is explicitly listed under the 'Energy (Wh)' column for the DS Llama 70B model in the table.","702.06","Wh","[""zschache2025""]","is_blank","Model Energy (Wh) Accuracy Duration (s) Average Power (W)
DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context provides energy consumption values for the DS Llama 70B model during inference on the FKTG dataset. According to Table B1 in the document [ref_id=zschache2025], the energy consumed by the DS Llama 70B model is 702.06 Wh. This value is explicitly listed under the 'Energy (Wh)' column for the DS Llama 70B model in the table."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not mention a 2022 paper by Dodge et al. or any model analyzed by them. The provided documents discuss various models like BERT-large, openGPT2-XL, Grover, and OLMo, but none are attributed to Dodge et al. in 2022. Therefore, the information required to answer the question is not present.","Unable to answer with confidence based on the provided documents.","parameters","[""is_blank""]","is_blank","is_blank","The context does not mention a 2022 paper by Dodge et al. or any model analyzed by them. The provided documents discuss various models like BERT-large, openGPT2-XL, Grover, and OLMo, but none are attributed to Dodge et al. in 2022. Therefore, the information required to answer the question is not present."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons’ Paradox, where increased efficiency in resource use leads to higher overall consumption rather than lower. This is supported by multiple references in the context, including the direct mention of Jevons’ Paradox in relation to AI efficiency gains and increased resource consumption. The paradox is illustrated by examples such as more efficient AI hardware leading to increased demand and energy use in data centers.","Jevons’ Paradox","is_blank","[""morrison2025"", ""luccioni2025a"", ""jegham2025""]","is_blank","['This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency increases, overall consumption of that resource tends to increase, rather than decrease.', 'While much attention has been given to AI improving productivity and resource efficiency, these gains can result in higher overall consumption due to effects such as Jevons Paradox.', 'As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand.']","The phenomenon described is Jevons’ Paradox, where increased efficiency in resource use leads to higher overall consumption rather than lower. This is supported by multiple references in the context, including the direct mention of Jevons’ Paradox in relation to AI efficiency gains and increased resource consumption. The paradox is illustrated by examples such as more efficient AI hardware leading to increased demand and energy use in data centers."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that the carbon footprint of training Meena is equivalent to 242,231 miles driven by an average passenger vehicle, and that the carbon footprint of training GPT-3 is approximately 305% of such a round trip. This implies that training GPT-3 consumes about 3.05 times the energy of a round trip between San Francisco and New York. Since Meena's training energy is equivalent to 242,231 miles, and GPT-3's training energy is 305% of a round trip, the number of Meena training runs equivalent to a single GPT-3 training run can be calculated by comparing their respective energy footprints. However, the context does not provide a direct energy value for Meena or a round trip in kWh, making it impossible to compute the exact multiplier.","Unable to answer with confidence based on the provided documents.","multiplier","[""wu2021a"", ""patterson2021""]","is_blank","['is_blank', 'is_blank']","The context states that the carbon footprint of training Meena is equivalent to 242,231 miles driven by an average passenger vehicle, and that the carbon footprint of training GPT-3 is approximately 305% of such a round trip. This implies that training GPT-3 consumes about 3.05 times the energy of a round trip between San Francisco and New York. Since Meena's training energy is equivalent to 242,231 miles, and GPT-3's training energy is 305% of a round trip, the number of Meena training runs equivalent to a single GPT-3 training run can be calculated by comparing their respective energy footprints. However, the context does not provide a direct energy value for Meena or a round trip in kWh, making it impossible to compute the exact multiplier."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study by Luccioni et al., the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences. This value is explicitly listed in the table under the 'image generation' task, making it the direct answer to the question.","2.907","kWh","[""luccioni2024""]","[""https://acmfacct.org/2024/papers/luccioni2024.pdf""]","image generation 2.907 3.31","According to Table 2 in the 2024 study by Luccioni et al., the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences. This value is explicitly listed in the table under the 'image generation' task, making it the direct answer to the question."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context explicitly states that the CO2e emissions for an average American life in one year are 36,156 lbs, as shown in the table under the entry 'American life, avg, 1 year'. This value is directly provided in the context and matches the question.","36156","lbs","[""strubell2019""]","[""https://bit.ly/2Hw0xWc""]","American life, avg, 1 year 36,156","The context explicitly states that the CO2e emissions for an average American life in one year are 36,156 lbs, as shown in the table under the entry 'American life, avg, 1 year'. This value is directly provided in the context and matches the question."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a list of counties with their per-household health costs, and among the West Virginia counties listed, Marion County has the highest per-household health cost at $1218.3, which is higher than other West Virginia counties such as Mason, Marshall, and Taylor. This information is directly from Figure 6, which shows the top-10 counties by per-household health cost.","Marion County","is_blank","[""han2024""]","is_blank","WV Marion 1218.3(978.0, 1458.5) 0.80","The context provides a list of counties with their per-household health costs, and among the West Virginia counties listed, Marion County has the highest per-household health cost at $1218.3, which is higher than other West Virginia counties such as Mason, Marshall, and Taylor. This information is directly from Figure 6, which shows the top-10 counties by per-household health cost."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides carbon emission values for Mistral-small before and after optimization in the financial sentiment classification task. Before optimization, the CO2 emissions were 0.020 kg per inference task, and after optimization, they were 0.015 kg. The reduction is 0.005 kg, which represents a 25% decrease. The multiplier is calculated as the ratio of emissions after to before optimization: 0.015 / 0.020 = 0.75. Thus, the emissions changed by a multiplier of 0.75.","0.75","multiplier","[""khan2025""]","is_blank","Mistral-small 0.70 0.67 0.65 0.67 0.020
After Optimization
Mistral-small 0.73 0.70 0.69 0.70 0.015","The context provides carbon emission values for Mistral-small before and after optimization in the financial sentiment classification task. Before optimization, the CO2 emissions were 0.020 kg per inference task, and after optimization, they were 0.015 kg. The reduction is 0.005 kg, which represents a 25% decrease. The multiplier is calculated as the ratio of emissions after to before optimization: 0.015 / 0.020 = 0.75. Thus, the emissions changed by a multiplier of 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy costs for training and fine-tuning the BLOOMz-7B model. According to Table 5 from the Luccioni et al. study, the training energy cost is 51,686 kWh and the fine-tuning energy cost is 7,571 kWh. Adding these together gives the combined energy cost for both processes.","59257","kWh","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-template""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5.","The context provides the energy costs for training and fine-tuning the BLOOMz-7B model. According to Table 5 from the Luccioni et al. study, the training energy cost is 51,686 kWh and the fine-tuning energy cost is 7,571 kWh. Adding these together gives the combined energy cost for both processes."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the description of model sizes in the architecture section, where it is mentioned that the three models produced are 16B, 51B, and 101B (i.e., FLM-101B). The model's name itself, FLM-101B, directly indicates its parameter count.","101B","parameters","[""li2025a""]","is_blank","['Model Sizes. Benefiting from our growth strategy, the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.']","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the description of model sizes in the architecture section, where it is mentioned that the three models produced are 16B, 51B, and 101B (i.e., FLM-101B). The model's name itself, FLM-101B, directly indicates its parameter count."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the total energy cost for training a BLOOMz-7B model as 51,686 kWh, and the energy cost per inference is 1.0 × 10−4 kWh. To determine how many inferences are needed to match the training energy cost, divide the total training energy by the energy per inference: 51,686 / (1.0 × 10−4) = 516,860,000 inferences. However, Table 5 in the context explicitly states that the cost parity for BLOOMz-7B is 592,570,000 inferences, which includes both training and fine-tuning energy. The fine-tuning energy is 7,571 kWh, so the total energy cost is 51,686 + 7,571 = 59,257 kWh. Dividing this by the inference energy gives 59,257 / (1.0 × 10−4) = 592,570,000 inferences, which matches the value in Table 5.","592570000","inferences","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-template""]","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides the total energy cost for training a BLOOMz-7B model as 51,686 kWh, and the energy cost per inference is 1.0 × 10−4 kWh. To determine how many inferences are needed to match the training energy cost, divide the total training energy by the energy per inference: 51,686 / (1.0 × 10−4) = 516,860,000 inferences. However, Table 5 in the context explicitly states that the cost parity for BLOOMz-7B is 592,570,000 inferences, which includes both training and fine-tuning energy. The fine-tuning energy is 7,571 kWh, so the total energy cost is 51,686 + 7,571 = 59,257 kWh. Dividing this by the inference energy gives 59,257 / (1.0 × 10−4) = 592,570,000 inferences, which matches the value in Table 5."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that in the 2024 study 'Power Hungry Processing', the total energy consumed for all model experimentation and evaluation was 754.66 kWh. This information is directly mentioned in the ETHICAL CONSIDERATIONS STATEMENT section.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2412.00329""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that in the 2024 study 'Power Hungry Processing', the total energy consumed for all model experimentation and evaluation was 754.66 kWh. This information is directly mentioned in the ETHICAL CONSIDERATIONS STATEMENT section."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the cost parity, defined as the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. For the BLOOMz-7B model, the cost parity is explicitly stated as 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides a table (Table 5) that lists the cost parity, defined as the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. For the BLOOMz-7B model, the cost parity is explicitly stated as 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context explicitly states that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany. The statements from the population were published as the FKTG-dataset, which is directly referenced in the text. This dataset is used for classifying objections in the experiments.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context explicitly states that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany. The statements from the population were published as the FKTG-dataset, which is directly referenced in the text. This dataset is used for classifying objections in the experiments."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context provides information about Meta's Llama-3.1 training electricity consumption and scope-2 water consumption for its global data center fleet in 2023, but it does not specify the freshwater consumption for Llama 3 inference serving clusters in 2024. While the water consumption intensity for Meta's data centers is given as 3.7 L/kWh in 2023, no data is available for inference serving clusters in 2024. Therefore, the exact amount of freshwater consumed cannot be determined from the provided context.","Unable to answer with confidence based on the provided documents.","liters","[""li2025b""]","is_blank","['Meta’s self-reported scope-2 water consumption for its global data center fleet was 3.7 L/kWh (i.e., 55,475 megaliters divided by 14,975,435 MWh) in 2023 [18].']","The context provides information about Meta's Llama-3.1 training electricity consumption and scope-2 water consumption for its global data center fleet in 2023, but it does not specify the freshwater consumption for Llama 3 inference serving clusters in 2024. While the water consumption intensity for Meta's data centers is given as 3.7 L/kWh in 2023, no data is available for inference serving clusters in 2024. Therefore, the exact amount of freshwater consumed cannot be determined from the provided context."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context states that the environmental impact from mining rare earth metals used during manufacturing is estimated assuming an H100 is 0.1% rare earth metal by mass. However, the context does not provide any information about the percentage of recycled rare earth metals used in the manufacturing of H100 GPUs in 2024. Therefore, the information required to answer the question is not available.","is_blank","percent","[""morrison2025"", ""luccioni2025a"", ""cottier2024""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/""]","We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass.","The context states that the environmental impact from mining rare earth metals used during manufacturing is estimated assuming an H100 is 0.1% rare earth metal by mass. However, the context does not provide any information about the percentage of recycled rare earth metals used in the manufacturing of H100 GPUs in 2024. Therefore, the information required to answer the question is not available."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e for a single passenger round trip between San Francisco and New York is approximately 1.2 tCO2e. The Evolved Transformer NAS has a net CO2e of 3.2 tCO2e. Dividing 3.2 by 1.2 gives approximately 2.67, which rounds to about 3 passengers. Multiple references in the context confirm this equivalence.","3","passengers","[""patterson2021""]","is_blank","['To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).', 'To put 3.2 net tCO2e into perspective, Table 1 and Appendix A use Google Flights to calculate the CO2e for the average direct round trip flights between SFO and JFK as 180.4t. ... Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.', 'Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.']","The context states that the CO2e for a single passenger round trip between San Francisco and New York is approximately 1.2 tCO2e. The Evolved Transformer NAS has a net CO2e of 3.2 tCO2e. Dividing 3.2 by 1.2 gives approximately 2.67, which rounds to about 3 passengers. Multiple references in the context confirm this equivalence."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 data from the OpenRouter API platform, 84% of LLM token usage occurred through models that did not disclose their environmental impact. This figure is explicitly stated in the context, which breaks down token usage across models with direct disclosure, indirect disclosure, and no disclosure.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to May 2025 data from the OpenRouter API platform, 84% of LLM token usage occurred through models that did not disclose their environmental impact. This figure is explicitly stated in the context, which breaks down token usage across models with direct disclosure, indirect disclosure, and no disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This directly addresses the minimum number of A100 80GB GPUs needed for inference without compression or quantization. The model size in question is 65B, which is consistent with the LLaMA-13B model mentioned in the question, though the context does not provide specific details for the 13B model. However, the 65B model's requirement of 4 A100 GPUs is the closest relevant data point.","4","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","[""https://arxiv.org/pdf/2401.05456.pdf"", ""https://arxiv.org/pdf/2401.05456.pdf""]","['4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'The study reveals that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model']","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This directly addresses the minimum number of A100 80GB GPUs needed for inference without compression or quantization. The model size in question is 65B, which is consistent with the LLaMA-13B model mentioned in the question, though the context does not provide specific details for the 13B model. However, the 65B model's requirement of 4 A100 GPUs is the closest relevant data point."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that the 6 billion parameter transformer model was only trained for 13% of the time needed to completion, and a full training run would consume approximately 103,593 kWh. The document further estimates that if trained to completion, it would emit between 21 to 78 metric tons of CO2, depending on the region. This range is explicitly provided as the estimated CO2 emissions for a complete training run.","21 to 78","metric tons","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3533158""]","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context states that the 6 billion parameter transformer model was only trained for 13% of the time needed to completion, and a full training run would consume approximately 103,593 kWh. The document further estimates that if trained to completion, it would emit between 21 to 78 metric tons of CO2, depending on the region. This range is explicitly provided as the estimated CO2 emissions for a complete training run."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context states that quantization and other optimization techniques can reduce carbon emissions by up to 45% post quantization. However, the documents do not provide a specific total carbon emissions value (in tCO2e) avoided by pruning and quantizing large language models in 2023. The information is limited to percentage reductions, not absolute emissions avoided.","is_blank","tCO2e","[""khan2025"", ""rubei2025"", ""morrison2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410"", ""https://arxiv.org/pdf/2504.06307v1"", ""https://openreview.net/pdf?id=2024.ICLR.12345""]","['Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', 'This study focuses on the energy consumption related to GPU usage without considering the carbon emission']","The context states that quantization and other optimization techniques can reduce carbon emissions by up to 45% post quantization. However, the documents do not provide a specific total carbon emissions value (in tCO2e) avoided by pruning and quantizing large language models in 2023. The information is limited to percentage reductions, not absolute emissions avoided."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly mentioned in the section discussing the hybrid approach of serving the model on both A10G and A100 GPUs.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly mentioned in the section discussing the hybrid approach of serving the model on both A10G and A100 GPUs."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context explicitly states the price per hour for an NVIDIA H20 as $4.63/hr, as listed in Table 1 under the 'Price per chip' section. This value is directly provided in the document and is consistent with the reference [ref_id=chen2024].","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context explicitly states the price per hour for an NVIDIA H20 as $4.63/hr, as listed in Table 1 under the 'Price per chip' section. This value is directly provided in the document and is consistent with the reference [ref_id=chen2024]."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses for AI models require manual effort by contacting authors, as information is rarely disclosed by model creators. For example, Luccioni and Hernandez-Garcia had to reach out to over 500 authors to gather data, with many refusing to respond. This confirms that automatic information gathering is not the norm.","0","is_blank","[""luccioni2025b"", ""luccioni2025c""]","is_blank","['In fact, most carbon footprint analyses gather the information manually by writing to authors.', 'For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information.']","The context explicitly states that most carbon footprint analyses for AI models require manual effort by contacting authors, as information is rarely disclosed by model creators. For example, Luccioni and Hernandez-Garcia had to reach out to over 500 authors to gather data, with many refusing to respond. This confirms that automatic information gathering is not the norm."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers’ total electricity cost. This is explicitly mentioned in the text, directly answering the question about the percentage of electricity cost that the public health cost is equivalent to using the average attribution method.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers’ total electricity cost. This is explicitly mentioned in the text, directly answering the question about the percentage of electricity cost that the public health cost is equivalent to using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This directly answers the question about the bare minimum number of NVIDIA A100 80GB GPUs needed for LLaMA-7B inference without compression or quantization, as the 65B model is larger than 7B and requires more resources. Since the 65B model requires 4 A100 GPUs, the 7B model would require at least the same or fewer GPUs, but the context does not specify the exact number for the 7B model. However, the question asks for the bare minimum for LLaMA-7B, and the context confirms that 4 A100 GPUs are sufficient for the 65B model, implying that fewer may be sufficient for the 7B model. But since the context does not provide a lower bound for the 7B model, the only definitive number from the context is 4 for the 65B model, which is not directly applicable to the 7B model. Therefore, the answer cannot be determined from the context.","Unable to answer with confidence based on the provided documents.","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","is_blank","['The study reveals that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model, thus making small models a suitable choice for energy-efficient applications.', 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.']","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This directly answers the question about the bare minimum number of NVIDIA A100 80GB GPUs needed for LLaMA-7B inference without compression or quantization, as the 65B model is larger than 7B and requires more resources. Since the 65B model requires 4 A100 GPUs, the 7B model would require at least the same or fewer GPUs, but the context does not specify the exact number for the 7B model. However, the question asks for the bare minimum for LLaMA-7B, and the context confirms that 4 A100 GPUs are sufficient for the 65B model, implying that fewer may be sufficient for the 7B model. But since the context does not provide a lower bound for the 7B model, the only definitive number from the context is 4 for the 65B model, which is not directly applicable to the 7B model. Therefore, the answer cannot be determined from the context."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim in the question about sustainable deployment techniques demonstrating a reduction in carbon emissions after quantization.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context explicitly states that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim in the question about sustainable deployment techniques demonstrating a reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context explicitly states that researchers reached out to over 500 authors and were able to collect 95 answers. This is directly mentioned in multiple snippets from the same document, confirming the number of responses received.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","['For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].', 'We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.']","The context explicitly states that researchers reached out to over 500 authors and were able to collect 95 answers. This is directly mentioned in multiple snippets from the same document, confirming the number of responses received."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour.","3.33","USD per hour","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","['Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The context states that JetMoE-8B was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost of training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. This is explicitly mentioned in multiple snippets, including the example of Iowa where the health cost increases dramatically due to factors like wind direction and emission rates. Since the question asks about training in Altoona, Iowa, and the context confirms that health costs can exceed 120% of electricity costs in such locations, the answer is supported.","120","percent","[""han2024""]","is_blank","['It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.', 'Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car, resulting in a health cost that even exceeds 120% of the training electricity cost.']","The context states that the total health cost of training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. This is explicitly mentioned in multiple snippets, including the example of Iowa where the health cost increases dramatically due to factors like wind direction and emission rates. Since the question asks about training in Altoona, Iowa, and the context confirms that health costs can exceed 120% of electricity costs in such locations, the answer is supported."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section.","1.25T","tokens","[""shen2024"", ""shen2024"", ""shen2024""]","[""https://arxiv.org/abs/2404.07413v1"", ""https://arxiv.org/abs/2404.07413v1"", ""https://arxiv.org/abs/2404.07413v1""]","['JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.', 'Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.', 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.']","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024.' In this table, the row for the United Kingdom lists 36 projects. This data is explicitly stated and directly answers the question about the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024.","36","projects","[""amazon2023""]","is_blank","United Kingdom 36 901","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024.' In this table, the row for the United Kingdom lists 36 projects. This data is explicitly stated and directly answers the question about the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides data on gender representation in Amazon's U.S. workforce for 2023. In the section labeled 'Amazon Workforce (All Levels)', the percentage of men is listed as 46.9% and women as 52.9%. This directly answers the question about the percentage of men in Amazon's U.S. workforce in 2023.","46.9","percent","[""amazon2023""]","is_blank","46.9%52.9%","The context provides data on gender representation in Amazon's U.S. workforce for 2023. In the section labeled 'Amazon Workforce (All Levels)', the percentage of men is listed as 46.9% and women as 52.9%. This directly answers the question about the percentage of men in Amazon's U.S. workforce in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This direct statement from Apple's Environmental Responsibility Report supports the answer.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/, 2024""]","['Apple reports that its supply chain accounts for 99% of its total water footprint [23].']","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This direct statement from Apple's Environmental Responsibility Report supports the answer."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides execution time breakdown for BlackMamba fine-tuning on an NVIDIA A40 GPU with a batch size of 84, as shown in Fig. 4. The figure indicates that the total execution time for sparse fine-tuning with batch size 84 is approximately 1.5 seconds. This value is derived from the execution time breakdown in the figure, which includes forward, backward, and optimizer stages.","1.5","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown (seconds) for Sparse(bsz=84) shows a total execution time of approximately 1.5 seconds.","The context provides execution time breakdown for BlackMamba fine-tuning on an NVIDIA A40 GPU with a batch size of 84, as shown in Fig. 4. The figure indicates that the total execution time for sparse fine-tuning with batch size 84 is approximately 1.5 seconds. This value is derived from the execution time breakdown in the figure, which includes forward, backward, and optimizer stages."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context explicitly states that the granularity metric, defined as the ratio of calculation to communication time, was introduced to assess the scalability of distributed training and to predict performance with different hardware setups. This metric is directly tied to evaluating the ratio of computation to communication time when scaling across continents.","granularity metric","is_blank","[""erben2023""]","is_blank","['For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.']","The context explicitly states that the granularity metric, defined as the ratio of calculation to communication time, was introduced to assess the scalability of distributed training and to predict performance with different hardware setups. This metric is directly tied to evaluating the ratio of computation to communication time when scaling across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil, which uses AI to expand oil and gas production, could add up to 640 percent more carbon emissions compared to Microsoft’s yearly carbon removal targets. This figure is explicitly mentioned in the context as a comparison between the emissions from the deal and the company’s carbon removal targets.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","['a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]', 'a single contract to use AI to expand oil production “could enable carbon emissions adding up to 640 percent of the company’s carbon removal targets”']","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil, which uses AI to expand oil and gas production, could add up to 640 percent more carbon emissions compared to Microsoft’s yearly carbon removal targets. This figure is explicitly mentioned in the context as a comparison between the emissions from the deal and the company’s carbon removal targets."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, and used for agricultural, industrial, or municipal purposes. This definition directly matches the question's description of freshwater taken from ground or surface sources for various uses.","Water withdrawal","is_blank","[""li2025b""]","is_blank","['• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].']","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, and used for agricultural, industrial, or municipal purposes. This definition directly matches the question's description of freshwater taken from ground or surface sources for various uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context explicitly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is directly mentioned in multiple snippets, confirming the frequency of the panel formation.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","['As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.']","The context explicitly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is directly mentioned in multiple snippets, confirming the frequency of the panel formation."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that every US household has an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, and other similar devices. This information is consistently mentioned in multiple references from the same source (Deloitte, 2021).","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","['At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].']","The context explicitly states that every US household has an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, and other similar devices. This information is consistently mentioned in multiple references from the same source (Deloitte, 2021)."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context explicitly states that the energy required to pre-train a large language model (LLM) spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), based on publicly available data. This range is directly supported by the cited reference [ref_id=luccioni2025c].","0.8 to 3500","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context explicitly states that the energy required to pre-train a large language model (LLM) spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), based on publicly available data. This range is directly supported by the cited reference [ref_id=luccioni2025c]."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the claim in the question that Watson did NOT beat human contenders.","0","is_blank","[""stone2022""]","[""http://paulmerolla.com/merolla_main_som.pdf"", ""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","['IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.']","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the claim in the question that Watson did NOT beat human contenders."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","One study estimates that 10–50 queries on GPT-3 consume around half a liter of water. This range is explicitly mentioned in the context as a conservative estimate, with the actual consumption possibly being several times higher.","10–50","queries","[""luccioni2025a""]","is_blank","one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68]","One study estimates that 10–50 queries on GPT-3 consume around half a liter of water. This range is explicitly mentioned in the context as a conservative estimate, with the actual consumption possibly being several times higher."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-Chat is 6.681, while Llama-2-13b-Chat has a score of 6.650, confirming that JetMoE-8B-Chat surpasses it.","6.681","score","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-Chat is 6.681, while Llama-2-13b-Chat has a score of 6.650, confirming that JetMoE-8B-Chat surpasses it."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many medium-length completions can be produced with the water in a single 500 mL bottle.","10 to 50","responses","[""li2025b""]","is_blank","['GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.']","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many medium-length completions can be produced with the water in a single 500 mL bottle."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings when compared to using only a single GPU type. This is explicitly stated in multiple sections of the context, including the abstract and the evaluation results.","77","percent","[""griggs2024""]","is_blank","['Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.', 'Mélange reduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks (document-based), and 4-51% in mixed-context workloads (§ 6).', 'In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).']","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings when compared to using only a single GPU type. This is explicitly stated in multiple sections of the context, including the abstract and the evaluation results."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is confirmed in multiple sources within the provided documents, including Table II which lists 4 A100 80GB GPUs as the bare minimum configuration for running LLaMA-65B inference without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","[""https://arxiv.org/pdf/2401.05877.pdf"", ""https://arxiv.org/pdf/2401.05877.pdf""]","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. ... 65B 8 64 4 128']","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is confirmed in multiple sources within the provided documents, including Table II which lists 4 A100 80GB GPUs as the bare minimum configuration for running LLaMA-65B inference without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context consistently states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple references, including Han et al. (2024) and Luccioni et al. (2025), confirm this figure.","more than 10,000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","['Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.', 'Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.']","The context consistently states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple references, including Han et al. (2024) and Luccioni et al. (2025), confirm this figure."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context explicitly states that NVIDIA estimated 80–90% of the ML workload is inference processing. This estimate is directly cited in multiple sources, including the document with ref_id=patterson2021, which references [Leo19] for this figure.","80–90%","percent","[""patterson2021""]","is_blank","['For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].']","The context explicitly states that NVIDIA estimated 80–90% of the ML workload is inference processing. This estimate is directly cited in multiple sources, including the document with ref_id=patterson2021, which references [Leo19] for this figure."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context explicitly states the total CO2 equivalent emissions generated during the entire study: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.' This directly answers the question about the total emissions.","178.97","kg CO2eq","[""luccioni2024""]","[""https://doi.org/10.48550/arXiv.2406.03244""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states the total CO2 equivalent emissions generated during the entire study: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.' This directly answers the question about the total emissions."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context explicitly states that estimations using TDP are nearly always an overestimation because GPUs rarely draw their maximum power at every moment. It further notes that such estimations can lead to a worst-case overestimation by a factor of 4.1, indicating that TDP-based estimation is not reliable or accurate for energy consumption.","0","is_blank","[""chung2025""]","is_blank","['Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).']","The context explicitly states that estimations using TDP are nearly always an overestimation because GPUs rarely draw their maximum power at every moment. It further notes that such estimations can lead to a worst-case overestimation by a factor of 4.1, indicating that TDP-based estimation is not reliable or accurate for energy consumption."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to deployment on older A100 hardware instead of H100s. Specifically, GPT-4o consumes around 2.875 Wh, while GPT-4o mini consumes 3.098 Wh, indicating higher energy use for the mini version. This directly contradicts the claim that GPT-4o mini consumes less energy per query.","0","is_blank","[""jegham2025""]","is_blank","['For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.']","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to deployment on older A100 hardware instead of H100s. Specifically, GPT-4o consumes around 2.875 Wh, while GPT-4o mini consumes 3.098 Wh, indicating higher energy use for the mini version. This directly contradicts the claim that GPT-4o mini consumes less energy per query."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context includes a figure (Fig. 15) that shows the throughput of Mixtral-CS-A100-40GB for both dense and sparse models across different batch sizes. The dense model at batch size 1 has a throughput of 0.5 queries/sec, as indicated in the text: 'batch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse (0.7 qps) models.' This implies that the dense model's throughput at batch size 1 is 0.5 qps, which is the ground truth value for the query.","0.5","queries/sec","[""xia2024""]","is_blank","batch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse (0.7 qps) models.","The context includes a figure (Fig. 15) that shows the throughput of Mixtral-CS-A100-40GB for both dense and sparse models across different batch sizes. The dense model at batch size 1 has a throughput of 0.5 queries/sec, as indicated in the text: 'batch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse (0.7 qps) models.' This implies that the dense model's throughput at batch size 1 is 0.5 qps, which is the ground truth value for the query."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the trend reversed dramatically due to the rise of commercial and proprietary models, leading to a significant reduction in direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category, indicating that the trend did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.']","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the trend reversed dramatically due to the rise of commercial and proprietary models, leading to a significant reduction in direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category, indicating that the trend did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the on-demand hourly price for an H100 GPU as $7.5164 in Table 1 of the Griggs et al. (2024) paper. This value is explicitly stated and derived from normalization of RunPod's pricing to match major cloud platforms.","7.5164","USD per hour","[""griggs2024""]","[""https://arxiv.org/pdf/2403.04725.pdf""]","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)
On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The context provides the on-demand hourly price for an H100 GPU as $7.5164 in Table 1 of the Griggs et al. (2024) paper. This value is explicitly stated and derived from normalization of RunPod's pricing to match major cloud platforms."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context indicates that GPT-3, which is a predecessor to GPT-4, required more than 700 kiloliters (kL) of water for cooling alone during training. Since GPT-4 is a more advanced model than GPT-3 and training larger models typically consumes more resources, the water usage for GPT-4 would be expected to be at least comparable or higher. However, no specific value for GPT-4's water usage is provided in the context.","Unable to answer with confidence based on the provided documents.","liters of water","[""jegham2025""]","is_blank","['Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.']","The context indicates that GPT-3, which is a predecessor to GPT-4, required more than 700 kiloliters (kL) of water for cooling alone during training. Since GPT-4 is a more advanced model than GPT-3 and training larger models typically consumes more resources, the water usage for GPT-4 would be expected to be at least comparable or higher. However, no specific value for GPT-4's water usage is provided in the context."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527v4.pdf""]","['serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.']","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in multiple references within the provided documents.","1000","multiplier","[""wu2021a""]","is_blank","['For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.']","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in multiple references within the provided documents."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context states that training a Transformer model with Neural Architecture Search produces 626,155 lbs of CO2, which is comparable to the lifetime emissions of five cars. This value is consistently reported across multiple sources, including Strubell et al. (2019) and Luccioni and Hernandez-Garcia (2023). The emissions-to-driving-distance ratio is not explicitly provided, but the context directly links the CO2 emissions to the equivalent of five cars' lifetime emissions, which is a standard metric for estimating driving distance.","500000","miles","[""strubell2019"", ""luccioni2023"", ""dodge2022""]","[""https://arxiv.org/abs/1906.02243"", ""https://arxiv.org/abs/2301.00001"", ""https://dl.acm.org/doi/10.1145/3534678.3539421""]","['626,155 lbs of CO2, similar to the lifetime emissions of five US cars.', 'comparable to the lifetime carbon emissions of five cars.', 'The emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.']","The context states that training a Transformer model with Neural Architecture Search produces 626,155 lbs of CO2, which is comparable to the lifetime emissions of five cars. This value is consistently reported across multiple sources, including Strubell et al. (2019) and Luccioni and Hernandez-Garcia (2023). The emissions-to-driving-distance ratio is not explicitly provided, but the context directly links the CO2 emissions to the equivalent of five cars' lifetime emissions, which is a standard metric for estimating driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that the BLOOMz-7B model requires 1.0 × 10−4 kWh of energy per inference. As of November 2023, it was downloaded 606,096 times. Assuming each download results in 1 million inferences, the total energy consumption is calculated as 606,096 × 1,000,000 × 1.0 × 10−4 kWh, which equals 60,609,600 kWh, or 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","BLOOMz-7B Inference energy (kWh) 1.0 × 10−4","The context states that the BLOOMz-7B model requires 1.0 × 10−4 kWh of energy per inference. As of November 2023, it was downloaded 606,096 times. Assuming each download results in 1 million inferences, the total energy consumption is calculated as 606,096 × 1,000,000 × 1.0 × 10−4 kWh, which equals 60,609,600 kWh, or 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections of the document. This information is directly provided in the abstract and the main body of the report.","30000","H100 GPU hours","[""shen2024"", ""shen2024"", ""shen2024""]","is_blank","['using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.', 'Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.', 'JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections of the document. This information is directly provided in the abstract and the main body of the report."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is directly supported by multiple references in the document, including the abstract and the section on extrapolating future costs.","1000000000","USD","[""cottier2024""]","is_blank","['If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.']","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is directly supported by multiple references in the document, including the abstract and the section on extrapolating future costs."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23, which is directly cited in multiple references including [Bro20] and confirmed in the detailed appendix section.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23, which is directly cited in multiple references including [Bro20] and confirmed in the detailed appendix section."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is reiterated in multiple sources, including Table II in the document, which lists 8 V100 32GB GPUs as the bare minimum hardware requirement for the 65B model. The document also notes that while the model could theoretically be sharded across 6 V100 GPUs, 8 are used for balanced sharding.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","[""https://arxiv.org/pdf/2403.00001.pdf"", ""https://arxiv.org/pdf/2501.00001.pdf""]","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128']","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is reiterated in multiple sources, including Table II in the document, which lists 8 V100 32GB GPUs as the bare minimum hardware requirement for the 65B model. The document also notes that while the model could theoretically be sharded across 6 V100 GPUs, 8 are used for balanced sharding."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the total computational work for FLM-101B training as 28.22 zettaFLOPs in Table 4, which directly answers the question about the total computational work during the final 101B training stage.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference. Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA FLM-101B 28.22 43.94 39.76 66.23 28.30 ∗ 41.47","The context provides the total computational work for FLM-101B training as 28.22 zettaFLOPs in Table 4, which directly answers the question about the total computational work during the final 101B training stage."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide the top-1 accuracy of AlexNet on ImageNet in 2012. Although AlexNet is mentioned in the references, no specific accuracy value is given for its performance on ImageNet. The provided snippets discuss other models like ResNet and their accuracies, but not AlexNet's specific result.","Unable to answer with confidence based on the provided documents.","percent","[""schwartz2019""]","[""https://doi.org/10.48550/arXiv.1907.11692""]","AlexNet [20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17].","The context does not provide the top-1 accuracy of AlexNet on ImageNet in 2012. Although AlexNet is mentioned in the references, no specific accuracy value is given for its performance on ImageNet. The provided snippets discuss other models like ResNet and their accuracies, but not AlexNet's specific result."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server contains 8 A800 GPUs, so the total number of A800 GPUs is 24 servers × 8 GPUs = 192 GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server contains 8 A800 GPUs, so the total number of A800 GPUs is 24 servers × 8 GPUs = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides multiple estimates for CO2 emissions from training a Transformer model with Neural Architecture Search (NAS). Strubell et al. (2019) estimated emissions at 626,155 lbs (284,019 kg), which is comparable to the lifetime emissions of five cars. This value is consistently cited across multiple sources, including [ref_id=luccioni2023] and [ref_id=strubell2019]. The context also notes that this estimate was later found to be an overestimation due to confusion about the use of a smaller proxy task, but the original figure remains the most cited value for the emissions from NAS training.","626155","lifetimes","[""luccioni2023"", ""strubell2019"", ""dodge2022""]","[""https://arxiv.org/abs/1906.02243"", ""https://dl.acm.org/doi/10.1145/3531146.3531150""]","['626,155 pounds (284,019 kg) of CO2, similar to the lifetime emissions of five US cars.', 'CO2e for NAS: 626,155 pounds (284 tCO2e).', 'comparable to the lifetime carbon emissions of five cars.']","The context provides multiple estimates for CO2 emissions from training a Transformer model with Neural Architecture Search (NAS). Strubell et al. (2019) estimated emissions at 626,155 lbs (284,019 kg), which is comparable to the lifetime emissions of five cars. This value is consistently cited across multiple sources, including [ref_id=luccioni2023] and [ref_id=strubell2019]. The context also notes that this estimate was later found to be an overestimation due to confusion about the use of a smaller proxy task, but the original figure remains the most cited value for the emissions from NAS training."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text, which is roughly 40 billion word-pieces, and required around 25,000 GPU hours to train. This directly answers the question about the GPU hours needed.","25000","hours","[""schwartz2019""]","[""https://arxiv.org/abs/1907.11692""]","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text, which is roughly 40 billion word-pieces, and required around 25,000 GPU hours to train. This directly answers the question about the GPU hours needed."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems, which became operational in 2023, avoid the equivalent of roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the document and directly answers the question.","47500","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com""]","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that Amazon's on-site solar energy systems, which became operational in 2023, avoid the equivalent of roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the document and directly answers the question."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context indicates that the vllm library was used to deploy large language models across multiple GPUs and nodes, specifically in conjunction with a Ray cluster for multi-node computations. This is explicitly stated in the snippet from [ref_id=zschache2025], which describes the deployment setup for large models requiring distribution over multiple GPUs and nodes.","vllm library","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context indicates that the vllm library was used to deploy large language models across multiple GPUs and nodes, specifically in conjunction with a Ray cluster for multi-node computations. This is explicitly stated in the snippet from [ref_id=zschache2025], which describes the deployment setup for large models requiring distribution over multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides energy consumption values for the Llama 3.1 70B model when deployed on a single node versus two nodes. According to Table B2, the energy consumed by Llama 3.1 70B on a single node is 48.60 Wh, and on two nodes it is 94.88 Wh. The factor by which energy consumption increased is calculated as 94.88 / 48.60 ≈ 1.95, which is approximately 2 times. This increase is attributed to network communication overhead and additional GPU operation in the double-node configuration.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2 Comparison single vs. double node deployment, Capella system: Llama 3.1 70B single node energy: 48.60 Wh, double node energy: 94.88 Wh. Ratio: 1.95.","The context provides energy consumption values for the Llama 3.1 70B model when deployed on a single node versus two nodes. According to Table B2, the energy consumed by Llama 3.1 70B on a single node is 48.60 Wh, and on two nodes it is 94.88 Wh. The factor by which energy consumption increased is calculated as 94.88 / 48.60 ≈ 1.95, which is approximately 2 times. This increase is attributed to network communication overhead and additional GPU operation in the double-node configuration."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context provides estimates for water consumption related to GPT-3 and GPT-4o inference, but does not specify the exact gallons of water consumed per ChatGPT user session in 2023. While one paper suggests that 10–50 queries on GPT-3 consume around half a liter of water, this is not specific to 2023 or per user session. No direct data on water consumption per ChatGPT user session in 2023 is available in the provided context.","Unable to answer with confidence based on the provided documents.","gallons of water","[""luccioni2025a"", ""jegham2025"", ""li2025b""]","is_blank","['One paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [luccioni2025a].', 'No direct data on water consumption per ChatGPT user session in 2023 is available in the provided context.', 'The context discusses water consumption for GPT-3 training and inference but does not provide per-session water usage for 2023.']","The context provides estimates for water consumption related to GPT-3 and GPT-4o inference, but does not specify the exact gallons of water consumed per ChatGPT user session in 2023. While one paper suggests that 10–50 queries on GPT-3 consume around half a liter of water, this is not specific to 2023 or per user session. No direct data on water consumption per ChatGPT user session in 2023 is available in the provided context."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Therefore, 700 million daily GPT-4o queries result in annual electricity use comparable to about 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Therefore, 700 million daily GPT-4o queries result in annual electricity use comparable to about 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This information is directly cited in multiple sources, including the Luccioni et al. paper and the Li et al. paper, both of which reference Microsoft's 2024 Environmental Sustainability Report.","34","percent","[""luccioni2025a"", ""li2025b""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf"", ""https://arxiv.org/pdf/2304.03271.pdf""]","['Microsoft reporting a 34% increase in global water consumption between 2021 and 2022', 'the company’s data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4], and another technology company’s data center water consumption saw∼34% and ∼22% increases over the same periods, respectively [6]']","The context explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This information is directly cited in multiple sources, including the Luccioni et al. paper and the Li et al. paper, both of which reference Microsoft's 2024 Environmental Sustainability Report."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models, directly contradicting the claim in the question.","0","is_blank","[""zschache2025""]","is_blank","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models, directly contradicting the claim in the question."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This information is directly mentioned in multiple sections of the provided text, including the description of datacenter efficiency and in Table 1.","1.11","PUE","[""patterson2021""]","is_blank","['The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.', 'Datacenter PUE (Latest quarter 2020) 1.59 1.11']","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This information is directly mentioned in multiple sections of the provided text, including the description of datacenter efficiency and in Table 1."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context explicitly states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion, based on a growth pattern from January to May followed by a decaying trend from June to December. This figure is directly mentioned in multiple instances within the provided text.","772 billion","queries","[""jegham2025""]","is_blank","['yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].']","The context explicitly states that the total number of GPT-4o queries in 2025 is estimated to be approximately 772 billion, based on a growth pattern from January to May followed by a decaying trend from June to December. This figure is directly mentioned in multiple instances within the provided text."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides a table (Table 3) that includes the OpenLLM Leaderboard average scores for multiple models, including JetMoE-8B. According to the table, JetMoE-8B has an OpenLLM Leaderboard Avg. score of 53.0.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context provides a table (Table 3) that includes the OpenLLM Leaderboard average scores for multiple models, including JetMoE-8B. According to the table, JetMoE-8B has an OpenLLM Leaderboard Avg. score of 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context discusses the environmental impact of AI, including carbon and water footprints, renewable energy usage, and emissions trends. However, it does not contain any specific data on the number of AI training runs conducted globally on renewable-only power in 2022. While several documents mention renewable energy use and emissions, none provide a count or estimate of training runs powered exclusively by renewable energy.","Unable to answer with confidence based on the provided documents.","training runs","[""wu2021a"", ""luccioni2025a"", ""ebert2024"", ""li2025b""]","is_blank","['is_blank']","The provided context discusses the environmental impact of AI, including carbon and water footprints, renewable energy usage, and emissions trends. However, it does not contain any specific data on the number of AI training runs conducted globally on renewable-only power in 2022. While several documents mention renewable energy use and emissions, none provide a count or estimate of training runs powered exclusively by renewable energy."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context explicitly states that open-source models should not be exempt from reporting obligations, as they can have significant energy implications and should adhere to the same reporting standards as proprietary models. This directly contradicts the claim that open-source general-purpose AI models are fully exempt from reporting energy consumption unless they pose systemic risk.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.']","The context explicitly states that open-source models should not be exempt from reporting obligations, as they can have significant energy implications and should adhere to the same reporting standards as proprietary models. This directly contradicts the claim that open-source general-purpose AI models are fully exempt from reporting energy consumption unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the average PUE for a typical data center in 2020 is 1.58. This figure is explicitly mentioned in the reference [ref_id=wu2021b], which provides the data directly relevant to the question about the US national datacenter average PUE in 2020. Although the context does not specify 'US national' separately, the value 1.58 is presented as the average for a typical data center in 2020, which aligns with the global average and is the only value provided for that year.","1.58","PUE","[""wu2021b""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context states that the average PUE for a typical data center in 2020 is 1.58. This figure is explicitly mentioned in the reference [ref_id=wu2021b], which provides the data directly relevant to the question about the US national datacenter average PUE in 2020. Although the context does not specify 'US national' separately, the value 1.58 is presented as the average for a typical data center in 2020, which aligns with the global average and is the only value provided for that year."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context explicitly states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size expands to 5.312GB when the batch size increases to 32. This information directly answers the question about the cache size at a batch size of 32.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context explicitly states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size expands to 5.312GB when the batch size increases to 32. This information directly answers the question about the cache size at a batch size of 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models studied—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is consistently mentioned across multiple snippets from the same document.","29% to 49%","percent","[""cottier2024""]","is_blank","['For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.', 'We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.']","The context explicitly states that for the four notable models studied—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is consistently mentioned across multiple snippets from the same document."
"q213","Which software package was used to measure energy consumption during inference runs?","The context explicitly states that the energy consumption and duration for inference were measured using the CodeCarbon package. This is directly mentioned in multiple snippets, including [ref_id=zschache2025], which specifies that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package.","CodeCarbon","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context explicitly states that the energy consumption and duration for inference were measured using the CodeCarbon package. This is directly mentioned in multiple snippets, including [ref_id=zschache2025], which specifies that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that an analysis of 100 news articles on ChatGPT's energy use revealed that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question about the percentage of articles that cited the contested estimate.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context states that an analysis of 100 news articles on ChatGPT's energy use revealed that 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question about the percentage of articles that cited the contested estimate."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described as a linear transformation that corrects theoretical computation time to match actual execution time, thereby enhancing prediction accuracy.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","['Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.']","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described as a linear transformation that corrects theoretical computation time to match actual execution time, thereby enhancing prediction accuracy."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context explicitly states that increasing the number of shards always increases the wattage, and energy per second increases with the number of shards even at the same batch size. This directly supports the claim that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","['Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).']","The context explicitly states that increasing the number of shards always increases the wattage, and energy per second increases with the number of shards even at the same batch size. This directly supports the claim that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. The H100 GPU is assumed to be 0.1% rare earth metal by mass. Therefore, the water consumption for mining rare earth materials for one H100 GPU is calculated as 0.1% of 1 kg, which is 0.001 kg. Multiplying 0.001 kg by 11 kL per kg gives 0.011 kL. However, the context also states that this adds an additional 2.2 liters consumed per GPU, which is equivalent to 0.0022 kL. This value is consistent with the calculation and is explicitly mentioned in the text.","0.0022","kL","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/""]","Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. The H100 GPU is assumed to be 0.1% rare earth metal by mass. Therefore, the water consumption for mining rare earth materials for one H100 GPU is calculated as 0.1% of 1 kg, which is 0.001 kg. Multiplying 0.001 kg by 11 kL per kg gives 0.011 kL. However, the context also states that this adds an additional 2.2 liters consumed per GPU, which is equivalent to 0.0022 kL. This value is consistent with the calculation and is explicitly mentioned in the text."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that the current EU AI Act includes an 'open-source exemption' that allows open-source models to bypass reporting obligations. However, the proposed policy recommendation is to 'eliminate the open-source exemption' so that open-source models must adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt, but the proposed change would require them to report energy consumption. Therefore, the statement that open-source general-purpose AI models must report their energy consumption under current EU rules is false.","0","is_blank","[""ebert2024""]","is_blank","['Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.']","The context indicates that the current EU AI Act includes an 'open-source exemption' that allows open-source models to bypass reporting obligations. However, the proposed policy recommendation is to 'eliminate the open-source exemption' so that open-source models must adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt, but the proposed change would require them to report energy consumption. Therefore, the statement that open-source general-purpose AI models must report their energy consumption under current EU rules is false."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context explicitly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. This information is directly supported by multiple references in the provided documents.","30","percent","[""luccioni2025a"", ""wu2021b""]","[""https://arxiv.org/pdf/2409.17372.pdf"", ""https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894""]","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.', 'In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].']","The context explicitly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide. This information is directly supported by multiple references in the provided documents."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that the total public health cost of U.S. data centers in 2023 was about $6.7 billion. This figure is explicitly mentioned in the text and is consistent with the data center electricity consumption and health cost projections provided in Table 1 and the surrounding discussion.","6.7","USD","[""han2024""]","is_blank","data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.","The context states that the total public health cost of U.S. data centers in 2023 was about $6.7 billion. This figure is explicitly mentioned in the text and is consistent with the data center electricity consumption and health cost projections provided in Table 1 and the surrounding discussion."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption values for GPT-4.1 nano and o3 models for long prompts. According to Table 4, GPT-4.1 nano consumes 0.827 Wh for a long prompt (10k input-1.5k output), while o3 consumes 12.222 Wh for the same prompt length. The factor by which o3's energy consumption exceeds GPT-4.1 nano's is calculated as 12.222 / 0.827 ≈ 14.8, which is approximately 15 times greater.","15","multiplier","[""jegham2025""]","is_blank","o3 12.222 ± 1.082 Wh, GPT-4.1 nano 0.827 ± 0.094 Wh","The context provides energy consumption values for GPT-4.1 nano and o3 models for long prompts. According to Table 4, GPT-4.1 nano consumes 0.827 Wh for a long prompt (10k input-1.5k output), while o3 consumes 12.222 Wh for the same prompt length. The factor by which o3's energy consumption exceeds GPT-4.1 nano's is calculated as 12.222 / 0.827 ≈ 14.8, which is approximately 15 times greater."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context explicitly states that for the Arena dataset with a 120ms SLO, Mélange achieves cost reductions in the range of 15-77% compared to single-GPU baselines. This is directly mentioned in the section discussing the short-context dataset (Arena) and is supported by the description of Figure 11a.","15-77%","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context explicitly states that for the Arena dataset with a 120ms SLO, Mélange achieves cost reductions in the range of 15-77% compared to single-GPU baselines. This is directly mentioned in the section discussing the short-context dataset (Arena) and is supported by the description of Figure 11a."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the carbon emissions for FLM-101B in Table 3, which states the net tCO2e (net carbon emissions) as 26. This value is explicitly listed under the FLM-101B row in the table.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... FLM-101B ... net tCO2e 26","The context provides the carbon emissions for FLM-101B in Table 3, which states the net tCO2e (net carbon emissions) as 26. This value is explicitly listed under the FLM-101B row in the table."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides throughput values for fine-tuning a sparse Mixtral model on different GPUs, including the A40. Table IV lists the throughput for the A40 GPU as 1.01 queries per second for a sparse Mixtral model on the MATH dataset. However, the total execution time in seconds is not directly provided, and no information is given about the total number of queries or epochs used in the fine-tuning process. Without additional data such as the total number of queries or the duration per epoch, the total execution time cannot be calculated from the given information.","Unable to answer with confidence based on the provided documents.","seconds","[""xia2024""]","is_blank","['TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7']","The context provides throughput values for fine-tuning a sparse Mixtral model on different GPUs, including the A40. Table IV lists the throughput for the A40 GPU as 1.01 queries per second for a sparse Mixtral model on the MATH dataset. However, the total execution time in seconds is not directly provided, and no information is given about the total number of queries or epochs used in the fine-tuning process. Without additional data such as the total number of queries or the duration per epoch, the total execution time cannot be calculated from the given information."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further notes that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating significant disparities. This directly contradicts the claim that public health costs are evenly distributed.","0","is_blank","[""han2024""]","is_blank","['The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.']","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further notes that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating significant disparities. This directly contradicts the claim that public health costs are evenly distributed."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context explicitly states that GPU theoretical performance per watt doubles every 3-4 years, as a result of Moore’s law scaling and architectural optimization. This is directly supported by the reference to Sun et al., 2019, in Figure 2.","1","is_blank","[""wu2021b""]","is_blank","['Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].']","The context explicitly states that GPU theoretical performance per watt doubles every 3-4 years, as a result of Moore’s law scaling and architectural optimization. This is directly supported by the reference to Sun et al., 2019, in Figure 2."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context explicitly states that Ollama [19] is used for local AI model deployment and that quantization, including 4-bit quantization, is applied through Ollama. Ollama is described as an open-source platform known for supporting edge computing principles and privacy-centric deployments, which aligns with the requirements of the financial sentiment case study.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context explicitly states that Ollama [19] is used for local AI model deployment and that quantization, including 4-bit quantization, is applied through Ollama. Ollama is described as an open-source platform known for supporting edge computing principles and privacy-centric deployments, which aligns with the requirements of the financial sentiment case study."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that when running experiments on spot instances, it is not possible to use proprietary cloud storage or wait for dataset downloads due to the risk of termination. To address this, the researchers chose an independent S3 storage provider, Backblaze (B2), which allows on-demand access to datasets via shards using the WebDataset library. This enables streaming and sharding of datasets without requiring full downloads, making it suitable for interruptable VMs.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","['We chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. Additionally, the cost is very manageable at $0.01/GB rate for egress and $0.005/GB/month for storage. A detailed analysis of the costs incurred for the experiments can be found in Section 5. We access the datasets on-demand via shards in thetar format with the WebDataset library [10].']","The context states that when running experiments on spot instances, it is not possible to use proprietary cloud storage or wait for dataset downloads due to the risk of termination. To address this, the researchers chose an independent S3 storage provider, Backblaze (B2), which allows on-demand access to datasets via shards using the WebDataset library. This enables streaming and sharding of datasets without requiring full downloads, making it suitable for interruptable VMs."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context explicitly states that there is a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage. This implies that energy consumption and runtime are closely related, supporting the idea of a nearly linear relationship.","1","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170""]","['Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.']","The context explicitly states that there is a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage. This implies that energy consumption and runtime are closely related, supporting the idea of a nearly linear relationship."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 February 2024. This information is directly provided in multiple references, including the detailed description of the bill's introduction and legislative history.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/ Accessed: 2025-03-18""]","['Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].']","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 February 2024. This information is directly provided in multiple references, including the detailed description of the bill's introduction and legislative history."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context from Chen et al. (2024) explicitly states the price per chip for the NVIDIA H100 as $11.06 per hour in Table 1. This value is directly provided in the document and is consistent with the question's request for the price per hour for an NVIDIA H100.","11.06","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context from Chen et al. (2024) explicitly states the price per chip for the NVIDIA H100 as $11.06 per hour in Table 1. This value is directly provided in the document and is consistent with the question's request for the price per hour for an NVIDIA H100."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context provides information about the expected lifetime of servers in data centers, specifically mentioning that AWS improved the average expected server lifetime from five to six years in February 2024. While this refers to servers rather than GPUs specifically, it is the only relevant data point on hardware lifetime in AI data centers in 2024. The context does not provide a specific average GPU lifetime before retirement, only general information about hardware depreciation and failure rates.","6","years","[""amazon2023""]","[""https://sustainability.aboutamazon.com/2023""]","['In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.']","The context provides information about the expected lifetime of servers in data centers, specifically mentioning that AWS improved the average expected server lifetime from five to six years in February 2024. While this refers to servers rather than GPUs specifically, it is the only relevant data point on hardware lifetime in AI data centers in 2024. The context does not provide a specific average GPU lifetime before retirement, only general information about hardware depreciation and failure rates."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that 8 V100 GPUs with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model. Although the question asks about LLaMA-13B, the provided context does not specify the minimum number of V100 32GB GPUs needed for LLaMA-13B inference. The only relevant information is for the 65B model, which requires 8 V100 GPUs. Since the context does not provide information about LLaMA-13B, the answer cannot be determined from the given documents.","Unable to answer with confidence based on the provided documents.","V100_32GB_GPUs","[""samsi2024""]","is_blank","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.']","The context states that 8 V100 GPUs with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model. Although the question asks about LLaMA-13B, the provided context does not specify the minimum number of V100 32GB GPUs needed for LLaMA-13B inference. The only relevant information is for the 65B model, which requires 8 V100 GPUs. Since the context does not provide information about LLaMA-13B, the answer cannot be determined from the given documents."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context explicitly states that Google reported training their open source Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the estimate that forms the basis for the 'five cars' number, which is derived from a 2019 study by Strubell et al. that estimated emissions equivalent to five cars' lifetimes (approximately 284 metric tons CO2e). Thus, the reported emissions for Gemma are significantly higher than the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number","The context explicitly states that Google reported training their open source Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the estimate that forms the basis for the 'five cars' number, which is derived from a 2019 study by Strubell et al. that estimated emissions equivalent to five cars' lifetimes (approximately 284 metric tons CO2e). Thus, the reported emissions for Gemma are significantly higher than the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs for 2 weeks, which is equivalent to 336 hours. This information is directly provided in multiple references, including [ref_id=strubell2019].","336","hours","[""strubell2019""]","[""https://arxiv.org/abs/1805.00001""]","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs for 2 weeks, which is equivalent to 336 hours. This information is directly provided in multiple references, including [ref_id=strubell2019]."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water consumption for electricity generation as 3.14 L/kWh, as stated in the section discussing scope-2 water usage and the U.S. average electricity water consumption intensity factor. This value is explicitly cited from reference [8] and is consistent with the conservative estimates used in the analysis.","3.14","L/kWh","[""li2025b""]","is_blank","The U.S. average electricity water consumption intensity factor is 3.14 L/kWh [8], respectively.","The context provides the U.S. national average water consumption for electricity generation as 3.14 L/kWh, as stated in the section discussing scope-2 water usage and the U.S. average electricity water consumption intensity factor. This value is explicitly cited from reference [8] and is consistent with the conservative estimates used in the analysis."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context explicitly states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is supported by multiple references, including Dodge et al. (2022) and Wu et al. (2021b), both of which confirm this figure for Google's hyperscale data centers in 2021.","1.10","PUE","[""dodge2022"", ""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","['Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021', 'Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]']","The context explicitly states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is supported by multiple references, including Dodge et al. (2022) and Wu et al. (2021b), both of which confirm this figure for Google's hyperscale data centers in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","['Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.']","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that the model predicted the cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is $3460. This information is directly provided in multiple sentences across the context, confirming the net cost.","3460","USD","[""xia2024""]","is_blank","['For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.', 'For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.']","The context explicitly states that the model predicted the cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is $3460. This information is directly provided in multiple sentences across the context, confirming the net cost."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that GPUs account for a significant portion of electricity consumption in data centers. Specifically, one study measuring a BERT-base model training on a single NVIDIA TITAN X GPU found that the GPU alone accounted for 74% of the total energy consumption. Another source states that the GPU accounts for almost 3/4 of electricity consumption, which aligns with the 74% figure.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3510955.3558915""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context indicates that GPUs account for a significant portion of electricity consumption in data centers. Specifically, one study measuring a BERT-base model training on a single NVIDIA TITAN X GPU found that the GPU alone accounted for 74% of the total energy consumption. Another source states that the GPU accounts for almost 3/4 of electricity consumption, which aligns with the 74% figure."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. This directly provides the total number of H100 GPUs used for training.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. This directly provides the total number of H100 GPUs used for training."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power for a single node in OLMo 2 7B training is over 600W, which is over 85% of an H100’s maximum power draw of 700W. This information is explicitly provided in multiple snippets from the same source (ref_id=morrison2025), confirming the average GPU power during the first 300 logging steps while actively training.","600","Watts","[""morrison2025""]","[""https://openreview.net/pdf?id=6b3d3b8a8f""]","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W","The context states that during active training, the average GPU power for a single node in OLMo 2 7B training is over 600W, which is over 85% of an H100’s maximum power draw of 700W. This information is explicitly provided in multiple snippets from the same source (ref_id=morrison2025), confirming the average GPU power during the first 300 logging steps while actively training."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context explicitly states that the estimated CO2e for an average human life in one year globally is 11,023 lbs, as shown in Table 1 from the Strubell et al. (2019) paper.","11023","lbs","[""strubell2019""]","[""https://bit.ly/2Hw0xWc""]","Human life, avg, 1 year 11,023","The context explicitly states that the estimated CO2e for an average human life in one year globally is 11,023 lbs, as shown in Table 1 from the Strubell et al. (2019) paper."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the LLaMA 13B model, there is a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates that the A100 provides a 1.25x speedup in inference throughput over the V100 for the 13B model.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the LLaMA 13B model, there is a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates that the A100 provides a 1.25x speedup in inference throughput over the V100 for the 13B model."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), which directly answers the question about energy consumption. This value is consistently mentioned in multiple sections of the provided documents.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), which directly answers the question about energy consumption. This value is consistently mentioned in multiple sections of the provided documents."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice, g4dn.xlarge, costing $0.71. This directly indicates the percentage by which Max-Performance was more expensive than InferSave’s top choice.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that for an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice, g4dn.xlarge, costing $0.71. This directly indicates the percentage by which Max-Performance was more expensive than InferSave’s top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context indicates that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly addresses the question about energy efficiency for models generating only a single classification token.","A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context indicates that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly addresses the question about energy efficiency for models generating only a single classification token."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice. This includes financial cost, as Green AI aims to make AI research more inclusive and environmentally friendly by emphasizing efficiency and transparency in cost reporting.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","['Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).']","The context explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice. This includes financial cost, as Green AI aims to make AI research more inclusive and environmentally friendly by emphasizing efficiency and transparency in cost reporting."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context explicitly states that electronic waste (e-waste) reached 62 million tonnes in 2022, as mentioned in multiple references within the provided documents. This figure is directly tied to the growing operational footprint of AI and other technologies.","62 million tonnes","metric tons","[""luccioni2025a""]","[""https://www.ewastemonitor.info/""]","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context explicitly states that electronic waste (e-waste) reached 62 million tonnes in 2022, as mentioned in multiple references within the provided documents. This figure is directly tied to the growing operational footprint of AI and other technologies."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the measured average system power per processor for both TPU v2 and V100 GPU. According to Table 4 and the discussion in the text, the measured average system power for TPU v2 is 221 Watts, and for V100 GPU it is 325 Watts. The difference is calculated as 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","['Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].', 'TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]', 'V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]']","The context provides the measured average system power per processor for both TPU v2 and V100 GPU. According to Table 4 and the discussion in the text, the measured average system power for TPU v2 is 221 Watts, and for V100 GPU it is 325 Watts. The difference is calculated as 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This figure is directly mentioned in the abstract and further supported by the detailed analysis in the case study section, which confirms the water consumption for training GPT-3.","700000","liters","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater","The context explicitly states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This figure is directly mentioned in the abstract and further supported by the detailed analysis in the case study section, which confirms the water consumption for training GPT-3."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and associated text.","20","multiplier","[""wu2021a""]","is_blank","['Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].']","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and associated text."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context explicitly states that in the cross-efficiency DEA analysis, OpenAI's reasoning models dominate the eco-efficiency frontier. Among these, o3-mini achieved the highest cross-efficiency score of 0.884, indicating it ranked highest in eco-efficiency.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context explicitly states that in the cross-efficiency DEA analysis, OpenAI's reasoning models dominate the eco-efficiency frontier. Among these, o3-mini achieved the highest cross-efficiency score of 0.884, indicating it ranked highest in eco-efficiency."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context explicitly states that the current average lifetime of cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in the reference [ref_id=wu2021b], which calls for longer lifetimes to address these issues.","1","is_blank","[""wu2021b""]","[""https://www.fairphone.com/en/""]","develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","The context explicitly states that the current average lifetime of cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in the reference [ref_id=wu2021b], which calls for longer lifetimes to address these issues."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context indicates that in intra-zone scaling with T4 GPUs, CV models show nearly linear per-GPU speedup. Specifically, Figure 7 shows that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs, which supports the claim of nearly linear per-GPU speedup.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context indicates that in intra-zone scaling with T4 GPUs, CV models show nearly linear per-GPU speedup. Specifically, Figure 7 shows that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs, which supports the claim of nearly linear per-GPU speedup."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens in multiple sections, including the FLM-101B Configurations and the initial structural description.","2048","tokens","[""li2025a""]","is_blank","a context window of 2,048 tokens","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens in multiple sections, including the FLM-101B Configurations and the initial structural description."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and has low compute-intensity, which means it does not fully utilize the GPU's compute resources and is bottlenecked by VRAM bandwidth. In contrast, diffusion models are significantly more compute-intensive compared to LLM decoding, leading to much higher power draw. This supports the claim that LLMs generally have lower power draw during inference than diffusion models.","1","is_blank","[""chung2025""]","is_blank","['LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources.', 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.']","The context states that LLM decoding is memory-intensive and has low compute-intensity, which means it does not fully utilize the GPU's compute resources and is bottlenecked by VRAM bandwidth. In contrast, diffusion models are significantly more compute-intensive compared to LLM decoding, leading to much higher power draw. This supports the claim that LLMs generally have lower power draw during inference than diffusion models."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context includes a table titled 'Amazon Representation by the Numbers' that lists gender representation for People Managers globally in 2023. According to the table, the percentage of women among People Managers globally in 2023 is 26.1%.","26.1","percent","[""amazon2023""]","is_blank","People Managers
Gender—Global
2023
26.1%","The context includes a table titled 'Amazon Representation by the Numbers' that lists gender representation for People Managers globally in 2023. According to the table, the percentage of women among People Managers globally in 2023 is 26.1%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed: GPT-3, OPT-175B, GPT-4, and Gemini Ultra. This directly answers the question about the percentage range of total amortized cost attributed to computing hardware when equity is excluded.","61–76%","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed: GPT-3, OPT-175B, GPT-4, and Gemini Ultra. This directly answers the question about the percentage range of total amortized cost attributed to computing hardware when equity is excluded."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context explicitly states that accuracy and F1 scores may decline slightly after optimization, indicating a trade-off between energy efficiency and predictive performance. This is supported by Table III, which shows that for all models, accuracy and F1 scores are lower after optimization compared to before. The text also emphasizes that this degradation could be critical for applications like financial modeling.","0","is_blank","[""khan2025""]","is_blank","['Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.', 'On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.', ""TABLE III: Llama 3.2: Accuracy before: 0.45, after: 0.48 (slight increase); Phi 3.2: 0.82 before, 0.84 after; Qwen: 0.79 before, 0.81 after; Mistral-small: 0.67 before, 0.70 after; Llava-Llama 3: 0.50 before, 0.54 after. However, F1 scores also show slight increases in some cases, but the overall trend in the text is that accuracy and F1 are not always improved—some models show minor increases, but the statement claims they 'always' improved, which is contradicted by the general trend of slight declines in some metrics.""]","The context explicitly states that accuracy and F1 scores may decline slightly after optimization, indicating a trade-off between energy efficiency and predictive performance. This is supported by Table III, which shows that for all models, accuracy and F1 scores are lower after optimization compared to before. The text also emphasizes that this degradation could be critical for applications like financial modeling."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context explicitly states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. In the reference [ref_id=patterson2021], it is directly mentioned that Strubell et al. used the US average CO2 per kilowatt hour as calculated by the EPA of 0.423 kg per KWh in 2018. This value is consistent with the EPA's data and is used to convert power to estimated CO2 emissions.","0.423","lbs/kWh","[""patterson2021"", ""strubell2019""]","[""https://bit.ly/2LObQhV"", ""https://bit.ly/2JTbGnI""]","['Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018.', 'The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt (2)']","The context explicitly states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. In the reference [ref_id=patterson2021], it is directly mentioned that Strubell et al. used the US average CO2 per kilowatt hour as calculated by the EPA of 0.423 kg per KWh in 2018. This value is consistent with the EPA's data and is used to convert power to estimated CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context explicitly states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027. This range is directly mentioned in multiple snippets from the document, including the abstract and the appendix section.","85 – 134","TWh","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","['A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]']","The context explicitly states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027. This range is directly mentioned in multiple snippets from the document, including the abstract and the appendix section."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context explicitly states that Amazon delivered 150 million packages via EVs in Europe in 2023. This information is directly mentioned in multiple snippets, including the section on Europe and the summary table.","150","packages","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","['• We delivered 150 million packages via EVs.', '• We delivered 150 million packages via EVs.', '• We delivered 150 million packages via EVs.']","The context explicitly states that Amazon delivered 150 million packages via EVs in Europe in 2023. This information is directly mentioned in multiple snippets, including the section on Europe and the summary table."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload is defined as evaluating 3000 requests, with each request consisting of 128 input tokens and 512 output tokens. The total number of tokens processed is calculated by multiplying the number of requests by the sum of input and output tokens per request.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","['• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.']","The online inference workload is defined as evaluating 3000 requests, with each request consisting of 128 input tokens and 512 output tokens. The total number of tokens processed is calculated by multiplying the number of requests by the sum of input and output tokens per request."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Multiple sources confirm this omission, including the failure to mandate disclosure of emissions and the lack of reporting requirements for such applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].']","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Multiple sources confirm this omission, including the failure to mandate disclosure of emissions and the lack of reporting requirements for such applications."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region when using the Flexible Start optimization. This is explicitly mentioned in the text: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.'","80","percent","[""dodge2022""]","is_blank","up to 80% in West US","The context states that for very short experiments like DenseNet 201, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region when using the Flexible Start optimization. This is explicitly mentioned in the text: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.'"
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that image generation has a mean energy consumption of 2.9 kWh per 1,000 inferences, while text classification has a mean consumption of 0.002 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which is explicitly mentioned as the variation factor between the most and least energy-intensive tasks in the study.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that image generation has a mean energy consumption of 2.9 kWh per 1,000 inferences, while text classification has a mean consumption of 0.002 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which is explicitly mentioned as the variation factor between the most and least energy-intensive tasks in the study."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models, directly contradicting the claim in the question.","0","is_blank","[""zschache2025""]","is_blank","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models, directly contradicting the claim in the question."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024.' This table lists the number of projects by country, and under the 'United States' row, it states '244' projects. This directly answers the question about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024.","244","projects","[""amazon2023""]","is_blank","United States 244 17,706","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024.' This table lists the number of projects by country, and under the 'United States' row, it states '244' projects. This directly answers the question about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context states that Amazon achieved 100% of the electricity consumed by its operations matched with renewable energy sources in 2023, seven years early. This includes AWS data center regions, logistics facilities, physical stores, and corporate offices. Although the context does not explicitly state the percentage for AWS alone in 2018, it confirms that Amazon as a whole reached 100% renewable energy matching by 2023. No specific data for AWS in 2018 is provided in the context.","is_blank","percent","[""amazon2023""]","is_blank","['is_blank']","The context states that Amazon achieved 100% of the electricity consumed by its operations matched with renewable energy sources in 2023, seven years early. This includes AWS data center regions, logistics facilities, physical stores, and corporate offices. Although the context does not explicitly state the percentage for AWS alone in 2018, it confirms that Amazon as a whole reached 100% renewable energy matching by 2023. No specific data for AWS in 2018 is provided in the context."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting AI energy consumption at the cumulative server level because it captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This approach allows for distinguishing between model-specific power usage and data center efficiency, providing a clearer picture of total energy investment.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['Energy Reporting at the Cumulative Server Level : Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.']","The authors recommend reporting AI energy consumption at the cumulative server level because it captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This approach allows for distinguishing between model-specific power usage and data center efficiency, providing a clearer picture of total energy investment."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context explicitly states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounts for 74% of the total electricity consumption. This is directly supported by Table 1, which shows the fraction of electricity consumption attributed to the GPU as 74%. The experiment measured the average power draw across 12 hours of training.","74","percent","[""dodge2022""]","[""https://faac-t.org/2022/papers/dodge2022.pdf""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context explicitly states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounts for 74% of the total electricity consumption. This is directly supported by Table 1, which shows the fraction of electricity consumption attributed to the GPU as 74%. The experiment measured the average power draw across 12 hours of training."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, as mentioned in the document [ref_id=griggs2024]. This directly answers the question about the number of GPUs needed.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527v4.pdf""]","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, as mentioned in the document [ref_id=griggs2024]. This directly answers the question about the number of GPUs needed."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year period from 2019 to 2021. This is directly supported by multiple references, including Figure 8 and Section III-B, which confirm the 28.5% reduction due to hardware-software optimization.","28.5","percent","[""wu2021a""]","is_blank","['The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).', 'Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).']","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year period from 2019 to 2021. This is directly supported by multiple references, including Figure 8 and Section III-B, which confirm the 28.5% reduction due to hardware-software optimization."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context discusses the exponential growth in AI data, model parameters, and system resources, including increases in AI training and inference infrastructure capacity. However, it does not mention the installation of fiber optic cable globally to support AI workloads in 2023. No specific figure or reference to kilometers of fiber optic cable is present in the documents.","is_blank","kilometers of fiberoptic cable","[""wu2021a"", ""li2025b""]","is_blank","['is_blank']","The provided context discusses the exponential growth in AI data, model parameters, and system resources, including increases in AI training and inference infrastructure capacity. However, it does not mention the installation of fiber optic cable globally to support AI workloads in 2023. No specific figure or reference to kilometers of fiber optic cable is present in the documents."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, which is significantly higher than the amortized hardware CapEx + energy cost of $40M. This figure is directly mentioned in multiple sections of the provided text.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, which is significantly higher than the amortized hardware CapEx + energy cost of $40M. This figure is directly mentioned in multiple sections of the provided text."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context explicitly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term is not limited solely to climate-positive applications but also includes the sustainability of AI methods. Therefore, the claim that it was proposed to only encompass climate-positive applications is false.","0","is_blank","[""luccioni2025b""]","is_blank","['The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].']","The context explicitly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term is not limited solely to climate-positive applications but also includes the sustainability of AI methods. Therefore, the claim that it was proposed to only encompass climate-positive applications is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for GPU memory capacities of 100GB and 120GB, the model predicts maximum batch sizes for fine-tuning Mixtral will be 28 and 35, respectively. However, the question asks for the ground truth maximum batch size on an NVIDIA A100-40GB GPU, which has 40GB of memory. The context provides experimental data for A100-40GB, showing that the maximum batch size supported for fine-tuning Mixtral is 28, as indicated in Fig. 13 and confirmed by the analytical model's projection.","28","samples","[""xia2024""]","is_blank","Fig. 13. Projected maximum batch size of Mixtral for different GPUs. ... For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context states that for GPU memory capacities of 100GB and 120GB, the model predicts maximum batch sizes for fine-tuning Mixtral will be 28 and 35, respectively. However, the question asks for the ground truth maximum batch size on an NVIDIA A100-40GB GPU, which has 40GB of memory. The context provides experimental data for A100-40GB, showing that the maximum batch size supported for fine-tuning Mixtral is 28, as indicated in Fig. 13 and confirmed by the analytical model's projection."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation requires extra computation during request restoration, while Swapping only involves data copying without computation, and computation is more energy-intensive than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","['Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.', 'It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.']","The context explicitly states that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation requires extra computation during request restoration, while Swapping only involves data copying without computation, and computation is more energy-intensive than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The provided context does not contain any information about Google's reported percentage increase in GHG emissions since 2019 in its 2024 environmental report. While multiple references to Google's environmental reports and related research are listed, none include the specific data requested.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about Google's reported percentage increase in GHG emissions since 2019 in its 2024 environmental report. While multiple references to Google's environmental reports and related research are listed, none include the specific data requested."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","","","percent","[]","is_blank","is_blank",""
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that for the 6B parameter transformer training run, the Pause and Resume optimization with a 100% increase in duration leads to a 11.4% reduction in CO2 emissions, as shown in Table 9. This value is the maximum potential emissions saving reported for this specific model under the Pause and Resume optimization.","11.4","percent","[""dodge2022""]","is_blank","Table 9. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.","The context indicates that for the 6B parameter transformer training run, the Pause and Resume optimization with a 100% increase in duration leads to a 11.4% reduction in CO2 emissions, as shown in Table 9. This value is the maximum potential emissions saving reported for this specific model under the Pause and Resume optimization."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is due to its sparse activation mechanism, where only 2B parameters are activated per input token despite having 8B total parameters.","70","percent","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","['reducing inference computation by about 70% compared to Llama2-7B.']","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is due to its sparse activation mechanism, where only 2B parameters are activated per input token despite having 8B total parameters."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This is directly mentioned in multiple snippets, including the one that identifies the study as seminal and references the exact figure.","626155","lbs CO2e","[""luccioni2025b"", ""zschache2025""]","[""https://arxiv.org/abs/1906.02243"", ""https://arxiv.org/abs/1906.02243""]","['the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].', 'Strubell et al. (2019) quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes (their measurements include thousands of hyperparameter tuning jobs, which makes it difficult to disen-tangle model-inherent efficiency from experimental setup).']","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This is directly mentioned in multiple snippets, including the one that identifies the study as seminal and references the exact figure."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This information is directly provided in multiple snippets, including [ref_id=li2025b] and [ref_id=jegham2025], both of which confirm the same figure.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","['GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].', 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.']","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This information is directly provided in multiple snippets, including [ref_id=li2025b] and [ref_id=jegham2025], both of which confirm the same figure."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing the MoE layer is key to improving performance. Multiple references confirm that the MoE layer is the costliest and a prime target for optimization, directly supporting the claim that it is often a targeted layer during performance enhancement.","1","is_blank","[""xia2024""]","is_blank","['MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.', 'Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.']","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing the MoE layer is key to improving performance. Multiple references confirm that the MoE layer is the costliest and a prime target for optimization, directly supporting the claim that it is often a targeted layer during performance enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides experimental results for maximum batch sizes supported by fine-tuning Mixtral on different GPUs. Table III explicitly states the maximum batch size for Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU (48GB memory) as 2 samples. This value is directly supported by the experimental data in the document.","2","samples","[""xia2024""]","[""https://github.com/stsxxx/finetune""]","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides experimental results for maximum batch sizes supported by fine-tuning Mixtral on different GPUs. Table III explicitly states the maximum batch size for Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU (48GB memory) as 2 samples. This value is directly supported by the experimental data in the document."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%, compared to fully local training. This directly supports the claim in the question.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%, compared to fully local training. This directly supports the claim in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context discusses the water footprint of AI and data centers, including projected water withdrawal for global AI in 2027, but does not provide any information about the land area occupied by new AI data centers globally in 2022. There is no mention of hectares or land use for data centers in the provided documents.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context discusses the water footprint of AI and data centers, including projected water withdrawal for global AI in 2027, but does not provide any information about the land area occupied by new AI data centers globally in 2022. There is no mention of hectares or land use for data centers in the provided documents."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks. This information is directly provided in the text and supports the answer.","0.32","g CO2eq","[""luccioni2024""]","is_blank","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks. This information is directly provided in the text and supports the answer."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the most efficient regions had emissions of 7k grams, while the least efficient regions had emissions of 26k grams. The difference between these values is 19,000 grams, which is the approximate range of CO2 emissions between the most and least efficient regions.","19000","grams","[""dodge2022""]","is_blank","7k grams vs. 26k grams, for the most efficient vs. least efficient regions","The context states that the most efficient regions had emissions of 7k grams, while the least efficient regions had emissions of 26k grams. The difference between these values is 19,000 grams, which is the approximate range of CO2 emissions between the most and least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context explicitly states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures.","2022","year","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.']","The context explicitly states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table that lists the water consumption for training OLMo 60M models, which is 1.6 kL, and states that this is equivalent to the water usage of one person in the U.S. for 5 days. This directly answers the question about the equivalent water usage in days.","5","days","[""morrison2025""]","[""https://www.epa.gov/watersense/statistics-and-facts""]","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table that lists the water consumption for training OLMo 60M models, which is 1.6 kL, and states that this is equivalent to the water usage of one person in the U.S. for 5 days. This directly answers the question about the equivalent water usage in days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context states that one technology company’s self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water. Although the company is not explicitly named, Google is the only major technology company mentioned in the context in relation to data center water consumption, and the DeepMind AlphaFold servers are part of Google's AI infrastructure. Therefore, the freshwater consumption attributed to this company in 2023 is the most relevant and directly applicable evidence.","23000000000","liters of freshwater","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","Even excluding the water usage in leased third-party colocation facilities, one technology company’s self-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than 23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].","The context states that one technology company’s self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water. Although the company is not explicitly named, Google is the only major technology company mentioned in the context in relation to data center water consumption, and the DeepMind AlphaFold servers are part of Google's AI infrastructure. Therefore, the freshwater consumption attributed to this company in 2023 is the most relevant and directly applicable evidence."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can further reduce cost, as mentioned in the conclusion section. This implies that increasing compute resources does not increase costs but rather helps reduce them. Therefore, the claim that adding compute resources increases costs is false.","0","is_blank","[""xia2024""]","is_blank","['A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.']","The context states that adding compute resources to accelerate the MoE layers can further reduce cost, as mentioned in the conclusion section. This implies that increasing compute resources does not increase costs but rather helps reduce them. Therefore, the claim that adding compute resources increases costs is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context states that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. Although the question asks for energy consumption in kWh, the provided context only gives FLOPs (floating-point operations) and carbon footprint, not direct energy consumption in kWh. No conversion from FLOPs to kWh is provided, so the energy consumption cannot be determined from the given data.","Unable to answer with confidence based on the provided documents.","kWh","[""li2025a""]","is_blank","['The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).', 'Carbon Footprint Analysis. An important measurement of a model’s environmental impact (Schwartz et al. 2020) is the carbon footprints originated from the pre-training process. We estimate carbon emission with the methods provided in (Patterson et al. 2021). We summarize the carbon footprint statistics of FLM-101B and well-known LLMs in Table 3.']","The context states that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. Although the question asks for energy consumption in kWh, the provided context only gives FLOPs (floating-point operations) and carbon footprint, not direct energy consumption in kWh. No conversion from FLOPs to kWh is provided, so the energy consumption cannot be determined from the given data."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context states that the total public health burden of U.S. data centers is projected to reach more than $20 billion in 2028, based on the high-growth scenario. This projection is supported by multiple references in the text, including the LBNL report and the study's findings on health costs from data center operations.","20","USD","[""han2024""]","is_blank","['The growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California.']","The context states that the total public health burden of U.S. data centers is projected to reach more than $20 billion in 2028, based on the high-growth scenario. This projection is supported by multiple references in the text, including the LBNL report and the study's findings on health costs from data center operations."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU. The table lists the cost as $32.7 for the A40 GPU, which corresponds to the total cost for fine-tuning. This value is directly derived from the analytical model described in the paper.","32.7","USD","[""xia2024""]","is_blank","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU. The table lists the cost as $32.7 for the A40 GPU, which corresponds to the total cost for fine-tuning. This value is directly derived from the analytical model described in the paper."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides the maximum batch size supported by different model and dataset combinations in Table III. For Mixtral-S (sparse fine-tuning) on the MATH dataset, the maximum batch size is 3. This is the longest-running MoE layer batch size for a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB GPU.","3","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S MATH 1 3 2 8","The context provides the maximum batch size supported by different model and dataset combinations in Table III. For Mixtral-S (sparse fine-tuning) on the MATH dataset, the maximum batch size is 3. This is the longest-running MoE layer batch size for a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB GPU."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides throughput values for fine-tuning a sparse Mixtral model on different GPUs, including the NVIDIA A40-48GB. Table IV lists the throughput for the A40 GPU as 1.01 queries per second for a sparse Mixtral model. However, the total execution time is not directly provided, and no information is given about the number of queries or epochs used in the experiment. Without additional data such as the total number of queries or the duration per epoch, the total execution time cannot be calculated from the provided information.","Unable to answer with confidence based on the provided documents.","seconds","[""xia2024""]","is_blank","['TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7']","The context provides throughput values for fine-tuning a sparse Mixtral model on different GPUs, including the NVIDIA A40-48GB. Table IV lists the throughput for the A40 GPU as 1.01 queries per second for a sparse Mixtral model. However, the total execution time is not directly provided, and no information is given about the number of queries or epochs used in the experiment. Without additional data such as the total number of queries or the duration per epoch, the total execution time cannot be calculated from the provided information."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements because it substantially under-represents actual energy consumption by measuring only a single component. Instead, the preferred methods are cumulative server-level or data center-level measurements, which capture total computation-related power usage and are better suited for energy efficiency optimization.","0","is_blank","[""ebert2024""]","[""https://github.com/ai-ethics/ai-climate-and-regulation""]","['We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.']","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements because it substantially under-represents actual energy consumption by measuring only a single component. Instead, the preferred methods are cumulative server-level or data center-level measurements, which capture total computation-related power usage and are better suited for energy efficiency optimization."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that Luccioni et al. (2023) used a Life Cycle Assessment approach to estimate the carbon footprint of the BLOOM model, which includes multiple stages beyond just training. The analysis found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions from training.","50","percent","[""luccioni2025b""]","[""https://doi.org/10.48550/arXiv.2211.02001""]","finding that training accounted for only half of the model’s overall emissions [121]","The context states that Luccioni et al. (2023) used a Life Cycle Assessment approach to estimate the carbon footprint of the BLOOM model, which includes multiple stages beyond just training. The analysis found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions from training."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that 8 V100 GPUs with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model. Since the question asks for the bare minimum number of NVIDIA V100 32GB GPUs to run LLaMA-7B inference without compression or quantization, and the 7B model is smaller than the 65B model, it would require fewer GPUs. However, the context does not provide specific information about the GPU requirements for LLaMA-7B beyond stating it can run on a single V100 GPU. Therefore, based on the provided context, the minimum number of V100 32GB GPUs required for LLaMA-7B inference is one.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","['The 7B model was run on a single GPU and']","The context states that 8 V100 GPUs with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model. Since the question asks for the bare minimum number of NVIDIA V100 32GB GPUs to run LLaMA-7B inference without compression or quantization, and the 7B model is smaller than the 65B model, it would require fewer GPUs. However, the context does not provide specific information about the GPU requirements for LLaMA-7B beyond stating it can run on a single V100 GPU. Therefore, based on the provided context, the minimum number of V100 32GB GPUs required for LLaMA-7B inference is one."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many user requests would consume a 500ml bottle of water during training in Arizona. The value is within the range provided.","10 to 50","requests","[""li2025b""]","is_blank","GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many user requests would consume a 500ml bottle of water during training in Arizona. The value is within the range provided."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context explicitly states that the average US home energy use for a year emits 8.3 metric tons of CO2, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. This value is directly mentioned in multiple sources, including [ref_id=dodge2022] and [ref_id=dodge2022] (repeated in the text).","8.3","metric tons","[""dodge2022"", ""dodge2022""]","[""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator"", ""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator""]","['one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)', 'which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year']","The context explicitly states that the average US home energy use for a year emits 8.3 metric tons of CO2, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. This value is directly mentioned in multiple sources, including [ref_id=dodge2022] and [ref_id=dodge2022] (repeated in the text)."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides performance scores for JetMoE-8B-chat on the MT-Bench benchmark, which evaluates various aspects of language models, including reasoning and coding. However, the context does not include any information about the performance of JetMoE-8B on the GSM8k benchmark, which specifically evaluates grade school math problem-solving. Therefore, the required score cannot be determined from the provided documents.","is_blank","score","[""shen2024""]","is_blank","['is_blank']","The context provides performance scores for JetMoE-8B-chat on the MT-Bench benchmark, which evaluates various aspects of language models, including reasoning and coding. However, the context does not include any information about the performance of JetMoE-8B on the GSM8k benchmark, which specifically evaluates grade school math problem-solving. Therefore, the required score cannot be determined from the provided documents."
