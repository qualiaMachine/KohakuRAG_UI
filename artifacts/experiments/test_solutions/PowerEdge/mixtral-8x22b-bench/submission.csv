"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This is the average increase in U.S. data center electricity consumption between 2010 and 2014.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This is the average increase in U.S. data center electricity consumption between 2010 and 2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This implies that the amount of cars taken off the road is directly proportional to the amount of CO2e avoided.","13,900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This implies that the amount of cars taken off the road is directly proportional to the amount of CO2e avoided."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context explicitly states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, including two data centers in Virginia, one in California, and one in Singapore.","24","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context explicitly states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, including two data centers in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context mentions that NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces. However, it also mentions that internally, they assume a 4 year lifespan for their GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","0.013","kg/GPU","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context mentions that NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces. However, it also mentions that internally, they assume a 4 year lifespan for their GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is not explicitly mentioned in the provided context. However, it is stated that GPT-4 has the largest amortized hardware and energy cost at $40M. The context also mentions that the most expensive publicly announced model by the start of 2027 will cost about $1 billion, implying a growth rate of 2.4x per year. Without the exact training cost of GPT-4, we cannot calculate the factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B.","is_blank","ratio","[""cottier2024""]","is_blank","is_blank","The estimated amortized training cost of GPT-4 is not explicitly mentioned in the provided context. However, it is stated that GPT-4 has the largest amortized hardware and energy cost at $40M. The context also mentions that the most expensive publicly announced model by the start of 2027 will cost about $1 billion, implying a growth rate of 2.4x per year. Without the exact training cost of GPT-4, we cannot calculate the factor by which the estimated amortized training cost of GPT-4 is greater than the total training budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context explicitly states that a single passenger round trip from San Francisco to New York is approximately 1.2t CO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context explicitly states that a single passenger round trip from San Francisco to New York is approximately 1.2t CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard is stated as 43.94 in the provided context.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard is stated as 43.94 in the provided context."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the typical microprocessor in 2021 is capable of running at 5,000,000kHz, while the Intel 4004 ran at 740kHz in 1971. This is a more than 6,750 fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the typical microprocessor in 2021 is capable of running at 5,000,000kHz, while the Intel 4004 ran at 740kHz in 1971. This is a more than 6,750 fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context explicitly states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3.","14.8","days","[""patterson2021""]","is_blank","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context explicitly states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table with measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The table includes a row for Llama 3.2 1B model at a request frequency of 8 req/s, which specifies a GPU Power Usage of 12.0 kWh.","12.0","kWh","[""morrison2025""]","is_blank","Table 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates.","The context provides a table with measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The table includes a row for Llama 3.2 1B model at a request frequency of 8 req/s, which specifies a GPU Power Usage of 12.0 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context explicitly states that the total permitted annual emission limits for data center diesel generators in Northern Virginia are approximately 13,000 tons of NOx.","13,000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context explicitly states that the total permitted annual emission limits for data center diesel generators in Northern Virginia are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to their emissions. This is based on the analysis using the COBRA modeling tool provided by the EPA.","1300","deaths","[""han2024""]","is_blank","Based on the emission data projected by the U.S. EPA’s COBRA modeling tool [39], our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 1,300 premature deaths in 2028.","The context mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to their emissions. This is based on the analysis using the COBRA modeling tool provided by the EPA."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context from [ref_id=dodge2022] states that the 6 billion parameter transformer was trained for 8 days and only trained for approximately 13% of the time it would take to train to completion. It is estimated that a full training run would take 60 days. Therefore, a full training run would take approximately 52 days more than the 8 days it was already trained for.","52","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. Measuring the Carbon Intensity of AI in Cloud Instances FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea","The context from [ref_id=dodge2022] states that the 6 billion parameter transformer was trained for 8 days and only trained for approximately 13% of the time it would take to train to completion. It is estimated that a full training run would take 60 days. Therefore, a full training run would take approximately 52 days more than the 8 days it was already trained for."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide information on the highest GPU energy consumption for 1,000 queries among the models listed in the appendix of a 2025 study. It only mentions the energy consumption for the inference of 1,024 examples and the effects of variations of data dimensionality, model architecture, decoding strategies, and software optimizations on inference energy use.","is_blank","is_blank","[""fernandez2025""]","is_blank","is_blank","The context does not provide information on the highest GPU energy consumption for 1,000 queries among the models listed in the appendix of a 2025 study. It only mentions the energy consumption for the inference of 1,024 examples and the effects of variations of data dimensionality, model architecture, decoding strategies, and software optimizations on inference energy use."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context mentions that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","is_blank","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014","The context mentions that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, while the entire training process required 51,686 kWh. However, the question asks for the energy consumption for pre-training the BLOOM model, which is not explicitly provided in the context.","is_blank","MWh","[""ebert2024""]","is_blank","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process.","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, while the entire training process required 51,686 kWh. However, the question asks for the energy consumption for pre-training the BLOOM model, which is not explicitly provided in the context."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context mentions that the Switch Transformer model has 1.5 trillion parameters and that only 0.1% of these parameters are activated per token. This implies that 0.1% of 1.5 trillion parameters, or 1.5 billion parameters, are activated per token.","0.1%","percent","[""wu2021a""]","is_blank","The Switch Transformer model equipped with 1.5 trillion parameters produces significantly less carbon emission than that of GPT-3. Large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context mentions that the Switch Transformer model has 1.5 trillion parameters and that only 0.1% of these parameters are activated per token. This implies that 0.1% of 1.5 trillion parameters, or 1.5 billion parameters, are activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. However, it does not provide the exact number of experts included in each MoE layer.","is_blank","experts","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. However, it does not provide the exact number of experts included in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide specific information on the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, which could impact the total execution time.","is_blank","second","[""xia2024""]","is_blank","is_blank","The context does not provide specific information on the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, which could impact the total execution time."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the provided context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, out of which 28.22 zettaFLOPs were required to train the English portion of the model.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","According to the provided context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, out of which 28.22 zettaFLOPs were required to train the English portion of the model."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for energy-efficient local inference in financial sentiment classification is not explicitly mentioned in the provided context. However, the context from [ref_id=samsi2024] mentions the use of NVIDIA GPUs for all experiments, and the context from [ref_id=khan2025] mentions the use of an 11th Gen Intel(R) Core(TM) i7-1165G7 processor for local AI model deployment. Therefore, it is not possible to determine the specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification.","is_blank","is_blank","[""samsi2024"", ""khan2025""]","is_blank","is_blank","The experimental setup for energy-efficient local inference in financial sentiment classification is not explicitly mentioned in the provided context. However, the context from [ref_id=samsi2024] mentions the use of NVIDIA GPUs for all experiments, and the context from [ref_id=khan2025] mentions the use of an 11th Gen Intel(R) Core(TM) i7-1165G7 processor for local AI model deployment. Therefore, it is not possible to determine the specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide explicit information on the number of machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study. The study only mentions that it covers both task-specific and 'general-purpose' models, without specifying the exact number of models.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide explicit information on the number of machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study. The study only mentions that it covers both task-specific and 'general-purpose' models, without specifying the exact number of models."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3 times.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% for Language Model (LM) training decreases the overall carbon footprint by 3 times."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is 1.2x to 4x larger than the final training run, with a median of 2.2x. This indicates that the total compute for model development is, on average, 2.2 times larger than the compute for the final training run alone.","2.2","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that the total compute for model development is 1.2x to 4x larger than the final training run, with a median of 2.2x. This indicates that the total compute for model development is, on average, 2.2 times larger than the compute for the final training run alone."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context explicitly states that the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is approximately 103,593 kWh, which is equivalent to 103.593 MWh.","103.593","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context explicitly states that the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is approximately 103,593 kWh, which is equivalent to 103.593 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate' discusses how efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox, an economic principle where increased efficiency can lead to increased consumption. This is the central argument of the paper.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","Recent work on indirect impacts [63] warns of potential 'rebound effects', whereby gains in efficiency spur higher overall consumption. These second-order effects challenge the presumption that purely technical optimizations alone will deliver sufficient climate benefits. Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate' discusses how efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox, an economic principle where increased efficiency can lead to increased consumption. This is the central argument of the paper."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context explicitly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that the decline of Red AI is not happening, contradicting the statement in the question.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that the decline of Red AI is not happening, contradicting the statement in the question."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","Under the growth strategy, the total time cost for training FLM-101B is mentioned as 21.54 days in the provided context.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","Under the growth strategy, the total time cost for training FLM-101B is mentioned as 21.54 days in the provided context."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context explicitly states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving significant room for improvements to efficiency and overall utilization.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context explicitly states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving significant room for improvements to efficiency and overall utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained with an estimated energy of 1287 MWh. This information is provided in the context snippet with the ref_id li2025b.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated energy of 1287 MWh. This information is provided in the context snippet with the ref_id li2025b."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score project as a collaborative project aiming to establish a unified approach for comparing the inference efficiency of AI models. This project is referred to as the 'AI Energy Score' in the context.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","The AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The context mentions the AI Energy Score project as a collaborative project aiming to establish a unified approach for comparing the inference efficiency of AI models. This project is referred to as the 'AI Energy Score' in the context."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context states that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. The execution time for the MoE layer includes both the forward and backward passes during fine-tuning. The kernel-level analysis within the MoE layer shows that matrix multiplication operations contribute significantly to the end-to-end execution time. However, the context does not provide the exact execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","microseconds","[""xia2024""]","is_blank","['is_blank']","The context states that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. The execution time for the MoE layer includes both the forward and backward passes during fine-tuning. The kernel-level analysis within the MoE layer shows that matrix multiplication operations contribute significantly to the end-to-end execution time. However, the context does not provide the exact execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The hyperparameters of JetMoE-8B show that the model has 8 experts and a top-k of 2 for every layer. This means that for each input token, 2 experts are selected for activation in each layer of the JetMoE-8B model.","2","experts","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632 Table 1: JetMoE-8B hyperparameters.","The hyperparameters of JetMoE-8B show that the model has 8 experts and a top-k of 2 for every layer. This means that for each input token, 2 experts are selected for activation in each layer of the JetMoE-8B model."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context from [ref_id=schwartz2019] states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which is a larger increase than the 200,000x mentioned in the question.","1","is_blank","[""schwartz2019""]","is_blank","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context from [ref_id=schwartz2019] states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which is a larger increase than the 200,000x mentioned in the question."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context explicitly states that the global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context explicitly states that the global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context explicitly states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy: Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context explicitly states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. Given that the question was asked in 2025, this would make the field approximately 69 years old.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. Given that the question was asked in 2025, this would make the field approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context states that the 'five cars' estimate originates from a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The estimate was based on the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter.","NAS approach, United States average electricity GHG emissions intensity, typical AI hardware, average-efficiency datacenter","is_blank","[""luccioni2025c""]","is_blank","The research article was written for a specialized audience of AI and NLP researchers, who would have the background knowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and featuring a table containing the 'five cars' estimate was widely shared on social media, leading to the publication being picked up by numerous media outlets (including MIT Technology Review32 and Forbes33). The 'five cars' number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions.","The context states that the 'five cars' estimate originates from a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The estimate was based on the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that by targeting an average TPOT of 100 ms, energy consumption per generation can be reduced by 44% compared to the configuration that simply minimizes latency. This is equivalent to a 44% decrease in energy use.","44","percent","[""chung2025""]","is_blank","In this context, a chatbot provider can target an average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]), which is sufficient for most reading or listening speeds. This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that by targeting an average TPOT of 100 ms, energy consumption per generation can be reduced by 44% compared to the configuration that simply minimizes latency. This is equivalent to a 44% decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that the maximum batch size supported by BlackMamba for fine-tuning on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is not explicitly stated. However, it does provide the maximum batch sizes supported by Mixtral and BlackMamba for different datasets and GPUs, as well as the factors affecting the maximum batch size such as GPU memory size, model size, sequence length, and MoE sparsity.","is_blank","samples","[""xia2024""]","is_blank","is_blank","The context mentions that the maximum batch size supported by BlackMamba for fine-tuning on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is not explicitly stated. However, it does provide the maximum batch sizes supported by Mixtral and BlackMamba for different datasets and GPUs, as well as the factors affecting the maximum batch size such as GPU memory size, model size, sequence length, and MoE sparsity."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon had invested in 513 global renewable energy projects, including 243 utility-scale wind and solar projects, and 270 solar rooftops at their facilities and stores around the world. The total capacity of renewable energy in Europe is now 7 GW, including 1.7 GW of renewable energy from offshore wind. However, the context does not provide information on the energy storage capacity in gigawatts (GW) as of 2023.","is_blank","GW","[""amazon2023""]","is_blank","The context states that Amazon had invested in 513 global renewable energy projects, including 243 utility-scale wind and solar projects, and 270 solar rooftops at their facilities and stores around the world. The total capacity of renewable energy in Europe is now 7 GW, including 1.7 GW of renewable energy from offshore wind.","The context states that Amazon had invested in 513 global renewable energy projects, including 243 utility-scale wind and solar projects, and 270 solar rooftops at their facilities and stores around the world. The total capacity of renewable energy in Europe is now 7 GW, including 1.7 GW of renewable energy from offshore wind. However, the context does not provide information on the energy storage capacity in gigawatts (GW) as of 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that a single model such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights. It also mentions that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, which exceeds the total electricity consumption of 35,000 U.S. residential households, 50 inpatient hospitals, and even 325 universities annually.","is_blank","flights","[""jegham2025""]","is_blank","A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights and consume water equivalent to the annual drinking needs of millions of people. GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, which exceeds the total electricity consumption of 35,000 U.S. residential households, 50 inpatient hospitals, and even 325 universities annually.","The context states that a single model such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights. It also mentions that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, which exceeds the total electricity consumption of 35,000 U.S. residential households, 50 inpatient hospitals, and even 325 universities annually."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide information on the percentage of AI inference workloads in Asia powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information on the percentage of AI inference workloads in Asia powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally. As the question asks for the global average PUE of AI-dedicated data centers in 2023, and no other information is provided, it can be assumed that this value also applies to AI-dedicated data centers.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74]","The context states that the average data center PUE in 2023 was 1.58 globally. As the question asks for the global average PUE of AI-dedicated data centers in 2023, and no other information is provided, it can be assumed that this value also applies to AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","2","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table that lists the GHG emissions associated with pre-training various models, including the Llama 7B model. The table shows that the Llama 7B model has GHG emissions of 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e) Llama 7B 63 Meta 356 14","The context provides a table that lists the GHG emissions associated with pre-training various models, including the Llama 7B model. The table shows that the Llama 7B model has GHG emissions of 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that Amazon's U.S. fleet included 11,800 electric delivery vans in 2023, up from more than 2,600 in 2022. Therefore, the total number of Amazon electric delivery vans added across 2022 and 2023 is the difference between these two numbers.","9,100+","electric delivery vans","[""amazon2023""]","is_blank","In 2023, our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that Amazon's U.S. fleet included 11,800 electric delivery vans in 2023, up from more than 2,600 in 2022. Therefore, the total number of Amazon electric delivery vans added across 2022 and 2023 is the difference between these two numbers."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides a table with energy consumption per model across three prompt sizes, including the o3 model. According to the table, the o3 model consumes 12.222 Wh for long prompts.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model Energy Consumption(10k input-1.5k output)(Wh) o3 12.222 ± 1.082","The context provides a table with energy consumption per model across three prompt sizes, including the o3 model. According to the table, the o3 model consumes 12.222 Wh for long prompts."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide information on the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024, as it only provides data for Microsoft's data centers and GPT-3 model.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide information on the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024, as it only provides data for Microsoft's data centers and GPT-3 model."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context mentions that with a max generation length of 512, it takes about 3-4 Joules for an output token. This information is provided for LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards.","3-4","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context mentions that with a max generation length of 512, it takes about 3-4 Joules for an output token. This information is provided for LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context explicitly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context explicitly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context mentions a Boston Consulting Group report that states AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030. However, it also notes that the reasoning behind this estimate is unclear and the underlying calculations are not detailed. Furthermore, applying observations made from individual projects to the entire planet's GHG emissions lacks scientific grounding, according to the context.","0","is_blank","[""luccioni2025c""]","is_blank","One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report... The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed... Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding.","The context mentions a Boston Consulting Group report that states AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030. However, it also notes that the reasoning behind this estimate is unclear and the underlying calculations are not detailed. Furthermore, applying observations made from individual projects to the entire planet's GHG emissions lacks scientific grounding, according to the context."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from [ref_id=patterson2021] states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.'","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context from [ref_id=patterson2021] states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.'"
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1 on a NVIDIA A40-48GB GPU.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1 on a NVIDIA A40-48GB GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate performs 1 billion queries per day and that the average energy for a comparable multi-purpose model (Flan-T5-xxl) is 0.083 kWh/1k queries. Therefore, the daily energy consumption can be estimated by multiplying the number of queries by the energy per query and converting to MWh.","83","MWh","[""luccioni2024""]","is_blank","1 billion queries per day and 0.083 kWh/1k queries","The context states that Google Translate performs 1 billion queries per day and that the average energy for a comparable multi-purpose model (Flan-T5-xxl) is 0.083 kWh/1k queries. Therefore, the daily energy consumption can be estimated by multiplying the number of queries by the energy per query and converting to MWh."
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average global data center PUE in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average global data center PUE in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain any information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context explicitly states that R&D staff costs, including equity, made up 49% of the total amortized model development costs for Gemini Ultra.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context explicitly states that R&D staff costs, including equity, made up 49% of the total amortized model development costs for Gemini Ultra."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the 2015 Study Panel was comprised of seventeen members.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that the 2015 Study Panel was comprised of seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, implying that this percentage is attributed to the manufacturing process of a client device.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, implying that this percentage is attributed to the manufacturing process of a client device."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The provided context suggests that models with more parameters do not necessarily consume more energy during inference. This is because different models with similar numbers of parameters often perform different amounts of work, and models with more parameters can sometimes consume less energy than smaller models due to factors such as batch size and memory consumption of operations.","0","is_blank","[""chung2025"", ""wu2021a"", ""schwartz2019""]","is_blank","['Models with more parameters do not necessarily result in longer training time nor higher carbon emissions.', 'different models with a similar number of parameters often perform different amounts of work.', 'Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.']","The provided context suggests that models with more parameters do not necessarily consume more energy during inference. This is because different models with similar numbers of parameters often perform different amounts of work, and models with more parameters can sometimes consume less energy than smaller models due to factors such as batch size and memory consumption of operations."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide information on the carbon emissions from OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information on the carbon emissions from OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate. This number is the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models.","11,390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context states that Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate. This number is the reported GHG emissions from the pre-training process for Meta's Llama 3 family of models."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9x increase in AI training infrastructure capacity over the 1.5 years between 2019 and 2021.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9x increase in AI training infrastructure capacity over the 1.5 years between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","The provided context does not contain any information regarding the distance from the Earth to the Sun. Therefore, it is not possible to answer this question based on the given context.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information regarding the distance from the Earth to the Sun. Therefore, it is not possible to answer this question based on the given context."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that the AlphaGo program recently defeated the current human champion at the game of Go, which is a true statement.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","For example, the AlphaGo program that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.","The context explicitly states that the AlphaGo program recently defeated the current human champion at the game of Go, which is a true statement."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions that continuous batching is a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, improving GPU utilization. This matches the description of the batching strategy asked in the question.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. It dynamically replaces completed requests with new ones, improving GPU utilization and reducing idle time.","The context mentions that continuous batching is a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, improving GPU utilization. This matches the description of the batching strategy asked in the question."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to the provided context, in the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was about 280% more expensive than the one selected by InferSave. However, the question asks for the percentage more expensive than the instance selected by InferSave in the 100 TPS SLO experiment, which is not directly provided in the context. Therefore, the answer cannot be determined based on the provided context.","is_blank","percent","[""is_blank""]","is_blank","is_blank","According to the provided context, in the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was about 280% more expensive than the one selected by InferSave. However, the question asks for the percentage more expensive than the instance selected by InferSave in the 100 TPS SLO experiment, which is not directly provided in the context. Therefore, the answer cannot be determined based on the provided context."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from [ref_id=luccioni2024] states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences","The context from [ref_id=luccioni2024] states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context mentions that the GPU energy usage for 1,000 queries spans from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This information is provided in the context with the ref_id=luccioni2025c.","0.06 to over 3,426 Wh","Wh","[""luccioni2025c""]","is_blank","GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)","The context mentions that the GPU energy usage for 1,000 queries spans from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity. This information is provided in the context with the ref_id=luccioni2025c."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context discusses the interdependence of AI ethics and sustainability, stating that they must go hand in hand to ensure a holistic societal impact. It also mentions that there is no one-size-fits-all solution for either ethics or sustainability, implying that a universal approach to AI ethics and sustainability is not feasible.","0","is_blank","[""luccioni2025b""]","is_blank","Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is not just beneficial but necessary, ensuring that AI advancements are not only technologically innovative and ethically sound but also maximizing their potential to engender sustainable advancement.","The context discusses the interdependence of AI ethics and sustainability, stating that they must go hand in hand to ensure a holistic societal impact. It also mentions that there is no one-size-fits-all solution for either ethics or sustainability, implying that a universal approach to AI ethics and sustainability is not feasible."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The provided context explicitly states that the gross carbon intensity of energy according to the U.S. average mix in 2021 was 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The provided context explicitly states that the gross carbon intensity of energy according to the U.S. average mix in 2021 was 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions that Hivemind is a PyTorch-based framework that enables collaborative deep learning training on heterogeneous hardware in a decentralized fashion. It is designed to handle peers that drop out at any stage of the training and uses a distributed hash table for metadata storage and a gradient averaging algorithm to reduce the impact of lost gradients. This framework is used for geo-distributed training and combines different techniques such as Delayed Parameter Updates, big-batch training, and aggressive communication quantization to reduce time and frequency of communication rounds, making training on heterogeneous devices and low-bandwidth networks possible.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions that Hivemind is a PyTorch-based framework that enables collaborative deep learning training on heterogeneous hardware in a decentralized fashion. It is designed to handle peers that drop out at any stage of the training and uses a distributed hash table for metadata storage and a gradient averaging algorithm to reduce the impact of lost gradients. This framework is used for geo-distributed training and combines different techniques such as Delayed Parameter Updates, big-batch training, and aggressive communication quantization to reduce time and frequency of communication rounds, making training on heterogeneous devices and low-bandwidth networks possible."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems. This includes a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint. This augmented view of transparency, which would integrate both social and environmental dimensions, is proposed to be called 'social transparency'","Social transparency","is_blank","[""luccioni2025b""]","is_blank","Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems. This includes a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint. This augmented view of transparency, which would integrate both social and environmental dimensions, is proposed to be called 'social transparency'"
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that for emotion classification, the linear model with sentence embeddings is among the top-performing models. However, it does not provide specific accuracy values for this model or any other models used in the classification experiments.","is_blank","is_blank","[""zschache2025""]","is_blank","Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","The context mentions that for emotion classification, the linear model with sentence embeddings is among the top-performing models. However, it does not provide specific accuracy values for this model or any other models used in the classification experiments."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The paper by Chen et al. in 2025 introduces model-attention disaggregation as an approach to improve the efficiency of LLM decoding. The LLM inference system developed using this approach is named Lamina.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The paper by Chen et al. in 2025 introduces model-attention disaggregation as an approach to improve the efficiency of LLM decoding. The LLM inference system developed using this approach is named Lamina."
"q093","How many parameters does the largest T5 model have?","The context mentions that the largest T5 model has 13 billion active parameters. This is the largest model size mentioned in the context.","13","parameters","[""morrison2025""]","is_blank","ranging in size from 20 million to 13 billion active parameters","The context mentions that the largest T5 model has 13 billion active parameters. This is the largest model size mentioned in the context."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in the JetMoE-8B model is stated as 8B in the provided context, specifically in the table of hyperparameters for JetMoE-8B.","8B","parameters","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632 Table 1: JetMoE-8B hyperparameters.","The total number of parameters in the JetMoE-8B model is stated as 8B in the provided context, specifically in the table of hyperparameters for JetMoE-8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context explicitly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022.","20","percent","[""li2025b""]","is_blank","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4]","The context explicitly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context from [ref_id=khan2025] provides a table of common carbon emission metrics in Green AI, which includes 'Carbon Intensity' defined as 'CO2 emissions per unit of electricity consumed'. This matches the definition provided in the question.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","TABLE I COMMON CARBON EMISSION METRICS IN GREEN AI Metric Unit Definition Reference Carbon Intensity gCO2/ kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context from [ref_id=khan2025] provides a table of common carbon emission metrics in Green AI, which includes 'Carbon Intensity' defined as 'CO2 emissions per unit of electricity consumed'. This matches the definition provided in the question."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","According to the provided context, in the final growth stage of FLM-101B training, the achieved FLOPs utilization percentage was 52.88%. This information is found in Table 2 of the context snippets.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 16 2 1 96 192 2304 162 51.90% 51 4 2 24 192 2304 160 51.30% 101 4 4 12 192 2160 165 52.88%","According to the provided context, in the final growth stage of FLM-101B training, the achieved FLOPs utilization percentage was 52.88%. This information is found in Table 2 of the context snippets."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the estimated cost of training OpenAI's GPT-4 model exceeds $100 million. This is the estimated amortized training cost for OpenAI's GPT-4.","100000000","USD","[""xia2024""]","is_blank","The estimated cost of training OpenAI’s GPT-4 model exceeds $100 million","The context states that the estimated cost of training OpenAI's GPT-4 model exceeds $100 million. This is the estimated amortized training cost for OpenAI's GPT-4."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context mentions that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","810","multiplier","[""wu2021a""]","is_blank","Efﬁciency Optimization: Optimization across the axes of al-gorithms, platforms, infrastructures, hardware can signiﬁcantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","The context mentions that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4. This indicates that the fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local was 41%.","41","multiplier","[""erben2023""]","is_blank","When comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4.","The context states that when comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4. This indicates that the fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local was 41%."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3.5 billion","liters","[""amazon2023""]","is_blank","By the end of 2023, AWS had invested in 15 water replenishment activities, returning water to the communities where it operates across 12 of our global infrastructure regions. These investments have expanded AWS replenishment activities to three new countries, bringing the total to 10: Australia, Brazil, India, Indonesia, Ireland, South Africa, Spain, Sweden, the UK, and the U.S. These activities help recharge groundwater, build wetlands, improve water quality, and reduce water loss in utility systems. In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context discusses a study that investigates the effects of prompt engineering techniques and custom tags on the energy consumption of LLMs during code completion tasks. The study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context discusses a study that investigates the effects of prompt engineering techniques and custom tags on the energy consumption of LLMs during code completion tasks. The study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context explicitly states that NVIDIA shipped 3.76 million data center GPUs in 2023, according to a study. However, the question asks for the number of GPUs shipped in 2024, which is not provided in the context.","is_blank","GPUs","[""is_blank""]","is_blank","is_blank","The context explicitly states that NVIDIA shipped 3.76 million data center GPUs in 2023, according to a study. However, the question asks for the number of GPUs shipped in 2024, which is not provided in the context."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context explicitly states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context explicitly states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10.","1.10","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is mentioned as the Finnish ETAIROS (Ethical AI for the Governance of the Society) project in the provided context.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is mentioned as the Finnish ETAIROS (Ethical AI for the Governance of the Society) project in the provided context."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context mentions that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, but it does not provide a breakdown of the costs or specify the amortized training costs for Gemini Ultra.","is_blank","USD","[""cottier2024""]","is_blank","is_blank","The context mentions that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, but it does not provide a breakdown of the costs or specify the amortized training costs for Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context explicitly states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems, and that these measures should also consider environmental risks. It also mentions that the Act requires climate risk assessment and mitigation, but no detailed reporting is mandated.","1","is_blank","[""ebert2024""]","is_blank","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems, and these measures should also consider environmental risks. The Act requires climate risk assessment and mitigation, but no detailed reporting is mandated.","The context explicitly states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems, and that these measures should also consider environmental risks. It also mentions that the Act requires climate risk assessment and mitigation, but no detailed reporting is mandated."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3.","9","µg/m³","[""han2024""]","is_blank","Under the Clean Air Act, the U.S. EPA is authorized to regulate the emission levels of criteria air pollutants, reducing concentrations to comply with the National Ambient Air Quality Standards (NAAQS). For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9µg/m 3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years.","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as 115 books. This information is provided in the context with the reference [ref_id=luccioni2025a].","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context states that a life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as 115 books. This information is provided in the context with the reference [ref_id=luccioni2025a]."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the provided context, the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times more than that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","According to the provided context, the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption for the DS Llama 70B model for inference on the FKTG dataset is mentioned in the context as 702.06 Wh. This information is provided in the table under the 'Energy Consumption(1k input-1k output)(Wh)' column.","702.06","Wh","[""samsi2024""]","is_blank","Table B1: Energy Consumption(1k input-1k output)(Wh) for DS Llama 70B is 702.06","The energy consumption for the DS Llama 70B model for inference on the FKTG dataset is mentioned in the context as 702.06 Wh. This information is provided in the table under the 'Energy Consumption(1k input-1k output)(Wh)' column."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions that Google's BERT-large contains roughly 350 million parameters. However, the question asks for the number of parameters in the large language model analyzed in the 2022 paper by Dodge et al., which is not provided in the given context.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context mentions that Google's BERT-large contains roughly 350 million parameters. However, the question asks for the number of parameters in the large language model analyzed in the 2022 paper by Dodge et al., which is not provided in the given context."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context mentions that when a resource's efficiency increases, overall consumption of that resource tends to increase, rather than decrease. This is referred to as Jevons' Paradox. In the case of AI, as the cost of training models decreases, the downstream impact may continue to grow, leading to an increase in both training cost for small models and total inference compute cost.","Jevons' Paradox","is_blank","[""morrison2025""]","is_blank","This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency in- creases, overall consumption of that resource tends to increase, rather than decrease.","The context mentions that when a resource's efficiency increases, overall consumption of that resource tends to increase, rather than decrease. This is referred to as Jevons' Paradox. In the case of AI, as the cost of training models decreases, the downstream impact may continue to grow, leading to an increase in both training cost for small models and total inference compute cost."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context does not provide specific information on the energy consumption of Meena training runs. However, it does provide the energy consumption of GPT-3 training, which is estimated to be 1287 MWh. Without the energy consumption of Meena, it is impossible to determine how many Meena training runs would use the same total energy as a single full training run of GPT-3.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the energy consumption of Meena training runs. However, it does provide the energy consumption of GPT-3 training, which is estimated to be 1287 MWh. Without the energy consumption of Meena, it is impossible to determine how many Meena training runs would use the same total energy as a single full training run of GPT-3."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the provided context, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. [...] image generation 2.907 3.31","According to Table 2 in the provided context, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context from [ref_id=strubell2019] provides the estimated CO2 emissions for an average American life in one year as 36,156 lbs.","36,156","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Consumption CO2e (lbs) American life, avg, 1 year 36,156","The context from [ref_id=strubell2019] provides the estimated CO2 emissions for an average American life in one year as 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","According to the provided context, the county in West Virginia with the highest per-household health cost due to air pollution from U.S. data centers is Marion County, with a cost of $1218.3.","Marion County","is_blank","[""han2024""]","is_blank","Table in the context: WV Marion 1218.3(978.0, 1458.5) 0.80","According to the provided context, the county in West Virginia with the highest per-household health cost due to air pollution from U.S. data centers is Marion County, with a cost of $1218.3."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","","","multiplier","[]","is_blank","is_blank",""
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, while the entire training process required 51,686 kWh. Therefore, the combined training and fine-tuning energy costs for the Bloomz-7B model were 51,686 kWh + 7,571 kWh = 59,257 kWh.","59,257","kWh","[""ebert2024""]","is_blank","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption.","The context states that the energy usage for fine-tuning the Bloomz-7B model required 7,571 kWh, while the entire training process required 51,686 kWh. Therefore, the combined training and fine-tuning energy costs for the Bloomz-7B model were 51,686 kWh + 7,571 kWh = 59,257 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context does not provide information on the total number of parameters in the final FLM-101B model.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total number of parameters in the final FLM-101B model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the energy cost of training a 6.1B parameter model and the energy cost per inference for a comparable model (BLOOMz-7B). Using this information, we can calculate the number of inferences needed to match the training energy cost.","592570000","inferences","[""luccioni2024""]","is_blank","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh and requires 592,570,000 inferences to sum to the training cost.","The context provides the energy cost of training a 6.1B parameter model and the energy cost per inference for a comparable model (BLOOMz-7B). Using this information, we can calculate the number of inferences needed to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was used.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was used."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table with the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for various BLOOMz models. For the BLOOMz-7B model, this number is 592,570,000 inferences.","592,570,000","inferences","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides a table with the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for various BLOOMz models. For the BLOOMz-7B model, this number is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany and was published as the FKTG-dataset. However, it does not explicitly mention the name of the dataset used for the German nuclear waste site objection texts classified in the experiments.","is_blank","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany and was published as the FKTG-dataset. However, it does not explicitly mention the name of the dataset used for the German nuclear waste site objection texts classified in the experiments."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The provided context does not contain information on the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The context only discusses the water consumption of GPT-3 and does not mention Meta's Llama 3.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The provided context does not contain information on the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The context only discusses the water consumption of GPT-3 and does not mention Meta's Llama 3."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","According to the provided context, the CO2e equivalent of the Evolved Transformer NAS is ~3 passengers taking a round trip between San Francisco and New York. Given that a single passenger round trip SF-NY is ~1.2t CO2e, the actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately 2.67 passengers taking a round trip between San Francisco and New York.","2.67","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. To put 1.2t CO2e into perspective, a single passenger round trip SF-NY is ~1.2t CO2e.","According to the provided context, the CO2e equivalent of the Evolved Transformer NAS is ~3 passengers taking a round trip between San Francisco and New York. Given that a single passenger round trip SF-NY is ~1.2t CO2e, the actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately 2.67 passengers taking a round trip between San Francisco and New York."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 data from the API platform OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","May 2025 data (Figure 2) indicates that of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1, DeepSeek V3, Mistral Nemo) release it indirectly. In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to May 2025 data from the API platform OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. Therefore, for the 13B model, which is smaller than the 65B model, fewer GPUs would be required. However, the context does not provide specific information on the number of A100 GPUs required for the 13B model.","is_blank","A100_80GB_GPU","[""samsi2024""]","is_blank","At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. Therefore, for the 13B model, which is smaller than the 65B model, fewer GPUs would be required. However, the context does not provide specific information on the number of A100 GPUs required for the 13B model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context explicitly states that the estimated full training run of the 6.1 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103,593,000 grams of CO2 emissions. Given that 1 metric ton is equal to 1,000,000 grams, this would be approximately 103.593 metric tons.","103.593","metric tons","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context explicitly states that the estimated full training run of the 6.1 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103,593,000 grams of CO2 emissions. Given that 1 metric ton is equal to 1,000,000 grams, this would be approximately 103.593 metric tons."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context mentions that experimental results reveal that optimization strategies can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide specific values for the total carbon emissions avoided in 2023.","is_blank","tCO2e","[""khan2025""]","is_blank","is_blank","The context mentions that experimental results reveal that optimization strategies can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide specific values for the total carbon emissions avoided in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only in a specific scenario of serving Llama2-7b at a range of request rates.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only in a specific scenario of serving Llama2-7b at a range of request rates."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to the context, the price per hour for an NVIDIA H20 is estimated to be $4.63/hr. This is mentioned in the table provided by Chen et al. (2024) with the ref_id chen2024.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to the context, the price per hour for an NVIDIA H20 is estimated to be $4.63/hr. This is mentioned in the table provided by Chen et al. (2024) with the ref_id chen2024."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, which contradicts the idea of gathering information automatically without needing to contact authors.","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon foot print analyses gather the information manually by writing to authors.","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, which contradicts the idea of gathering information automatically without needing to contact authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the provided context, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Based on the low- and high-growth scenarios considered, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively. However, the context does not provide information on the percentage of the data centers' total electricity cost that their public health cost equivalent to in 2023 using the average attribution method.","is_blank","percent","[""han2024""]","is_blank","is_blank","According to the provided context, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Based on the low- and high-growth scenarios considered, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively. However, the context does not provide information on the percentage of the data centers' total electricity cost that their public health cost equivalent to in 2023 using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. Since the question asks for the minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization, and the context does not provide information on the LLaMA-7B model, it is not possible to answer this question based on the provided context.","is_blank","A100_80GB_GPU","[""is_blank""]","is_blank","is_blank","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. Since the question asks for the minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization, and the context does not provide information on the LLaMA-7B model, it is not possible to answer this question based on the provided context."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which supports the claim that sustainable deployment techniques demonstrated up to a 45% reduction in carbon emissions after quantization.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context explicitly states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, which supports the claim that sustainable deployment techniques demonstrated up to a 45% reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context explicitly states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context explicitly states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that the JetMoE-8B project was trained with a budget of $100k and used 30,000 H100 GPU hours. Therefore, the approximate cost per H100 GPU-hour can be estimated by dividing the total budget by the total GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that the JetMoE-8B project was trained with a budget of $100k and used 30,000 H100 GPU hours. Therefore, the approximate cost per H100 GPU-hour can be estimated by dividing the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost can even exceed 120% of the electricity cost when training a Llama-3.1 scale model in Iowa. This indicates that the health cost is more than 120% of the electricity cost in this specific scenario.",">120","percent","[""han2024""]","is_blank","It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.","The context states that the total health cost can even exceed 120% of the electricity cost when training a Llama-3.1 scale model in Iowa. This indicates that the health cost is more than 120% of the electricity cost in this specific scenario."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that JetMoE-8B is trained with a batch size of 4M tokens with a sequence length of 4096. However, it does not provide the total number of tokens used for pre-training. Therefore, the exact number of tokens used to pre-train the JetMoE-8B model is not explicitly mentioned in the provided context.","is_blank","tokens","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B is trained with a batch size of 4M tokens with a sequence length of 4096. However, it does not provide the total number of tokens used for pre-training. Therefore, the exact number of tokens used to pre-train the JetMoE-8B model is not explicitly mentioned in the provided context."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the provided context, Amazon announced 36 renewable energy projects in the United Kingdom as of January 2024.","36","projects","[""amazon2023""]","is_blank","Project Location
Number of Projects
Total MW Capacity†
...
United Kingdom 36 901","According to the provided context, Amazon announced 36 renewable energy projects in the United Kingdom as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides Amazon's workforce representation by the numbers, including gender representation. In 2023, 46.6% of Amazon's workforce in the United States across all levels identified as men.","46.6","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) - 46.6%","The context provides Amazon's workforce representation by the numbers, including gender representation. In 2023, 46.6% of Amazon's workforce in the United States across all levels identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context explicitly states that Apple reports that its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context explicitly states that Apple reports that its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific information on the total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context does not provide specific information on the total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84. However, it does mention that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the granularity metric, which is defined as the ratio of calculation to communication time, to assess the scalability of models for distributed spot training. This metric is used to compare the performance of different models on the same hardware and to estimate training performance with additional resources.","The granularity metric","is_blank","[""erben2023""]","is_blank","For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context introduces the granularity metric, which is defined as the ratio of calculation to communication time, to assess the scalability of models for distributed spot training. This metric is used to compare the performance of different models on the same hardware and to estimate training performance with additional resources."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This deal involves using AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day.","640","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","A coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This deal involves using AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context explicitly states that water withdrawal refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses. This matches the definition of the term being asked for.","Water withdrawal","is_blank","[""li2025b""]","is_blank","It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context explicitly states that water withdrawal refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses. This matches the definition of the term being asked for."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that the average household in the US is equipped with an average of 25 connected devices, which includes smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context explicitly states that the average household in the US is equipped with an average of 25 connected devices, which includes smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout).","0.8-3,500","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The context states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which contradicts the statement in the question.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which contradicts the statement in the question."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that 10-50 queries on GPT-3 consume around half a liter of water. Therefore, to estimate the number of queries that consume approximately half a liter of water, we can use the range provided.","10-50","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context states that 10-50 queries on GPT-3 consume around half a liter of water. Therefore, to estimate the number of queries that consume approximately half a liter of water, we can use the range provided."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to the provided context, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. The MT-Bench score for JetMoE-8B-Chat is 6.681, while the score for Llama-2-13b-Chat is 6.650.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models","According to the provided context, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. The MT-Bench score for JetMoE-8B-Chat is 6.681, while the score for Llama-2-13b-Chat is 6.650."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. Therefore, the number of medium-length GPT-3 completions that could be produced with the water required to fill a single 500 mL bottle would be between 10 and 50.","Between 10 and 50","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. Therefore, the number of medium-length GPT-3 completions that could be produced with the water required to fill a single 500 mL bottle would be between 10 and 50."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context explicitly states that Mélange reduces deployment costs by up to 77% in conversational settings.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context explicitly states that Mélange reduces deployment costs by up to 77% in conversational settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context explicitly states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","10,000","round trips","[""han2024""]","is_blank","Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context explicitly states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from [ref_id=patterson2021] states that NVIDIA estimated that 80-90% of the ML workload is inference processing. This is further supported by [ref_id=luccioni2024] which mentions that a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training.","80-90%","percent","[""patterson2021"", ""luccioni2024""]","is_blank","NVIDIA estimated that 80-90% of the ML workload is inference processing [Leo19]; similarly, a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training [40].","The context from [ref_id=patterson2021] states that NVIDIA estimated that 80-90% of the ML workload is inference processing. This is further supported by [ref_id=luccioni2024] which mentions that a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context explicitly states that for all of the model experimentation and evaluation, a total of 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted. This includes running each of the 88 models on 3 datasets 10 times to ensure statistical significance of the measurements.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that for all of the model experimentation and evaluation, a total of 754.66 kWh of energy was used and 178.97 kg of CO2eq was emitted. This includes running each of the 88 models on 3 datasets 10 times to ensure statistical significance of the measurements."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The provided context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU to draw its maximum power at every moment in time. This indicates that estimating GPU energy consumption based on its TDP is not a reliable and accurate method.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.","The provided context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU to draw its maximum power at every moment in time. This indicates that estimating GPU energy consumption based on its TDP is not a reliable and accurate method."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context explicitly states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. It also provides specific energy consumption values for both models, with GPT-4o consuming around 2.875 Wh and GPT-4o mini consuming slightly higher at 3.098 Wh.","0","is_blank","[""jegham2025""]","is_blank","GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context explicitly states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. It also provides specific energy consumption values for both models, with GPT-4o consuming around 2.875 Wh and GPT-4o mini consuming slightly higher at 3.098 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a table that shows the throughput of Mixtral-CS for dense models with batch sizes of 1 and 2. The throughput for a batch size of 1 is 0.3 queries/second.","0.3","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)","The context provides a table that shows the throughput of Mixtral-CS for dense models with batch sizes of 1 and 2. The throughput for a batch size of 1 is 0.3 queries/second."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, and no direct environmental impact data was released during this period. However, it also mentions that after 2018, transparency improved as awareness of impacts grew and open-weights model releases became more common, with the direct release of environmental information peaking in 2022. After 2022, the trend reversed due to the introduction of increasingly commercial and proprietary models, reducing direct environmental disclosures.","0","is_blank","[""luccioni2025c""]","is_blank","From 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training (ranging from 0 to 33% each year); no direct environmental impact data was released during this period. This is expected, given that AI models of that era required significantly less compute and resource usage transparency was not yet common practice. From 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that from 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training, and no direct environmental impact data was released during this period. However, it also mentions that after 2018, transparency improved as awareness of impacts grew and open-weights model releases became more common, with the direct release of environmental information peaking in 2022. After 2022, the trend reversed due to the introduction of increasingly commercial and proprietary models, reducing direct environmental disclosures."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU is provided in the table under the 'On-demand Price ($/h)' column, which lists the price as 7.5164 USD per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM) On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The normalized on-demand hourly price for an H100 GPU is provided in the table under the 'On-demand Price ($/h)' column, which lists the price as 7.5164 USD per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","According to the provided context, GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh. The table titled 'Estimate of GPT-3’s operational water consumption footprint' provides water consumption footprint for different locations. However, the specific location for training GPT-3 is not public, so it is not possible to determine the exact amount of water used for cooling during OpenAI's GPT-4 training run.","is_blank","liters of water","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint.","According to the provided context, GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh. The table titled 'Estimate of GPT-3’s operational water consumption footprint' provides water consumption footprint for different locations. However, the specific location for training GPT-3 is not public, so it is not possible to determine the exact amount of water used for cooling during OpenAI's GPT-4 training run."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama-2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days in a month, the hourly cost can be calculated by dividing the monthly cost by the number of hours in a month.","17.33","USD per hour","[""griggs2024""]","is_blank","The context states: 'serving Llama-2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.'","The context states that serving Llama-2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days in a month, the hourly cost can be calculated by dividing the monthly cost by the number of hours in a month."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that to increase the model quality BLEU score from 5 to 40 requires a model 1,000 times larger in size for GPT-3 based language translation tasks.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000 times larger in size.","The context explicitly states that to increase the model quality BLEU score from 5 to 40 requires a model 1,000 times larger in size for GPT-3 based language translation tasks."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to the context, the training and neural architecture search for a Transformer model produced 626,155 lbs of CO2. Using the emissions-to-driving-distance ratio from a recent study, we can estimate the equivalent driving distance. However, the context does not provide the specific ratio, so we cannot calculate the exact distance.","is_blank","miles","[""dodge2022"", ""luccioni2023"", ""strubell2019""]","is_blank","The emissions produced by training a Transformer model with Neural Architecture search were found to be comparable to the lifetime carbon emissions of five cars [dodge2022]. The emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs of CO2 [luccioni2023]. The estimated CO2 emissions from training common NLP models, including Transformer with neural architecture search, are shown in Table 1 [strubell2019].","According to the context, the training and neural architecture search for a Transformer model produced 626,155 lbs of CO2. Using the emissions-to-driving-distance ratio from a recent study, we can estimate the equivalent driving distance. However, the context does not provide the specific ratio, so we cannot calculate the exact distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","According to the provided context, the BLOOMz-7B model was downloaded 606,096 times as of November 2023. The energy consumption per inference for this model is reported to be 1.0 × 10−4 kWh. Therefore, if every download resulted in 1 million inferences, the total energy consumption would be 606,096 * 1,000,000 * 1.0 × 10−4 kWh, which is 60.6096 MWh.","60.6096","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M Training energy (kWh) 51,686 25,634 17,052 10,505 Finetuning energy (kWh) 7,571 3,242 1,081 543 Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5 Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","According to the provided context, the BLOOMz-7B model was downloaded 606,096 times as of November 2023. The energy consumption per inference for this model is reported to be 1.0 × 10−4 kWh. Therefore, if every download resulted in 1 million inferences, the total energy consumption would be 606,096 * 1,000,000 * 1.0 × 10−4 kWh, which is 60.6096 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that the JetMoE-8B model was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","The JetMoE-8B model was trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The context explicitly states that the JetMoE-8B model was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","1000000001","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. This is the total number of FLOPS used to train GPT-3.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. This is the total number of FLOPS used to train GPT-3."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context states that the single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%. To estimate the total computational work performed during the 101B training stage, we can use the throughput data and the duration of the stage. However, the duration of the stage is not provided in the context.","is_blank","zettaFLOPs","[""li2025a""]","is_blank","['is_blank']","The context states that the single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%. To estimate the total computational work performed during the 101B training stage, we can use the throughput data and the duration of the stage. However, the duration of the stage is not provided in the context."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that a ResNet-50 model achieved 69.3% top-1 validation accuracy after SSL pre-training for 15 epochs, but it does not provide information on the top-1 accuracy on ImageNet associated with AlexNet 2012.","is_blank","percent","[""wu2021a""]","is_blank","is_blank","The context mentions that a ResNet-50 model achieved 69.3% top-1 validation accuracy after SSL pre-training for 15 epochs, but it does not provide information on the top-1 accuracy on ImageNet associated with AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context does not provide information on the exact number of A800 GPUs used for training the FLM-101B model. However, it mentions that the model is trained on a cluster of 24 DGX-A800 GPU servers, each with 8 ×80G GPUs. Without specific information on how many GPUs were used per server, it is not possible to determine the total number of A800 GPUs used.","is_blank","GPUs","[""li2025a""]","is_blank","The model is trained on a cluster of 24 DGX-A800 GPU servers, each with 8 ×80G GPUs.","The context does not provide information on the exact number of A800 GPUs used for training the FLM-101B model. However, it mentions that the model is trained on a cluster of 24 DGX-A800 GPU servers, each with 8 ×80G GPUs. Without specific information on how many GPUs were used per server, it is not possible to determine the total number of A800 GPUs used."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context from [ref_id=strubell2019] states that the estimated CO2 emissions from training a Transformer model with Neural Architecture Search is 626,155 lbs. According to the EPA, the average American emits about 16.2 metric tons of CO2 per year, which is equivalent to 35,714.3 lbs. Therefore, the estimated CO2 emissions from performing NAS to train a Transformer-based model for machine translation is equivalent to approximately 17.5 average American lifetimes.","17.5","lifetimes","[""strubell2019""]","is_blank","Table 1 from [ref_id=strubell2019]","The context from [ref_id=strubell2019] states that the estimated CO2 emissions from training a Transformer model with Neural Architecture Search is 626,155 lbs. According to the EPA, the average American emits about 16.2 metric tons of CO2 per year, which is equivalent to 35,714.3 lbs. Therefore, the estimated CO2 emissions from performing NAS to train a Transformer-based model for machine translation is equivalent to approximately 17.5 average American lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train.","25,000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems are estimated to generate 123,000 MWh annually and avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems are estimated to generate 123,000 MWh annually and avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that a large-scale model is trained across many servers in a distributed manner, and that each server includes GPUs responsible for the primary computation. It also mentions that the power required to operate the entire data center is measured, including both the direct power consumption of computing equipment and the additional overhead for cooling and maintaining the data center. This suggests that the framework used to deploy large language models across multiple GPUs and nodes is distributed training.","1","is_blank","[""khan2025""]","[""https://openreview.net/forum?id=khan2025""]","A large-scale model is trained across many servers in a distributed manner. Each server includes GPUs responsible for the primary computation. To accurately monitor the power consumption over time, a local power distribution unit (PDU), capable of measuring the provided power, is attached to each server. Aggregating these measurements yields a highly precise figure of the total energy consumption attributable to the model’s computations.","The context mentions that a large-scale model is trained across many servers in a distributed manner, and that each server includes GPUs responsible for the primary computation. It also mentions that the power required to operate the entire data center is measured, including both the direct power consumption of computing equipment and the additional overhead for cooling and maintaining the data center. This suggests that the framework used to deploy large language models across multiple GPUs and nodes is distributed training."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context states that using two nodes increased energy consumption by a factor that depends on the model, and specifically for the Llama 3.1 70B model, energy consumption increased by a factor of 1.89. However, this factor is for the duration, not the energy consumption. The factor for energy consumption is 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2 in the context shows that the energy consumption ratio for the Llama 3.1 70B model is 1.95 when deployed on two nodes instead of one.","The context states that using two nodes increased energy consumption by a factor that depends on the model, and specifically for the Llama 3.1 70B model, energy consumption increased by a factor of 1.89. However, this factor is for the duration, not the energy consumption. The factor for energy consumption is 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The provided context does not contain information on the gallons of water consumed per ChatGPT user session in 2023. However, it does provide information on the estimated 2025 annual water consumption of GPT-4o, which is presented in Figure 5.","is_blank","gallons of water","[""jegham2025""]","is_blank","Figure 5: (Bottom Left) The estimated 2025 annual water consumption of GPT-4o.","The provided context does not contain information on the gallons of water consumed per ChatGPT user session in 2023. However, it does provide information on the estimated 2025 annual water consumption of GPT-4o, which is presented in Figure 5."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","35,000","homes","[""jegham2025""]","is_blank","GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, which implies that traditional models do not achieve accuracy comparable to large language models in this benchmark.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, which implies that traditional models do not achieve accuracy comparable to large language models in this benchmark."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where Evolved Transformer was run is 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where Evolved Transformer was run is 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that there will be approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024.","772","queries","[""jegham2025""]","is_blank","There will be approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024.","The context states that there will be approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The provided context includes a table comparing the performance of four different models on various benchmarks, including the OpenLLM Leaderboard. The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is listed as 53.0.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming.","The provided context includes a table comparing the performance of four different models on various benchmarks, including the OpenLLM Leaderboard. The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is listed as 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022. It discusses the carbon footprint of AI, the use of renewable energy, and the growth of AI, but does not provide the exact number of training runs.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022. It discusses the carbon footprint of AI, the use of renewable energy, and the growth of AI, but does not provide the exact number of training runs."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context mentions a proposal to eliminate the open-source exemption, which currently allows open-source models to bypass reporting obligations. This implies that open-source models are not exempt from reporting their energy consumption under the AI Act.","0","is_blank","[""ebert2024""]","is_blank","Elimination of open-source exemption : Remove the exemption that allows open-source models to bypass reporting obligations.","The context mentions a proposal to eliminate the open-source exemption, which currently allows open-source models to bypass reporting obligations. This implies that open-source models are not exempt from reporting their energy consumption under the AI Act."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context does not provide information on the US national datacenter average Power Usage Effectiveness (PUE) in 2020. However, it does provide the global average PUE for data centers in 2023 and 2020, as well as the EU average PUE in 2023.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide information on the US national datacenter average Power Usage Effectiveness (PUE) in 2020. However, it does provide the global average PUE for data centers in 2023 and 2020, as well as the EU average PUE in 2023."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a table that shows the KV Cache Size for the OPT-2.7B model on a g4dn.xlarge instance with different batch sizes. When the batch size is 32, the KV Cache Size is 8GB.","8","GB","[""kim2025""]","is_blank","KV Cache Size (GB) / Batch Size: 2 / 4 / 8 / 16 / 32 / KV Cache Size: 0 / 2 / 4 / 6 / 8","The context provides a table that shows the KV Cache Size for the OPT-2.7B model on a g4dn.xlarge instance with different batch sizes. When the batch size is 32, the KV Cache Size is 8GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that for the four notable models studied in-depth by Cottier et al. (2024), R&D staff costs including equity are between 29% and 49% of the total amortized cost.","29% to 49%","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context states that for the four notable models studied in-depth by Cottier et al. (2024), R&D staff costs including equity are between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package. This package uses the NVIDIA Management Library (NVML) and the Intel RAPL files to track the power usage of GPU and CPU.","CodeCarbon","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package. This package uses the NVIDIA Management Library (NVML) and the Intel RAPL files to track the power usage of GPU and CPU."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to the provided context, an analysis of 100 news articles on ChatGPT's energy use revealed that 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This analysis was conducted on articles published as of April 11, 2025, that appear when searching for 'ChatGPT energy consumption' on Google News.","53","percent","[""luccioni2025c""]","is_blank","75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","According to the provided context, an analysis of 100 news articles on ChatGPT's energy use revealed that 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This analysis was conducted on articles published as of April 11, 2025, that appear when searching for 'ChatGPT energy consumption' on Google News."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly mentions that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context explicitly mentions that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context explicitly states that increasing the number of shards always increases the energy per second for inference with LLaMA 65B, even at the same batch size. This indicates that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","Overall, we see an average increase in energy per second with the number of shards. While there is a slight correlation as energy per second increases with increasing batch size, increasing the number of shards always increases the wattage. Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).","The context explicitly states that increasing the number of shards always increases the energy per second for inference with LLaMA 65B, even at the same batch size. This indicates that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. It also mentions that an H100 is 0.1% rare earth metal by mass. Therefore, to manufacture a single H100 GPU, 0.0011 kL of water would be consumed for mining the rare earth materials.","0.0011","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU. Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in total, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. It also mentions that an H100 is 0.1% rare earth metal by mass. Therefore, to manufacture a single H100 GPU, 0.0011 kL of water would be consumed for mining the rare earth materials."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context proposes an interpretation that would bring energy consumption for single inferences back into the scope and includes both single and overall inferences as a reporting category. It also mentions the elimination of the open-source exemption, which removes the exemption that allows open-source models to bypass reporting obligations.","1","is_blank","[""ebert2024""]","is_blank","Elimination of open-source exemption : Remove the exemption that allows open-source models to bypass reporting obligations.","The context proposes an interpretation that would bring energy consumption for single inferences back into the scope and includes both single and overall inferences as a reporting category. It also mentions the elimination of the open-source exemption, which removes the exemption that allows open-source models to bypass reporting obligations."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","[""https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894"", ""https://www.nytimes.com/2020/09/23/world/asia/china-climate-change.html""]","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The context states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, with the total public health impact of U.S. data centers estimated to reach $11.7 billion and $20.9 billion in 2028, respectively, under the low- and high-growth scenarios. However, it does not provide the exact total public health cost in 2023 based on the average attribution method.","is_blank","USD","[""han2024""]","is_blank","is_blank","The context states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, with the total public health impact of U.S. data centers estimated to reach $11.7 billion and $20.9 billion in 2028, respectively, under the low- and high-growth scenarios. However, it does not provide the exact total public health cost in 2023 based on the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","According to the provided context, the energy consumption of the o3 model for a long prompt is 12.222 Wh, while the energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. Therefore, the energy consumption of the o3 model is greater than that of GPT-4.1 nano by a factor of approximately 14.77 (12.222 / 0.827).","14.77","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model: o3, Energy Consumption(10k input-1.5k output)(Wh): 12.222 ± 1.082; Model: GPT-4.1 nano, Energy Consumption(10k input-1.5k output)(Wh): 0.827 ± 0.094","According to the provided context, the energy consumption of the o3 model for a long prompt is 12.222 Wh, while the energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. Therefore, the energy consumption of the o3 model is greater than that of GPT-4.1 nano by a factor of approximately 14.77 (12.222 / 0.827)."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that in the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines.","15-77%","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that in the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The carbon emissions of FLM-101B are listed in Table 3 of the provided context, which states that the net carbon emissions for FLM-101B are 26 metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The carbon emissions of FLM-101B are listed in Table 3 of the provided context, which states that the net carbon emissions for FLM-101B are 26 metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide information on the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. It also mentions that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. It also mentions that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context explicitly states that as a result of Moore's law scaling and architectural optimization, GPU theoretical performance per watt doubles every 3-4 years according to Sun et al., 2019.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context explicitly states that as a result of Moore's law scaling and architectural optimization, GPU theoretical performance per watt doubles every 3-4 years according to Sun et al., 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama is used for local AI model deployment, which ensures data privacy by processing entirely on-device. It also supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. The context further specifies that a 4-bit quantization strategy is used, which substantially reduces computational and memory requirements without significantly compromising model performance.","Ollama","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions. In this work, we use a 4-bit quantization strategy (b = 4), which substantially reduces computational and memory requirements without significantly compromising model performance.","The context mentions that Ollama is used for local AI model deployment, which ensures data privacy by processing entirely on-device. It also supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. The context further specifies that a 4-bit quantization strategy is used, which substantially reduces computational and memory requirements without significantly compromising model performance."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the datasets are accessed on-demand via shards in the tar format with the WebDataset library, which is chosen due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format. The storage service used is Backblaze (B2), an independent S3 storage provider.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","Backblaze (B2) [4]","The context mentions that the datasets are accessed on-demand via shards in the tar format with the WebDataset library, which is chosen due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format. The storage service used is Backblaze (B2), an independent S3 storage provider."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context from [ref_id=zschache2025] states that there is a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption.","1","is_blank","[""zschache2025""]","is_blank","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context from [ref_id=zschache2025] states that there is a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Senator Edward J. Markey (D-MA)","is_blank","[""ebert2024""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","The bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","The context explicitly states that the bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to the table provided in the context, the price per hour for an NVIDIA H100 is $11.06.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to the table provided in the context, the price per hour for an NVIDIA H100 is $11.06."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The provided context does not explicitly state the estimated average GPU lifetime before retirement in AI data centers in 2024. However, it mentions that an average of 1 to 2 failures per week occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs, and even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years. This information is from 2024, which is close to the requested year of 2024.","3.7","years","[""cottier2024""]","is_blank","Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.","The provided context does not explicitly state the estimated average GPU lifetime before retirement in AI data centers in 2024. However, it mentions that an average of 1 to 2 failures per week occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs, and even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years. This information is from 2024, which is close to the requested year of 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. However, the question asks for the minimum number of GPUs required for LLaMA-13B, not LLaMA-65B. As the context does not provide information on the minimum number of GPUs required for LLaMA-13B, the answer cannot be determined based on the provided context.","is_blank","V100_32GB_GPUs","[""is_blank""]","is_blank","is_blank","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. However, the question asks for the minimum number of GPUs required for LLaMA-13B, not LLaMA-65B. As the context does not provide information on the minimum number of GPUs required for LLaMA-13B, the answer cannot be determined based on the provided context."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4x the estimate that forms the basis for the 'five cars' number, which is estimated to emit 284 metric tons CO2e.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number.","The context states that Google reports training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4x the estimate that forms the basis for the 'five cars' number, which is estimated to emit 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours)."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh. This value is provided as the off-site water consumption for each kWh of electricity consumption, which measures the electricity water intensity factor (EWIF).","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh. This value is provided as the off-site water consumption for each kWh of electricity consumption, which measures the electricity water intensity factor (EWIF)."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context from [ref_id=dodge2022] states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is also confirmed by [ref_id=wu2021b] which states that Google's PUE improved from 1.21 in 2008 to 1.10 in 2021.","1.10","PUE","[""dodge2022"", ""wu2021b""]","[""https://doi.org/10.1145/3494526""]","Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021 [dodge2022]. Google’s PUE improved from 1.21 in 2008 to 1.10 in 2021 [wu2021b].","The context from [ref_id=dodge2022] states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021. This is also confirmed by [ref_id=wu2021b] which states that Google's PUE improved from 1.21 in 2008 to 1.10 in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the provided context, AWS states that in North America, it can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the provided context, AWS states that in North America, it can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context mentions that in a single cloud instance, the majority of the electricity consumption is due to the GPU, with the GPU accounting for almost 3/4 of electricity consumption. This suggests that GPUs account for a significant percentage of the total provisioned power in a typical datacenter.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3533585""]","As expected the GPU accounts for almost 3/4 of electricity consumption.","The context mentions that in a single cloud instance, the majority of the electricity consumption is due to the GPU, with the GPU accounting for almost 3/4 of electricity consumption. This suggests that GPUs account for a significant percentage of the total provisioned power in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. Given that the training infrastructure consisted of a cluster of 12 nodes, we can infer that the total number of H100 GPUs used for the training was 96, as each node had 8 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. Given that the training infrastructure consisted of a cluster of 12 nodes, we can infer that the total number of H100 GPUs used for the training was 96, as each node had 8 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context explicitly states that when actively training, the average GPU power is over 600W during the first 300 logging steps of OLMo 2 7B training.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context explicitly states that when actively training, the average GPU power is over 600W during the first 300 logging steps of OLMo 2 7B training."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context from [ref_id=strubell2019] provides a table that lists the estimated CO2 emissions from training common NLP models, including 'Human life, avg, 1 year' which is given as 11,023 lbs. This value is estimated for an average human life in one year globally.","11,023","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Whereas a decade ago most Consumption CO 2e (lbs) Human life, avg, 1 year 11,023","The context from [ref_id=strubell2019] provides a table that lists the estimated CO2 emissions from training common NLP models, including 'Human life, avg, 1 year' which is given as 11,023 lbs. This value is estimated for an average human life in one year globally."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that the Max-Performance instance (g6e.xlarge) has a cost of $2.699, which is about 280% more expensive than InferSave's top choice. Given that the cost of InferSave's top choice is not provided, we cannot calculate the exact percentage difference. However, we can infer that the Max-Performance instance is significantly more expensive than InferSave's top choice.","is_blank","percent","[""kim2025""]","is_blank","The Max-Performance instance (g6e.xlarge) has a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that the Max-Performance instance (g6e.xlarge) has a cost of $2.699, which is about 280% more expensive than InferSave's top choice. Given that the cost of InferSave's top choice is not provided, we cannot calculate the exact percentage difference. However, we can infer that the Max-Performance instance is significantly more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","According to the provided context, for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This suggests that these GPU architectures are more energy-efficient for models generating only a single classification token.","is_blank","is_blank","[""zschache2025""]","is_blank","['For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.']","According to the provided context, for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This suggests that these GPU architectures are more energy-efficient for models generating only a single classification token."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, which aligns with the definition of Green AI involving providing the financial cost of these processes.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","The context explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, which aligns with the definition of Green AI involving providing the financial cost of these processes."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context explicitly states that electronic waste reached 62 million tonnes in 2022.","62","metric tons","[""luccioni2025a""]","is_blank","Electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reached 62 million tonnes in 2022.","The context explicitly states that electronic waste reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the average system power per processor for both TPU v2 and V100 GPU. The average system power per processor for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference in average system power per processor between the TPU v2 and the V100 GPU is 104 Watts.","104","Watts","[""patterson2021""]","is_blank","The average system power per processor for TPU v2 is 221 Watts and for V100 GPU is 325 Watts.","The context provides the average system power per processor for both TPU v2 and V100 GPU. The average system power per processor for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference in average system power per processor between the TPU v2 and the V100 GPU is 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is provided in the abstract of the research paper 'Making AI Less “Thirsty”: Uncovering and Addressing the Secret Water Footprint of AI Models' by Pengfei Li et al.","700,000 liters","liters","[""li2025b""]","is_blank","The abstract of the research paper 'Making AI Less “Thirsty”: Uncovering and Addressing the Secret Water Footprint of AI Models' by Pengfei Li et al.","The context states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is provided in the abstract of the research paper 'Making AI Less “Thirsty”: Uncovering and Addressing the Secret Water Footprint of AI Models' by Pengfei Li et al."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","Facebook’s recommendation and ranking model sizes have increased by 20× between 2019 and 2021.","The context explicitly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the provided context, OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score of 0.884. This indicates that o3-mini is the model that ranked highest in the recent eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884)","According to the provided context, OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score of 0.884. This indicates that o3-mini is the model that ranked highest in the recent eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from [ref_id=wu2021b] states that the current averages of smartphone lifetimes are less than 3 years, contributing to e-waste concerns.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","The context from [ref_id=wu2021b] states that the current averages of smartphone lifetimes are less than 3 years, contributing to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context mentions that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models, but does not provide specific values or comparisons to other types of GPUs.","is_blank","is_blank","[""erben2023""]","is_blank","The context mentions that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models, but does not provide specific values or comparisons to other types of GPUs.","The context mentions that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models, but does not provide specific values or comparisons to other types of GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, resulting in lower power draw during inference compared to diffusion models, which are significantly more compute-intensive.","1","is_blank","[""chung2025""]","is_blank","Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, resulting in lower power draw during inference compared to diffusion models, which are significantly more compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides a table with the percentage of Amazon's People Managers globally who identified as women in 2023, which is 26.1%.","26.1","percent","[""amazon2023""]","is_blank","2021
2022
2023
2021
2022
2023
0.3% 1.7% 2.6%
2021
2022
2023
2021
2022
2023
16.6% 31.9%26.1%23.5%","The context provides a table with the percentage of Amazon's People Managers globally who identified as women in 2023, which is 26.1%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction for computing hardware costs ranges from 61% to 76% for the four key models analyzed by Cottier et al. (2025).","61-76%","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%. However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when excluding equity, the fraction for computing hardware costs ranges from 61% to 76% for the four key models analyzed by Cottier et al. (2025)."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context explicitly states that metrics such as F1 score and overall accuracy may decline slightly post-optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is further supported by the table comparing performance metrics before and after optimization, which shows slight declines in accuracy and F1 scores for some models.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context explicitly states that metrics such as F1 score and overall accuracy may decline slightly post-optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is further supported by the table comparing performance metrics before and after optimization, which shows slight declines in accuracy and F1 scores for some models."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. The context mentions that the EPA's value is used to convert power to estimated CO2 emissions, but the exact value is not provided in the context.","is_blank","lbs/kWh","[""strubell2019""]","[""https://bit.ly/2JTbGnI""]","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt (2) This conversion takes into account the relative proportions of different energy sources (primarily natural gas, coal, nuclear and renewable) consumed to produce energy in the United States.","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. The context mentions that the EPA's value is used to convert power to estimated CO2 emissions, but the exact value is not provided in the context."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context states that a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment. This range is provided as the projected electricity consumption by global AI in 2027.","85 – 134 TWh","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]","The context states that a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment. This range is provided as the projected electricity consumption by global AI in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","In Europe, we delivered 150 million packages via EVs.","The context states that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context mentions that the online inference workload evaluates a total of 3000 requests with a pattern of 128 input tokens and 512 output tokens. Therefore, the total number of tokens processed during the entire online inference workload evaluation is (3000 requests * (128 input tokens + 512 output tokens)) = 1,200,000 tokens.","1200000","tokens","[""kim2025""]","is_blank","Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The context mentions that the online inference workload evaluates a total of 3000 requests with a pattern of 128 input tokens and 512 output tokens. Therefore, the total number of tokens processed during the entire online inference workload evaluation is (3000 requests * (128 input tokens + 512 output tokens)) = 1,200,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, including sectors like oil and gas exploration. It also mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, including sectors like oil and gas exploration. It also mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is up to 80%. This information is found in the context snippets from the paper.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region.","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is up to 80%. This information is found in the context snippets from the paper."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","According to the provided context, the most energy-intensive task is image generation, and the least energy-intensive task is text classification. The mean energy consumption for image generation is 2.9kWh, while the mean energy consumption for text classification is 0.002 KwH. This means that image generation requires over 1450 times more energy than text classification for the same number of inferences.","1450","times","[""luccioni2024""]","is_blank","Mean energy consumption for image generation is 2.9kWh and for text classification is 0.002 KwH, which means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","According to the provided context, the most energy-intensive task is image generation, and the least energy-intensive task is text classification. The mean energy consumption for image generation is 2.9kWh, while the mean energy consumption for text classification is 0.002 KwH. This means that image generation requires over 1450 times more energy than text classification for the same number of inferences."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models do not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models do not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","According to the provided context, as of January 2024, there were 244 Amazon Renewable Energy Projects announced in the United States, with a total MW capacity of 17,706.","244","projects","[""amazon2023""]","is_blank","Project Location
Number of Projects
Total MW Capacity
...
United States 244 17,706
...","According to the provided context, as of January 2024, there were 244 Amazon Renewable Energy Projects announced in the United States, with a total MW capacity of 17,706."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context states that Amazon achieved its goal of matching 100% of the electricity consumed by Amazon with renewable energy sources in 2023, up from 90% in 2022. However, it does not provide specific information on the percentage of power usage covered by renewable energy for AWS in 2018.","is_blank","percent","[""amazon2023""]","is_blank","is_blank","The context states that Amazon achieved its goal of matching 100% of the electricity consumed by Amazon with renewable energy sources in 2023, up from 90% in 2022. However, it does not provide specific information on the percentage of power usage covered by renewable energy for AWS in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors propose that energy consumption from AI inferences should be reported at the cumulative server level for better optimization of AI models and algorithms for energy efficiency. They also suggest that the PUE factor of each data center should be reported and published, which provides a useful estimate of overall energy consumption.","Cumulative server level and PUE factor","is_blank","[""ebert2024""]","is_blank","The authors propose a specific interpretation of the AI Act to bring reporting on the previously unaddressed energy consumption from AI inferences back into the scope. They also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications. Furthermore, for the purpose of energy consumption reporting, they compare levels of measurement within data centers and recommend measurement at the cumulative server level.","The authors propose that energy consumption from AI inferences should be reported at the cumulative server level for better optimization of AI models and algorithms for energy efficiency. They also suggest that the PUE factor of each data center should be reported and published, which provides a useful estimate of overall energy consumption."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context explicitly states that the GPU accounts for 74% of the total electricity consumption when training BERT base on a single NVIDIA TITAN X GPU in a commodity server. This percentage is also presented in Table 1.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context explicitly states that the GPU accounts for 74% of the total electricity consumption when training BERT base on a single NVIDIA TITAN X GPU in a commodity server. This percentage is also presented in Table 1."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. However, the question asks for the number of NVIDIA A100-80GB GPUs required to serve a Llama2-70b model at BF16 precision, which is 2.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The context states that 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs'.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. However, the question asks for the number of NVIDIA A100-80GB GPUs required to serve a Llama2-70b model at BF16 precision, which is 2."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context explicitly states that the iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period. This is further supported by the statement that an average of 20% operational energy footprint reduction was achieved every 6 months across the machine learning hardware-software stack.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context explicitly states that the iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period. This is further supported by the statement that an average of 20% operational energy footprint reduction was achieved every 6 months across the machine learning hardware-software stack."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context does not contain any information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context explicitly states that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context explicitly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to both aim to use AI in climate-positive applications and improve the (environmental) sustainability of AI approaches themselves.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context explicitly states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to both aim to use AI in climate-positive applications and improve the (environmental) sustainability of AI approaches themselves."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context mentions that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. This information is based on the analytical model used in the study.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context mentions that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. This information is based on the analytical model used in the study."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to the provided context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation when restoring requests, while Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","1","is_blank","[""chung2025""]","is_blank","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.","According to the provided context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation when restoring requests, while Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context includes a reference to Google's 2024 Environmental Report, which would contain the information about the percentage increase in GHG emissions since 2019. However, the exact percentage is not provided in the context.","is_blank","percent","[""jegham2025"", ""luccioni2025a""]","[""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","is_blank","The context includes a reference to Google's 2024 Environmental Report, which would contain the information about the percentage increase in GHG emissions since 2019. However, the exact percentage is not provided in the context."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to the context, McKinsey projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030. However, the question asks for the percentage of US national electricity consumption, not the total US energy demand. Therefore, the exact percentage of US national electricity consumption that data centers are anticipated to account for in 2030 is not provided in the context.","is_blank","percent","[""fernandez2025""]","[""https://arxiv.org/abs/2504.17674v1""]","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).","According to the context, McKinsey projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030. However, the question asks for the percentage of US national electricity consumption, not the total US energy demand. Therefore, the exact percentage of US national electricity consumption that data centers are anticipated to account for in 2030 is not provided in the context."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that the Pause and Resume optimization for the training of a 6B Parameter Transformer on 256 A100s resulted in a CO2 emissions decrease of up to 80%. This is shown in Figure 7, which specifically mentions the Pause and Resume optimization.","80","percent","[""dodge2022""]","is_blank","Figure 7. Optimization results for the training of a 6B Parameter Transformer on 256 A100s. Without optimization, the job ran for approximately 8 days and consumed 13,812 kWh. East USWest USWest US2West US3Central US N. Central USS. Central USW. Central US CanadaFrance Germany West EuropeNorth Europe NorwayUK SouthAustralia 0 10 20 30 40 50 60 70 80CO2 emissions decrease in % 6h 12h 18h 24h (a) Flexible Start optimization. East USWest USWest US2West US3Central US N. Central USS. Central USW. Central US CanadaFrance Germany West EuropeNorth Europe NorwayUK SouthAustralia 0 1 2 3 4 5CO2 emissions decrease in % 25% 50% 75% 100% (b) Pause and Resume optimization.","The context states that the Pause and Resume optimization for the training of a 6B Parameter Transformer on 256 A100s resulted in a CO2 emissions decrease of up to 80%. This is shown in Figure 7, which specifically mentions the Pause and Resume optimization."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B due to its sparse activation mechanism that only activates 2B parameters for each input token, despite having 8B parameters in total.","70","percent","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B due to its sparse activation mechanism that only activates 2B parameters for each input token, despite having 8B parameters in total."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context explicitly states that the carbon footprint of training the BERT large language model was quantified as reaching 626,155 pounds of CO2 emissions in a seminal 2019 study.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context explicitly states that the carbon footprint of training the BERT large language model was quantified as reaching 626,155 pounds of CO2 emissions in a seminal 2019 study."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained with an estimated energy of 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. It also mentions that optimizing the MoE layer performance is key to improving the overall cost of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","The MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average. Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context explicitly states that the MoE layer is the most time-consuming layer in LLM fine-tuning, accounting for 85% of the overall execution time on average. It also mentions that optimizing the MoE layer performance is key to improving the overall cost of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that for a GPU memory capacity of 48GB, the maximum batch size supported for fine-tuning Mixtral is 4. However, the question asks for the maximum batch size supported for fine-tuning Mixtral with a dense setup on the Hellaswag dataset. As the context does not provide this specific information, the answer is marked as 'is_blank'.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context mentions that for a GPU memory capacity of 48GB, the maximum batch size supported for fine-tuning Mixtral is 4. However, the question asks for the maximum batch size supported for fine-tuning Mixtral with a dense setup on the Hellaswag dataset. As the context does not provide this specific information, the answer is marked as 'is_blank'."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training. This is in contrast to NLP tasks, which saw a significant performance drop of 41% with intercontinental training.","1","is_blank","[""erben2023""]","is_blank","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). [...] However, as the CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.","The context states that for CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training. This is in contrast to NLP tasks, which saw a significant performance drop of 41% with intercontinental training."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context does not contain any information about the number of hectares of land occupied by new AI data centers globally in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of hectares of land occupied by new AI data centers globally in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the most efficient region emits approximately 7k grams of CO2, while the least efficient region emits approximately 26k grams of CO2. The difference between these two values is the approximate range of CO2 emissions between the most and least efficient regions.","19","grams","[""dodge2022""]","is_blank","Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that the most efficient region emits approximately 7k grams of CO2, while the least efficient region emits approximately 26k grams of CO2. The difference between these two values is the approximate range of CO2 emissions between the most and least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table with the water consumption for training various models, including the OLMo 60M model. The water consumption for training this model is listed as 1.6 kL, which is equivalent to 5 days of water usage for one person in the US.","5","days","[""morrison2025""]","is_blank","Table 2: We list the estimated power usage, carbon emissions, and water consumption from training our dense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and a mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We find that the environmental impact is quite high, even for our relatively small models. Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years. ... OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table with the water consumption for training various models, including the OLMo 60M model. The water consumption for training this model is listed as 1.6 kL, which is equivalent to 5 days of water usage for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context mentions that one technology company's self-owned data centers directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023. However, it does not provide specific information on the water consumption of Google's DeepMind AlphaFold servers.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context mentions that one technology company's self-owned data centers directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023. However, it does not provide specific information on the water consumption of Google's DeepMind AlphaFold servers."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can further reduce the cost of fine-tuning LLMs. This implies that adding compute resources can increase costs initially, but ultimately lead to cost savings.","1","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers can further reduce the cost of fine-tuning LLMs. This implies that adding compute resources can increase costs initially, but ultimately lead to cost savings."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to the provided context, the total energy consumption for training the FLM-101B model is not explicitly stated. However, the context does mention the estimated floating-point operations (zettaFLOPs) of the training process, which is a measure of computational complexity rather than energy consumption.","is_blank","kWh","[""is_blank""]","is_blank","is_blank","According to the provided context, the total energy consumption for training the FLM-101B model is not explicitly stated. However, the context does mention the estimated floating-point operations (zettaFLOPs) of the training process, which is a measure of computational complexity rather than energy consumption."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to a recent study's projections for 2028, the total public health burden of U.S. data centers could reach up to $20.9 billion. This value is based on the high-growth scenario considered in the study.","20.9","USD","[""han2024""]","is_blank","Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","According to a recent study's projections for 2028, the total public health burden of U.S. data centers could reach up to $20.9 billion. This value is based on the high-growth scenario considered in the study."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","According to Table IV in the provided context, the estimated cost for fine-tuning Mixtral on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is $32.7.","32.7","USD","[""xia2024""]","is_blank","Table IV, ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE","According to Table IV in the provided context, the estimated cost for fine-tuning Mixtral on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context does not provide explicit information about the batch size of the longest-running MoE layer in a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB. However, it does mention that the maximum batch size in fine-tuning is determined by GPU memory size, model size, sequence length, and MoE sparsity. It also provides a table with the maximum batch size supported by different model and dataset combinations, but it does not specify which model and dataset correspond to the longest-running MoE layer.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide explicit information about the batch size of the longest-running MoE layer in a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB. However, it does mention that the maximum batch size in fine-tuning is determined by GPU memory size, model size, sequence length, and MoE sparsity. It also provides a table with the maximum batch size supported by different model and dataset combinations, but it does not specify which model and dataset correspond to the longest-running MoE layer."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The provided context does not contain explicit information about the total execution time of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. Therefore, it is not possible to provide an accurate answer.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The provided context does not contain explicit information about the total execution time of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. Therefore, it is not possible to provide an accurate answer."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level power consumption monitoring is not recommended for overall AI energy use measurement, as it substantially underrepresents the actual energy consumption since it measures only a single component.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that GPU-level power consumption monitoring is not recommended for overall AI energy use measurement, as it substantially underrepresents the actual energy consumption since it measures only a single component."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation and found that training accounted for only half of the model’s overall emissions. This means that training accounted for 50% of the model's overall emissions.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, and found that training accounted for only half of the model’s overall emissions.","The context states that in a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation and found that training accounted for only half of the model’s overall emissions. This means that training accounted for 50% of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. However, the question asks for the minimum number of GPUs required for LLaMA-7B, not LLaMA-65B. As the context does not provide information on the minimum number of GPUs required for LLaMA-7B, the answer cannot be determined based on the provided context.","is_blank","V100_32GB_GPU","[""is_blank""]","is_blank","is_blank","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. However, the question asks for the minimum number of GPUs required for LLaMA-7B, not LLaMA-65B. As the context does not provide information on the minimum number of GPUs required for LLaMA-7B, the answer cannot be determined based on the provided context."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to the provided context, training GPT-3 in a data center in Arizona would consume 9.629 million liters of water for training. Given that a 500ml bottle of water is equivalent to 0.5 liters, it would take approximately 19,258,000 user requests to consume a 500ml bottle of water during the training process.","19,258,000","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. 'Water for Training(million L)' for Arizona is 9.629.","According to the provided context, training GPT-3 in a data center in Arizona would consume 9.629 million liters of water for training. Given that a 500ml bottle of water is equivalent to 0.5 liters, it would take approximately 19,258,000 user requests to consume a 500ml bottle of water during the training process."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context from [ref_id=dodge2022] states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","[""https://arxiv.org/abs/2202.05933""]","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context from [ref_id=dodge2022] states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The provided context does not directly mention the score of the JetMoE-8B model on the GSM8k benchmark. Therefore, the answer is unable to be determined based on the given context.","is_blank","score","[""is_blank""]","is_blank","is_blank","The provided context does not directly mention the score of the JetMoE-8B model on the GSM8k benchmark. Therefore, the answer is unable to be determined based on the given context."
