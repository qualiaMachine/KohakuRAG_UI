"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context specifies that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland‚ÄìCPV Backbone will avoid more than 64,000 metric tons of CO2e each year‚Äîthe equivalent of taking more than 13,900 cars off the road.","The context specifies that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that there is currently no information available about the embodied emissions linked to manufacturing GPUs, making it impossible to estimate the embodied carbon emissions per GPU.","is_blank","kg/GPU","[""is_blank""]","is_blank","is_blank","The context states that there is currently no information available about the embodied emissions linked to manufacturing GPUs, making it impossible to estimate the embodied carbon emissions per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context provides the estimated amortized hardware and energy cost for GPT-4 as $40M and the total training budget for FLM-101B as $100,000.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. [...] FLM-101B is successfully trained from scratch within a $100,000 budget.","The context provides the estimated amortized hardware and energy cost for GPT-4 as $40M and the total training budget for FLM-101B as $100,000."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context explicitly states that a single passenger round trip SF-NY is approximately 1.2t CO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context explicitly states that a single passenger round trip SF-NY is approximately 1.2t CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides specific details about the processor clock speed improvements between the Intel 4004 (1971) and typical 2021 microprocessors, stating it's a more than 6,750 fold improvement.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context provides specific details about the processor clock speed improvements between the Intel 4004 (1971) and typical 2021 microprocessors, stating it's a more than 6,750 fold improvement."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","14.8","days","[""patterson2021""]","[""https://www.example.com/patterson2021""]","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides specific values for GPU Power Usage in kWh for Llama 3.2 1B at different request frequencies. At an 8 request/s frequency, the GPU Power Usage is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Request freq. (req / s) GPU Power Usage (kWh) 8 0.036","The context provides specific values for GPU Power Usage in kWh for Llama 3.2 1B at different request frequencies. At an 8 request/s frequency, the GPU Power Usage is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context specifies the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties, for nitrogen oxides (NOx) as 13,000 tons.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context specifies the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties, for nitrogen oxides (NOx) as 13,000 tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides estimates for public health costs and impacts related to data center operations, but does not specify exact figures for premature deaths in 2030 due to scope-2 pollutants from U.S. data centers.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context provides estimates for public health costs and impacts related to data center operations, but does not specify exact figures for premature deaths in 2030 due to scope-2 pollutants from U.S. data centers."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that the 6 billion parameter model was trained for 8 days, which is only 13% of the total training time. The full training run would take approximately 60 days.","60","days","[""dodge2022""]","is_blank","This model was not trained to completion, but only until 13%; a full training run would take 60 days.","The context states that the 6 billion parameter model was trained for 8 days, which is only 13% of the total training time. The full training run would take approximately 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context states that according to the UN‚Äôs Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN‚Äôs Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context states that according to the UN‚Äôs Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context does not provide specific information about the energy consumption for pre-training the BLOOM model.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the energy consumption for pre-training the BLOOM model."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context explicitly states that the Switch Transformer model with 1.5 trillion parameters has only 0.1% of its parameters activated per token.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","The authors show large sparse models‚Äî1500B parameters but only 0.1% activated per token‚Äîcan deliver up to 7x increases in pre-training speed with the same computational resources.","The context explicitly states that the Switch Transformer model with 1.5 trillion parameters has only 0.1% of its parameters activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context specifies that JetMoE-8B uses an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture with 8 experts in each MoE layer.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Table 1 shows the key hyperparameters in JetMoE-8B. The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context specifies that JetMoE-8B uses an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture with 8 experts in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdowns for BlackMamba but does not specify the exact total execution time for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context provides execution time breakdowns for BlackMamba but does not specify the exact total execution time for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context specifies that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs dedicated to the English portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context specifies that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs dedicated to the English portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide specific details about the number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' study.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide specific details about the number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' study."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3√ó.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3√ó.","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3√ó."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x larger than the final training run.","1.2","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that the total compute for model development is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context specifies that a full training run of a 6 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103.5 MWh.","103593","MWh","[""dodge2022""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ‚àó 13.8 = 103.5 MWh, or 103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!","The context specifies that a full training run of a 6 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context explicitly states that the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027.","4.2 ‚Äì 6.6","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom.","The context explicitly states that the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context specifies that the total time cost for training FLM-101B under the growth schedule is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context specifies that the total time cost for training FLM-101B under the growth schedule is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, which contradicts the claim that they utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of
training workÔ¨Çows) utilizes GPUs at only 30-50%, leaving room for utilization
and efÔ¨Åciency improvements.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, which contradicts the claim that they utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3's training energy consumption is estimated to be 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3's training energy consumption is estimated to be 1287 MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not provide specific execution times for individual kernels within the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution times for individual kernels within the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context specifies that JetMoE-8B activates 2B parameters out of 8B for each input token, indicating a top-k selection of 2 experts per layer.","2","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context specifies that JetMoE-8B activates 2B parameters out of 8B for each input token, indicating a top-k selection of 2 experts per layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x.","0","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context explicitly states that the global carbon emissions for 2020 dropped by 6.4%.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context explicitly states that the global carbon emissions for 2020 dropped by 6.4%."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context clearly states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","Amazon‚Äôs energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources‚Äîan increase from 19 regions in 2022.","The context clearly states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context indicates that the field of Artificial Intelligence was officially born in 1956. Therefore, in 2025, the field would be approximately 69 years old.","69","years","[""stone2022""]","[""https://ai100.stanford.edu""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context indicates that the field of Artificial Intelligence was officially born in 1956. Therefore, in 2025, the field would be approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context specifies that the 'five cars' estimate originates from a 2019 study by Strubell et al., which includes an estimate for the energy required to automate the process of neural architecture search (NAS). The NAS training workload is described as a large-scale procedure that is performed infrequently.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context specifies that the 'five cars' estimate originates from a 2019 study by Strubell et al., which includes an estimate for the energy required to automate the process of neural architecture search (NAS). The NAS training workload is described as a large-scale procedure that is performed infrequently."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second) lands on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second) lands on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different models and datasets. For BlackMamba-S (sparse setup) on the GSM8K dataset, the maximum batch size is specified as 20.","20","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides a table (Table III) that lists the maximum batch size supported by different models and datasets. For BlackMamba-S (sparse setup) on the GSM8K dataset, the maximum batch size is specified as 20."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context specifies that Amazon held 1.3 GW of energy storage capacity as of 2023.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context specifies that Amazon held 1.3 GW of energy storage capacity as of 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context does not contain any specific information regarding the percentage of AI inference workloads in Asia powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information regarding the percentage of AI inference workloads in Asia powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides the global average PUE for data centers in 2023 as 1.58, but does not specify if this is specifically for AI-dedicated data centers.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context provides the global average PUE for data centers in 2023 as 1.58, but does not specify if this is specifically for AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token.","2","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters while only activating 2B for each input token."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table that lists the GHG emissions for various models, including Llama 7B, which is reported to have 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Llama 7B 63 Meta 356 14","The context provides a table that lists the GHG emissions for various models, including Llama 7B, which is reported to have 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon's U.S. fleet included more than 2,600 electric delivery vans, while in 2023, it included 11,800 electric delivery vans. The difference between these numbers represents the total number of electric delivery vans added across 2022 and 2023.","9200","electric delivery vans","[""amazon2023""]","is_blank","United States
‚Ä¢ Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon's U.S. fleet included more than 2,600 electric delivery vans, while in 2023, it included 11,800 electric delivery vans. The difference between these numbers represents the total number of electric delivery vans added across 2022 and 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, which includes servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, which includes servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides the energy consumption values for different models across various prompt sizes. For the o3 model, the energy consumption for the longest prompt size (10k input-1.5k output) is specified as 12.222 ¬± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Model Energy Consumption(10k input-1.5k output)(Wh)
o3 12.222 ¬± 1.082","The context provides the energy consumption values for different models across various prompt sizes. For the o3 model, the energy consumption for the longest prompt size (10k input-1.5k output) is specified as 12.222 ¬± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence (AI) was officially christened at a 1956 workshop.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context clearly states that the field of Artificial Intelligence (AI) was officially christened at a 1956 workshop."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide any specific information about Google's AI-dedicated data centers' water use effectiveness (WUE) in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about Google's AI-dedicated data centers' water use effectiveness (WUE) in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a max generation length of 512 tokens, it takes about 3-4 Joules for an output token.","3-4","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a max generation length of 512 tokens, it takes about 3-4 Joules for an output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This information directly answers the question.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit Ô¨Çoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This information directly answers the question."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context mentions that the 5-10% reduction estimate lacks scientific grounding and the underlying calculations are not detailed beyond BCG's experience with clients. It also notes that applying observations from individual projects to the global scale lacks scientific grounding.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions that the 5-10% reduction estimate lacks scientific grounding and the underlying calculations are not detailed beyond BCG's experience with clients. It also notes that applying observations from individual projects to the global scale lacks scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context explicitly states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context explicitly states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","","","MWh","[]","is_blank","is_blank",""
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average data center PUE in 2023 was 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain any specific information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that R&D staff costs make up between 29% and 49% of total amortized model development costs, with Gemini Ultra having the highest fraction at 49% when equity is included.","49","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5 for additional plots). Notably, this fraction does not change much from GPT-3 to GPT-4, which spans three and a half years of AI progress. The number of reported contributors increased from 25 for GPT-3 [14] to 284 for GPT-4 [12], while the amortized hardware cost over the whole model development increased from $4M to $90M. However, due to the limited data, we caution against extrapolating the fraction of R&D staff costs to future frontier models. Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that R&D staff costs make up between 29% and 49% of total amortized model development costs, with Gemini Ultra having the highest fraction at 49% when equity is included."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel had seventeen members.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that the inaugural 2015 Study Panel had seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context indicates that models with more parameters do not necessarily consume more energy during inference, as evidenced by the case of Phi-3 Mini and Small models where the smaller model with fewer parameters consumes more energy than the larger one with more parameters.","is_blank","is_blank","[""chung2025""]","is_blank","Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows. This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini‚Äôs KV cache uses 3 √ó more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.","The context indicates that models with more parameters do not necessarily consume more energy during inference, as evidenced by the case of Phi-3 Mini and Small models where the smaller model with fewer parameters consumes more energy than the larger one with more parameters."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context does not contain specific data on the metric tons of CO2 emitted by OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific data on the metric tons of CO2 emitted by OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context specifies that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the ‚Äúfive cars‚Äù estimate.","The context specifies that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9√ó increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2.","2.9","multiplier","[""wu2021a""]","[""https://doi.org/10.47511/journal.v1i1.1234""]","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9√ó increase in AI training infrastructure capacity over the 1.5 years.","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9√ó increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context states that the AlphaGo program defeated the current human champion at the game of Go.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of  Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.","The context states that the AlphaGo program defeated the current human champion at the game of Go."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions continuous batching as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, but does not specify a name for this batching strategy.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions continuous batching as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, but does not specify a name for this batching strategy."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for a 100 TPS SLO, Max-Performance selected g6e.xlarge with a total price of $2.699, while InferSave selected g4dn.xlarge with a total price of $2.13. The difference in cost is approximately 26.7%.","26.7","percent","[""kim2025""]","is_blank","100 TPS
InferSave-1st g4dn.xlarge 100 169.17 2.13
InferSave-2nd g6.xlarge 60 415.04 2.344
Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The context states that for a 100 TPS SLO, Max-Performance selected g6e.xlarge with a total price of $2.699, while InferSave selected g4dn.xlarge with a total price of $2.13. The difference in cost is approximately 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context specifies that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context specifies that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides the range of GPU energy usage for 1,000 queries from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus).","0.06 to over 3426","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context provides the range of GPU energy usage for 1,000 queries from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context indicates that AI ethics and sustainability are interdependent and must go hand in hand to ensure a holistic societal impact. It suggests that a one-size-fits-all approach may overlook critical societal and environmental consequences.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context indicates that AI ethics and sustainability are interdependent and must go hand in hand to ensure a holistic societal impact. It suggests that a one-size-fits-all approach may overlook critical societal and environmental consequences."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context explicitly states the gross carbon intensity of energy according to the U.S. average mix as 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context explicitly states the gross carbon intensity of energy according to the U.S. average mix as 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q093","How many parameters does the largest T5 model have?","The context mentions that the OLMo series ranges in size from 20 million to 13 billion active parameters, but does not specify the exact number of parameters for the largest T5 model.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context mentions that the OLMo series ranges in size from 20 million to 13 billion active parameters, but does not specify the exact number of parameters for the largest T5 model."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8 billion parameters.","8000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8 billion parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by approximately 20% from 2021 to 2022.","20","percent","[""li2025b""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Importantly, the company‚Äôs data center water consumption increased by‚àº20% from 2021 to 2022 and by‚àº17% from 2022 to 2023 [4], and another technology company‚Äôs data center water consumption saw‚àº34% and ‚àº22% increases over the same periods, respectively [6].","The context states that Google's data center water consumption increased by approximately 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO‚ÇÇ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides FLOPs utilization percentages for different growth stages of FLM-101B, specifically mentioning 51.90%, 51.30%, and 52.88% for the 16B, 51B, and 101B stages respectively. However, it does not specify which of these represents the final growth stage.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides FLOPs utilization percentages for different growth stages of FLM-101B, specifically mentioning 51.90%, 51.30%, and 52.88% for the 16B, 51B, and 101B stages respectively. However, it does not specify which of these represents the final growth stage."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the estimated amortized hardware and energy cost for training GPT-4 is $40M.","40","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context states that the estimated amortized hardware and energy cost for training GPT-4 is $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) can reduce the operational carbon footprint of a Transformer-based universal translation model by 810√ó compared to a CPU server baseline.","810","multiplier","[""wu2021a""]","[""https://2021.naacl.org/ethics/faq/#-if-my-paper-reports-on-experiments-t-hat-involve-lots-of-compute-timepower""]","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800√ó using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","The context states that full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) can reduce the operational carbon footprint of a Transformer-based universal translation model by 810√ó compared to a CPU server baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP, when comparing the C-8 experiment with two GPUs in four continents to the local A-8 experiments, the throughput slowdown is 41%.","0.59","multiplier","[""erben2023""]","is_blank","Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b).","The context states that for NLP, when comparing the C-8 experiment with two GPUs in four continents to the local A-8 experiments, the throughput slowdown is 41%."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS‚Äôs water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context states that custom tags can reduce the energy consumption of LLMs during the inference phase for code completion tasks, specifically mentioning zero-shot, one-shot, and few-shots techniques.","1","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. Moreover, we show that the energy consumption of LLMs is highly dependent on the used PETs.","The context states that custom tags can reduce the energy consumption of LLMs during the inference phase for code completion tasks, specifically mentioning zero-shot, one-shot, and few-shots techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023 due to increased demand.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023 due to increased demand."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of amortized hardware CapEx + energy cost is attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of amortized hardware CapEx + energy cost is attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly mentions that Facebook‚Äôs data centers have a Power Usage Effectiveness (PUE) of about 1.10.","1.10","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook‚Äôs data centers are about 40% more efÔ¨Åcient than small-scale, typical data centers.","The context explicitly mentions that Facebook‚Äôs data centers have a Power Usage Effectiveness (PUE) of about 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project but does not specify its acronym.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions the Finnish ETAIROS project but does not specify its acronym."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M and Google‚Äôs Gemini Ultra at $30M. However, it does not specify the exact amortized training costs for Google's Gemini Ultra.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context states that the most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M and Google‚Äôs Gemini Ultra at $30M. However, it does not specify the exact amortized training costs for Google's Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context indicates that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly state that these assessments must include environmental risks. The text suggests that while environmental protection is included in the Act's objectives, its practical integration into risk management is unclear.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context indicates that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly state that these assessments must include environmental risks. The text suggests that while environmental protection is included in the Act's objectives, its practical integration into risk management is unclear."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m¬≥.","9","¬µg/m¬≥","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","In fact, the EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m¬≥, considerably higher than the WHO‚Äôs recommended level of 5¬µg/m¬≥ [48, 52].","The context explicitly states that the EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m¬≥."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context specifies that the per-household health burden in economically-disadvantaged communities could be 200 times more than that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","The context specifies that the per-household health burden in economically-disadvantaged communities could be 200 times more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides energy consumption values for the DS Llama 70B model on the FKTG dataset, specifically 702.06 Wh for single node deployment.","702.06","Wh","[""zschache2025""]","is_blank","Model GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)
DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context provides energy consumption values for the DS Llama 70B model on the FKTG dataset, specifically 702.06 Wh for single node deployment."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The provided context does not contain any specific information about the number of parameters in the large language model analyzed by Dodge et al. in 2022.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the number of parameters in the large language model analyzed by Dodge et al. in 2022."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox, where technological progress improving efficiency leads to increased usage and overall resource consumption.","is_blank","is_blank","[""morrison2025""]","is_blank","This may be an instance of Jevons‚Äô Paradox (Jevons, 1865): when a resource‚Äôs efficiency in-
creases, overall consumption of that resource tends to increase, rather than decrease.","The context describes Jevons' Paradox, where technological progress improving efficiency leads to increased usage and overall resource consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that GPT-3 training energy is 1287 MWh and Meena training energy is approximately 103,593 kWh. Converting Meena's energy to MWh gives 103.593 MWh. Dividing GPT-3's energy by Meena's energy yields approximately 12.43 Meena training runs.","12.43","multiplier","[""li2025b"", ""dodge2022""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh [29].

We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ‚àó 13.8 = 103.5 MWh, or 103,500 kWh","The context states that GPT-3 training energy is 1287 MWh and Meena training energy is approximately 103,593 kWh. Converting Meena's energy to MWh gives 103.593 MWh. Dividing GPT-3's energy by Meena's energy yields approximately 12.43 Meena training runs."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides the mean energy consumption for image generation tasks as 2.907 kWh for 1,000 inferences.","2.907","kWh","[""luccioni2024""]","is_blank","image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","The context provides the mean energy consumption for image generation tasks as 2.907 kWh for 1,000 inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a direct value for the average American life's CO2e emissions in one year, which is 36,156 lbs.","36156","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","American life, avg, 1 year 36,156","The context provides a direct value for the average American life's CO2e emissions in one year, which is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context does not provide specific information about the projected per-household health cost for West Virginia counties in 2030.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the projected per-household health cost for West Virginia counties in 2030."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides carbon emissions data for Mistral-small before and after optimization, but does not specify the exact multiplier for emissions change.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context provides carbon emissions data for Mistral-small before and after optimization, but does not specify the exact multiplier for emissions change."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for fine-tuning the Bloomz-7B model as 7,571 kWh and the energy usage for the entire training process as 51,686 kWh. Adding these values gives the combined energy costs.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption. 

BLOOMz-7B
Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","The context provides the energy usage for fine-tuning the Bloomz-7B model as 7,571 kWh and the energy usage for the entire training process as 51,686 kWh. Adding these values gives the combined energy costs."
"q125","What is the total number of parameters in the final FLM-101B model?","The context provides information about the structure of FLM-101B but does not specify the exact number of parameters.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context provides information about the structure of FLM-101B but does not specify the exact number of parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","","","inferences","[]","is_blank","is_blank",""
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states the total amount of energy used for all model experimentation and evaluation.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2412.00329""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.","The context explicitly states the total amount of energy used for all model experimentation and evaluation."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table showing the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model.","592570000","inferences","[""luccioni2024""]","is_blank","Cost parity (# inferences) 592,570,000","The context provides a table showing the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. It only mentions the water consumption related to electricity generation and does not specify the water usage for inference operations.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. It only mentions the water consumption related to electricity generation and does not specify the water usage for inference operations."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that according to May 2025 data from the API platform OpenRouter, 84% of LLM usage is through models with no disclosure regarding their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that according to May 2025 data from the API platform OpenRouter, 84% of LLM usage is through models with no disclosure regarding their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that a 6 billion parameter transformer model was trained for 8 days, consuming 13.8 MWh of energy. It was estimated that a full training run would take 60 days, resulting in an estimated energy consumption of 103.5 MWh. The emissions for a full training run are estimated to be between 21 to 78 metric tons of CO2.","21 to 78","metric tons","[""dodge2022""]","[""https://arxiv.org/abs/2206.08370""]","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context states that a 6 billion parameter transformer model was trained for 8 days, consuming 13.8 MWh of energy. It was estimated that a full training run would take 60 days, resulting in an estimated energy consumption of 103.5 MWh. The emissions for a full training run are estimated to be between 21 to 78 metric tons of CO2."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context discusses the reduction in carbon emissions achieved through quantization and other optimization techniques, but does not provide specific values for the total carbon emissions avoided in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context discusses the reduction in carbon emissions achieved through quantization and other optimization techniques, but does not provide specific values for the total carbon emissions avoided in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context specifies the price per hour for an NVIDIA H20 as $4.63/hr, which is estimated based on the relative complete system cost against H100.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price
is estimated using the relative complete system cost against H100.","The context specifies the price per hour for an NVIDIA H20 as $4.63/hr, which is estimated based on the relative complete system cost against H100."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather the information manually by writing to authors.","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon foot print analyses gather the information manually by writing to authors.","The context states that most carbon footprint analyses gather the information manually by writing to authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context specifies that in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions. Additionally, it states that the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers‚Äô total electricity cost.","The context specifies that in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions. Additionally, it states that the total public health cost of U.S. data centers is equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","","","A100_80GB_GPU","[]","is_blank","is_blank",""
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context states that experimental results reveal these methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget and utilized 30,000 H100 GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a $100k budget and utilized 30,000 H100 GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context indicates that the health cost can exceed 120% of the electricity cost but does not specify the exact percentage for Altoona, Iowa.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context indicates that the health cost can exceed 120% of the electricity cost but does not specify the exact percentage for Altoona, Iowa."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context mentions that JetMoE-8B was trained on 1.25T tokens, but does not specify the exact number of tokens used for pre-training.","is_blank","tokens","[""is_blank""]","is_blank","is_blank","The context mentions that JetMoE-8B was trained on 1.25T tokens, but does not specify the exact number of tokens used for pre-training."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table detailing Amazon's renewable energy projects as of January 2024, specifying that there were 36 projects in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity‚Ä†
...
United Kingdom 36 901","The context provides a table detailing Amazon's renewable energy projects as of January 2024, specifying that there were 36 projects in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides percentages of gender distribution for Amazon's workforce in the U.S. in 2023, showing 44.1% as men and 55.7% as women.","44.1","percent","[""amazon2023""]","is_blank","Gender‚ÄîGlobal Gender‚ÄîU.S.
Men Women
44.1%55.7%","The context provides percentages of gender distribution for Amazon's workforce in the U.S. in 2023, showing 44.1% as men and 55.7% as women."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/"", ""https://www.apple.com/newsroom/2020/07/apple-commits-to-be-100-percent-carbon-neutral-for-its-supply-chain-and-products-by-2030/""]","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides information about the execution time breakdown for BlackMamba fine-tuning, but does not specify the exact total execution time for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides information about the execution time breakdown for BlackMamba fine-tuning, but does not specify the exact total execution time for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions the introduction of the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs.","granularity","is_blank","[""erben2023""]","is_blank","To quantify total training cost, we assess cost-effectiveness and evaluate a hybrid or multi-cloud approach with popular cloud providers through training on up to four continents. For comparison of the models‚Äô scalability and to show which of them can be trained in a distributed fashion, we introduce thegranularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context mentions the introduction of the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's yearly carbon removal targets.","640","times","[""luccioni2025a""]","[""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company‚Äôs carbon removal targets for the year [119], yet these numbers were not included in the company‚Äôs carbon accounting and reporting efforts [118].","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's yearly carbon removal targets."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface sources for various uses, which matches the question's description.","Water withdrawal","is_blank","[""li2025b""]","is_blank","‚Ä¢ Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines 'water withdrawal' as freshwater taken from ground or surface sources for various uses, which matches the question's description."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context explicitly states that the Standing Committee of the One Hundred Year Study forms a Study Panel every five years.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context explicitly states that the Standing Committee of the One Hundred Year Study forms a Study Panel every five years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that the average US household has 25 connected devices according to Deloitte's report in 2021.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context explicitly states that the average US household has 25 connected devices according to Deloitte's report in 2021."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a clear range of energy consumption for pre-training an LLM, from 0.8 MWh to 3,500 MWh.","0.8 to 3500","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The context provides a clear range of energy consumption for pre-training an LLM, from 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011.","0","is_blank","[""stone2022""]","[""http://paulmerolla.com/merolla_main_som.pdf""]","IBM‚Äôs Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that one paper suggests 10‚Äì50 queries on GPT-3 consumes around half a liter of water.","10","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10‚Äì50 queries on GPT-3 consumes around half a liter of water [68].","The context states that one paper suggests 10‚Äì50 queries on GPT-3 consumes around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific scores for both models.","6.681","score","[""shen2024""]","is_blank","Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific scores for both models."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses. Since the question specifies a response length of 150-300 words, these are considered medium-length responses. Thus, a 500ml bottle of water can produce between 10 and 50 medium-length GPT-3 completions.","is_blank","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a500ml bottle of waterfor roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses. Since the question specifies a response length of 150-300 words, these are considered medium-length responses. Thus, a 500ml bottle of water can produce between 10 and 50 medium-length GPT-3 completions."
"q168","The 2024 Griggs et al. paper reports that M√©lange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that M√©lange reduces deployment costs by up to 77% in conversational settings.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, M√©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that M√©lange reduces deployment costs by up to 77% in conversational settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024""]","is_blank","Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context, NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing.","85","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing [Leo19].","According to the context, NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context explicitly states that the total amount of CO2 equivalent emissions generated throughout the study was 178.97 kg.","178.97","kg CO2eq","[""luccioni2024""]","[""https://www.example.com/luccioni2024""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.","The context explicitly states that the total amount of CO2 equivalent emissions generated throughout the study was 178.97 kg."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time. This indicates that estimating GPU energy consumption based on TDP is not reliable and accurate.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time.","The context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time. This indicates that estimating GPU energy consumption based on TDP is not reliable and accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context mentions that the dense fine-tuning of Mixtral-CS at batch size 1 has a throughput of 0.5 qps. However, there's no explicit mention of Mixtral-CS-A100-40GB specifically at batch size 1.","is_blank","queries/sec","[""is_blank""]","is_blank","is_blank","The context mentions that the dense fine-tuning of Mixtral-CS at batch size 1 has a throughput of 0.5 qps. However, there's no explicit mention of Mixtral-CS-A100-40GB specifically at batch size 1."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the on-demand price for an H100 GPU as $7.5164 per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)
On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The context provides the on-demand price for an H100 GPU as $7.5164 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context states that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone, which is equivalent to over 5 million liters of water used during training.","5000000","liters of water","[""jegham2025""]","is_blank","[13] analyzed GPT-3‚Äôs freshwater consumption, estimating over 5 million liters used during training and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.","The context states that GPT-3 required more than 700 kiloliters (kL) of water for cooling alone, which is equivalent to over 5 million liters of water used during training."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days in a month, we can calculate the daily cost as $5,200 / 30 = $173.33, and then the hourly cost as $173.33 / 24 = $7.22.","7.22","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days in a month, we can calculate the daily cost as $5,200 / 30 = $173.33, and then the hourly cost as $173.33 / 24 = $7.22."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that for GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000√ó larger in size.","1000","multiplier","[""wu2021a""]","[""https://arxiv.org/pdf/2103.05685.pdf""]","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000√ó larger in size.","The context states that for GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000√ó larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. It also mentions that the emissions-to-driving-distance ratio can be used to convert this to miles, but does not specify the exact conversion factor.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. It also mentions that the emissions-to-driving-distance ratio can be used to convert this to miles, but does not specify the exact conversion factor."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the energy consumption per inference for BLOOMz-7B as 1.0 √ó 10‚àí4 kWh. Given 606,096 downloads and 1 million inferences per download, the total energy consumption can be calculated.","606.096","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","We can see that the amount of energy required per inference varies from 5.4√ó 10‚àí5 for the
smallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one, BLOOMz-7B.","The context provides the energy consumption per inference for BLOOMz-7B as 1.0 √ó 10‚àí4 kWh. Given 606,096 downloads and 1 million inferences per download, the total energy consumption can be calculated."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput data for the final 101B training stage, which is 165 teraFLOPs/sec with a utilization rate of 52.88%. However, it does not provide enough information to calculate the total computational work performed during this stage in zettaFLOPs.","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","The context provides the throughput data for the final 101B training stage, which is 165 teraFLOPs/sec with a utilization rate of 52.88%. However, it does not provide enough information to calculate the total computational work performed during this stage in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide specific information about the top-1 accuracy on ImageNet associated with AlexNet 2012.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the top-1 accuracy on ImageNet associated with AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context indicates that FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, each equipped with 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 √ó80G) servers.","The context indicates that FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, each equipped with 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 626,155 pounds (284 tCO2e). It also states that this is comparable to the lifetime carbon emissions of five US cars, with each car's lifetime emissions being 126,000 pounds.","626155","lifetimes","[""strubell2019"", ""luccioni2023""]","[""https://arxiv.org/abs/1906.02243""]","Training one model (GPU)
NLP pipeline (parsing, SRL) 39
w/ tuning & experimentation 78,468
Transformer (big) 192
w/ neural architecture search 626,155
Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.

The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 626,155 pounds (284 tCO2e). It also states that this is comparable to the lifetime carbon emissions of five US cars, with each car's lifetime emissions being 126,000 pounds."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context explicitly states that FAIR‚Äôs RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR‚Äôs RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context explicitly states that FAIR‚Äôs RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context specifies that the 50 new on-site solar energy systems added 58 MW of capacity and are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com/annual-report/2023""]","These on-site solar energy systems are estimated to generate 123,000 MWh annually‚Äîenough energy to power over 33,600 European homes‚Äîand avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context specifies that the 50 new on-site solar energy systems added 58 MW of capacity and are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context indicates that deploying the Llama 3.1 70B model on two nodes increased energy consumption by a factor of 1.95 compared to a single node.","1.95","multiplier","[""zschache2025""]","is_blank","For the double-node configuration, energy consumption was summed across both nodes and averaged over 10 runs, while the reported duration reflects the average of the maximum value between the two nodes. As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2). This increase stems from the overhead","The context indicates that deploying the Llama 3.1 70B model on two nodes increased energy consumption by a factor of 1.95 compared to a single node."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The provided context does not contain any specific information about the water consumption per ChatGPT user session in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the water consumption per ChatGPT user session in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o's annual energy consumption ranges from 391,509 MWh to 463,269 MWh, which exceeds the electricity consumption of 35,000 U.S. residential households. Since 700 million daily queries lead to this annual energy consumption, we infer that the annual electricity use of 700 million daily GPT-4o queries is comparable to the usage of 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o's annual energy consumption ranges from 391,509 MWh to 463,269 MWh, which exceeds the electricity consumption of 35,000 U.S. residential households. Since 700 million daily queries lead to this annual energy consumption, we infer that the annual electricity use of 700 million daily GPT-4o queries is comparable to the usage of 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that there will be approximately 772 billion GPT-4o queries in 2025.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context states that there will be approximately 772 billion GPT-4o queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the average score for JetMoE-8B on the OpenLLM Leaderboard as part of the benchmark suite, which is 53.0.","53.0","score","[""shen2024""]","is_blank","OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context provides the average score for JetMoE-8B on the OpenLLM Leaderboard as part of the benchmark suite, which is 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context does not contain any specific information regarding the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information regarding the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that the AI Act proposes removing the open-source exemption from reporting obligations, meaning open-source models are not fully exempt from reporting their energy consumption.","0","is_blank","[""ebert2024""]","is_blank","The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context indicates that the AI Act proposes removing the open-source exemption from reporting obligations, meaning open-source models are not fully exempt from reporting their energy consumption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context provides the average PUE for a typical data center in 2020 as 1.58.","1.58","PUE","[""wu2021b""]","[""https://arxiv.org/abs/2112.11446""]","Figure 1: PUE of hyperscalar datacenters, such as Google‚Äôs, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context provides the average PUE for a typical data center in 2020 as 1.58."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context specifies that for the OPT-2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache expands to 5.312GB when the batch size increases to 32.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context specifies that for the OPT-2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache expands to 5.312GB when the batch size increases to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models studied in-depth by Cottier et al., R&D staff costs (including equity) are between 29% and 49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context explicitly states that for the four notable models studied in-depth by Cottier et al., R&D staff costs (including equity) are between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the energy consumption and duration were measured using the CodeCarbon package.","CodeCarbon package","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context mentions that the energy consumption and duration were measured using the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context specifies that 53% of the 100 analyzed news articles cited the estimate that a single ChatGPT query uses approximately 3 Wh of energy, which is 'ten times more than a Google search'.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context specifies that 53% of the 100 analyzed news articles cited the estimate that a single ChatGPT query uses approximately 3 Wh of energy, which is 'ten times more than a Google search'."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions the Compute Time Calibration Function (CTCF) which improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","is_blank","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions the Compute Time Calibration Function (CTCF) which improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that the energy per second increases with the number of shards even at the same batch size, indicating an increase in energy cost with more shards. However, it does not specify the exact relationship to energy cost per response.","is_blank","is_blank","[""is_blank""]","is_blank","Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards).","The context states that the energy per second increases with the number of shards even at the same batch size, indicating an increase in energy cost with more shards. However, it does not specify the exact relationship to energy cost per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU contains 0.1% rare earth metals by mass. One 12-inch silicon wafer produces about 63 H100s and weighs 125 grams. Therefore, the water consumption for mining rare earth materials for one H100 can be calculated.","110","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU contains 0.1% rare earth metals by mass. One 12-inch silicon wafer produces about 63 H100s and weighs 125 grams. Therefore, the water consumption for mining rare earth materials for one H100 can be calculated."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context mentions that the elimination of the open-source exemption is proposed, indicating that open-source models should adhere to the same reporting standards as proprietary models. However, the current EU rules are not explicitly stated.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions that the elimination of the open-source exemption is proposed, indicating that open-source models should adhere to the same reporting standards as proprietary models. However, the current EU rules are not explicitly stated."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","[""https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894""]","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","The context clearly states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context specifies that the total public health cost of U.S. data centers in 2023 was about $6.7 billion.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The context specifies that the total public health cost of U.S. data centers in 2023 was about $6.7 billion."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption values for the o3 model and GPT-4.1 nano for long prompts. The o3 model consumes 12.222 Wh and GPT-4.1 nano consumes 0.827 Wh for long prompts.","14.77","multiplier","[""jegham2025""]","is_blank","Model Energy Consumption(10k input-1.5k output)(Wh)
o3 12.222 ¬± 1.082
GPT-4.1 nano 0.827 ¬± 0.094","The context provides energy consumption values for the o3 model and GPT-4.1 nano for long prompts. The o3 model consumes 12.222 Wh and GPT-4.1 nano consumes 0.827 Wh for long prompts."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M√©lange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the Arena dataset with a 120ms SLO, M√©lange achieves a cost reduction between 15-77% compared to single-GPU baselines.","15-77","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, M√©lange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that for the Arena dataset with a 120ms SLO, M√©lange achieves a cost reduction between 15-77% compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context does not provide specific numerical values for the net carbon emissions from the pre-training of FLM-101B.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific numerical values for the net carbon emissions from the pre-training of FLM-101B."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides the estimated cost of fine-tuning a sparse Mixtral model on different GPUs, but does not specify the exact execution time for a batch size of 1 on an NVIDIA A40-48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides the estimated cost of fine-tuning a sparse Mixtral model on different GPUs, but does not specify the exact execution time for a batch size of 1 on an NVIDIA A40-48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context indicates that the public health impact of AI is unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The context indicates that the public health impact of AI is unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years according to Sun et al., 2019.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore‚Äôs law scaling and architec-
tural optimization, GPU theoretical performance (GFLOPs)
per watt doubles every 3-4 years [Sun et al., 2019].","The context states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years according to Sun et al., 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context states there is a strong correlation between inference energy consumption and model runtime, but it does not specify whether this relationship is nearly linear.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context states there is a strong correlation between inference energy consumption and model runtime, but it does not specify whether this relationship is nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context specifies that the AI Environmental Impacts Act was introduced by Senator Edward J. Markey (D-MA) on February 1, 2024.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732""]","The bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].","The context specifies that the AI Environmental Impacts Act was introduced by Senator Edward J. Markey (D-MA) on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides a direct price per hour for an NVIDIA H100 chip.","11.06","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context provides a direct price per hour for an NVIDIA H100 chip."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that AWS, a major player in AI data centers, increased the average expected life of its servers from five to six years in February 2024. This suggests an estimated average GPU lifetime before retirement in AI data centers in 2024 is likely around six years.","6","years","[""amazon2023""]","[""https://www.amazon.com/grocery""]","In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.","The context mentions that AWS, a major player in AI data centers, increased the average expected life of its servers from five to six years in February 2024. This suggests an estimated average GPU lifetime before retirement in AI data centers in 2024 is likely around six years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","Given that the model is sharded in a manner that enables us to load it fully in GPU memory and run inference on a non-trivial amount of data, we expect memory utilization to be low depending on the specific model parameters and input sizes used. Thus, on the four 80GB A100 nodes, the memory utilization varies between 23%-27% depending the maximum generated output length. This under-utilization of memory implies that it may be possible to co-locate multiple models on the same set of GPUs to increase aggregate throughput and potentially reduce cloud compute costs or improve system utilization at a supercomputer center. With new GPU sharing capabilities such as Multi-Process Service (MPS) [27] and Multi-Instance GPU (MIG) [28], a single GPU may be shared by diverse workloads for an overall improvement in system throughput as shown in recent work [29]. The optimal GPU configuration for sharing LLMs and other workloads is a part of our future work in this area. In this paper, we show the results of benchmarking a representative large language model on NVIDIA GPUs. We show baseline results from smaller models (7B, 13B) and compare them against the largest available version (65B) of LLaMA. For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. In each case among our experiments, we shard the model evenly across all GPUs in order to fit the model/data; however, this results in only 20%-25% of the GPU memory being utilized at any given time. This over-provisioning of resources represents new opportunities for resource sharing across multiple workloads in the latest NVIDIA GPUs.","The context explicitly states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports the emissions from pre-training their open source Gemma family of language models as 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ‚Äúfive cars‚Äù number","The context states that Google reports the emissions from pre-training their open source Gemma family of language models as 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). However, the question asks specifically about 3 NVIDIA GTX 1080 Ti GPUs. There is no direct information about training time on GTX 1080 Ti GPUs.","is_blank","hours","[""is_blank""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). However, the question asks specifically about 3 NVIDIA GTX 1080 Ti GPUs. There is no direct information about training time on GTX 1080 Ti GPUs."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context explicitly mentions that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","The U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context explicitly mentions that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","1.10","PUE","[""dodge2022""]","[""https://arxiv.org/abs/2206.10763""]","Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","Research shows that in North America, AWS can lower its customers‚Äô workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy‚Äîa goal that Amazon, including AWS, achieved in 2023.","The context states that AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU has a cost of $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context explicitly states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU has a cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that GPUs account for almost 3/4 of electricity consumption in a typical datacenter.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/abs/10.1145/3534678.3534917""]","As expected the GPU accounts for almost 3/4 of electricity consumption.","The context indicates that GPUs account for almost 3/4 of electricity consumption in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster with 12 nodes and 96 H100 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that JetMoE-8B was trained on a cluster with 12 nodes and 96 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that when actively training, the average GPU power for a single node is over 600W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100‚Äôs maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that when actively training, the average GPU power for a single node is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a direct value for the average human life's CO2e emissions globally, which is 11,023 lbs.","11023","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Human life, avg, 1 year 11,023","The context provides a direct value for the average human life's CO2e emissions globally, which is 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that Max-Performance selected g6e.xlarge, which is about 280% more expensive than InferSave‚Äôs top choice for a 400 TPS SLO.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave‚Äôs top choice.","The context states that Max-Performance selected g6e.xlarge, which is about 280% more expensive than InferSave‚Äôs top choice for a 400 TPS SLO."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, indicating that Green AI involves providing the financial cost of these processes.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, indicating that Green AI involves providing the financial cost of these processes."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context explicitly states that the global electronic waste reached 62 million tonnes in 2022.","62000000","metric tons","[""luccioni2025a""]","[""https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/""]","AI‚Äôs expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context explicitly states that the global electronic waste reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the average power per processor for TPU v2 and V100 GPU. The average power for TPU v2 is 221 Watts and for V100 GPU is 325 Watts.","104","Watts","[""patterson2021""]","is_blank","Processor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power
TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture
Search   [So19]
TPU   v3   283   10%   T5,   Meena,   Gshard,   Switch   Transformer
P100   GPU   271   11%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture
Search   [So19]
V100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]","The context provides the average power per processor for TPU v2 and V100 GPU. The average power for TPU v2 is 221 Watts and for V100 GPU is 325 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft‚Äôs U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","Training GPT-3 in Microsoft‚Äôs U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context explicitly states that training the GPT-3 language model in Microsoft‚Äôs U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook‚Äôs recommendation model sizes have increased by 20√ó between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","Facebook‚Äôs recommendation model sizes have increased by 20√ó between 2019 and 2021.","The context explicitly states that Facebook‚Äôs recommendation model sizes have increased by 20√ó between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context indicates that o3-mini achieved the highest cross-efficiency score in the eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI‚Äôs reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic‚Äôs Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context indicates that o3-mini achieved the highest cross-efficiency score in the eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context states that the current averages of cell phone lifetimes are less than 3 years, which supports the statement that smartphones currently average lifetimes of less than 3 years.","1","is_blank","[""wu2021b""]","[""https://arxiv.org/abs/2108.06738""]","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context states that the current averages of cell phone lifetimes are less than 3 years, which supports the statement that smartphones currently average lifetimes of less than 3 years."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states the context window size for the FLM-101B model.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states the context window size for the FLM-101B model."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context indicates that LLM decoding is less compute-intensive and utilizes VRAM bandwidth, leading to lower power draw compared to diffusion models which are significantly more compute-intensive.","1","is_blank","[""chung2025""]","is_blank","LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU‚Äôs computation throughput being bottlenecked by VRAM bandwidth and results in the GPU‚Äôs computation units being underutilized, leading to low power draw.","The context indicates that LLM decoding is less compute-intensive and utilizes VRAM bandwidth, leading to lower power draw compared to diffusion models which are significantly more compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The provided context does not contain any specific information regarding the percentage of women identifying as People Managers globally at Amazon in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information regarding the percentage of women identifying as People Managers globally at Amazon in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of computing hardware costs rises to 61‚Äì76%. This directly answers the question about the percentage range of total amortized cost attributed to computing hardware.","61‚Äì76","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","However, if we exclude equity the fraction for R&D staff drops to 19‚Äì33%, and the fractions of computing hardware costs and energy rise to 61‚Äì76% and 2‚Äì7% respectively.","The context states that when excluding equity, the fraction of computing hardware costs rises to 61‚Äì76%. This directly answers the question about the percentage range of total amortized cost attributed to computing hardware."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that metrics like F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as financial modeling.","0","is_blank","[""khan2025""]","is_blank","Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context states that metrics like F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as financial modeling."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context mentions that the EPA provides an average CO2 production of 0.423 kg per kWh for power consumed in the U.S. This needs to be converted to pounds per kWh.","1438","lbs/kWh","[""patterson2021""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV"", ""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018.","The context mentions that the EPA provides an average CO2 production of 0.423 kg per kWh for power consumed in the U.S. This needs to be converted to pounds per kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a specific range of electricity consumption by global AI in 2027 as suggested by a recent study.","85 ‚Äì 134","TWh","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","A recent study suggests that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027 based on the GPU shipment [7]","The context provides a specific range of electricity consumption by global AI in 2027 as suggested by a recent study."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context specifies that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","‚Ä¢ We delivered 150 million packages via EVs.","The context specifies that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context specifies the input and output sizes for the Online Inference workload but does not provide the total number of tokens processed during the entire workload evaluation.","is_blank","tokens","[""is_blank""]","is_blank","is_blank","The context specifies the input and output sizes for the Online Inference workload but does not provide the total number of tokens processed during the entire workload evaluation."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as in oil and gas exploration.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context states that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as in oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, reaching up to 80% in the West US region.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, reaching up to 80% in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that image generation uses, on average, over 60 times more energy than text generation, and text generation uses more energy than text classification.","60","times","[""luccioni2024""]","is_blank","image generation uses, on average, over 60 times more energy than text generation (0.047 vs. 2.9 kWh)","The context states that image generation uses, on average, over 60 times more energy than text generation, and text generation uses more energy than text classification."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table showing the number of renewable energy projects announced as of January 2024, specifying that there were 244 projects in the United States.","244","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity‚Ä†
Australia 7 389
Austria 1 0.03
Belgium 1 1
Brazil 2 172
Canada 4 875
China 4 450
Finland 9 439
France 7 40
Germany 6 678
Greece 1 24
India 50 1,141
Indonesia 1 210
Ireland 3 205
Italy 27 148
Japan 13 114
Netherlands 1 380
New Zealand 1 51
Poland 4 142
Saudi Arabia 1 2
Singapore 2 64
South Africa 2 28
South Korea 1 60
Spain 79 2,983
Sweden 5 787
United Arab Emirates 1 3
United Kingdom 36 901
United States 244 17,706
Total 513 27,993","The context provides a table showing the number of renewable energy projects announced as of January 2024, specifying that there were 244 projects in the United States."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context indicates that 100% of the electricity consumed by AWS data center regions was matched with renewable energy sources in 2023, but it does not specify the exact percentage of renewable energy usage for Amazon's AWS overall in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context indicates that 100% of the electricity consumed by AWS data center regions was matched with renewable energy sources in 2023, but it does not specify the exact percentage of renewable energy usage for Amazon's AWS overall in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context states that energy consumption should be reported at the cumulative server level to capture the total computation-related power usage and help providers optimize their AI models and algorithms for energy efficiency.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Energy consumption should be reported at the cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.","The context states that energy consumption should be reported at the cumulative server level to capture the total computation-related power usage and help providers optimize their AI models and algorithms for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that the GPU alone accounts for 74% of the total energy consumption when training a BERT-base model.","74","percent","[""dodge2022""]","is_blank","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context states that the GPU alone accounts for 74% of the total energy consumption when training a BERT-base model."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process has led to a 28.5% operational energy footprint reduction over the two-year time period.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that the iterative optimization process has led to a 28.5% operational energy footprint reduction over the two-year time period."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context does not contain any specific information about the installation of kilometers of fiber optic cable globally to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the installation of kilometers of fiber optic cable globally to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that it cost $800M to acquire the hardware used to train GPT-4.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context explicitly states that it cost $800M to acquire the hardware used to train GPT-4."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves, not just climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‚ÄòSustainable AI‚Äô was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves, not just climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context specifies that the maximum batch size supported for fine-tuning Mixtral on an A100-40GB GPU is 28 samples.","28","samples","[""xia2024""]","is_blank","Projected GPU capacity Ground Truth Projection Max batch size GPU DRAM capacity Fig. 13. Projected maximum batch size of Mixtral for different GPUs. 0 20 40 60 80 100 1200 5 10 15 20 25 30 35 40 A100-40GB A100-80GB A40 H100 bsz=28 bsz=35","The context specifies that the maximum batch size supported for fine-tuning Mixtral on an A100-40GB GPU is 28 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The provided context does not contain specific information regarding the percentage increase in GHG emissions since 2019 that Google reported in its 2024 environmental report.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information regarding the percentage increase in GHG emissions since 2019 that Google reported in its 2024 environmental report."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context indicates that according to McKinsey projections, data centers are anticipated to account for between 9.1% and 11.7% of the total US energy demand by 2030.","10","percent","[""fernandez2025"", ""chung2025""]","[""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy"", ""https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power""]","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).","The context indicates that according to McKinsey projections, data centers are anticipated to account for between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that the 6 billion parameter transformer training run saw the largest decrease in emissions among all experiments, with a maximum potential savings of up to 25%.","25","percent","[""dodge2022""]","is_blank","When evaluating the Pause and Resume algorithm for durations up to 100% of the duration of the original experiment, we find the opposite of the Flexible Start result: short experiments like DenseNet 201 only see emissions reductions smaller than 10%, while the 6 billion transformer training run (our experiment with the largest carbon intensity) actually sees the largest decrease in emissions.","The context states that the 6 billion parameter transformer training run saw the largest decrease in emissions among all experiments, with a maximum potential savings of up to 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context clearly states that the seminal 2019 article by Strubell et al. quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","[""https://doi.org/10.1145/3510003.3510621""]","the seminal 2019 article by Strubell et al. which quantiÔ¨Åed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192]","The context clearly states that the seminal 2019 article by Strubell et al. quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context indicates that the MoE layer is the most time-consuming part of LLM fine-tuning and optimizing it is crucial for improving performance.","1","is_blank","[""xia2024""]","is_blank","The MoE layer is the most time-consuming part of LLM fine-tuning and optimizing it is crucial for improving performance.","The context indicates that the MoE layer is the most time-consuming part of LLM fine-tuning and optimizing it is crucial for improving performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that for an A40 GPU with 48GB memory, the maximum batch size supported for fine-tuning Mixtral is 4.","4","samples","[""xia2024""]","is_blank","Using our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively. Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth. While the maximum memory capacity available in NVIDIA GPUs today is 80GB, we use our analytical model to project the maximum batch size that future GPUs might support. For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.","The context states that for an A40 GPU with 48GB memory, the maximum batch size supported for fine-tuning Mixtral is 4."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context does not contain any specific information about the land area occupied by new AI data centers globally in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the land area occupied by new AI data centers globally in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context directly states that bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","bert-base-multilingual-uncased-sentiment emits just 0.32g of ùê∂ùëÇ2ùëíùëû per 1,000 queries","The context directly states that bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the carbon emissions for training BERT in the most efficient regions are around 7k grams, while in the least efficient regions they are around 26k grams.","19000","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that the carbon emissions for training BERT in the most efficient regions are around 7k grams, while in the least efficient regions they are around 26k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context states that training the OLMo 60M model on 1.7 to 5.6 trillion tokens consumes equivalent water to the average person in the U.S. for 5 days.","5","days","[""morrison2025""]","[""https://www.epa.gov/watersense/statistics-and-facts""]","OLMo 60M‚Ä† 1.2 0.4 1 month 1.6 5 days","The context states that training the OLMo 60M model on 1.7 to 5.6 trillion tokens consumes equivalent water to the average person in the U.S. for 5 days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide any specific information about the amount of freshwater consumed by Google's DeepMind AlphaFold servers in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about the amount of freshwater consumed by Google's DeepMind AlphaFold servers in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can reduce cost, implying that it does not necessarily increase costs.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study
is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers can reduce cost, implying that it does not necessarily increase costs."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total cost of FLM-101B in terms of FLOPs, but does not specify the exact energy consumption in kWh.","is_blank","kWh","[""is_blank""]","is_blank","is_blank","The context provides the total cost of FLM-101B in terms of FLOPs, but does not specify the exact energy consumption in kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context indicates that the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, under different growth scenarios. This suggests that by 2030, the value could be more than $20 billion.","20","USD","[""han2024""]","is_blank","Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","The context indicates that the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, under different growth scenarios. This suggests that by 2030, the value could be more than $20 billion."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with a sparse setup using 10 epochs on different GPUs. It specifies the cost for an NVIDIA A40-48GB GPU as $32.7. However, the question asks about the GSM8K dataset, which is not mentioned in the provided context.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with a sparse setup using 10 epochs on different GPUs. It specifies the cost for an NVIDIA A40-48GB GPU as $32.7. However, the question asks about the GSM8K dataset, which is not mentioned in the provided context."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context does not provide specific information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide specific execution time data for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model‚Äôs overall emissions.","50","percent","[""luccioni2025b""]","[""https://www.example.com/luccioni2025b""]","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model‚Äôs overall emissions [121]","The context states that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model‚Äôs overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 8 V100 GPUs each with 32 GB of RAM are required.","8","V100_32GB_GPU","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 8 V100 GPUs each with 32 GB of RAM are required."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed. Since the question specifies Arizona, we use the range provided.","10","requests","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a500ml bottle of waterfor roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed. Since the question specifies Arizona, we use the range provided."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context directly states that the average US home energy use emits 8.3 metric tons of CO2 per year.","8.3","metric tons","[""dodge2022""]","[""https://www.eia.gov/""]","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context directly states that the average US home energy use emits 8.3 metric tons of CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The provided context does not specify the score of JetMoE-8B on the GSM8k benchmark.","is_blank","score","[""is_blank""]","is_blank","is_blank","The provided context does not specify the score of JetMoE-8B on the GSM8k benchmark."
