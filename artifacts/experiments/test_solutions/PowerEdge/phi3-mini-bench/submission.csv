"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context provides specific percentages for the increase in U.S. data center electricity consumption between 2010 and 2014, stating a 4% increase.","4","percent","[""wu2021b"", ""luccioni2024"", ""han2024""]","[""https://arxiv.org/abs/2406.06288v2"", ""https://arxiv.org/abs/2302.13971"", ""https://arxiv.org/abs/2307.09288"", ""https://arxiv.org/abs/2312.07104""]","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context provides specific percentages for the increase in U.S. data center electricity consumption between 2010 and 2014, stating a 4% increase."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13,900","cars","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","is_blank","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including specific locations in Virginia, California, and Singapore.","24","data centers","[""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including specific locations in Virginia, California, and Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context mentions that NVIDIA does not release the embodied carbon emissions of its hardware, and the authors note the lack of available information about the embodied emissions linked to manufacturing GPUs, making it impossible to estimate the emissions per GPU.","is_blank","kg/GPU","[""luccioni2025b"", ""morrison2025""]","[""https://link.springer.com/article/10.1007/978-985-162-008-1"", ""https://dl.acm.org/doi/10.1145/34053557""]","is_blank","The context mentions that NVIDIA does not release the embodied carbon emissions of its hardware, and the authors note the lack of available information about the embodied emissions linked to manufacturing GPUs, making it impossible to estimate the emissions per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context provides the total cost of training FLM-101B as $40M and estimates that the most expensive model by 2027 will cost about $1 billion, suggesting a growth rate that can be used to extrapolate the cost.","1","ratio","[""li2025a, cottier2024""]","[""li2025a"", ""cottier2024""]","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M. GPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.","The context provides the total cost of training FLM-101B as $40M and estimates that the most expensive model by 2027 will cost about $1 billion, suggesting a growth rate that can be used to extrapolate the cost."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides a comparison of the CO2e emissions from training large NLP models to the emissions from a round trip flight from SF to NY, stating that a single passenger round trip is approximately 1.2t CO2e. It also mentions that the training of one model consumed about one-tenth of a round trip's CO2e.","1.2","tCO2e","[""patterson2021"", ""strubell2019""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2). Having spent 13 pages on the cost of large NLP models and neural architecture search, we conclude our discussion with three examples of the potential benefits of NLP models. CO2e from training one model consumed ~2.8 MWh and produced ~0.13 tCO2e, about one-tenth of a SF-NY round trip by one passenger.","The context provides a comparison of the CO2e emissions from training large NLP models to the emissions from a round trip flight from SF to NY, stating that a single passenger round trip is approximately 1.2t CO2e. It also mentions that the training of one model consumed about one-tenth of a round trip's CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","","","score","[]","is_blank","is_blank",""
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","","","fold","[]","is_blank","is_blank",""
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides information on the training time for GPT-3 using 10,000 V100 GPUs and the FLOPS they can achieve, allowing for the calculation of the number of days required based on the total FLOPS needed to train the model.","14.8","days","[""patterson2021"", ""luccioni2023"", ""strubell2019"", ""samsi2024"", ""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://arxiv.org/abs/2310.03003v1"", ""https://facc-2022.conf.osceo.edu/""]","is_blank","The context provides information on the training time for GPT-3 using 10,000 V100 GPUs and the FLOPS they can achieve, allowing for the calculation of the number of days required based on the total FLOPS needed to train the model."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context discusses the energy consumption of various LLM models, including LLaMA variants, on different GPUs. It mentions the energy per second required to run inference on these models and the comparison of A100 and V100 GPUs.","varies for different models and GPUs","kWh","[""samsi2024"", ""jegham2025""]","[""N/A""]","Figure 3 shows a comparison of the energy per second required to run inference on LLaMA 7B, 13B, and 65B, with different GPUs under the same bare minimum hardware settings as the above.","The context discusses the energy consumption of various LLM models, including LLaMA variants, on different GPUs. It mentions the energy per second required to run inference on these models and the comparison of A100 and V100 GPUs."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context mentions the total permitted annual emission limits for diesel generators in Virginia, which are used as a reference for backup generators in data centers. These limits include specific amounts for NOx, VOCs, SO2, and PM2.5.","13000, 1400, 50, 600","tons","[""han2024""]","is_blank","is_blank","The context mentions the total permitted annual emission limits for diesel generators in Virginia, which are used as a reference for backup generators in data centers. These limits include specific amounts for NOx, VOCs, SO2, and PM2.5."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context indicates that using a growth strategy for training the 101B model resulted in a 72% time-saving or a 3.56x speedup compared to training from scratch, as stated in the document.","72","percent","[""li2025a"", ""erben2023""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context indicates that using a growth strategy for training the 101B model resulted in a 72% time-saving or a 3.56x speedup compared to training from scratch, as stated in the document."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","","","deaths","[]","is_blank","is_blank",""
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) mention that a full training run of a 6.1 billion parameter model would take approximately 60 days, as indicated by the phrase '60 days' following the model's description.","60","days","[""dodge2022""]","[""https://www.example.com/dodge2022""]","is_blank","Dodge et al. (2022) mention that a full training run of a 6.1 billion parameter model would take approximately 60 days, as indicated by the phrase '60 days' following the model's description."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context discusses the energy consumption during the inference phase of language models, comparing different models and hardware configurations. It mentions the use of CodeCarbon to measure energy consumption and the importance of considering both accuracy and energy efficiency.","is_blank","is_blank","[""zschache2025"", ""samsi2024"", ""luccioni2024""]","[""https://mlco2.github.io/codecarbon/output.html"", ""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%F"", ""https://arxiv.org/abs/2508.14170v1""]","Comparing energy consumption and accuracy in text classification inference; Energy consumption during inference is shown to highly correlate with model runtime; Energy consumption during inference is measured using CodeCarbon.","The context discusses the energy consumption during the inference phase of language models, comparing different models and hardware configurations. It mentions the use of CodeCarbon to measure energy consumption and the importance of considering both accuracy and energy efficiency."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context mentions that the One Hundred Year Study on AI was launched in the fall of 2014 and that it is a long-term investigation that forms a Study Panel every five years. The first report mentioned is from 2015, indicating that the study began in 2014.","2014","year","[""stone2022"", ""stone2022"", ""stone2022""]","[""https://ai100.stanford.edu"", ""https://ai100.stanford.edu"", ""https://ai100.stanford.edu""]","is_blank","The context mentions that the One Hundred Year Study on AI was launched in the fall of 2014 and that it is a long-term investigation that forms a Study Panel every five years. The first report mentioned is from 2015, indicating that the study began in 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN’s Global E-Waste Monitor 2024 report states that approximately 22% of e-waste has been formally collected and recycled.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN’s Global E-Waste Monitor 2024 report states that approximately 22% of e-waste has been formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information on the energy consumption for fine-tuning the Bloomz-7B model but does not directly state the energy consumption for pre-training the BLOOM model.","is_blank","MWh","[""ebert2024, cottier2024, luccioni2024, dodge2022""]","is_blank","is_blank","The context provides information on the energy consumption for fine-tuning the Bloomz-7B model but does not directly state the energy consumption for pre-training the BLOOM model."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context provides information on the energy consumption and carbon emissions of various models during their training runs on different GPUs. It mentions that the training of a 6 billion parameter transformer used 179 MWh and would estimate a full training run to consume approximately 103,593 kWh.","103,593","percent","[""patterson2021"", ""dodge2022""]","[""https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py"", ""https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.","The context provides information on the energy consumption and carbon emissions of various models during their training runs on different GPUs. It mentions that the training of a 6 billion parameter transformer used 179 MWh and would estimate a full training run to consume approximately 103,593 kWh."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context mentions that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, which includes attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token. This information directly supports the answer to the question about the number of experts in each MoE layer.","1","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1"", ""https://github.com/myshell-ai/JetMoE""]","is_blank","The context mentions that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, which includes attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token. This information directly supports the answer to the question about the number of experts in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","","","second","[]","is_blank","is_blank",""
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides a table listing the estimated floating-point operations (zettaFLOPs) for the training process of various models, including FLM-101B, which specifies 28.22 zettaFLOPs for the English portion of the model.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context provides a table listing the estimated floating-point operations (zettaFLOPs) for the training process of various models, including FLM-101B, which specifies 28.22 zettaFLOPs for the English portion of the model."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context discusses experiments conducted on various hardware configurations, specifically mentioning NVIDIA GPUs and the use of different models on these setups. It also details the hardware configurations used for the experiments, including the number of GPUs and CPUs per node.","1","is_blank","[""zschache2025,samsi2024,khan2025""]","is_blank","MIT Supercloud HPC system with NVIDIA GPUs, NVIDIA A100 GPUs, and Intel Xeon and Phi-3-mini-128k-Instruct models","The context discusses experiments conducted on various hardware configurations, specifically mentioning NVIDIA GPUs and the use of different models on these setups. It also details the hardware configurations used for the experiments, including the number of GPUs and CPUs per node."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","","","models","[]","is_blank","is_blank",""
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3 times.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3 times."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, indicating that the total compute for model development is 1.2x to 4x larger than the final training run.","1.2 to 4","multiplier","[""cottier2024"", ""morrison2025""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/prices.py#L210-L294"", ""https://github.com/epoch-research/training-cost-trends/blob/main/uncertainty.ipynb""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, indicating that the total compute for model development is 1.2x to 4x larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides a direct estimate of the energy consumption for training a 6.1 billion parameter transformer model, stating that it would take approximately 103,593 kWh to complete the training.","103,593","MWh","[""dodge2022"", ""han2024"", ""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/"", ""URL not provided for the other references""]","is_blank","The context provides a direct estimate of the energy consumption for training a 6.1 billion parameter transformer model, stating that it would take approximately 103,593 kWh to complete the training."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context discusses Jevons' Paradox in relation to AI, explaining how efficiency gains can lead to increased consumption, which is supported by the paper's argument that technical efficiency gains in AI may not lead to net environmental benefits due to this economic principle.","1","is_blank","[""luccioni2025a"", ""zschache2025""]","[""https://doi.org/10.1145/3715275.3732007"", ""https://org/10.1038/s41558-022-01377-7""]","Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context discusses Jevons' Paradox in relation to AI, explaining how efficiency gains can lead to increased consumption, which is supported by the paper's argument that technical efficiency gains in AI may not lead to net environmental benefits due to this economic principle."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context directly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context directly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context indicates that despite the increasing computational costs in AI research, Red AI is on the rise and the paper advocates for making efﬁciency a common evaluation criterion alongside accuracy. It also suggests reporting the computational cost to provide baselines for more data-efﬁcient research.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597v3""]","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This paper identiﬁes key factors that contribute to Red AI and advocates the introduction of a simple, easy-to-compute efﬁciency metric that could help make some AI research greener, more inclusive, and perhaps more cognitively plausible.","The context indicates that despite the increasing computational costs in AI research, Red AI is on the rise and the paper advocates for making efﬁciency a common evaluation criterion alongside accuracy. It also suggests reporting the computational cost to provide baselines for more data-efﬁcient research."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context indicates that the total wall-clock time required to train the FLM-101B model using a growth strategy is 21.54 days, which is a 72% time-saving compared to training a 101B model from scratch.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context indicates that the total wall-clock time required to train the FLM-101B model using a growth strategy is 21.54 days, which is a 72% time-saving compared to training a 101B model from scratch."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","","","is_blank","[]","is_blank","is_blank",""
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context provides an estimated training energy consumption of 1287 MWh for GPT-3 and speculates about the water consumption during its training, mentioning that over 5 million liters of water were used.","1287","MWh","[""li2025b"", ""jegham2025"", ""han2024"", ""morrison2025""]","is_blank","is_blank","The context provides an estimated training energy consumption of 1287 MWh for GPT-3 and speculates about the water consumption during its training, mentioning that over 5 million liters of water were used."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score 21 as a project that provides a standardized methodology for comparing the inference efficiency of AI models.","1","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the AI Energy Score 21 as a project that provides a standardized methodology for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context discusses the execution time breakdown for MoE layers in Mixtral and BlackMamba models, indicating that matrix multiplication is the largest component of the MoE layer for both models. However, the exact execution time for the longest kernel of the MoE layer in a dense BlackMamba model with a batch size of 30 is not explicitly stated in the provided context.","is_blank","microseconds","[""xia2024""]","is_blank","is_blank","The context discusses the execution time breakdown for MoE layers in Mixtral and BlackMamba models, indicating that matrix multiplication is the largest component of the MoE layer for both models. However, the exact execution time for the longest kernel of the MoE layer in a dense BlackMamba model with a batch size of 30 is not explicitly stated in the provided context."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that both layers in JetMoE-8B are sparsely activated, allowing the model to have 8B parameters while only activating 2B for each input token. This information directly supports the answer to the question about the number of experts selected for activation in each layer of the JetMoE-8B model.","2","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token.","The context states that both layers in JetMoE-8B are sparsely activated, allowing the model to have 8B parameters while only activating 2B for each input token. This information directly supports the answer to the question about the number of experts selected for activation in each layer of the JetMoE-8B model."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context directly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, from 2012 to 2018.","300,000","is_blank","[""schwartz2019"", ""wu2021a"", ""morrison2025""]","[""https://arxiv.org/abs/1907.10597"", ""https://pubmed.ncbi.nlm.nih.gov/25629940/"", ""https://time.com/7021709/""]","is_blank","The context directly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, from 2012 to 2018."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","","","percent","[]","is_blank","is_blank",""
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, an increase from 19 regions in 2022.","100","data centers","[""amazon2023"", ""amazon2023""]","is_blank","is_blank","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","","","years","[]","is_blank","is_blank",""
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context discusses the misinterpretation of the 'five cars' carbon footprint estimate from a 2019 study, highlighting that it does not accurately represent the emissions of every AI training workload and that recent reports indicate actual emissions from language model pretraining exceed this estimate.","284","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context discusses the misinterpretation of the 'five cars' carbon footprint estimate from a 2019 study, highlighting that it does not accurately represent the emissions of every AI training workload and that recent reports indicate actual emissions from language model pretraining exceed this estimate."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context discusses the energy consumption of various models, including Llama 3.1 8B, when targeting an average Time Per Output Token (TPOT) of 100 ms. It mentions that by sacrificing some latency, significant energy savings can be achieved, landing on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to configurations that minimize latency.","44","percent","[""chung2025"", ""jegham2025""]","is_blank","is_blank","The context discusses the energy consumption of various models, including Llama 3.1 8B, when targeting an average Time Per Output Token (TPOT) of 100 ms. It mentions that by sacrificing some latency, significant energy savings can be achieved, landing on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to configurations that minimize latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context discusses the maximum batch size for fine-tuning models on NVIDIA GPUs, specifically mentioning the A40 GPU with 48GB memory. It provides a table with maximum batch sizes for different models and datasets but does not specify the exact number for BlackMamba on the GSM8K dataset.","is_blank","samples","[""xia2024"", ""xia2024""]","is_blank","is_blank","The context discusses the maximum batch size for fine-tuning models on NVIDIA GPUs, specifically mentioning the A40 GPU with 48GB memory. It provides a table with maximum batch sizes for different models and datasets but does not specify the exact number for BlackMamba on the GSM8K dataset."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","","","GW","[]","is_blank","is_blank",""
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context indicates that the annual carbon emissions from GPT-4o inference are comparable to the emissions from 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context indicates that the annual carbon emissions from GPT-4o inference are comparable to the emissions from 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context provides information on the energy consumption of AI inference workloads, with Meta reporting that inference workloads constitute up to 70% of their AI power consumption and Google attributing 60% of their ML energy to inference. However, the specific percentage of AI inference workloads in Asia powered by coal in 2023 is not directly stated.","is_blank","percent","[""fernandez2025, morrison2025, li2025a, li2025b, luccioni2024""]","is_blank","is_blank","The context provides information on the energy consumption of AI inference workloads, with Meta reporting that inference workloads constitute up to 70% of their AI power consumption and Google attributing 60% of their ML energy to inference. However, the specific percentage of AI inference workloads in Asia powered by coal in 2023 is not directly stated."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides specific figures for the global average Power Usage Effectiveness (PUE) of data centers in 2023, stating it as 1.58 globally and 1.6 in the EU.","1.58","PUE","[""ebert2024"", ""patterson2021"", ""li2025b"", ""dodge2022""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide"", ""https://www.congress.gov/bill/118th-congress/senate-bill/3732/"", ""https://arxiv.org/abs/2305.05733"", ""https://www.google.com/about/datacenters/efficiency/""]","The global average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides specific figures for the global average Power Usage Effectiveness (PUE) of data centers in 2023, stating it as 1.58 globally and 1.6 in the EU."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters and only activates 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B.","2","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters and only activates 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","","","tCO2e","[]","is_blank","is_blank",""
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides specific numbers for the increase in electric delivery vans from Rivian in the U.S. from 2022 to 2023, stating the fleet increased from more than 2,600 to 11,800.","11,800","electric delivery vans","[""amazon2023"", ""amazon2023""]","is_blank","is_blank","The context provides specific numbers for the increase in electric delivery vans from Rivian in the U.S. from 2022 to 2023, stating the fleet increased from more than 2,600 to 11,800."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from energy sources used to power model training and deployment, which encompasses servers and data center cooling.","Operational environmental impacts of LLMs","is_blank","[""morrison2025"", ""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410"", ""https://arxiv.org/abs/2504.06307v1""]","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from energy sources used to power model training and deployment, which encompasses servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","","","Wh","[]","is_blank","is_blank",""
"q056","When was the field of Artificial Intelligence officially christened?","","","year","[]","is_blank","is_blank",""
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context discusses the water consumption of AI models, particularly GPT-3, during training and inference, and the importance of considering both on-site and off-site water efficiencies. It also highlights the need for transparency in reporting AI's water footprint.","1","WUE","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint.","The context discusses the water consumption of AI models, particularly GPT-3, during training and inference, and the importance of considering both on-site and off-site water efficiencies. It also highlights the need for transparency in reporting AI's water footprint."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people worldwide do not have access to a stable supply of electricity, as per the International Energy Agency.","770","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people worldwide do not have access to a stable supply of electricity, as per the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","","","joules per token","[]","is_blank","is_blank",""
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context mentions that by converting 32-bit floating-point representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly supports the answer to the question.","15","percent","[""wu2021a""]","is_blank","is_blank","The context mentions that by converting 32-bit floating-point representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly supports the answer to the question."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context discusses the claim that AI can help reduce global GHG emissions by 5-10%, tracing it back to a 2021 Boston Consulting Group report and a more recent one commissioned by Google. However, it also highlights the lack of detailed calculations and the need for scientific grounding, mentioning the methodological requirements for calculating avoided emissions.","1","is_blank","[""luccioni2025c"", ""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007"", ""https://doi.org/10.1145/3715275.3732007""]","This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union”48. The same number appears in a more recent BCG report from 2023, which was commissioned by Google and published ahead of COP2649.","The context discusses the claim that AI can help reduce global GHG emissions by 5-10%, tracing it back to a 2021 Boston Consulting Group report and a more recent one commissioned by Google. However, it also highlights the lack of detailed calculations and the need for scientific grounding, mentioning the methodological requirements for calculating avoided emissions."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context mentions that sparsely activated models can have many parameters while requiring much less computation than dense models, and that they can provide more than 10 times reductions in computation and energy costs while offering higher accuracy. This supports the statement that sparsely activated DNNs can consume less energy than large, dense DNNs without sacrificing accuracy.","1","is_blank","[""patterson2021""]","is_blank","Sparsely activated mixture-of-expert-style models can provide more than 10X reductions in computation and energy costs for both training and inference while providing significantly higher accuracy than dense Transformer or LSTM-based models of equivalent computational cost per token.","The context mentions that sparsely activated models can have many parameters while requiring much less computation than dense models, and that they can provide more than 10 times reductions in computation and energy costs while offering higher accuracy. This supports the statement that sparsely activated DNNs can consume less energy than large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019"", ""cottier2024""]","[""https://github.com/google-research/bert"", ""https://opensource.google.com/projects/open-images-dataset""]","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context indicates that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when using a batch size of 1, while the optimizer stage in Mixtral fine-tuning takes up a negligible amount of time. This difference is attributed to the distinct fine-tuning strategies applied to these models, with only parameters in the LoRA module being updated for Mixtral and full fine-tuning for BlackMamba.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context indicates that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when using a batch size of 1, while the optimizer stage in Mixtral fine-tuning takes up a negligible amount of time. This difference is attributed to the distinct fine-tuning strategies applied to these models, with only parameters in the LoRA module being updated for Mixtral and full fine-tuning for BlackMamba."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides information on the energy consumption of various tasks performed by AI models, including a comparison of the energy used by a single query to ChatGPT versus a Google search.","0.42","MWh","[""luccioni2024"", ""jegham2025"", ""li2025b""]","[""https://blog.google/products/translate/ten-years-of-google-translate/"", ""https://jegham2025.pdf""]","Our framework estimates 0.42 Wh ( ±0.13 Wh) for a short GPT-4o prompt (0.37 Wh without datacenter overhead), within 19% of Altman’s figure.","The context provides information on the energy consumption of various tasks performed by AI models, including a comparison of the energy used by a single query to ChatGPT versus a Google search."
"q067","What was the average global data center PUE in 2023?","The context directly states the average global data center PUE in 2023 as 1.58 globally.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide""]","is_blank","The context directly states the average global data center PUE in 2023 as 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","","","wind turbines","[]","is_blank","is_blank",""
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a detailed breakdown of the costs associated with developing frontier AI models, including the significant role of R&D staff costs, which range from 29% to 49% of the total amortized model development costs depending on the model.","29-49","percent","[""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024""]","is_blank","Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total.","The context provides a detailed breakdown of the costs associated with developing frontier AI models, including the significant role of R&D staff costs, which range from 29% to 49% of the total amortized model development costs depending on the model."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","","","people","[]","is_blank","is_blank",""
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","","","percent","[]","is_blank","is_blank",""
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context discusses the relationship between model size, energy consumption, and accuracy, indicating that larger models tend to consume more energy but may not necessarily be more accurate. It also mentions variability in energy consumption influenced by model type and hardware specifications, and a correlation between energy consumption and model runtime, suggesting that execution time can be a proxy for energy usage.","1","is_blank","[""chung2025"", ""wu2021a"", ""schwartz2019"", ""chung2025"", ""zschache2025""]","[""https://ml.energy/leaderboard"", ""https://github.com/Swall0w/torchstat"", ""https://github.com/Lyken17/pytorch-OpCounter"", ""https://arxiv.org/abs/2505.06371v2"", ""https://arxiv.org/abs/2508.14170v1""]","Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context discusses the relationship between model size, energy consumption, and accuracy, indicating that larger models tend to consume more energy but may not necessarily be more accurate. It also mentions variability in energy consumption influenced by model type and hardware specifications, and a correlation between energy consumption and model runtime, suggesting that execution time can be a proxy for energy usage."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating that they do not view AI as an immediate danger.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating that they do not view AI as an immediate danger."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context discusses the methodology for estimating the environmental impact of developing large language models, including operational GHG emissions and embodied emissions from manufacturing, but does not provide specific figures for OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context discusses the methodology for estimating the environmental impact of developing large language models, including operational GHG emissions and embodied emissions from manufacturing, but does not provide specific figures for OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides specific figures for GHG emissions from training Meta's Llama 3 models, stating they emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c"", ""morrison2025""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the 'five cars' estimate.","The context provides specific figures for GHG emissions from training Meta's Llama 3 models, stating they emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context directly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2 between 2019 and 2021.","2.9","multiplier","[""wu2021a"", ""cottier2024""]","[""https://paperswithcode.com/sota/image-classi\ufb01cation-on"", ""https://2021.naacl.org/ethics/faq/""]","The explosive growth in AI has driven 2.9× increase in AI training infrastructure capacity at Facebook over the 1.5 years.","The context directly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2 between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","The context discusses the water consumption of training large AI models like GPT-3 but does not provide a specific figure for the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context discusses the water consumption of training large AI models like GPT-3 but does not provide a specific figure for the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","","","is_blank","[]","is_blank","is_blank",""
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context discusses continuous batching and its benefits in improving GPU utilization and reducing idle time by dynamically replacing completed requests with new ones, as mentioned in the context related to ref_id=fernandez2025.","is_blank","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context discusses continuous batching and its benefits in improving GPU utilization and reducing idle time by dynamically replacing completed requests with new ones, as mentioned in the context related to ref_id=fernandez2025."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context mentions that the entire alignment process takes 60 H100 GPU hours for JetMoE-8B.","60","H100 GPU hours","[""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024""]","is_blank","60 H100 GPU hours","The context mentions that the entire alignment process takes 60 H100 GPU hours for JetMoE-8B."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context shows that for a 100 TPS SLO, InferSave selected g4dn.xlarge, which cost $2.13, while Max-Performance selected g6e.xlarge, which cost $2.699. This indicates that Max-Performance's choice was about 280% more expensive than InferSave's choice.","280","percent","[""kim2025""]","is_blank","100 TPS SLO: InferSave-1st g4dn.xlarge 100 169.17 2.13, Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The context shows that for a 100 TPS SLO, InferSave selected g4dn.xlarge, which cost $2.13, while Max-Performance selected g6e.xlarge, which cost $2.699. This indicates that Max-Performance's choice was about 280% more expensive than InferSave's choice."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","","","g CO2eq","[]","is_blank","is_blank",""
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","","","Wh","[]","is_blank","is_blank",""
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","","","is_blank","[]","is_blank","is_blank",""
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context directly states that the gross carbon intensity of energy according to the U.S. average mix in 2021 was 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021"", ""USE21""]","is_blank","The U.S. average mix carbon intensity of energy in 2021 was 0.429 kg of CO2e/KWh [USE21].","The context directly states that the gross carbon intensity of energy according to the U.S. average mix in 2021 was 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","","","is_blank","[]","is_blank","is_blank",""
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context discusses the concept of social transparency in AI, which includes the societal impacts, ethical considerations, and environmental footprint of AI systems. It suggests expanding transparency to encompass social and environmental aspects to create AI systems that are more robust, socially responsible, and accountable for their environmental impacts.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context discusses the concept of social transparency in AI, which includes the societal impacts, ethical considerations, and environmental footprint of AI systems. It suggests expanding transparency to encompass social and environmental aspects to create AI systems that are more robust, socially responsible, and accountable for their environmental impacts."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context indicates that a traditional linear model using pre-trained sentence embeddings achieved the highest accuracy in classification experiments on German public administration texts.","1","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets""]","The linear model with sentence embeddings is among the top-performing models for emotion classification.","The context indicates that a traditional linear model using pre-trained sentence embeddings achieved the highest accuracy in classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context discusses the development of Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster, as evidenced by the document with ref_id=chen2024.","1","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814v2""]","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context discusses the development of Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster, as evidenced by the document with ref_id=chen2024."
"q093","How many parameters does the largest T5 model have?","","","parameters","[]","is_blank","is_blank",""
"q094","What is the total number of parameters in the JetMoE-8B model?","The context provides a table labeled 'JetMoE-8B hyperparameters' which lists the hyperparameters used in the model, including the number of parameters (Ptotal).","8","parameters","[""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat"", ""ajibawa 2023. Code-290k-sharegpt"", ""ajibawa-2023/Code-290k-ShareGPT"", ""shen2024""]","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context provides a table labeled 'JetMoE-8B hyperparameters' which lists the hyperparameters used in the model, including the number of parameters (Ptotal)."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context directly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022.","20","percent","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","Google's data center water consumption increased by∼20% from 2021 to 2022 [4]","The context directly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context discusses various metrics for evaluating the environmental impact of AI models, including energy consumption and carbon emissions. However, it does not explicitly mention a specific metric called 'CO₂ emissions per unit of electricity consumed'. The closest reference is to 'Operational carbon emissions (gCO2e)', which is calculated by multiplying energy consumption by the carbon intensity of the region and time frame.","is_blank","is_blank","[""chung2025,khan2025,jegham2025,patterson2021,rubei2025""]","is_blank","This is also supported by Zeus [2], the energy measurement library employed by the benchmark.","The context discusses various metrics for evaluating the environmental impact of AI models, including energy consumption and carbon emissions. However, it does not explicitly mention a specific metric called 'CO₂ emissions per unit of electricity consumed'. The closest reference is to 'Operational carbon emissions (gCO2e)', which is calculated by multiplying energy consumption by the carbon intensity of the region and time frame."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","","","percent","[]","is_blank","is_blank",""
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context discusses the costs associated with training large AI models like GPT-4, mentioning that the acquisition cost was $800M, while the amortized cost was $40M. It also provides a growth rate estimate for the cost of training the most expensive models, suggesting that by 2027, the cost will be around $1B.","1","USD","[""cottier2024"", ""li2025b""]","[""https://arxiv.org/pdf/2408.04693v1.pdf"", ""https://arxiv.org/pdf/2405.21015v2.pdf""]","The estimated cloud compute costs for the final training run of frontier models are $40M for GPT-4, as mentioned in the document with ref_id cottier2024.","The context discusses the costs associated with training large AI models like GPT-4, mentioning that the acquisition cost was $800M, while the amortized cost was $40M. It also provides a growth rate estimate for the cost of training the most expensive models, suggesting that by 2027, the cost will be around $1B."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","","","multiplier","[]","is_blank","is_blank",""
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","","","multiplier","[]","is_blank","is_blank",""
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","","","liters","[]","is_blank","is_blank",""
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context discusses an investigation into how custom tags, when used in prompts, can reduce the energy consumption of LLMs during the inference phase for code completion tasks. Initial results from experiments using the CodeXGLUE benchmark and the CodeCarbon tool suggest that specific tags can distinguish different parts of prompts, leading to reduced energy consumption without compromising performance.","1","is_blank","[""rubei2025""]","[""https://github.com/riccardoRubei/Greens-2025-Replication-Package"", ""arXiv:2501.05899v1  [cs.SE]  10 Jan 2025""]","Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.","The context discusses an investigation into how custom tags, when used in prompts, can reduce the energy consumption of LLMs during the inference phase for code completion tasks. Initial results from experiments using the CodeXGLUE benchmark and the CodeCarbon tool suggest that specific tags can distinguish different parts of prompts, leading to reduced energy consumption without compromising performance."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context directly states that NVIDIA shipped 3.76 million data-center GPUs in 2024, as reported in a 2024 article.","3760000000","GPUs","[""luccioni2025a"", ""luccioni2025a"", ""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/"", ""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study.","The context directly states that NVIDIA shipped 3.76 million data-center GPUs in 2024, as reported in a 2024 article."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context mentions that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10, which is a measure of energy efficiency. It also states that these data centers are about 40% more efficient than small-scale, typical data centers.","1.10","PUE","[""wu2021a"", ""wu2021b"", ""ebert2024"", ""patterson2021""]","[""https://doi.org/10.2139/ssrn.12335119"", ""https://www.congress.gov/bill/118th-congress/senate-bill/3732"", ""https://www.sustainability.google/reports/google-2024-environmental-report/"", ""https://www.google.com/en-us/corporate-responsibility/sustainability/report""]","Facebook’s data centers have a Power Usage Effectiveness (PUE) of about 1.10, which is a measure of energy efficiency. It also states that these data centers are about 40% more efficient than small-scale, typical data centers.","The context mentions that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10, which is a measure of energy efficiency. It also states that these data centers are about 40% more efficient than small-scale, typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","","","is_blank","[]","is_blank","is_blank",""
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","","","USD","[]","is_blank","is_blank",""
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context discusses the AI Act's requirements for risk assessment and mitigation for providers of GPAI models with systemic risk, interpreting these as including environmental risks. However, it also highlights that these provisions lack sufficient emphasis on environmental factors and that detailed reporting on mitigation efforts concerning environmental risks is not currently required.","1","is_blank","[""ebert2024"", ""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For providers of GPAI models with systemic risk, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context discusses the AI Act's requirements for risk assessment and mitigation for providers of GPAI models with systemic risk, interpreting these as including environmental risks. However, it also highlights that these provisions lack sufficient emphasis on environmental factors and that detailed reporting on mitigation efforts concerning environmental risks is not currently required."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context directly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is higher than the WHO's recommended level.","9","µg/m³","[""han2024""]","is_blank","EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context directly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is higher than the WHO's recommended level."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device, indicating the environmental impact of the Kindle is comparable to that of multiple books.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/"", ""https://ref.geneontology.org/pub/enzyme/G0779"", ""https://www.refworld.org/legal/resolution/unga/2015/en/111816""]","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","A life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device, indicating the environmental impact of the Kindle is comparable to that of multiple books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context indicates that training a large AI model can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City, and that the per-household health burden in disadvantaged communities could be 200 times greater than in less-impacted communities.","200","multiplier","[""han2024"", ""han2024""]","[""https://arxiv.org/abs/2412.06288v2"", ""https://arxiv.org/abs/2412.06288v2""]","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","The context indicates that training a large AI model can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City, and that the per-household health burden in disadvantaged communities could be 200 times greater than in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","","","Wh","[]","is_blank","is_blank",""
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions that the 13 billion parameter model was trained on 5.6 trillion tokens, indicating the total number of parameters in the large language model analyzed by Dodge et al. in their 2022 paper.","13000000000000","parameters","[""schwartz2019"", ""morrison2025""]","[""https://github.com/google-research/bert"", ""https://opensource.google.com/projects/open-images-dataset""]","ranging in size from 20 million to 13 billion active parameters, trained on 1.7 to 5.6 trillion tokens.","The context mentions that the 13 billion parameter model was trained on 5.6 trillion tokens, indicating the total number of parameters in the large language model analyzed by Dodge et al. in their 2022 paper."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","","","is_blank","[]","is_blank","is_blank",""
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides information on the energy consumption of training GPT-3 and the cost of training runs for AI models like GPT-4 and Gemini Ultra, but does not directly provide a comparison of the water consumption between Meena training runs and a single full training run of GPT-3.","is_blank","multiplier","[""li2025b, wu2021a, patterson2021, dodge2022, jegham2025, cottier2024""]","is_blank","is_blank","The context provides information on the energy consumption of training GPT-3 and the cost of training runs for AI models like GPT-4 and Gemini Ultra, but does not directly provide a comparison of the water consumption between Meena training runs and a single full training run of GPT-3."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides a range of energy consumption values for various AI tasks, including image generation, which uses significantly more energy than text-based tasks. Specifically, it states that image generation uses, on average, over 60 times more energy than text generation for the same number of inferences.","2.9","kWh","[""luccioni2024"", ""luccioni2025c"", ""jegham2025""]","[""https://acm.org/facc2024"", ""https://arxiv.org/abs/2503.05804"", ""https://openrouter.ai/rankings?view=month""]","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh.","The context provides a range of energy consumption values for various AI tasks, including image generation, which uses significantly more energy than text-based tasks. Specifically, it states that image generation uses, on average, over 60 times more energy than text generation for the same number of inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides estimated CO2 emissions for various NLP model training experiments, including a comparison to the average emissions from an American home for a year.","1","lbs","[""strubell2019"", ""dodge2022"", ""morrison2025"", ""patterson2021"", ""jegham2025""]","[""https://arxiv.org/abs/1906.02243v1"", ""https://www.academia.edu/34216912/Even_partially_trained_experiments_can_emit_more_CO2_than_all_emissions_from_the_average_US_home_for_a_year"", ""https://dodge2022"", ""https://morrison2025"", ""https://patterson2021"", ""https://jegham2025""]","American life, avg, 1 year 36,156","The context provides estimated CO2 emissions for various NLP model training experiments, including a comparison to the average emissions from an American home for a year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides information on the projected increase in public health costs due to U.S. data centers from 2023 to 2028, with a focus on the uneven distribution across counties, particularly affecting low-income communities. It mentions that under high-growth scenarios, the total public health impact could reach $20.9 billion, potentially tripling from 2023.","20.90","is_blank","[""han2024"", ""han2024""]","is_blank","is_blank","The context provides information on the projected increase in public health costs due to U.S. data centers from 2023 to 2028, with a focus on the uneven distribution across counties, particularly affecting low-income communities. It mentions that under high-growth scenarios, the total public health impact could reach $20.9 billion, potentially tripling from 2023."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context discusses the reduction in carbon emissions achieved through optimization techniques such as quantization and local inference, but does not provide a specific multiplier for Mistral-small's emissions after optimization.","is_blank","multiplier","[""jegham2025"", ""dodge2022"", ""khan2025"", ""patterson2021""]","is_blank","is_blank","The context discusses the reduction in carbon emissions achieved through optimization techniques such as quantization and local inference, but does not provide a specific multiplier for Mistral-small's emissions after optimization."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy costs for fine-tuning and training the BLOOMz-7B model, with specific values given for each. The fine-tuning energy is reported as 7,571 kWh, and the training energy is reported as 51,686 kWh.","7,571","kWh","[""ebert2024"", ""luccioni2024""]","[""https://link_to_ebert2024_document"", ""https://link_to_luccioni2024_document""]","is_blank","The context provides the energy costs for fine-tuning and training the BLOOMz-7B model, with specific values given for each. The fine-tuning energy is reported as 7,571 kWh, and the training energy is reported as 51,686 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context provides information on the performance of FLM-101B on various benchmarks, including knowledge-oriented and IQ-related tasks, but does not specify the total number of parameters in the final FLM-101B model.","is_blank","parameters","[""li2025a""]","is_blank","is_blank","The context provides information on the performance of FLM-101B on various benchmarks, including knowledge-oriented and IQ-related tasks, but does not specify the total number of parameters in the final FLM-101B model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","","","inferences","[]","is_blank","is_blank",""
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","","","kWh","[]","is_blank","is_blank",""
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides information on the energy costs of training, fine-tuning, and inference for BLOOMz models, and the number of inferences required for the cost of inference to equal the training cost. It mentions that for the BLOOMz-7B model, this number is over 590 million inferences.","592,570,000","inferences","[""luccioni2024"", ""ebert2024""]","[""https://www.similarweb.com/website/chat.openai.com/"", ""https://www.similarweb.com/website/chat.openai.com/""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5.","The context provides information on the energy costs of training, fine-tuning, and inference for BLOOMz models, and the number of inferences required for the cost of inference to equal the training cost. It mentions that for the BLOOMz-7B model, this number is over 590 million inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany and is published as the FKTG-dataset. The dataset includes the text of the submission and its categorized topic.","is_blank","is_blank","[""zschache2025"", ""shen2024"", ""chung2025""]","[""https://beteiligung.bge.de/index.php"", ""https://github.com/ml-energy/leaderboard"", ""https://ml.energy/leaderboard""]","is_blank","The context mentions that the data used in the experiments originates from the process of selecting a repository site for high-level radioactive waste in Germany and is published as the FKTG-dataset. The dataset includes the text of the submission and its categorized topic."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context discusses the methodology for estimating AI's water consumption footprint, including operational and embodied water footprints, and provides specific figures for water consumption per kWh for different locations and technologies. However, it does not provide a specific figure for the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""li2025b, han2024""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint, Table 2: The county-level per-household health cost of two U.S. technology companies in 2023, Table 3: Scope-2 Water Usage, Listing 2: Fragment of a prompt including custom tags, Listing 3: Fragment of a prompt including custom tags explanation, Listing 4: Example of a zero-shot prompt including the definition of custom tags, Listing 5: Fragment of a prompt including custom tags, D. Metrics, C. Metrics, 3 Estimating AI’s Water Footprint, 3.1 Operational Water Footprint, 3.2 Embodied Water Footprint","The context discusses the methodology for estimating AI's water consumption footprint, including operational and embodied water footprints, and provides specific figures for water consumption per kWh for different locations and technologies. However, it does not provide a specific figure for the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context discusses the environmental impact of manufacturing NVIDIA H100 GPUs, including the use of rare earth metals and the embodied carbon emissions and water consumption. However, it does not provide a specific percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/"", ""luccioni2025a"", ""cottier2024""]","is_blank","The context discusses the environmental impact of manufacturing NVIDIA H100 GPUs, including the use of rare earth metals and the embodied carbon emissions and water consumption. However, it does not provide a specific percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a comparison between the CO2e of the Evolved Transformer NAS and the CO2e of a round trip passenger flight between San Francisco and New York, stating that the CO2e of the Evolved Transformer NAS is equivalent to approximately 3 passengers taking such a flight.","3","passengers","[""patterson2021""]","is_blank","Google Flights calculated the average CO2 emission for all the direct flights between San Francisco (SFO) and New York (JFK) in its database as 90.2t, so the average round trip is 180.4t.","The context provides a comparison between the CO2e of the Evolved Transformer NAS and the CO2e of a round trip passenger flight between San Francisco and New York, stating that the CO2e of the Evolved Transformer NAS is equivalent to approximately 3 passengers taking such a flight."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the May 2025 data from OpenRouter, only 2% of LLM token usage occurred through models that directly disclosed their environmental impact.","2","percent","[""luccioni2025c"", ""morrison2025""]","[""https://link_to_openrouter28_may2025_data"", ""https://link_to_morrison2025_paper""]","May 2025 data (Figure 2) indicates that of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1, DeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by releasing their model weights to enable efficiency analysis). In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the May 2025 data from OpenRouter, only 2% of LLM token usage occurred through models that directly disclosed their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","8","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","is_blank","8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","","","metric tons","[]","is_blank","is_blank",""
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context discusses the reduction of carbon emissions by up to 45% post quantization in large language models, indicating a focus on quantization techniques to lower the carbon footprint without compromising operational effectiveness.","45","tCO2e","[""khan2025"", ""rubei2025""]","[""https://arxiv.org/pdf/2504.06307v1.pdf"", ""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","The context discusses the reduction of carbon emissions by up to 45% post quantization in large language models, indicating a focus on quantization techniques to lower the carbon footprint without compromising operational effectiveness."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context provides a case where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and a 31% saving over an A10G-only strategy, as illustrated in Fig. 9 and supported by the data in Table 5.","24","percent","[""griggs2024""]","is_blank","Table 5: Instance allocations for the mixed context dataset, SLO=120ms.","The context provides a case where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and a 31% saving over an A10G-only strategy, as illustrated in Fig. 9 and supported by the data in Table 5."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context discusses the cost of renting GPU resources for fine-tuning LLM models, mentioning NVIDIA H100 and its cost for renting per hour based on CUDO compute. It also talks about estimating costs on other clouds and the cost for fine-tuning Mixtral on the MATH dataset.","1","USD per hour","[""xia2024"", ""cottier2024"", ""griggs2024"", ""morrison2025""]","[""https://arxiv.org/abs/25/0326.0008"", ""https://www.nvidia.com/content/en_us/data-center/products/data-center-gpus/nvidia-h100-tpu-v3/"", ""https://runpod.io/pricing"", ""https://www.iowa.gov/energy/"", ""https://techtarget.com/searchdatacenter/definition/power-usage-effectiveness-PUE""]","resource renting per hour is calculated based on CUDO compute [33], as other popular cloud providers do not offer cost/hour rates for the NVIDIA A40 GPU. However, one can easily adjust the GPU renting cost per hour to estimate the cost on other clouds such as Amazon AWS [34] or Lambda [35]. Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate.","The context discusses the cost of renting GPU resources for fine-tuning LLM models, mentioning NVIDIA H100 and its cost for renting per hour based on CUDO compute. It also talks about estimating costs on other clouds and the cost for fine-tuning Mixtral on the MATH dataset."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","","","is_blank","[]","is_blank","is_blank",""
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context provides specific percentages indicating that in 2023, the total public health costs attributable to data center operations are equivalent to 44% of the data centers' total electricity cost using the average attribution method.","44","percent","[""han2024""]","is_blank","is_blank","The context provides specific percentages indicating that in 2023, the total public health costs attributable to data center operations are equivalent to 44% of the data centers' total electricity cost using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","8","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible for each combination, assuming no further model compression, optimization, quantization, distillation etc.","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","","","is_blank","[]","is_blank","is_blank",""
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","Researchers were able to collect information from 95 out of 500 authors contacted for their carbon footprint analysis, as stated in the context.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","Researchers were able to collect information from 95 out of 500 authors contacted for their carbon footprint analysis, as stated in the context."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","","","USD per hour","[]","is_blank","is_blank",""
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context indicates that the health cost in Iowa for training Llama-3.1 scale models can exceed 120% of the electricity cost due to various factors, including wind direction and pollutant emission rates for electricity generation. It also highlights the location-dependent nature of public health impacts from AI model training.","120%","percent","[""han2024""]","is_blank","is_blank","The context indicates that the health cost in Iowa for training Llama-3.1 scale models can exceed 120% of the electricity cost due to various factors, including wind direction and pollutant emission rates for electricity generation. It also highlights the location-dependent nature of public health impacts from AI model training."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context mentions that JetMoE-8B is trained with 1.25T tokens from open-source datasets and 30,000 H100 GPU hours, indicating the number of tokens used for pre-training the model.","12500000000000000","tokens","[""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024"", ""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat"", ""https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT"", ""https://arxiv.org/abs/2108.07732"", ""https://arxiv.org/abs/2304.01373"", ""https://github.com/bigcode-project/bigcode-evaluation-harness"", ""https://github.com/myshell-ai/JetMoE""]","1.25T tokens of primarily English data from web documents, mathematics, and code","The context mentions that JetMoE-8B is trained with 1.25T tokens from open-source datasets and 30,000 H100 GPU hours, indicating the number of tokens used for pre-training the model."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context mentions that as of January 2024, Amazon signed a corporate PPA with ENGIE to increase its share of output from the Moray West offshore wind farm in Scotland to 473 MW, indicating the announcement of new renewable energy projects.","36","projects","[""amazon2023"", ""amazon2023""]","[""URL_for_amazon2023_announcement"", ""URL_for_amazon2023_ENGIE_PPA""]","Amazon signed a corporate PPA with the low-carbon energy and services company ENGIE to increase our share of output from the Moray West offshore wind farm in Scotland to 473 megawatts (MW) once the site becomes operational in 2024.","The context mentions that as of January 2024, Amazon signed a corporate PPA with ENGIE to increase its share of output from the Moray West offshore wind farm in Scotland to 473 MW, indicating the announcement of new renewable energy projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides percentages of Amazon's workforce by gender in the U.S. for various years but does not specify the exact percentage for 2023. The percentages listed range from 30.8% to 69.2% for different groups but do not include a specific figure for the entire workforce identified as men in 2023.","is_blank","percent","[""amazon2023""]","is_blank","is_blank","The context provides percentages of Amazon's workforce by gender in the U.S. for various years but does not specify the exact percentage for 2023. The percentages listed range from 30.8% to 69.2% for different groups but do not include a specific figure for the entire workforce identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context directly states that Apple's supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b"", ""wu2021b""]","[""https://www.apple.com/environment/"", ""https://www.wbdg.org/continuing-education/femp-courses/fempodw034""]","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context directly states that Apple's supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","","","seconds","[]","is_blank","is_blank",""
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context discusses the introduction of a granularity metric to assess the ratio of computation to communication time in distributed training, which is used to predict performance with different hardware setups.","1","is_blank","[""erben2023""]","is_blank","To quantify total training cost, we assess cost-effectiveness and evaluate a hybrid or multi-cloud approach with popular cloud providers through training on up to four continents. For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce thegranularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context discusses the introduction of a granularity metric to assess the ratio of computation to communication time in distributed training, which is used to predict performance with different hardware setups."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the coalition of Microsoft employees, a single deal with Exxon Mobil could add up to 640 percent more carbon emissions than the company's carbon removal targets for the year. This claim is supported by the context mentioning their estimation and the fact that these numbers were not included in Microsoft's carbon accounting and reporting efforts.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","According to the coalition of Microsoft employees, a single deal with Exxon Mobil could add up to 640 percent more carbon emissions than the company's carbon removal targets for the year. This claim is supported by the context mentioning their estimation and the fact that these numbers were not included in Microsoft's carbon accounting and reporting efforts."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","","","is_blank","[]","is_blank","is_blank",""
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022"", ""stone2022""]","[""https://ai100.stanford.edu"", ""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context directly states the average number of connected devices per U.S. household.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf"", ""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context directly states the average number of connected devices per U.S. household."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training large language models, specifically mentioning the minimum and maximum values.","0.8 to 3500","MWh","[""luccioni2025c"", ""dodge2022""]","[""https://link_to_context_luccioni2025c"", ""https://link_to_context_dodge2022""]","Infact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The context provides a range of energy consumption for pre-training large language models, specifically mentioning the minimum and maximum values."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","","","is_blank","[]","is_blank","is_blank",""
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context mentions that one paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water. This information directly supports the answer to the question.","0.5","queries","[""luccioni2025a""]","[""https://faccess2025.onlinelibrary.wsu.edu/files/3/Luccioni_et_al_2025_FAccT.pdf/attachment/1c5c5c3a-8e8f-4f5b-9f4e-5f6a9e3e7e9b/luccioni2025a.pdf""]","is_blank","The context mentions that one paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water. This information directly supports the answer to the question."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context indicates that after alignment, the JetMoE-8B-Chat model achieved a higher MT-Bench score than the Llama-2-13b-Chat model, demonstrating superior performance.","1","score","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1"", ""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.","The context indicates that after alignment, the JetMoE-8B-Chat model achieved a higher MT-Bench score than the Llama-2-13b-Chat model, demonstrating superior performance."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","","","responses","[]","is_blank","is_blank",""
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as evidenced by the paper's evaluation results and the formulation of the GPU allocation task as a cost-aware bin packing problem.","77","percent","[""griggs2024""]","[""arXiv:2404.14527v4  [cs.DC]  22 Jul 2024""]","Mélange consistently demonstrates significant reductions in deployment costs (up to 77%) while providing high SLO attainment.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as evidenced by the paper's evaluation results and the formulation of the GPU allocation task as a cost-aware bin packing problem."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context indicates that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","8","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context indicates that at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","","","round trips","[]","is_blank","is_blank",""
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","","","percent","[]","is_blank","is_blank",""
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context provides specific figures for the total energy used and the corresponding CO2 equivalent emissions for the entire 'Power Hungry Processing' study, including the energy consumed during experimentation and evaluation.","178.97","kg CO2eq","[""luccioni2024"", ""patterson2021"", ""luccioni2024""]","[""https://doi.org/10.18653/v1/2022.bigscience-1.8"", ""https://doi.org/10.48550/ARXIV.2204.05149"", ""https://doi.org/10.18653/v1/2022.bigscience-1.8""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context provides specific figures for the total energy used and the corresponding CO2 equivalent emissions for the entire 'Power Hungry Processing' study, including the energy consumed during experimentation and evaluation."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context discusses the limitations of using Thermal Design Power (TDP) for estimating GPU energy consumption, highlighting that TDP is often an overestimation and does not account for the actual power drawn at every moment in time by a GPU.","is_blank","is_blank","[""chung2025"", ""cottier2024"", ""luccioni2023"", ""ebert2024""]","is_blank","is_blank","The context discusses the limitations of using Thermal Design Power (TDP) for estimating GPU energy consumption, highlighting that TDP is often an overestimation and does not account for the actual power drawn at every moment in time by a GPU."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on A100 hardware instead of H100s, with a specific consumption of 3.098 Wh compared to GPT-4o's 2.875 Wh.","3.098","is_blank","[""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025""]","is_blank","is_blank","The context indicates that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on A100 hardware instead of H100s, with a specific consumption of 3.098 Wh compared to GPT-4o's 2.875 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","","","queries/sec","[]","is_blank","is_blank",""
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","","","is_blank","[]","is_blank","is_blank",""
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context discusses the normalized on-demand hourly price for different GPU types, including H100, but does not provide a specific numeric value for the H100 GPU.","is_blank","USD per hour","[""griggs2024""]","is_blank","is_blank","The context discusses the normalized on-demand hourly price for different GPU types, including H100, but does not provide a specific numeric value for the H100 GPU."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides information on the water consumption footprint of GPT-3 during its training phase, using Microsoft's data centers as the location for training. It mentions the use of the Power Usage Effectiveness (PUE) and Water Usage Effectiveness (WUE) metrics to estimate the water consumption, with specific values given for different locations. However, the exact amount of water used for cooling during the training run of GPT-4 is not directly stated.","is_blank","liters of water","[""li2025b"", ""jegham2025"", ""cottier2024"", ""patterson2021"", ""li2025b""]","is_blank","is_blank","The context provides information on the water consumption footprint of GPT-3 during its training phase, using Microsoft's data centers as the location for training. It mentions the use of the Power Usage Effectiveness (PUE) and Water Usage Effectiveness (WUE) metrics to estimate the water consumption, with specific values given for different locations. However, the exact amount of water used for cooling during the training run of GPT-4 is not directly stated."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs on major cloud platforms. Using this information, we can estimate the hourly cost.","15.64","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","The context mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs on major cloud platforms. Using this information, we can estimate the hourly cost."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation tasks, a model 1,000 times larger in size is required.","1,000","multiplier","[""wu2021a"", ""patterson2021""]","[""https://commoncrawl.org/2019/07/"", ""https://github.com/google-research/bert"", ""https://github.com/zihangdai/xlnet""]","1000× model size increase for GPT3-based language translation tasks [ 12], [13], whereas for Baidu’s search engine, the model of 1000× larger in size improves accuracy in AUC by 0.030.","The context states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation tasks, a model 1,000 times larger in size is required."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","","","miles","[]","is_blank","is_blank",""
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","","","MWh","[]","is_blank","is_blank",""
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context states that JetMoE-8B is trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","30,000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1"", ""https://github.com/myshell-ai/JetMoE""]","is_blank","The context states that JetMoE-8B is trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","","","USD","[]","is_blank","is_blank",""
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","","","FLOPS","[]","is_blank","is_blank",""
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context indicates that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides throughput data for different growth stages of a model trained on NVIDIA A800 GPUs, including the final 101B training stage, which shows a parallel size of 4, a size per GPU of 12, and a utilization rate of 52.88%. However, the exact number of FLOPs for the 101B stage is not explicitly stated.","is_blank","zettaFLOPs","[""li2025a, li2025a, li2025a""]","is_blank","is_blank","The context provides throughput data for different growth stages of a model trained on NVIDIA A800 GPUs, including the final 101B training stage, which shows a parallel size of 4, a size per GPU of 12, and a utilization rate of 52.88%. However, the exact number of FLOPs for the 101B stage is not explicitly stated."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions a specific top-1 accuracy achieved with AlexNet after SSL pre-training, which is 69.3%.","69.3","percent","[""wu2021a""]","[""https://github.com/sovrasov/flops-counter.pytorch""]","report achieving 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for 15","The context mentions a specific top-1 accuracy achieved with AlexNet after SSL pre-training, which is 69.3%."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 A800 GPUs.","24","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a clus-ter of 24 DGX-A800 GPU (8 ×80G) servers.","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 A800 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides an estimate of CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs, which is comparable to the lifetime emissions of five cars. This estimate is based on the work of Strubell et al., which is used as a reference in the context.","626,155","lifetimes","[""dodge2022"", ""luccioni2023"", ""patterson2021"", ""strubell2019""]","[""https://arxiv.org/abs/1906.02243"", ""https://arxiv.org/abs/1906.02243v1"", ""https://arxiv.org/abs/1906.02243v1"", ""https://arxiv.org/abs/1906.02243v1""]","626,155 lbs (from table 1 in strubell2019)","The context provides an estimate of CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs, which is comparable to the lifetime emissions of five cars. This estimate is based on the work of Strubell et al., which is used as a reference in the context."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context directly states that FAIR’s RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context directly states that FAIR’s RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that the 50 new on-site solar energy systems added in 2023 have a total capacity of 58 MW and are estimated to generate 123,000 MWh annually, avoiding roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources.","47,500","metric tons","[""amazon2023"", ""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf"", ""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that the 50 new on-site solar energy systems added in 2023 have a total capacity of 58 MW and are estimated to generate 123,000 MWh annually, avoiding roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","","","is_blank","[]","is_blank","is_blank",""
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context discusses the energy consumption of deploying the Llama 3.1 70B model on two nodes, indicating an increase in energy consumption due to the overhead of multiple nodes. The exact factor is not provided but is mentioned to depend on the model.","1","multiplier","[""zschache2025"", ""chung2025"", ""jegham2025""]","is_blank","is_blank","The context discusses the energy consumption of deploying the Llama 3.1 70B model on two nodes, indicating an increase in energy consumption due to the overhead of multiple nodes. The exact factor is not provided but is mentioned to depend on the model."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context discusses the energy and carbon footprint of AI models like GPT-4o but does not provide specific figures for water consumption per user session. It mentions the environmental impact of AI, including water usage for cooling servers, but does not give a direct answer for the question asked.","is_blank","gallons of water","[""jegham2025, li2025b, morrison2025, luccioni2025a""]","is_blank","is_blank","The context discusses the energy and carbon footprint of AI models like GPT-4o but does not provide specific figures for water consumption per user session. It mentions the environmental impact of AI, including water usage for cooling servers, but does not give a direct answer for the question asked."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides an estimate of the annual energy consumption of GPT-4o based on 700 million daily queries, with the framework attributing 700 million daily queries to GPT-4o's default model status and considering a baseline of 1 billion queries per day across all ChatGPT deployments. The estimated range for GPT-4o's annual energy consumption is given as approximately 391,509 MWh to 463,269 MWh.","391509-463269","homes","[""jegham2025"", ""jegham2025b""]","is_blank","is_blank","The context provides an estimate of the annual energy consumption of GPT-4o based on 700 million daily queries, with the framework attributing 700 million daily queries to GPT-4o's default model status and considering a baseline of 1 billion queries per day across all ChatGPT deployments. The estimated range for GPT-4o's annual energy consumption is given as approximately 391,509 MWh to 463,269 MWh."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context directly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Microsoft reports a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","The context directly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context indicates that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the higher energy costs associated with deploying LLMs.","1","is_blank","[""zschache2025"", ""strubell2019""]","[""https://huggingface.co/datasets"", ""https://arxiv.org/abs/1906.02243""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context indicates that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the higher energy costs associated with deploying LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context mentions that the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is 1.11, which is a factor of 1.4X better than the average.","1.11","PUE","[""patterson2021""]","is_blank","is_blank","The context mentions that the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is 1.11, which is a factor of 1.4X better than the average."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","","","queries","[]","is_blank","is_blank",""
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context mentions JetMoE-8B's performance on the OpenLLM Leaderboard, stating it outperforms other models and achieves the best scores in most tasks, except ARC-challenge and WinoGrande.","51.0","score","[""shen2024"", ""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat"", ""https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT""]","OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context mentions JetMoE-8B's performance on the OpenLLM Leaderboard, stating it outperforms other models and achieves the best scores in most tasks, except ARC-challenge and WinoGrande."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context discusses the environmental impact of AI, particularly focusing on the carbon and water footprints of AI training and inference processes. It mentions the use of renewable energy to reduce operational carbon footprints and the challenges of data center water usage.","Water consumption","training runs","[""wu2021a"", ""li2025b""]","[""https://arxiv.org/pdf/2304.03271v5.pdf"", ""https://arxiv.org/pdf/2304.03271v5.pdf""]","The water consumption of AI training has also been a concern [49, 83]. Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker the water consumption of AI training has also been a concern [49, 83].","The context discusses the environmental impact of AI, particularly focusing on the carbon and water footprints of AI training and inference processes. It mentions the use of renewable energy to reduce operational carbon footprints and the challenges of data center water usage."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context discusses the need for AI models to include energy consumption in reporting and the elimination of exemptions for open-source models. It also highlights the importance of including environmental risks in risk assessments.","1","is_blank","[""ebert2024"", ""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Elimination of the Open-Source Exemption : Remove the exemption that allows open-source models to bypass reporting obligations [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary mod-els.","The context discusses the need for AI models to include energy consumption in reporting and the elimination of exemptions for open-source models. It also highlights the importance of including environmental risks in risk assessments."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context provides information on the average Power Usage Effectiveness (PUE) of data centers worldwide and specifically in the US, with references to various sources and studies.","1.58","PUE","[""ebert2024"", ""wu2021b"", ""ebert2024"", ""wu2021b""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide"", ""https://www.congress.gov/bill/118th-congress/senate-bill/3732"", ""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide"", ""https://www.congress.gov/bill/118th-congress/senate-bill/3732""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides information on the average Power Usage Effectiveness (PUE) of data centers worldwide and specifically in the US, with references to various sources and studies."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a detailed analysis of KV Cache size growth in response to increasing batch sizes, specifically mentioning the cache size for a batch size of 32 in the OPT-2.7B model.","5.312","GB","[""kim2025""]","is_blank","KV Cache Size (GB) for Batch Size: 32 - 5.312GB (kim2025)","The context provides a detailed analysis of KV Cache size growth in response to increasing batch sizes, specifically mentioning the cache size for a batch size of 32 in the OPT-2.7B model."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides specific percentages for R&D staff costs as a part of the total amortized cost for notable models, ranging from 29% to 49% when equity is included, and 19% to 33% when equity is excluded.","29-49","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Excluding equity, the fraction decreases to 19% to 33%.","The context provides specific percentages for R&D staff costs as a part of the total amortized cost for notable models, ranging from 29% to 49% when equity is included, and 19% to 33% when equity is excluded."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the energy consumption and duration were measured using the CodeCarbon package, which tracks power usage of GPU and CPU.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""samsi2024""]","[""https://github.com/mlco2/codecarbon"", ""https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"", ""https://www.ray.io/"", ""https://github.com/vllm-project/vllm""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context mentions that the energy consumption and duration were measured using the CodeCarbon package, which tracks power usage of GPU and CPU."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context discusses the energy consumption of ChatGPT queries and compares it to Google searches, with 75% of articles relaying energy estimates without mentioning uncertainties or citing sources. It mentions that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c"", ""jegham2025""]","is_blank","is_blank","The context discusses the energy consumption of ChatGPT queries and compares it to Google searches, with 75% of articles relaying energy estimates without mentioning uncertainties or citing sources. It mentions that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","","","is_blank","[]","is_blank","is_blank",""
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context indicates that increasing the number of shards always increases the wattage, suggesting an increase in energy cost per second for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards). For both datasets, increasing the max generation length from 512 to 1024 does seem to increase the energy per second for each batch size within each shard configuration, but the overall effect is less clear or consistent. Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context indicates that increasing the number of shards always increases the wattage, suggesting an increase in energy cost per second for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information on water consumption during the manufacturing of GPUs, including estimates for the environmental impact of mining rare earth metals and the water usage per H100 GPU. However, it does not provide a direct figure for the water consumption of mining the rare earth metals specifically for a single H100 GPU.","is_blank","kL","[""morrison2025, li2025b""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth"", ""ref_id=morrison2025"", ""ref_id=li2025b""]","is_blank","The context provides information on water consumption during the manufacturing of GPUs, including estimates for the environmental impact of mining rare earth metals and the water usage per H100 GPU. However, it does not provide a direct figure for the water consumption of mining the rare earth metals specifically for a single H100 GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses the need for AI models to report energy consumption at the cumulative server level and the elimination of exemptions for open-source models from reporting obligations. It suggests that open-source models should adhere to the same reporting standards as proprietary models.","1","is_blank","[""ebert2024"", ""ebert2024""]","is_blank","is_blank","The context discusses the need for AI models to report energy consumption at the cumulative server level and the elimination of exemptions for open-source models from reporting obligations. It suggests that open-source models should adhere to the same reporting standards as proprietary models."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context directly states that Amazon, Google, Facebook, and Microsoft accounted for 30% of all PPAs purchased by corporations worldwide in 2020.","30","percent","[""luccioni2025a"", ""wu2021b""]","[""https://www.facc-2025.org/"", ""https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894""]","30% of all PPAs purchased by corporations worldwide [131]","The context directly states that Amazon, Google, Facebook, and Microsoft accounted for 30% of all PPAs purchased by corporations worldwide in 2020."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides information on the projected increase in public health costs due to U.S. data centers from 2023 to 2028, estimating the total public health cost to potentially triple during this period based on the average attribution method.","11.7 billion to 20.9 billion","USD","[""han2024""]","is_blank","is_blank","The context provides information on the projected increase in public health costs due to U.S. data centers from 2023 to 2028, estimating the total public health cost to potentially triple during this period based on the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context indicates that GPT-4.1 nano consumes approximately 0.207 Wh for a long prompt, while LLaMA-3.1-8B, the most efficient model, consumes 0.443 Wh for a similar prompt size. This suggests that the energy consumption of GPT-4.1 nano is nearly twice that of LLaMA-3.1-8B for long prompts.","0.207","multiplier","[""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025"", ""jegham2025""]","is_blank","GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly twice the energy of LLaMA-3.1-8B for long prompts.","The context indicates that GPT-4.1 nano consumes approximately 0.207 Wh for a long prompt, while LLaMA-3.1-8B, the most efficient model, consumes 0.443 Wh for a similar prompt size. This suggests that the energy consumption of GPT-4.1 nano is nearly twice that of LLaMA-3.1-8B for long prompts."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context provides detailed information on the cost reductions achieved by Mélange in various datasets and SLOs, specifically mentioning the percentage range for the Arena dataset with a 120ms SLO.","15-77","percent","[""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024"", ""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO).","The context provides detailed information on the cost reductions achieved by Mélange in various datasets and SLOs, specifically mentioning the percentage range for the Arena dataset with a 120ms SLO."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table (Table 3) with carbon emissions data for various models, including FLM-101B, which shows the net carbon emissions in metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","[""https://huggingface.co/datasets/laion/OIG"", ""https://huggingface.co/datasets/BAAI/COIG"", ""https://apps.timwhitlock.info/emoji/tables/unicode""]","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references.","The context provides a table (Table 3) with carbon emissions data for various models, including FLM-101B, which shows the net carbon emissions in metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context discusses the cost of fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on different GPUs, with a focus on the NVIDIA H100 and A40 GPUs. It provides specific cost estimates for these setups.","32.7","seconds","[""xia2024"", ""xia2024""]","is_blank","is_blank","The context discusses the cost of fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on different GPUs, with a focus on the NVIDIA H100 and A40 GPUs. It provides specific cost estimates for these setups."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context discusses the uneven distribution of public health impacts of AI across communities, particularly affecting low-income communities more severely, with per-household impacts potentially up to 200 times higher in the most impacted areas compared to less-affected areas.","1","is_blank","[""han2024, han2024, han2024, han2024""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html"", ""https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/"", ""https://www.epri.com/research/products/3002028905"", ""https://arxiv.org/abs/2506.15572v1""]","Recommendation 3: Promoting Public Health Equity - The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context discusses the uneven distribution of public health impacts of AI across communities, particularly affecting low-income communities more severely, with per-household impacts potentially up to 200 times higher in the most impacted areas compared to less-affected areas."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context mentions that the theoretical performance per watt of GPUs doubles every 3-4 years, as stated in the provided context.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context mentions that the theoretical performance per watt of GPUs doubles every 3-4 years, as stated in the provided context."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions Ollama being used for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases and is lightweight, making it suitable for localized AI solutions.","1","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications.","The context mentions Ollama being used for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases and is lightweight, making it suitable for localized AI solutions."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","","","is_blank","[]","is_blank","is_blank",""
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","1","is_blank","[""zschache2025"", ""samsi2024""]","[""https://arxiv.org/abs/2508.14170"", ""https://arxiv.org/abs/2310.03003""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context mentions a U.S. Senator introducing the AI Environmental Impacts Act bill in February 2024 but does not provide the name of the Senator. The only reference to a bill is S.3732 - Artificial Intelligence Environmental Impacts Act of 2024, but it does not specify which Senator introduced it.","is_blank","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/"", ""https://www.hpe.com/psnow/doc/a50005151enw"", ""https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/9""]","is_blank","The context mentions a U.S. Senator introducing the AI Environmental Impacts Act bill in February 2024 but does not provide the name of the Senator. The only reference to a bill is S.3732 - Artificial Intelligence Environmental Impacts Act of 2024, but it does not specify which Senator introduced it."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context does not provide a specific price per hour for an NVIDIA H100 based on Chen et al. (2025).","is_blank","USD per hour","[""chen2024"", ""griggs2024"", ""xia2024"", ""cottier2024""]","is_blank","is_blank","The context does not provide a specific price per hour for an NVIDIA H100 based on Chen et al. (2025)."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context discusses the methodology for estimating the cost of hardware and energy consumption during AI model training, including the depreciation of hardware value over time and the calculation of amortized training costs. However, it does not provide a specific estimate for the average GPU lifetime before retirement in AI data centers in 2024.","is_blank","years","[""cottier2024"", ""amazon2023""]","[""https://www.amazon.com/dp/B08LLSC5R7"", ""https://aws.amazon.com/infrastructure/aws-graviton4/""]","is_blank","The context discusses the methodology for estimating the cost of hardware and energy consumption during AI model training, including the depreciation of hardware value over time and the calculation of amortized training costs. However, it does not provide a specific estimate for the average GPU lifetime before retirement in AI data centers in 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context indicates that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For any meaningful inferences with the 65B LLaMA model, at a minimum, 8 V100 GPUs each with 32 GB of RAM are required.","The context indicates that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","","","tCO2e","[]","is_blank","is_blank",""
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context clearly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019"", ""dodge2022""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV"", ""https://bit.ly/2JTbGnI"", ""https://bit.ly/2QHE5O3""]","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context clearly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides specific U.S. average electricity water withdrawal and consumption intensity factors for electricity generation, which are 43.83 L/kWh and 3.14 L/kWh, respectively. These values are used to estimate the water consumption for electricity generation in the U.S.","43.83","L/kWh","[""li2025b"", ""li2025b"", ""li2025b""]","is_blank","is_blank","The context provides specific U.S. average electricity water withdrawal and consumption intensity factors for electricity generation, which are 43.83 L/kWh and 3.14 L/kWh, respectively. These values are used to estimate the water consumption for electricity generation in the U.S."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","","","PUE","[]","is_blank","is_blank",""
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS achieved a goal of reducing its customers’ workload carbon footprints by up to 96% in North America when the electricity used is matched with 100% renewable energy, a goal that was achieved in 2023.","96","percent","[""amazon2023"", ""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf"", ""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable  energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that AWS achieved a goal of reducing its customers’ workload carbon footprints by up to 96% in North America when the electricity used is matched with 100% renewable energy, a goal that was achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a detailed breakdown of the costs associated with fine-tuning a sparse Mixtral model using 2 million queries on an NVIDIA H100 GPU, specifically mentioning a net cost of $3460.","3460","USD","[""xia2024""]","is_blank","NVIDIA H100 GPU with a cost of $3460 for fine-tuning a sparse Mixtral model using 2M queries.","The context provides a detailed breakdown of the costs associated with fine-tuning a sparse Mixtral model using 2 million queries on an NVIDIA H100 GPU, specifically mentioning a net cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context provides an experiment where a BERT-base model was trained on a single NVIDIA TITAN X GPU, showing that the GPU accounts for almost 74% of the total energy consumption during the training process.","74","percent","[""ebert2024"", ""dodge2022""]","[""https://example.com/document1"", ""https://example.com/document2""]","Our measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity consumption.","The context provides an experiment where a BERT-base model was trained on a single NVIDIA TITAN X GPU, showing that the GPU accounts for almost 74% of the total energy consumption during the training process."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context mentions that JetMoE-8B is trained using 1.25T tokens from various open-source datasets and 30,000 H100 GPU hours, indicating the number of GPU hours used for training.","30000","H100 GPUs","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1"", ""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B is trained with 30,000 H100 GPU hours.","The context mentions that JetMoE-8B is trained using 1.25T tokens from various open-source datasets and 30,000 H100 GPU hours, indicating the number of GPU hours used for training."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","","","Watts","[]","is_blank","is_blank",""
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides information on the carbon emissions associated with training various NLP models, including dense transformers and mixture-of-experts models, and compares these to household and personal water usage equivalents.","131 25","lbs","[""dodge2022"", ""jegham2025"", ""morrison2025""]","[""https://arxiv.org/abs/1906.02243v1"", ""https://patterson2021"", ""https://dodge2022"", ""https://jegham2025"", ""https://morrison2025""]","Total (Ours) 913 312 65 years, 1,921 17 yrs, 1 mo","The context provides information on the carbon emissions associated with training various NLP models, including dense transformers and mixture-of-experts models, and compares these to household and personal water usage equivalents."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context indicates that using A100 GPUs results in anywhere from a 2 to 1.25 times increase in inference latency compared to V100 GPUs for smaller LLaMA models, specifically 7B and 13B.","1.25 to 2","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context indicates that using A100 GPUs results in anywhere from a 2 to 1.25 times increase in inference latency compared to V100 GPUs for smaller LLaMA models, specifically 7B and 13B."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","","","Wh","[]","is_blank","is_blank",""
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context indicates that Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699, about 280% more expensive than InferSave's top choice.","280","percent","[""kim2025""]","is_blank","Max-Performance g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context indicates that Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699, about 280% more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context discusses the energy consumption of various models during inference, with a focus on the efficiency of different GPUs for models generating a single token. It mentions that a V100 or A30 GPU is more efficient for such tasks.","V100 or A30","is_blank","[""zschache2025"", ""luccioni2024""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%F-energy-metrics"", ""https://arxiv.org/abs/2508.14170v1""]","For models generating a single token per inference, a V100 or even A30 GPU is more efficient in inference.","The context discusses the energy consumption of various models during inference, with a focus on the efficiency of different GPUs for models generating a single token. It mentions that a V100 or A30 GPU is more efficient for such tasks."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context discusses the concept of Green AI as AI research that yields results without increasing computational costs, ideally reducing them. It also mentions the importance of reporting the financial cost of developing, training, and running models as a key practice in Green AI.","1","is_blank","[""schwartz2019"", ""luccioni2025b""]","[""https://doi.org/10.1145/3593013.3594010"", ""https://doi.org/10.1145/3593013.3594010""]","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","The context discusses the concept of Green AI as AI research that yields results without increasing computational costs, ideally reducing them. It also mentions the importance of reporting the financial cost of developing, training, and running models as a key practice in Green AI."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context directly states that electronic waste reached 62 million metric tons in 2022, which is the answer to the question about the total amount of electronic waste generated worldwide in that year.","62000000","metric tons","[""luccioni2025a"", ""fernandez2025""]","[""https://arxiv.org/abs/2504.17674v1"", ""https://arxiv.org/abs/2501.12948""]","62 million tonnes in 2022.","The context directly states that electronic waste reached 62 million metric tons in 2022, which is the answer to the question about the total amount of electronic waste generated worldwide in that year."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides specific measurements of system average power per processor for TPU v2 and V100 GPU, but does not directly state the difference between the two.","is_blank","Watts","[""patterson2021""]","is_blank","is_blank","The context provides specific measurements of system average power per processor for TPU v2 and V100 GPU, but does not directly state the difference between the two."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context provides information about the water consumption of training GPT-3 in Microsoft's U.S. data centers, stating it can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","[""arXiv:2304.03271v5"", ""arXiv:2505.09598v6""]","is_blank","The context provides information about the water consumption of training GPT-3 in Microsoft's U.S. data centers, stating it can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context directly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a"", ""wu2021a""]","is_blank","05101520Model Size Time (c) Model Growth TrendDLRM Parameter #s 00.0050.010.0150.020.0250.030.035 051015202530354045 0.1101000 Absolute AUC Improvement BLEU Score Model Size (Billions of Parameters in Log Scale) (a) 1000x Model Size ScalingGPT English2FrenchGPT French2EnglishRecSys SearchRecSys Images x x x 2019-21 x 0.511.522.533.544.5Data & Ingestion BandwidthTime (b) Data Growth TrendDLRM1 DataDLRM2 DataData Ingestion BW 1 1.5 2 2.5 3","The context directly states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","","","is_blank","[]","is_blank","is_blank",""
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context discusses the environmental impact of technology, specifically mentioning the average lifetimes of smartphones and the associated e-waste concerns. It also references efforts to reduce e-waste through sustainable computing practices.","1","is_blank","[""wu2021b, luccioni2024, han2024""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://www.eia.gov/energyexplained/electricity/use-of-electricity.php"", ""https://earth.org/data_visualization/ai-can-it-help-achieve-environmental-sustainable/""]","Socio-Technological Challenges and Opportunities: Paths Forward A PREPRINT infrastructure may be better served by (co-)designing agile applications and data centers, such as [Íñigo Goiri et al., 2015, Radovanovic et al., 2021, Lin et al., 2021, Zhang et al., 2021], that implicitly assume power ﬂuctuations and variability.","The context discusses the environmental impact of technology, specifically mentioning the average lifetimes of smartphones and the associated e-waste concerns. It also references efforts to reduce e-waste through sustainable computing practices."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context discusses the performance of various models when scaled across different zones and continents using T4 GPUs, with a focus on the per-GPU speedup and the impact of geo-distributed settings on throughput.","0","is_blank","[""q261""]","is_blank","FALSE","The context discusses the performance of various models when scaled across different zones and continents using T4 GPUs, with a focus on the per-GPU speedup and the impact of geo-distributed settings on throughput."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context mentions that the FLM-101B model has a context window of 2,048 tokens. This information is directly stated in the text.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context mentions that the FLM-101B model has a context window of 2,048 tokens. This information is directly stated in the text."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context indicates that LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low, leading to the GPU's computation throughput being bottlenecked by VRAM bandwidth and resulting in the GPU's computation units being underutilized, which in turn leads to low power draw.","1","is_blank","[""chung2025""]","is_blank","This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low [37, 58].","The context indicates that LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low, leading to the GPU's computation throughput being bottlenecked by VRAM bandwidth and resulting in the GPU's computation units being underutilized, which in turn leads to low power draw."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context does not provide a specific percentage of women among Amazon's People Managers globally for the year 2023. It mentions percentages related to race/ethnicity and gender but does not directly link these to the People Managers role.","is_blank","percent","[""amazon2023""]","is_blank","is_blank","The context does not provide a specific percentage of women among Amazon's People Managers globally for the year 2023. It mentions percentages related to race/ethnicity and gender but does not directly link these to the People Managers role."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","Excluding equity, the context indicates that the fraction of computing hardware costs rises to 61–76%, while the fraction of R&D staff costs drops to 19–33%. This is derived from the detailed breakdown of costs for the four key models after excluding equity.","61-76,19-33","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","Excluding equity, the context indicates that the fraction of computing hardware costs rises to 61–76%, while the fraction of R&D staff costs drops to 19–33%. This is derived from the detailed breakdown of costs for the four key models after excluding equity."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context indicates that while optimization techniques like quantization and local inference reduce carbon emissions and energy use, they may result in slight declines in accuracy and F1 scores, which is critical for high-precision applications.","1","is_blank","[""khan2025""]","is_blank","and others showing slight declines. For instance, precision and recall generally exhibit minor increases in specific cases, suggesting that optimization can enhance certain aspects of the models’ ability to correctly identify relevant patterns in the data. On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context indicates that while optimization techniques like quantization and local inference reduce carbon emissions and energy use, they may result in slight declines in accuracy and F1 scores, which is critical for high-precision applications."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., which is used to convert power to estimated CO2 emissions.","1438","lbs/kWh","[""strubell2019"", ""patterson2021""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV""]","EPA, 2018, which we use to convert power to estimated CO2 emissions: CO2e = 0 .954pt (2)","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., which is used to convert power to estimated CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context directly states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027.","85 – 134","TWh","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7], whereas a more aggressive estimate by the U.S. data center energy report projects that AI servers’ electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1].","The context directly states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context directly states that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023"", ""amazon2023""]","is_blank","150 million packages via EVs in Europe in 2023","The context directly states that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context provides information on the number of tokens used in both online and offline inference workloads, specifically mentioning 128 input tokens and 512 output tokens for the online inference workload, and 1024 input tokens and 128 output tokens for the offline inference workload.","128, 512, 1024, 128","tokens","[""kim2025, fernandez2025""]","is_blank","is_blank","The context provides information on the number of tokens used in both online and offline inference workloads, specifically mentioning 128 input tokens and 512 output tokens for the online inference workload, and 1024 input tokens and 128 output tokens for the offline inference workload."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications, such as those in oil and gas exploration, and restricts energy consumption disclosures to authorities.","0","is_blank","[""ebert2024"", ""luccioni2025a""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://arxiv.org/abs/2410.06681v2""]","The AI Act contains energy- and climate-related transparency and risk management obligations, primarily for providers of AI systems or models. Under certain conditions (e.g., fine-tuning), however, deployers become providers, which triggers much more onerous duties, also with respect to climate impacts.","The context indicates that the AI Act does not mandate the disclosure of greenhouse gas emissions from AI applications, such as those in oil and gas exploration, and restricts energy consumption disclosures to authorities."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context indicates that for very short experiments like DenseNet 201, significant reductions in CO2 emissions greater than 30% were found in multiple regions, with up to 80% in the West US region when the start time is changed by up to 24 hours.","80","percent","[""dodge2022"", ""dodge2022""]","[""https://link_to_dodge2022_paper"", ""https://link_to_dodge2022_paper""]","For very short experiments (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region.","The context indicates that for very short experiments like DenseNet 201, significant reductions in CO2 emissions greater than 30% were found in multiple regions, with up to 80% in the West US region when the start time is changed by up to 24 hours."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","","","times","[]","is_blank","is_blank",""
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context indicates that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the higher energy costs associated with deploying LLMs.","1","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context indicates that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the higher energy costs associated with deploying LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context mentions that as of January 2024, Amazon signed a corporate PPA with ENGIE to increase its share of output from the Moray West offshore wind farm in Scotland to 473 MW once operational, indicating that there were renewable energy projects announced as of that time.","24","projects","[""amazon2023"", ""amazon2023""]","[""URL1"", ""URL2""]","In January 2024, Amazon signed a corporate PPA with the low-carbon energy and services company ENGIE to increase our share of output from the Moray West offshore wind farm in Scotland to 473 megawatts (MW) once the site becomes operational in 2024.","The context mentions that as of January 2024, Amazon signed a corporate PPA with ENGIE to increase its share of output from the Moray West offshore wind farm in Scotland to 473 MW once operational, indicating that there were renewable energy projects announced as of that time."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","","","percent","[]","is_blank","is_blank",""
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context discusses the importance of reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility, as it captures total computation-related power usage and helps providers optimize AI models for energy efficiency.","Cumulative server level","is_blank","[""ebert2024"", ""li2025b"", ""li2025b""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn"", ""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.","The context discusses the importance of reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility, as it captures total computation-related power usage and helps providers optimize AI models for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides a detailed account of an experiment where a BERT-base model was trained on a single NVIDIA TITAN X GPU, showing that the GPU accounted for 74% of the total electricity consumption.","74","percent","[""dodge2022""]","[""https://arxiv.org/abs/2206.01397""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context provides a detailed account of an experiment where a BERT-base model was trained on a single NVIDIA TITAN X GPU, showing that the GPU accounted for 74% of the total electricity consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","The context mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","","","percent","[]","is_blank","is_blank",""
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context discusses the growth in AI data, model sizes, and system resources, as well as the environmental impact of AI, but does not provide a specific figure for the kilometers of fiber optic cable installed to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""wu2021a""]","is_blank","is_blank","The context discusses the growth in AI data, model sizes, and system resources, as well as the environmental impact of AI, but does not provide a specific figure for the kilometers of fiber optic cable installed to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides an estimate of the hardware acquisition cost to train GPT-4 as $800M, compared to the amortized cost of $40M. This indicates that the upfront hardware acquisition cost is significantly higher than the amortized cost spread over the useful lifetime of the hardware.","8000000000","USD","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/prices.py#L210-L294"", ""https://github.com/epoch-research/training-cost-trends/blob/main/uncertainty.ipynb""]","is_blank","The context provides an estimate of the hardware acquisition cost to train GPT-4 as $800M, compared to the amortized cost of $40M. This indicates that the upfront hardware acquisition cost is significantly higher than the amortized cost spread over the useful lifetime of the hardware."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context indicates that 'Sustainable AI' was proposed to include both the use of AI in climate-positive applications and the improvement of the environmental sustainability of AI approaches themselves, but it does not limit the term solely to climate-positive applications.","is_blank","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that 'Sustainable AI' was proposed to include both the use of AI in climate-positive applications and the improvement of the environmental sustainability of AI approaches themselves, but it does not limit the term solely to climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a model's prediction for maximum batch sizes for fine-tuning Mixtral on GPUs with different memory capacities, specifically stating that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes will be 28 and 35, respectively. This information is supported by the context referring to the ground truth projection and the model's prediction for different GPU capacities.","28, 35","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context provides a model's prediction for maximum batch sizes for fine-tuning Mixtral on GPUs with different memory capacities, specifically stating that for GPU memory capacities of 100GB and 120GB, the maximum batch sizes will be 28 and 35, respectively. This information is supported by the context referring to the ground truth projection and the model's prediction for different GPU capacities."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context indicates that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","1","is_blank","[""chung2025""]","is_blank","Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section). Furthermore, as the server gets more and more overloaded, energy consumption generally increases. This is because with higher overload, more preemptions – and thus more recomputation or data movement – occur. Since preemptions do not directly contribute to the completion of the request, the extra energy consumption from preemptions increases the average energy consumption of completing each request.","The context indicates that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context directly mentions Google's 2024 environmental report but does not provide a specific percentage increase in GHG emissions since 2019.","is_blank","percent","[""jegham2025, jegham2025a, luccioni2025a, jegham2025, jegham2025a""]","[""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://sustainability.google/reports/google-2024-environmental-report/""]","is_blank","The context directly mentions Google's 2024 environmental report but does not provide a specific percentage increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","","","percent","[]","is_blank","is_blank",""
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that doubling the duration of the 6B parameter transformer training run, which originally took 8 days, can lead to significant emissions savings up to about 25%. This is based on the Pause and Resume optimization results for the transformer model.","25","percent","[""dodge2022""]","[""https://doi.org/100.56/2022/6/21\u201324/FAccT/Seoul/Dodge""]","Pause and Resume optimization for 6B parameters Transformer. Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context indicates that doubling the duration of the 6B parameter transformer training run, which originally took 8 days, can lead to significant emissions savings up to about 25%. This is based on the Pause and Resume optimization results for the transformer model."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B by activating only 2B for each input token while having 8B parameters.","70","percent","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B by activating only 2B for each input token while having 8B parameters."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context directly states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e in a seminal 2019 study by Strubell et al.","626,155","lbs CO2e","[""luccioni2025b"", ""zschache2025""]","[""https://dl.acm.org/doi/10.1145/3510003.3510621"", ""https://doi.org/10.1145/3510003.3510221""]","Strubell et al. quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes [192].","The context directly states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e in a seminal 2019 study by Strubell et al."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context directly states that the estimated training energy of GPT-3 is 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","is_blank","The context directly states that the estimated training energy of GPT-3 is 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context discusses the importance of optimizing the Mixture of Experts (MoE) layer in LLM fine-tuning to enhance performance, stating that it consumes the highest fraction of execution time and that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.","1","is_blank","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL]  8 Aug 2024""]","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context discusses the importance of optimizing the Mixture of Experts (MoE) layer in LLM fine-tuning to enhance performance, stating that it consumes the highest fraction of execution time and that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides specific information about the maximum batch sizes supported by fine-tuning Mixtral on different GPUs with varying memory capacities. For a NVIDIA A40 GPU with 48GB memory, the maximum batch sizes are predicted to be 28 and 35 for different GPU memory capacities.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context provides specific information about the maximum batch sizes supported by fine-tuning Mixtral on different GPUs with varying memory capacities. For a NVIDIA A40 GPU with 48GB memory, the maximum batch sizes are predicted to be 28 and 35 for different GPU memory capacities."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","","","is_blank","[]","is_blank","is_blank",""
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","","","hectares","[]","is_blank","is_blank",""
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides specific emissions values for BERT-based models when performing text classification tasks, comparing them to larger multi-purpose models.","0.32","g CO2eq","[""luccioni2024""]","[""ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al""]","is_blank","The context provides specific emissions values for BERT-based models when performing text classification tasks, comparing them to larger multi-purpose models."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","","","grams","[]","is_blank","is_blank",""
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","","","year","[]","is_blank","is_blank",""
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","","","days","[]","is_blank","is_blank",""
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context provides information on the water consumption of AI models, specifically mentioning the training and inference energy consumption of GPT-3 and the water efficiency factors for Microsoft's data centers. However, it does not provide a specific figure for the freshwater consumption of Google's DeepMind AlphaFold servers in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context provides information on the water consumption of AI models, specifically mentioning the training and inference energy consumption of GPT-3 and the water efficiency factors for Microsoft's data centers. However, it does not provide a specific figure for the freshwater consumption of Google's DeepMind AlphaFold servers in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context discusses the benefits of sparse MoE models over dense ones in terms of memory consumption and throughput during fine-tuning, suggesting that adding compute resources to accelerate MoE layers can further reduce costs.","1","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context discusses the benefits of sparse MoE models over dense ones in terms of memory consumption and throughput during fine-tuning, suggesting that adding compute resources to accelerate MoE layers can further reduce costs."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","","","kWh","[]","is_blank","is_blank",""
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","","","USD","[]","is_blank","is_blank",""
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides an estimated cost for fine-tuning Mixtral models on various GPUs with sparse MoE, but does not specify the exact cost for the NVIDIA A40-48GB GPU on the GSM8K dataset.","is_blank","USD","[""xia2024""]","is_blank","is_blank","The context provides an estimated cost for fine-tuning Mixtral models on various GPUs with sparse MoE, but does not specify the exact cost for the NVIDIA A40-48GB GPU on the GSM8K dataset."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","","","samples","[]","is_blank","is_blank",""
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","","","seconds","[]","is_blank","is_blank",""
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","","","is_blank","[]","is_blank","is_blank",""
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context indicates that training accounted for only half of the BLOOM model's overall emissions, suggesting that similar studies focusing only on training might underestimate emissions by half.","50","percent","[""luccioni2025b"", ""morrison2025""]","[""https://link.springer.com/article/10.1038/s41598-023-02158-z"", ""https://arxiv.org/abs/2305.03271""]","finding that training accounted for only half of the model’s overall emissions [121]","The context indicates that training accounted for only half of the BLOOM model's overall emissions, suggesting that similar studies focusing only on training might underestimate emissions by half."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context mentions that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPU","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context mentions that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context discusses the water consumption of training large AI models like GPT-3, but does not provide a specific number of user requests to consume a 500ml bottle of water. It mentions the total water consumption for training but does not correlate it to user requests.","is_blank","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint.","The context discusses the water consumption of training large AI models like GPT-3, but does not provide a specific number of user requests to consume a 500ml bottle of water. It mentions the total water consumption for training but does not correlate it to user requests."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","","","metric tons","[]","is_blank","is_blank",""
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context mentions that JetMoE-8B outperforms the Llama2-7B model and JetMoE-8B-Chat surpasses the Llama2-13B-Chat model on the MT-Bench score, indicating its competitive performance in the open-source LLM landscape.","6.681","score","[""shen2024"", ""shen2024"", ""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat"", ""https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT"", ""https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT""]","JetMoE-8B-chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.","The context mentions that JetMoE-8B outperforms the Llama2-7B model and JetMoE-8B-Chat surpasses the Llama2-13B-Chat model on the MT-Bench score, indicating its competitive performance in the open-source LLM landscape."
