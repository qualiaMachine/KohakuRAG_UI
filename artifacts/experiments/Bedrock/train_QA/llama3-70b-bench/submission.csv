"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly supports the answer as it mentions the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly supports the answer as it mentions the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context does not provide a clear answer to the question about the net CO2e emissions from training the GShard-600B model. The provided information discusses the carbon footprint of various machine learning models, but it does not specifically mention the net CO2e emissions from training the GShard-600B model.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about the net CO2e emissions from training the GShard-600B model. The provided information discusses the carbon footprint of various machine learning models, but it does not specifically mention the net CO2e emissions from training the GShard-600B model."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides a table (Table 3) that lists the model parameters for different LLaMA models, including LLaMA-33B. The table specifies the model size in gigabytes (GB) for LLaMA-33B as 64.7 GB.","64.7","GB","[""[ref_id=chen2024]""]","is_blank","is_blank","The context provides a table (Table 3) that lists the model parameters for different LLaMA models, including LLaMA-33B. The table specifies the model size in gigabytes (GB) for LLaMA-33B as 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context supports the answer as it mentions that hyperscale data centers have a more than 40% higher efficiency compared to traditional data centers, as stated in the ref_id=wu2021b document.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context supports the answer as it mentions that hyperscale data centers have a more than 40% higher efficiency compared to traditional data centers, as stated in the ref_id=wu2021b document."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context provides information about the water consumption of GPT-3, stating that it needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. To find the number of 500ml bottles of water consumed for every medium-length GPT-3 completion, we can use the given range of 10-50 responses per bottle.","is_blank","500 mL bottles","[""is_blank""]","is_blank","is_blank","The context provides information about the water consumption of GPT-3, stating that it needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. To find the number of 500ml bottles of water consumed for every medium-length GPT-3 completion, we can use the given range of 10-50 responses per bottle."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context provides information about the proportion of papers from top AI conferences that target accuracy and efficiency. Specifically, it states that 75% of CVPR papers target accuracy and 20% target efficiency. The difference between these percentages can be calculated to answer the question.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context provides information about the proportion of papers from top AI conferences that target accuracy and efficiency. Specifically, it states that 75% of CVPR papers target accuracy and 20% target efficiency. The difference between these percentages can be calculated to answer the question."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context does not clearly support the answer, as it discusses the AI Act's provisions and proposals for energy consumption reporting, but does not explicitly state that energy consumption data from providers is made publicly available to NGOs, analysts, and the general public. In fact, it mentions that transparency measures are restricted to authorities, limiting broader accountability and public scrutiny.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context does not clearly support the answer, as it discusses the AI Act's provisions and proposals for energy consumption reporting, but does not explicitly state that energy consumption data from providers is made publicly available to NGOs, analysts, and the general public. In fact, it mentions that transparency measures are restricted to authorities, limiting broader accountability and public scrutiny."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides information about the projected maximum batch size of Mixtral for different GPUs, including a projected GPU capacity of 100GB. According to the text, for a GPU memory capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28.","28","samples","[""xia2024""]","is_blank","is_blank","The context provides information about the projected maximum batch size of Mixtral for different GPUs, including a projected GPU capacity of 100GB. According to the text, for a GPU memory capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides information about the performance of LLaMA models on different GPUs, including the A100 and V100. Specifically, it mentions that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 2 times increase in inference latency for the 7B model. This suggests that the A100 provides a significant speedup in inference throughput for the LLaMA-7B model compared to the V100.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides information about the performance of LLaMA models on different GPUs, including the A100 and V100. Specifically, it mentions that the A100 outperforms the V100 on both the Alpaca and GSM8K datasets, with a 2 times increase in inference latency for the 7B model. This suggests that the A100 provides a significant speedup in inference throughput for the LLaMA-7B model compared to the V100."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for different locations, including the U.S. Average. The value for 'Water for Training' in million liters is given as 4.731 for the U.S. Average.","4.731","liters","[""li2025b""]","is_blank","is_blank","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for different locations, including the U.S. Average. The value for 'Water for Training' in million liters is given as 4.731 for the U.S. Average."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context supports the answer because it explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context supports the answer because it explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context provides information about AWS's water use effectiveness (WUE) goal and its progress towards achieving it. Specifically, it mentions that AWS aims to improve its overall WUE by reducing how much incoming water it uses. The WUE for AWS data centers in 2023 is mentioned as 0.18 liters of water per kilowatt-hour (L/kWh).","0.18","L/kWh","[""[ref_id=amazon2023]""]","is_blank","is_blank","The context provides information about AWS's water use effectiveness (WUE) goal and its progress towards achieving it. Specifically, it mentions that AWS aims to improve its overall WUE by reducing how much incoming water it uses. The WUE for AWS data centers in 2023 is mentioned as 0.18 liters of water per kilowatt-hour (L/kWh)."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context supports the answer because it mentions that local inference reduces both network overhead and carbon footprint when deploying large language models. This is stated in the sentence: 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].'","1","is_blank","[""khan2025""]","is_blank","is_blank","The context supports the answer because it mentions that local inference reduces both network overhead and carbon footprint when deploying large language models. This is stated in the sentence: 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].'"
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context supports the answer as it discusses the importance of tracking runtime and computational resources for estimating costs in GPU-based or cloud environments. For instance, the text mentions that 'Authors should report training time and sensitivity to hyperparameters' and provides examples of how tracking these metrics can help reduce costs.","1","is_blank","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","The context supports the answer as it discusses the importance of tracking runtime and computational resources for estimating costs in GPU-based or cloud environments. For instance, the text mentions that 'Authors should report training time and sensitivity to hyperparameters' and provides examples of how tracking these metrics can help reduce costs."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context supports the answer by providing information about the performance improvement of the LLaMA-65B model with automated resource utilization overlapping enabled. According to the text, the model experiences a significant improvement in performance, achieving up to a 13.2% reduction in latency.","13.2","percent","[""chen2024""]","is_blank","is_blank","The context supports the answer by providing information about the performance improvement of the LLaMA-65B model with automated resource utilization overlapping enabled. According to the text, the model experiences a significant improvement in performance, achieving up to a 13.2% reduction in latency."
"q164","How much does an elephant weigh?","The context does not provide information about the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context does not clearly support an answer to the question about which of the five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to the question about which of the five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides the estimated CO₂ emissions from training BERT base and the average American life, allowing us to calculate the equivalent number of days.","6325","days","[""strubell2019"", ""luccioni2025b""]","is_blank","is_blank","The context provides the estimated CO₂ emissions from training BERT base and the average American life, allowing us to calculate the equivalent number of days."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide information about the Transformer architecture eventually outperforming the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about the Transformer architecture eventually outperforming the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not provide information about a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context supports the answer as it mentions that eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, specifically stating 'Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.'","1","is_blank","[""[ref_id=erben2023]""]","is_blank","is_blank","The context supports the answer as it mentions that eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, specifically stating 'Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.'"
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context clearly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, which directly supports the answer to the question.","0","is_blank","[""[ref_id=luccioni2025b]""]","is_blank","is_blank","The context clearly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, which directly supports the answer to the question."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context states that under Germany's 2023 Energy Efficiency Act, data centers must run on 50% renewable energy, increasing to 100% by January 1, 2027, as per Section 11 of the Act.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that under Germany's 2023 Energy Efficiency Act, data centers must run on 50% renewable energy, increasing to 100% by January 1, 2027, as per Section 11 of the Act."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","is_blank","is_blank","papers","[""is_blank""]","is_blank","is_blank","is_blank"
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context supports the answer through multiple references to the percentage of a model's total lifecycle energy use that inference can account for, with estimates ranging from 80% to 90%.","90","percent","[""jegham2025"", ""luccioni2024"", ""chung2025""]","is_blank","is_blank","The context supports the answer through multiple references to the percentage of a model's total lifecycle energy use that inference can account for, with estimates ranging from 80% to 90%."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context does not clearly support the answer, as it discusses the AI Act's requirements and loopholes regarding energy consumption reporting, but does not explicitly state that providers must report both training and inference energy consumption for general-purpose AI models.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support the answer, as it discusses the AI Act's requirements and loopholes regarding energy consumption reporting, but does not explicitly state that providers must report both training and inference energy consumption for general-purpose AI models."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context clearly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which directly answers the question. This information is found in multiple parts of the provided context, reinforcing the conclusion.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which directly answers the question. This information is found in multiple parts of the provided context, reinforcing the conclusion."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context does not support the statement that new AI data centers often rely on air cooling due to high server power densities. In fact, it states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context does not support the statement that new AI data centers often rely on air cooling due to high server power densities. In fact, it states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context supports the answer by providing information about the optimization benefits of platform-level caching for the cross-lingual Transformer language model. Specifically, it states that application-level caching improves power efficiency by 6.7 ×.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context supports the answer by providing information about the optimization benefits of platform-level caching for the cross-lingual Transformer language model. Specifically, it states that application-level caching improves power efficiency by 6.7 ×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table (Table 3) that lists the estimated cost of training a BERT base model using 64 V100 GPUs for 79 hours. The table includes an estimate of CO2 emissions in pounds.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table (Table 3) that lists the estimated cost of training a BERT base model using 64 V100 GPUs for 79 hours. The table includes an estimate of CO2 emissions in pounds."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context provides estimates from various sources, including AWS, Meta, and Google, regarding the percentage of total compute demand accounted for by ML inference. According to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Similarly, Meta and Google attribute significant portions of their ML energy use to inference.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","The context provides estimates from various sources, including AWS, Meta, and Google, regarding the percentage of total compute demand accounted for by ML inference. According to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Similarly, Meta and Google attribute significant portions of their ML energy use to inference."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides the estimated total energy consumption to train a 6.1B-parameter language model to completion, which is approximately 103,593 kWh. We can calculate the equivalent U.S. household-years of electricity consumption by dividing this value by the average annual electricity consumption for a U.S. household.","9.97","household-years","[""dodge2022""]","is_blank","is_blank","The context provides the estimated total energy consumption to train a 6.1B-parameter language model to completion, which is approximately 103,593 kWh. We can calculate the equivalent U.S. household-years of electricity consumption by dividing this value by the average annual electricity consumption for a U.S. household."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context states that for NLP experiments in GC and Azure, egress costs can account for more than 90% of the total cost per VM. This is evident from the statement 'A significant portion of the cost may be hidden in egress costs, accounting for more than 90% of the total cost in our NLP experiments in GC and Azure.'","1","is_blank","[""[ref_id=erben2023]""]","is_blank","is_blank","The context states that for NLP experiments in GC and Azure, egress costs can account for more than 90% of the total cost per VM. This is evident from the statement 'A significant portion of the cost may be hidden in egress costs, accounting for more than 90% of the total cost in our NLP experiments in GC and Azure.'"
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","","","days","[]","is_blank","is_blank",""
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context clearly defines water consumption as 'water withdrawal minus water discharge', which refers to the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context clearly defines water consumption as 'water withdrawal minus water discharge', which refers to the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context supports the answer by providing information about the energy per second for inference with LLaMA 65B across different GPU shard configurations. Specifically, it states that the energy per second is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","is_blank","The context supports the answer by providing information about the energy per second for inference with LLaMA 65B across different GPU shard configurations. Specifically, it states that the energy per second is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""[ref_id=zschache2025]""]","is_blank","is_blank","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The percentage reduction in Qwen's carbon emissions is calculated using the data from TABLE III, showing a reduction from 0.009 kg to 0.004 kg after optimization.","55.56","percent","[""khan2025""]","is_blank","is_blank","The percentage reduction in Qwen's carbon emissions is calculated using the data from TABLE III, showing a reduction from 0.009 kg to 0.004 kg after optimization."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks. This information is found in the abstract and introduction sections of the paper.","40","models","[""chung2025""]","is_blank","is_blank","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks. This information is found in the abstract and introduction sections of the paper."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context provides information about the public health impact of training an AI model of the Llama-3.1 scale in different locations. Specifically, it mentions that the total health cost can vary widely depending on the training data center locations, with an example of $0.23 million in Oregon and $2.5 million in Iowa. This suggests that the health cost of training a Llama-3.1 scale model in Altoona, Iowa would be significant.","2500000","USD","[""han2024""]","is_blank","is_blank","The context provides information about the public health impact of training an AI model of the Llama-3.1 scale in different locations. Specifically, it mentions that the total health cost can vary widely depending on the training data center locations, with an example of $0.23 million in Oregon and $2.5 million in Iowa. This suggests that the health cost of training a Llama-3.1 scale model in Altoona, Iowa would be significant."
