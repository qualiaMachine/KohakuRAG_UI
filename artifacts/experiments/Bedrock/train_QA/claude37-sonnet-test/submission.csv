"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","According to the context, specifically from wu2021b, the total energy consumption of US data centers increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","is_blank","According to the context, specifically from wu2021b, the total energy consumption of US data centers increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the Amazon 2023 Sustainability Report, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","According to the Amazon 2023 Sustainability Report, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","According to the context, in 2023 AWS increased the number of data centers using recycled water for cooling from 20 to 24, which represents an increase of 4 data centers.","4","data centers","[""amazon2023""]","is_blank","is_blank","According to the context, in 2023 AWS increased the number of data centers using recycled water for cooling from 20 to 24, which represents an increase of 4 data centers."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to Morrison et al. 2025, they use an embodied carbon emissions estimate of 463 kg per GPU. This figure comes from assuming the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","According to Morrison et al. 2025, they use an embodied carbon emissions estimate of 463 kg per GPU. This figure comes from assuming the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","According to the provided context, the amortized training cost of GPT-4 was $40M, while the total training budget for FLM-101B was $100,000. This means GPT-4's training cost was 400 times greater than FLM-101B's budget.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","According to the provided context, the amortized training cost of GPT-4 was $40M, while the total training budget for FLM-101B was $100,000. This means GPT-4's training cost was 400 times greater than FLM-101B's budget."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","According to the provided context from Strubell et al. (2019), air travel for one passenger on a round trip from New York to San Francisco produces approximately 1984 pounds of CO2e emissions.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","According to the provided context from Strubell et al. (2019), air travel for one passenger on a round trip from New York to San Francisco produces approximately 1984 pounds of CO2e emissions."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","According to the provided context from document li2025a, when evaluated on the Open LLM Leaderboard, FLM-101B achieved an average performance score of 43.94 across four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA.","43.94","score","[""li2025a""]","is_blank","is_blank","According to the provided context from document li2025a, when evaluated on the Open LLM Leaderboard, FLM-101B achieved an average performance score of 43.94 across four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context clearly states that there was a 6,750 fold improvement in processor clock speed between the Intel 4004 from 1971 (which ran at 740kHz) and typical microprocessors from 2021 (which run at 5,000,000kHz).","6750","fold","[""wu2021b""]","is_blank","is_blank","The context clearly states that there was a 6,750 fold improvement in processor clock speed between the Intel 4004 from 1971 (which ran at 740kHz) and typical microprocessors from 2021 (which run at 5,000,000kHz)."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to the context, GPT-3 was trained for 14.8 days using 10,000 GPUs. This information is explicitly stated in the Luccioni et al. document which mentions 'the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)'.","14.8","days","[""luccioni2023""]","is_blank","is_blank","According to the context, GPT-3 was trained for 14.8 days using 10,000 GPUs. This information is explicitly stated in the Luccioni et al. document which mentions 'the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)'."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","According to Table 3 from Morrison 2025, the GPU Power Usage for the Llama 3.2 1B model at 8 requests per second frequency is explicitly listed as 0.036 kWh when benchmarking on 2400 prompts from ShareGPT.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","According to Table 3 from Morrison 2025, the GPU Power Usage for the Llama 3.2 1B model at 8 requests per second frequency is explicitly listed as 0.036 kWh when benchmarking on 2400 prompts from ShareGPT."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","According to the provided context, the total permitted annual emission limits for diesel generators in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax counties) between January 1, 2023 and December 1, 2024 were approximately 13,000 tons of NOx, as stated in the document.","13000","tons","[""han2024""]","is_blank","is_blank","According to the provided context, the total permitted annual emission limits for diesel generators in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax counties) between January 1, 2023 and December 1, 2024 were approximately 13,000 tons of NOx, as stated in the document."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","According to the context, the total time cost for training FLM-101B using the growth strategy was 21.54 days, which represents a 72% time-saving compared to the estimated 76.74 days required to train a 101B model from scratch.","72","percent","[""li2025a""]","is_blank","is_blank","According to the context, the total time cost for training FLM-101B using the growth strategy was 21.54 days, which represents a 72% time-saving compared to the estimated 76.74 days required to train a 101B model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","According to the context, the growing demand for AI is projected to increase the total annual public health burden of U.S. data centers to more than $20 billion in 2028. Within this, the document specifically mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutants.","1300","deaths","[""han2024""]","is_blank","is_blank","According to the context, the growing demand for AI is projected to increase the total annual public health burden of U.S. data centers to more than $20 billion in 2028. Within this, the document specifically mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutants."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to the Dodge et al. (2022) paper, they trained a 6.1 billion parameter model for only approximately 13% of the time it would take to train to completion. They estimate that a full training run would take 60 days, which equals approximately 8 days divided by 0.13.","60","days","[""dodge2022""]","is_blank","is_blank","According to the Dodge et al. (2022) paper, they trained a 6.1 billion parameter model for only approximately 13% of the time it would take to train to completion. They estimate that a full training run would take 60 days, which equals approximately 8 days divided by 0.13."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","According to the document, the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is explicitly stated in the preface of the report.","2014","year","[""stone2022""]","is_blank","is_blank","According to the document, the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is explicitly stated in the preface of the report."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling.","22","percent","[""luccioni2025a""]","is_blank","is_blank","According to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","According to Table 5 in the Luccioni et al. study, the training energy for the BLOOM model was 51,686 kWh. This refers specifically to the pre-training energy consumption for the BLOOM-7B model (which is the base model for BLOOMz-7B).","51,686","MWh","[""luccioni2024""]","is_blank","is_blank","According to Table 5 in the Luccioni et al. study, the training energy for the BLOOM model was 51,686 kWh. This refers specifically to the pre-training energy consumption for the BLOOM-7B model (which is the base model for BLOOMz-7B)."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","According to the context, the Switch Transformer has 1.5 trillion (1500 billion) parameters but uses a sparse architecture where each token only activates a small subset of parameters. While the exact percentage is not explicitly stated in the context, there is no clear information about what percentage of parameters are activated per token.","is_blank","percent","[""is_blank""]","is_blank","is_blank","According to the context, the Switch Transformer has 1.5 trillion (1500 billion) parameters but uses a sparse architecture where each token only activates a small subset of parameters. While the exact percentage is not explicitly stated in the context, there is no clear information about what percentage of parameters are activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to the context, the JetMoE-8B architecture has 8 experts in each MoE layer. This is specifically mentioned in Table 1 of the paper, which lists the hyperparameters of JetMoE-8B, showing 'Nexperts' as 8.","8","experts","[""shen2024""]","is_blank","is_blank","According to the context, the JetMoE-8B architecture has 8 experts in each MoE layer. This is specifically mentioned in Table 1 of the paper, which lists the hyperparameters of JetMoE-8B, showing 'Nexperts' as 8."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","According to Figure 4 in the context, the execution time breakdown for a dense BlackMamba model with batch size of 30 on a NVIDIA A40-48 GB GPU is approximately 1.5 seconds total, combining the forward, backward, and optimizer stages.","1.5","second","[""xia2024""]","is_blank","is_blank","According to Figure 4 in the context, the execution time breakdown for a dense BlackMamba model with batch size of 30 on a NVIDIA A40-48 GB GPU is approximately 1.5 seconds total, combining the forward, backward, and optimizer stages."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 zettaFLOPs for the Chinese portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 zettaFLOPs for the Chinese portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","According to the experimental setup described in Khan's paper, the hardware processor used for energy-efficient local inference in financial sentiment classification was an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency).","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","is_blank","According to the experimental setup described in Khan's paper, the hardware processor used for energy-efficient local inference in financial sentiment classification was an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency)."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The study by Luccioni, Jernite, and Strubell (2024) titled 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' systematically analyzed various categories of ML systems. According to multiple references to this work across the provided documents, they compared both task-specific and general-purpose models, examining their energy and carbon requirements for inference.","is_blank","models","[""is_blank""]","is_blank","is_blank","The study by Luccioni, Jernite, and Strubell (2024) titled 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' systematically analyzed various categories of ML systems. According to multiple references to this work across the provided documents, they compared both task-specific and general-purpose models, examining their energy and carbon requirements for inference."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the context in document wu2021a, specifically on page 7, it is stated that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This clearly indicates that improving GPU utilization to 80% results in a three-fold decrease in the overall carbon footprint for Language Model (LM) training.","3","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context in document wu2021a, specifically on page 7, it is stated that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This clearly indicates that improving GPU utilization to 80% results in a three-fold decrease in the overall carbon footprint for Language Model (LM) training."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to Cottier et al. (2024), they sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","1.2 to 4","multiplier","[""cottier2024""]","is_blank","is_blank","According to Cottier et al. (2024), they sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","According to multiple references in the context, the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model would be 103.5 MWh. The document explicitly states that while they only trained the model to 13% completion over 8 days, they estimate a full training run would take 60 days and consume approximately (60/8) * 13.8 = 103.5 MWh (or 103,500 kWh).","103.5","MWh","[""dodge2022""]","is_blank","is_blank","According to multiple references in the context, the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model would be 103.5 MWh. The document explicitly states that while they only trained the model to 13% completion over 8 days, they estimate a full training run would take 60 days and consume approximately (60/8) * 13.8 = 103.5 MWh (or 103,500 kWh)."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The 2025 paper by Luccioni, Strubell, and Crawford examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains in technology paradoxically spur increased consumption rather than reducing resource use. The paper specifically discusses how technical efficiency improvements in AI may not lead to net environmental benefits due to these rebound effects.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni, Strubell, and Crawford examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains in technology paradoxically spur increased consumption rather than reducing resource use. The paper specifically discusses how technical efficiency improvements in AI may not lead to net environmental benefits due to these rebound effects."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to the provided context, specifically from document li2025b, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027.","4.2-6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","According to the provided context, specifically from document li2025b, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context clearly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the statement that Red AI is on the decline.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context clearly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the statement that Red AI is on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","According to the provided context, specifically reference li2025a, the total wall-clock time required to train the FLM-101B model using a growth strategy was 21.54 days, which represented a 72% time-saving compared to training a 101B model from scratch (estimated at 76.74 days).","21.54","days","[""li2025a""]","is_blank","is_blank","According to the provided context, specifically reference li2025a, the total wall-clock time required to train the FLM-101B model using a growth strategy was 21.54 days, which represented a 72% time-saving compared to training a 101B model from scratch (estimated at 76.74 days)."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","According to the context, specifically from Wu et al. (2021), a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, not over 80%. This is explicitly stated multiple times in the context, including in a figure caption.","0","is_blank","[""wu2021a""]","is_blank","is_blank","According to the context, specifically from Wu et al. (2021), a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, not over 80%. This is explicitly stated multiple times in the context, including in a figure caption."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","According to multiple sources in the context, GPT-3's training is consistently estimated to have consumed 1287 MWh of electricity. This information is explicitly stated in both li2025b and jegham2025 references.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","According to multiple sources in the context, GPT-3's training is consistently estimated to have consumed 1287 MWh of electricity. This information is explicitly stated in both li2025b and jegham2025 references."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","According to the context, the AI Energy Score is identified as a collaborative project that aims to establish a standardized methodology for comparing the inference efficiency of AI models. This is explicitly stated in document luccioni2025c, which mentions that the AI Energy Score project provides 'a standardized methodology for comparing models across different tasks'.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the context, the AI Energy Score is identified as a collaborative project that aims to establish a standardized methodology for comparing the inference efficiency of AI models. This is explicitly stated in document luccioni2025c, which mentions that the AI Energy Score project provides 'a standardized methodology for comparing models across different tasks'."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","According to the context, Figure 6 shows the kernel-level MoE time breakdown for BlackMamba with different batch sizes. For the dense BlackMamba with batch size 30, the longest kernel in the MoE layer is matmul(w1) with an execution time of approximately 1600 microseconds.","1600","microseconds","[""xia2024""]","is_blank","is_blank","According to the context, Figure 6 shows the kernel-level MoE time breakdown for BlackMamba with different batch sizes. For the dense BlackMamba with batch size 30, the longest kernel in the MoE layer is matmul(w1) with an execution time of approximately 1600 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","According to Table 1 in the paper, JetMoE-8B has a Top-k value of 2, which means 2 experts are selected for activation for each input token. This is also mentioned in the text where it states they 'set the same number of experts to 8 and top-k to 2 for every layer'.","2","experts","[""shen2024""]","is_blank","is_blank","According to Table 1 in the paper, JetMoE-8B has a Top-k value of 2, which means 2 experts are selected for activation for each input token. This is also mentioned in the text where it states they 'set the same number of experts to 8 and top-k to 2 for every layer'."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","According to Schwartz2019, the amount of compute used to train deep learning models has increased by 300,000x from 2012 to 2018, as stated in the abstract and shown in Figure 1. This matches the claim in the question which mentions a 200,000x increase over a six-year span.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","According to Schwartz2019, the amount of compute used to train deep learning models has increased by 300,000x from 2012 to 2018, as stated in the abstract and shown in Figure 1. This matches the claim in the question which mentions a 200,000x increase over a six-year span."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context, specifically from document wu2021b, global carbon emissions in 2020 during the COVID-19 pandemic dropped by 6.4% as mentioned in the reference to Tollefson (2021).","6.4","percent","[""wu2021b""]","is_blank","is_blank","According to the context, specifically from document wu2021b, global carbon emissions in 2020 during the COVID-19 pandemic dropped by 6.4% as mentioned in the reference to Tollefson (2021)."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the 2023 Amazon Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","is_blank","According to the 2023 Amazon Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","Based on the context, the field of Artificial Intelligence was officially born at a 1956 workshop organized by John McCarthy at Dartmouth. By 2025, this would make the field approximately 69 years old.","69","years","[""stone2022""]","is_blank","is_blank","Based on the context, the field of Artificial Intelligence was officially born at a 1956 workshop organized by John McCarthy at Dartmouth. By 2025, this would make the field approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","According to the 2019 study by Strubell et al., the 'five cars' carbon footprint estimate specifically refers to emissions from neural architecture search (NAS), an approach used to identify the best model architecture. The context states that 'NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.'","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the 2019 study by Strubell et al., the 'five cars' carbon footprint estimate specifically refers to emissions from neural architecture search (NAS), an approach used to identify the best model architecture. The context states that 'NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.'"
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context, targeting an average Time Per Output Token (TPOT) of 100 ms for Llama 3.1 8B will land on the Pareto frontier at 77 ms TPOT, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","is_blank","According to the context, targeting an average Time Per Output Token (TPOT) of 100 ms for Llama 3.1 8B will land on the Pareto frontier at 77 ms TPOT, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the context, which shows the maximum batch sizes supported by LLM fine-tuning with different configurations, BlackMamba with a sparse setup (BlackMamba-S) on GSM8K dataset supports a maximum batch size of 8 samples when using an NVIDIA A40 GPU with 48GB memory.","8","samples","[""xia2024""]","is_blank","is_blank","According to Table III in the context, which shows the maximum batch sizes supported by LLM fine-tuning with different configurations, BlackMamba with a sparse setup (BlackMamba-S) on GSM8K dataset supports a maximum batch size of 8 samples when using an NVIDIA A40 GPU with 48GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to multiple references in the documents, Amazon held 1.3 gigawatts (GW) of energy storage capacity in 2023, up from 445 megawatts (MW) in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","According to multiple references in the documents, Amazon held 1.3 gigawatts (GW) of energy storage capacity in 2023, up from 445 megawatts (MW) in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","According to the context, GPT-4o's annual carbon emissions are projected to be comparable to approximately 272 transatlantic flights between Boston and London. This is explicitly stated in section 6.4 of the document, which discusses the estimated 2025 annual carbon footprint of GPT-4o inference.","272","flights","[""jegham2025""]","is_blank","is_blank","According to the context, GPT-4o's annual carbon emissions are projected to be comparable to approximately 272 transatlantic flights between Boston and London. This is explicitly stated in section 6.4 of the document, which discusses the estimated 2025 annual carbon footprint of GPT-4o inference."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","According to multiple sources in the context, the global average power usage effectiveness (PUE) of data centers in 2023 was 1.58. While the context discusses data centers that host AI workloads, it does not specifically provide a separate PUE value for AI-dedicated data centers.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","According to multiple sources in the context, the global average power usage effectiveness (PUE) of data centers in 2023 was 1.58. While the context discusses data centers that host AI workloads, it does not specifically provide a separate PUE value for AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context clearly states that JetMoE-8B has 8 billion total parameters but only activates 2 billion parameters for each input token. This is explicitly mentioned multiple times in the document shen2024.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B has 8 billion total parameters but only activates 2 billion parameters for each input token. This is explicitly mentioned multiple times in the document shen2024."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","According to the provided context, specifically from the document 'luccioni2025c', the GHG emissions associated with pre-training the Llama 7B model are explicitly stated as 14 tCO2e in the table showing the range of pre-training environmental impacts for representative models.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the provided context, specifically from the document 'luccioni2025c', the GHG emissions associated with pre-training the Llama 7B model are explicitly stated as 14 tCO2e in the table showing the range of pre-training environmental impacts for representative models."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","According to the context, Amazon's U.S. fleet included 11,800 electric delivery vans from Rivian in 2023, up from 2,600 in 2022. This represents an increase of 9,200 electric delivery vans in the U.S. alone from 2022 to 2023.","9200","electric delivery vans","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon's U.S. fleet included 11,800 electric delivery vans from Rivian in 2023, up from 2,600 in 2022. This represents an increase of 9,200 electric delivery vans in the U.S. alone from 2022 to 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","According to the context, specifically from ref_id=morrison2025, operational environmental impacts of LLMs explicitly include 'the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","0","is_blank","[""morrison2025""]","is_blank","is_blank","According to the context, specifically from ref_id=morrison2025, operational environmental impacts of LLMs explicitly include 'the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.'"
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in the context, o3 model consumes 12.222 ± 1.082 Wh for long prompts (10k input-1.5k output). This represents the energy consumption of the o3 model for a significant prompt length.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","According to Table 4 in the context, o3 model consumes 12.222 ± 1.082 Wh for long prompts (10k input-1.5k output). This represents the energy consumption of the o3 model for a significant prompt length."
"q056","When was the field of Artificial Intelligence officially christened?","According to the provided context, the field of Artificial Intelligence (AI) was officially born and christened at a workshop held in 1956, which was organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","is_blank","According to the provided context, the field of Artificial Intelligence (AI) was officially born and christened at a workshop held in 1956, which was organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the context discusses water use effectiveness in general and provides WUE values for Microsoft's data centers, it does not contain data specifically about Google's AI-dedicated data centers in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the context discusses water use effectiveness in general and provides WUE values for Microsoft's data centers, it does not contain data specifically about Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","According to the context, the document wu2021b explicitly states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This statement directly confirms the claim in the question.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context, the document wu2021b explicitly states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This statement directly confirms the claim in the question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, at a maximum generation length of 512 tokens, LLaMA-65B consumes about 3-4 Joules per output token. This is explicitly stated in the text where it says 'with length 512, we see that it takes about 3-4 Joules for a output token'.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","According to the context, at a maximum generation length of 512 tokens, LLaMA-65B consumes about 3-4 Joules per output token. This is explicitly stated in the text where it says 'with length 512, we see that it takes about 3-4 Joules for a output token'."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","According to the context from document wu2021a, by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","According to the context from document wu2021a, by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","According to the context, the 5-10% emission reduction claim originates from a 2021 Boston Consulting Group (BCG) report and was repeated in a 2023 BCG report commissioned by Google. However, the reasoning behind this estimate is unclear, the underlying calculations are not detailed beyond BCG's experience with clients, and the application of observations from individual projects to global emissions 'lacks any scientific grounding'.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the context, the 5-10% emission reduction claim originates from a 2021 Boston Consulting Group (BCG) report and was repeated in a 2023 BCG report commissioned by Google. However, the reasoning behind this estimate is unclear, the underlying calculations are not detailed beyond BCG's experience with clients, and the application of observations from individual projects to global emissions 'lacks any scientific grounding'."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","Based on the provided context, while sparsely activated neural networks are mentioned as improving model scalability and achieving higher accuracy at lower operational energy footprint, there is no specific mention that they consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. The documents mention sparse activation reducing computational costs, but do not quantify the reduction as being specifically less than 1/10th.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context, while sparsely activated neural networks are mentioned as improving model scalability and achieving higher accuracy at lower operational energy footprint, there is no specific mention that they consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. The documents mention sparse activation reducing computational costs, but do not quantify the reduction as being specifically less than 1/10th."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","According to the context, AI2's Grover model was trained on 256 TPU chips for two weeks, with an estimated cost of $25,000. This specific cost is directly stated in the Schwartz et al. (2019) document.","25000","USD","[""schwartz2019""]","is_blank","is_blank","According to the context, AI2's Grover model was trained on 256 TPU chips for two weeks, with an estimated cost of $25,000. This specific cost is directly stated in the Schwartz et al. (2019) document."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","According to Figure 4 in the document, the optimizer stage in BlackMamba fine-tuning takes 'up to 53% when conducting sparse fine-tuning with batch size = 1' when using NVIDIA A40-48GB GPU.","53","percent","[""xia2024""]","is_blank","is_blank","According to Figure 4 in the document, the optimizer stage in BlackMamba fine-tuning takes 'up to 53% when conducting sparse fine-tuning with batch size = 1' when using NVIDIA A40-48GB GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","Based on the context, Google Translate is estimated to process 1 billion queries per day. Using the energy consumption of a comparable multi-purpose model (Flan-T5-xxl) at 0.083 kWh per 1,000 queries, we can calculate that 1 billion queries would consume 83,000 kWh or 83 MWh of energy daily.","83","MWh","[""luccioni2024""]","is_blank","is_blank","Based on the context, Google Translate is estimated to process 1 billion queries per day. Using the energy consumption of a comparable multi-purpose model (Flan-T5-xxl) at 0.083 kWh per 1,000 queries, we can calculate that 1 billion queries would consume 83,000 kWh or 83 MWh of energy daily."
"q067","What was the average global data center PUE in 2023?","According to multiple sources in the provided context, specifically from ref_id=ebert2024, the average global data center PUE in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","According to multiple sources in the provided context, specifically from ref_id=ebert2024, the average global data center PUE in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain any specific information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While there is information about Microsoft's data centers, renewable energy usage, and sustainability efforts, there is no explicit mention of wind turbines contracted specifically for Azure AI clusters.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While there is information about Microsoft's data centers, renewable energy usage, and sustainability efforts, there is no explicit mention of wind turbines contracted specifically for Azure AI clusters."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to Cottier et al. (2024), when analyzing the total amortized model development costs for Gemini Ultra, R&D staff costs (including equity) accounted for 49% of the total cost. The authors specifically state that 'Gemini Ultra has the highest fraction of R&D staff cost at 49%' among the frontier models they analyzed.","49","percent","[""cottier2024""]","is_blank","is_blank","According to Cottier et al. (2024), when analyzing the total amortized model development costs for Gemini Ultra, R&D staff costs (including equity) accounted for 49% of the total cost. The authors specifically state that 'Gemini Ultra has the highest fraction of R&D staff cost at 49%' among the frontier models they analyzed."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","According to the context from document stone2022, the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members. This is explicitly stated in multiple places in the document, including the description of the panel's formation in mid-fall 2015.","17","people","[""stone2022""]","is_blank","is_blank","According to the context from document stone2022, the inaugural 2015 Study Panel of the One Hundred Year Study on AI comprised seventeen members. This is explicitly stated in multiple places in the document, including the description of the panel's formation in mid-fall 2015."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the provided context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices. This is explicitly stated in document wu2021a, which discusses the carbon footprint of AI at the edge and client devices.","74","percent","[""wu2021a""]","is_blank","is_blank","According to the provided context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices. This is explicitly stated in document wu2021a, which discusses the carbon footprint of AI at the edge and client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states 'Memory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case.' This is supported by specific examples such as Phi-3 Mini and Small, where the smaller model (Mini) with 3.8B parameters can consume more energy than the larger model (Small) with 7B parameters at higher batch sizes due to differences in attention mechanisms.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context explicitly states 'Memory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case.' This is supported by specific examples such as Phi-3 Mini and Small, where the smaller model (Mini) with 3.8B parameters can consume more energy than the larger model (Small) with 7B parameters at higher batch sizes due to differences in attention mechanisms."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","According to the context from stone2022, the Study Panel specifically states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'","0","is_blank","[""stone2022""]","is_blank","is_blank","According to the context from stone2022, the Study Panel specifically states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'"
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","After thoroughly reviewing all the context snippets, I cannot find any specific information about the exact metric tons of CO2 emitted by OpenAI's API requests in January 2024. While the documents discuss carbon emissions from AI models in general, including some OpenAI models, there is no data specifically about OpenAI's emissions in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","After thoroughly reviewing all the context snippets, I cannot find any specific information about the exact metric tons of CO2 emitted by OpenAI's API requests in January 2024. While the documents discuss carbon emissions from AI models in general, including some OpenAI models, there is no data specifically about OpenAI's emissions in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context, Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e during the pre-training process, which is over 40 times higher than the '5 cars' estimate that is often cited when discussing AI training emissions.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context, Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e during the pre-training process, which is over 40 times higher than the '5 cars' estimate that is often cited when discussing AI training emissions."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","Figure 2(d) in the provided context explicitly states that 'the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years' from Year 1, Quarter 1 to Year 2, Quarter 2.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","Figure 2(d) in the provided context explicitly states that 'the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years' from Year 1, Quarter 1 to Year 2, Quarter 2."
"q079","How many miles is the Earth from the Sun?","The context does not explicitly state how many miles the Earth is from the Sun. The provided context mainly discusses topics related to energy consumption, emissions, and data center electricity usage, but does not contain any astronomical information about Earth's distance from the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not explicitly state how many miles the Earth is from the Sun. The provided context mainly discusses topics related to energy consumption, emissions, and data center electricity usage, but does not contain any astronomical information about Earth's distance from the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","Multiple context snippets clearly state that AlphaGo, a computer program developed by Google DeepMind, defeated the human Go champion. Specifically, the context mentions that AlphaGo 'beat the human Go champion in a five-game match' and 'recently defeated the current human champion at the game of Go'.","1","is_blank","[""stone2022""]","is_blank","is_blank","Multiple context snippets clearly state that AlphaGo, a computer program developed by Google DeepMind, defeated the human Go champion. Specifically, the context mentions that AlphaGo 'beat the human Go champion in a five-game match' and 'recently defeated the current human champion at the game of Go'."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","According to the provided context, continuous batching is the strategy that reduces idle GPU time by dynamically replacing completed requests with new ones. This technique improves GPU utilization and reduces idle time during inference compared to static batching which maintains a fixed batch size throughout inference.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","According to the provided context, continuous batching is the strategy that reduces idle GPU time by dynamically replacing completed requests with new ones. This technique improves GPU utilization and reduces idle time during inference compared to static batching which maintains a fixed batch size throughout inference."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","According to the context, the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, required 60 H100 GPU hours. This information is explicitly stated in the context where it discusses the fine-tuning process using dDPO on the UltraFeedback dataset following the dSFT phase.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","According to the context, the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, required 60 H100 GPU hours. This information is explicitly stated in the context where it discusses the fine-tuning process using dDPO on the UltraFeedback dataset following the dSFT phase."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to Table VI in the provided context, for the 100 TPS SLO constraint in the offline workload experiment, the Max-Performance policy selected the g6e.xlarge instance with a total price of $2.699, while InferSave's first choice was g4dn.xlarge with a total price of $2.13. The percentage difference can be calculated as: ((2.699 - 2.13) / 2.13) × 100 = 26.7%.","26.7","percent","[""kim2025""]","is_blank","is_blank","According to Table VI in the provided context, for the 100 TPS SLO constraint in the offline workload experiment, the Max-Performance policy selected the g6e.xlarge instance with a total price of $2.699, while InferSave's first choice was g4dn.xlarge with a total price of $2.13. The percentage difference can be calculated as: ((2.699 - 2.13) / 2.13) × 100 = 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","According to the context snippet from the Luccioni 2024 paper, the stable-diffusion-xl-base-1.0 model, identified as the most carbon-intensive image generation model in their 2024 study, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","According to the context snippet from the Luccioni 2024 paper, the stable-diffusion-xl-base-1.0 model, identified as the most carbon-intensive image generation model in their 2024 study, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","According to Luccioni 2025c, GPU energy usage for 1,000 inference queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), varying by model size, architecture, and task complexity.","0.06 Wh to 3,426 Wh","Wh","[""luccioni2025c""]","is_blank","is_blank","According to Luccioni 2025c, GPU energy usage for 1,000 inference queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), varying by model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states in multiple places that there is no one-size-fits-all solution for either ethics or sustainability in AI. It emphasizes the need for context-specific approaches that recognize varied ethical dimensions and cultural perspectives beyond Western traditions.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states in multiple places that there is no one-size-fits-all solution for either ethics or sustainability in AI. It emphasizes the need for context-specific approaches that recognize varied ethical dimensions and cultural perspectives beyond Western traditions."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","Based on the context provided, specifically from luccioni2024, the document mentions that the experiments were run in the AWS us-west-2 region, which is based in Oregon and has an average carbon intensity of 297.6 grams of CO2eq per kWh. This value represents the U.S. average mix carbon intensity for 2021.","0.2976","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","Based on the context provided, specifically from luccioni2024, the document mentions that the experiments were run in the AWS us-west-2 region, which is based in Oregon and has an average carbon intensity of 297.6 grams of CO2eq per kWh. This value represents the U.S. average mix carbon intensity for 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","Based on the provided context, Hivemind is the decentralized PyTorch-based framework used to enable distributed spot instance training across clouds and continents. The context describes Hivemind as 'a PyTorch-based framework developed initially to enable collaborative DL training' that 'runs in a decentralized fashion and can handle peers that drop out at any stage of the training,' making it suitable for distributed spot instance training across various cloud environments.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","Based on the provided context, Hivemind is the decentralized PyTorch-based framework used to enable distributed spot instance training across clouds and continents. The context describes Hivemind as 'a PyTorch-based framework developed initially to enable collaborative DL training' that 'runs in a decentralized fashion and can handle peers that drop out at any stage of the training,' making it suitable for distributed spot instance training across various cloud environments."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","According to the provided context, Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems. This concept includes portraying an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the provided context, Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems. This concept includes portraying an AI system's societal impacts, ethical considerations, and eventually its environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","According to the document, the highest accuracy in classification experiments on German public administration texts was achieved by 'a traditional linear model using pre-trained sentence embeddings.' This information is explicitly stated on page 7 of the document where it discusses the results of their experiments on the FKTG dataset.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","According to the document, the highest accuracy in classification experiments on German public administration texts was achieved by 'a traditional linear model using pre-trained sentence embeddings.' This information is explicitly stated on page 7 of the document where it discusses the results of their experiments on the FKTG dataset."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","Based on the context from the Chen et al. 2024 paper, the LLM inference system that uses model-attention disaggregation is named 'Lamina'. Multiple snippets from the paper clearly mention developing and deploying 'Lamina' as the system that incorporates their model-attention disaggregation approach.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","Based on the context from the Chen et al. 2024 paper, the LLM inference system that uses model-attention disaggregation is named 'Lamina'. Multiple snippets from the paper clearly mention developing and deploying 'Lamina' as the system that incorporates their model-attention disaggregation approach."
"q093","How many parameters does the largest T5 model have?","Based on the context snippets, there is no clear information about the parameter count of the largest T5 model. The snippets mention various models like OLMo (up to 13 billion parameters), BERT (350 million), GPT-2-XL (1.5 billion), and a 6 billion parameter transformer, but none specifically identify the parameter count of the largest T5 model.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","Based on the context snippets, there is no clear information about the parameter count of the largest T5 model. The snippets mention various models like OLMo (up to 13 billion parameters), BERT (350 million), GPT-2-XL (1.5 billion), and a 6 billion parameter transformer, but none specifically identify the parameter count of the largest T5 model."
"q094","What is the total number of parameters in the JetMoE-8B model?","According to Table 1 in the document, the JetMoE-8B model has a total parameter count (Ptotal) of 8B parameters, while only 2B parameters are activated (Pactive) for each input token.","8000000000","parameters","[""shen2024""]","is_blank","is_blank","According to Table 1 in the document, the JetMoE-8B model has a total parameter count (Ptotal) of 8B parameters, while only 2B parameters are activated (Pactive) for each input token."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","According to the context, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This is explicitly stated in document li2025b, which notes 'the company's data center water consumption increased by∼20% from 2021 to 2022'.","20","percent","[""li2025b""]","is_blank","is_blank","According to the context, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This is explicitly stated in document li2025b, which notes 'the company's data center water consumption increased by∼20% from 2021 to 2022'."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","According to the provided context, specifically from document 'khan2025', the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' is called Carbon Intensity. This is clearly stated in Table I titled 'COMMON CARBON EMISSION METRICS IN GREEN AI' which lists various carbon emission metrics used in evaluating energy-efficient AI systems.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","According to the provided context, specifically from document 'khan2025', the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' is called Carbon Intensity. This is clearly stated in Table I titled 'COMMON CARBON EMISSION METRICS IN GREEN AI' which lists various carbon emission metrics used in evaluating energy-efficient AI systems."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","According to Table 2 in the context, for the final growth stage of FLM-101B (101 billion parameters), the FLOPs utilization percentage was 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","According to Table 2 in the context, for the final growth stage of FLM-101B (101 billion parameters), the FLOPs utilization percentage was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","According to the Cottier (2024) study, the amortized hardware and energy cost of training OpenAI's GPT-4 model was $40 million. The study distinguishes this from the hardware acquisition cost, which was much higher at $800 million.","40000000","USD","[""cottier2024""]","is_blank","is_blank","According to the Cottier (2024) study, the amortized hardware and energy cost of training OpenAI's GPT-4 model was $40 million. The study distinguishes this from the hardware acquisition cost, which was much higher at $800 million."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to the paper by Wu et al., full-stack optimization including platform-level caching, GPU accelerators, and algorithmic optimizations (like numerical optimizations and faster transformer implementations) can reduce the operational carbon footprint of a Transformer-based universal translation model by 810 times compared to a CPU server baseline.","810","multiplier","[""wu2021a""]","is_blank","is_blank","According to the paper by Wu et al., full-stack optimization including platform-level caching, GPU accelerators, and algorithmic optimizations (like numerical optimizations and faster transformer implementations) can reduce the operational carbon footprint of a Transformer-based universal translation model by 810 times compared to a CPU server baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","According to the context, when comparing experiment C-8 (training spread across four continents) to experiment A-8 (fully local training), the NLP task experienced a 41% performance drop. This means that only 59% of the local throughput was achieved when training was spread across four continents.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","According to the context, when comparing experiment C-8 (training spread across four continents) to experiment A-8 (fully local training), the NLP task experienced a 41% performance drop. This means that only 59% of the local throughput was achieved when training was spread across four continents."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information appears multiple times in the provided documents.","3.5","liters","[""amazon2023""]","is_blank","is_blank","The context clearly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information appears multiple times in the provided documents."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context clearly states that custom tags can reduce the energy consumption of LLMs during the inference phase for source code completion tasks. The study by Rubei et al. specifically shows that using custom tags with zero-shot, one-shot, and few-shots techniques resulted in reduced energy consumption in their experiments with the Llama 3 model.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context clearly states that custom tags can reduce the energy consumption of LLMs during the inference phase for source code completion tasks. The study by Rubei et al. specifically shows that using custom tags with zero-shot, one-shot, and few-shots techniques resulted in reduced energy consumption in their experiments with the Llama 3 model."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","Based on the context, I can only find information about NVIDIA shipping 3.76 million data center GPUs in 2023 as reported in a 2024 paper by Agam Shah. There is also a mention that NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more than in 2023), but this claim is from a paper dated 2025 without specific figures for 2024.","is_blank","GPUs","[""is_blank""]","is_blank","is_blank","Based on the context, I can only find information about NVIDIA shipping 3.76 million data center GPUs in 2023 as reported in a 2024 paper by Agam Shah. There is also a mention that NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more than in 2023), but this claim is from a paper dated 2025 without specific figures for 2024."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the context, when breaking down the components of amortized hardware CapEx + energy, the study found that 'on average, 44% goes toward AI accelerator chips.' This percentage is explicitly stated in the referenced document.","44","percent","[""cottier2024""]","is_blank","is_blank","According to the context, when breaking down the components of amortized hardware CapEx + energy, the study found that 'on average, 44% goes toward AI accelerator chips.' This percentage is explicitly stated in the referenced document."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","According to multiple sources in the context, Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is explicitly stated in the Wu2021a document and confirmed by Wu2021b.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","According to multiple sources in the context, Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is explicitly stated in the Wu2021a document and confirmed by Wu2021b."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","Based on the provided context, the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is explicitly identified as ETAIROS, which stands for 'Ethical AI for the Governance of the Society'.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","Based on the provided context, the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is explicitly identified as ETAIROS, which stands for 'Ethical AI for the Governance of the Society'."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the provided context, specifically from reference cottier2024, Gemini Ultra's estimated amortized training costs were $30 million. The text explicitly states 'We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.'","30000000","USD","[""cottier2024""]","is_blank","is_blank","According to the provided context, specifically from reference cottier2024, Gemini Ultra's estimated amortized training costs were $30 million. The text explicitly states 'We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.'"
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","According to the provided context, the AI Act requires providers of GPAI models with systemic risk and providers of HRAI systems to conduct risk assessments (Articles 55(1)(b) and 9). The authors argue that these risk assessments should include environmental risks, as the purpose of the AI Act (Art. 1(1) and Recital 1) is defined as protecting health, safety, fundamental rights, and environmental protection.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the provided context, the AI Act requires providers of GPAI models with systemic risk and providers of HRAI systems to conduct risk assessments (Articles 55(1)(b) and 9). The authors argue that these risk assessments should include environmental risks, as the purpose of the AI Act (Art. 1(1) and Recital 1) is defined as protecting health, safety, fundamental rights, and environmental protection."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","According to the context, the EPA's recently tightened primary standard for PM2.5 sets an annual average limit of 9 μg/m³. This is mentioned multiple times in the document, including in comparison to the WHO's recommended level of 5 μg/m³.","9","µg/m³","[""han2024""]","is_blank","is_blank","According to the context, the EPA's recently tightened primary standard for PM2.5 sets an annual average limit of 9 μg/m³. This is mentioned multiple times in the document, including in comparison to the WHO's recommended level of 5 μg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","According to the context, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","is_blank","According to the context, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the study by Han et al. (2024), the public health costs of data centers are disproportionately borne by disadvantaged communities. The study explicitly states that 'the per-household health burden could be 200x more than that in less-impacted communities' when discussing the uneven distribution of health impacts.","200","multiplier","[""han2024""]","is_blank","is_blank","According to the study by Han et al. (2024), the public health costs of data centers are disproportionately borne by disadvantaged communities. The study explicitly states that 'the per-household health burden could be 200x more than that in less-impacted communities' when discussing the uneven distribution of health impacts."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","According to Table B1 in the study by Zschache et al. (2025), the energy consumption of the DS Llama 70B model for inference on the FKTG dataset was 702.06 Wh, as shown in the measurements for all models on the FKTG dataset using the Capella system with a single node (average over 10 runs).","702.06","Wh","[""zschache2025""]","is_blank","is_blank","According to Table B1 in the study by Zschache et al. (2025), the energy consumption of the DS Llama 70B model for inference on the FKTG dataset was 702.06 Wh, as shown in the measurements for all models on the FKTG dataset using the Capella system with a single node (average over 10 runs)."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","After reviewing all context snippets, I cannot find any information about a 2022 paper by Dodge et al. that mentions a specific number of parameters in a large language model. The context snippets mention various models and their parameters (like BERT-large with 350 million, GPT-2-XL with 1.5 billion, OLMo models ranging from 20 million to 13 billion), but none specifically attributed to a 2022 paper by Dodge et al.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","After reviewing all context snippets, I cannot find any information about a 2022 paper by Dodge et al. that mentions a specific number of parameters in a large language model. The context snippets mention various models and their parameters (like BERT-large with 350 million, GPT-2-XL with 1.5 billion, OLMo models ranging from 20 million to 13 billion), but none specifically attributed to a 2022 paper by Dodge et al."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons Paradox, which refers to when technological progress improves efficiency but leads to increased resource consumption overall rather than decreased usage. Multiple sources specifically name this phenomenon, with ref_id morrison2025 stating 'when a resource's efficiency increases, overall consumption of that resource tends to increase, rather than decrease.'","Jevons Paradox","is_blank","[""morrison2025"", ""luccioni2025a""]","is_blank","is_blank","The context describes Jevons Paradox, which refers to when technological progress improves efficiency but leads to increased resource consumption overall rather than decreased usage. Multiple sources specifically name this phenomenon, with ref_id morrison2025 stating 'when a resource's efficiency increases, overall consumption of that resource tends to increase, rather than decrease.'"
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","According to the context, training the Meena model is equivalent to 242,231 miles driven by an average passenger vehicle. The training energy of GPT-3 is estimated at 1287 MWh. Although the context doesn't directly state how many Meena training runs would equate to one GPT-3 training run, there's insufficient information to calculate this multiplier accurately.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","According to the context, training the Meena model is equivalent to 242,231 miles driven by an average passenger vehicle. The training energy of GPT-3 is estimated at 1287 MWh. Although the context doesn't directly state how many Meena training runs would equate to one GPT-3 training run, there's insufficient information to calculate this multiplier accurately."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in Luccioni et al.'s 2024 study, image generation has a mean energy consumption of 2.907 kWh per 1,000 inferences. This value is also referred to as 2.9 kWh in the text discussion of the study's findings.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","According to Table 2 in Luccioni et al.'s 2024 study, image generation has a mean energy consumption of 2.907 kWh per 1,000 inferences. This value is also referred to as 2.9 kWh in the text discussion of the study's findings."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","According to the table in the strubell2019 document, the estimated CO2e (carbon dioxide equivalent) emissions for an average American life in one year is 36,156 pounds. This information is presented in a comparative table showing different consumption activities and their associated carbon emissions.","36156","lbs","[""strubell2019""]","is_blank","is_blank","According to the table in the strubell2019 document, the estimated CO2e (carbon dioxide equivalent) emissions for an average American life in one year is 36,156 pounds. This information is presented in a comparative table showing different consumption activities and their associated carbon emissions."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","According to Figure 6c in the provided context, which shows the top 10 counties by per-household health cost from U.S. data centers from 2019 to 2023, Marion County in West Virginia is listed as having the highest per-household health cost at $1218.3 (with a range of $978.0-$1458.5).","is_blank","is_blank","[""han2024""]","is_blank","is_blank","According to Figure 6c in the provided context, which shows the top 10 counties by per-household health cost from U.S. data centers from 2019 to 2023, Marion County in West Virginia is listed as having the highest per-household health cost at $1218.3 (with a range of $978.0-$1458.5)."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","According to Table III in Khan et al., Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization techniques such as quantization and local inference were applied for the financial sentiment classification task.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","According to Table III in Khan et al., Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization techniques such as quantization and local inference were applied for the financial sentiment classification task."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to Table 5 in the Luccioni et al. paper, the combined training and fine-tuning energy costs for BLOOMz-7B model were 51,686 kWh for training plus 7,571 kWh for fine-tuning, which totals 59,257 kWh.","59257","kWh","[""luccioni2024""]","is_blank","is_blank","According to Table 5 in the Luccioni et al. paper, the combined training and fine-tuning energy costs for BLOOMz-7B model were 51,686 kWh for training plus 7,571 kWh for fine-tuning, which totals 59,257 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","According to the context, the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, 80 attention heads, and a vocabulary size of 100,256, which confirms it has 101 billion parameters.","101","parameters","[""li2025a""]","is_blank","is_blank","According to the context, the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, 80 attention heads, and a vocabulary size of 100,256, which confirms it has 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","According to the context, a 6.1 billion parameter transformer model would consume approximately 103,500 kWh for a full training run. For BLOOMz-7B, the inference energy per query is 1.0 × 10^-4 kWh, and it would take 592,570,000 inferences to match its training energy costs.","592570000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","According to the context, a 6.1 billion parameter transformer model would consume approximately 103,500 kWh for a full training run. For BLOOMz-7B, the inference energy per query is 1.0 × 10^-4 kWh, and it would take 592,570,000 inferences to match its training energy costs."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","According to the 'Power Hungry Processing' study by Luccioni et al. (2024), the researchers explicitly state that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","According to the 'Power Hungry Processing' study by Luccioni et al. (2024), the researchers explicitly state that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","According to Table 5 in the Luccioni et al. (2024) paper, the BLOOMz-7B model requires 592,570,000 inferences for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. This figure is calculated by comparing the energy used per inference (1.0 × 10^-4 kWh) with the total energy used for both training (51,686 kWh) and fine-tuning (7,571 kWh).","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","According to Table 5 in the Luccioni et al. (2024) paper, the BLOOMz-7B model requires 592,570,000 inferences for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning. This figure is calculated by comparing the energy used per inference (1.0 × 10^-4 kWh) with the total energy used for both training (51,686 kWh) and fine-tuning (7,571 kWh)."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","According to the context, the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset, which was scraped from the website https://beteiligung.bge.de/index.php. This dataset contains statements from the population that were categorized during the process of selecting a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","According to the context, the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset, which was scraped from the website https://beteiligung.bge.de/index.php. This dataset contains statements from the population that were categorized during the process of selecting a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024. While there is information about the training energy consumption of Llama-3.1 (approximately 30 GWh) and water consumption estimates for GPT-3, there are no specific data about water consumption for Meta's Llama 3 inference clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024. While there is information about the training energy consumption of Llama-3.1 (approximately 30 GWh) and water consumption estimates for GPT-3, there are no specific data about water consumption for Meta's Llama 3 inference clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","According to Strubell et al. 2019, the Evolved Transformer NAS produced 626,155 lbs of CO2e (equivalent to 284,019 kg or 284 tCO2e as noted by Luccioni 2023). The same source states that air travel for 1 passenger from NY to SF produces 1984 lbs of CO2e. Therefore, 3.2 tCO2e would be equivalent to approximately 3.2 passengers taking this round trip.","3.2","passengers","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","According to Strubell et al. 2019, the Evolved Transformer NAS produced 626,155 lbs of CO2e (equivalent to 284,019 kg or 284 tCO2e as noted by Luccioni 2023). The same source states that air travel for 1 passenger from NY to SF produces 1984 lbs of CO2e. Therefore, 3.2 tCO2e would be equivalent to approximately 3.2 passengers taking this round trip."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 data from OpenRouter, as shown in Figure 2, 84% of LLM token usage occurred through models with no disclosure of their environmental impact, while 14% was through models with indirect disclosure and only 2% through models with direct disclosure.","84","percent","[""luccioni2025c""]","is_blank","is_blank","According to May 2025 data from OpenRouter, as shown in Figure 2, 84% of LLM token usage occurred through models with no disclosure of their environmental impact, while 14% was through models with indirect disclosure and only 2% through models with direct disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to the context, specifically from Table II in the Samsi et al. paper, the bare minimum hardware requirement for running LLaMA-13B inference without compression or quantization on A100 80GB GPUs is 1 GPU.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the context, specifically from Table II in the Samsi et al. paper, the bare minimum hardware requirement for running LLaMA-13B inference without compression or quantization on A100 80GB GPUs is 1 GPU."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context from the document 'dodge2022' explicitly states that for a 6.1 billion parameter transformer model, a full training run would consume approximately 103,593 kWh. Converting kWh to carbon emissions would require information about the carbon intensity of the electricity used, which is not provided in the given context snippets.","is_blank","metric tons","[""dodge2022""]","is_blank","is_blank","The context from the document 'dodge2022' explicitly states that for a 6.1 billion parameter transformer model, a full training run would consume approximately 103,593 kWh. Converting kWh to carbon emissions would require information about the carbon intensity of the electricity used, which is not provided in the given context snippets."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","While the provided context discusses the environmental impact of LLMs and mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization (khan2025), it does not provide a specific total figure for carbon emissions avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","While the provided context discusses the environmental impact of LLMs and mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization (khan2025), it does not provide a specific total figure for carbon emissions avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","According to the context, in a specific scenario described in Figure 9, using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and 31% over an A10G-only strategy.","24","percent","[""griggs2024""]","is_blank","is_blank","According to the context, in a specific scenario described in Figure 9, using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy and 31% over an A10G-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This information is clearly presented in their table comparing specifications of H100, H20, and TPU v6e, with an asterisk noting that this price is estimated using relative complete system cost against H100 as H20 is not readily available on cloud service providers.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This information is clearly presented in their table comparing specifications of H100, H20, and TPU v6e, with an asterisk noting that this price is estimated using relative complete system cost against H100 as H20 is not readily available on cloud service providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context clearly states 'In fact, most carbon footprint analyses gather the information manually by writing to authors.' This indicates that carbon footprint analyses for AI models typically require manual contact with authors rather than automatic information gathering.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context clearly states 'In fact, most carbon footprint analyses gather the information manually by writing to authors.' This indicates that carbon footprint analyses for AI models typically require manual contact with authors rather than automatic information gathering."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the context, in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as stated directly in the document: 'This is equivalent to approximately 44% of the data centers' total electricity cost.'","44","percent","[""han2024""]","is_blank","is_blank","According to the context, in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as stated directly in the document: 'This is equivalent to approximately 44% of the data centers' total electricity cost.'"
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the context, specifically from document samsi2024, the bare minimum hardware requirements for different LLaMA models are listed in Table II, which shows that for LLaMA-7B without compression or quantization, 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the context, specifically from document samsi2024, the bare minimum hardware requirements for different LLaMA models are listed in Table II, which shows that for LLaMA-7B without compression or quantization, 1 A100 80GB GPU is required."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","According to the provided context from Khan 2025, the study's experimental results demonstrated that the proposed strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the provided context from Khan 2025, the study's experimental results demonstrated that the proposed strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to multiple sources, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers. This represents an author response rate of 15.4%.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","According to multiple sources, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers. This represents an author response rate of 15.4%."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","According to the context, JetMoE-8B was trained with a limited $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours: $100,000 ÷ 30,000 = $3.33 per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained with a limited $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours: $100,000 ÷ 30,000 = $3.33 per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","According to Table 2 in the context, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost. This is specifically stated in the row for Altoona, IA, which shows the health cost as a percentage of electricity cost.","122","percent","[""han2024""]","is_blank","is_blank","According to Table 2 in the context, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost. This is specifically stated in the row for Altoona, IA, which shows the health cost as a percentage of electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens of primarily English data from web documents, mathematics, and code. This is explicitly stated in section 4.3 Training Data Mixture.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens of primarily English data from web documents, mathematics, and code. This is explicitly stated in section 4.3 Training Data Mixture."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the table in the context showing 'Amazon Renewable Energy Projects* Projects announced as of January 2024', there were 36 projects announced in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","is_blank","According to the table in the context showing 'Amazon Renewable Energy Projects* Projects announced as of January 2024', there were 36 projects announced in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","According to Amazon's 2023 Sustainability Report, specifically in the 'Amazon Representation by the Numbers' table, in 2023, 53.2% of Amazon's workforce (all levels) in the United States identified as men.","53.2","percent","[""amazon2023""]","is_blank","is_blank","According to Amazon's 2023 Sustainability Report, specifically in the 'Amazon Representation by the Numbers' table, in 2023, 53.2% of Amazon's workforce (all levels) in the United States identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context clearly states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The context clearly states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","According to Figure 4 in the provided context, the total execution time for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84 is about 2.0 seconds. The figure shows the execution time breakdown for the BlackMamba model with sparse fine-tuning at batch size=84.","2.0","seconds","[""xia2024""]","is_blank","is_blank","According to Figure 4 in the provided context, the total execution time for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84 is about 2.0 seconds. The figure shows the execution time breakdown for the BlackMamba model with sparse fine-tuning at batch size=84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","Based on the context, the authors introduce the granularity metric as the ratio of calculation to communication time when scaling distributed training. This metric is explicitly defined in the context as allowing comparison of model suitability for distributed spot training and for predicting performance with different hardware setups, including across continents.","Granularity","is_blank","[""erben2023""]","is_blank","is_blank","Based on the context, the authors introduce the granularity metric as the ratio of calculation to communication time when scaling distributed training. This metric is explicitly defined in the context as allowing comparison of model suitability for distributed spot training and for predicting performance with different hardware setups, including across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to multiple context snippets, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil and gas production in Texas and New Mexico by 50,000 barrels per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","According to multiple context snippets, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil and gas production in Texas and New Mexico by 50,000 barrels per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","Based on the provided context, 'water withdrawal' is the term for freshwater taken from ground or surface water sources, either temporarily or permanently, for various uses including agricultural, industrial, or municipal purposes.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","Based on the provided context, 'water withdrawal' is the term for freshwater taken from ground or surface water sources, either temporarily or permanently, for various uses including agricultural, industrial, or municipal purposes."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","According to the document, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI, as explicitly stated in the context.","5","years","[""stone2022""]","is_blank","is_blank","According to the document, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI, as explicitly stated in the context."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","According to the Deloitte 2021 report cited multiple times in the context, the average U.S. household was equipped with 25 connected devices including cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","is_blank","According to the Deloitte 2021 report cited multiple times in the context, the average U.S. household was equipped with 25 connected devices including cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","Based on the provided context, the energy consumption required to pre-train a large language model (LLM) ranges from as little as 0.8 MWh for smaller models like OLMo 20M to 3,500 MWh for larger models like LLaMa 4 Scout. Other examples include 356 MWh for Llama 7B, 520 MWh for BLOOM, and an estimated 103.5 MWh for a 6 billion parameter transformer model.","0.8 to 3500","MWh","[""luccioni2025c"", ""dodge2022""]","is_blank","is_blank","Based on the provided context, the energy consumption required to pre-train a large language model (LLM) ranges from as little as 0.8 MWh for smaller models like OLMo 20M to 3,500 MWh for larger models like LLaMa 4 Scout. Other examples include 356 MWh for Llama 7B, 520 MWh for BLOOM, and an estimated 103.5 MWh for a 6 billion parameter transformer model."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","According to the context in ref_id stone2022, IBM's Watson program did beat human contenders in the Jeopardy challenge in 2011. This is explicitly stated in multiple places, such as: 'IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.'","0","is_blank","[""stone2022""]","is_blank","is_blank","According to the context in ref_id stone2022, IBM's Watson program did beat human contenders in the Jeopardy challenge in 2011. This is explicitly stated in multiple places, such as: 'IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.'"
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context, one study suggests that 10-50 queries on GPT-3 consumes around half a liter of water. This information is specifically mentioned in reference luccioni2025a.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","According to the context, one study suggests that 10-50 queries on GPT-3 consumes around half a liter of water. This information is specifically mentioned in reference luccioni2025a."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to the context, JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which is higher than Llama-2-13b-Chat's score of 6.650. This is explicitly stated in Table 4 and reinforced in the text.","6.681","score","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which is higher than Llama-2-13b-Chat's score of 6.650. This is explicitly stated in Table 4 and reinforced in the text."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to reference li2025b, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10-50 medium-length responses. This directly answers the question about how many medium-length GPT-3 completions could be produced with the water required to fill a single 500 mL bottle.","10-50","responses","[""li2025b""]","is_blank","is_blank","According to reference li2025b, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10-50 medium-length responses. This directly answers the question about how many medium-length GPT-3 completions could be produced with the water required to fill a single 500 mL bottle."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","According to the 2024 Griggs et al. paper on Mélange, the framework reduces deployment costs by up to 77% in conversational chat settings (referred to as 'short-context' or 'Arena' dataset in the paper). This is specifically mentioned multiple times in the provided context.","77","percent","[""griggs2024""]","is_blank","is_blank","According to the 2024 Griggs et al. paper on Mélange, the framework reduces deployment costs by up to 77% in conversational chat settings (referred to as 'short-context' or 'Arena' dataset in the paper). This is specifically mentioned multiple times in the provided context."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to the context, specifically from document samsi2024, a minimum of 4 A100 GPUs each with 80GB of memory are required for meaningful inferences with the 65B LLaMA model without compression or optimization.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","According to the context, specifically from document samsi2024, a minimum of 4 A100 GPUs each with 80GB of memory are required for meaningful inferences with the 65B LLaMA model without compression or optimization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","According to the research by Han et al., training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This figure is mentioned multiple times throughout the context.","10000","round trips","[""han2024""]","is_blank","is_blank","According to the research by Han et al., training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This figure is mentioned multiple times throughout the context."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to multiple sources in the context, AWS, the largest global cloud provider, estimated in 2019 that inference processing made up 80 to 90% of total ML cloud computing demand. This figure was specifically attributed to 2019 in the Fernandez2025 reference, which cites 'Barr, 2019' as the source.","80-90","percent","[""fernandez2025"", ""chung2025""]","is_blank","is_blank","According to multiple sources in the context, AWS, the largest global cloud provider, estimated in 2019 that inference processing made up 80 to 90% of total ML cloud computing demand. This figure was specifically attributed to 2019 in the Fernandez2025 reference, which cites 'Barr, 2019' as the source."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study explicitly states the total emissions for all model experimentation and evaluation: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.' This information appears in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the paper.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The study explicitly states the total emissions for all model experimentation and evaluation: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.' This information appears in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the paper."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","According to the context, estimating GPU energy consumption based on TDP is not reliable because it 'nearly always' leads to 'overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.' The context shows that TDP-based estimates can lead to overestimations by factors ranging from 1.0x to 4.1x compared to actual measured energy consumption.","0","is_blank","[""chung2025""]","is_blank","is_blank","According to the context, estimating GPU energy consumption based on TDP is not reliable because it 'nearly always' leads to 'overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.' The context shows that TDP-based estimates can lead to overestimations by factors ranging from 1.0x to 4.1x compared to actual measured energy consumption."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","According to multiple context snippets from the document, GPT-4o mini actually consumes more energy per query than GPT-4o, despite having a smaller architecture. Specifically, it states that 'GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes' and gives the specific consumption values as 'GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.'","0","is_blank","[""jegham2025""]","is_blank","is_blank","According to multiple context snippets from the document, GPT-4o mini actually consumes more energy per query than GPT-4o, despite having a smaller architecture. Specifically, it states that 'GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes' and gives the specific consumption values as 'GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.'"
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","According to Figure 8 in the provided context, which shows query throughput of different models including Mixtral, the dense Mixtral-CS with batch size 1 has a throughput of 0.3 queries per second. This is clearly indicated in the figure that presents throughput measurements for different configurations.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","According to Figure 8 in the provided context, which shows query throughput of different models including Mixtral, the dense Mixtral-CS with batch size 1 has a throughput of 0.3 queries per second. This is clearly indicated in the figure that presents throughput measurements for different configurations."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","According to the 2025 paper by Luccioni, there was a peak in direct environmental information disclosure for notable AI models in 2022 at 10%, but after 2022 there was 'a notable reversal in this trend, dramatically reducing direct environmental disclosures' due to increasingly commercial and proprietary models. By the first quarter of 2025, most AI models fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the 2025 paper by Luccioni, there was a peak in direct environmental information disclosure for notable AI models in 2022 at 10%, but after 2022 there was 'a notable reversal in this trend, dramatically reducing direct environmental disclosures' due to increasingly commercial and proprietary models. By the first quarter of 2025, most AI models fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to Table 1 in Griggs et al. (2024), the on-demand price for an H100 (SXM) GPU is listed as $7.5164 per hour. This information is provided in the context of comparing different GPU types for cost efficiency in LLM serving.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","According to Table 1 in Griggs et al. (2024), the on-demand price for an H100 (SXM) GPU is listed as $7.5164 per hour. This information is provided in the context of comparing different GPU types for cost efficiency in LLM serving."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","Based on the context, Li et al. estimated GPT-3's water consumption for training across different Microsoft data center locations. For the U.S. average, the total water consumption for training is estimated at 5.439 million liters, with variations depending on location.","5.439","liters of water","[""li2025b""]","is_blank","is_blank","Based on the context, Li et al. estimated GPT-3's water consumption for training across different Microsoft data center locations. For the U.S. average, the total water consumption for training is estimated at 5.439 million liters, with variations depending on location."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To calculate the hourly cost, I need to divide this monthly cost by the number of hours in 30 days.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To calculate the hourly cost, I need to divide this monthly cost by the number of hours in 30 days."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the provided context, specifically from document wu2021a, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","According to the provided context, specifically from document wu2021a, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to Strubell et al., training a Transformer model with Neural Architecture Search (NAS) produces 626,155 lbs of CO2. This figure represents the carbon emissions from the training and neural architecture search process for a Transformer model, which is equivalent to driving a car for a significant distance.","626155","miles","[""strubell2019""]","is_blank","is_blank","According to Strubell et al., training a Transformer model with Neural Architecture Search (NAS) produces 626,155 lbs of CO2. This figure represents the carbon emissions from the training and neural architecture search process for a Transformer model, which is equivalent to driving a car for a significant distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","According to the provided context, BLOOMz-7B was downloaded 606,096 times as of November 2023 and has an inference energy of 1.0 × 10^-4 kWh per inference. To calculate the total energy consumption if each download resulted in 1 million inferences: 606,096 downloads × 1,000,000 inferences per download × (1.0 × 10^-4 kWh per inference) = 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","is_blank","According to the provided context, BLOOMz-7B was downloaded 606,096 times as of November 2023 and has an inference energy of 1.0 × 10^-4 kWh per inference. To calculate the total energy consumption if each download resulted in 1 million inferences: 606,096 downloads × 1,000,000 inferences per download × (1.0 × 10^-4 kWh per inference) = 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","According to multiple instances in the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned across different sections of the document, specifically noting that the model was trained with 'a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.'","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","According to multiple instances in the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned across different sections of the document, specifically noting that the model was trained with 'a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.'"
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","According to the research paper by Cottier et al. (2024), the trend of growing development costs for training frontier AI models indicates that the largest training runs will cost more than one billion dollars by the start of 2027. This is based on their analysis showing that training costs for the most compute-intensive models have been growing at a rate of 2.4× per year since 2016.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","According to the research paper by Cottier et al. (2024), the trend of growing development costs for training frontier AI models indicates that the largest training runs will cost more than one billion dollars by the start of 2027. This is based on their analysis showing that training costs for the most compute-intensive models have been growing at a rate of 2.4× per year since 2016."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","According to the context, specifically from ref_id=cottier2024, training GPT-3 required 4.5e22 FLOP (floating point operations), which is calculated using the formula: compute = 6 × parameters × tokens.","4.5e22","FLOPS","[""cottier2024""]","is_blank","is_blank","According to the context, specifically from ref_id=cottier2024, training GPT-3 required 4.5e22 FLOP (floating point operations), which is calculated using the formula: compute = 6 × parameters × tokens."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to the context, specifically from document samsi2024, 'at a minimum, 8 V100 GPUs each with 32 GB of RAM...are required for any meaningful inferences with the 65B LLaMA model.' This information is consistently stated multiple times throughout the provided contexts.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","According to the context, specifically from document samsi2024, 'at a minimum, 8 V100 GPUs each with 32 GB of RAM...are required for any meaningful inferences with the 65B LLaMA model.' This information is consistently stated multiple times throughout the provided contexts."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","According to Li2025a, the 101B parameter model used 165 teraFLOPs/s per GPU across 192 GPUs, and was trained with 26.54B tokens. To calculate the total computation: 165 teraFLOPs/s × 192 GPUs = 31.68 petaFLOPs/s. Multiplying this by the training time needed to process 26.54B tokens gives us the total computational work in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","According to Li2025a, the 101B parameter model used 165 teraFLOPs/s per GPU across 192 GPUs, and was trained with 26.54B tokens. To calculate the total computation: 165 teraFLOPs/s × 192 GPUs = 31.68 petaFLOPs/s. Multiplying this by the training time needed to process 26.54B tokens gives us the total computational work in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","Based on the provided context snippets, there is no information about the top-1 accuracy on ImageNet for AlexNet 2012. While there are references to various models and their ImageNet accuracies (such as ResNet-50 with 69.3% or 76.1% top-1 accuracy), none specifically mention AlexNet 2012's performance on ImageNet.","is_blank","percent","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, there is no information about the top-1 accuracy on ImageNet for AlexNet 2012. While there are references to various models and their ImageNet accuracies (such as ResNet-50 with 69.3% or 76.1% top-1 accuracy), none specifically mention AlexNet 2012's performance on ImageNet."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs (80GB each). This means there were 24 × 8 = 192 A800 GPUs used in total for training the model.","192","GPUs","[""li2025a""]","is_blank","is_blank","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs (80GB each). This means there were 24 × 8 = 192 A800 GPUs used in total for training the model."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","According to the Strubell et al. 2019 study, training a Transformer model with neural architecture search (NAS) for machine translation produced 626,155 pounds (284 metric tons) of CO2 emissions, which is equivalent to about 5 times the emissions of a car during its lifetime.","5","lifetimes","[""strubell2019"", ""luccioni2025c""]","is_blank","is_blank","According to the Strubell et al. 2019 study, training a Transformer model with neural architecture search (NAS) for machine translation produced 626,155 pounds (284 metric tons) of CO2 emissions, which is equivalent to about 5 times the emissions of a car during its lifetime."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","According to the context, FAIR's RoBERTa was trained on 160GB of text (roughly 40 billion word-pieces) and required around 25,000 GPU hours to train. This information is explicitly stated in the context from the Schwartz2019 document.","25000","hours","[""schwartz2019""]","is_blank","is_blank","According to the context, FAIR's RoBERTa was trained on 160GB of text (roughly 40 billion word-pieces) and required around 25,000 GPU hours to train. This information is explicitly stated in the context from the Schwartz2019 document."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the 2023 Amazon Sustainability Report, Amazon's on-site solar energy systems avoid roughly 47,400-47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This is clearly stated on page 26 of the report where they discuss their on-site solar energy projects.","47500","metric tons","[""amazon2023""]","is_blank","is_blank","According to the 2023 Amazon Sustainability Report, Amazon's on-site solar energy systems avoid roughly 47,400-47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. This is clearly stated on page 26 of the report where they discuss their on-site solar energy projects."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","According to the context, Megatron-LM was used as the framework to deploy large language models across multiple GPUs and nodes. This is mentioned explicitly in multiple references such as Narayanan et al.'s work on 'Efficient large-scale language model training on GPU clusters using megatron-lm'.","Megatron-LM","is_blank","[""shen2024"", ""li2025a"", ""fernandez2025""]","is_blank","is_blank","According to the context, Megatron-LM was used as the framework to deploy large language models across multiple GPUs and nodes. This is mentioned explicitly in multiple references such as Narayanan et al.'s work on 'Efficient large-scale language model training on GPU clusters using megatron-lm'."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in reference zschache2025, the Llama 3.1 70B model showed an energy consumption of 48.60 Wh when deployed on a single node versus 94.88 Wh when deployed on two nodes, representing an increase by a factor of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","According to Table B2 in reference zschache2025, the Llama 3.1 70B model showed an energy consumption of 48.60 Wh when deployed on a single node versus 94.88 Wh when deployed on two nodes, representing an increase by a factor of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide a specific measurement of gallons of water consumed per ChatGPT user session in 2023. While there are estimates of water consumption for GPT-3 (ranging from approximately 7-48 milliliters per request depending on location) and mentions that 10-50 queries on GPT-3 consumes around half a liter of water, there is no direct data about ChatGPT's per-session water consumption in 2023 specifically.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide a specific measurement of gallons of water consumed per ChatGPT user session in 2023. While there are estimates of water consumption for GPT-3 (ranging from approximately 7-48 milliliters per request depending on location) and mentions that 10-50 queries on GPT-3 consumes around half a liter of water, there is no direct data about ChatGPT's per-session water consumption in 2023 specifically."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the provided context, GPT-4o's annual energy consumption from 700 million daily queries is estimated to be between 391,509 MWh and 463,269 MWh annually. This level of electricity consumption exceeds that of 35,000 U.S. residential households (377,685 MWh).","35000","homes","[""jegham2025""]","is_blank","is_blank","According to the provided context, GPT-4o's annual energy consumption from 700 million daily queries is estimated to be between 391,509 MWh and 463,269 MWh annually. This level of electricity consumption exceeds that of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the 2025 paper by Luccioni et al., Microsoft reported a 34% increase in global water consumption between 2021 and 2022, with their total consumption exceeding 1.7 billion gallons.","34","percent","[""luccioni2025a""]","is_blank","is_blank","According to the 2025 paper by Luccioni et al., Microsoft reported a 34% increase in global water consumption between 2021 and 2022, with their total consumption exceeding 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in Yelp sentiment analysis benchmarks, 'traditional models perform considerably worse than LLMs,' which directly contradicts the statement. The document specifically mentions that for the Yelp dataset, the performance gap between traditional models and LLMs justifies the higher energy costs of LLM deployment.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that in Yelp sentiment analysis benchmarks, 'traditional models perform considerably worse than LLMs,' which directly contradicts the statement. The document specifically mentions that for the Yelp dataset, the performance gap between traditional models and LLMs justifies the higher energy costs of LLM deployment."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","According to the provided context, when the Evolved Transformer (referenced as 'The evolved transformer' in strubell2019) was run, it would have been at Google's Iowa datacenter (Augusta). Morrison2025 explicitly states that 'Augusta's trailing twelve-month average was 1.12' for PUE.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","According to the provided context, when the Evolved Transformer (referenced as 'The evolved transformer' in strubell2019) was run, it would have been at Google's Iowa datacenter (Augusta). Morrison2025 explicitly states that 'Augusta's trailing twelve-month average was 1.12' for PUE."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the provided context, the analysis estimates a total of approximately 772 billion GPT-4o queries in 2025, which is calculated based on a monthly prompt growth rate of 20% from January to May 2025, followed by a decaying growth pattern from June to December.","772 billion","queries","[""jegham2025""]","is_blank","is_blank","According to the provided context, the analysis estimates a total of approximately 772 billion GPT-4o queries in 2025, which is calculated based on a monthly prompt growth rate of 20% from January to May 2025, followed by a decaying growth pattern from June to December."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","According to the evaluation results presented in Table 3, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard benchmark suite. This is explicitly stated in the row 'OpenLLM Leaderboard Avg.' where JetMoE-8B's score of 53.0 is compared against other models like LLaMA2 (51.0), DeepseekMoE (51.1), and Gemma (46.4).","53.0","score","[""shen2024""]","is_blank","is_blank","According to the evaluation results presented in Table 3, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard benchmark suite. This is explicitly stated in the row 'OpenLLM Leaderboard Avg.' where JetMoE-8B's score of 53.0 is compared against other models like LLaMA2 (51.0), DeepseekMoE (51.1), and Gemma (46.4)."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","Based on the provided context, there is no specific information about how many AI training runs were conducted globally on renewable-only power in 2022. While the context discusses renewable energy use for AI training and its benefits in reducing carbon footprint, it does not provide a global count of training runs powered exclusively by renewable energy for 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","Based on the provided context, there is no specific information about how many AI training runs were conducted globally on renewable-only power in 2022. While the context discusses renewable energy use for AI training and its benefits in reducing carbon footprint, it does not provide a global count of training runs powered exclusively by renewable energy for 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","According to the policy proposal in the document, there is currently an 'open-source exemption from reporting obligations' that should be removed. The document specifically states that 'open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models,' indicating that open-source models are currently exempt from reporting obligations under the AI Act.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the policy proposal in the document, there is currently an 'open-source exemption from reporting obligations' that should be removed. The document specifically states that 'open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models,' indicating that open-source models are currently exempt from reporting obligations under the AI Act."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","is_blank","is_blank","PUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","According to Figure 1 in the document, the KV Cache size for the OPT-2.7B model running on a g4dn.xlarge instance with batch size 32 expanded to 5.312GB. This is explicitly stated in the text where it says 'When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.'","5.312","GB","[""kim2025""]","is_blank","is_blank","According to Figure 1 in the document, the KV Cache size for the OPT-2.7B model running on a g4dn.xlarge instance with batch size 32 expanded to 5.312GB. This is explicitly stated in the text where it says 'When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.'"
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context clearly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) studied in depth by Cottier et al., R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context clearly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) studied in depth by Cottier et al., R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","Based on the provided context, multiple documents indicate that CodeCarbon was used to measure energy consumption during inference runs. Morrison2025 explicitly states 'In our inference experiments, we measure cumulative energy consumption using CodeCarbon', while Zschache2025 confirms 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'. Rubei2025 also mentions using CodeCarbon to monitor the Llama 3 process for measurement.","CodeCarbon","is_blank","[""morrison2025"", ""zschache2025"", ""rubei2025""]","is_blank","is_blank","Based on the provided context, multiple documents indicate that CodeCarbon was used to measure energy consumption during inference runs. Morrison2025 explicitly states 'In our inference experiments, we measure cumulative energy consumption using CodeCarbon', while Zschache2025 confirms 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'. Rubei2025 also mentions using CodeCarbon to monitor the Llama 3 process for measurement."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","Based on an analysis of 100 news articles on ChatGPT's energy consumption, 53% cited the figure that a single ChatGPT query consumes 3 Wh or is 10 times more energy-intensive than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","Based on an analysis of 100 news articles on ChatGPT's energy consumption, 53% cited the figure that a single ChatGPT query consumes 3 Wh or is 10 times more energy-intensive than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","Based on the context, the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance is called the Compute Time Calibration Function (CTCF). This function is clearly described in Section IV-F of the paper as a linear transformation function that adjusts theoretical computation time to match actual execution time.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","Based on the context, the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance is called the Compute Time Calibration Function (CTCF). This function is clearly described in Section IV-F of the paper as a linear transformation function that adjusts theoretical computation time to match actual execution time."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","According to the provided context, energy per token and energy per response tend to follow a similar pattern in relation to the number of shards: as the number of shards increases, the energy per output token also increases. This indicates that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","is_blank","According to the provided context, energy per token and energy per response tend to follow a similar pattern in relation to the number of shards: as the number of shards increases, the energy per output token also increases. This indicates that increasing the number of GPU shards increases the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","To calculate the water consumption from mining rare earth materials for a single H100 GPU, we need to know the mass of rare earth metals in the H100 and the water consumption per kg of rare earth material mined. According to the context, an H100 is assumed to be 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. This directly adds 2.2 liters of water consumption per GPU.","0.0022","kL","[""morrison2025""]","is_blank","is_blank","To calculate the water consumption from mining rare earth materials for a single H100 GPU, we need to know the mass of rare earth metals in the H100 and the water consumption per kg of rare earth material mined. According to the context, an H100 is assumed to be 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. This directly adds 2.2 liters of water consumption per GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","According to the context, one of the proposed policy changes is the 'Elimination of open-source exemption' which would 'Remove the exemption that allows open-source models to bypass reporting obligations.' This clearly indicates that under current EU rules, open-source models are exempt from reporting obligations, including energy consumption reporting.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, one of the proposed policy changes is the 'Elimination of open-source exemption' which would 'Remove the exemption that allows open-source models to bypass reporting obligations.' This clearly indicates that under current EU rules, open-source models are exempt from reporting obligations, including energy consumption reporting."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","According to the context, in 2020, Amazon, Microsoft, Meta (Facebook), and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","According to the context, in 2020, Amazon, Microsoft, Meta (Facebook), and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","According to the document han2024, the total public health cost attributable to U.S. data centers was about $6.7 billion in 2023 using the average attribution method. This is explicitly stated in multiple places in the document.","6.7","USD","[""han2024""]","is_blank","is_blank","According to the document han2024, the total public health cost attributable to U.S. data centers was about $6.7 billion in 2023 using the average attribution method. This is explicitly stated in multiple places in the document."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","According to Table 4 on page 8 of the document, for long prompts (10k input-1.5k output), o3 consumes 12.222 Wh while GPT-4.1 nano consumes 0.827 Wh. Dividing these values (12.222/0.827) gives us the factor by which o3's energy consumption exceeds GPT-4.1 nano's.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","According to Table 4 on page 8 of the document, for long prompts (10k input-1.5k output), o3 consumes 12.222 Wh while GPT-4.1 nano consumes 0.827 Wh. Dividing these values (12.222/0.827) gives us the factor by which o3's energy consumption exceeds GPT-4.1 nano's."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","According to the context, for the Arena dataset (which represents short-context workloads) with a 120ms SLO, Mélange achieved cost reductions ranging from 15-77% compared to single-GPU baselines. This information is explicitly stated in Figure 11a's analysis.","15-77","percent","[""griggs2024""]","is_blank","is_blank","According to the context, for the Arena dataset (which represents short-context workloads) with a 120ms SLO, Mélange achieved cost reductions ranging from 15-77% compared to single-GPU baselines. This information is explicitly stated in Figure 11a's analysis."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","According to Table 3 in the paper about FLM-101B, the net carbon emissions for the pre-training of FLM-101B were 26 tCO2e (metric tons of CO2 equivalent).","26","tCO2e","[""li2025a""]","is_blank","is_blank","According to Table 3 in the paper about FLM-101B, the net carbon emissions for the pre-training of FLM-101B were 26 tCO2e (metric tons of CO2 equivalent)."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","According to the context, sparse fine-tuning of Mixtral-CS with a batch size of 1 has a throughput of 0.3 queries per second. Since throughput is queries per second, the execution time for 1 query would be the inverse, which is 1/0.3 = 3.33 seconds.","3.33","seconds","[""xia2024""]","is_blank","is_blank","According to the context, sparse fine-tuning of Mixtral-CS with a batch size of 1 has a throughput of 0.3 queries per second. Since throughput is queries per second, the execution time for 1 query would be the inverse, which is 1/0.3 = 3.33 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","According to the provided context, the public health impact of AI is highly unevenly distributed across communities. The context explicitly states that 'the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities' with the ratio of the highest county-level per-household health cost to the lowest cost being approximately 200. Additionally, 'disadvantaged communities bear a disproportionate share' of health impacts.","0","is_blank","[""han2024""]","is_blank","is_blank","According to the provided context, the public health impact of AI is highly unevenly distributed across communities. The context explicitly states that 'the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities' with the ratio of the highest county-level per-household health cost to the lowest cost being approximately 200. Additionally, 'disadvantaged communities bear a disproportionate share' of health impacts."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","According to the context, Figure 2 from Wu et al. 2021 explicitly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' based on data from Sun et al., 2019.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context, Figure 2 from Wu et al. 2021 explicitly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' based on data from Sun et al., 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","According to the context, the study specifically mentions using Ollama, an open-source platform, to apply 4-bit quantization in their financial sentiment case study. They state: 'In this work, we use a 4-bit quantization strategy (b = 4)... We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, the study specifically mentions using Ollama, an open-source platform, to apply 4-bit quantization in their financial sentiment case study. They state: 'In this work, we use a 4-bit quantization strategy (b = 4)... We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'"
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","According to the context, when running experiments in a multi-cloud environment on spot instances that could terminate at any time, the researchers used Backblaze (B2) as an independent S3 storage provider and accessed datasets on-demand via shards in the tar format with the WebDataset library.","Backblaze B2 with WebDataset","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, when running experiments in a multi-cloud environment on spot instances that could terminate at any time, the researchers used Backblaze (B2) as an independent S3 storage provider and accessed datasets on-demand via shards in the tar format with the WebDataset library."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","According to the context, specifically from document zschache2025, there is 'a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.' This indicates that the relationship between runtime and energy consumption in LLM inference was found to be strongly correlated, which suggests a nearly linear relationship.","1","is_blank","[""zschache2025""]","is_blank","is_blank","According to the context, specifically from document zschache2025, there is 'a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.' This indicates that the relationship between runtime and energy consumption in LLM inference was found to be strongly correlated, which suggests a nearly linear relationship."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","According to multiple references in the provided context, the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","According to multiple references in the provided context, the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to the table in Chen et al. (2024), the price per hour for an NVIDIA H100 is clearly listed as $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","According to the table in Chen et al. (2024), the price per hour for an NVIDIA H100 is clearly listed as $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","Based on the provided context, an estimated GPU lifetime can be derived from Cottier et al. (2024) which mentions that 'Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.' This is referring to GPUs used in AI data centers and is the most specific estimate provided in the context for GPU lifetime before retirement.","3.7","years","[""cottier2024""]","is_blank","is_blank","Based on the provided context, an estimated GPU lifetime can be derived from Cottier et al. (2024) which mentions that 'Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.' This is referring to GPUs used in AI data centers and is the most specific estimate provided in the context for GPU lifetime before retirement."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the provided context, LLaMA-13B requires a minimum of 2 V100 32GB GPUs to run inference without compression or quantization, with a maximum batch size of 64 and a maximum response length of 256.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","According to Table II in the provided context, LLaMA-13B requires a minimum of 2 V100 32GB GPUs to run inference without compression or quantization, with a maximum batch size of 64 and a maximum response length of 256."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the provided context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4 times higher than the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the provided context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4 times higher than the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to the provided context, specifically from the Strubell 2019 paper, ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","According to the provided context, specifically from the Strubell 2019 paper, ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","According to the context, the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh. This is mentioned explicitly in section 2.2.2 of the document.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","According to the context, the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh. This is mentioned explicitly in section 2.2.2 of the document."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","According to the context, Google reported a PUE (Power Usage Effectiveness) of 1.10 for its hyperscale data centers in 2021. This information is explicitly stated in multiple sources, including wu2021b and dodge2022.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","According to the context, Google reported a PUE (Power Usage Effectiveness) of 1.10 for its hyperscale data centers in 2021. This information is explicitly stated in multiple sources, including wu2021b and dodge2022."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the provided context from Amazon's 2023 Sustainability Report, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","is_blank","According to the provided context from Amazon's 2023 Sustainability Report, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","According to the paper by Xia et al., their analytical model predicted that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU would cost $3460. This was specifically mentioned for the OpenOrca dataset containing more than 2M queries.","3460","USD","[""xia2024""]","is_blank","is_blank","According to the paper by Xia et al., their analytical model predicted that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU would cost $3460. This was specifically mentioned for the OpenOrca dataset containing more than 2M queries."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to the context provided by Dodge et al. (2022), the GPU accounts for approximately 74% of the total electricity consumption in a datacenter instance running AI workloads. This was demonstrated through an experiment where they trained a BERT-base model and measured the electricity consumption of different hardware components.","74","percent","[""dodge2022""]","is_blank","is_blank","According to the context provided by Dodge et al. (2022), the GPU accounts for approximately 74% of the total electricity consumption in a datacenter instance running AI workloads. This was demonstrated through an experiment where they trained a BERT-base model and measured the electricity consumption of different hardware components."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","According to the context, the training infrastructure for JetMoE-8B consisted of 'a cluster containing 12 nodes and 96 H100s.' This clearly states that 96 H100 GPUs were used for the training.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","According to the context, the training infrastructure for JetMoE-8B consisted of 'a cluster containing 12 nodes and 96 H100s.' This clearly states that 96 H100 GPUs were used for the training."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","According to the context, specifically from ref_id=morrison2025, during active training of OLMo 2 7B, the average GPU power for a single node is over 600W, which is more than 85% of an H100's maximum power draw of 700W. This information is explicitly stated in the caption of Figure 2 which shows the average GPU power for the first 300 logging steps during OLMo 2 7B training.","600","Watts","[""morrison2025""]","is_blank","is_blank","According to the context, specifically from ref_id=morrison2025, during active training of OLMo 2 7B, the average GPU power for a single node is over 600W, which is more than 85% of an H100's maximum power draw of 700W. This information is explicitly stated in the caption of Figure 2 which shows the average GPU power for the first 300 logging steps during OLMo 2 7B training."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","According to the Strubell et al. 2019 paper, the average human life globally produces 11,023 pounds of CO2e in one year. This information is presented in a table comparing various consumption sources and their CO2e emissions.","11023","lbs","[""strubell2019""]","is_blank","is_blank","According to the Strubell et al. 2019 paper, the average human life globally produces 11,023 pounds of CO2e in one year. This information is presented in a table comparing various consumption sources and their CO2e emissions."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for the LLaMA-13B model, the A100 GPUs provided a 1.25 times increase in inference latency compared to the V100 GPUs across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","According to the context, for the LLaMA-13B model, the A100 GPUs provided a 1.25 times increase in inference latency compared to the V100 GPUs across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","According to the provided context, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This information is consistently stated multiple times in the context, specifically in the GPT-4o Environmental Impact Case Study section.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","According to the provided context, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This information is consistently stated multiple times in the context, specifically in the GPT-4o Environmental Impact Case Study section."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to the provided context, for the online workload experiment with a 400 TPS SLO, InferSave selected g4dn.xlarge as its top choice at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The context explicitly states this is 'about 280% more expensive than InferSave's top choice.'","280","percent","[""kim2025""]","is_blank","is_blank","According to the provided context, for the online workload experiment with a 400 TPS SLO, InferSave selected g4dn.xlarge as its top choice at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The context explicitly states this is 'about 280% more expensive than InferSave's top choice.'"
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","According to the provided context, for models generating a single token per inference (such as in classification tasks), the V100 or A30 GPU architecture is more energy-efficient. This is explicitly stated in the zschache2025 document, which mentions that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.'","V100 or A30","is_blank","[""zschache2025""]","is_blank","is_blank","According to the provided context, for models generating a single token per inference (such as in classification tasks), the V100 or A30 GPU architecture is more energy-efficient. This is explicitly stated in the zschache2025 document, which mentions that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.'"
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","According to the context, specifically the Schwartz 2019 paper, Green AI involves 'reporting the computational price tag of finding, training, and running models' as a key practice.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","According to the context, specifically the Schwartz 2019 paper, Green AI involves 'reporting the computational price tag of finding, training, and running models' as a key practice."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","According to the 2025 paper by Luccioni et al., electronic waste (e-waste) reached 62 million tonnes worldwide in 2022. This is specifically mentioned in the context as 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","According to the 2025 paper by Luccioni et al., electronic waste (e-waste) reached 62 million tonnes worldwide in 2022. This is specifically mentioned in the context as 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'"
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context does not provide sufficient information to calculate the difference in average system power per processor between TPU v2 and V100 GPU. While there is information about the power consumption of TPUs and V100 GPUs in different contexts, there is no direct comparison of average system power per processor between these specific models.","is_blank","Watts","[""is_blank""]","is_blank","is_blank","The context does not provide sufficient information to calculate the difference in average system power per processor between TPU v2 and V100 GPU. While there is information about the power consumption of TPUs and V100 GPUs in different contexts, there is no direct comparison of average system power per processor between these specific models."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to Li et al.'s paper, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This refers specifically to the scope-1 on-site water consumption during the training process.","700000","liters","[""li2025b""]","is_blank","is_blank","According to Li et al.'s paper, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This refers specifically to the scope-1 on-site water consumption during the training process."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","According to multiple instances in the provided context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is explicitly stated in Figure 2(c) caption and mentioned repeatedly in the text.","20","multiplier","[""wu2021a""]","is_blank","is_blank","According to multiple instances in the provided context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is explicitly stated in Figure 2(c) caption and mentioned repeatedly in the text."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the cross-efficiency Data Envelopment Analysis (DEA) results presented in Figure 8, o3-mini achieved the highest cross-efficiency score (0.884) among all evaluated models, closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825).","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","According to the cross-efficiency Data Envelopment Analysis (DEA) results presented in Figure 8, o3-mini achieved the highest cross-efficiency score (0.884) among all evaluated models, closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825)."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","According to the context provided in ref_id=wu2021b, cell phones currently have average lifetimes of 'less than 3 years', which contributes to electronic waste concerns. This is specifically mentioned in the discussion about sustainability and the need to develop expandable hardware and software stacks that facilitate significantly longer lifetimes than the current averages.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context provided in ref_id=wu2021b, cell phones currently have average lifetimes of 'less than 3 years', which contributes to electronic waste concerns. This is specifically mentioned in the discussion about sustainability and the need to develop expandable hardware and software stacks that facilitate significantly longer lifetimes than the current averages."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context shows that intra-zone experiments were conducted with T4 GPUs, but it specifically states ""CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)"", which indicates nearly linear scaling for CV models. This is reinforced by statements about good scaling potential with T4 GPUs.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context shows that intra-zone experiments were conducted with T4 GPUs, but it specifically states ""CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)"", which indicates nearly linear scaling for CV models. This is reinforced by statements about good scaling potential with T4 GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","According to the provided context, specifically from reference li2025a, the FLM-101B model has a context window of 2,048 tokens, as stated in the model configuration details.","2048","tokens","[""li2025a""]","is_blank","is_blank","According to the provided context, specifically from reference li2025a, the FLM-101B model has a context window of 2,048 tokens, as stated in the model configuration details."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context explains that LLMs consume significantly less power than the GPU's TDP because LLM decoding is memory-intensive with low compute-intensity, meaning the number of arithmetic operations per byte of memory loaded is low. This leads to the GPU being bottlenecked by VRAM bandwidth, resulting in underutilized computation units and low power draw. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU when batch size is not small.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context explains that LLMs consume significantly less power than the GPU's TDP because LLM decoding is memory-intensive with low compute-intensity, meaning the number of arithmetic operations per byte of memory loaded is low. This leads to the GPU being bottlenecked by VRAM bandwidth, resulting in underutilized computation units and low power draw. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU when batch size is not small."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","According to the Amazon Representation by the Numbers data from 2023, the percentage of Amazon's People Managers globally who identified as women was 31.1%. This information is clearly displayed in the Gender-Global section of the representation chart, in the People Managers row for 2023.","31.1","percent","[""amazon2023""]","is_blank","is_blank","According to the Amazon Representation by the Numbers data from 2023, the percentage of Amazon's People Managers globally who identified as women was 31.1%. This information is clearly displayed in the Gender-Global section of the representation chart, in the People Managers row for 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","According to the context, when equity is excluded from R&D costs for the four models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), the fraction of computing hardware costs rises to 61–76% of the total amortized cost.","61-76","percent","[""cottier2024""]","is_blank","is_blank","According to the context, when equity is excluded from R&D costs for the four models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), the fraction of computing hardware costs rises to 61–76% of the total amortized cost."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","According to Table III, which compares performance metrics before and after optimization, the accuracy and F1 scores generally decreased after optimization. The text explicitly states 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.'","0","is_blank","[""khan2025""]","is_blank","is_blank","According to Table III, which compares performance metrics before and after optimization, the accuracy and F1 scores generally decreased after optimization. The text explicitly states 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.'"
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","According to the context, the U.S. Environmental Protection Agency (EPA) provides a value of 0.954 pounds per kilowatt-hour for the average CO2 produced for power consumed in the U.S., as shown in the formula CO2e = 0.954pt where pt represents power consumption in kilowatt-hours.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","According to the context, the U.S. Environmental Protection Agency (EPA) provides a value of 0.954 pounds per kilowatt-hour for the average CO2 produced for power consumed in the U.S., as shown in the formula CO2e = 0.954pt where pt represents power consumption in kilowatt-hours."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to a recent study cited in Li et al. 2025, global AI is projected to consume 85 – 134 TWh of electricity in 2027. This range is mentioned multiple times throughout the document as the basis for further calculations regarding water usage and environmental impact.","85-134","TWh","[""li2025b""]","is_blank","is_blank","According to a recent study cited in Li et al. 2025, global AI is projected to consume 85 – 134 TWh of electricity in 2027. This range is mentioned multiple times throughout the document as the basis for further calculations regarding water usage and environmental impact."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","According to the context, Amazon delivered 150 million packages via EVs in Europe in 2023, as explicitly stated in the report.","150","packages","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon delivered 150 million packages via EVs in Europe in 2023, as explicitly stated in the report."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","According to the context from kim2025, in the online inference workload, each request consisted of 128 input tokens and 512 output tokens. The workload evaluated a total of 3000 requests. Therefore, the total number of tokens processed was (128 + 512) × 3000 = 1,920,000 tokens.","1920000","tokens","[""kim2025""]","is_blank","is_blank","According to the context from kim2025, in the online inference workload, each request consisted of 128 input tokens and 512 output tokens. The workload evaluated a total of 3000 requests. Therefore, the total number of tokens processed was (128 + 512) × 3000 = 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","According to the context, the AI Act does not mandate the disclosure of greenhouse gas emissions of AI applications. Specifically, the source states that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, the AI Act does not mandate the disclosure of greenhouse gas emissions of AI applications. Specifically, the source states that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to Figure 3(a) in the Dodge et al. 2022 paper, the Flexible Start optimization for DenseNet 201, which is described as a short job running for less than half an hour, shows potential CO2 emissions reductions 'up to 80% in West US' region. This is clearly stated in both the figure caption and in the explanatory text.","80","percent","[""dodge2022""]","is_blank","is_blank","According to Figure 3(a) in the Dodge et al. 2022 paper, the Flexible Start optimization for DenseNet 201, which is described as a short job running for less than half an hour, shows potential CO2 emissions reductions 'up to 80% in West US' region. This is clearly stated in both the figure caption and in the explanatory text."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","According to the 2024 study by Luccioni et al., there is a large variation in energy consumption across tasks, with the most energy-intensive task (image generation) having a mean consumption of 2.9 kWh per 1,000 inferences, while the least energy-intensive task (text classification) has a mean consumption of 0.002 kWh per 1,000 inferences. The study explicitly states that 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.'","1450","times","[""luccioni2024""]","is_blank","is_blank","According to the 2024 study by Luccioni et al., there is a large variation in energy consumption across tasks, with the most energy-intensive task (image generation) having a mean consumption of 2.9 kWh per 1,000 inferences, while the least energy-intensive task (text classification) has a mean consumption of 0.002 kWh per 1,000 inferences. The study explicitly states that 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.'"
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","According to the context, in the Yelp sentiment analysis benchmarks, traditional models perform considerably worse than LLMs. The text explicitly states 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This indicates that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","is_blank","According to the context, in the Yelp sentiment analysis benchmarks, traditional models perform considerably worse than LLMs. The text explicitly states 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This indicates that traditional models did not achieve accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","According to the context, specifically in the table titled 'Amazon Renewable Energy Projects*' showing projects announced as of January 2024, the United States had 244 renewable energy projects.","244","projects","[""amazon2023""]","is_blank","is_blank","According to the context, specifically in the table titled 'Amazon Renewable Energy Projects*' showing projects announced as of January 2024, the United States had 244 renewable energy projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide information about what percentage of power usage Amazon's AWS covered with renewable energy specifically in 2018. While the documents discuss Amazon's renewable energy procurement and AWS's energy sources, they do not give the specific percentage for AWS's renewable energy coverage in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about what percentage of power usage Amazon's AWS covered with renewable energy specifically in 2018. While the documents discuss Amazon's renewable energy procurement and AWS's energy sources, they do not give the specific percentage for AWS's renewable energy coverage in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","According to Ebert et al. (2024), energy consumption for AI should be reported at the cumulative server level as this approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","According to Ebert et al. (2024), energy consumption for AI should be reported at the cumulative server level as this approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","According to Table 1 in the dodge2022 paper, when training BERT base on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption. The table shows the GPU consuming 187.1 watts out of a total of 251.6 watts across all hardware components.","74","percent","[""dodge2022""]","is_blank","is_blank","According to Table 1 in the dodge2022 paper, when training BERT base on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption. The table shows the GPU consuming 187.1 watts out of a total of 251.6 watts across all hardware components."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","According to the context from griggs2024, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The model referenced (Llama2-70b) is essentially equivalent to the Llama2-70b model mentioned in the question.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","According to the context from griggs2024, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The model referenced (Llama2-70b) is essentially equivalent to the Llama2-70b model mentioned in the question."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","According to Figure 8 in the paper by Wu et al. (2021), the iterative optimization process across the machine learning hardware-software stack led to 28.5% operational energy footprint reduction over a two-year time period from 2019 to 2021.","28.5","percent","[""wu2021a""]","is_blank","is_blank","According to Figure 8 in the paper by Wu et al. (2021), the iterative optimization process across the machine learning hardware-software stack led to 28.5% operational energy footprint reduction over a two-year time period from 2019 to 2021."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","Based on the provided context snippets, there is no specific information about how many kilometers of fiber optic cable were installed globally to support AI workloads in 2023. While the documents discuss various aspects of AI's environmental impact, growth trends, and resource consumption, they do not mention fiber optic cable installation metrics.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, there is no specific information about how many kilometers of fiber optic cable were installed globally to support AI workloads in 2023. While the documents discuss various aspects of AI's environmental impact, growth trends, and resource consumption, they do not mention fiber optic cable installation metrics."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","According to the provided context, specifically from ref_id=cottier2024, the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This is explicitly stated multiple times in the context, distinguishing it from the amortized hardware CapEx + energy cost of $40M.","800000000","USD","[""cottier2024""]","is_blank","is_blank","According to the provided context, specifically from ref_id=cottier2024, the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This is explicitly stated multiple times in the context, distinguishing it from the amortized hardware CapEx + energy cost of $40M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","According to the context, the umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves. This definition includes two aspects, not just using AI for climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, the umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves. This definition includes two aspects, not just using AI for climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","According to the provided context, specifically in Fig. 13 which shows the projected maximum batch size of Mixtral for different GPUs, the ground truth maximum batch size for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU is clearly visible in the figure.","3","samples","[""xia2024""]","is_blank","is_blank","According to the provided context, specifically in Fig. 13 which shows the projected maximum batch size of Mixtral for different GPUs, the ground truth maximum batch size for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU is clearly visible in the figure."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. Figure 8 shows this comparison using the Mistral Nemo model, and the text explicitly states that 'when the server is overloaded, Swapping consistently consumes less energy' because Recomputation performs extra computation when restoring requests while Swapping just copies data without running computation.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. Figure 8 shows this comparison using the Mistral Nemo model, and the text explicitly states that 'when the server is overloaded, Swapping consistently consumes less energy' because Recomputation performs extra computation when restoring requests while Swapping just copies data without running computation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context does not provide specific information about the percentage increase in GHG emissions since 2019 that Google reported in its 2024 environmental report. While multiple references to Google's 2024 environmental report exist in the context, none of them mention the specific percentage increase in GHG emissions since 2019.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage increase in GHG emissions since 2019 that Google reported in its 2024 environmental report. While multiple references to Google's 2024 environmental report exist in the context, none of them mention the specific percentage increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to projections cited by Fernandez et al. (2025), data centers are estimated to consume between 9.1% and 11.7% of the total US energy demand by 2030, primarily motivated by increased demands from LLM and AI workloads.","9.1-11.7","percent","[""fernandez2025""]","is_blank","is_blank","According to projections cited by Fernandez et al. (2025), data centers are estimated to consume between 9.1% and 11.7% of the total US energy demand by 2030, primarily motivated by increased demands from LLM and AI workloads."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","Figure 7(b) and its caption in the dodge2022 document explicitly shows the optimization results for the 'Pause and Resume' technique applied to training a 6B Parameter Transformer on 256 A100s. Looking at the graph, we can see that the maximum CO2 emissions decrease percentage across all regions is around 30%. This is further supported by Table 9 which shows that for the 6B Transformer with a 100% increase in job duration, the Pause and Resume optimization provides an 11.4% reduction on average across all regions, indicating that some regions achieve significantly higher savings up to approximately 30%.","30","percent","[""dodge2022""]","is_blank","is_blank","Figure 7(b) and its caption in the dodge2022 document explicitly shows the optimization results for the 'Pause and Resume' technique applied to training a 6B Parameter Transformer on 256 A100s. Looking at the graph, we can see that the maximum CO2 emissions decrease percentage across all regions is around 30%. This is further supported by Table 9 which shows that for the 6B Transformer with a 100% increase in job duration, the Pause and Resume optimization provides an 11.4% reduction on average across all regions, indicating that some regions achieve significantly higher savings up to approximately 30%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","According to multiple instances in the provided context, JetMoE-8B has 8B parameters but only activates 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B. This specific percentage reduction is consistently mentioned across several sections of the document.","70","percent","[""shen2024""]","is_blank","is_blank","According to multiple instances in the provided context, JetMoE-8B has 8B parameters but only activates 2B for each input token, which reduces inference computation by about 70% compared to Llama2-7B. This specific percentage reduction is consistently mentioned across several sections of the document."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","According to the context in ref_id=luccioni2025b, the 2019 seminal study by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","According to the context in ref_id=luccioni2025b, the 2019 seminal study by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","According to the context, GPT-3 was trained in Microsoft's data centers with an estimated training energy of 1287 MWh. This information is directly stated in the context snippet from document li2025b.","1287","MWh","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 was trained in Microsoft's data centers with an estimated training energy of 1287 MWh. This information is directly stated in the context snippet from document li2025b."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","According to the context, the MoE layer is identified as the most time-consuming layer, accounting for 85% of the overall execution time in LLM fine-tuning. It is explicitly stated that 'MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.'","1","is_blank","[""xia2024""]","is_blank","is_blank","According to the context, the MoE layer is identified as the most time-consuming layer, accounting for 85% of the overall execution time in LLM fine-tuning. It is explicitly stated that 'MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.'"
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Figure 14 in the document by Xia et al. (2024), which shows the estimation and validation of LLM fine-tuning throughput for different models and datasets on an NVIDIA A40 GPU, the maximum batch size for Mixtral with a dense setup on the Hellaswag dataset is 4 samples.","4","samples","[""xia2024""]","is_blank","is_blank","According to Figure 14 in the document by Xia et al. (2024), which shows the estimation and validation of LLM fine-tuning throughput for different models and datasets on an NVIDIA A40 GPU, the maximum batch size for Mixtral with a dense setup on the Hellaswag dataset is 4 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","According to the context, for high granularity tasks like CV (computer vision), distributing VMs over four continents only slowed down performance by 7% compared to local training. This is stated explicitly in multiple parts of the text.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, for high granularity tasks like CV (computer vision), distributing VMs over four continents only slowed down performance by 7% compared to local training. This is stated explicitly in multiple parts of the text."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide specific information about how many hectares of land were occupied by new AI data centers globally in 2022. While there are mentions of data centers and their energy/water consumption, there is no data specifically about the land area occupied by new AI data centers in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about how many hectares of land were occupied by new AI data centers globally in 2022. While there are mentions of data centers and their energy/water consumption, there is no data specifically about the land area occupied by new AI data centers in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","According to the 2024 study by Luccioni et al., bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries, which is much less than larger multi-purpose models like Flan-T5-XL (2.66g) and BLOOMz-7B (4.67g).","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","According to the 2024 study by Luccioni et al., bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries, which is much less than larger multi-purpose models like Flan-T5-XL (2.66g) and BLOOMz-7B (4.67g)."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","According to the context, the study on emissions from training BERT across different regions shows that there is a large variation between the least and most carbon-intensive regions. Specifically, Figure 1 indicates that choosing the region for experiments can be very impactful, with emissions ranging from '7k grams' in the most efficient region to '26k grams' in the least efficient region.","19000","grams","[""dodge2022""]","is_blank","is_blank","According to the context, the study on emissions from training BERT across different regions shows that there is a large variation between the least and most carbon-intensive regions. Specifically, Figure 1 indicates that choosing the region for experiments can be very impactful, with emissions ranging from '7k grams' in the most efficient region to '26k grams' in the least efficient region."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information for notable AI models peaked in 2022 with 10% of notable models that year releasing some degree of information, before declining afterward due to the introduction of increasingly commercial and proprietary models.","2022","year","[""luccioni2025c""]","is_blank","is_blank","According to the context, the direct release of environmental information for notable AI models peaked in 2022 with 10% of notable models that year releasing some degree of information, before declining afterward due to the introduction of increasingly commercial and proprietary models."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","According to Table 2 in the Morrison 2025 paper, training an OLMo 60M model consumes 1.6 kL of water, which is equivalent to 5 days of water usage for one person in the U.S.","5","days","[""morrison2025""]","is_blank","is_blank","According to Table 2 in the Morrison 2025 paper, training an OLMo 60M model consumes 1.6 kL of water, which is equivalent to 5 days of water usage for one person in the U.S."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about the total liters of freshwater consumed by Google's DeepMind AlphaFold servers in 2023. While there is mention of one technology company's data centers consuming more than 23 billion liters of freshwater in 2023, this is not specifically attributed to Google's DeepMind AlphaFold servers. The reference to Google only mentions their sustainability report acknowledging that AI products and services are driving water consumption increases, but doesn't provide AlphaFold-specific data.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total liters of freshwater consumed by Google's DeepMind AlphaFold servers in 2023. While there is mention of one technology company's data centers consuming more than 23 billion liters of freshwater in 2023, this is not specifically attributed to Google's DeepMind AlphaFold servers. The reference to Google only mentions their sustainability report acknowledging that AI products and services are driving water consumption increases, but doesn't provide AlphaFold-specific data."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","According to the context from document xia2024, the authors explicitly state 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This directly confirms that adding compute resources to accelerate MoE layers when fine-tuning LLMs can reduce costs, not increase them.","0","is_blank","[""xia2024""]","is_blank","is_blank","According to the context from document xia2024, the authors explicitly state 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This directly confirms that adding compute resources to accelerate MoE layers when fine-tuning LLMs can reduce costs, not increase them."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to Table 3 in the document, which shows carbon emissions of various LLMs including FLM-101B, the total energy consumption for training the FLM-101B model was 40 MkWh (megakilowatt hours).","40","kWh","[""li2025a""]","is_blank","is_blank","According to Table 3 in the document, which shows carbon emissions of various LLMs including FLM-101B, the total energy consumption for training the FLM-101B model was 40 MkWh (megakilowatt hours)."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to the provided context, a study projecting the public health burden of U.S. data centers for 2030 estimates that the total could reach more than $20 billion. Specifically, the paper states that 'The overall public health costs could reach more than $20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with ∼35 million registered vehicles.'","20000000000","USD","[""han2024""]","is_blank","is_blank","According to the provided context, a study projecting the public health burden of U.S. data centers for 2030 estimates that the total could reach more than $20 billion. Specifically, the paper states that 'The overall public health costs could reach more than $20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with ∼35 million registered vehicles.'"
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","Table IV in document xia2024 explicitly shows the estimated cost of fine-tuning Mixtral on GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU as $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","Table IV in document xia2024 explicitly shows the estimated cost of fine-tuning Mixtral on GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU as $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","According to Table III in the context, the maximum batch size for Mixtral-S (sparse Mixtral) on MATH dataset is 3 samples when using an NVIDIA A40-48GB GPU for fine-tuning.","3","samples","[""xia2024""]","is_blank","is_blank","According to Table III in the context, the maximum batch size for Mixtral-S (sparse Mixtral) on MATH dataset is 3 samples when using an NVIDIA A40-48GB GPU for fine-tuning."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","According to Table IV in the paper by Xia et al., the throughput for fine-tuning a sparse Mixtral model on a NVIDIA A40-48GB GPU is 1.01 queries/second. The total execution time can be calculated from the cost information: cost of $32.7 divided by hourly rate of $0.79 gives approximately 41.4 seconds.","41.4","seconds","[""xia2024""]","is_blank","is_blank","According to Table IV in the paper by Xia et al., the throughput for fine-tuning a sparse Mixtral model on a NVIDIA A40-48GB GPU is 1.01 queries/second. The total execution time can be calculated from the cost information: cost of $32.7 divided by hourly rate of $0.79 gives approximately 41.4 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context clearly states that GPU-level power consumption monitoring is not recommended for overall AI energy use. Ebert2024 specifically states: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements' because 'it substantially under-represents the actual energy consumption since it measures just a single component.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that GPU-level power consumption monitoring is not recommended for overall AI energy use. Ebert2024 specifically states: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements' because 'it substantially under-represents the actual energy consumption since it measures just a single component.'"
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to a 2023 article by Luccioni et al. estimating the carbon footprint of BLOOM (a 176 billion parameter LLM), they found that training accounted for only half of the model's overall emissions. The article used a Life Cycle Assessment approach that considered different stages including manufacturing of computing hardware, idle energy usage, and model deployment.","50","percent","[""luccioni2025b""]","is_blank","is_blank","According to a 2023 article by Luccioni et al. estimating the carbon footprint of BLOOM (a 176 billion parameter LLM), they found that training accounted for only half of the model's overall emissions. The article used a Life Cycle Assessment approach that considered different stages including manufacturing of computing hardware, idle energy usage, and model deployment."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the provided context from [ref_id=samsi2024], specifically from Table II, the bare minimum hardware requirement for running LLaMA-7B inference without compression or quantization is 1 V100 32GB GPU with a maximum batch size of 64.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the provided context from [ref_id=samsi2024], specifically from Table II, the bare minimum hardware requirement for running LLaMA-7B inference without compression or quantization is 1 V100 32GB GPU with a maximum batch size of 64."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to Table 1 in the provided context, when GPT-3 is trained in a data center in Arizona, the number of requests for 500ml water is 16.7, which means it would take approximately 16.7 user requests to consume a 500ml bottle of water.","16.7","requests","[""li2025b""]","is_blank","is_blank","According to Table 1 in the provided context, when GPT-3 is trained in a data center in Arizona, the number of requests for 500ml water is 16.7, which means it would take approximately 16.7 user requests to consume a 500ml bottle of water."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","According to the context, specifically ref_id=dodge2022, one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.30","metric tons","[""dodge2022""]","is_blank","is_blank","According to the context, specifically ref_id=dodge2022, one average US home energy use is estimated to emit 8.30 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","After reviewing all context snippets, there is no information provided about the JetMoE-8B model's score on the GSM8k benchmark which evaluates grade school math problem-solving. The provided context discusses performance on MT-Bench and comparisons to Llama models, but does not mention GSM8k benchmark results.","is_blank","score","[""is_blank""]","is_blank","is_blank","After reviewing all context snippets, there is no information provided about the JetMoE-8B model's score on the GSM8k benchmark which evaluates grade school math problem-solving. The provided context discusses performance on MT-Bench and comparisons to Llama models, but does not mention GSM8k benchmark results."
