"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","Based on the context, the paper presents a benchmark suite called the 'ML.ENERGY Benchmark' for measuring inference energy consumption. This is clearly stated in multiple sections of the paper, including the abstract where it says 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments...'","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","Based on the context, the paper presents a benchmark suite called the 'ML.ENERGY Benchmark' for measuring inference energy consumption. This is clearly stated in multiple sections of the paper, including the abstract where it says 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments...'"
"q009","What were the net CO2e emissions from training the GShard-600B model?","Based on the context provided, specifically in the document wu2021a, Figure 4 shows the operational carbon footprint of large-scale ML tasks including GShard-600B. The figure indicates that the CO2e emissions from training GShard-600B were approximately 0.4 million kg CO2e (or 400 tCO2e).","400","tCO2e","[""wu2021a""]","is_blank","is_blank","Based on the context provided, specifically in the document wu2021a, Figure 4 shows the operational carbon footprint of large-scale ML tasks including GShard-600B. The figure indicates that the CO2e emissions from training GShard-600B were approximately 0.4 million kg CO2e (or 400 tCO2e)."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","According to the context information in Table 3 from document chen2024, the LLaMA-33B model has a parameters size of 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","According to the context information in Table 3 from document chen2024, the LLaMA-33B model has a parameters size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The provided context snippets do not contain any information about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. While there is information about electricity consumption for various AI models and data centers, none specifically addresses Google Cloud TPU pods' worldwide electricity consumption for 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The provided context snippets do not contain any information about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. While there is information about electricity consumption for various AI models and data centers, none specifically addresses Google Cloud TPU pods' worldwide electricity consumption for 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","According to the context from wu2021b, hyperscale data centers achieved 'more than 40% higher efficiency for hyperscale data centers' compared to traditional data centers. This is stated directly in Figure 1's caption and also in the main text discussing efficiency differences between traditional and hyperscale data centers in 2020.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context from wu2021b, hyperscale data centers achieved 'more than 40% higher efficiency for hyperscale data centers' compared to traditional data centers. This is stated directly in Figure 1's caption and also in the main text discussing efficiency differences between traditional and hyperscale data centers in 2020."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the context, GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on when and where it is deployed.","1/10 - 1/50","500 mL bottles","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","According to the context, 75% of CVPR papers target accuracy, while 20% of CVPR papers argue for a new efficiency result. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency is 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","According to the context, 75% of CVPR papers target accuracy, while 20% of CVPR papers argue for a new efficiency result. Therefore, the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","According to the provided context, energy consumption data from AI providers is not publicly available to NGOs, analysts, or the general public. Specifically, the document states that 'Where the Act does mandate the disclosure of energy consumption, this information is restricted to authorities and is not accessible to downstream providers...or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1).' The document actually recommends as a policy proposal that 'All climate-related disclosures, including energy consumption, should be made accessible to the general public.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the provided context, energy consumption data from AI providers is not publicly available to NGOs, analysts, or the general public. Specifically, the document states that 'Where the Act does mandate the disclosure of energy consumption, this information is restricted to authorities and is not accessible to downstream providers...or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1).' The document actually recommends as a policy proposal that 'All climate-related disclosures, including energy consumption, should be made accessible to the general public.'"
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to Figure 13 and the accompanying text in the paper, the study used an analytical model to project the maximum batch size that future GPUs might support. For a GPU memory capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","According to Figure 13 and the accompanying text in the paper, the study used an analytical model to project the maximum batch size that future GPUs might support. For a GPU memory capacity of 100GB, the model predicts that the maximum batch size supported for fine-tuning Mixtral will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the provided context, specifically the text from document samsi2024, for the smaller LLaMA 7B model, the A100 GPUs provided a 2 times increase in inference throughput compared to V100 GPUs.","2","multiplier","[""samsi2024""]","is_blank","is_blank","According to the provided context, specifically the text from document samsi2024, for the smaller LLaMA 7B model, the A100 GPUs provided a 2 times increase in inference throughput compared to V100 GPUs."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","According to Table 1 in the provided context, the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 5.439 million liters. This value is shown in the 'Total Water' column under 'Water for Training(million L)' for 'U.S. Average'.","5.439","liters","[""li2025b""]","is_blank","is_blank","According to Table 1 in the provided context, the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 5.439 million liters. This value is shown in the 'Total Water' column under 'Water for Training(million L)' for 'U.S. Average'."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","According to the source [ref_id=ebert2024], the authors explicitly state that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' They justify this by noting that 'the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.'","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the source [ref_id=ebert2024], the authors explicitly state that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' They justify this by noting that 'the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.'"
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","According to Amazon's 2023 Sustainability Report, AWS improved its global data center Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022, representing a 5% year-over-year improvement and a 28% improvement since 2021.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","According to Amazon's 2023 Sustainability Report, AWS improved its global data center Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022, representing a 5% year-over-year improvement and a 28% improvement since 2021."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","According to the context, local inference optimization is emphasized as a sustainability measure because 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint.' The framework includes local inference as a key component to reduce energy consumption and carbon emissions.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, local inference optimization is emphasized as a sustainability measure because 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint.' The framework includes local inference as a key component to reduce energy consumption and carbon emissions."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context strongly supports this statement. Strubell et al. (2019) explicitly suggest that ""Authors should report training time and sensitivity to hyperparameters"" to enable cost-benefit analysis and allow for accurate assessment of computational resources required. They present detailed tables showing how training time is directly related to cloud compute costs for various models. Additionally, Luccioni (2023) shows how training time is a key factor in the equation for calculating CO2 emissions: C = P × T × I, where T represents training time.","1","is_blank","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","The context strongly supports this statement. Strubell et al. (2019) explicitly suggest that ""Authors should report training time and sensitivity to hyperparameters"" to enable cost-benefit analysis and allow for accurate assessment of computational resources required. They present detailed tables showing how training time is directly related to cloud compute costs for various models. Additionally, Luccioni (2023) shows how training time is a key factor in the equation for calculating CO2 emissions: C = P × T × I, where T represents training time."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the Chen et al. 2024 study, the LLaMA-65B model experienced 'a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.' This maximum performance improvement (latency reduction) of 13.2% was achieved specifically when using the automated resource utilization overlapping feature, with the speedup being more pronounced for larger batch sizes.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to the Chen et al. 2024 study, the LLaMA-65B model experienced 'a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.' This maximum performance improvement (latency reduction) of 13.2% was achieved specifically when using the automated resource utilization overlapping feature, with the speedup being more pronounced for larger batch sizes."
"q164","How much does an elephant weigh?","The provided context snippets from Amazon's 2023 Sustainability Report mention that wild Asian elephants live in the Western Ghats (a UNESCO World Heritage Site and wildlife conservation zone in India), but do not provide any information about how much an elephant weighs.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The provided context snippets from Amazon's 2023 Sustainability Report mention that wild Asian elephants live in the Western Ghats (a UNESCO World Heritage Site and wildlife conservation zone in India), but do not provide any information about how much an elephant weighs."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context does not contain specific energy consumption data for the five large NLP DNNs mentioned in the question: Meena, T5, GPT-3, GShard-600B, or Switch Transformer. While various tables and figures show energy consumption for other models like LLaMA, GPT-4 variants, DeepSeek, and BERT, there is insufficient information to compare the specific models requested.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not contain specific energy consumption data for the five large NLP DNNs mentioned in the question: Meena, T5, GPT-3, GShard-600B, or Switch Transformer. While various tables and figures show energy consumption for other models like LLaMA, GPT-4 variants, DeepSeek, and BERT, there is insufficient information to compare the specific models requested."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","According to Strubell et al. (2019), training the BERT base model resulted in 626,155 pounds of CO₂ emissions. When contextualizing this in terms of an average American life, the same study states that an average American life produces 36,156 pounds of CO₂ per year. Dividing 626,155 by 36,156 gives approximately 17.32 years of emissions, which converts to about 6,321 days.","6321","days","[""strubell2019""]","is_blank","is_blank","According to Strubell et al. (2019), training the BERT base model resulted in 626,155 pounds of CO₂ emissions. When contextualizing this in terms of an average American life, the same study states that an average American life produces 36,156 pounds of CO₂ per year. Dividing 626,155 by 36,156 gives approximately 17.32 years of emissions, which converts to about 6,321 days."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The question asks about a dataset with 5,842 labeled entries used to test energy-efficient language models in finance, but none of the provided context snippets mention such a dataset. The context discusses various datasets like BurstGPT, Azure Chat, Azure Code, and classical NLP tasks, but there is no specific mention of a financial domain dataset with exactly 5,842 entries.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The question asks about a dataset with 5,842 labeled entries used to test energy-efficient language models in finance, but none of the provided context snippets mention such a dataset. The context discusses various datasets like BurstGPT, Azure Chat, Azure Code, and classical NLP tasks, but there is no specific mention of a financial domain dataset with exactly 5,842 entries."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","According to the context, the authors demonstrate that using eight T4 spot instances in a distributed, decentralized way can be more cost-efficient than a DGX-2 node from the same cloud provider, specifically for CV (computer vision) models. For the tested CV model, 8xT4 was 58% cheaper than DGX-2, though 37% slower.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, the authors demonstrate that using eight T4 spot instances in a distributed, decentralized way can be more cost-efficient than a DGX-2 node from the same cloud provider, specifically for CV (computer vision) models. For the tested CV model, 8xT4 was 58% cheaper than DGX-2, though 37% slower."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context, the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage. This is explicitly stated in document luccioni2025b.","1","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage. This is explicitly stated in document luccioni2025b."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to section 4.3 of the document, Germany's 2023 Energy Efficiency Act 'sets targets on energy efficiency and renewable energy use, requiring data centers [...] to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).' This clearly confirms the statement is true.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to section 4.3 of the document, Germany's 2023 Energy Efficiency Act 'sets targets on energy efficiency and renewable energy use, requiring data centers [...] to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).' This clearly confirms the statement is true."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to the study by Schwartz et al., they sampled 60 papers from top AI conferences (ACL, NeurIPS, and CVPR) and categorized them based on whether they targeted accuracy, efficiency, both, or other. The text specifically states that out of the papers from ACL (which is one subset of the 60 total papers), only a small portion (10%) argue for both accuracy and efficiency.","is_blank","papers","[""schwartz2019""]","is_blank","is_blank","According to the study by Schwartz et al., they sampled 60 papers from top AI conferences (ACL, NeurIPS, and CVPR) and categorized them based on whether they targeted accuracy, efficiency, both, or other. The text specifically states that out of the papers from ACL (which is one subset of the 60 total papers), only a small portion (10%) argue for both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","According to the document jegham2025, recent estimates suggest that inference can account for up to 90% of a model's total lifecycle energy use. This information is directly stated in the context with reference to studies [14, 15].","90","percent","[""jegham2025""]","is_blank","is_blank","According to the document jegham2025, recent estimates suggest that inference can account for up to 90% of a model's total lifecycle energy use. This information is directly stated in the context with reference to studies [14, 15]."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase. It specifically notes that while Article 53(1)(a) requires reporting energy consumption during model development, there is 'a significant gap as it only covers the energy used during the model's development phase, but leaves out the inference phase'.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase. It specifically notes that while Article 53(1)(a) requires reporting energy consumption during model development, there is 'a significant gap as it only covers the energy used during the model's development phase, but leaves out the inference phase'."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","According to the document, the AI Act does not currently mandate the disclosure of energy consumption during the inference phase. This is explicitly stated as ""a crucial omission given the long-term environmental impact of AI applications"" and is one of the authors' policy proposals to include energy consumption from both single and cumulative inferences in reporting.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the document, the AI Act does not currently mandate the disclosure of energy consumption during the inference phase. This is explicitly stated as ""a crucial omission given the long-term environmental impact of AI applications"" and is one of the authors' policy proposals to include energy consumption from both single and cumulative inferences in reporting."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","According to the context, specifically from reference li2025b, ""new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities."" This directly contradicts the statement that they rely on air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","According to the context, specifically from reference li2025b, ""new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities."" This directly contradicts the statement that they rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to the context from Wu et al. (2021), platform-level caching improved power efficiency by 6.7× for the cross-lingual Transformer language model. This is explicitly stated in the section discussing optimization benefits where they note 'application-level caching improves power efficiency by 6.7×' by pre-computing and caching frequently accessed embeddings.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context from Wu et al. (2021), platform-level caching improved power efficiency by 6.7× for the cross-lingual Transformer language model. This is explicitly stated in the section discussing optimization benefits where they note 'application-level caching improves power efficiency by 6.7×' by pre-computing and caching frequently accessed embeddings."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to Strubell et al. (2019), training a BERT base model on 64 V100 GPUs for 79 hours produces approximately 1438 pounds of CO2 emissions. This is explicitly stated in their Table 3, which provides the estimated cost of training models in terms of CO2 emissions and cloud compute cost.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to Strubell et al. (2019), training a BERT base model on 64 V100 GPUs for 79 hours produces approximately 1438 pounds of CO2 emissions. This is explicitly stated in their Table 3, which provides the estimated cost of training models in terms of CO2 emissions and cloud compute cost."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","Multiple sources in the context consistently report that ML inference accounts for 80-90% of the total ML compute demand. This is specifically mentioned in the papers by Chung et al., Luccioni et al., and Fernandez et al., all citing sources like AWS cloud computing demand figures.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","Multiple sources in the context consistently report that ML inference accounts for 80-90% of the total ML compute demand. This is specifically mentioned in the papers by Chung et al., Luccioni et al., and Fernandez et al., all citing sources like AWS cloud computing demand figures."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to the context, training a 6.1 billion parameter language model to completion would consume approximately 103,500 kWh of electricity. This value represents the complete training energy requirement for a large language model of this size.","103500","household-years","[""dodge2022""]","is_blank","is_blank","According to the context, training a 6.1 billion parameter language model to completion would consume approximately 103,500 kWh of electricity. This value represents the complete training energy requirement for a large language model of this size."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","According to the context in document erben2023, for NLP experiments in geo-distributed settings (specifically the C-8 experiment with VMs in four continents), the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context in document erben2023, for NLP experiments in geo-distributed settings (specifically the C-8 experiment with VMs in four continents), the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours on a cluster containing 12 nodes and 96 H100s. To calculate the wall-clock time, we need to divide the total GPU hours by the number of GPUs used: 30,000 H100 GPU hours ÷ 96 H100 GPUs = 312.5 hours or approximately 13 days.","13","days","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours on a cluster containing 12 nodes and 96 H100s. To calculate the wall-clock time, we need to divide the total GPU hours by the number of GPUs used: 30,000 H100 GPU hours ÷ 96 H100 GPUs = 312.5 hours or approximately 13 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","According to the provided context, the term for the amount of water evaporated, transpired, or incorporated into products is 'water consumption', which is defined as 'water withdrawal minus water discharge'.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","According to the provided context, the term for the amount of water evaporated, transpired, or incorporated into products is 'water consumption', which is defined as 'water withdrawal minus water discharge'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the context, the energy per second for inference with LLaMA-65B ranges from 300 Watts to 1 Kilowatt, varying based on the GPU shard configuration, with lower energy consumption (300W) for the 8 GPU configuration and higher energy consumption (1kW) for the 32 GPU configuration.","300 W to 1000 W","W","[""samsi2024""]","is_blank","is_blank","According to the context, the energy per second for inference with LLaMA-65B ranges from 300 Watts to 1 Kilowatt, varying based on the GPU shard configuration, with lower energy consumption (300W) for the 8 GPU configuration and higher energy consumption (1kW) for the 32 GPU configuration."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context snippet from [ref_id=zschache2025], when comparing Qwen models in zero-shot classification, the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with only a minor accuracy reduction.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context snippet from [ref_id=zschache2025], when comparing Qwen models in zero-shot classification, the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with only a minor accuracy reduction."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","According to Table III in the paper by Khan et al. (2025), Qwen's carbon emissions decreased from 0.009 kg before optimization to 0.004 kg after applying quantization and local inference techniques for sentiment classification tasks. This represents a reduction of about 55.6%.","55.6","percent","[""khan2025""]","is_blank","is_blank","According to Table III in the paper by Khan et al. (2025), Qwen's carbon emissions decreased from 0.009 kg before optimization to 0.004 kg after applying quantization and local inference techniques for sentiment classification tasks. This represents a reduction of about 55.6%."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the context provided, specifically from ref_id=chung2025, the early 2025 iteration of the ML.ENERGY Benchmark included 'energy measurements of 40 widely used model architectures across 6 different tasks'.","40","models","[""chung2025""]","is_blank","is_blank","According to the context provided, specifically from ref_id=chung2025, the early 2025 iteration of the ML.ENERGY Benchmark included 'energy measurements of 40 widely used model architectures across 6 different tasks'."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","According to the provided context, when training a Llama-3.1 scale model in Iowa, the health cost 'will increase dramatically to $2.5 million' due to factors such as wind direction and pollutant emission rates for electricity generation. This specific figure is clearly stated in the document.","2500000","USD","[""han2024""]","is_blank","is_blank","According to the provided context, when training a Llama-3.1 scale model in Iowa, the health cost 'will increase dramatically to $2.5 million' due to factors such as wind direction and pollutant emission rates for electricity generation. This specific figure is clearly stated in the document."
