"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","","","is_blank","[]","is_blank","is_blank",""
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context does not provide specific information on the net CO2e emissions from training the GShard-600B model.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the net CO2e emissions from training the GShard-600B model."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides a table listing the model parameters for LLaMA-33B, including its size in gigabytes.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context provides a table listing the model parameters for LLaMA-33B, including its size in gigabytes."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that hyperscale data centers have more than 40% higher efficiency compared to traditional data centers in 2020.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that hyperscale data centers have more than 40% higher efficiency compared to traditional data centers in 2020."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses.","10 to 50","500 mL bottles","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context provides the percentages of CVPR papers that target accuracy (75%) and efficiency (20%). The difference between these percentages is 55%.","55","percent","[""schwartz2019"", ""schwartz2019""]","is_blank","is_blank","The context provides the percentages of CVPR papers that target accuracy (75%) and efficiency (20%). The difference between these percentages is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","","","is_blank","[]","is_blank","is_blank",""
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context explicitly states that for a GPU memory capacity of 100GB, the projected maximum batch size for fine-tuning Mixtral is 28 samples.","28","samples","[""xia2024"", ""xia2024"", ""xia2024"", ""xia2024""]","is_blank","is_blank","The context explicitly states that for a GPU memory capacity of 100GB, the projected maximum batch size for fine-tuning Mixtral is 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 when compared to the V100.","2","multiplier","[""samsi2024"", ""samsi2024""]","is_blank","is_blank","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 when compared to the V100."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is 16.904 million liters.","16.904","liters","[""li2025b""]","is_blank","is_blank","The context provides the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is 16.904 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context explicitly states that sustainability impact assessments (SIAs) should apply to all AI systems, not just high-risk ones.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that sustainability impact assessments (SIAs) should apply to all AI systems, not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context clearly states that local inference optimization reduces both network overhead and carbon footprint by allowing models to run directly on user devices, thereby minimizing data transmission between clients and remote servers.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context clearly states that local inference optimization reduces both network overhead and carbon footprint by allowing models to run directly on user devices, thereby minimizing data transmission between clients and remote servers."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the importance of tracking training time for estimating costs and comparing models, which supports the idea that tracking runtime is important for estimating compute cost.","1","is_blank","[""strubell2019""]","is_blank","is_blank","The context discusses the importance of tracking training time for estimating costs and comparing models, which supports the idea that tracking runtime is important for estimating compute cost."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","","","percent","[]","is_blank","is_blank",""
"q164","How much does an elephant weigh?","The context does not provide specific information about the weight of an elephant.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides energy consumption data for various models, including GPT-3, but does not include data for Meena, T5, GShard-600B, or Switch Transformer, making it impossible to determine which has the highest energy consumption among the listed models.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context provides energy consumption data for various models, including GPT-3, but does not include data for Meena, T5, GShard-600B, or Switch Transformer, making it impossible to determine which has the highest energy consumption among the listed models."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides a direct comparison between the CO2 emissions from training BERT and the average annual CO2 emissions of an American life.","1.72","days","[""strubell2019""]","is_blank","is_blank","The context provides a direct comparison between the CO2 emissions from training BERT and the average annual CO2 emissions of an American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide sufficient information to determine whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide sufficient information to determine whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not provide specific information about a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context states that leveraging multiple spot instances with one T4 GPU each can be more cost-efficient than a DGX-2 node.","1","is_blank","[""erben2023"", ""erben2023""]","is_blank","is_blank","The context states that leveraging multiple spot instances with one T4 GPU each can be more cost-efficient than a DGX-2 node."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context explicitly states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing that factor to 100% by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing that factor to 100% by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context mentions that only a small portion of ACL papers (10%) argue for a new efficiency result, indicating that very few papers target both accuracy and efficiency.","6","papers","[""schwartz2019""]","is_blank","is_blank","The context mentions that only a small portion of ACL papers (10%) argue for a new efficiency result, indicating that very few papers target both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context provides multiple references indicating that inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025"", ""luccioni2024"", ""chung2025""]","is_blank","is_blank","The context provides multiple references indicating that inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","","","is_blank","[]","is_blank","is_blank",""
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","","","is_blank","[]","is_blank","is_blank",""
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","Error: Read timeout on endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-2%3A183295408236%3Aapplication-inference-profile%2Fuajyxwktl068/invoke""","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Read timeout on endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-2%3A183295408236%3Aapplication-inference-profile%2Fuajyxwktl068/invoke"""
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context snippet explicitly states that platform-level caching improves power efficiency by 6.7× for the cross-lingual Transformer language model.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context snippet explicitly states that platform-level caching improves power efficiency by 6.7× for the cross-lingual Transformer language model."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table (Table 3) that lists the estimated CO2 emissions for training various models, including the BERT base model trained on 64 V100 GPUs for 79 hours, which is stated to emit 1438 lbs of CO2.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table (Table 3) that lists the estimated CO2 emissions for training various models, including the BERT base model trained on 64 V100 GPUs for 79 hours, which is stated to emit 1438 lbs of CO2."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context mentions that ML inference reportedly accounts for 80–90% of the total compute demand.","80–90%","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","The context mentions that ML inference reportedly accounts for 80–90% of the total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides a detailed breakdown of the energy consumption for training various models, including a 6 billion parameter transformer. It estimates that training this model to completion would consume approximately 103,593 kWh. Given that the average U.S. household consumes about 10,649 kWh per year, this equates to roughly 9.73 household-years of electricity consumption.","9.73","household-years","[""dodge2022""]","is_blank","is_blank","The context provides a detailed breakdown of the energy consumption for training various models, including a 6 billion parameter transformer. It estimates that training this model to completion would consume approximately 103,593 kWh. Given that the average U.S. household consumes about 10,649 kWh per year, this equates to roughly 9.73 household-years of electricity consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly states that for NLP experiments, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This indicates that egress costs can indeed account for more than 90% of the total cost per VM in geo-distributed NLP experiments.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context explicitly states that for NLP experiments, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This indicates that egress costs can indeed account for more than 90% of the total cost per VM in geo-distributed NLP experiments."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides the total pre-training GPU hours (30,000) and the number of GPUs used (96). To estimate the total wall-clock time in days, we divide the total GPU hours by the number of GPUs and then convert hours to days.","31.25","days","[""shen2024"", ""shen2024""]","is_blank","is_blank","The context provides the total pre-training GPU hours (30,000) and the number of GPUs used (96). To estimate the total wall-clock time in days, we divide the total GPU hours by the number of GPUs and then convert hours to days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as 'water withdrawal minus water discharge', which includes the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context defines water consumption as 'water withdrawal minus water discharge', which includes the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","","","W","[]","is_blank","is_blank",""
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B).","7","multiplier","[""zschache2025"", ""zschache2025""]","is_blank","is_blank","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B)."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context provides a table comparing carbon emissions before and after optimization for various models, including Qwen. The reduction in carbon emissions for Qwen can be calculated by comparing the CO2 values before and after optimization.","55.6","percent","[""khan2025""]","is_blank","is_blank","The context provides a table comparing carbon emissions before and after optimization for various models, including Qwen. The reduction in carbon emissions for Qwen can be calculated by comparing the CO2 values before and after optimization."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context explicitly states that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks.","40","models","[""chung2025""]","is_blank","is_blank","The context explicitly states that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","","","USD","[]","is_blank","is_blank",""
