"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","According to the context, U.S. data center electricity consumption increased by 4% from 2010-2014, as directly stated in multiple snippets from wu2021b.","4","percent","[""wu2021b""]","is_blank","is_blank","According to the context, U.S. data center electricity consumption increased by 4% from 2010-2014, as directly stated in multiple snippets from wu2021b."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the context, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is explicitly stated to be equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","According to the context, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is explicitly stated to be equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, representing an increase of 4 data centers.","4","data centers","[""amazon2023""]","is_blank","is_blank","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, representing an increase of 4 data centers."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to the context, NVIDIA does not release embodied carbon emissions data for their hardware. While some researchers estimate it at 463 kg per GPU based on certain assumptions, there is explicit acknowledgment that the environmental impact of manufacturing GPUs is 'essentially unknown' and lacks transparency.","is_blank","kg/GPU","[""morrison2025"", ""luccioni2025b""]","is_blank","is_blank","According to the context, NVIDIA does not release embodied carbon emissions data for their hardware. While some researchers estimate it at 463 kg per GPU based on certain assumptions, there is explicit acknowledgment that the environmental impact of manufacturing GPUs is 'essentially unknown' and lacks transparency."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","According to the context, GPT-4's amortized hardware and energy cost was $40M, while FLM-101B was trained within a $100,000 budget. This means GPT-4's estimated training cost was 400 times greater than FLM-101B's total training budget.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","According to the context, GPT-4's amortized hardware and energy cost was $40M, while FLM-101B was trained within a $100,000 budget. This means GPT-4's estimated training cost was 400 times greater than FLM-101B's total training budget."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","According to Strubell 2019, a one-passenger round trip flight between New York and San Francisco produces 1,984 lbs CO2e.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","According to Strubell 2019, a one-passenger round trip flight between New York and San Francisco produces 1,984 lbs CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","According to the context, when evaluated on the Open LLM Leaderboard, FLM-101B achieved an average score of 43.94 across the four tasks (ARC, HellaSwag, MMLU, and TruthfulQA), reaching over 90% of GLM-130B's performance while using 7 times fewer FLOPs.","43.94","score","[""li2025a""]","is_blank","is_blank","According to the context, when evaluated on the Open LLM Leaderboard, FLM-101B achieved an average score of 43.94 across the four tasks (ARC, HellaSwag, MMLU, and TruthfulQA), reaching over 90% of GLM-130B's performance while using 7 times fewer FLOPs."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","According to the context, the Intel 4004 from 1971 ran at 740kHz, while typical 2021 microprocessors run at 5,000,000kHz, representing a 6,750-fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","is_blank","is_blank","According to the context, the Intel 4004 from 1971 ran at 740kHz, while typical 2021 microprocessors run at 5,000,000kHz, representing a 6,750-fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to the context, GPT-3's total training time was 14.8 days using 10,000 GPUs. This figure is directly stated in the cited document.","14.8","days","[""luccioni2023""]","is_blank","is_blank","According to the context, GPT-3's total training time was 14.8 days using 10,000 GPUs. This figure is directly stated in the cited document."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","According to Table 3 in morrison2025, the GPU Power Usage for Llama 3.2 1B model at 8 requests/s frequency is 0.036 kWh when benchmarking on 2400 prompts from ShareGPT.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","According to Table 3 in morrison2025, the GPU Power Usage for Llama 3.2 1B model at 8 requests/s frequency is 0.036 kWh when benchmarking on 2400 prompts from ShareGPT."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","According to the context, the total permitted annual emission limits for diesel generators at data centers in northern Virginia were approximately 13,000 tons of NOx between January 1, 2023 and December 1, 2024. This is explicitly stated in the context which discusses emission limits for data center backup generators in that region.","13000","tons","[""han2024""]","is_blank","is_blank","According to the context, the total permitted annual emission limits for diesel generators at data centers in northern Virginia were approximately 13,000 tons of NOx between January 1, 2023 and December 1, 2024. This is explicitly stated in the context which discusses emission limits for data center backup generators in that region."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","According to the paper, using the growth strategy for training FLM-101B took 21.54 days compared to an estimated 76.74 days for training from scratch, resulting in a 72% time savings.","72","percent","[""li2025a""]","is_blank","is_blank","According to the paper, using the growth strategy for training FLM-101B took 21.54 days compared to an estimated 76.74 days for training from scratch, resulting in a 72% time savings."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","According to the context, based on the EPA's COBRA modeling tool analysis, U.S. data centers are projected to contribute to approximately 1,300 premature deaths in 2028 through scope-2 pollutants from electricity generation.","1300","deaths","[""han2024""]","is_blank","is_blank","According to the context, based on the EPA's COBRA modeling tool analysis, U.S. data centers are projected to contribute to approximately 1,300 premature deaths in 2028 through scope-2 pollutants from electricity generation."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to Dodge et al. (2022), their 6 billion parameter transformer model was trained for approximately 13% of the full training time, and they estimate a full training run would take 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","According to Dodge et al. (2022), their 6 billion parameter transformer model was trained for approximately 13% of the full training time, and they estimate a full training run would take 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","Based on the provided context, I cannot definitively determine which model had the highest GPU energy consumption for 1,000 queries among those listed in a 2025 study appendix. While several papers from 2025 discuss energy consumption of various models, none specifically compare GPU energy usage for exactly 1,000 queries in an appendix.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context, I cannot definitively determine which model had the highest GPU energy consumption for 1,000 queries among those listed in a 2025 study appendix. While several papers from 2025 discuss energy consumption of various models, none specifically compare GPU energy usage for exactly 1,000 queries in an appendix."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","According to the context, the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as explicitly stated in multiple instances throughout the document.","2014","year","[""stone2022""]","is_blank","is_blank","According to the context, the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as explicitly stated in multiple instances throughout the document."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024, approximately 22% of e-waste has been formally collected and recycled, while the rate of global e-waste generation is increasing five times faster than e-waste recycling.","22","percent","[""luccioni2025a""]","is_blank","is_blank","According to the UN's Global E-Waste Monitor 2024, approximately 22% of e-waste has been formally collected and recycled, while the rate of global e-waste generation is increasing five times faster than e-waste recycling."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","According to the context, for the full BLOOM model project, the final training run for the 176B parameter model used 37.24% of the total energy. Since we know from [ref_id=cottier2024] that the final training run consumed 37.24% of the total BLOOM project energy, we cannot definitively determine the exact total energy consumption of pre-training the BLOOM model from the provided context snippets.","is_blank","MWh","[""cottier2024""]","is_blank","is_blank","According to the context, for the full BLOOM model project, the final training run for the 176B parameter model used 37.24% of the total energy. Since we know from [ref_id=cottier2024] that the final training run consumed 37.24% of the total BLOOM project energy, we cannot definitively determine the exact total energy consumption of pre-training the BLOOM model from the provided context snippets."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context discusses the Switch Transformer having 1.5 trillion parameters, but does not explicitly state what percentage of parameters are activated per token during inference.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context discusses the Switch Transformer having 1.5 trillion parameters, but does not explicitly state what percentage of parameters are activated per token during inference."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to Table 1 in the context, JetMoE-8B has 8 experts (Nexperts=8) in each MoE layer. This is explicitly shown in the hyperparameters table and mentioned in the text where they 'set the same number of experts to 8 and top-k to 2 for every layer.'","8","experts","[""shen2024""]","is_blank","is_blank","According to Table 1 in the context, JetMoE-8B has 8 experts (Nexperts=8) in each MoE layer. This is explicitly shown in the hyperparameters table and mentioned in the text where they 'set the same number of experts to 8 and top-k to 2 for every layer.'"
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","Based on Figure 4 in the context, for a dense BlackMamba model with batch size of 30, the total execution time (combining Forward, Backward and Optimizer stages) is approximately 1.5 seconds.","1.5","second","[""xia2024""]","is_blank","is_blank","Based on Figure 4 in the context, for a dense BlackMamba model with batch size of 30, the total execution time (combining Forward, Backward and Optimizer stages) is approximately 1.5 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the context, the English portion of FLM-101B's training required 28.22 zettaFLOPs, which is part of the total cost of 52.76 zettaFLOPs (split between 28.22 zettaFLOPs for English and 24.54 for Chinese).","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","According to the context, the English portion of FLM-101B's training required 28.22 zettaFLOPs, which is part of the total cost of 52.76 zettaFLOPs (split between 28.22 zettaFLOPs for English and 24.54 for Chinese)."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","Based on the context from khan2025, the experimental setup used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency) for financial sentiment classification tasks.","Intel Core i7-1165G7","is_blank","[""khan2025""]","is_blank","is_blank","Based on the context from khan2025, the experimental setup used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency) for financial sentiment classification tasks."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","Based on the provided context documents, the number of models sampled and analyzed in the 'Power Hungry Processing' (2024) study is not explicitly stated. While the study compared task-specific and general-purpose models for inference costs, the exact number of models analyzed is not mentioned in any of the context snippets.","is_blank","models","[""is_blank""]","is_blank","is_blank","Based on the provided context documents, the number of models sampled and analyzed in the 'Power Hungry Processing' (2024) study is not explicitly stated. While the study compared task-specific and general-purpose models for inference costs, the exact number of models analyzed is not mentioned in any of the context snippets."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the context, increasing GPU utilization up to 80% decreases the overall carbon footprint by a factor of 3× (which means it decreases by 3 times or to 1/3 of the original footprint).","3","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context, increasing GPU utilization up to 80% decreases the overall carbon footprint by a factor of 3× (which means it decreases by 3 times or to 1/3 of the original footprint)."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to Cottier et al. (2024), they sampled from a log-normal distribution with a 90% confidence interval of 1.2x to 4x, meaning the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x.","1.2-4","multiplier","[""cottier2024""]","is_blank","is_blank","According to Cottier et al. (2024), they sampled from a log-normal distribution with a 90% confidence interval of 1.2x to 4x, meaning the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","According to the context, a full training run of the 6.1 billion parameter transformer model would consume approximately 103.5 MWh of energy. This is explicitly stated in the text, which notes that while their initial run only reached 13% completion using 13.8 MWh, they estimate a full training run would require (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","According to the context, a full training run of the 6.1 billion parameter transformer model would consume approximately 103.5 MWh of energy. This is explicitly stated in the text, which notes that while their initial run only reached 13% completion using 13.8 MWh, they estimate a full training run would require (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The 2025 paper by Luccioni et al. focuses on how Jevons' Paradox applies to AI, where efficiency gains in technology paradoxically lead to increased consumption rather than decreased resource use. This economic principle is directly referenced and explained in the context of AI's environmental impacts.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni et al. focuses on how Jevons' Paradox applies to AI, where efficiency gains in technology paradoxically lead to increased consumption rather than decreased resource use. This economic principle is directly referenced and explained in the context of AI's environmental impacts."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to multiple mentions in the context, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027, which is equivalent to the total annual water withdrawal of 4-6 Denmark or half of the United Kingdom.","4.2-6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","According to multiple mentions in the context, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027, which is equivalent to the total annual water withdrawal of 4-6 Denmark or half of the United Kingdom."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states multiple times that 'Red AI is on the rise despite the well-known diminishing returns of increased cost' and shows that computational costs are increasing exponentially. Far from declining, Red AI approaches continue to grow despite their known inefficiencies.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context explicitly states multiple times that 'Red AI is on the rise despite the well-known diminishing returns of increased cost' and shows that computational costs are increasing exponentially. Far from declining, Red AI approaches continue to grow despite their known inefficiencies."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","According to the context, using a growth strategy, the total time cost for training FLM-101B was 21.54 days, which represented a 72% time savings compared to training a 101B model from scratch.","21.54","days","[""li2025a""]","is_blank","is_blank","According to the context, using a growth strategy, the total time cost for training FLM-101B was 21.54 days, which represented a 72% time savings compared to training a 101B model from scratch."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","According to the context from wu2021a, a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, not over 80%. This is explicitly stated multiple times in the text.","0","is_blank","[""wu2021a""]","is_blank","is_blank","According to the context from wu2021a, a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, not over 80%. This is explicitly stated multiple times in the text."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","Multiple sources consistently state that GPT-3's training consumed 1,287 MWh of electricity. This is explicitly mentioned in both li2025b and jegham2025.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","Multiple sources consistently state that GPT-3's training consumed 1,287 MWh of electricity. This is explicitly mentioned in both li2025b and jegham2025."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","According to the context, the AI Energy Score is a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This is directly stated in multiple instances, particularly in providing a standardized methodology for comparing models across different tasks.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the context, the AI Energy Score is a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This is directly stated in multiple instances, particularly in providing a standardized methodology for comparing models across different tasks."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","Looking at Figure 6 in document xia2024, for BlackMamba with dense fine-tuning and batch size of 30, the longest kernel execution time in the MoE layer appears to be about 1600 microseconds for the matmul(w1) operation.","1600","microseconds","[""xia2024""]","is_blank","is_blank","Looking at Figure 6 in document xia2024, for BlackMamba with dense fine-tuning and batch size of 30, the longest kernel execution time in the MoE layer appears to be about 1600 microseconds for the matmul(w1) operation."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","According to the hyperparameters table and descriptions in the context, JetMoE-8B has a top-k value of 2 for every layer, meaning 2 experts are selected for activation per token in each layer.","2","experts","[""shen2024""]","is_blank","is_blank","According to the hyperparameters table and descriptions in the context, JetMoE-8B has a top-k value of 2 for every layer, meaning 2 experts are selected for activation per token in each layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","According to the paper by Schwartz et al. (2019), deep learning computations have increased by an estimated 300,000x from 2012 to 2018, which directly matches the claim in the question.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","According to the paper by Schwartz et al. (2019), deep learning computations have increased by an estimated 300,000x from 2012 to 2018, which directly matches the claim in the question."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction.","6.4","percent","[""wu2021b""]","is_blank","is_blank","According to the context, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the 2023 Amazon Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","is_blank","According to the 2023 Amazon Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","According to the context, the field of AI was officially born at a 1956 workshop, meaning it will be approximately 69 years old in 2025.","69","years","[""stone2022""]","is_blank","is_blank","According to the context, the field of AI was officially born at a 1956 workshop, meaning it will be approximately 69 years old in 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","According to the 2019 Strubell study, the 'five cars' estimate comes from neural architecture search (NAS), which is a large-scale procedure performed infrequently to identify optimal model architectures. This process yielded 284 metric tons of CO2 emissions, equivalent to five cars' lifetime emissions.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the 2019 Strubell study, the 'five cars' estimate comes from neural architecture search (NAS), which is a large-scale procedure performed infrequently to identify optimal model architectures. This process yielded 284 metric tons of CO2 emissions, equivalent to five cars' lifetime emissions."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context, targeting an average TPOT of 100ms for Llama 3.1 8B reduces energy consumption by 44% compared to the configuration that minimizes latency.","44","percent","[""chung2025""]","is_blank","is_blank","According to the context, targeting an average TPOT of 100ms for Llama 3.1 8B reduces energy consumption by 44% compared to the configuration that minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the context, for BlackMamba with sparse setup (BlackMamba-S) on GSM8K dataset (GS), the maximum batch size supported is 20 samples when using the NVIDIA A40 GPU with 48GB memory.","20","samples","[""xia2024""]","is_blank","is_blank","According to Table III in the context, for BlackMamba with sparse setup (BlackMamba-S) on GSM8K dataset (GS), the maximum batch size supported is 20 samples when using the NVIDIA A40 GPU with 48GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to multiple references in the 2023 Amazon Sustainability Report, Amazon held 28 gigawatts (GW) of renewable energy capacity in 2023, making them the world's largest corporate purchaser of renewable energy for the fourth consecutive year.","28","GW","[""amazon2023""]","is_blank","is_blank","According to multiple references in the 2023 Amazon Sustainability Report, Amazon held 28 gigawatts (GW) of renewable energy capacity in 2023, making them the world's largest corporate purchaser of renewable energy for the fourth consecutive year."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","According to the context, GPT-4o's projected annual carbon emissions for 2025 are comparable to approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","According to the context, GPT-4o's projected annual carbon emissions for 2025 are comparable to approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","While the context provides information about AI workload energy consumption and data center power sources, it does not specifically state what percentage of AI inference workloads in Asia were powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","While the context provides information about AI workload energy consumption and data center power sources, it does not specifically state what percentage of AI inference workloads in Asia were powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","According to multiple sources in the context, the global average PUE for data centers in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","According to multiple sources in the context, the global average PUE for data centers in 2023 was 1.58."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","According to multiple sections of the context, JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token. This is explicitly stated multiple times and is a key feature of the model's sparse activation architecture.","2000000000","parameters","[""shen2024""]","is_blank","is_blank","According to multiple sections of the context, JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token. This is explicitly stated multiple times and is a key feature of the model's sparse activation architecture."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","According to multiple sources including a data table in Luccioni 2025, the GHG emissions from pre-training the Llama 7B model were 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to multiple sources including a data table in Luccioni 2025, the GHG emissions from pre-training the Llama 7B model were 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","According to the context, Amazon's U.S. fleet increased from 2,600 electric delivery vans in 2022 to 11,800 in 2022 (+9,200). Adding the European fleet increase from 1,220 to 3,000+ (+1,780) and the Indian fleet increase from 3,800 to 7,200+ (+3,400), the total increase across these regions was approximately 14,380 electric delivery vans.","14380","electric delivery vans","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon's U.S. fleet increased from 2,600 electric delivery vans in 2022 to 11,800 in 2022 (+9,200). Adding the European fleet increase from 1,220 to 3,000+ (+1,780) and the Indian fleet increase from 3,800 to 7,200+ (+3,400), the total increase across these regions was approximately 14,380 electric delivery vans."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","According to the context, operational environmental impacts of LLMs explicitly include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","is_blank","According to the context, operational environmental impacts of LLMs explicitly include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in the context, the o3 model consumes 12.222 ± 1.082 Wh for long prompts (10k input-1.5k output).","12.222","Wh","[""jegham2025""]","is_blank","is_blank","According to Table 4 in the context, the o3 model consumes 12.222 ± 1.082 Wh for long prompts (10k input-1.5k output)."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","is_blank","The context clearly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The provided context does not contain specific information about the water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the documents discuss water usage in data centers and AI systems generally, they do not provide this specific metric for Google's AI data centers in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While the documents discuss water usage in data centers and AI systems generally, they do not provide this specific metric for Google's AI data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity', according to the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context explicitly states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity', according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, for LLaMA-65B at maximum generation length 512, it takes about 3-4 Joules per output token.","3.5","joules per token","[""samsi2024""]","is_blank","is_blank","According to the context, for LLaMA-65B at maximum generation length 512, it takes about 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","According to the context, by converting Facebook's RM2 model from 32-bit floating-point to 16-bit numerical representation, the overall model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","According to the context, by converting Facebook's RM2 model from 32-bit floating-point to 16-bit numerical representation, the overall model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","According to the context, the widely cited 5-10% GHG reduction claim originated from BCG reports but lacks clear calculations and scientific grounding. The documents indicate the reasoning is unclear and based only on BCG's client experience, without detailed calculations or consideration of necessary methodological requirements like counterfactual scenarios and rebound effects.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the context, the widely cited 5-10% GHG reduction claim originated from BCG reports but lacks clear calculations and scientific grounding. The documents indicate the reasoning is unclear and based only on BCG's client experience, without detailed calculations or consideration of necessary methodological requirements like counterfactual scenarios and rebound effects."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","While the context discusses energy efficiency of sparse neural networks and mentions various energy optimization approaches, it does not specifically claim or quantify that sparsely activated DNNs consume less than 1/10th the energy of dense DNNs. The closest relevant statement is from wu2021a which notes that sparse activation can improve scalability and accuracy at lower operational energy, but does not specify the magnitude of reduction.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","While the context discusses energy efficiency of sparse neural networks and mentions various energy optimization approaches, it does not specifically claim or quantify that sparsely activated DNNs consume less than 1/10th the energy of dense DNNs. The closest relevant statement is from wu2021a which notes that sparse activation can improve scalability and accuracy at lower operational energy, but does not specify the magnitude of reduction."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","According to the Schwartz 2019 paper, Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","According to the Schwartz 2019 paper, Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","According to the context, when conducting sparse fine-tuning with batch size = 1 on BlackMamba, the optimizer stage takes up to 53% of the running time.","53","percent","[""xia2024""]","is_blank","is_blank","According to the context, when conducting sparse fine-tuning with batch size = 1 on BlackMamba, the optimizer stage takes up to 53% of the running time."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","Using the energy consumption of 0.083 kWh per 1,000 queries for Flan-T5-xxl (a comparable multi-purpose model) and Google Translate's daily volume of 1 billion queries, we can calculate that 1 billion * 0.083 = 83 MWh of energy would be consumed daily.","83","MWh","[""luccioni2024""]","is_blank","is_blank","Using the energy consumption of 0.083 kWh per 1,000 queries for Flan-T5-xxl (a comparable multi-purpose model) and Google Translate's daily volume of 1 billion queries, we can calculate that 1 billion * 0.083 = 83 MWh of energy would be consumed daily."
"q067","What was the average global data center PUE in 2023?","According to multiple sources in the context, the global average data center PUE in 2023 was 1.58. This is explicitly stated in [ref_id=ebert2024] which notes 'The average data center PUE in 2023 was 1.58 globally'.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","According to multiple sources in the context, the global average data center PUE in 2023 was 1.58. This is explicitly stated in [ref_id=ebert2024] which notes 'The average data center PUE in 2023 was 1.58 globally'."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","Based on the provided context, there is no specific mention of the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While there are mentions of Microsoft's data centers and sustainability efforts, the number of wind turbines for Azure AI specifically is not discussed.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","Based on the provided context, there is no specific mention of the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While there are mentions of Microsoft's data centers and sustainability efforts, the number of wind turbines for Azure AI specifically is not discussed."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to Cottier et al., Gemini Ultra had the highest fraction of R&D staff cost at 49% of total amortized model development costs when including equity compensation. This figure is considered unusually high among frontier models, partly due to Gemini Ultra being trained on Google's cheaper TPUs.","49","percent","[""cottier2024""]","is_blank","is_blank","According to Cottier et al., Gemini Ultra had the highest fraction of R&D staff cost at 49% of total amortized model development costs when including equity compensation. This figure is considered unusually high among frontier models, partly due to Gemini Ultra being trained on Google's cheaper TPUs."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural 2015 Study Panel consisted of seventeen members, comprised of AI experts from academia, corporate laboratories and industry, as well as AI-savvy scholars in law, political science, policy, and economics.","17","people","[""stone2022""]","is_blank","is_blank","The context clearly states that the inaugural 2015 Study Panel consisted of seventeen members, comprised of AI experts from academia, corporate laboratories and industry, as well as AI-savvy scholars in law, political science, policy, and economics."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices.","74","percent","[""wu2021a""]","is_blank","is_blank","According to the context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","Based on the context, specifically from chung2025, models with more parameters do not always consume more energy during inference. This is demonstrated by the Phi-3 Mini (3.8B) and Small (7B) example, where the larger Small model actually consumes less energy than Mini at higher batch sizes due to more efficient attention mechanisms.","0","is_blank","[""chung2025""]","is_blank","is_blank","Based on the context, specifically from chung2025, models with more parameters do not always consume more energy during inference. This is demonstrated by the Phi-3 Mini (3.8B) and Small (7B) example, where the larger Small model actually consumes less energy than Mini at higher batch sizes due to more efficient attention mechanisms."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel explicitly states that they 'found no cause for concern that AI is an imminent threat to humankind.' They specifically contrast this with 'more fantastic predictions for AI in the popular press.'","0","is_blank","[""stone2022""]","is_blank","is_blank","The Study Panel explicitly states that they 'found no cause for concern that AI is an imminent threat to humankind.' They specifically contrast this with 'more fantastic predictions for AI in the popular press.'"
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context snippets do not provide specific information about OpenAI's API request emissions for January 2024. While some documents discuss emissions from AI systems generally, none provide the specific data point requested.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context snippets do not provide specific information about OpenAI's API request emissions for January 2024. While some documents discuss emissions from AI systems generally, none provide the specific data point requested."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times larger than the 'five cars' estimate from the original Strubell et al. study (which was approximately 284 metric tons CO2e).","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times larger than the 'five cars' estimate from the original Strubell et al. study (which was approximately 284 metric tons CO2e)."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to the context, Facebook's AI training infrastructure capacity increased by 2.9x over the 1.5 year period from Yr1-Q1 to Yr2-Q2, as shown in Figure 2(d) and explicitly stated multiple times in the document.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context, Facebook's AI training infrastructure capacity increased by 2.9x over the 1.5 year period from Yr1-Q1 to Yr2-Q2, as shown in Figure 2(d) and explicitly stated multiple times in the document."
"q079","How many miles is the Earth from the Sun?","The provided context does not contain information about Earth's distance from the Sun. While the context discusses various topics like data center energy usage, AI model training impacts, and vehicle emissions, it does not mention the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context does not contain information about Earth's distance from the Sun. While the context discusses various topics like data center energy usage, AI model training impacts, and vehicle emissions, it does not mention the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","According to multiple mentions in the context, AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is explicitly stated in reference to its achievement being due in large part to reinforcement learning.","1","is_blank","[""stone2022""]","is_blank","is_blank","According to multiple mentions in the context, AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is explicitly stated in reference to its achievement being due in large part to reinforcement learning."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","Based on the context, continuous batching is identified as the strategy that dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time during inference.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","Based on the context, continuous batching is identified as the strategy that dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time during inference."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context clearly states that 'The entire alignment process takes 60 H100 GPU hours' for JetMoE-8B, which includes both dSFT and dDPO fine-tuning steps.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context clearly states that 'The entire alignment process takes 60 H100 GPU hours' for JetMoE-8B, which includes both dSFT and dDPO fine-tuning steps."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to Table VI, for the 100 TPS SLO requirement, the Max-Performance policy selected g6e.xlarge at $2.699, while InferSave's first choice was g4dn.xlarge at $2.13. The percentage difference shows the Max-Performance selection was 26.7% more expensive.","26.7","percent","[""kim2025""]","is_blank","is_blank","According to Table VI, for the 100 TPS SLO requirement, the Max-Performance policy selected g6e.xlarge at $2.699, while InferSave's first choice was g4dn.xlarge at $2.13. The percentage difference shows the Max-Performance selection was 26.7% more expensive."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","According to a 2024 study reported in luccioni2024, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences, which is equivalent to driving 4.1 miles in an average gasoline-powered passenger vehicle.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","According to a 2024 study reported in luccioni2024, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences, which is equivalent to driving 4.1 miles in an average gasoline-powered passenger vehicle."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","According to the context, a 2025 study by Luccioni found that GPU energy usage for 1,000 inference queries ranged from 0.06 Wh for bert-tiny to 3,426 Wh for Command-R Plus, depending on factors like model size, architecture, and task complexity.","0.06-3426","Wh","[""luccioni2025c""]","is_blank","is_blank","According to the context, a 2025 study by Luccioni found that GPU energy usage for 1,000 inference queries ranged from 0.06 Wh for bert-tiny to 3,426 Wh for Command-R Plus, depending on factors like model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states 'There is no one-size-fits-all solution for either ethics or sustainability' and emphasizes the need for nuanced, context-specific approaches to AI ethics that consider varied ethical dimensions and different cultural perspectives.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states 'There is no one-size-fits-all solution for either ethics or sustainability' and emphasizes the need for nuanced, context-specific approaches to AI ethics that consider varied ethical dimensions and different cultural perspectives."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","According to the provided context from luccioni2024, experiments were run in AWS us-west-2 region (Oregon) which had an average carbon intensity of 297.6 grams of CO2eq per kWh.","297.6","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","According to the provided context from luccioni2024, experiments were run in AWS us-west-2 region (Oregon) which had an average carbon intensity of 297.6 grams of CO2eq per kWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","Based on the context, Hivemind was used as the PyTorch-based decentralized framework for distributed spot instance training. The context describes Hivemind as a PyTorch-based framework that enables collaborative distributed training in a decentralized fashion, with features like handling peer dropouts and gradient averaging across nodes.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","Based on the context, Hivemind was used as the PyTorch-based decentralized framework for distributed spot instance training. The context describes Hivemind as a PyTorch-based framework that enables collaborative distributed training in a decentralized fashion, with features like handling peer dropouts and gradient averaging across nodes."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","According to the context, Ehsan et al. proposed expanding transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and includes portraying an AI system's societal impacts, ethical considerations, and environmental footprint.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, Ehsan et al. proposed expanding transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and includes portraying an AI system's societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","Based on the provided context about classification experiments on German public administration texts (FKTG dataset), a traditional linear model using pre-trained sentence embeddings achieved the highest accuracy. This is explicitly stated in the context which mentions 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.'","Linear model with pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","is_blank","Based on the provided context about classification experiments on German public administration texts (FKTG dataset), a traditional linear model using pre-trained sentence embeddings achieved the highest accuracy. This is explicitly stated in the context which mentions 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.'"
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","According to the context, the LLM inference system developed in the Chen et al. 2025 paper is called Lamina, which incorporates model-attention disaggregation in a distributed heterogeneous cluster.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","According to the context, the LLM inference system developed in the Chen et al. 2025 paper is called Lamina, which incorporates model-attention disaggregation in a distributed heterogeneous cluster."
"q093","How many parameters does the largest T5 model have?","Based on the context, the T5 model's parameters are not mentioned in any of the provided documents. While several other models' parameters are discussed (like BERT, GPT-2, OLMo, and Whisper), there is no specific mention of T5's parameter count.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","Based on the context, the T5 model's parameters are not mentioned in any of the provided documents. While several other models' parameters are discussed (like BERT, GPT-2, OLMo, and Whisper), there is no specific mention of T5's parameter count."
"q094","What is the total number of parameters in the JetMoE-8B model?","According to Table 1 in the context, JetMoE-8B has a total parameter count (Ptotal) of 8B parameters, though it only activates 2B parameters for each input token.","8","parameters","[""shen2024""]","is_blank","is_blank","According to Table 1 in the context, JetMoE-8B has a total parameter count (Ptotal) of 8B parameters, though it only activates 2B parameters for each input token."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","According to multiple context documents, Google's data center water consumption increased by 20% from 2021 to 2022, as reported in both company sustainability reports and academic papers.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","According to multiple context documents, Google's data center water consumption increased by 20% from 2021 to 2022, as reported in both company sustainability reports and academic papers."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","According to Table I in Khan 2025, 'Carbon Intensity' is explicitly defined as 'CO₂ emissions per unit of electricity consumed' with units of gCO2/kWh.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","According to Table I in Khan 2025, 'Carbon Intensity' is explicitly defined as 'CO₂ emissions per unit of electricity consumed' with units of gCO2/kWh."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","According to Table 2 in the context, in the final growth stage when FLM-101B reached 101B parameters, the FLOPs utilization was 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","According to Table 2 in the context, in the final growth stage when FLM-101B reached 101B parameters, the FLOPs utilization was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","According to multiple sources, the amortized hardware and energy cost to train GPT-4 was $40M. This is distinct from the total hardware acquisition cost of $800M or the total model development cost of $90M which includes experiments and staff costs.","40000000","USD","[""cottier2024""]","is_blank","is_blank","According to multiple sources, the amortized hardware and energy cost to train GPT-4 was $40M. This is distinct from the total hardware acquisition cost of $800M or the total model development cost of $90M which includes experiments and staff costs."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to the context, full-stack optimization including platform-level caching, GPU acceleration, and algorithmic changes reduced the operational carbon footprint of a Transformer-based universal translation model by 810x compared to a CPU server baseline.","810","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context, full-stack optimization including platform-level caching, GPU acceleration, and algorithmic changes reduced the operational carbon footprint of a Transformer-based universal translation model by 810x compared to a CPU server baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","According to the context, when training was spread across four continents (C-8) instead of remaining local (A-8), NLP throughput was 41% slower compared to the local run, meaning 59% of local throughput was achieved.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","According to the context, when training was spread across four continents (C-8) instead of remaining local (A-8), NLP throughput was 41% slower compared to the local run, meaning 59% of local throughput was achieved."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","According to the context, AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023 through various replenishment activities across 12 global infrastructure regions.","3500000000","liters","[""amazon2023""]","is_blank","is_blank","According to the context, AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023 through various replenishment activities across 12 global infrastructure regions."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","According to the research findings, using custom tags with prompt engineering techniques (zero-shot, one-shot, and few-shots) reduced energy consumption during inference phase of LLMs for code completion tasks. The study showed specific reductions in energy consumption - zero-shot reduced by 7%, one-shot by 99%, and few-shots by 83% when using custom tags compared to baseline configurations.","1","is_blank","[""rubei2025""]","is_blank","is_blank","According to the research findings, using custom tags with prompt engineering techniques (zero-shot, one-shot, and few-shots) reduced energy consumption during inference phase of LLMs for code completion tasks. The study showed specific reductions in energy consumption - zero-shot reduced by 7%, one-shot by 99%, and few-shots by 83% when using custom tags compared to baseline configurations."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","According to the 2025 paper by Luccioni et al., NVIDIA shipped 3.7 million GPUs in 2024, which was reported to be more than a million more units than in 2023.","3700000","GPUs","[""luccioni2025a""]","is_blank","is_blank","According to the 2025 paper by Luccioni et al., NVIDIA shipped 3.7 million GPUs in 2024, which was reported to be more than a million more units than in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to section 3.4 of the document, when breaking down the components of amortized hardware CapEx + energy costs, on average 44% goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","According to section 3.4 of the document, when breaking down the components of amortized hardware CapEx + energy costs, on average 44% goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","Multiple sources consistently report that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of 1.10, which makes them about 40% more efficient than typical data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","Multiple sources consistently report that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of 1.10, which makes them about 40% more efficient than typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish ETAIROS (Ethical AI for the Governance of the Society) project is explicitly mentioned in the context as proposing the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The Finnish ETAIROS (Ethical AI for the Governance of the Society) project is explicitly mentioned in the context as proposing the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the context, Google's Gemini Ultra had an estimated training cost of $30M for the final training run, as explicitly stated in the text.","30000000","USD","[""cottier2024""]","is_blank","is_blank","According to the context, Google's Gemini Ultra had an estimated training cost of $30M for the final training run, as explicitly stated in the text."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","According to the context, while the AI Act mandates risk assessment and mitigation for GPAI models with systemic risk and HRAI systems, these provisions currently lack sufficient emphasis on environmental factors. The Act requires climate risk assessment but no detailed reporting is mandated, and its practical integration into risk management remains unclear.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, while the AI Act mandates risk assessment and mitigation for GPAI models with systemic risk and HRAI systems, these provisions currently lack sufficient emphasis on environmental factors. The Act requires climate risk assessment but no detailed reporting is mandated, and its practical integration into risk management remains unclear."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","Multiple passages from the context clearly state that the EPA's recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³.","9","µg/m³","[""han2024""]","is_blank","is_blank","Multiple passages from the context clearly state that the EPA's recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","According to a life cycle assessment comparing print books to e-readers, one Amazon Kindle device produces the same amount of CO2 as 115 physical print books.","115","books","[""luccioni2025a""]","is_blank","is_blank","According to a life cycle assessment comparing print books to e-readers, one Amazon Kindle device produces the same amount of CO2 as 115 physical print books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the study, in disadvantaged communities, the per-household health burden from data center air pollutants could be 200 times higher than in less-impacted communities. This ratio is explicitly stated multiple times in the paper.","200","multiplier","[""han2024""]","is_blank","is_blank","According to the study, in disadvantaged communities, the per-household health burden from data center air pollutants could be 200 times higher than in less-impacted communities. This ratio is explicitly stated multiple times in the paper."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","According to Table B1 in the Zschache 2025 paper, the energy consumption for DS Llama 70B model on the FKTG dataset was 702.06 Wh, averaged over 10 runs using 2 GPUs on a single node Capella system.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","According to Table B1 in the Zschache 2025 paper, the energy consumption for DS Llama 70B model on the FKTG dataset was 702.06 Wh, averaged over 10 runs using 2 GPUs on a single node Capella system."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The question appears to be based on a misunderstanding - there is no 2022 paper by Dodge et al. mentioned in any of the provided context snippets. While the context discusses various language models and their parameters (like BERT-large with 350M parameters, GPT2-XL with 1.5B parameters), none are from a 2022 Dodge paper.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The question appears to be based on a misunderstanding - there is no 2022 paper by Dodge et al. mentioned in any of the provided context snippets. While the context discusses various language models and their parameters (like BERT-large with 350M parameters, GPT2-XL with 1.5B parameters), none are from a 2022 Dodge paper."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context clearly describes Jevons' Paradox as when technological improvements in efficiency lead to increased overall resource consumption rather than decreased consumption, due to increased usage and adoption. This is explicitly defined and exemplified across multiple sources with respect to AI technologies.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The context clearly describes Jevons' Paradox as when technological improvements in efficiency lead to increased overall resource consumption rather than decreased consumption, due to increased usage and adoption. This is explicitly defined and exemplified across multiple sources with respect to AI technologies."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","From the context, GPT-3 training used 1287 MWh of energy, while Meena's carbon footprint is equivalent to 242,231 miles driven. Since the context doesn't provide enough information to convert between these different metrics or directly compare their energy usage, we cannot determine how many Meena training runs would equal one GPT-3 training run.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","From the context, GPT-3 training used 1287 MWh of energy, while Meena's carbon footprint is equivalent to 242,231 miles driven. Since the context doesn't provide enough information to convert between these different metrics or directly compare their energy usage, we cannot determine how many Meena training runs would equal one GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study, image generation tasks consume on average 2.907 kWh of energy per 1,000 inferences, with a standard deviation of 3.31 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","According to Table 2 in the 2024 study, image generation tasks consume on average 2.907 kWh of energy per 1,000 inferences, with a standard deviation of 3.31 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","According to the table in Strubell 2019, an average American life produces 36,156 pounds of CO2e emissions in one year.","36156","lbs","[""strubell2019""]","is_blank","is_blank","According to the table in Strubell 2019, an average American life produces 36,156 pounds of CO2e emissions in one year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","According to Figure 6(c) in the paper, Marion County in West Virginia is projected to have the highest per-household health cost at $1,218.3 (with a range of $978.0 to $1,458.5) among all counties, and has a County-to-Nation Per-Household Median Income Ratio of 0.80.","Marion County","is_blank","[""han2024""]","is_blank","is_blank","According to Figure 6(c) in the paper, Marion County in West Virginia is projected to have the highest per-household health cost at $1,218.3 (with a range of $978.0 to $1,458.5) among all counties, and has a County-to-Nation Per-Household Median Income Ratio of 0.80."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","According to Table III in khan2025, Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg after optimization, representing a 0.75x change (from 0.020 to 0.015).","0.75","multiplier","[""khan2025""]","is_blank","is_blank","According to Table III in khan2025, Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg after optimization, representing a 0.75x change (from 0.020 to 0.015)."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to multiple sources, the BLOOMz-7B model's combined training and fine-tuning energy costs were 51,686 kWh for training plus 7,571 kWh for fine-tuning, totaling 59,257 kWh.","59257","kWh","[""luccioni2024"", ""ebert2024""]","is_blank","is_blank","According to multiple sources, the BLOOMz-7B model's combined training and fine-tuning energy costs were 51,686 kWh for training plus 7,571 kWh for fine-tuning, totaling 59,257 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","According to the context, FLM-101B has 101 billion parameters based on its name and multiple references in the documents indicating its size. Specifically, the model's final configuration is described with a hidden state dimension of 10,240, 80 layers, 80 attention heads, and a vocabulary size of 100,256.","101000000000","parameters","[""li2025a""]","is_blank","is_blank","According to the context, FLM-101B has 101 billion parameters based on its name and multiple references in the documents indicating its size. Specifically, the model's final configuration is described with a hidden state dimension of 10,240, 80 layers, 80 attention heads, and a vocabulary size of 100,256."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","According to the context, a 6.1B parameter model would require approximately 103,500 kWh for a full training run. Using BLOOMz-7B's inference energy cost of 1.0 × 10^-4 kWh per inference as a comparable model, it would take approximately 1.035 billion inferences to match the training energy cost.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","According to the context, a 6.1B parameter model would require approximately 103,500 kWh for a full training run. Using BLOOMz-7B's inference energy cost of 1.0 × 10^-4 kWh per inference as a comparable model, it would take approximately 1.035 billion inferences to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","According to the methodology section of the paper, the authors explicitly state that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","According to the methodology section of the paper, the authors explicitly state that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","According to the context, BLOOMz-7B requires 592,570,000 inferences for the cumulative inference energy cost to equal the combined training and fine-tuning energy costs. This is shown directly in Table 5 of the paper as the 'Cost parity (# inferences)' value for BLOOMz-7B.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","According to the context, BLOOMz-7B requires 592,570,000 inferences for the cumulative inference energy cost to equal the combined training and fine-tuning energy costs. This is shown directly in Table 5 of the paper as the 'Cost parity (# inferences)' value for BLOOMz-7B."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context explicitly states that the dataset used was called 'FKTG-dataset' (Fachkonferenz Teilgebiete), which contained statements from the German population regarding nuclear waste site selection.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that the dataset used was called 'FKTG-dataset' (Fachkonferenz Teilgebiete), which contained statements from the German population regarding nuclear waste site selection."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific water consumption data for Meta's Llama 3 inference serving clusters in 2024. While there is discussion of Llama 3 training energy consumption and water consumption data for other models like GPT-3, there is no direct information about Llama 3's inference water consumption in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific water consumption data for Meta's Llama 3 inference serving clusters in 2024. While there is discussion of Llama 3 training energy consumption and water consumption data for other models like GPT-3, there is no direct information about Llama 3's inference water consumption in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information about what percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals. While the documents mention that H100 GPUs contain 0.1% rare earth metals by mass, there is no data about recycled content.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about what percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals. While the documents mention that H100 GPUs contain 0.1% rare earth metals by mass, there is no data about recycled content."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","According to the context from Luccioni et al. 2023, the Evolved Transformer NAS produced 626,155 lbs (284,019 kg) of CO2 emissions. Since one round trip flight between NY and SF produces 1,984 lbs of CO2 according to Strubell 2019, we can calculate that 626,155/1,984 ≈ 315.6 passengers.","315.6","passengers","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","According to the context from Luccioni et al. 2023, the Evolved Transformer NAS produced 626,155 lbs (284,019 kg) of CO2 emissions. Since one round trip flight between NY and SF produces 1,984 lbs of CO2 according to Strubell 2019, we can calculate that 626,155/1,984 ≈ 315.6 passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 OpenRouter data, 84% of LLM token usage was through models with no environmental impact disclosure, as shown directly in the context.","84","percent","[""luccioni2025c""]","is_blank","is_blank","According to May 2025 OpenRouter data, 84% of LLM token usage was through models with no environmental impact disclosure, as shown directly in the context."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, LLaMA 13B requires 1 A100 80GB GPU for inference without compression or optimization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to Table II in the context, LLaMA 13B requires 1 A100 80GB GPU for inference without compression or optimization."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context indicates that training a 6.1 billion parameter transformer model to completion would consume approximately 103,500 kWh of electricity. Using standard carbon emissions factors for electricity consumption, this would result in several metric tons of CO2 emissions, but the exact CO2 emissions amount is not directly stated in the context.","is_blank","metric tons","[""dodge2022""]","is_blank","is_blank","The context indicates that training a 6.1 billion parameter transformer model to completion would consume approximately 103,500 kWh of electricity. Using standard carbon emissions factors for electricity consumption, this would result in several metric tons of CO2 emissions, but the exact CO2 emissions amount is not directly stated in the context."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","Based on the research papers cited in the context, while there are discussions about reducing carbon emissions through optimization techniques like quantization (up to 45% reduction possible), there is no specific total carbon emissions amount avoided in 2023 mentioned.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","Based on the research papers cited in the context, while there are discussions about reducing carbon emissions through optimization techniques like quantization (up to 45% reduction possible), there is no specific total carbon emissions amount avoided in 2023 mentioned."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","According to the context, in a specific scenario using 2 A100s and 1 A10G GPU resulted in a 24% cost saving compared to an A100-only strategy.","24","percent","[""griggs2024""]","is_blank","is_blank","According to the context, in a specific scenario using 2 A100s and 1 A10G GPU resulted in a 24% cost saving compared to an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr, though this is noted as an estimate since H20 is not readily available on cloud service providers, calculated using the relative complete system cost against H100.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr, though this is noted as an estimate since H20 is not readily available on cloud service providers, calculated using the relative complete system cost against H100."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","According to the context, most carbon footprint analyses gather information manually by writing to authors, as evidenced by Luccioni and Hernandez-Garcia having to reach out to over 500 authors to collect information, with many refusing to provide the data. The text explicitly states that 'most carbon footprint analyses gather the information manually by writing to authors.'","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, most carbon footprint analyses gather information manually by writing to authors, as evidenced by Luccioni and Hernandez-Garcia having to reach out to over 500 authors to collect information, with many refusing to provide the data. The text explicitly states that 'most carbon footprint analyses gather the information manually by writing to authors.'"
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the context, in 2023 the total public health cost of U.S. data centers was approximately $6.7 billion, which was explicitly stated to be equivalent to 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","According to the context, in 2023 the total public health cost of U.S. data centers was approximately $6.7 billion, which was explicitly stated to be equivalent to 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the context, LLaMA 7B requires 1 A100 80GB GPU to run inference with a batch size of 64, which represents the bare minimum hardware requirements without any compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to Table II in the context, LLaMA 7B requires 1 A100 80GB GPU to run inference with a batch size of 64, which represents the bare minimum hardware requirements without any compression or quantization."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","According to the context, the experimental results explicitly stated that 'these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which directly matches the claim in the question.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, the experimental results explicitly stated that 'these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which directly matches the claim in the question."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to multiple references in the context, the researchers reached out to 500 authors and were able to collect 95 answers total, which represented a 15.4% response rate. Many authors refused to provide information citing privacy concerns and lack of experimental logs.","95","answers","[""luccioni2023"", ""luccioni2025b""]","is_blank","is_blank","According to multiple references in the context, the researchers reached out to 500 authors and were able to collect 95 answers total, which represented a 15.4% response rate. Many authors refused to provide information citing privacy concerns and lack of experimental logs."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","Based on the context, JetMoE-8B used $100k budget and 30,000 H100 GPU hours, which equals approximately $3.33 per GPU hour ($100,000/30,000 hours).","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","Based on the context, JetMoE-8B used $100k budget and 30,000 H100 GPU hours, which equals approximately $3.33 per GPU hour ($100,000/30,000 hours)."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","According to Table 2 in the document, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","is_blank","According to Table 2 in the document, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens of primarily English data from web documents, mathematics, and code, as explicitly stated multiple times in the documents.","1250000000000","tokens","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens of primarily English data from web documents, mathematics, and code, as explicitly stated multiple times in the documents."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the renewable energy projects table in the context, as of January 2024, Amazon had 36 renewable energy projects announced in the United Kingdom with a total MW capacity of 901.","36","projects","[""amazon2023""]","is_blank","is_blank","According to the renewable energy projects table in the context, as of January 2024, Amazon had 36 renewable energy projects announced in the United Kingdom with a total MW capacity of 901."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","According to Amazon's 2023 Sustainability Report representation data, in 2023, 68.4% of Amazon's workforce across all levels in the United States identified as men.","68.4","percent","[""amazon2023""]","is_blank","is_blank","According to Amazon's 2023 Sustainability Report representation data, in 2023, 68.4% of Amazon's workforce across all levels in the United States identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","According to the provided context, Apple reports that its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","According to the provided context, Apple reports that its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","Based on Figure 4 in the document xia2024, which shows execution time breakdown for BlackMamba with sparse fine-tuning at batch size 84, the total execution time (sum of forward, backward and optimizer stages) is approximately 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","is_blank","Based on Figure 4 in the document xia2024, which shows execution time breakdown for BlackMamba with sparse fine-tuning at batch size 84, the total execution time (sum of forward, backward and optimizer stages) is approximately 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","Based on the context, the authors introduced the 'granularity metric', which is defined as the ratio of calculation to communication time, to assess model suitability for distributed spot training and estimate training performance with additional spot VMs.","The granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","Based on the context, the authors introduced the 'granularity metric', which is defined as the ratio of calculation to communication time, to assess model suitability for distributed spot training and estimate training performance with additional spot VMs."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to a coalition of Microsoft employees, a single deal with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent (or 6.4 times) more carbon emissions compared to the company's carbon removal targets for the year.","6.4","times","[""luccioni2025a""]","is_blank","is_blank","According to a coalition of Microsoft employees, a single deal with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent (or 6.4 times) more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","Based on the context, water withdrawal refers to freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses (excluding hydroelectricity generation).","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","Based on the context, water withdrawal refers to freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses (excluding hydroelectricity generation)."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","According to the context, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. This is explicitly stated multiple times in the documents.","5","years","[""stone2022""]","is_blank","is_blank","According to the context, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. This is explicitly stated multiple times in the documents."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","According to multiple mentions in the provided context from a 2021 Deloitte report, the average U.S. household had 25 connected devices including smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","is_blank","According to multiple mentions in the provided context from a 2021 Deloitte report, the average U.S. household had 25 connected devices including smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","Based on the provided context, the energy consumption to pre-train LLMs ranges from 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout, representing a wide spectrum of energy requirements depending on model size and architecture.","0.8-3500","MWh","[""luccioni2025c""]","is_blank","is_blank","Based on the provided context, the energy consumption to pre-train LLMs ranges from 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout, representing a wide spectrum of energy requirements depending on model size and architecture."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","Based on the context, IBM's Watson program actually beat human contenders to win the Jeopardy challenge in 2011, as mentioned multiple times in the source documents. The question asks if Watson did NOT beat humans, which makes this statement false.","0","is_blank","[""stone2022""]","is_blank","is_blank","Based on the context, IBM's Watson program actually beat human contenders to win the Jeopardy challenge in 2011, as mentioned multiple times in the source documents. The question asks if Watson did NOT beat humans, which makes this statement false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context, one study suggests that 10-50 queries on GPT-3 consume around half a liter of water.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","According to the context, one study suggests that 10-50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to Table 4 in the context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which was higher than Llama-2-13b-chat's score of 6.650 after alignment.","6.681","score","[""shen2024""]","is_blank","is_blank","According to Table 4 in the context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which was higher than Llama-2-13b-chat's score of 6.650 after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to the context, GPT-3 needs a 500ml bottle of water for roughly 10-50 medium-length responses, meaning one 500ml bottle could produce between 10-50 such responses.","10-50","responses","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs a 500ml bottle of water for roughly 10-50 medium-length responses, meaning one 500ml bottle could produce between 10-50 such responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","According to the Griggs et al. 2024 paper, Mélange achieves 15-77% cost reduction with a 120ms SLO and 9-68% reduction with a 40ms SLO in conversational settings (using the Arena short-context dataset), with 77% being the maximum reduction achieved.","77","percent","[""griggs2024""]","is_blank","is_blank","According to the Griggs et al. 2024 paper, Mélange achieves 15-77% cost reduction with a 120ms SLO and 9-68% reduction with a 40ms SLO in conversational settings (using the Arena short-context dataset), with 77% being the maximum reduction achieved."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to multiple context passages, the minimum requirement for running LLaMA-65B inference without compression or quantization is 4 A100 80GB GPUs. This is consistently stated across different sources and is needed to fit the model in GPU memory while enabling meaningful inference.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","According to multiple context passages, the minimum requirement for running LLaMA-65B inference without compression or quantization is 4 A100 80GB GPUs. This is consistently stated across different sources and is needed to fit the model in GPU memory while enabling meaningful inference."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","Multiple sources consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024""]","is_blank","is_blank","Multiple sources consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to multiple sources including AWS data from 2019, machine learning inference workloads accounted for 80-90% of total ML cloud computing demand.","85","percent","[""fernandez2025"", ""chung2025""]","is_blank","is_blank","According to multiple sources including AWS data from 2019, machine learning inference workloads accounted for 80-90% of total ML cloud computing demand."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","According to the ethical considerations section, for all model experimentation and evaluation in the study, they emitted a total of 178.97 kg of CO2eq.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","According to the ethical considerations section, for all model experimentation and evaluation in the study, they emitted a total of 178.97 kg of CO2eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","According to multiple sources, using TDP to estimate GPU energy consumption is not reliable, as TDP-based estimations almost always overestimate actual consumption since GPUs rarely draw maximum power continuously. Evidence shows this can lead to overestimation by up to 4.1x in some cases.","0","is_blank","[""chung2025""]","is_blank","is_blank","According to multiple sources, using TDP to estimate GPU energy consumption is not reliable, as TDP-based estimations almost always overestimate actual consumption since GPUs rarely draw maximum power continuously. Evidence shows this can lead to overestimation by up to 4.1x in some cases."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","According to the context, GPT-4o mini actually consumes MORE energy per query than GPT-4o (3.098 Wh vs 2.875 Wh) due to its reliance on older A100 GPU hardware instead of H100s.","0","is_blank","[""jegham2025""]","is_blank","is_blank","According to the context, GPT-4o mini actually consumes MORE energy per query than GPT-4o (3.098 Wh vs 2.875 Wh) due to its reliance on older A100 GPU hardware instead of H100s."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","Based on Figure 8 in the paper, which shows query throughput for different models and batch sizes, the dense Mixtral-CS with batch size 1 has a throughput of 0.3 queries/second.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","Based on Figure 8 in the paper, which shows query throughput for different models and batch sizes, the dense Mixtral-CS with batch size 1 has a throughput of 0.3 queries/second."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","According to the context, direct disclosure of environmental information peaked in 2022 at 10% of notable models, but after 2022 there was a 'notable reversal in this trend' with dramatically reduced direct environmental disclosures, leading to the majority of models falling under 'no disclosure' by early 2025. This indicates the trend did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the context, direct disclosure of environmental information peaked in 2022 at 10% of notable models, but after 2022 there was a 'notable reversal in this trend' with dramatically reduced direct environmental disclosures, leading to the majority of models falling under 'no disclosure' by early 2025. This indicates the trend did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to Table 1 in Griggs et al. (2024), the on-demand hourly price for an H100 GPU from RunPod was $7.5164 per hour, which was normalized to match the pricing structures of major cloud platforms.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","According to Table 1 in Griggs et al. (2024), the on-demand hourly price for an H100 GPU from RunPod was $7.5164 per hour, which was normalized to match the pricing structures of major cloud platforms."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","According to the context, GPT-3's training water consumption varied by data center location, with values ranging from around 2.4 to 15.3 million liters total water consumption depending on location. For the U.S. average, the total water consumption for training was 5.439 million liters (0.708 million L on-site + 4.731 million L off-site).","5439000","liters of water","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3's training water consumption varied by data center location, with values ranging from around 2.4 to 15.3 million liters total water consumption depending on location. For the U.S. average, the total water consumption for training was 5.439 million liters (0.708 million L on-site + 4.731 million L off-site)."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs with on-demand rental costs over $5,200 per month. Therefore, dividing $5,200 by (30 days * 24 hours), the hourly cost is approximately $7.22.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs with on-demand rental costs over $5,200 per month. Therefore, dividing $5,200 by (30 days * 24 hours), the hourly cost is approximately $7.22."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context, to increase GPT-3's BLEU score from 5 to 40 for language translation requires making the model 1,000× larger in size. This is explicitly stated multiple times in the document.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context, to increase GPT-3's BLEU score from 5 to 40 for language translation requires making the model 1,000× larger in size. This is explicitly stated multiple times in the document."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to Strubell et al.'s research, training a Transformer model with neural architecture search produced 626,155 lbs of CO2 emissions. This can be used directly to represent the equivalent driving distance since this CO2 emission amount is exactly provided in the context.","626155","miles","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","According to Strubell et al.'s research, training a Transformer model with neural architecture search produced 626,155 lbs of CO2 emissions. This can be used directly to represent the equivalent driving distance since this CO2 emission amount is exactly provided in the context."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","According to the context, BLOOMz-7B was downloaded 606,096 times as of November 2023, and its inference energy consumption is 1.0 × 10^-4 kWh per inference. For 1 million inferences per download, the total energy would be: 606,096 downloads × 1 million inferences × (1.0 × 10^-4 kWh) = 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","is_blank","According to the context, BLOOMz-7B was downloaded 606,096 times as of November 2023, and its inference energy consumption is 1.0 × 10^-4 kWh per inference. For 1 million inferences per download, the total energy would be: 606,096 downloads × 1 million inferences × (1.0 × 10^-4 kWh) = 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","According to multiple mentions in the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This is explicitly stated in multiple places in the documents, including the abstract and detailed model descriptions.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","According to multiple mentions in the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This is explicitly stated in multiple places in the documents, including the abstract and detailed model descriptions."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","According to the Cottier 2024 paper, if current trends continue with training costs growing at 2.4x per year, the largest training runs will cost more than $1 billion by 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","According to the Cottier 2024 paper, if current trends continue with training costs growing at 2.4x per year, the largest training runs will cost more than $1 billion by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","According to the context from Cottier 2024, smaller versions of GPT-3 used 4.5e22 FLOP based on the compute formula of 6 × parameters × tokens. While this represents only 14% of the total compute used, it provides a reference point but not the total FLOPS for the full GPT-3 training.","is_blank","FLOPS","[""cottier2024"", ""samsi2024""]","is_blank","is_blank","According to the context from Cottier 2024, smaller versions of GPT-3 used 4.5e22 FLOP based on the compute formula of 6 × parameters × tokens. While this represents only 14% of the total compute used, it provides a reference point but not the total FLOPS for the full GPT-3 training."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to multiple sources in the context, LLaMA-65B requires a minimum of 8 V100 GPUs with 32GB of RAM each for meaningful inference without compression or quantization. While it could technically be sharded across 6 V100 GPUs, 8 GPUs are used since the model architecture makes it better suited for balanced sharding.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","According to multiple sources in the context, LLaMA-65B requires a minimum of 8 V100 GPUs with 32GB of RAM each for meaningful inference without compression or quantization. While it could technically be sharded across 6 V100 GPUs, 8 GPUs are used since the model architecture makes it better suited for balanced sharding."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","From Table 4 in the FLM-101B paper, we can estimate the computational work for the 101B model's final training stage based on total training FLOPs. The paper reports 28.22 zettaFLOPs for the English portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","From Table 4 in the FLM-101B paper, we can estimate the computational work for the 101B model's final training stage based on total training FLOPs. The paper reports 28.22 zettaFLOPs for the English portion."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","According to the context in Schwartz 2019, AlexNet's performance is mentioned but its specific top-1 ImageNet accuracy is not stated. The context only indicates that AlexNet has more parameters than ResNet but dramatically less FPO and lower accuracy, without giving the exact accuracy number.","is_blank","percent","[""is_blank""]","is_blank","is_blank","According to the context in Schwartz 2019, AlexNet's performance is mentioned but its specific top-1 ImageNet accuracy is not stated. The context only indicates that AlexNet has more parameters than ResNet but dramatically less FPO and lower accuracy, without giving the exact accuracy number."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 x 80G GPUs. Therefore, the total number of A800 GPUs used was 24 servers × 8 GPUs per server = 192 GPUs.","192","GPUs","[""li2025a""]","is_blank","is_blank","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 x 80G GPUs. Therefore, the total number of A800 GPUs used was 24 servers × 8 GPUs per server = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","According to multiple sources including Strubell et al. 2019, training a Transformer model with Neural Architecture Search produced 626,155 lbs (284,019 kg) of CO2 emissions. Given that the average American lifetime carbon footprint is 36,156 lbs/year according to the same paper, this equals approximately 17.3 American lifetimes worth of emissions.","17.3","lifetimes","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","According to multiple sources including Strubell et al. 2019, training a Transformer model with Neural Architecture Search produced 626,155 lbs (284,019 kg) of CO2 emissions. Given that the average American lifetime carbon footprint is 36,156 lbs/year according to the same paper, this equals approximately 17.3 American lifetimes worth of emissions."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","According to the context, FAIR's RoBERTa model required approximately 25,000 GPU hours to train on 160GB of text containing roughly 40 billion word-pieces.","25000","hours","[""schwartz2019""]","is_blank","is_blank","According to the context, FAIR's RoBERTa model required approximately 25,000 GPU hours to train on 160GB of text containing roughly 40 billion word-pieces."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the context, Amazon's on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon's on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","According to the context, Megatron-LM was used to deploy large language models across multiple GPUs and nodes, as evidenced by multiple references in different documents discussing its use for efficient large-scale language model training on GPU clusters.","Megatron-LM","is_blank","[""shen2024"", ""fernandez2025"", ""li2025a""]","is_blank","is_blank","According to the context, Megatron-LM was used to deploy large language models across multiple GPUs and nodes, as evidenced by multiple references in different documents discussing its use for efficient large-scale language model training on GPU clusters."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in the context, when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95, as shown by the energy consumed ratio between double and single node deployment (94.88 Wh vs 48.60 Wh).","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","According to Table B2 in the context, when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95, as shown by the energy consumed ratio between double and single node deployment (94.88 Wh vs 48.60 Wh)."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","According to the source, about 10-50 queries on GPT-3 consumes around half a liter (500ml) of water. This means one user session would consume approximately 10-50ml of water.","30","gallons of water","[""luccioni2025a""]","is_blank","is_blank","According to the source, about 10-50 queries on GPT-3 consumes around half a liter (500ml) of water. This means one user session would consume approximately 10-50ml of water."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the context, GPT-4o's estimated annual electricity consumption of 391,509-463,269 MWh would exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","35000","homes","[""jegham2025""]","is_blank","is_blank","According to the context, GPT-4o's estimated annual electricity consumption of 391,509-463,269 MWh would exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to a 2025 paper by Luccioni et al., Microsoft reported a 34% increase in global water consumption between 2021 and 2022, reaching 1.7 billion gallons.","34","percent","[""luccioni2025a""]","is_blank","is_blank","According to a 2025 paper by Luccioni et al., Microsoft reported a 34% increase in global water consumption between 2021 and 2022, reaching 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that for sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, with traditional models failing to match LLM performance. This is explicitly mentioned multiple times in the source documents.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that for sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, with traditional models failing to match LLM performance. This is explicitly mentioned multiple times in the source documents."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","According to the context, the Augusta datacenter where the Evolved Transformer model was run had a trailing twelve-month average PUE of 1.12.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","According to the context, the Augusta datacenter where the Evolved Transformer model was run had a trailing twelve-month average PUE of 1.12."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the context, a growth pattern simulation shows that GPT-4o will reach approximately 772 billion queries in 2025, with a 20% monthly growth rate from January to May 2025 followed by a decaying pattern from June to December.","772","queries","[""jegham2025""]","is_blank","is_blank","According to the context, a growth pattern simulation shows that GPT-4o will reach approximately 772 billion queries in 2025, with a 20% monthly growth rate from January to May 2025 followed by a decaying pattern from June to December."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","According to the evaluation results shown in Table 3 of the paper, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard benchmark suite, outperforming models like LLaMA2 (51.0), DeepseekMoE (51.1), and Gemma (46.4).","53.0","score","[""shen2024""]","is_blank","is_blank","According to the evaluation results shown in Table 3 of the paper, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard benchmark suite, outperforming models like LLaMA2 (51.0), DeepseekMoE (51.1), and Gemma (46.4)."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide a specific count of how many AI training runs were conducted globally using only renewable power in 2022. While some documents discuss renewable energy use in AI training, none give concrete statistics for renewable-only training runs in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide a specific count of how many AI training runs were conducted globally using only renewable power in 2022. While some documents discuss renewable energy use in AI training, none give concrete statistics for renewable-only training runs in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source models currently have an exemption from reporting obligations, but the authors argue this exemption should be removed since open-source models can have significant energy implications and should follow the same reporting standards as proprietary models. This shows that open-source models are currently exempt unless they pose systemic risk.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that open-source models currently have an exemption from reporting obligations, but the authors argue this exemption should be removed since open-source models can have significant energy implications and should follow the same reporting standards as proprietary models. This shows that open-source models are currently exempt unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","Multiple sources in the context consistently indicate that the average data center PUE globally in 2023 was 1.58, which would be the most recent available data point prior to 2020.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","Multiple sources in the context consistently indicate that the average data center PUE globally in 2023 was 1.58, which would be the most recent available data point prior to 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","According to the context, when analyzing the KV Cache size growth for the OPT-2.7B model on g4dn.xlarge instance, the KV Cache expanded to 5.312GB when the batch size was increased to 32.","5.312","GB","[""kim2025""]","is_blank","is_blank","According to the context, when analyzing the KV Cache size growth for the OPT-2.7B model on g4dn.xlarge instance, the KV Cache expanded to 5.312GB when the batch size was increased to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","According to Cottier et al. (2024), for the four notable models studied in-depth (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for 29-49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","is_blank","According to Cottier et al. (2024), for the four notable models studied in-depth (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for 29-49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The CodeCarbon package was used to measure energy consumption during inference runs across multiple studies. This is explicitly stated in multiple sources, with zschache2025 directly stating 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","is_blank","The CodeCarbon package was used to measure energy consumption during inference runs across multiple studies. This is explicitly stated in multiple sources, with zschache2025 directly stating 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to an analysis of 100 news articles published by April 11, 2025, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","According to an analysis of 100 news articles published by April 11, 2025, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context clearly states that the Compute Time Calibration Function (CTCF) is used to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function","is_blank","[""kim2025""]","is_blank","is_blank","The context clearly states that the Compute Time Calibration Function (CTCF) is used to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context clearly shows that increasing the number of GPU shards leads to increased energy costs for LLaMA-65B inference. This is explicitly stated multiple times, including 'increasing the number of shards always increases the wattage' and 'the energy per second increases with the number of shards even at the same batch size'.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context clearly shows that increasing the number of GPU shards leads to increased energy costs for LLaMA-65B inference. This is explicitly stated multiple times, including 'increasing the number of shards always increases the wattage' and 'the energy per second increases with the number of shards even at the same batch size'."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","Based on the context, the H100 is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. For a single H100, this means 0.001 kg of rare earth material (0.1%) would require 0.011 kL of water (11 kL * 0.001).","0.011","kL","[""morrison2025""]","is_blank","is_blank","Based on the context, the H100 is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. For a single H100, this means 0.001 kg of rare earth material (0.1%) would require 0.011 kL of water (11 kL * 0.001)."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","According to the context, there is currently an open-source exemption from reporting obligations under the EU AI Act, and one of the policy proposals is to eliminate this exemption. This indicates that open-source models are currently not required to report energy consumption.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, there is currently an open-source exemption from reporting obligations under the EU AI Act, and one of the policy proposals is to eliminate this exemption. This indicates that open-source models are currently not required to report energy consumption."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","According to multiple sources from 2020, Amazon, Microsoft, Meta (Facebook), and Google accounted for 30% of all corporate Power Purchase Agreements (PPAs) worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","According to multiple sources from 2020, Amazon, Microsoft, Meta (Facebook), and Google accounted for 30% of all corporate Power Purchase Agreements (PPAs) worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","According to the context, U.S. data centers resulted in a total public health cost of $6.7 billion in 2023, which is stated to be equivalent to approximately 44% of the data centers' total electricity cost. This figure was calculated using the average attribution method as specified in section A.2 of the document.","6700000000","USD","[""han2024""]","is_blank","is_blank","According to the context, U.S. data centers resulted in a total public health cost of $6.7 billion in 2023, which is stated to be equivalent to approximately 44% of the data centers' total electricity cost. This figure was calculated using the average attribution method as specified in section A.2 of the document."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","According to the data in Table 4, the o3 model consumes 12.222 Wh for long prompts (10k input-1.5k output) while GPT-4.1 nano consumes 0.827 Wh. This means o3's energy consumption is approximately 14.78 times greater.","14.78","multiplier","[""jegham2025""]","is_blank","is_blank","According to the data in Table 4, the o3 model consumes 12.222 Wh for long prompts (10k input-1.5k output) while GPT-4.1 nano consumes 0.827 Wh. This means o3's energy consumption is approximately 14.78 times greater."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","For the Arena dataset (short-context workloads) with a 120ms SLO, Mélange achieved cost reductions of 15-77% compared to single-GPU baselines. This range is explicitly stated in the context when discussing the Arena dataset results.","77","percent","[""griggs2024""]","is_blank","is_blank","For the Arena dataset (short-context workloads) with a 120ms SLO, Mélange achieved cost reductions of 15-77% compared to single-GPU baselines. This range is explicitly stated in the context when discussing the Arena dataset results."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","According to Table 3 in document li2025a, FLM-101B had net carbon emissions of 26 tCO2e during pre-training, which was about 1/10 of the carbon footprint of typical large language models.","26","tCO2e","[""li2025a""]","is_blank","is_blank","According to Table 3 in document li2025a, FLM-101B had net carbon emissions of 26 tCO2e during pre-training, which was about 1/10 of the carbon footprint of typical large language models."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","Based on Figures 8 and 14 from xia2024, for Mixtral with sparse fine-tuning at batch size 1, the throughput is approximately 0.3 queries per second, which means each query takes about 3.33 seconds to execute.","3.33","seconds","[""xia2024""]","is_blank","is_blank","Based on Figures 8 and 14 from xia2024, for Mixtral with sparse fine-tuning at batch size 1, the throughput is approximately 0.3 queries per second, which means each query takes about 3.33 seconds to execute."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","According to multiple passages in han2024, the public health costs of AI are highly uneven, with disadvantaged and low-income communities bearing a disproportionate burden. The ratio between the highest and lowest county-level per-household health costs is approximately 200 to 1, and the top 10 most impacted counties all have below-median household incomes.","0","is_blank","[""han2024""]","is_blank","is_blank","According to multiple passages in han2024, the public health costs of AI are highly uneven, with disadvantaged and low-income communities bearing a disproportionate burden. The ratio between the highest and lowest county-level per-household health costs is approximately 200 to 1, and the top 10 most impacted counties all have below-median household incomes."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","According to the context in [wu2021b], GPU theoretical performance per watt was observed to double every 3-4 years as of 2019, based on data from Sun et al. 2019.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to the context in [wu2021b], GPU theoretical performance per watt was observed to double every 3-4 years as of 2019, based on data from Sun et al. 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","According to the context, Ollama was used as the open-source platform to apply 4-bit quantization for local deployment of the models. Specifically, the case study mentions using 'a 4-bit quantization strategy (b = 4)' implemented through Ollama [19], which is described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments.'","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, Ollama was used as the open-source platform to apply 4-bit quantization for local deployment of the models. Specifically, the case study mentions using 'a 4-bit quantization strategy (b = 4)' implemented through Ollama [19], which is described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments.'"
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","According to the context, the researchers used Backblaze (B2) as their independent S3 storage provider, accessing datasets on-demand via shards in the tar format using the WebDataset library. Backblaze was chosen because it had replicated data centers worldwide that could serve requests from any continent with reasonable ingress rates.","Backblaze B2","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, the researchers used Backblaze (B2) as their independent S3 storage provider, accessing datasets on-demand via shards in the tar format using the WebDataset library. Backblaze was chosen because it had replicated data centers worldwide that could serve requests from any continent with reasonable ingress rates."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","Multiple documents indicate a strong correlation between inference energy consumption and model runtime, with the Zschache 2025 paper explicitly stating this correlation multiple times. However, the context does not specifically characterize this relationship as 'nearly linear'.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","Multiple documents indicate a strong correlation between inference energy consumption and model runtime, with the Zschache 2025 paper explicitly stating this correlation multiple times. However, the context does not specifically characterize this relationship as 'nearly linear'."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","According to the context, Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024. The bill was referred to the Committee on Commerce, Science and Transportation.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024. The bill was referred to the Committee on Commerce, Science and Transportation."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2024), the price per hour for an NVIDIA H100 is $11.06/hr, as shown in their specifications table comparing H100, H20, and TPU v6e chips.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","According to Chen et al. (2024), the price per hour for an NVIDIA H100 is $11.06/hr, as shown in their specifications table comparing H100, H20, and TPU v6e chips."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","According to the context, AWS announced in February 2024 that the average expected server lifetime had improved from 5 to 6 years. While this specifically refers to AWS servers, it represents one of the few concrete data points about hardware lifetime in AI data centers in 2024.","6","years","[""amazon2023""]","is_blank","is_blank","According to the context, AWS announced in February 2024 that the average expected server lifetime had improved from 5 to 6 years. While this specifically refers to AWS servers, it represents one of the few concrete data points about hardware lifetime in AI data centers in 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, LLaMA-13B requires 2 V100 32GB GPUs as the minimum hardware configuration for inference without compression or quantization, with a maximum batch size of 64.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","According to Table II in the context, LLaMA-13B requires 2 V100 32GB GPUs as the minimum hardware configuration for inference without compression or quantization, with a maximum batch size of 64."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to multiple context snippets, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to multiple context snippets, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to the context, Peters et al. (2018) reported that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","According to the context, Peters et al. (2018) reported that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","According to the context, the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","According to the context, the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","Multiple sources confirm that Google reported a PUE of 1.10 for its hyperscale data centers in 2021. This is specifically mentioned in context snippets from wu2021b and dodge2022.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","Multiple sources confirm that Google reported a PUE of 1.10 for its hyperscale data centers in 2021. This is specifically mentioned in context snippets from wu2021b and dodge2022."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to AWS research, customers in North America can reduce their workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS's electricity is matched with 100% renewable energy, which Amazon achieved in 2023.","96","percent","[""amazon2023""]","is_blank","is_blank","According to AWS research, customers in North America can reduce their workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS's electricity is matched with 100% renewable energy, which Amazon achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","According to the paper, their model predicted that fine-tuning a sparse Mixtral model using 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. This was specifically calculated for the OpenOrca dataset.","3460","USD","[""xia2024""]","is_blank","is_blank","According to the paper, their model predicted that fine-tuning a sparse Mixtral model using 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. This was specifically calculated for the OpenOrca dataset."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to measurements from training BERT on a single GPU server, the GPU accounts for 74% of the total electricity consumption among the measured hardware components in an AI training setup.","74","percent","[""dodge2022""]","is_blank","is_blank","According to measurements from training BERT on a single GPU server, the GPU accounts for 74% of the total electricity consumption among the measured hardware components in an AI training setup."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","According to the context, JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s GPUs total, as explicitly stated in section 4.1 of the paper.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s GPUs total, as explicitly stated in section 4.1 of the paper."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","According to morrison2025, during active training of OLMo 2 7B, the average GPU power for a single node is over 600W, which is more than 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","is_blank","According to morrison2025, during active training of OLMo 2 7B, the average GPU power for a single node is over 600W, which is more than 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","According to the context in strubell2019, the average human life produces 11,023 lbs of CO2e emissions in one year globally.","11023","lbs","[""strubell2019""]","is_blank","is_blank","According to the context in strubell2019, the average human life produces 11,023 lbs of CO2e emissions in one year globally."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for LLaMA-13B specifically, there was a 1.25 times (1.25x) increase in inference throughput on A100 compared to V100 GPUs across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","According to the context, for LLaMA-13B specifically, there was a 1.25 times (1.25x) increase in inference throughput on A100 compared to V100 GPUs across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","Multiple passages from the context clearly state that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is consistently mentioned and validated against OpenAI CEO Sam Altman's disclosure of 0.34 Wh per query.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","Multiple passages from the context clearly state that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is consistently mentioned and validated against OpenAI CEO Sam Altman's disclosure of 0.34 Wh per query."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to the context, with a 400 TPS SLO requirement, Max-Performance selected g6e.xlarge which cost $2.699, while InferSave's top choice (g4dn.xlarge) cost $0.71. This means the Max-Performance instance was approximately 280% more expensive than InferSave's choice.","280","percent","[""kim2025""]","is_blank","is_blank","According to the context, with a 400 TPS SLO requirement, Max-Performance selected g6e.xlarge which cost $2.699, while InferSave's top choice (g4dn.xlarge) cost $0.71. This means the Max-Performance instance was approximately 280% more expensive than InferSave's choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","According to the context, for models generating only a single token per inference, the V100 or A30 GPU is mentioned as being more efficient. This is stated directly in the source document.","V100/A30","is_blank","[""zschache2025""]","is_blank","is_blank","According to the context, for models generating only a single token per inference, the V100 or A30 GPU is mentioned as being more efficient. This is stated directly in the source document."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","According to Schwartz 2019, reporting the computational price tag (financial cost) of finding, training, and running models is explicitly described as a key Green AI practice.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","According to Schwartz 2019, reporting the computational price tag (financial cost) of finding, training, and running models is explicitly described as a key Green AI practice."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","According to a 2025 paper discussing AI's environmental impacts, global electronic waste generation reached 62 million tonnes in 2022.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","According to a 2025 paper discussing AI's environmental impacts, global electronic waste generation reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","Based on the provided context, there is not enough information to calculate the difference in average system power per processor between TPU v2 and V100 GPU. While there is information about TPU manufacturing costs and some power specifications, the context does not provide clear power consumption data for both TPU v2 and V100 that would allow for this comparison.","is_blank","Watts","[""is_blank""]","is_blank","is_blank","Based on the provided context, there is not enough information to calculate the difference in average system power per processor between TPU v2 and V100 GPU. While there is information about TPU manufacturing costs and some power specifications, the context does not provide clear power consumption data for both TPU v2 and V100 that would allow for this comparison."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, training GPT-3 in Microsoft's U.S. data centers directly evaporates 700,000 liters of clean freshwater as scope-1 on-site water consumption.","700000","liters","[""li2025b""]","is_blank","is_blank","According to the context, training GPT-3 in Microsoft's U.S. data centers directly evaporates 700,000 liters of clean freshwater as scope-1 on-site water consumption."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","According to multiple mentions in the context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is consistently stated in both text descriptions and Figure 2(c) of the paper.","20","multiplier","[""wu2021a""]","is_blank","is_blank","According to multiple mentions in the context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is consistently stated in both text descriptions and Figure 2(c) of the paper."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the cross-efficiency DEA results shown in Figure 8, o3-mini achieved the highest cross-efficiency score of 0.884 among all evaluated models, followed by o1-mini (0.836) and Claude 3.7 Sonnet (0.825).","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","According to the cross-efficiency DEA results shown in Figure 8, o3-mini achieved the highest cross-efficiency score of 0.884 among all evaluated models, followed by o1-mini (0.836) and Claude 3.7 Sonnet (0.825)."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context explicitly states that cell phones currently have 'average lifetimes of less than 3 years' according to Cordella et al. 2020, which contributes to electronic waste concerns as discussed in the sustainability section.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context explicitly states that cell phones currently have 'average lifetimes of less than 3 years' according to Cordella et al. 2020, which contributes to electronic waste concerns as discussed in the sustainability section."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","According to the intra-zone experiments described in section 4A and Figure 7, CV's per-GPU speedup was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) with T4 GPUs, demonstrating nearly linear per-GPU scaling for CV models.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the intra-zone experiments described in section 4A and Figure 7, CV's per-GPU speedup was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) with T4 GPUs, demonstrating nearly linear per-GPU scaling for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context directly states that LLMs have low power draw because LLM decoding is memory-intensive and bottlenecked by VRAM bandwidth, while diffusion models are more compute-intensive and consume nearly maximum GPU power. This makes the statement true.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context directly states that LLMs have low power draw because LLM decoding is memory-intensive and bottlenecked by VRAM bandwidth, while diffusion models are more compute-intensive and consume nearly maximum GPU power. This makes the statement true."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","According to the Amazon Representation by the Numbers chart in the 2023 Sustainability Report, 31.6% of People Managers at Amazon globally identified as women in 2023.","31.6","percent","[""amazon2023""]","is_blank","is_blank","According to the Amazon Representation by the Numbers chart in the 2023 Sustainability Report, 31.6% of People Managers at Amazon globally identified as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","According to the context, when excluding equity from the cost analysis of the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), computing hardware costs made up 61-76% of total amortized costs.","61-76","percent","[""cottier2024""]","is_blank","is_blank","According to the context, when excluding equity from the cost analysis of the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), computing hardware costs made up 61-76% of total amortized costs."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","According to the context, metrics like accuracy and F1 score consistently showed slight declines after optimization. This is explicitly stated multiple times in the text and shown in Table III where most models exhibit lower accuracy and F1 scores post-optimization.","0","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, metrics like accuracy and F1 score consistently showed slight declines after optimization. This is explicitly stated multiple times in the text and shown in Table III where most models exhibit lower accuracy and F1 scores post-optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","Based on the provided context from strubell2019, the EPA's conversion factor for CO2 emissions from power consumed in the U.S. is 0.954 lbs/kWh. This is explicitly stated in the paper where they use this EPA value to convert power consumption to estimated CO2 emissions.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","Based on the provided context from strubell2019, the EPA's conversion factor for CO2 emissions from power consumed in the U.S. is 0.954 lbs/kWh. This is explicitly stated in the paper where they use this EPA value to convert power consumption to estimated CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to a recent study cited in the paper, global AI electricity consumption is projected to be between 85-134 TWh in 2027.","85-134","TWh","[""li2025b""]","is_blank","is_blank","According to a recent study cited in the paper, global AI electricity consumption is projected to be between 85-134 TWh in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","According to the context, Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","From the Online Inference workload with 128 input + 512 output tokens per request for 3000 requests, plus the Offline Inference workload with 1024 input + 128 tokens per request for 1000 requests, we can calculate total tokens as: (3000 * (128 + 512)) + (1000 * (1024 + 128)) = 2,872,000 tokens.","2872000","tokens","[""kim2025""]","is_blank","is_blank","From the Online Inference workload with 128 input + 512 output tokens per request for 3000 requests, plus the Offline Inference workload with 1024 input + 128 tokens per request for 1000 requests, we can calculate total tokens as: (3000 * (128 + 512)) + (1000 * (1024 + 128)) = 2,872,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","According to the context, the AI Act does not mandate disclosure of greenhouse gas emissions from AI applications like oil and gas exploration. The text explicitly states this is a failure/gap in the Act, noting that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, the AI Act does not mandate disclosure of greenhouse gas emissions from AI applications like oil and gas exploration. The text explicitly states this is a failure/gap in the Act, noting that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to Figure 3(a) and the accompanying text in the 2022 Dodge et al. paper, for DenseNet 201 (which ran for less than half an hour), the Flexible Start optimization showed up to 80% reduction in CO2 emissions specifically in the West US region.","80","percent","[""dodge2022""]","is_blank","is_blank","According to Figure 3(a) and the accompanying text in the 2022 Dodge et al. paper, for DenseNet 201 (which ran for less than half an hour), the Flexible Start optimization showed up to 80% reduction in CO2 emissions specifically in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","According to Table 2 and the text, image generation is the most energy-intensive task with mean consumption of 2.907 kWh per 1,000 inferences, while text classification is the least energy-intensive with 0.002 kWh per 1,000 inferences. This means the energy required for image generation exceeds that of text classification by a factor of 1450.","1450","times","[""luccioni2024""]","is_blank","is_blank","According to Table 2 and the text, image generation is the most energy-intensive task with mean consumption of 2.907 kWh per 1,000 inferences, while text classification is the least energy-intensive with 0.002 kWh per 1,000 inferences. This means the energy required for image generation exceeds that of text classification by a factor of 1450."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that for sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This finding is mentioned multiple times, noting that traditional models fail to match LLM performance, with LLMs achieving better accuracy despite higher energy costs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that for sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This finding is mentioned multiple times, noting that traditional models fail to match LLM performance, with LLMs achieving better accuracy despite higher energy costs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","According to the table in the context showing Amazon Renewable Energy Projects announced as of January 2024, there were 244 projects in the United States.","244","projects","[""amazon2023""]","is_blank","is_blank","According to the table in the context showing Amazon Renewable Energy Projects announced as of January 2024, there were 244 projects in the United States."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide a specific percentage of renewable energy usage for AWS in 2018. While there are mentions of AWS's renewable energy usage and achievements, including reaching 100% renewable energy for 22 data center regions in 2023, no data is given for 2018 specifically.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide a specific percentage of renewable energy usage for AWS in 2018. While there are mentions of AWS's renewable energy usage and achievements, including reaching 100% renewable energy for 22 data center regions in 2023, no data is given for 2018 specifically."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","According to the context, the authors specifically recommend measuring and reporting energy consumption at the 'cumulative server level' as this approach captures the total computation-related power usage and helps providers optimize their AI models for energy efficiency.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, the authors specifically recommend measuring and reporting energy consumption at the 'cumulative server level' as this approach captures the total computation-related power usage and helps providers optimize their AI models for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","According to Table 1 in the document, when training BERT base on a single NVIDIA TITAN X GPU, the GPU consumed 74% of the total electricity consumption compared to other components like CPU and DRAM.","74","percent","[""dodge2022""]","is_blank","is_blank","According to Table 1 in the document, when training BERT base on a single NVIDIA TITAN X GPU, the GPU consumed 74% of the total electricity consumption compared to other components like CPU and DRAM."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","According to multiple sources, serving Llama2-70b requires 2 NVIDIA A100-80GB GPUs, which contradicts any claim that it would need 4 A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","According to multiple sources, serving Llama2-70b requires 2 NVIDIA A100-80GB GPUs, which contradicts any claim that it would need 4 A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","According to the context, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative hardware-software optimization, as explicitly stated in multiple places in the document wu2021a.","28.5","percent","[""wu2021a""]","is_blank","is_blank","According to the context, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative hardware-software optimization, as explicitly stated in multiple places in the document wu2021a."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","According to Li et al. 2025, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027, which includes water consumed for cooling servers and electricity generation. The context directly specifies this projected figure for global AI water use.","4.2-6.6","kilometers of fiberoptic cable","[""li2025b""]","is_blank","is_blank","According to Li et al. 2025, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027, which includes water consumed for cooling servers and electricity generation. The context directly specifies this projected figure for global AI water use."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","According to the context, the upfront hardware acquisition cost to train GPT-4 was estimated at $800M. This represents the cost of purchasing the necessary hardware outright, which is one to two orders of magnitude larger than the amortized hardware costs.","800000000","USD","[""cottier2024""]","is_blank","is_blank","According to the context, the upfront hardware acquisition cost to train GPT-4 was estimated at $800M. This represents the cost of purchasing the necessary hardware outright, which is one to two orders of magnitude larger than the amortized hardware costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","According to the context, van Wynsberghe proposed 'Sustainable AI' as a field that encompasses both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves - not just climate-positive applications alone.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, van Wynsberghe proposed 'Sustainable AI' as a field that encompasses both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves - not just climate-positive applications alone."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","According to Figure 13 and accompanying text in the paper, when using an A100-40GB GPU for fine-tuning Mixtral, the experimental ground truth maximum batch size is shown in the plot and matches the ground truth projection line.","2","samples","[""xia2024""]","is_blank","is_blank","According to Figure 13 and accompanying text in the paper, when using an A100-40GB GPU for fine-tuning Mixtral, the experimental ground truth maximum batch size is shown in the plot and matches the ground truth projection line."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to Figure 8 and its explanation in the context, when comparing the two preemption mechanisms (Recomputation vs Swapping) for the Mistral Nemo model under server overload conditions, Swapping consistently consumes less energy. This is because Recomputation requires extra computation to restore requests, while Swapping only needs to copy data, and computation operations consume more energy than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","According to Figure 8 and its explanation in the context, when comparing the two preemption mechanisms (Recomputation vs Swapping) for the Mistral Nemo model under server overload conditions, Swapping consistently consumes less energy. This is because Recomputation requires extra computation to restore requests, while Swapping only needs to copy data, and computation operations consume more energy than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context snippets repeatedly reference Google's 2024 environmental report but do not provide any specific percentage increase in GHG emissions since 2019. While the report URL is mentioned multiple times, the actual emissions data is not included in the provided context.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippets repeatedly reference Google's 2024 environmental report but do not provide any specific percentage increase in GHG emissions since 2019. While the report URL is mentioned multiple times, the actual emissions data is not included in the provided context."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to multiple context snippets, data center electricity consumption is projected to reach between 9.1% and 11.7% of total US energy demand by 2030, as cited in the Fernandez 2025 paper.","9.1-11.7","percent","[""fernandez2025""]","is_blank","is_blank","According to multiple context snippets, data center electricity consumption is projected to reach between 9.1% and 11.7% of total US energy demand by 2030, as cited in the Fernandez 2025 paper."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","According to the paper, using the Pause and Resume optimization with 100% increase in job duration for the 6B parameter transformer model achieves an 11.4% emissions reduction when averaged across all 16 regions over the year. This represents the maximum potential emissions saving discussed in the context for this specific model and optimization approach.","11.4","percent","[""dodge2022""]","is_blank","is_blank","According to the paper, using the Pause and Resume optimization with 100% increase in job duration for the 6B parameter transformer model achieves an 11.4% emissions reduction when averaged across all 16 regions over the year. This represents the maximum potential emissions saving discussed in the context for this specific model and optimization approach."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","According to multiple mentions in the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B through its sparse activation architecture that only activates 2B parameters for each input token while having 8B total parameters.","70","percent","[""shen2024""]","is_blank","is_blank","According to multiple mentions in the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B through its sparse activation architecture that only activates 2B parameters for each input token while having 8B total parameters."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","According to the context, Strubell et al.'s 2019 seminal study quantified that training the BERT large language model resulted in 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","According to the context, Strubell et al.'s 2019 seminal study quantified that training the BERT large language model resulted in 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","According to multiple sources in the context, GPT-3's training energy consumption was 1,287 MWh. This figure is explicitly stated in both the Li 2025b paper and the Jegham 2025 paper.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","According to multiple sources in the context, GPT-3's training energy consumption was 1,287 MWh. This figure is explicitly stated in both the Li 2025b paper and the Jegham 2025 paper."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","Based on the context, the MoE layer is indeed a key target for optimization in LLM fine-tuning, as it is shown to be the most time-consuming layer accounting for 85% of overall execution time in both Mixtral and BlackMamba models during fine-tuning.","1","is_blank","[""xia2024""]","is_blank","is_blank","Based on the context, the MoE layer is indeed a key target for optimization in LLM fine-tuning, as it is shown to be the most time-consuming layer accounting for 85% of overall execution time in both Mixtral and BlackMamba models during fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","Based on the graphs in Fig. 14, when fine-tuning Mixtral on the Hellaswag dataset with a dense setup on an NVIDIA A40 GPU (48GB), the maximum batch size shown is 4. This can be seen in the top-left plot labeled 'Mixtral-CS' which shows batch size values up to 4 for the dense (non-sparse) configuration.","4","samples","[""xia2024""]","is_blank","is_blank","Based on the graphs in Fig. 14, when fine-tuning Mixtral on the Hellaswag dataset with a dense setup on an NVIDIA A40 GPU (48GB), the maximum batch size shown is 4. This can be seen in the top-left plot labeled 'Mixtral-CS' which shows batch size values up to 4 for the dense (non-sparse) configuration."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","According to the context, when distributing VMs over four continents (intercontinental training), CV models with high granularity experienced only a 7% slowdown in performance compared to local training. This is explicitly stated multiple times in the text.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, when distributing VMs over four continents (intercontinental training), CV models with high granularity experienced only a 7% slowdown in performance compared to local training. This is explicitly stated multiple times in the text."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not directly state the number of hectares occupied by new AI data centers globally in 2022. While several documents discuss data center growth and AI's increasing demand for data center space, none provide specific land area measurements in hectares for 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not directly state the number of hectares occupied by new AI data centers globally in 2022. While several documents discuss data center growth and AI's increasing demand for data center space, none provide specific land area measurements in hectares for 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","According to the 2024 study by Luccioni et al., the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries, which is significantly less than larger multi-purpose models like Flan-T5-XL (2.66g) and BLOOMz-7B (4.67g).","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","According to the 2024 study by Luccioni et al., the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries, which is significantly less than larger multi-purpose models like Flan-T5-XL (2.66g) and BLOOMz-7B (4.67g)."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","According to Figure 1 in the document, there was a range of approximately 19,000 grams between the most efficient region at 7,000 grams and least efficient region at 26,000 grams when training BERT across 16 different geographic regions.","19000","grams","[""dodge2022""]","is_blank","is_blank","According to Figure 1 in the document, there was a range of approximately 19,000 grams between the most efficient region at 7,000 grams and least efficient region at 26,000 grams when training BERT across 16 different geographic regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information peaked in 2022 with 10% of notable models releasing environmental information, before declining due to the introduction of increasingly commercial and proprietary models after 2022, catalyzed by ChatGPT's launch.","2022","year","[""luccioni2025c""]","is_blank","is_blank","According to the context, the direct release of environmental information peaked in 2022 with 10% of notable models releasing environmental information, before declining due to the introduction of increasingly commercial and proprietary models after 2022, catalyzed by ChatGPT's launch."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","According to Table 2 in morrison2025, the OLMo 60M model training consumed water equivalent to 5 days of usage by an average person in the US.","5","days","[""morrison2025""]","is_blank","is_blank","According to Table 2 in morrison2025, the OLMo 60M model training consumed water equivalent to 5 days of usage by an average person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","While the context shows that one technology company's data centers consumed over 23 billion liters of freshwater in 2023, it does not specifically state how much water Google's DeepMind AlphaFold servers used in 2023. Even though Google is mentioned briefly in relation to acknowledging AI's impact on data center water consumption, their specific AlphaFold water usage is not provided.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","While the context shows that one technology company's data centers consumed over 23 billion liters of freshwater in 2023, it does not specifically state how much water Google's DeepMind AlphaFold servers used in 2023. Even though Google is mentioned briefly in relation to acknowledging AI's impact on data center water consumption, their specific AlphaFold water usage is not provided."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","According to the paper, adding compute resources specifically to accelerate the MoE layers can help reduce costs in LLM fine-tuning. This is stated directly in the conclusions section and supported by their findings that MoE layers are the most computationally intensive component of fine-tuning.","1","is_blank","[""xia2024""]","is_blank","is_blank","According to the paper, adding compute resources specifically to accelerate the MoE layers can help reduce costs in LLM fine-tuning. This is stated directly in the conclusions section and supported by their findings that MoE layers are the most computationally intensive component of fine-tuning."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to Table 3 in the context, FLM-101B had an energy consumption of 40 MkWh (megakilowatt-hours) during training.","40","kWh","[""li2025a""]","is_blank","is_blank","According to Table 3 in the context, FLM-101B had an energy consumption of 40 MkWh (megakilowatt-hours) during training."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to the context, based on growth projections for data centers, their public health impact could reach more than $20 billion by 2028. This is specifically stated multiple times in the context, including that 'the growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028.'","20000000000","USD","[""han2024""]","is_blank","is_blank","According to the context, based on growth projections for data centers, their public health impact could reach more than $20 billion by 2028. This is specifically stated multiple times in the context, including that 'the growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028.'"
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","According to Table IV in the paper, fine-tuning Mixtral with sparse MoE on an NVIDIA A40-48GB GPU costs $32.7, based on their analytical model which considers throughput of 1.01 queries/sec and a cost of $0.79/hour.","32.7","USD","[""xia2024""]","is_blank","is_blank","According to Table IV in the paper, fine-tuning Mixtral with sparse MoE on an NVIDIA A40-48GB GPU costs $32.7, based on their analytical model which considers throughput of 1.01 queries/sec and a cost of $0.79/hour."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","According to Table III in the paper, for a sparse Mixtral model (Mixtral-S) running on MATH dataset, which was tested on an NVIDIA A40-48GB GPU, the maximum batch size supported was 3 samples.","3","samples","[""xia2024""]","is_blank","is_blank","According to Table III in the paper, for a sparse Mixtral model (Mixtral-S) running on MATH dataset, which was tested on an NVIDIA A40-48GB GPU, the maximum batch size supported was 3 samples."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","According to Table IV in the paper, when fine-tuning Mixtral with sparse MoE on an A40-48GB GPU with batch size=4, the throughput is 1.01 queries/second. Therefore, for one query, the execution time would be 1/1.01 ≈ 0.99 seconds.","0.99","seconds","[""xia2024""]","is_blank","is_blank","According to Table IV in the paper, when fine-tuning Mixtral with sparse MoE on an A40-48GB GPU with batch size=4, the throughput is 1.01 queries/second. Therefore, for one query, the execution time would be 1/1.01 ≈ 0.99 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements' because GPU measurements 'substantially underrepresents the actual energy consumption since it measures just a single component.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements' because GPU measurements 'substantially underrepresents the actual energy consumption since it measures just a single component.'"
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to the 2023 carbon footprint study of BLOOM, training accounted for only half (50%) of the model's overall emissions, with the rest coming from other factors like hardware manufacturing, idle energy usage, and model deployment.","50","percent","[""luccioni2025b""]","is_blank","is_blank","According to the 2023 carbon footprint study of BLOOM, training accounted for only half (50%) of the model's overall emissions, with the rest coming from other factors like hardware manufacturing, idle energy usage, and model deployment."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the context, LLaMA-7B can run on 1 V100 32GB GPU as shown in Table II which lists the bare minimum hardware requirements for different LLaMA variants. This is the minimum number of V100 32GB GPUs needed for running LLaMA-7B without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the context, LLaMA-7B can run on 1 V100 32GB GPU as shown in Table II which lists the bare minimum hardware requirements for different LLaMA variants. This is the minimum number of V100 32GB GPUs needed for running LLaMA-7B without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to Table 1 in the context, when GPT-3 is deployed in Arizona, it takes 16.7 requests to consume 500ml of water. This number is directly provided in the '# of Requests for 500ml Water' column for the Arizona location.","16.7","requests","[""li2025b""]","is_blank","is_blank","According to Table 1 in the context, when GPT-3 is deployed in Arizona, it takes 16.7 requests to consume 500ml of water. This number is directly provided in the '# of Requests for 500ml Water' column for the Arizona location."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","According to multiple context snippets from dodge2022, the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","According to multiple context snippets from dodge2022, the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","Based on the provided context snippets, there is no explicit mention of JetMoE-8B's performance on the GSM8k benchmark. While the documents discuss various performance metrics like MT-Bench scores, they do not contain any information about GSM8k benchmark results.","is_blank","score","[""is_blank""]","is_blank","is_blank","Based on the provided context snippets, there is no explicit mention of JetMoE-8B's performance on the GSM8k benchmark. While the documents discuss various performance metrics like MT-Bench scores, they do not contain any information about GSM8k benchmark results."
