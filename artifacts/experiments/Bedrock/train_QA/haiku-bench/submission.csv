"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context presents the ML.ENERGY Benchmark, a benchmark suite and tool for measuring the inference energy consumption of generative AI models.","The ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context presents the ML.ENERGY Benchmark, a benchmark suite and tool for measuring the inference energy consumption of generative AI models."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context indicates that the training footprint of the GShard-600B model is shown in the graph, and it has a carbon footprint of around 0.5 million kg CO2e.","0.5","tCO2e","[""wu2021a""]","is_blank","is_blank","The context indicates that the training footprint of the GShard-600B model is shown in the graph, and it has a carbon footprint of around 0.5 million kg CO2e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context indicates that the LLaMA-33B model has 64.7 GB of parameters.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context indicates that the LLaMA-33B model has 64.7 GB of parameters."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide any information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The context focuses on measuring the carbon intensity and electricity consumption of AI workloads on a single cloud instance, rather than on the overall electricity consumption of Google Cloud TPU pods.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The context focuses on measuring the carbon intensity and electricity consumption of AI workloads on a single cloud instance, rather than on the overall electricity consumption of Google Cloud TPU pods."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers, based on the power usage effectiveness (PUE) metric.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers, based on the power usage effectiveness (PUE) metric."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context indicates that for every 10-50 medium-length responses on GPT-3, the model 'drinks' around half a liter (500 mL) of water.","10 - 50","500 mL bottles","[""li2025b""]","is_blank","is_blank","The context indicates that for every 10-50 medium-length responses on GPT-3, the model 'drinks' around half a liter (500 mL) of water."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The provided context shows that 75% of CVPR papers target accuracy while 20% target efficiency, indicating a difference of 55 percentage points between accuracy-focused and efficiency-focused CVPR papers.","55","percent","[""schwartz2019""]","is_blank","is_blank","The provided context shows that 75% of CVPR papers target accuracy while 20% target efficiency, indicating a difference of 55 percentage points between accuracy-focused and efficiency-focused CVPR papers."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context does not indicate that the AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public. The context discusses omissions in the AI Act regarding transparency and disclosure of energy consumption.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The context does not indicate that the AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public. The context discusses omissions in the AI Act regarding transparency and disclosure of energy consumption."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context indicates that the projected maximum batch size for fine-tuning a Mixtral model on GPUs with 100GB and 120GB of memory capacity is 28 and 35 samples, respectively.","28 and 35","samples","[""xia2024""]","is_blank","is_blank","The context indicates that the projected maximum batch size for fine-tuning a Mixtral model on GPUs with 100GB and 120GB of memory capacity is 28 and 35 samples, respectively."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context indicates that the A100 GPU outperformed the V100 GPU for the smaller LLaMA 7B and 13B models, showing anywhere from a 2 times (7B) to a 1.25 times (13B) increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second.","1","multiplier","[""samsi2024""]","is_blank","is_blank","The context indicates that the A100 GPU outperformed the V100 GPU for the smaller LLaMA 7B and 13B models, showing anywhere from a 2 times (7B) to a 1.25 times (13B) increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides an estimate of the total operational water consumption for training the full GPT-3 model with 175 billion parameters across different Microsoft data center locations in the U.S. and other countries. The total water consumption for training GPT-3 is estimated to be 5.439 million liters.","5.439","liters","[""li2025b""]","is_blank","is_blank","The context provides an estimate of the total operational water consumption for training the full GPT-3 model with 175 billion parameters across different Microsoft data center locations in the U.S. and other countries. The total water consumption for training GPT-3 is estimated to be 5.439 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context clearly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models, but should also apply to all AI systems regardless of the associated risk to health or safety.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models, but should also apply to all AI systems regardless of the associated risk to health or safety."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context indicates that in 2023, AWS improved its water use effectiveness (WUE) for its global data centers to 0.18 liters per kilowatt-hour (L/kWh), a 5% improvement from 0.19 L/kWh in 2022.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context indicates that in 2023, AWS improved its water use effectiveness (WUE) for its global data centers to 0.18 liters per kilowatt-hour (L/kWh), a 5% improvement from 0.19 L/kWh in 2022."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context indicates that local inference, which reduces network overhead and carbon footprint, was emphasized as a sustainability measure for deploying large language models.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context indicates that local inference, which reduces network overhead and carbon footprint, was emphasized as a sustainability measure for deploying large language models."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context indicates that tracking the runtime of a training job is an important step for estimating the compute cost in GPU-based or cloud environments, as it allows for the calculation of the total energy consumed during training.","1","is_blank","[""luccioni2023""]","is_blank","is_blank","The context indicates that tracking the runtime of a training job is an important step for estimating the compute cost in GPU-based or cloud environments, as it allows for the calculation of the total energy consumed during training."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% improvement in latency reduction by enabling the automated resource utilization overlapping feature.","13.2","percent","[""chen2024""]","is_blank","is_blank","The context states that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% improvement in latency reduction by enabling the automated resource utilization overlapping feature."
"q164","How much does an elephant weigh?","The context does not contain any information about the weight of an elephant. There are no details provided about the size or weight of elephants.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not contain any information about the weight of an elephant. There are no details provided about the size or weight of elephants."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context states that the DeepSeek-R1 (DS) model consumes significantly more energy (29.075 Wh) than the most efficient model, LLaMA-3.1-8B (0.443 Wh), which is around 65 times more energy consumption.","DeepSeek-R1 (DS)","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that the DeepSeek-R1 (DS) model consumes significantly more energy (29.075 Wh) than the most efficient model, LLaMA-3.1-8B (0.443 Wh), which is around 65 times more energy consumption."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context mentions that the BERT base model was trained on 16 TPU chips for 4 days, which consumed 626,155 pounds of CO2 emissions.","626,155","days","[""strubell2019""]","is_blank","is_blank","The context mentions that the BERT base model was trained on 16 TPU chips for 4 days, which consumed 626,155 pounds of CO2 emissions."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide any information about whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow. The provided context snippets focus on other aspects of transformers and transformer architectures, but do not discuss their comparative performance on the WMT'24 EN-DE BLUE task.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide any information about whether the Transformer architecture eventually outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow. The provided context snippets focus on other aspects of transformers and transformer architectures, but do not discuss their comparative performance on the WMT'24 EN-DE BLUE task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not mention a dataset of 5,842 labeled entries that was used to test energy-efficient large language models in the financial domain. The information provided in the context is not sufficient to answer this question.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention a dataset of 5,842 labeled entries that was used to test energy-efficient large language models in the financial domain. The information provided in the context is not sufficient to answer this question."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context indicates that by using multiple T4 spot instances, it is possible to be more cost-efficient than using a single DGX-2 node for distributed training. The comparison shows that eight T4 spot instances are more cost-efficient than a DGX-2 node.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context indicates that by using multiple T4 spot instances, it is possible to be more cost-efficient than using a single DGX-2 node for distributed training. The comparison shows that eight T4 spot instances are more cost-efficient than a DGX-2 node."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI, according to the context.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI, according to the context."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context states that the German 2023 Energy Efficiency Act requires data centers to run on 100% renewable energy by January 1, 2027.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the German 2023 Energy Efficiency Act requires data centers to run on 100% renewable energy by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The figure shows that 10% of ACL papers targeted both accuracy and efficiency, so the answer is 6 papers targeted both accuracy and efficiency out of the sample of 60 papers.","6","papers","[""schwartz2019""]","is_blank","is_blank","The figure shows that 10% of ACL papers targeted both accuracy and efficiency, so the answer is 6 papers targeted both accuracy and efficiency out of the sample of 60 papers."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context indicates that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025""]","is_blank","is_blank","The context indicates that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase of AI models, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase of AI models, which is a crucial omission given the long-term environmental impact of AI applications."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context indicates that the AI Act currently does not require providers to report on energy use during the inference phase of AI models, which is a significant omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act currently does not require providers to report on energy use during the inference phase of AI models, which is a significant omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context indicates that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, rather than air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context indicates that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, rather than air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that platform-level caching for the cross-lingual Transformer language model improved the power efficiency by 6.7x compared to a CPU server baseline.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that platform-level caching for the cross-lingual Transformer language model improved the power efficiency by 6.7x compared to a CPU server baseline."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context states that the BERT base model, which used 64 V100 GPUs, consumed 1507 kWh of electricity, which corresponds to 1438 lbs of CO2 emissions after accounting for PUE.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context states that the BERT base model, which used 64 V100 GPUs, consumed 1507 kWh of electricity, which corresponds to 1438 lbs of CO2 emissions after accounting for PUE."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to the context, recent studies have reported that ML inference accounts for 80-90% of total ML cloud computing demand.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","According to the context, recent studies have reported that ML inference accounts for 80-90% of total ML cloud computing demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training the 6.1 billion parameter language model consumed around 13,812.4 kWh of electricity over 8 days on 256 NVIDIA A100 GPUs. This training run was estimated to consume approximately 103,500 kWh or 103.5 MWh of electricity if trained to completion, which is almost 2800 times more than training the smaller BERT-small model.","103.5","household-years","[""dodge2022""]","is_blank","is_blank","The context states that training the 6.1 billion parameter language model consumed around 13,812.4 kWh of electricity over 8 days on 256 NVIDIA A100 GPUs. This training run was estimated to consume approximately 103,500 kWh or 103.5 MWh of electricity if trained to completion, which is almost 2800 times more than training the smaller BERT-small model."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context states that for NLP experiments, the external egress costs on Google Cloud (GC) can account for more than 90% of the total cost per VM, indicating that egress costs can be a major portion of the overall cost.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that for NLP experiments, the external egress costs on Google Cloud (GC) can account for more than 90% of the total cost per VM, indicating that egress costs can be a major portion of the overall cost."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained with 30,000 H100 GPU hours. To estimate the total wall-clock time in days, we would need to know the number of GPUs used.","is_blank","days","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B was trained with 30,000 H100 GPU hours. To estimate the total wall-clock time in days, we would need to know the number of GPUs used."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines 'water consumption' as 'water withdrawal minus water discharge', meaning the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context defines 'water consumption' as 'water withdrawal minus water discharge', meaning the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context indicates that the observed range of energy per second for LLaMA-65B was 300 Watts to 1 Kilowatt, across GPU shard configurations ranging from 8 to 32 GPUs.","is_blank","W","[""samsi2024""]","is_blank","is_blank","The context indicates that the observed range of energy per second for LLaMA-65B was 300 Watts to 1 Kilowatt, across GPU shard configurations ranging from 8 to 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the 72B version of the Qwen model consumed seven times more energy than the 7B version, with only a minor accuracy reduction.","7","multiplier","[""zschache2025""]","is_blank","is_blank","The context states that the 72B version of the Qwen model consumed seven times more energy than the 7B version, with only a minor accuracy reduction."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","","","percent","[]","is_blank","is_blank",""
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The passage states that the ML.ENERGY Benchmark benchmarks 'generative AI model architectures across a wide range of tasks – including Large Language Model (LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models'.","40","models","[""chung2025""]","is_blank","is_blank","The passage states that the ML.ENERGY Benchmark benchmarks 'generative AI model architectures across a wide range of tasks – including Large Language Model (LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models'."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context states that training an AI model of the Llama-3.1 scale in Altoona, Iowa would result in a health cost of $2.5 million, which exceeds 120% of the training electricity cost.","2500000","USD","[""han2024""]","is_blank","is_blank","The context states that training an AI model of the Llama-3.1 scale in Altoona, Iowa would result in a health cost of $2.5 million, which exceeds 120% of the training electricity cost."
