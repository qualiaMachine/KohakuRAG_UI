"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet [ref_id=wu2021b] provides the average increase in U.S. data center electricity consumption between 2010 and 2014 as 4%.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet [ref_id=wu2021b] provides the average increase in U.S. data center electricity consumption between 2010 and 2014 as 4%."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023"", ""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides an estimate for the embodied carbon emissions per GPU, which is used in the analysis since NVIDIA does not release this information.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The context provides an estimate for the embodied carbon emissions per GPU, which is used in the analysis since NVIDIA does not release this information."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40 million, while the total training budget for FLM-101B is $101,000 (as derived from the GPU hours and chip power/TDP). The ratio of these two values is 396.04.","396.04","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","The estimated amortized training cost of GPT-4 is $40 million, while the total training budget for FLM-101B is $101,000 (as derived from the GPU hours and chip power/TDP). The ratio of these two values is 396.04."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context mentions that Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip from San Francisco to New York.","1.2","tCO2e","[""patterson2021""]","is_blank","Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.","The context mentions that Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip from San Francisco to New York."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard.","43.94","score","[""[ref_id=li2025a]""]","is_blank","Results. On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that there was a 'more than 6,750 fold improvement in processor clock speed' between the Intel 4004 (1971) and typical 2021 microprocessors.","6750","fold","[""wu2021b""]","is_blank","'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.'","The context states that there was a 'more than 6,750 fold improvement in processor clock speed' between the Intel 4004 (1971) and typical 2021 microprocessors."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides the exact number of days it took to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec, which is 14.8 days.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context provides the exact number of days it took to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec, which is 14.8 days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table with measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The specific row for the Llama 3.2 1B model at an 8 request/s frequency shows the GPU Power Usage in kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The context provides a table with measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The specific row for the Llama 3.2 1B model at an 8 request/s frequency shows the GPU Power Usage in kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context specifies that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context specifies that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context does not provide a specific estimate for the number of premature deaths in 2030 caused by scope-2 pollutants from U.S. data centers alone. It only provides estimates for 2028.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context does not provide a specific estimate for the number of premature deaths in 2030 caused by scope-2 pollutants from U.S. data centers alone. It only provides estimates for 2028."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that the 6 billion parameter transformer model was trained for 8 days and only reached 13% completion. Therefore, to estimate the full training duration, we can multiply the 8 days by 100/13, which gives approximately 60 days.","60","days","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days.","The context states that the 6 billion parameter transformer model was trained for 8 days and only reached 13% completion. Therefore, to estimate the full training duration, we can multiply the 8 days by 100/13, which gives approximately 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context provides a table listing various models and their GPU energy consumption for 1,000 queries. The model with the highest GPU energy consumption is 'Command-R Plus' with 3,426.38 Wh.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context provides a table listing various models and their GPU energy consumption for 1,000 queries. The model with the highest GPU energy consumption is 'Command-R Plus' with 3,426.38 Wh."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","is_blank","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of  2014, is a long-term investigation of  the field of  Artificial Intelligence (AI) and its influences on people, their communities, and society .","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context states that the UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context states that the UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides an estimate for the energy consumption of pre-training the BLOOM model, which is 3,500 MWh.","3500","MWh","[""luccioni2025c""]","is_blank","From the limited data that is publicly available, we can observe significant disparities in energy use and emissions across models. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides an estimate for the energy consumption of pre-training the BLOOM model, which is 3,500 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context snippet provides the percentage of the Switch Transformer's parameters that are activated per token. The answer is directly extracted from the table in the context.","0.1","percent","[""patterson2021""]","is_blank","Table 4. CO2e for NLP models (see Appendix A) 12 . V100’s TDP is closer to average power due to Turbo mode and DVFS . TPUs don’t offer them, so their TDP is much higher than their average power. 12 The peak TeraFLOPS/second is 46 for TPU v2, 123 for TPU v3, and 125 for V100. 6 Model Evolved Trans- former NAS T5 Meena Gshard -600B Switch Trans- former GPT-3 Number of Parameters (B) 0.064 per model 11 2.6 619 1500 175 Percent of model activated on every token 100% 100% 100% 0.25% 0.10% 100% Developer Google OpenAI Datacenter of original experiment Google Georgia Google Taiwan Google Georgia Google North Carolina Google Georgia Microsoft When model ran Dec 2018 Sep 2019 Dec 2019 Apr 2020 Oct 2020 2020 Datacenter Gross CO2e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.201 0.403 0.429 Datacenter Net CO2e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.177 0.330 0.429 Datacenter PUE (when it was run) 1.10 1.12 1.09 1.09 1.10 1.10 Processor TPU v2 TPU v3 V100 Chip Thermal Design Power (TDP in Watts) 280 450 300 Measured System Average Power per Accelerator, including memory, network interface, fans, host CPU (W) 208 310 289 288 245 330 Measured Performance (TFLOPS/s) 12 24.8 45.6 42.3 48.0 34.4 24.6 Number of Chips 200 512 1024 1024 1024 10,000 Training time (days) 6.8 20 30 3.1 27 14.8 Total Computation (floating point operations) 2.91E+21 4.05E+22 1.12E+23 1.33E+22 8.22E+22 3.14E+23 Energy Consumption (MWh) 7.5 85.7 232 24.1 179 1,287 % of Google 2019 total energy consumption (12.2 TWh = 12,200,000 MWh) [Goo20] 0.00006% 0.00070% 0.00190% 0.00020% 0.00147% 0.01055% Gross tCO2e for Model Training 3.2 46.7 96.4 4.8 72.2 552.1 Net tCO2e for Model Training 3.2 46.7 96.4 4.3 59.1 552.1 Fraction of NAS Estimate in [Str19] (284 tCO2e) 0.011 0.164 0.340 0.015 0.208 1.944 Fraction of equivalent jet plane CO2e round trip San Francisco ↔ New York (~180 t; see Ap. A) 0.018 0.258 0.533 0.024 0.327 3.054 tCO2e savings by Meena using Evolved Transformer -- -- 48.5 -- -- -- % 24/x7 carbon free energy (when run) 31% 19% 30% 73% 43% N/A","The context snippet provides the percentage of the Switch Transformer's parameters that are activated per token. The answer is directly extracted from the table in the context."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context specifies that JetMoE-8B uses 8 experts in each MoE layer, as indicated in the hyperparameters table and the description of the model architecture.","8","experts","[""shen2024""]","is_blank","The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context specifies that JetMoE-8B uses 8 experts in each MoE layer, as indicated in the hyperparameters table and the description of the model architecture."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a breakdown of execution time for different models and batch sizes, but it does not specify the total execution time for a dense BlackMamba model with a batch size of 30 on a NVIDIA A40-48 GB GPU.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context provides a breakdown of execution time for different models and batch sizes, but it does not specify the total execution time for a dense BlackMamba model with a batch size of 30 on a NVIDIA A40-48 GB GPU."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides the cost estimation for training the English portion of the FLM-101B model, which is 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context provides the cost estimation for training the English portion of the FLM-101B model, which is 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context specifies the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context specifies the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context mentions 'The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.' This indicates that 80 different machine learning models were sampled and analyzed.","80","models","[""luccioni2024"", ""luccioni2024"", ""luccioni2024"", ""luccioni2024"", ""luccioni2024""]","is_blank","The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","The context mentions 'The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.' This indicates that 80 different machine learning models were sampled and analyzed."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3 times.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3 times."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","[1.2,4]","multiplier","[""cottier2024""]","is_blank","is_blank","The context states that the total compute for model development is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context snippet [ref_id=dodge2022] mentions that the total energy consumption to train the 6 billion parameter transformer to completion would be approximately 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context snippet [ref_id=dodge2022] mentions that the total energy consumption to train the 6 billion parameter transformer to completion would be approximately 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context discusses how efficiency gains in AI may paradoxically lead to increased consumption, a phenomenon known as Jevons' Paradox. This principle suggests that improvements in efficiency can lead to higher overall consumption, undermining the assumption that technical efficiency alone will ensure net reductions in environmental harm.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","The addition of increasingly efficient AI to systems from commerce to transportation can have far-reaching effects on our societies, our behaviors, and the future paths available to us in the race against climate change. [...] Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context discusses how efficiency gains in AI may paradoxically lead to increased consumption, a phenomenon known as Jevons' Paradox. This principle suggests that improvements in efficiency can lead to higher overall consumption, undermining the assumption that technical efficiency alone will ensure net reductions in environmental harm."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","[4.2,6.6]","billion cubic meters","[""li2025b""]","is_blank","The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost, not on the decline.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost, not on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that the total time cost for training FLM-101B using a growth strategy is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B using a growth strategy is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, which is significantly less than 80%.","0","is_blank","[""wu2021a"", ""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, which is significantly less than 80%."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippet explicitly states that the training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","The context snippet explicitly states that the training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing models across different tasks, which aligns with the question's requirement for a standardized method for comparing inference efficiency.","AI Energy Score","is_blank","[""luccioni2025c"", ""luccioni2025c""]","is_blank","For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing models across different tasks, which aligns with the question's requirement for a standardized method for comparing inference efficiency."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a detailed breakdown of the execution time for various kernels in the MoE layer for both Mixtral and BlackMamba models. For a dense BlackMamba model with a batch size of 30, the longest kernel execution time is for 'matmul(w1)', which is 2000 microseconds.","2000","microseconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0
400
800
1200
1600
2000
matmul(w1)
gelu
matmul(w2)
elementwise_mult
top_k
sigmoid
matmul_(router)","The context provides a detailed breakdown of the execution time for various kernels in the MoE layer for both Mixtral and BlackMamba models. For a dense BlackMamba model with a batch size of 30, the longest kernel execution time is for 'matmul(w1)', which is 2000 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context specifies that in the JetMoE-8B model, the top-k experts are selected for activation in each layer. The value of top-k is explicitly mentioned as 2.","2","experts","[""shen2024""]","is_blank","Of these, we select the top k experts. When k < N , we are using a Sparse Mixture of Experts (SMoE, Shazeer et al. 2017).","The context specifies that in the JetMoE-8B model, the top-k experts are selected for activation in each layer. The value of top-k is explicitly mentioned as 2."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet directly states that global carbon emissions dropped by 6.4% in 2020 due to the COVID-19 pandemic.","6.4","percent","[""wu2021b"", ""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3"", ""https://www.nature.com/articles/d41586-021-00090-3""]","['In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].', 'Looking forward, information technology can improve efficiencies in practically every sector, from manufacturing to food production to transportation to controlling the climate in our homes and offices. Although there is a carbon cost associated with manufacturing and operating the IT ecosystem, this cost must be evaluated holistically [Chang et al., 2010, Bardon et al., 2020, Gupta et al., 2021, Patterson et al., 2021] in light of the benefits such an ecosystem can provide in other domains [Hager et al., 2019, Tomasev et al., 2020, Mulhern, 2021, United Nation, 2021].']","The context snippet directly states that global carbon emissions dropped by 6.4% in 2020 due to the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources.","22","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy 
Amazon’s energy supply from utilities, combined with the 
renewable energy we procure globally, means that 100% of 
the electricity consumed by 22 AWS data center regions is 
matched with renewable energy sources—an increase from 
19 regions in 2022.‡","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context mentions that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we subtract 1956 from 2025.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context mentions that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we subtract 1956 from 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context specifies that the 'five cars' estimate is based on a large-scale procedure called neural architecture search (NAS), which is performed less frequently than average AI model training.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context specifies that the 'five cars' estimate is based on a large-scale procedure called neural architecture search (NAS), which is performed less frequently than average AI model training."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch sizes supported by different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8 samples.","8","samples","[""xia2024""]","is_blank","Table III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE. BlackMamba-S MATH 8","The context provides a table (Table III) that lists the maximum batch sizes supported by different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context snippet states that Amazon held 1.3 GW of storage capacity in 2023, which directly answers the question.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context snippet states that Amazon held 1.3 GW of storage capacity in 2023, which directly answers the question."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions of GPT-4o are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions of GPT-4o are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific information on the percentage of AI inference workloads in Asia powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the percentage of AI inference workloads in Asia powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides the global average PUE of data centers in 2023, which is relevant to AI-dedicated data centers as they are a subset of all data centers.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the global average PUE of data centers in 2023, which is relevant to AI-dedicated data centers as they are a subset of all data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context mentions that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference.","2000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context mentions that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table with GHG emissions for various models, including the Llama 7B model, which is listed as having 14 tCO2e emissions.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel Organization Energy Consumption (MWh) GHG Emissions (tCO2e)\nLlama 7B 63 Meta 356 14","The context provides a table with GHG emissions for various models, including the Llama 7B model, which is listed as having 14 tCO2e emissions."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in 2022 and 2023 for the U.S., Europe, and India. By summing the differences between the 2023 and 2022 figures for each region, we can determine the total number of electric delivery vans added across both years.","17200","electric delivery vans","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","U.S.: 11,800 (2023) - 2,600 (2022) = 9,200
Europe: 3,000+ (2023) - 1,220 (2022) = 1,780+
India: 7,200+ (2023) - 3,800 (2022) = 3,400+","The context provides the number of electric delivery vans in 2022 and 2023 for the U.S., Europe, and India. By summing the differences between the 2023 and 2022 figures for each region, we can determine the total number of electric delivery vans added across both years."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context mentions that Power Usage Effectiveness (PUE) accounts for non-computational energy overheads such as cooling, indicating that GHG emissions from cooling are included in the operational environmental impacts of LLMs.","0","is_blank","[""jegham2025""]","is_blank","PUE accounts for non-computational energy overheads such as cooling, lighting, and power distribution.","The context mentions that Power Usage Effectiveness (PUE) accounts for non-computational energy overheads such as cooling, indicating that GHG emissions from cooling are included in the operational environmental impacts of LLMs."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides a table (Table 4) that lists the energy consumption of various models for different prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is given as 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","The context provides a table (Table 4) that lists the energy consumption of various models for different prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is given as 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially christened at a workshop in 1956.","1956","year","[""stone2022"", ""stone2022"", ""stone2022"", ""stone2022"", ""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context explicitly states that the field of Artificial Intelligence was officially christened at a workshop in 1956."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency.","1","is_blank","[""wu2021b"", ""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity"", ""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context mentions that with a max generation length of 512, it takes about 3-4 Joules for an output token.","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context mentions that with a max generation length of 512, it takes about 3-4 Joules for an output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context snippet states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed, indicating a lack of clear, publicly available calculations and sound scientific grounding.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context states that the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed, indicating a lack of clear, publicly available calculations and sound scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context snippet directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context snippet directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the average energy consumption for the Flan-T5-xxl model as 0.083 kWh per 1,000 queries. Given that Google Translate can perform 1 billion queries per day, we can calculate the total daily energy consumption by multiplying the energy per 1,000 queries by the number of queries and converting the result to MWh.","83","MWh","[""luccioni2024""]","[""https://blog.google/products/translate/ten-years-of-google-translate/""]","By painting a more detailed picture of widely varying energy requirements for ML model 1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day. Source: https://blog.google/products/translate/ten-years-of-google-translate/","The context provides the average energy consumption for the Flan-T5-xxl model as 0.083 kWh per 1,000 queries. Given that Google Translate can perform 1 billion queries per day, we can calculate the total daily energy consumption by multiplying the energy per 1,000 queries by the number of queries and converting the result to MWh."
"q067","What was the average global data center PUE in 2023?","The context snippet directly states that the average global data center PUE in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context snippet directly states that the average global data center PUE in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide specific information on the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, including equity.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, including equity."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context mentions that the'seventeen-member Study Panel' was launched in mid-fall 2015, indicating the number of members in the inaugural panel.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of  experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context mentions that the'seventeen-member Study Panel' was launched in mid-fall 2015, indicating the number of members in the inaugural panel."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context states that generally, models with more parameters consume more energy, but this is not always the case. This indicates that there are exceptions where a model with more parameters might not necessarily consume more energy.","0","is_blank","[""chung2025""]","is_blank","Generally, models with more parameters consume more energy, but this is not always the case.","The context states that generally, models with more parameters consume more energy, but this is not always the case. This indicates that there are exceptions where a model with more parameters might not necessarily consume more energy."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific information on the metric tons of CO2 emitted by OpenAI's API requests in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the metric tons of CO2 emitted by OpenAI's API requests in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","'Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.'","The context states that Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context snippet [ref_id=wu2021a] explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years.","2.9","multiplier","[""wu2021a""]","is_blank","The explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context snippet [ref_id=wu2021a] explicitly states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years."
"q079","How many miles is the Earth from the Sun?","The context does not provide information about the distance of the Earth from the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide information about the distance of the Earth from the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context mentions that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match.","1","is_blank","[""stone2022"", ""schwartz2019"", ""wu2021a""]","is_blank","The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context mentions that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.' This directly answers the question about the batching strategy that reduces idle GPU time.","Continuous batching","is_blank","[""fernandez2025"", ""chen2024""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.' This directly answers the question about the batching strategy that reduces idle GPU time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context snippet mentions that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context snippet mentions that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that with a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100.","26.7","percent","[""kim2025""]","is_blank","With a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that with a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) generates as much carbon as 0.0006 miles driven by a similar vehicle, i.e.","The context snippet directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a table with GPU energy usage for 1,000 inference queries ranging from 0.06 Wh to 3,426.38 Wh.","[0.06,3426.38]","Wh","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)","The context provides a table with GPU energy usage for 1,000 inference queries ranging from 0.06 Wh to 3,426.38 Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI, indicating that a universal approach cannot be developed.","0","is_blank","[""luccioni2025b"", ""luccioni2025b""]","is_blank","""There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.""","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI, indicating that a universal approach cannot be developed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context snippet directly states the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context snippet directly states the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as the decentralized PyTorch-based framework used for distributed spot instance training.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions Hivemind as the decentralized PyTorch-based framework used for distributed spot instance training."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly mentions 'social transparency' as the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system.","social transparency","is_blank","[""luccioni2025b"", ""luccioni2025b"", ""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context explicitly mentions 'social transparency' as the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that the linear model with sentence embeddings achieved the highest accuracy in classification experiments on German public administration texts.","linear model with sentence embeddings","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets""]","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context mentions that the linear model with sentence embeddings achieved the highest accuracy in classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions the development of an LLM inference system named Lamina, which incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions the development of an LLM inference system named Lamina, which incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context mentions that the largest T5 model has 11 billion parameters.","11000000000","parameters","[""patterson2021""]","is_blank","The context states: 'Number of Parameters (B) 0.064 per model 11 2.6 619 1500 175'.","The context mentions that the largest T5 model has 11 billion parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters.","8000000000","parameters","[""shen2024""]","is_blank","The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer. Such that the model has approximately two times the computation compared to a 1B model.","The context explicitly states that JetMoE-8B has 8B parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022.","20","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed', which matches the question's description of the emissions metric.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/ kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed', which matches the question's description of the emissions metric."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides a table (Table 2) that lists the FLOPs utilization percentages for different growth stages of FLM-101B training. The final growth stage, with 101 billion parameters, achieved a FLOPs utilization percentage of 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 101 4 4 12 192 2160 165 52.88%","The context provides a table (Table 2) that lists the FLOPs utilization percentages for different growth stages of FLM-101B training. The final growth stage, with 101 billion parameters, achieved a FLOPs utilization percentage of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context snippet from 'cottier2024' explicitly states that the estimated amortized training cost for OpenAI's GPT-4 is $40M. This value is directly extracted and formatted as per the specified 'answer_unit' of USD.","40000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context snippet from 'cottier2024' explicitly states that the estimated amortized training cost for OpenAI's GPT-4 is $40M. This value is directly extracted and formatted as per the specified 'answer_unit' of USD."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","810","multiplier","[""wu2021a"", ""wu2021a"", ""wu2021a"", ""wu2021a"", ""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The context states that optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that NLP is 36% slower when training across four continents compared to local training. This means that the throughput achieved is 64% of the local throughput.","0.64","multiplier","[""erben2023""]","is_blank","However, at NLP, things get more interesting as we now have more overall communication with four peers, but they can average locally first and only later transmit across the Atlantic. However, compared to their A-counterparts, we do not see a difference in relative scalability with either B-4, B-6, or B-8. This means that training across regions (B) is slower, but the contribution per GPU decreases at the same rate as in training within a zone(A). The per-GPU speedup with additional hardware reduces at the same rate for either setup (between 0.05 and 0.06). This results in two observations: First, communication overhead scales linearly with the number of peers. Second, we only have to pay the penalty for transatlantic training once. However, we cannot expect a significant improvement in communication efficiency when we increase the amount of available local resources. Summarizing, with an transatlantic setup, CV achieves a virtually identical maximum speedup of 3.2x with 8 GPUs compared to A-1 (B-8 is 2% slower than A-8), while NLP is more affected by lower network bandwidth and only achieves a speedup of 2.15x (B-8 is 22% slower than A-8). The transatlantic training penalty is applied once; however, it does not affect the relative scaling with additional compute resources. (C) Intercontinental scalability.To take geo-distribution to the extreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS, to see how much worse bandwidth affects the training throughput (C-3,4,6,8 in Table 2). How does the intercontinental penalty investigated in(B) affect deployments with a single GPU on each continent? Comparing the A-3 and C-3 experiments with three local versus three fully remote GPUs, CV is only 5% slower, while NLP suffers a 34% drop in throughput (Figure 9a) and does not even reach the baseline single GPU performance (A-1). The peak egress for each region was 318, 258, and 237 Mb/s for the US, EU, and ASIA, respectively. Since our bandwidth measurements were 210 and 130 Mb/s from the US to the EU and ASIA, respectively (Table 3), this suggests that the averaging was done over the US node and not an N-to-N all-reduce (a detailed analysis of how averaging affects bandwidths is discussed in Section 6). Thus, the limiting factor was the US-ASIA connection at 130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend continues with the C-4 run, which adds AUS as a continent with one additional VM. As we know from the transatlantic experiments(B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a). Again, the US VM is used as an averaging intermediary with a peak egress of 365 Mb/s, while the other continents are between 318 and 330 Mb/s. When comparing the two continents (B-4) versus four continents (C-4) experiments, one GPU on each continent (C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs on two continents (B-4). This reinforces that local hardware should be preferred whenever possible. However, we are always faster than the baseline (A-1), starting from 4 GPUs in both the transatlantic and intercontinental settings. While these experiments were specifically designed to be a worst-case scenario, what about a more balanced GPU distribution with at least two GPUs in each region?","The context states that NLP is 36% slower when training across four continents compared to local training. This means that the throughput achieved is 64% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3500000000","liters","[""amazon2023"", ""amazon2023"", ""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states that the energy consumption of LLMs can be reduced by using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.","The context explicitly states that the energy consumption of LLMs can be reduced by using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that 'NVIDIA shipped 3.7 million GPUs in 2024' in a 2025 paper.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context mentions that 'NVIDIA shipped 3.7 million GPUs in 2024' in a 2025 paper."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context mentions that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, indicating their efficiency compared to typical data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","[""https://sustainability.fb.com/report-page/data-centers/""]","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers.","The context mentions that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, indicating their efficiency compared to typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish project 'ETAIROS' which stands for 'Ethical AI for the Governance of the Society'. This project proposed the integration of ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133].","The context mentions the Finnish project 'ETAIROS' which stands for 'Ethical AI for the Governance of the Society'. This project proposed the integration of ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context snippet directly states that the estimated amortized training cost for Google's Gemini Ultra is $30M. The answer unit is USD, so the answer value is expressed as 30000000.","30000000","USD","[""cottier2024""]","is_blank","The most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context snippet directly states that the estimated amortized training cost for Google's Gemini Ultra is $30M. The answer unit is USD, so the answer value is expressed as 30000000."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context explicitly states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and that these measures should consider environmental risks.","1","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context explicitly states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and that these measures should consider environmental risks."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³.","9","µg/m³","[""han2024"", ""han2024"", ""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a"", ""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/"", ""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","['For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].', 'For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].']","The context states that a life cycle assessment found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","The context states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides a table (Table B1) that lists the energy consumption for various models, including the DS Llama 70B model, on the FKTG dataset. The energy consumption for the DS Llama 70B model is given as 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","Table B1: Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs.","The context provides a table (Table B1) that lists the energy consumption for various models, including the DS Llama 70B model, on the FKTG dataset. The energy consumption for the DS Llama 70B model is given as 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide information about a 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information about a 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the phenomenon of technological progress improving efficiency, which then results in increased usage and overall resource consumption as 'Jevons’ Paradox'.","Jevons’ Paradox","is_blank","[""luccioni2025a"", ""fernandez2025"", ""jegham2025""]","is_blank","is_blank","The context describes the phenomenon of technological progress improving efficiency, which then results in increased usage and overall resource consumption as 'Jevons’ Paradox'."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides the energy consumption for both Meena and GPT-3 training runs. By dividing the energy consumption of GPT-3 by that of Meena, we can determine how many Meena training runs would use the same total energy as a single full training run of GPT-3.","53.6","multiplier","[""patterson2021""]","is_blank","The energy consumption for Meena training is 24.1 MWh, and for GPT-3 training is 1,287 MWh. Dividing 1,287 by 24.1 gives approximately 53.6.","The context provides the energy consumption for both Meena and GPT-3 training runs. By dividing the energy consumption of GPT-3 by that of Meena, we can determine how many Meena training runs would use the same total energy as a single full training run of GPT-3."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides Table 2, which lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for 1,000 image generation inferences is given as 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","The context provides Table 2, which lists the mean energy consumption for various tasks, including image generation. The mean energy consumption for 1,000 image generation inferences is given as 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context snippet provides a table with estimated CO2 emissions for various activities, including the average American life in one year, which is 36,156 pounds of CO2e.","36156","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs)\nAir travel, 1 passenger, NY ↔SF 1984\nHuman life, avg, 1 year 11,023\nAmerican life, avg, 1 year 36,156\nCar, avg incl.","The context snippet provides a table with estimated CO2 emissions for various activities, including the average American life in one year, which is 36,156 pounds of CO2e."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a table with per-household health cost estimates for various counties in West Virginia, but it does not specify which county is projected to have the highest per-household health cost in 2030.","is_blank","is_blank","[""han2024""]","is_blank","is_blank","The context provides a table with per-household health cost estimates for various counties in West Virginia, but it does not specify which county is projected to have the highest per-household health cost in 2030."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides the carbon emissions for Mistral-small before and after optimization. To find the multiplier, we divide the emissions after optimization by the emissions before optimization. The emissions before optimization were 0.020 kg CO2, and after optimization, they were 0.015 kg CO2. The multiplier is 0.015 / 0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","Before Optimization: Mistral-small 0.70 0.67 0.65 0.67 0.020
After Optimization: Mistral-small 0.73 0.70 0.69 0.70 0.015","The context provides the carbon emissions for Mistral-small before and after optimization. To find the multiplier, we divide the emissions after optimization by the emissions before optimization. The emissions before optimization were 0.020 kg CO2, and after optimization, they were 0.015 kg CO2. The multiplier is 0.015 / 0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The combined training and fine-tuning energy costs for the BLOOMz-7B model can be calculated by adding the training energy cost (51,686 kWh) and the fine-tuning energy cost (7,571 kWh) as reported in the context.","59257","kWh","[""luccioni2024""]","is_blank","Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571
(from Table 5 in the context)","The combined training and fine-tuning energy costs for the BLOOMz-7B model can be calculated by adding the training energy cost (51,686 kWh) and the fine-tuning energy cost (7,571 kWh) as reported in the context."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that the final model, FLM-101B, has 101 billion parameters.","101000000000","parameters","[""li2025a""]","is_blank","Benefiting from our growth strategy, the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.","The context explicitly states that the final model, FLM-101B, has 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context does not provide the exact energy cost for a full training run of a 6.1B parameter model. However, it does provide the energy cost for BLOOMz-7B, which is 51,686 kWh for training and 7,571 kWh for fine-tuning, totaling 59,257 kWh. Using the inference energy cost of BLOOMz-7B, which is 1.0 × 10−4 kWh per inference, we can estimate the number of inferences needed to match the training energy cost. The calculation is 59,257 kWh / 1.0 × 10−4 kWh per inference = 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B Training energy (kWh): 51,686, Finetuning energy (kWh): 7,571, Inference energy (kWh): 1.0 × 10−4, Cost parity (# inferences): 592,570,000","The context does not provide the exact energy cost for a full training run of a 6.1B parameter model. However, it does provide the energy cost for BLOOMz-7B, which is 51,686 kWh for training and 7,571 kWh for fine-tuning, totaling 59,257 kWh. Using the inference energy cost of BLOOMz-7B, which is 1.0 × 10−4 kWh per inference, we can estimate the number of inferences needed to match the training energy cost. The calculation is 59,257 kWh / 1.0 × 10−4 kWh per inference = 592,570,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context snippet from the 2024 study 'Power Hungry Processing' explicitly states that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy'.","754.66","kWh","[""luccioni2024""]","is_blank","""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.""","The context snippet from the 2024 study 'Power Hungry Processing' explicitly states that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy'."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for various BLOOMz models, including BLOOMz-7B.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for various BLOOMz models, including BLOOMz-7B."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information on the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the freshwater consumption by Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is approximately 3 passengers taking a round trip between San Francisco and New York. This is directly mentioned in the context provided.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is approximately 3 passengers taking a round trip between San Francisco and New York. This is directly mentioned in the context provided."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that 84% of LLM usage is through models with no disclosure.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that 84% of LLM usage is through models with no disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies that the LLaMA-13B model requires 1 A100 80GB GPU for inference without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context specifies that the LLaMA-13B model requires 1 A100 80GB GPU for inference without compression or quantization."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context snippet from [ref_id=dodge2022] estimates that if the 6 billion parameter transformer model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region it was run in.","[21,78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context snippet from [ref_id=dodge2022] estimates that if the 6 billion parameter transformer model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region it was run in."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide specific data on the total carbon emissions avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on the total carbon emissions avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides the price per hour for an NVIDIA H20 as $4.63, which is directly extracted from the table in the document.","4.63","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.\nH100 H20 TPU v6e [7]\nBF16 TFLOPs 989 148 918\nMemory capacity 80 GB 96 GB 32 GB\nMemory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s\nPower rating 700 W 400 W unlisted\nInter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s\nNetwork bandwidth 400 Gbps 400 Gbps 200 Gbps\nPrice per chip [2] $11.06/hr $4.63/hr * $2.70/hr\n*: As H20 is not readily available on cloud service providers, the listed price\nis estimated using the relative complete system cost against H100.","The context provides the price per hour for an NVIDIA H20 as $4.63, which is directly extracted from the table in the document."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses gather the information manually by writing to authors, contradicting the statement that information is gathered automatically.","0","is_blank","[""luccioni2025b"", ""luccioni2023""]","is_blank","In fact, most carbon footprint analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context explicitly states that most carbon footprint analyses gather the information manually by writing to authors, contradicting the statement that information is gathered automatically."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that the public health cost of U.S. data centers in 2023 was equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that the public health cost of U.S. data centers in 2023 was equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context specifies that the LLaMA-7B model can be run on a single A100 80GB GPU with a batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","The 7B model was run on a single GPU and TABLE II shows the bare minimum hardware requirements for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context specifies that the LLaMA-7B model can be run on a single A100 80GB GPU with a batch size of 64."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that researchers were able to collect information for a total of 95 models from 77 papers after reaching out to over 500 authors.","95","answers","[""luccioni2023""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context states that researchers were able to collect information for a total of 95 models from 77 papers after reaching out to over 500 authors."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE project was trained with a budget of $100,000 and used 30,000 H100 GPU hours. To find the cost per H100 GPU-hour, we divide the total budget by the total GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The JetMoE project was trained with a budget of $100,000 and used 30,000 H100 GPU hours. To find the cost per H100 GPU-hour, we divide the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table (Table 2) that lists the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in various locations, including Altoona, Iowa.","122","percent","[""han2024""]","is_blank","Table 2:The public health cost of training a large AI model in selected U.S. data centers.
Location Electricity Price
(¢/kWh) Electricity
(million$) Health Cost
(million$) % of Electricity
Cost Emission(Metric Ton) PM2.5(LA-NYC) NOx(LA-NYC) SO2
Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context provides a table (Table 2) that lists the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in various locations, including Altoona, Iowa."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1250000000000","tokens","[""shen2024"", ""shen2024"", ""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table listing the number of Amazon Renewable Energy Projects announced as of January 2024, including those in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","The table under 'Amazon Renewable Energy Projects' lists 'United Kingdom' with '36' projects.","The context provides a table listing the number of Amazon Renewable Energy Projects announced as of January 2024, including those in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage of Amazon's Workforce in the United States across all levels that identified as men in 2023. The answer is derived directly from the provided data.","44.3","percent","[""amazon2023""]","is_blank","Amazon Representation by the Numbers* † Other Native American and NHOPI+ (2021 and 2022) Latino/e+ Black+ Asian+ White+ Multiracial Native American and Alaskan+ (2023) NHOPI+ (2023) Amazon Workforce (All Levels) 43.1%56.8% 44.1%55.7% 30.8%69.2% 31.4%68.5% 31.5%68.4% 46.9%52.9% 45.5%54.3% 46.6%53.2% 44.3%55.6% 31.1%68.8% 48 31.7%68.3% 31.6%68.3%","The context provides the percentage of Amazon's Workforce in the United States across all levels that identified as men in 2023. The answer is derived directly from the provided data."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a batch size of 84 on a NVIDIA A40-48GB GPU. Therefore, the answer is blank.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a batch size of 84 on a NVIDIA A40-48GB GPU. Therefore, the answer is blank."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions the 'granularity metric' as the method to assess the ratio of computation to communication time when scaling distributed training across continents.","granularity metric","is_blank","[""erben2023""]","is_blank","We found the granularity metric [ 22], typically used in high-performance computing, practical to attach a comparable value to each setup to quantify the ratio of the calculation to communication time.","The context mentions the 'granularity metric' as the method to assess the ratio of computation to communication time when scaling distributed training across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","A coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119].","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines 'water withdrawal' as freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that the average U.S. household had 25 connected devices in 2021.","25","devices","[""wu2021b""]","is_blank","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context explicitly states that the average U.S. household had 25 connected devices in 2021."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training LLMs, from 0.8 MWh to 3,500 MWh.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides a range of energy consumption for pre-training LLMs, from 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders in the Jeopardy challenge.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program beat human contenders in the Jeopardy challenge."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet directly states that 10–50 queries on GPT-3 consume around half a liter of water.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context snippet directly states that 10–50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context explicitly states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific score for JetMoE-8B-Chat as 6.681.","6.681","score","[""shen2024""]","is_blank","Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269
Table 4: MT-Bench score comparison of various models","The context explicitly states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and provides the specific score for JetMoE-8B-Chat as 6.681."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context indicates that GPT-3 needs to consume a 500ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed. This range is directly applicable to the question, providing the number of responses per 500ml of water.","[10,50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context indicates that GPT-3 needs to consume a 500ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed. This range is directly applicable to the question, providing the number of responses per 500ml of water."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces costs by 9-77% for short-context tasks (interactive chats). The highest value in this range is 77%.","77","percent","[""griggs2024"", ""griggs2024"", ""griggs2024""]","is_blank","Mélange reduces costs by 9-77% for short-context tasks (interactive chats).","The context states that Mélange reduces costs by 9-77% for short-context tasks (interactive chats). The highest value in this range is 77%."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context specifies that at least 4 A100 GPUs each with 80GB of memory are required for meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","The context states: 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The context specifies that at least 4 A100 GPUs each with 80GB of memory are required for meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context repeatedly mentions that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","The context states: 'Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","The context repeatedly mentions that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context states that NVIDIA estimated that 80–90% of the ML workload is inference processing in 2019.","[80,90]","percent","[""patterson2021"", ""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","The context states that NVIDIA estimated that 80–90% of the ML workload is inference processing in 2019."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context snippet explicitly states that the total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study was 178.97 kg.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","ETHICAL CONSIDERATIONS STATEMENT
The main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon emissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements. In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context snippet explicitly states that the total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study was 178.97 kg."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not reliable or accurate, as it often leads to overestimation.","0","is_blank","[""chung2025"", ""luccioni2023"", ""cottier2024"", ""patterson2021""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. (chung2025)","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not reliable or accurate, as it often leads to overestimation."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a table with throughput values for different configurations. The specific value for dense Mixtral-CS-A100-40GB at batch size 1 is 0.5 queries/sec.","0.5","queries/sec","[""xia2024""]","is_blank","The table in the context shows 'Dense(bsz=1)' for Mixtral-CS-A100-40GB with a throughput of '0.5'.","The context provides a table with throughput values for different configurations. The specific value for dense Mixtral-CS-A100-40GB at batch size 1 is 0.5 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models reversed and decreased, not increased.","0","is_blank","[""luccioni2025c""]","is_blank","However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models reversed and decreased, not increased."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context does not provide specific normalized on-demand hourly prices for H100 GPUs in Griggs et al. (2024).","is_blank","USD per hour","[""griggs2024""]","is_blank","is_blank","The context does not provide specific normalized on-demand hourly prices for H100 GPUs in Griggs et al. (2024)."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide specific information about the water usage for cooling during GPT-4's training run.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the water usage for cooling during GPT-4's training run."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs, which is over $5,200 per month. To estimate the cost per hour, we divide the monthly cost by the number of hours in a month (30 days * 24 hours/day). This gives us the cost per hour in USD.","2.17","USD per hour","[""griggs2024""]","is_blank","Serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","The context provides the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs, which is over $5,200 per month. To estimate the cost per hour, we divide the monthly cost by the number of hours in a month (30 days * 24 hours/day). This gives us the cost per hour in USD."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000 times larger in size is required.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000 times larger in size is required."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training a Transformer model with neural architecture search as 626,155 lbs. A recent study by Strubell et al. estimated that the emissions from training a Transformer model with neural architecture search is equivalent to the emissions from driving 242,231 miles. Using this ratio, we can calculate the approximate driving distance equivalent to the carbon emissions from training a Transformer model with neural architecture search.","242231","miles","[""strubell2019"", ""wu2021a""]","[""https://arxiv.org/abs/1906.02243"", ""https://arxiv.org/abs/2105.00510""]","['Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. (strubell2019)', 'Although recent work shows the carbon footprint of training one large ML model, such as Meena [21], is equivalent to 242,231 miles driven by an average passenger vehicle [22], this is only one aspect; (wu2021a)']","The context provides the CO2 emissions for training a Transformer model with neural architecture search as 626,155 lbs. A recent study by Strubell et al. estimated that the emissions from training a Transformer model with neural architecture search is equivalent to the emissions from driving 242,231 miles. Using this ratio, we can calculate the approximate driving distance equivalent to the carbon emissions from training a Transformer model with neural architecture search."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy of 1.0 × 10−4 kWh per inference. If every download results in 1 million inferences, the total energy consumed would be 606,096 downloads * 1,000,000 inferences/download * 1.0 × 10−4 kWh/inference = 60.6096 MWh.","60.6096","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5.","The BLOOMz-7B model has an inference energy of 1.0 × 10−4 kWh per inference. If every download results in 1 million inferences, the total energy consumed would be 606,096 downloads * 1,000,000 inferences/download * 1.0 × 10−4 kWh/inference = 60.6096 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context snippet directly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context snippet directly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context indicates that if the current trend of 2.4x per year growth continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context indicates that if the current trend of 2.4x per year growth continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippet directly states that 'OpenAI published the total number of floating point operations to train their model: 3.14E+23'. This directly answers the question with the required unit of FLOPS.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context snippet directly states that 'OpenAI published the total number of floating point operations to train their model: 3.14E+23'. This directly answers the question with the required unit of FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at least 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","The context mentions: 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The context explicitly states that at least 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput for the final 101B training stage as 2160 teraFLOPs/sec. To convert this to zettaFLOPs, we multiply by the number of seconds in a day and the number of days in the training stage. The training stage took 21.54 days, and there are 86400 seconds in a day. Therefore, the total computational work is 2160 * 10^-12 * 86400 * 21.54 zettaFLOPs.","0.040","zettaFLOPs","[""li2025a""]","is_blank","Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 101 4 4 12 192 2160 165 52.88%","The context provides the throughput for the final 101B training stage as 2160 teraFLOPs/sec. To convert this to zettaFLOPs, we multiply by the number of seconds in a day and the number of days in the training stage. The training stage took 21.54 days, and there are 86400 seconds in a day. Therefore, the total computational work is 2160 * 10^-12 * 86400 * 21.54 zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide specific top-1 accuracy values for AlexNet 2012 on ImageNet.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific top-1 accuracy values for AlexNet 2012 on ImageNet."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 servers * 8 GPUs per server.","192","GPUs","[""li2025a""]","is_blank","The Parallel Strategies. FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 servers * 8 GPUs per server."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides an estimate of the CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation. It states that the emissions are equivalent to the lifetime emissions of five US cars. Given that the average lifetime emissions of an American is 36,156 lbs of CO2, the emissions from NAS are equivalent to approximately 1.73 average American lifetimes (626,155 lbs / 36,156 lbs).","1.73","lifetimes","[""luccioni2023"", ""strubell2019"", ""luccioni2025c"", ""patterson2021"", ""dodge2022"", ""zschache2025""]","is_blank","is_blank","The context provides an estimate of the CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation. It states that the emissions are equivalent to the lifetime emissions of five US cars. Given that the average lifetime emissions of an American is 36,156 lbs of CO2, the emissions from NAS are equivalent to approximately 1.73 average American lifetimes (626,155 lbs / 36,156 lbs)."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train. The answer unit is 'hours', so the answer value is 25,000.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train. The answer unit is 'hours', so the answer value is 25,000."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions the use of the vllm library for deploying large language models across multiple GPUs and nodes.","vllm","is_blank","[""zschache2025""]","is_blank","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions the use of the vllm library for deploying large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table B2) that shows the energy consumption for the Llama 3.1 70B model when deployed on one node versus two nodes. The ratio of energy consumption between the two configurations is given as 1.95, indicating a 95% increase in energy consumption when using two nodes.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2 shows that the energy consumption for the Llama 3.1 70B model increased from 48.60 Wh on one node to 94.88 Wh on two nodes, resulting in a ratio of 1.95.","The context provides a table (Table B2) that shows the energy consumption for the Llama 3.1 70B model when deployed on one node versus two nodes. The ratio of energy consumption between the two configurations is given as 1.95, indicating a 95% increase in energy consumption when using two nodes."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific information on the water consumption per user session in 2023 for ChatGPT. It only mentions estimates for 2025 and general water consumption figures for AI models and data centers.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the water consumption per user session in 2023 for ChatGPT. It only mentions estimates for 2025 and general water consumption figures for AI models and data centers."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that even a 0.42 Wh short query, when scaled to 700 million queries per day, aggregates to annual electricity comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes.","The context states that even a 0.42 Wh short query, when scaled to 700 million queries per day, aggregates to annual electricity comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippet from the 2025 paper states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context snippet from the 2025 paper states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that there will be approximately 772 billion GPT-4o queries in 2025.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context states that there will be approximately 772 billion GPT-4o queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the average score for JetMoE-8B on the OpenLLM Leaderboard, which is 53.0.","53.0","score","[""shen2024""]","is_blank","OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context provides the average score for JetMoE-8B on the OpenLLM Leaderboard, which is 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This indicates that they are not fully exempt unless they pose systemic risk.","False","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This indicates that they are not fully exempt unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context snippet [ref_id=patterson2021] mentions that the US national datacenter average PUE in 2020 was 1.59.","1.59","PUE","[""patterson2021""]","is_blank","A useful quantitative metric of datacenter efficiency is the energy overhead above and beyond what directly powers the computing equipment inside the datacenters. If the overhead were 50%, the Power Usage Effectiveness (PUE) would be 1.50. The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59 . Google publishes its datacenter PUE online every quarter . The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are roughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see [Höl20]), but we’ll limit the quantitative improvement in this paper to the easy-to-measure PUE.","The context snippet [ref_id=patterson2021] mentions that the US national datacenter average PUE in 2020 was 1.59."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context snippet directly states that the KV Cache size for the OPT-2.7B model with a batch size of 32 is 5.312GB.","5.312","GB","[""kim2025""]","is_blank","For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context snippet directly states that the KV Cache size for the OPT-2.7B model with a batch size of 32 is 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that R&D staff costs including equity are between 29% and 49% of the total amortized cost for the four models studied.","[29,49]","percent","[""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024"", ""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context states that R&D staff costs including equity are between 29% and 49% of the total amortized cost for the four models studied."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the CodeCarbon package was used to measure energy consumption during inference runs.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context mentions that the CodeCarbon package was used to measure energy consumption during inference runs."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 53% of the analyzed articles cited the figure of 3 Wh per query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context states that 53% of the analyzed articles cited the figure of 3 Wh per query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","Error: Read timeout on endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/us.amazon.nova-pro-v1%3A0/invoke""","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Read timeout on endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/us.amazon.nova-pro-v1%3A0/invoke"""
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA-65B.","1","is_blank","[""samsi2024"", ""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 is 0.1% rare earth metal by mass, the additional water consumed per GPU is 2.2 liters, which is equivalent to 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Given that an H100 is 0.1% rare earth metal by mass, the additional water consumed per GPU is 2.2 liters, which is equivalent to 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that there is currently an exemption for open-source models from reporting obligations, which means they are not required to report their energy consumption under current EU rules.","0","is_blank","[""ebert2024""]","is_blank","The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context indicates that there is currently an exemption for open-source models from reporting obligations, which means they are not required to report their energy consumption under current EU rules."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context mentions that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context mentions that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context provides the total public health cost of U.S. data centers in 2023 as approximately $6.7 billion, which is equivalent to $47.5 per household.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The context provides the total public health cost of U.S. data centers in 2023 as approximately $6.7 billion, which is equivalent to $47.5 per household."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model for a long prompt is 12.222 Wh, while the energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. The factor is calculated by dividing the energy consumption of the o3 model by that of GPT-4.1 nano.","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model Energy Consumption(100 input-300 output)(Wh) Energy Consumption(1k input-1k output)(Wh) Energy Consumption(10k input-1.5k output)(Wh) GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094 o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The energy consumption of the o3 model for a long prompt is 12.222 Wh, while the energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. The factor is calculated by dividing the energy consumption of the o3 model by that of GPT-4.1 nano."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context specifies that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context specifies that Mélange achieves 15-77% cost reduction for the Arena dataset with a 120ms SLO."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table (Table 3) that lists the carbon emissions of FLM-101B and other well-known LLMs. The net carbon emissions for FLM-101B are given as 26 tCO2e.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The context provides a table (Table 3) that lists the carbon emissions of FLM-101B and other well-known LLMs. The net carbon emissions for FLM-101B are given as 26 tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a breakdown of execution time for different stages (forward, backward, optimizer) for a sparse Mixtral model with a batch size of 1. By summing these values, we can determine the total execution time.","8.0","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32) 0.0 2.0 4.0 6.0 8.0 Forward Backward Optimizer","The context provides a breakdown of execution time for different stages (forward, backward, optimizer) for a sparse Mixtral model with a batch size of 1. By summing these values, we can determine the total execution time."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet [ref_id=wu2021b] explicitly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' based on 2019 product data.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context snippet [ref_id=wu2021b] explicitly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' based on 2019 product data."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama, an open-source platform, was used to apply 4-bit quantization and support local deployment of large language models.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context mentions that Ollama, an open-source platform, was used to apply 4-bit quantization and support local deployment of large language models."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that Backblaze (B2) was chosen as the storage provider to shard and stream datasets for spot VMs that could terminate at any time.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","When we run our experiments in a multi-cloud environment on spot instances, we cannot plug in proprietary cloud storage or wait for the dataset to download, as the instances can be terminated any-time. To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].","The context mentions that Backblaze (B2) was chosen as the storage provider to shard and stream datasets for spot VMs that could terminate at any time."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context states that energy consumption during inference is shown to highly correlate with the model’s runtime, and this relationship is illustrated in Figure 6.","1","is_blank","[""zschache2025""]","is_blank","Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear. Figure 6 illustrates this relationship for","The context states that energy consumption during inference is shown to highly correlate with the model’s runtime, and this relationship is illustrated in Figure 6."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context mentions that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Senator Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. Under the new administration, it is unlikely that the bill will pass Congress. However, even if enacted, it would not contain any significant hard regulation. The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI’s environmental impacts without imposing significant regulatory obligations.","The context mentions that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides the price per hour for an NVIDIA H100 as $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context provides the price per hour for an NVIDIA H100 as $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context snippet [ref_id=cottier2024] mentions that even if all failures during training were catastrophic, the expected hardware lifetime would be 3.7 years.","3.7","years","[""cottier2024""]","is_blank","Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.","The context snippet [ref_id=cottier2024] mentions that even if all failures during training were catastrophic, the expected hardware lifetime would be 3.7 years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies that the bare minimum hardware required for LLaMA-13B inference is 2 V100 32GB GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context specifies that the bare minimum hardware required for LLaMA-13B inference is 2 V100 32GB GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports the Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number.","The context states that Google reports the Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context snippet directly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context snippet directly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water consumption are estimated at about 3.1 L/kWh [8].","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context mentions that Google's hyperscale data centers had a PUE of 1.10 in 2021.","1.10","PUE","[""wu2021b"", ""dodge2022"", ""patterson2021""]","[""https://huggingface.co/blog/sasha/ai-environment-primer"", ""https://www.google.com/about/datacenters/efficiency/"", ""https://arxiv.org/abs/2104.10350""]","['Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]', 'Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].', 'Google publishes its datacenter PUE online every quarter. The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.']","The context mentions that Google's hyperscale data centers had a PUE of 1.10 in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is $3460.","3460","USD","[""xia2024""]","is_blank","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","The context explicitly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU is $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context states that GPUs account for almost 3/4 of the total electricity consumption in a typical datacenter, which translates to approximately 75%.","75","percent","[""dodge2022""]","is_blank","As expected the GPU accounts for almost 3/4 of electricity consumption.","The context states that GPUs account for almost 3/4 of the total electricity consumption in a typical datacenter, which translates to approximately 75%."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context snippet states that the training infrastructure for JetMoE-8B consisted of a cluster containing 12 nodes and 96 H100 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context snippet states that the training infrastructure for JetMoE-8B consisted of a cluster containing 12 nodes and 96 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that when actively training, the average GPU power is over 600W.","600","Watts","[""morrison2025"", ""morrison2025"", ""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that when actively training, the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context snippet provides a table that lists the estimated CO2 emissions for various activities, including the average human life in one year globally, which is 11,023 pounds of CO2e.","11023","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs)
Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl.","The context snippet provides a table that lists the estimated CO2 emissions for various activities, including the average human life in one year globally, which is 11,023 pounds of CO2e."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This implies a speedup of approximately 1.25 times in throughput for LLaMA-13B.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 7B and 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This implies a speedup of approximately 1.25 times in throughput for LLaMA-13B."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context snippet directly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context snippet directly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with a 400 TPS SLO, InferSave selected g4dn.xlarge as its top choice with a cost of $0.71, while Max-Performance selected g6e.xlarge with a cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 0.71) / 0.71) * 100.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with a 400 TPS SLO, InferSave selected g4dn.xlarge as its top choice with a cost of $0.71, while Max-Performance selected g6e.xlarge with a cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 0.71) / 0.71) * 100."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference.","V100 or A30","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context mentions that Green AI involves reporting the computational price tag of finding, training, and running models, which implies providing the financial cost.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","The context mentions that Green AI involves reporting the computational price tag of finding, training, and running models, which implies providing the financial cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context snippet from the 2025 paper states that electronic waste (e-waste) reached 62 million tonnes in 2022.","62000000","metric tons","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context snippet from the 2025 paper states that electronic waste (e-waste) reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the average system power per processor for both TPU v2 and V100 GPU. The difference is calculated by subtracting the average power of TPU v2 from that of V100 GPU.","104","Watts","[""patterson2021""]","is_blank","Processor Average (Watts) StDev % DNNs used to calculate average power TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]","The context provides the average system power per processor for both TPU v2 and V100 GPU. The difference is calculated by subtracting the average power of TPU v2 from that of V100 GPU."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","Training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","The context explicitly states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context mentions that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context mentions that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)' in the eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These results suggest that downsizing reasoning models can yield meaningful sustainability gains without compromising performance.","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)' in the eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context mentions that smartphones currently have average lifetimes of less than 3 years, which contributes to e-waste concerns.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020].","The context mentions that smartphones currently have average lifetimes of less than 3 years, which contributes to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively, indicating nearly linear per-GPU speedup for CV models.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup
#GPUs ) is almost
linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster
(0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively, indicating nearly linear per-GPU speedup for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context window size for the FLM-101B model is explicitly stated as 2,048 tokens in the provided context.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context window size for the FLM-101B model is explicitly stated as 2,048 tokens in the provided context."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLMs generally consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. In contrast, diffusion models consume nearly the maximum power of the GPU when batch size is not small due to their higher compute-intensity.","1","is_blank","[""chung2025""]","is_blank","Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. VLMs show slightly higher power consumption than LLMs due to its additional modality encoder, which is compute-intensive. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context states that LLMs generally consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. In contrast, diffusion models consume nearly the maximum power of the GPU when batch size is not small due to their higher compute-intensity."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides the percentage of Amazon's People Managers globally who identified as women in 2023 as 54.3%.","54.3","percent","[""amazon2023""]","is_blank","45.5%54.3%","The context provides the percentage of Amazon's People Managers globally who identified as women in 2023 as 54.3%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76%.","[61,76]","percent","[""cottier2024""]","is_blank","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as 0.954 lbs/kWh.","0.954","lbs/kWh","[""strubell2019""]","[""https://bit.ly/2JTbGnI""]","The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0 .954pt (2)","The context provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as 0.954 lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context mentions a study that suggests the global AI could consume 85 – 134 TWh of electricity in 2027.","[85,134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7].","The context mentions a study that suggests the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context explicitly states that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context explicitly states that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context specifies that the online inference workload uses a pattern of 128 input tokens and 512 output tokens, and evaluates a total of 3000 requests. Therefore, the total number of tokens processed is (128 + 512) * 3000.","1920000","tokens","[""kim2025""]","is_blank","Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The context specifies that the online inference workload uses a pattern of 128 input tokens and 512 output tokens, and evaluates a total of 3000 requests. Therefore, the total number of tokens processed is (128 + 512) * 3000."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those in oil and gas exploration.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context indicates that the AI Act fails to address the greenhouse gas emissions generated by AI applications, such as those in oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction, greater than 30% in multiple regions, and up to 80% in West US, can be achieved. Therefore, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is 80%.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region.","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction, greater than 30% in multiple regions, and up to 80% in West US, can be achieved. Therefore, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is 80%."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the different models examined in the study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences. This is derived from the comparison between the most energy-intensive task (image generation) and the least energy-intensive task (text classification).","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the different models examined in the study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences. This is derived from the comparison between the most energy-intensive task (image generation) and the least energy-intensive task (text classification)."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis, justifying the energy costs of LLM deployment.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that traditional models perform considerably worse than LLMs in Yelp sentiment analysis, justifying the energy costs of LLM deployment."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table listing the number of Amazon Renewable Energy Projects announced as of January 2024, including those in the United States. The table specifies that there are 244 projects in the United States.","244","projects","[""amazon2023""]","is_blank","The table in the context lists 'United States 244 17,706' under the 'Project Location' and 'Number of Projects' columns.","The context provides a table listing the number of Amazon Renewable Energy Projects announced as of January 2024, including those in the United States. The table specifies that there are 244 projects in the United States."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides a table showing the percentage of energy sourced from renewable sources for Amazon-AWS in 2018, which is 17%.","17","percent","[""strubell2019""]","is_blank","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al. , 2017), compared to the United States, 4 China5 and Germany (Burger, 2019).","The context provides a table showing the percentage of energy sourced from renewable sources for Amazon-AWS in 2018, which is 17%."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context suggests that the authors recommend measuring and reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility.","cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The context suggests that the authors recommend measuring and reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that the GPU accounts for almost 3/4 of the electricity consumption, which is equivalent to 74%.","74","percent","[""dodge2022""]","is_blank","As expected the GPU accounts for almost 3/4 of electricity consumption.","The context states that the GPU accounts for almost 3/4 of the electricity consumption, which is equivalent to 74%."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context explicitly states that serving the Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context explicitly states that serving the Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context mentions that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context mentions that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide specific information on the amount of fiber optic cable installed globally to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the amount of fiber optic cable installed globally to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context snippet [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This value is directly extracted and formatted as per the 'answer_unit' specified.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context snippet [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This value is directly extracted and formatted as per the 'answer_unit' specified."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves.","0","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2111.00364""]","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The context states that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a figure (Fig. 13) that shows the projected maximum batch size for different GPUs, including the A100-40GB. The ground truth projection for the A100-40GB GPU is indicated as 'bsz=28', which means the maximum batch size is 28 samples.","28","samples","[""xia2024""]","is_blank","Fig. 13. Projected maximum batch size of Mixtral for different GPUs.","The context provides a figure (Fig. 13) that shows the projected maximum batch size for different GPUs, including the A100-40GB. The ground truth projection for the A100-40GB GPU is indicated as 'bsz=28', which means the maximum batch size is 28 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context explicitly states that Google reported a 48% increase in GHG emissions since 2019 in its 2024 environmental report.","48","percent","[""luccioni2025b"", ""luccioni2025a"", ""han2024""]","[""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf"", ""https://sustainability.google/reports/google-2024-environmental-report/""]","Google’s Emissions Shot Up 48% Over Five Years Due to AI. [ref_id=luccioni2025b]
Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [ref_id=luccioni2025a]
Google Environmental Report 2024. [ref_id=han2024]","The context explicitly states that Google reported a 48% increase in GHG emissions since 2019 in its 2024 environmental report."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context mentions projections that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030.","[9.1,11.7]","percent","[""fernandez2025"", ""morrison2025""]","is_blank","is_blank","The context mentions projections that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the 6 billion parameter transformer training run, which had the largest carbon intensity, the Pause and Resume algorithm led to the largest decrease in emissions, up to about 25%.","25","percent","[""dodge2022""]","is_blank","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context states that for the 6 billion parameter transformer training run, which had the largest carbon intensity, the Pause and Resume algorithm led to the largest decrease in emissions, up to about 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from Strubell et al. (2019) directly provides the carbon footprint of training the BERT large language model as 626,155 pounds of CO2e.","626155","lbs CO2e","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.","The context snippet from Strubell et al. (2019) directly provides the carbon footprint of training the BERT large language model as 626,155 pounds of CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippet directly states that GPT-3 was trained with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context snippet directly states that GPT-3 was trained with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning, making it a targeted layer for enhancement.","1","is_blank","[""[ref_id=xia2024]""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context explicitly states that optimizing the MoE layer is crucial for improving the performance of LLM fine-tuning, making it a targeted layer for enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch sizes supported by different model and dataset combinations. For Mixtral with a dense setup on the Hellaswag dataset, the maximum batch size is 2 samples.","2","samples","[""xia2024""]","is_blank","Table III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides a table (Table III) that lists the maximum batch sizes supported by different model and dataset combinations. For Mixtral with a dense setup on the Hellaswag dataset, the maximum batch size is 2 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7%."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide specific information on the land area occupied by new AI data centers globally in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the land area occupied by new AI data centers globally in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context snippet [ref_id=luccioni2024] directly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32 grams of CO2eq per 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context snippet [ref_id=luccioni2024] directly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32 grams of CO2eq per 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context indicates that the range of CO2 emissions between the most and least efficient regions for training a BERT model was approximately 7k grams to 26k grams.","[7000,26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context indicates that the range of CO2 emissions between the most and least efficient regions for training a BERT model was approximately 7k grams to 26k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c"", ""luccioni2025c"", ""luccioni2025c"", ""luccioni2025c"", ""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the equivalent water usage for training the OLMo 60M model as '5 days' for one person in the US.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides the equivalent water usage for training the OLMo 60M model as '5 days' for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information on the freshwater consumption of Google's DeepMind AlphaFold servers in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the freshwater consumption of Google's DeepMind AlphaFold servers in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context suggests that adding compute resources to accelerate the MoE layers can reduce costs, not increase them. Specifically, it mentions that this approach can further reduce cost based on their study.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context suggests that adding compute resources to accelerate the MoE layers can reduce costs, not increase them. Specifically, it mentions that this approach can further reduce cost based on their study."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context does not provide specific information on the total energy consumption for training the FLM-101B model.","is_blank","kWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the total energy consumption for training the FLM-101B model."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context mentions that the total public health burden of U.S. data centers could reach more than $20 billion by 2028, which is the closest projection to 2030 provided in the context.","20000000000","USD","[""han2024""]","is_blank","The growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California.","The context mentions that the total public health burden of U.S. data centers could reach more than $20 billion by 2028, which is the closest projection to 2030 provided in the context."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (Table IV) that estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using 10 epochs on different GPUs. The cost for fine-tuning on an NVIDIA A40 GPU with 48GB memory is given as $32.7 per hour. However, the specific cost for the GSM8K dataset is not provided, and the context does not offer a clear conversion or additional information to determine the exact cost for GSM8K.","is_blank","USD","[""xia2024""]","is_blank","is_blank","The context provides a table (Table IV) that estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using 10 epochs on different GPUs. The cost for fine-tuning on an NVIDIA A40 GPU with 48GB memory is given as $32.7 per hour. However, the specific cost for the GSM8K dataset is not provided, and the context does not offer a clear conversion or additional information to determine the exact cost for GSM8K."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context does not provide specific information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide specific execution time data for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB GPU at a batch size of 10.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB GPU at a batch size of 10."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context suggests that GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use, as GPUs typically consume the majority of electricity in AI workloads and are easier to measure.","1","is_blank","[""dodge2022"", ""chung2025"", ""schwartz2019""]","is_blank","is_blank","The context suggests that GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use, as GPUs typically consume the majority of electricity in AI workloads and are easier to measure."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that training accounted for only half of the model's overall emissions, which translates to 50%.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions [121].","The context states that training accounted for only half of the model's overall emissions, which translates to 50%."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context specifies that the LLaMA-7B model requires a single V100 32GB GPU for inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context specifies that the LLaMA-7B model requires a single V100 32GB GPU for inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides a table with the number of requests needed to consume 500ml of water for different locations. For Arizona, it specifies that 6.520 requests are needed.","6.52","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. Arizona 1.180 1.630 4.959 2.098 7.531 9.629 6.520 23.406 29.926 16.7","The context provides a table with the number of requests needed to consume 500ml of water for different locations. For Arizona, it specifies that 6.520 requests are needed."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides the estimated CO2 emission for one year of average US home energy use as 8.30 metric tons.","8.30","metric tons","[""dodge2022""]","is_blank","One average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil).","The context provides the estimated CO2 emission for one year of average US home energy use as 8.30 metric tons."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides a table (Table 3) that lists the performance of various models on different benchmarks, including GSM8k. The JetMoE-8B model achieved a score of 27.8 on the GSM8k benchmark.","27.8","score","[""shen2024""]","is_blank","Table 3 shows the OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. The GSM8k score for JetMoE-8B is 27.8.","The context provides a table (Table 3) that lists the performance of various models on different benchmarks, including GSM8k. The JetMoE-8B model achieved a score of 27.8 on the GSM8k benchmark."
