"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context provides information about the total energy consumption of U.S. data centers, stating that it increased by about 4% from 2010-2014. The 'answer_unit' field specifies that the answer should be in percent, which matches the format of the information given in the context.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context provides information about the total energy consumption of U.S. data centers, stating that it increased by about 4% from 2010-2014. The 'answer_unit' field specifies that the answer should be in percent, which matches the format of the information given in the context."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. The 'answer_unit' is 'cars', so the answer_value should be the number of cars.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. The 'answer_unit' is 'cars', so the answer_value should be the number of cars."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, the number of data centers AWS began using recycled water for cooling in 2023 is 4.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, the number of data centers AWS began using recycled water for cooling in 2023 is 4."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides an estimate of embodied carbon emissions per GPU based on an assumption made in the study. It states that the embodied carbon emissions are assumed to be the same as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, which equals 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The context provides an estimate of embodied carbon emissions per GPU based on an assumption made in the study. It states that the embodied carbon emissions are assumed to be the same as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, which equals 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized hardware and energy cost for GPT-4 was $40M, and the total training budget for FLM-101B was $100,000. To find the factor by which the cost of GPT-4 is greater than FLM-101B, we divide the cost of GPT-4 by the cost of FLM-101B. The answer_unit is 'ratio', so we express the answer as a number.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","['The estimated amortized hardware and energy cost for GPT-4 was $40M [cottier2024]. The total training budget for FLM-101B was $100,000 [li2025a].']","The estimated amortized hardware and energy cost for GPT-4 was $40M, and the total training budget for FLM-101B was $100,000. To find the factor by which the cost of GPT-4 is greater than FLM-101B, we divide the cost of GPT-4 by the cost of FLM-101B. The answer_unit is 'ratio', so we express the answer as a number."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context snippets provide information about the 1.2t CO2e for a single passenger round trip from San Francisco (SF) to New York (NY). This information is directly stated in multiple places, including Table 1 and Appendix A. The answer_unit is 'tCO2e', which matches the unit given in the context.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context snippets provide information about the 1.2t CO2e for a single passenger round trip from San Francisco (SF) to New York (NY). This information is directly stated in multiple places, including Table 1 and Appendix A. The answer_unit is 'tCO2e', which matches the unit given in the context."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context provides a table (Table 4) that details the performance of FLM-101B and other baseline models on the Open LLM Leaderboard. The average score for FLM-101B is listed as 43.94. The answer_unit is 'score', which implies that the answer_value should be a numeric value representing the score.","43.94","score","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA; FLM-101B 28.22 43.94 39.76 66.23 28.30 41.47","The context provides a table (Table 4) that details the performance of FLM-101B and other baseline models on the Open LLM Leaderboard. The average score for FLM-101B is listed as 43.94. The answer_unit is 'score', which implies that the answer_value should be a numeric value representing the score."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides a comparison between the Intel 4004 microprocessor from 1971 and typical microprocessors from 2021, stating that there was a more than 6,750 fold improvement in processor clock speed. The 'answer_unit' is 'fold', which indicates the expected format is a numeric value representing the fold improvement.","6750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context provides a comparison between the Intel 4004 microprocessor from 1971 and typical microprocessors from 2021, stating that there was a more than 6,750 fold improvement in processor clock speed. The 'answer_unit' is 'fold', which indicates the expected format is a numeric value representing the fold improvement."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides the total number of floating point operations to train GPT-3 as 3.14E+23 and the performance of 10,000 V100 GPUs at 24.6 TeraFLOPS/sec. Using these values, the training time is calculated as ~14.8 days.","14.8","days","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context provides the total number of floating point operations to train GPT-3 as 3.14E+23 and the performance of 10,000 V100 GPUs at 24.6 TeraFLOPS/sec. Using these values, the training time is calculated as ~14.8 days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 4) from the document with ref_id 'morrison2025' that lists measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. For a Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is listed as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: ... Llama 3.2 1B ... 8 0.036 12.0 0.054 12.64 21.5 bil.","The context provides a table (Table 4) from the document with ref_id 'morrison2025' that lists measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. For a Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is listed as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides information on the total permitted annual emission limits for data center diesel generators in Northern Virginia. The total permitted annual emission limits for NOx are approximately 13,000 tons.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context provides information on the total permitted annual emission limits for data center diesel generators in Northern Virginia. The total permitted annual emission limits for NOx are approximately 13,000 tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly supports the answer.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly supports the answer."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides information on the projected public health impacts of U.S. data centers in 2028, including approximately 1,300 premature deaths due to scope-1 and scope-2 pollutants. While the exact number for 2030 is not directly stated, the data for 2028 can be used as a reference point. The question asks for the number of premature deaths in 2030, which is not directly available in the context.","is_blank","deaths","[""han2024""]","is_blank","Our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028, exceeding 1/3 of asthma deaths in the U.S. each year [40].","The context provides information on the projected public health impacts of U.S. data centers in 2028, including approximately 1,300 premature deaths due to scope-1 and scope-2 pollutants. While the exact number for 2030 is not directly stated, the data for 2028 can be used as a reference point. The question asks for the number of premature deaths in 2030, which is not directly available in the context."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that a full training run of the 6.1 billion parameter transformer would take 60 days. The context supports this answer by providing a direct estimate from the study.","60","days","[""dodge2022""]","is_blank","Table 2: ... we estimate a full training run would consume approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","Dodge et al. (2022) state that a full training run of the 6.1 billion parameter transformer would take 60 days. The context supports this answer by providing a direct estimate from the study."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The question asks for the model with the highest GPU energy consumption for 1,000 queries. According to Table 2 in the context, the model 'Command-R Plus' by Cohere has the highest energy consumption at 3,426.38 Wh for 1,000 queries. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed) from [ref_id=luccioni2025c]","The question asks for the model with the highest GPU energy consumption for 1,000 queries. According to Table 2 in the context, the model 'Command-R Plus' by Cohere has the highest energy consumption at 3,426.38 Wh for 1,000 queries. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.' The answer_unit is 'year', so the answer_value should be the year in which the study was launched.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context states that 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.' The answer_unit is 'year', so the answer_value should be the year in which the study was launched."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. The answer_unit is 'percent', so the answer_value should be a number representing the percentage.","22","percent","[""luccioni2025a""]","is_blank","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. The answer_unit is 'percent', so the answer_value should be a number representing the percentage."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The energy consumption for pre-training the BLOOM model is not directly stated in the context. However, the training energy for BLOOMz-7B is given as 51,686 kWh. Since BLOOMz models are fine-tuned versions of the original BLOOM family of models, we can refer to the BLOOM carbon footprint estimation paper. The context mentions BLOOM-176B's training cost and other related details but does not directly provide the energy consumption for pre-training BLOOM. However, it is mentioned that BLOOMz-7B required 51,686 kWh for training. To convert kWh to MWh, we divide by 1000.","51.686","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The energy consumption for pre-training the BLOOM model is not directly stated in the context. However, the training energy for BLOOMz-7B is given as 51,686 kWh. Since BLOOMz models are fine-tuned versions of the original BLOOM family of models, we can refer to the BLOOM carbon footprint estimation paper. The context mentions BLOOM-176B's training cost and other related details but does not directly provide the energy consumption for pre-training BLOOM. However, it is mentioned that BLOOMz-7B required 51,686 kWh for training. To convert kWh to MWh, we divide by 1000."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context provides information about the Switch Transformer model, including its total number of parameters and the percentage of parameters activated per token. According to Table 4 in the context, the Switch Transformer has 1500 billion parameters, and only 0.1% of these parameters are activated per token. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","0.1","percent","[""patterson2021""]","is_blank","Table 4: Model - Switch Transformer; Number of Parameters (B) - 1500; Percent of model activated on every token - 0.10%","The context provides information about the Switch Transformer model, including its total number of parameters and the percentage of parameters activated per token. According to Table 4 in the context, the Switch Transformer has 1500 billion parameters, and only 0.1% of these parameters are activated per token. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context provides information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. According to the provided context, JetMoE-8B has 8 experts in each MoE layer, as indicated by the hyperparameter 'Nexperts' in Table 1. The 'answer_unit' is 'experts', which requires the answer to be a numeric value representing the number of experts.","8","experts","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","The context provides information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. According to the provided context, JetMoE-8B has 8 experts in each MoE layer, as indicated by the hyperparameter 'Nexperts' in Table 1. The 'answer_unit' is 'experts', which requires the answer to be a numeric value representing the number of experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a figure (Fig. 4) showing the execution time breakdown for Mixtral and BlackMamba models. For BlackMamba with a batch size of 1 and maximum batch size supported, the execution time is broken down into forward, backward, and optimizer stages. Although the exact total execution time for a batch size of 30 is not directly provided, the figure shows the execution time breakdown for different batch sizes.","1.5","second","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown. Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84) 0.0 0.5 1.0 1.5 2.0    Execution Time Breakdown (seconds) Mixtral Mamba","The context provides a figure (Fig. 4) showing the execution time breakdown for Mixtral and BlackMamba models. For BlackMamba with a batch size of 1 and maximum batch size supported, the execution time is broken down into forward, backward, and optimizer stages. Although the exact total execution time for a batch size of 30 is not directly provided, the figure shows the execution time breakdown for different batch sizes."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. The 'answer_unit' is zettaFLOPs, so the answer_value should be a number in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. The 'answer_unit' is zettaFLOPs, so the answer_value should be a number in zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context provides details about the experimental setup for energy-efficient local inference in financial sentiment classification, specifically mentioning the hardware used. The processor reported is an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable). The system type is a 64-bit operating system with an x64-based processor, running on Windows 11 Pro.","The context provides details about the experimental setup for energy-efficient local inference in financial sentiment classification, specifically mentioning the hardware used. The processor reported is an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context provides a table listing the full list of task-specific models tested for the 'Power Hungry Processing' study. The table includes 80 finetuned models across ten tasks.","80","models","[""luccioni2024""]","is_blank","Table 6: The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","The context provides a table listing the full list of task-specific models tested for the 'Power Hungry Processing' study. The table includes 80 finetuned models across ten tasks."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This directly supports the answer. The 'answer_unit' is 'multiplier', so the answer_value should be a number representing the factor by which the carbon footprint decreases.","3","multiplier","[""wu2021a""]","is_blank","Figure 9 illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This directly supports the answer. The 'answer_unit' is 'multiplier', so the answer_value should be a number representing the factor by which the carbon footprint decreases."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2024) - not 2025 - reports that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x. The 'answer_unit' is 'multiplier', so the answer should be a number representing this multiplier.","[1.2,4]","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2024) - not 2025 - reports that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run compute, with a median of 2.2x. The 'answer_unit' is 'multiplier', so the answer should be a number representing this multiplier."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption for training a 6.1 billion parameter transformer model on 256 NVIDIA A100s for 8 days, which consumed 13.8 MWh. It was not trained to completion but only to 13%, and a full training run would take 60 days. The estimated total energy consumption for a full training run is calculated as (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh.","The context provides the energy consumption for training a 6.1 billion parameter transformer model on 256 NVIDIA A100s for 8 days, which consumed 13.8 MWh. It was not trained to completion but only to 13%, and a full training run would take 60 days. The estimated total energy consumption for a full training run is calculated as (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context supports the answer by referencing a 2025 paper that discusses how efficiency gains in AI may paradoxically lead to increased consumption, known as Jevons' Paradox. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name. The relevant text from the context is used to identify the economic principle.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","Similarly to Jevons’ Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible.","The context supports the answer by referencing a 2025 paper that discusses how efficiency gains in AI may paradoxically lead to increased consumption, known as Jevons' Paradox. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name. The relevant text from the context is used to identify the economic principle."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a clear estimate of the global AI demand's water withdrawal by 2027. According to the text, 'the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.' The answer_unit is 'billion cubic meters', so the answer_value should be a numeric range in that unit.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context provides a clear estimate of the global AI demand's water withdrawal by 2027. According to the text, 'the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.' The answer_unit is 'billion cubic meters', so the answer_value should be a numeric range in that unit."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost. This directly contradicts the statement that Red AI is on the decline.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context indicates that Red AI is on the rise despite the well-known diminishing returns of increased cost. This directly contradicts the statement that Red AI is on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context provides the total time cost for training FLM-101B under a growth schedule, which is 21.54 days. This directly answers the question about the total wall-clock time required to train the FLM-101B model using a growth strategy.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context provides the total time cost for training FLM-101B under a growth schedule, which is 21.54 days. This directly answers the question about the total wall-clock time required to train the FLM-101B model using a growth strategy."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that 'A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.' This directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity. Therefore, the statement is False.","0","is_blank","[""wu2021a""]","is_blank","Figure 10: A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context states that 'A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.' This directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity. Therefore, the statement is False."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippets provide information about the estimated electricity consumption for training GPT-3. According to multiple references in the context, the training of GPT-3 is estimated to have consumed 1,287 MWh of electricity. The 'answer_unit' field specifies that the answer should be in MWh, which matches the unit provided in the context.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].","The context snippets provide information about the estimated electricity consumption for training GPT-3. According to multiple references in the context, the training of GPT-3 is estimated to have consumed 1,287 MWh of electricity. The 'answer_unit' field specifies that the answer should be in MWh, which matches the unit provided in the context."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project as a collaborative effort to establish a unified approach for comparing the inference efficiency of AI models. This matches the question's requirement for a standardized method for comparing inference efficiency.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the 'AI Energy Score' project as a collaborative effort to establish a unified approach for comparing the inference efficiency of AI models. This matches the question's requirement for a standardized method for comparing inference efficiency."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a figure (Fig. 6) showing the execution breakdown of the MoE layer for different kernels in Mixtral and BlackMamba models. For BlackMamba, which is referred to as 'Mamba' in the figure, the execution time breakdown is given for different batch sizes. The longest kernel for the MoE layer in BlackMamba with a batch size of 30 is 'matmul(w1)', and its execution time is approximately 1200 microseconds.","1200","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context provides a figure (Fig. 6) showing the execution breakdown of the MoE layer for different kernels in Mixtral and BlackMamba models. For BlackMamba, which is referred to as 'Mamba' in the figure, the execution time breakdown is given for different batch sizes. The longest kernel for the MoE layer in BlackMamba with a batch size of 30 is 'matmul(w1)', and its execution time is approximately 1200 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context snippet from [ref_id=shen2024] states that 'we set the same number of experts to 8 and top-k to 2 for every layer.' This indicates that for a given token in each layer of the JetMoE-8B model, 2 experts are selected for activation. The answer_unit is 'experts', which matches the context.","2","experts","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","The context snippet from [ref_id=shen2024] states that 'we set the same number of experts to 8 and top-k to 2 for every layer.' This indicates that for a given token in each layer of the JetMoE-8B model, 2 experts are selected for activation. The answer_unit is 'experts', which matches the context."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context snippets from [schwartz2019] and [wu2021a] support the statement that deep learning models are increasingly large and computationally-intensive. Specifically, [schwartz2019] mentions a '300,000x increase from 2012 to 2018' in the computations required for deep learning research, which aligns with the claim of a significant increase over a six-year span.","1","is_blank","[""schwartz2019"", ""wu2021a""]","is_blank","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context snippets from [schwartz2019] and [wu2021a] support the statement that deep learning models are increasingly large and computationally-intensive. Specifically, [schwartz2019] mentions a '300,000x increase from 2012 to 2018' in the computations required for deep learning research, which aligns with the claim of a significant increase over a six-year span."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from [ref_id=wu2021b] mentions that 'the global carbon emissions for 2020 dropped by 6.4%'. This directly answers the question about the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic. The answer_unit is 'percent', so the answer_value is expressed as a number in that unit.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context snippet from [ref_id=wu2021b] mentions that 'the global carbon emissions for 2020 dropped by 6.4%'. This directly answers the question about the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic. The answer_unit is 'percent', so the answer_value is expressed as a number in that unit."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources' in 2023. The answer_unit is 'data centers', which aligns with the context provided.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources' in 2023. The answer_unit is 'data centers', which aligns with the context provided."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find its age in 2025, we calculate 2025 - 1956 = 69 years. The answer_unit is 'years', so we express the answer_value as a number in that unit.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find its age in 2025, we calculate 2025 - 1956 = 69 years. The answer_unit is 'years', so we express the answer_value as a number in that unit."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context supports that the 'five cars' estimate is based on a specific AI process described in a 2019 study by Strubell et al. The process is identified as 'neural architecture search (NAS)', which is a large-scale procedure performed less frequently than average AI model training.","neural architecture search (NAS)","is_blank","[""luccioni2025c"", ""dodge2022""]","is_blank","Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS)...","The context supports that the 'five cars' estimate is based on a specific AI process described in a 2019 study by Strubell et al. The process is identified as 'neural architecture search (NAS)', which is a large-scale procedure performed less frequently than average AI model training."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms for Llama 3.1 8B lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. The answer_unit is 'percent', so the answer_value is directly taken as 44.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms for Llama 3.1 8B lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to the configuration that minimizes latency. The answer_unit is 'percent', so the answer_value is directly taken as 44."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III. The table lists the maximum batch size for different model and dataset combinations. For BlackMamba-S (sparse) on the CS dataset, the maximum batch size is 20, and on the MATH dataset, it is 8. Since GSM8K is referred to as 'GS' in the context, we can infer that the value for the CS dataset (Hellaswag, referred to as 'HE' or 'CS') is relevant. Thus, the maximum batch size for BlackMamba-S on 'CS' (HE) is 20.","20","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III. The table lists the maximum batch size for different model and dataset combinations. For BlackMamba-S (sparse) on the CS dataset, the maximum batch size is 20, and on the MATH dataset, it is 8. Since GSM8K is referred to as 'GS' in the context, we can infer that the value for the CS dataset (Hellaswag, referred to as 'HE' or 'CS') is relevant. Thus, the maximum batch size for BlackMamba-S on 'CS' (HE) is 20."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context provides information about Amazon's energy storage capacity as of 2023. According to the 2023 Amazon Sustainability Report Appendix, Amazon held 1.3 GW of storage capacity, up from 445 MW in 2022. The answer_unit is 'GW', so the answer_value should be expressed as a number in gigawatts.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context provides information about Amazon's energy storage capacity as of 2023. According to the 2023 Amazon Sustainability Report Appendix, Amazon held 1.3 GW of storage capacity, up from 445 MW in 2022. The answer_unit is 'GW', so the answer_value should be expressed as a number in gigawatts."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that 'A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights.' This directly supports the answer by providing a comparison between GPT-4o's annual carbon emissions and the emissions from transatlantic flights. The answer_unit is 'flights', so the answer_value should be a number representing the number of flights.","1000","flights","[""jegham2025""]","is_blank","A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights and consume water equivalent to the annual drinking needs of millions of people.","The context states that 'A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights.' This directly supports the answer by providing a comparison between GPT-4o's annual carbon emissions and the emissions from transatlantic flights. The answer_unit is 'flights', so the answer_value should be a number representing the number of flights."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. The closest relevant information is about the reliance on fossil fuels for electricity production globally and in the U.S., but it does not specify the percentage of AI inference workloads powered by coal in Asia.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. The closest relevant information is about the reliance on fossil fuels for electricity production globally and in the U.S., but it does not specify the percentage of AI inference workloads powered by coal in Asia."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides information about the average data center PUE in 2023, which was 1.58 globally. Although it does not specifically mention 'AI-dedicated data centers', it is the closest relevant information available in the context.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides information about the average data center PUE in 2023, which was 1.58 globally. Although it does not specifically mention 'AI-dedicated data centers', it is the closest relevant information available in the context."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token. This directly answers the question regarding the number of parameters activated during inference for each input token.","2","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token. This directly answers the question regarding the number of parameters activated during inference for each input token."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table (Table 1 in the Appendix of [ref_id=luccioni2025c]) that lists the GHG emissions for various AI models, including the Llama 7B model. According to this table, the GHG emissions associated with pre-training the Llama 7B model are 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1 in the Appendix: 'Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)... Llama 7B 63 Meta 356 14'","The context provides a table (Table 1 in the Appendix of [ref_id=luccioni2025c]) that lists the GHG emissions for various AI models, including the Llama 7B model. According to this table, the GHG emissions associated with pre-training the Llama 7B model are 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in 2022 and 2023 for different regions. To find the total number added across 2022 and 2023, we need to look at the data for each region and sum the differences. The table 'Last Mile Electric Delivery Vehicles by Region' provides the necessary data: U.S. had 2,600 in 2022 and 11,800 in 2023, Europe had 1,220 in 2022 and 3,000+ in 2023, and India had 3,800 in 2022 and 7,200+ in 2023. We calculate the total added by summing the differences for each region.","19200","electric delivery vans","[""amazon2023""]","is_blank","Last Mile Electric Delivery Vehicles by Region: U.S. (2,600 to 11,800), Europe (1,220 to 3,000+), India (3,800 to 7,200+). Total added = (11,800 - 2,600) + (3,000 - 1,220) + (7,200 - 3,800) = 9,200 + 1,780 + 3,400 = 14,380. However, since the exact numbers for Europe and India in 2023 are not given but rather '3,000+' and '7,200+', we can only confirm a minimum of 9,200 (9,200 = 9,200+). The actual number could be higher due to the '+' in the 2023 values for Europe and India.","The context provides the number of electric delivery vans in 2022 and 2023 for different regions. To find the total number added across 2022 and 2023, we need to look at the data for each region and sum the differences. The table 'Last Mile Electric Delivery Vehicles by Region' provides the necessary data: U.S. had 2,600 in 2022 and 11,800 in 2023, Europe had 1,220 in 2022 and 3,000+ in 2023, and India had 3,800 in 2022 and 7,200+ in 2023. We calculate the total added by summing the differences for each region."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The energy consumption of the o3 model for a long prompt is found in Table 4 of the document with ref_id 'jegham2025'. The table lists the energy consumption for various models across three prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is given as 12.222 ± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). o3 | 1.177 ± 0.224 | 5.153 ± 2.107 | 12.222 ± 1.082","The energy consumption of the o3 model for a long prompt is found in Table 4 of the document with ref_id 'jegham2025'. The table lists the energy consumption for various models across three prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is given as 12.222 ± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context snippets from the document 'stone2022' clearly state that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. The 'answer_unit' is 'year', so the answer should be a numeric value representing the year.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context snippets from the document 'stone2022' clearly state that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. The 'answer_unit' is 'year', so the answer should be a numeric value representing the year."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The closest information is from Amazon's 2023 report, which mentions its own WUE for data centers, but not Google's. Therefore, the answer cannot be determined from the provided context.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. The closest information is from Amazon's 2023 report, which mentions its own WUE for data centers, but not Google's. Therefore, the answer cannot be determined from the provided context."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context snippet from [ref_id=wu2021b] states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity' as reported by the International Energy Agency. This directly supports the answer to the True or False question.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context snippet from [ref_id=wu2021b] states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity' as reported by the International Energy Agency. This directly supports the answer to the True or False question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides information about the energy consumption of LLaMA-65B at different maximum generation lengths. Specifically, it states that at a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token.","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context provides information about the energy consumption of LLaMA-65B at different maximum generation lengths. Specifically, it states that at a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet from [ref_id=wu2021a] states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. This directly answers the question regarding the percentage reduction in model size after quantization.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context snippet from [ref_id=wu2021a] states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. This directly answers the question regarding the percentage reduction in model size after quantization."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the claim 'AI can reduce global GHG emissions by 5-10%' is mentioned in multiple Boston Consulting Group (BCG) reports. However, it is stated that the reasoning behind this estimate is unclear and the underlying calculations are not detailed, lacking scientific grounding. Therefore, the claim is not supported by clear, publicly available calculations and sound scientific grounding.","0","is_blank","[""luccioni2025c""]","is_blank","One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that 'Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union'. The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes.","The context indicates that the claim 'AI can reduce global GHG emissions by 5-10%' is mentioned in multiple Boston Consulting Group (BCG) reports. However, it is stated that the reasoning behind this estimate is unclear and the underlying calculations are not detailed, lacking scientific grounding. Therefore, the claim is not supported by clear, publicly available calculations and sound scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context supports the answer by stating that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly answers the True or False question. The answer_unit is 'is_blank', indicating that the answer should be a direct identifier or a simple '1' or '0' for True or False questions.","1","is_blank","[""patterson2021""]","is_blank","● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context supports the answer by stating that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly answers the True or False question. The answer_unit is 'is_blank', indicating that the answer should be a direct identifier or a simple '1' or '0' for True or False questions."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. The answer_unit is USD, so the answer_value is the number in USD.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context directly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. The answer_unit is USD, so the answer_value is the number in USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context snippet from [ref_id=xia2024] states that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly supports the answer to the question about the percentage of running time the optimizer stage takes during BlackMamba sparse fine-tuning with a batch size of 1. The answer_unit is 'percent', so the answer_value is expressed as a number.","53","percent","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","The context snippet from [ref_id=xia2024] states that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly supports the answer to the question about the percentage of running time the optimizer stage takes during BlackMamba sparse fine-tuning with a batch size of 1. The answer_unit is 'percent', so the answer_value is expressed as a number."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","To estimate the daily energy consumption of a large-scale service like Google Translate, we use the given average energy consumption for a comparable multi-purpose model, Flan-T5-xxl, which is 0.083 kWh per 1,000 queries. For 1 billion queries per day, the total energy consumption is calculated as (1,000,000,000 queries / 1,000 queries) * 0.083 kWh = 83,000 kWh. To convert this to MWh, we divide by 1,000, resulting in 83 MWh.","83","MWh","[""luccioni2024""]","is_blank","1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.","To estimate the daily energy consumption of a large-scale service like Google Translate, we use the given average energy consumption for a comparable multi-purpose model, Flan-T5-xxl, which is 0.083 kWh per 1,000 queries. For 1 billion queries per day, the total energy consumption is calculated as (1,000,000,000 queries / 1,000 queries) * 0.083 kWh = 83,000 kWh. To convert this to MWh, we divide by 1,000, resulting in 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context provides the average global data center PUE in 2023 as 1.58. The answer_unit is 'PUE', which is a ratio and does not require conversion.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the average global data center PUE in 2023 as 1.58. The answer_unit is 'PUE', which is a ratio and does not require conversion."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. The available information is about Amazon's renewable energy projects, not Microsoft's.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. The available information is about Amazon's renewable energy projects, not Microsoft's."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a clear percentage attributed to R&D staff costs for Gemini Ultra, including equity. The relevant information is found in multiple references from the document 'cottier2024'.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context provides a clear percentage attributed to R&D staff costs for Gemini Ultra, including equity. The relevant information is found in multiple references from the document 'cottier2024'."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context mentions that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the number of members in the inaugural 2015 Study Panel.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of  experts in AI from academia, corporate laboratories and industry , and AI-savvy scholars in law , political science, policy , and economics, was launched in mid-fall 2015.","The context mentions that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the number of members in the inaugural 2015 Study Panel."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet from [ref_id=wu2021a] states that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly supports the answer and matches the expected format of a percentage.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context snippet from [ref_id=wu2021a] states that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly supports the answer and matches the expected format of a percentage."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context snippets indicate that the energy consumption during inference is influenced by multiple factors including model type, model size, and hardware specifications. While larger models tend to consume more energy, it is not a universal rule that a model with more parameters will always consume more energy during inference. The best-performing model in terms of accuracy can also be energy-efficient.","0","is_blank","[""zschache2025""]","is_blank","Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy.","The context snippets indicate that the energy consumption during inference is influenced by multiple factors including model type, model size, and hardware specifications. While larger models tend to consume more energy, it is not a universal rule that a model with more parameters will always consume more energy during inference. The best-performing model in terms of accuracy can also be energy-efficient."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context clearly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, as mentioned in the EXECUTIVE SUMMARY section of the report from the 100 Year Study on AI.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future.","The context clearly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, as mentioned in the EXECUTIVE SUMMARY section of the report from the 100 Year Study on AI."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide information about the CO2 emissions from OpenAI's API requests in January 2024. Therefore, the answer cannot be determined based on the provided context snippets.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information about the CO2 emissions from OpenAI's API requests in January 2024. Therefore, the answer cannot be determined based on the provided context snippets."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides information about the GHG emissions from Meta's Llama 3 family of models. According to the text, Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is significantly lower.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the 'five cars' estimate.","The context provides information about the GHG emissions from Meta's Llama 3 family of models. According to the text, Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is significantly lower."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context provides a clear statement that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over 1.5 years. This directly supports the answer to the question.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context provides a clear statement that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over 1.5 years. This directly supports the answer to the question."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context snippets provide information about AlphaGo, a computer program developed by Google DeepMind, and its achievement in defeating the human Go champion. The text explicitly states that AlphaGo defeated the current human champion at the game of Go.","1","is_blank","[""stone2022""]","is_blank","For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of  Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.","The context snippets provide information about AlphaGo, a computer program developed by Google DeepMind, and its achievement in defeating the human Go champion. The text explicitly states that AlphaGo defeated the current human champion at the game of Go."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This directly supports the answer that the batching strategy is called 'Continuous batching'. The answer_unit is 'is_blank', indicating that the answer should be an exact identifier/name from the context.","Continuous batching","is_blank","[""fernandez2025"", ""chen2024""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This directly supports the answer that the batching strategy is called 'Continuous batching'. The answer_unit is 'is_blank', indicating that the answer should be an exact identifier/name from the context."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context snippet from [ref_id=shen2024] explicitly states that 'The entire alignment process takes 60 H100 GPU hours.' This directly answers the question regarding the number of H100 GPU hours required for the JetMoE-8B alignment process.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context snippet from [ref_id=shen2024] explicitly states that 'The entire alignment process takes 60 H100 GPU hours.' This directly answers the question regarding the number of H100 GPU hours required for the JetMoE-8B alignment process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context provides a comparison between the Max-Performance policy and InferSave in the offline workload experiment with a 100 TPS SLO. According to Table VI, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100 = 26.7%.","26.7","percent","[""kim2025""]","is_blank","TABLE VI: Comparison of Instance Selection Results by SLO Constraints (100 TPS and 200 TPS). For 100 TPS, InferSave-1st selected g4dn.xlarge with a total cost of $2.13, while Max-Perf. selected g6e.xlarge with a total cost of $2.699.","The context provides a comparison between the Max-Performance policy and InferSave in the offline workload experiment with a 100 TPS SLO. According to Table VI, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100 = 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet from [ref_id=luccioni2024] directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit is 'g CO2eq', which matches the unit provided in the context.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) generates as much carbon as 0.0006 miles driven by a similar vehicle, i.e. 6,833 times less.","The context snippet from [ref_id=luccioni2024] directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit is 'g CO2eq', which matches the unit provided in the context."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a table (Table B1) in the appendix of a 2025 study, listing the energy consumption of various models for inference tasks. The energy usage ranges from less than 0.01 Wh to 702.06 Wh for different models. To find the range for 1,000 queries, we consider the values given for a single query and scale them. The lowest value is less than 0.01 Wh (e.g., Linear BoW, Linear Tf-idf), and the highest is 702.06 Wh (DS Llama 70B). For 1,000 queries, the range becomes [less than 10 Wh, 702,060 Wh] or [less than 10, 702060] in Wh.","[0.01, 702060]","Wh","[""zschache2025""]","is_blank","Table B1 in Appendix B of the 2025 study by Zschache et al., which lists energy consumption for various models.","The context provides a table (Table B1) in the appendix of a 2025 study, listing the energy consumption of various models for inference tasks. The energy usage ranges from less than 0.01 Wh to 702.06 Wh for different models. To find the range for 1,000 queries, we consider the values given for a single query and scale them. The lowest value is less than 0.01 Wh (e.g., Linear BoW, Linear Tf-idf), and the highest is 702.06 Wh (DS Llama 70B). For 1,000 queries, the range becomes [less than 10 Wh, 702,060 Wh] or [less than 10, 702060] in Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context indicates that researchers do not believe in a universal, one-size-fits-all approach to AI ethics and sustainability. The text states, 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly supports the answer.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context indicates that researchers do not believe in a universal, one-size-fits-all approach to AI ethics and sustainability. The text states, 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly supports the answer."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The gross carbon intensity of energy according to the U.S. average mix is mentioned in the context snippet from [ref_id=patterson2021]. The value is given as 0.429 kg of CO2e/KWh. The answer_unit is 'kg of CO2e/KWh', so the answer_value should be a number in this unit.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The gross carbon intensity of energy according to the U.S. average mix is mentioned in the context snippet from [ref_id=patterson2021]. The value is given as 0.429 kg of CO2e/KWh. The answer_unit is 'kg of CO2e/KWh', so the answer_value should be a number in this unit."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions that Hivemind is a PyTorch-based framework used for decentralized deep learning, which enabled distributed spot training across multiple clouds and continents. The 'answer_unit' is 'is_blank', indicating that the answer should be the exact identifier/name from the context.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions that Hivemind is a PyTorch-based framework used for decentralized deep learning, which enabled distributed spot training across multiple clouds and continents. The 'answer_unit' is 'is_blank', indicating that the answer should be the exact identifier/name from the context."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context supports the answer by mentioning that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems. This includes portraying an AI system's societal impacts, ethical considerations, and environmental footprint.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.","The context supports the answer by mentioning that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems. This includes portraying an AI system's societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.' This directly answers the question about which model achieved the highest accuracy in classification experiments. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name.","traditional linear model","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.' This directly answers the question about which model achieved the highest accuracy in classification experiments. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the authors develop and deploy 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This directly answers the question about the name of the LLM inference system.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions that the authors develop and deploy 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This directly answers the question about the name of the LLM inference system."
"q093","How many parameters does the largest T5 model have?","The context provides information about various large language models, including their parameter counts. The T5 model is mentioned in the context with a parameter count of 11 billion.","11","parameters","[""patterson2021""]","is_blank","Table 4 in [patterson2021] lists the number of parameters for various models, including T5, which has 11 billion parameters.","The context provides information about various large language models, including their parameter counts. The T5 model is mentioned in the context with a parameter count of 11 billion."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in the JetMoE-8B model is directly stated in Table 3 as 8B.","8B","parameters","[""shen2024""]","is_blank","Table 3: # Total Params 7B 16B 2B 8B","The total number of parameters in the JetMoE-8B model is directly stated in Table 3 as 8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context snippet from [ref_id=luccioni2025a] states that Google observed a 20% uptick in data center water consumption from 2021 to 2022. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","20","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context snippet from [ref_id=luccioni2025a] states that Google observed a 20% uptick in data center water consumption from 2021 to 2022. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context does not explicitly define 'CO₂ emissions per unit of electricity consumed' as a specific emissions metric name. However, it discusses various metrics and methods for estimating carbon emissions and energy efficiency in AI models. The closest relevant term is 'carbon intensity,' which is mentioned in the context of cloud instances and AI energy consumption.","carbon intensity","is_blank","[""dodge2022""]","is_blank","The paper provides a framework for measuring software carbon intensity, and proposes to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit.","The context does not explicitly define 'CO₂ emissions per unit of electricity consumed' as a specific emissions metric name. However, it discusses various metrics and methods for estimating carbon emissions and energy efficiency in AI models. The closest relevant term is 'carbon intensity,' which is mentioned in the context of cloud instances and AI energy consumption."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides a table (Table 2) that lists the parallelism configurations and training throughput for different growth stages of FLM-101B. The table includes the FLOPs utilization percentage for each stage. For the final growth stage (101B), the FLOPs utilization percentage is given as 52.88%. The answer_unit is 'percent', so the answer_value should be the numeric value without the unit.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... Params (billion) ... FLOPs Utilization ... 101 4 4 12 192 2160 165 52.88%","The context provides a table (Table 2) that lists the parallelism configurations and training throughput for different growth stages of FLM-101B. The table includes the FLOPs utilization percentage for each stage. For the final growth stage (101B), the FLOPs utilization percentage is given as 52.88%. The answer_unit is 'percent', so the answer_value should be the numeric value without the unit."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context provides an estimate of the amortized training cost for OpenAI's GPT-4. According to the document with ref_id 'cottier2024', the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M. The answer_unit is 'USD', so the answer_value is expressed as a number in that unit.","40","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context provides an estimate of the amortized training cost for OpenAI's GPT-4. According to the document with ref_id 'cottier2024', the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M. The answer_unit is 'USD', so the answer_value is expressed as a number in that unit."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context provides a specific example of optimizing the carbon footprint of a Transformer-based universal translation model (LM) at Facebook. It mentions that through judicious hardware-software co-design, including platform-level caching, GPU acceleration, low precision data format, and additional algorithmic optimization, the operational energy footprint can be significantly reduced by more than 800×. This directly supports the answer to the question regarding the factor by which full-stack optimization can reduce the operational carbon footprint.","810","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The context provides a specific example of optimizing the carbon footprint of a Transformer-based universal translation model (LM) at Facebook. It mentions that through judicious hardware-software co-design, including platform-level caching, GPU acceleration, low precision data format, and additional algorithmic optimization, the operational energy footprint can be significantly reduced by more than 800×. This directly supports the answer to the question regarding the factor by which full-stack optimization can reduce the operational carbon footprint."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information on the performance of NLP training across different geographic distributions. Specifically, it compares the throughput of NLP training when done locally (A-4) versus when spread across four continents (C-4). The context states that for the four continents experiment, C-4, the throughput is 36% slower compared to the A-4 runs. This implies that the throughput achieved for NLP when training was spread across four continents is 64% of the local throughput.","0.64","multiplier","[""erben2023""]","is_blank","For the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a).","The context provides information on the performance of NLP training across different geographic distributions. Specifically, it compares the throughput of NLP training when done locally (A-4) versus when spread across four continents (C-4). The context states that for the four continents experiment, C-4, the throughput is 36% slower compared to the A-4 runs. This implies that the throughput achieved for NLP when training was spread across four continents is 64% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context provides information about Amazon's water replenishment projects in 2023. It states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit is 'liters', so the answer_value should be expressed as a number in liters.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context provides information about Amazon's water replenishment projects in 2023. It states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit is 'liters', so the answer_value should be expressed as a number in liters."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context supports the answer by stating that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This is directly related to the question about using custom tags with one-shot, zero-shot, and few-shots techniques.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context supports the answer by stating that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This is directly related to the question about using custom tags with one-shot, zero-shot, and few-shots techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context snippet from the 2025 paper [ref_id=luccioni2025a] mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. The 'answer_unit' is 'GPUs', so the answer_value should be a numeric value representing the number of GPUs shipped.","3.7","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context snippet from the 2025 paper [ref_id=luccioni2025a] mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. The 'answer_unit' is 'GPUs', so the answer_value should be a numeric value representing the number of GPUs shipped."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides a breakdown of the components of amortized hardware CapEx + energy in Figure 5, stating that on average, 44% goes toward AI accelerator chips. This directly supports the answer to the question about the percentage attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context provides a breakdown of the components of amortized hardware CapEx + energy in Figure 5, stating that on average, 44% goes toward AI accelerator chips. This directly supports the answer to the question about the percentage attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context snippets provide information about Facebook's data center efficiency, specifically mentioning that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This value is directly mentioned in multiple context snippets.","1.1","PUE","[""wu2021a"", ""wu2021b""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers. [ref_id=wu2021a]; Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020]. [ref_id=wu2021b]","The context snippets provide information about Facebook's data center efficiency, specifically mentioning that Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This value is directly mentioned in multiple context snippets."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions a Finnish project named 'ETAIROS' that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym is directly mentioned in the context.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context mentions a Finnish project named 'ETAIROS' that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym is directly mentioned in the context."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context provides the estimated amortized training costs for Google's Gemini Ultra. According to the document 'THE RISING COSTS OF TRAINING FRONTIER AI MODELS' by Ben Cottier et al., the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD.","30M","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context provides the estimated amortized training costs for Google's Gemini Ultra. According to the document 'THE RISING COSTS OF TRAINING FRONTIER AI MODELS' by Ben Cottier et al., the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context supports that the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments. It is interpreted that these risk assessments should include environmental risks as they are related to fundamental rights.","1","is_blank","[""ebert2024""]","is_blank","Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].","The context supports that the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments. It is interpreted that these risk assessments should include environmental risks as they are related to fundamental rights."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context provides information about the EPA's recently tightened standard for PM2.5, stating that it sets an annual average limit of 9µg/m³. The answer_unit is 'µg/m³', which matches the unit provided in the context.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m³ [48, 52].","The context provides information about the EPA's recently tightened standard for PM2.5, stating that it sets an annual average limit of 9µg/m³. The answer_unit is 'µg/m³', which matches the unit provided in the context."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context provides a life cycle assessment (LCA) comparison between print books and e-readers, specifically stating that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This directly answers the question.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context provides a life cycle assessment (LCA) comparison between print books and e-readers, specifically stating that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This directly answers the question."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context provides information on the public health impacts of AI and data centers, highlighting that economically-disadvantaged communities bear a disproportionate share of the health burden. Specifically, it mentions that the per-household health cost in the most impacted counties can be up to 200 times higher than in less-affected areas.","200","multiplier","[""han2024""]","is_blank","The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context provides information on the public health impacts of AI and data centers, highlighting that economically-disadvantaged communities bear a disproportionate share of the health burden. Specifically, it mentions that the per-household health cost in the most impacted counties can be up to 200 times higher than in less-affected areas."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is directly found in Table B1 under the 'Energy (Wh)' column. The value is 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","Table B1: Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs. The row for 'DS Llama 70B' lists the energy consumption as 702.06 Wh.","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is directly found in Table B1 under the 'Energy (Wh)' column. The value is 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not mention a 2022 paper by Dodge et al. analyzing a large language model's total number of parameters. Therefore, the question cannot be answered based on the provided context.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not mention a 2022 paper by Dodge et al. analyzing a large language model's total number of parameters. Therefore, the question cannot be answered based on the provided context."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox as a phenomenon where efficiency gains lead to increased consumption. This is directly related to the question about technological progress improving efficiency resulting in increased usage and overall resource consumption. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name from the context.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","Economists refer to such transformations as Jevons' Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context describes Jevons' Paradox as a phenomenon where efficiency gains lead to increased consumption. This is directly related to the question about technological progress improving efficiency resulting in increased usage and overall resource consumption. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name from the context."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The energy consumption of Meena and GPT-3 can be found in Table 4 of [ref_id=patterson2021]. Meena consumed 232 MWh, while GPT-3 consumed 1,287 MWh. To find out how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to divide the energy consumption of GPT-3 by that of Meena. The answer_unit is 'multiplier', so we will calculate this ratio.","5.55","multiplier","[""patterson2021""]","is_blank","Table 4 in [ref_id=patterson2021]: Energy Consumption (MWh) for Meena is 232 and for GPT-3 is 1,287.","The energy consumption of Meena and GPT-3 can be found in Table 4 of [ref_id=patterson2021]. Meena consumed 232 MWh, while GPT-3 consumed 1,287 MWh. To find out how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to divide the energy consumption of GPT-3 by that of Meena. The answer_unit is 'multiplier', so we will calculate this ratio."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The question asks for the average energy consumption for performing 1,000 image generation inferences according to Table 2 in a 2024 study on AI's power consumption. The context provides Table 2 from a 2024 study by Luccioni et al., which lists the mean energy consumption for various AI tasks, including image generation.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2 from Luccioni et al. (2024): inference energy (kWh) for image generation = 2.907 kWh (mean) per 1,000 queries.","The question asks for the average energy consumption for performing 1,000 image generation inferences according to Table 2 in a 2024 study on AI's power consumption. The context provides Table 2 from a 2024 study by Luccioni et al., which lists the mean energy consumption for various AI tasks, including image generation."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a table comparing estimated CO2 emissions from various activities, including the average American life in one year. According to the table, an average American life in one year is estimated to emit 36,156 pounds of CO2e.","36156","lbs","[""strubell2019""]","is_blank","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. 1
Consumption CO2e (lbs)
Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl. fuel, 1 lifetime 126,000","The context provides a table comparing estimated CO2 emissions from various activities, including the average American life in one year. According to the table, an average American life in one year is estimated to emit 36,156 pounds of CO2e."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context does not provide 2030 projections; it only discusses projections for 2028. However, it does provide information on the per-household health cost of U.S. data centers from 2019 to 2023. According to Figure 6c, Marion County in West Virginia is listed as having the highest per-household health cost among the top-10 counties.","Marion","is_blank","[""han2024""]","is_blank","Figure 6c: Top-10 counties by per-household health cost. State County Per-household Health Cost($) IR WV Marion 1218.3(978.0, 1458.5) 0.80","The context does not provide 2030 projections; it only discusses projections for 2028. However, it does provide information on the per-household health cost of U.S. data centers from 2019 to 2023. According to Figure 6c, Marion County in West Virginia is listed as having the highest per-household health cost among the top-10 counties."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides a table (TABLE III) comparing performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the CO2 emissions changed from 0.020 kg to 0.015 kg after optimization. To find the multiplier, we divide the emissions after optimization by the emissions before optimization: 0.015 / 0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLM S BEFORE AND AFTER OPTIMIZATION. Mistral-small's CO2 emissions changed from 0.020 kg to 0.015 kg.","The context provides a table (TABLE III) comparing performance metrics and carbon emissions for five LLMs before and after optimization. For Mistral-small, the CO2 emissions changed from 0.020 kg to 0.015 kg after optimization. To find the multiplier, we divide the emissions after optimization by the emissions before optimization: 0.015 / 0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The combined training and fine-tuning energy costs for the BLOOMz-7B model are directly reported in Table 5 of the 'Power Hungry Processing' study. The training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh. The sum of these values gives the combined energy cost.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The combined training and fine-tuning energy costs for the BLOOMz-7B model are directly reported in Table 5 of the 'Power Hungry Processing' study. The training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh. The sum of these values gives the combined energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The context provides the number of parameters for the FLM-101B model in multiple places. Specifically, it is mentioned that the final model has 101B parameters. The 'answer_unit' is 'parameters', which indicates that the answer should be a numeric value representing the number of parameters.","101","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with ... 101B parameters (Section 2.3); Table 2: Parallel strategies and throughput for different growth stages, Params (billion): 101","The context provides the number of parameters for the FLM-101B model in multiple places. Specifically, it is mentioned that the final model has 101B parameters. The 'answer_unit' is 'parameters', which indicates that the answer should be a numeric value representing the number of parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The energy required for a full training run of a 6.1B parameter model is estimated to be 103,500 kWh. The energy cost per inference for BLOOMz-7B is 1.0 × 10−4 kWh. To find the number of inferences needed to match the training energy cost, we divide the total training energy by the energy per inference.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","Table 2 from dodge2022: '...we estimate a full training run would consume approximately 103,593 kWh.' and Table 5 from luccioni2024: 'Inference energy (kWh) 1.0 × 10−4 for BLOOMz-7B'","The energy required for a full training run of a 6.1B parameter model is estimated to be 103,500 kWh. The energy cost per inference for BLOOMz-7B is 1.0 × 10−4 kWh. To find the number of inferences needed to match the training energy cost, we divide the total training energy by the energy per inference."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context snippet from 'Power Hungry Processing' (2024) study by Luccioni et al. directly states the total energy consumed for all model experimentation and evaluation. The answer_unit is specified as 'kWh', which matches the unit provided in the context.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context snippet from 'Power Hungry Processing' (2024) study by Luccioni et al. directly states the total energy consumed for all model experimentation and evaluation. The answer_unit is specified as 'kWh', which matches the unit provided in the context."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training for various BLOOMz models, including BLOOMz-7B. The table directly gives the number of inferences for BLOOMz-7B as 592,570,000.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides a table (Table 5) that lists the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training for various BLOOMz models, including BLOOMz-7B. The table directly gives the number of inferences for BLOOMz-7B as 592,570,000."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context snippet from [ref_id=zschache2025] mentions that the data originates from the process of selecting a repository site for high-level radioactive waste in Germany, specifically from a process called FKTG (Fachkonferenz Teilgebiete), and the dataset is referred to as the FKTG-dataset.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context snippet from [ref_id=zschache2025] mentions that the data originates from the process of selecting a repository site for high-level radioactive waste in Germany, specifically from a process called FKTG (Fachkonferenz Teilgebiete), and the dataset is referred to as the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The available data and discussions are about the general environmental impact of AI models, including water consumption, but do not directly address Llama 3's inference water usage in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The available data and discussions are about the general environmental impact of AI models, including water consumption, but do not directly address Llama 3's inference water usage in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The context only discusses the estimation of environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass, but does not mention the use of recycled rare earth metals.","is_blank","percent","[""morrison2025""]","is_blank","is_blank","The context does not provide information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The context only discusses the estimation of environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass, but does not mention the use of recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a comparison between the CO2e of training NLP models and the CO2e of a round trip flight between San Francisco and New York. It states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context provides a comparison between the CO2e of training NLP models and the CO2e of a round trip flight between San Francisco and New York. It states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context provides data from OpenRouter in May 2025, stating that 84% of LLM token usage is through models with no disclosure of their environmental impact. The answer_unit is 'percent', so the answer_value is directly taken as 84.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context provides data from OpenRouter in May 2025, stating that 84% of LLM token usage is through models with no disclosure of their environmental impact. The answer_unit is 'percent', so the answer_value is directly taken as 84."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA models. For LLaMA-13B, it specifies that 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 13B 2 64 1 64","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA models. For LLaMA-13B, it specifies that 1 A100 80GB GPU is required."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides an estimate of CO2 emissions for a 6 billion parameter transformer model trained to completion. It states that if the model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region it was run in. The question asks for the estimated range of CO2 emissions for a complete training run of a 6.1 billion parameter transformer model, which is close enough to the 6 billion parameter model discussed in the context to infer a similar range.","[21,78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context provides an estimate of CO2 emissions for a 6 billion parameter transformer model trained to completion. It states that if the model had been trained to completion, it would have emitted 21 to 78 metric tons of CO2, depending on the region it was run in. The question asks for the estimated range of CO2 emissions for a complete training run of a 6.1 billion parameter transformer model, which is close enough to the 6 billion parameter model discussed in the context to infer a similar range."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. The provided snippets discuss various aspects of carbon emissions related to machine learning models but do not directly address the specific question about the total carbon emissions avoided through pruning and quantizing in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. The provided snippets discuss various aspects of carbon emissions related to machine learning models but do not directly address the specific question about the total carbon emissions avoided through pruning and quantizing in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context provides a specific scenario where using 2 A100s and 1 A10G results in a cost saving compared to using A100-only or A10G-only. The percentage of cost savings achieved over an A100-only strategy is directly mentioned in the context.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context provides a specific scenario where using 2 A100s and 1 A10G results in a cost saving compared to using A100-only or A10G-only. The percentage of cost savings achieved over an A100-only strategy is directly mentioned in the context."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is listed in Table 1. The table provides specifications and prices for various GPUs, including the H20. The price is given as $4.63/hr.","4.63","USD per hour","[""chen2024""]","is_blank","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.
Table 1: H100, H20, and TPU v6e specifications.","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is listed in Table 1. The table provides specifications and prices for various GPUs, including the H20. The price is given as $4.63/hr."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors,' which directly contradicts the statement in the question. Therefore, the answer is False, which corresponds to 0.","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon footprint analyses gather the information manually by writing to authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors,' which directly contradicts the statement in the question. Therefore, the answer is False, which corresponds to 0."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that the public health cost of U.S. data centers in 2023 was about $6.7 billion, which is equivalent to approximately 44% of the data centers' total electricity cost. This information directly supports the answer.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers' total electricity cost.","The context states that the public health cost of U.S. data centers in 2023 was about $6.7 billion, which is equivalent to approximately 44% of the data centers' total electricity cost. This information directly supports the answer."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, it specifies that 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size | V100 32GB | A100 80GB
Count | Max. Batch size | Count | Max. Batch size
7B | 1 | 64 | 1 | 64","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, it specifies that 1 A100 80GB GPU is required."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context snippet from [ref_id=khan2025] states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', supporting the answer.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization","The context snippet from [ref_id=khan2025] states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', supporting the answer."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context snippets indicate that researchers reached out to over 500 authors of AI papers to gather information for estimating the carbon footprint of their models. They were able to collect 95 answers, which is directly stated in the provided text.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers...","The context snippets indicate that researchers reached out to over 500 authors of AI papers to gather information for estimating the carbon footprint of their models. They were able to collect 95 answers, which is directly stated in the provided text."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours. This gives us a cost per hour.","3.33","USD per hour","[""shen2024""]","is_blank","JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The JetMoE project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours. This gives us a cost per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table (Table 2) that lists the health cost and electricity cost for training a Llama-3.1 scale model in various locations, including Altoona, Iowa. The table shows that in Altoona, Iowa, the health cost is $2.51 million and the percentage of the electricity cost is 122%. The answer_unit is 'percent', so the answer_value should be a number representing the percentage.","122","percent","[""han2024""]","is_blank","Table 2: The public health cost of training a large AI model in selected U.S. data centers. Location Electricity Price (¢/kWh) Electricity (million$) Health Cost (million$) % of Electricity Cost Emission(Metric Ton) PM2.5(LA-NYC) NOx(LA-NYC) SO2 Huntsville, AL 7.11 2.1 0.70(0.54, 0.87) 33% 0.61 (13800) 2.80 (2500) 2.72 Stanton Springs, GA 6.88 2.0 0.85(0.65, 1.04) 41% 0.69 (15500) 3.37 (3000) 3.35 DeKalb, IL 8.20 2.4 1.92(1.41, 2.42) 79% 1.25 (28100) 7.31 (6600) 7.83 Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context provides a table (Table 2) that lists the health cost and electricity cost for training a Llama-3.1 scale model in various locations, including Altoona, Iowa. The table shows that in Altoona, Iowa, the health cost is $2.51 million and the percentage of the electricity cost is 122%. The answer_unit is 'percent', so the answer_value should be a number representing the percentage."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context snippet from [ref_id=shen2024] states that 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.' The 'T' in '1.25T' denotes trillion, so 1.25T tokens is 1.25 trillion tokens. The answer_unit is 'tokens', which matches the unit in the question.","1250000000000.0","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context snippet from [ref_id=shen2024] states that 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.' The 'T' in '1.25T' denotes trillion, so 1.25T tokens is 1.25 trillion tokens. The answer_unit is 'tokens', which matches the unit in the question."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table titled 'Amazon Renewable Energy Projects*' with data as of January 2024, which includes the number of renewable energy projects by country. The table lists 36 projects in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","Project Location | Number of Projects | Total MW Capacity†
United Kingdom | 36 | 901","The context provides a table titled 'Amazon Renewable Energy Projects*' with data as of January 2024, which includes the number of renewable energy projects by country. The table lists 36 projects in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides a table showing Amazon's representation by the numbers for different demographics, including gender, across various levels and regions. For the U.S. workforce across all levels in 2023, the percentage of men is given as 68.3%.","68.3","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) - 2023: 31.6%68.3%","The context provides a table showing Amazon's representation by the numbers for different demographics, including gender, across various levels and regions. For the U.S. workforce across all levels in 2023, the percentage of men is given as 68.3%."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint accounted for by its supply chain.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint accounted for by its supply chain."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The total execution time for a sparse BlackMamba model fine-tuned with a batch size of 84 is found in Fig. 4, which provides the execution time breakdown for Mixtral and BlackMamba models under different configurations. The figure shows that for BlackMamba with a batch size of 84, the total execution time is approximately 2 seconds.","2","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","The total execution time for a sparse BlackMamba model fine-tuned with a batch size of 84 is found in Fig. 4, which provides the execution time breakdown for Mixtral and BlackMamba models under different configurations. The figure shows that for BlackMamba with a batch size of 84, the total execution time is approximately 2 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces a metric called 'granularity' to compare model suitability for distributed spot training and estimate training performance with additional spot VMs. It is defined as the ratio of calculation to communication time. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context.","granularity","is_blank","[""erben2023""]","is_blank","For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context introduces a metric called 'granularity' to compare model suitability for distributed spot training and estimate training performance with additional spot VMs. It is defined as the ratio of calculation to communication time. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context mentions that a coalition of Microsoft employees estimated a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This translates to 6.4 times more carbon emissions.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year","The context mentions that a coalition of Microsoft employees estimated a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This translates to 6.4 times more carbon emissions."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context provides a definition for 'water withdrawal' as freshwater taken from ground or surface water sources for various uses. This definition directly answers the question.","water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context provides a definition for 'water withdrawal' as freshwater taken from ground or surface water sources for various uses. This definition directly answers the question."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.' This directly supports the answer. The 'answer_unit' is 'years', so the answer_value should be a number representing the frequency in years.","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.' This directly supports the answer. The 'answer_unit' is 'years', so the answer_value should be a number representing the frequency in years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context snippets provide information about the average number of connected devices per U.S. household in 2021. According to the text, the average U.S. household had 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. The answer_unit is 'devices', which matches the context.","25","devices","[""wu2021b""]","is_blank","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context snippets provide information about the average number of connected devices per U.S. household in 2021. According to the text, the average U.S. household had 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. The answer_unit is 'devices', which matches the context."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides information on the energy consumption of pre-training large language models (LLMs). According to the snippet from [ref_id=luccioni2025c], the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is directly extracted from the provided context.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides information on the energy consumption of pre-training large language models (LLMs). According to the snippet from [ref_id=luccioni2025c], the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is directly extracted from the provided context."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context snippets clearly state that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of  information gathered from various sources.","The context snippets clearly state that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from [ref_id=luccioni2025a] mentions that 'one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water'. This directly supports the answer, and the 'answer_unit' is 'queries', so the answer should be a numeric range.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context snippet from [ref_id=luccioni2025a] mentions that 'one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water'. This directly supports the answer, and the 'answer_unit' is 'queries', so the answer should be a numeric range."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context provides a comparison of MT-Bench scores for various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to Table 4, JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models
Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269","The context provides a comparison of MT-Bench scores for various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to Table 4, JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context provides a case study on estimating GPT-3's operational water consumption footprint, including the number of requests that can be made with a 500ml bottle of water. According to Table 1 in the context, the number of requests for 500ml water varies by location. The question is answered based on this information.","[10.5,70.4]","responses","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. '# of Requests for 500ml Water' column provides the relevant data.","The context provides a case study on estimating GPT-3's operational water consumption footprint, including the number of requests that can be made with a 500ml bottle of water. According to Table 1 in the context, the number of requests for 500ml water varies by location. The question is answered based on this information."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational settings. The context directly supports this answer by stating 'Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings'. The answer_unit is 'percent', so the answer_value is expressed as a number.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational settings. The context directly supports this answer by stating 'Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings'. The answer_unit is 'percent', so the answer_value is expressed as a number."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware requirements for different sizes of LLaMA models. For LLaMA-65B, it specifies that 4 A100 80GB GPUs are required. This directly answers the question regarding the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 65B | 8 | 64 | 4 | 128","The context provides a table (Table II) that lists the bare minimum hardware requirements for different sizes of LLaMA models. For LLaMA-65B, it specifies that 4 A100 80GB GPUs are required. This directly answers the question regarding the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context snippets from [ref_id=han2024] state that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City. The answer_unit is 'round trips', so the answer_value should be a number representing the count of round trips.","10000","round trips","[""han2024""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost.","The context snippets from [ref_id=han2024] state that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City. The answer_unit is 'round trips', so the answer_value should be a number representing the count of round trips."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context snippets from the documents [ref_id=patterson2021] and [ref_id=chung2025] both mention that NVIDIA estimated 80–90% of the ML workload is inference processing. This directly supports the answer and matches the expected 'percent' unit.","[80,90]","percent","[""patterson2021"", ""chung2025""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context snippets from the documents [ref_id=patterson2021] and [ref_id=chung2025] both mention that NVIDIA estimated 80–90% of the ML workload is inference processing. This directly supports the answer and matches the expected 'percent' unit."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context snippet from 'Power Hungry Processing' (2024) directly states the total CO2 equivalent emissions generated throughout the study. The answer_unit is 'kg CO2eq', which matches the unit provided in the context.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The context snippet from 'Power Hungry Processing' (2024) directly states the total CO2 equivalent emissions generated throughout the study. The answer_unit is 'kg CO2eq', which matches the unit provided in the context."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not entirely accurate as it often leads to overestimation. The dimmed bars in Figure 3 show estimated energy consumption based on TDP, which is a common practice but results in overestimation by a factor of up to 4.1.","0","is_blank","[""chung2025""]","is_blank","Importance of measuring.The dimmed bars behind each solid bar in Figure 3 show the estimated energy consumption based on the GPU’s Thermal Design Power (TDP) instead of measuring the real GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74]. Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not entirely accurate as it often leads to overestimation. The dimmed bars in Figure 3 show estimated energy consumption based on TDP, which is a common practice but results in overestimation by a factor of up to 4.1."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Additionally, it is mentioned that GPT-4o mini's consumption is slightly higher at 3.098 Wh compared to GPT-4o's 2.875 Wh. This indicates that GPT-4o mini does not consume less energy per query than GPT-4o.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Additionally, it is mentioned that GPT-4o mini's consumption is slightly higher at 3.098 Wh compared to GPT-4o's 2.875 Wh. This indicates that GPT-4o mini does not consume less energy per query than GPT-4o."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a figure (Fig. 15) that shows the estimation and validation of fine-tuning throughput for Mixtral-CS for different GPUs, including A100-40GB. The ground truth throughput for a dense Mixtral-CS-A100-40GB at batch size 1 can be inferred from the plot.","0.25","queries/sec","[""xia2024""]","is_blank","Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.","The context provides a figure (Fig. 15) that shows the estimation and validation of fine-tuning throughput for Mixtral-CS for different GPUs, including A100-40GB. The ground truth throughput for a dense Mixtral-CS-A100-40GB at batch size 1 can be inferred from the plot."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that after 2022, the trend of AI developers directly disclosing environmental information for notable models 'triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.' This indicates that the trend did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","Figure 1. Environmental Impact Transparency of Notable AI Models by Release Year27 yet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of training data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. This period includes the the work of Strubell et al. 12, Luccioni11 and others. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that after 2022, the trend of AI developers directly disclosing environmental information for notable models 'triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.' This indicates that the trend did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides a table (Table 1) from Griggs et al. (2024) that lists the on-demand prices for four NVIDIA GPU types. The price for H100 is given as $7.516 per hour after normalization.","7.516","USD per hour","[""griggs2024""]","is_blank","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)
On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The context provides a table (Table 1) from Griggs et al. (2024) that lists the on-demand prices for four NVIDIA GPU types. The price for H100 is given as $7.516 per hour after normalization."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","","","liters of water","[]","is_blank","is_blank",""
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (assuming 30 days/month, so 30 * 24 = 720 hours). Thus, $5,200 / 720 hours = approximately $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (assuming 30 days/month, so 30 * 24 = 720 hours). Thus, $5,200 / 720 hours = approximately $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that 'with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.' This directly supports the answer. The 'answer_unit' is 'multiplier', indicating that the answer should be a numeric value representing how much larger the model must be.","1000","multiplier","[""wu2021a""]","is_blank","Figure 2(a) depicts the 1000× model size increase for GPT3-based language translation tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.","The context states that 'with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.' This directly supports the answer. The 'answer_unit' is 'multiplier', indicating that the answer should be a numeric value representing how much larger the model must be."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. Although the context does not directly provide an emissions-to-driving-distance ratio, it mentions that the emissions are similar to the lifetime emissions of five US cars. Using this information, we can infer the equivalent driving distance.","is_blank","miles","[""luccioni2023"", ""strubell2019""]","is_blank","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. Transformer (big) w/ neural architecture search: 626,155 lbs CO2e.","The context provides the CO2 emissions for training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. Although the context does not directly provide an emissions-to-driving-distance ratio, it mentions that the emissions are similar to the lifetime emissions of five US cars. Using this information, we can infer the equivalent driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The inference energy for BLOOMz-7B is given as 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the inference energy by the number of downloads and the number of inferences per download, then convert the result to MWh.","60.6096","MWh","[""luccioni2024""]","is_blank","Table 5: BLOOMz-7B inference energy (kWh) = 1.0 × 10−4; Number of downloads = 606,096 (from text).","The inference energy for BLOOMz-7B is given as 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the inference energy by the number of downloads and the number of inferences per download, then convert the result to MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context snippet from [ref_id=shen2024] directly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This matches the expected unit for the answer.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context snippet from [ref_id=shen2024] directly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This matches the expected unit for the answer."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.' This directly supports the answer. The answer_unit is 'USD', so the answer_value should be a number in USD.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","The context states that 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.' This directly supports the answer. The answer_unit is 'USD', so the answer_value should be a number in USD."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context provides the total number of floating point operations to train GPT-3 as published by OpenAI. The value is directly mentioned in the context snippet from [ref_id=patterson2021]. The answer_unit is FLOPS, which is a unit of measurement for floating-point operations.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context provides the total number of floating point operations to train GPT-3 as published by OpenAI. The value is directly mentioned in the context snippet from [ref_id=patterson2021]. The answer_unit is FLOPS, which is a unit of measurement for floating-point operations."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA variants. For LLaMA-65B, it specifies that 8 V100 32GB GPUs are required.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA variants. For LLaMA-65B, it specifies that 8 V100 32GB GPUs are required."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the throughput data for the final 101B training stage in Table 2. The throughput is given as 165 teraFLOP/s, and the number of GPUs used is 2160. To estimate the total computational work performed during this stage, we need to calculate the total FLOPs. First, we convert the throughput to FLOPs per second: 165 teraFLOP/s * 2160 GPUs = 356,400 teraFLOP/s. Then, we need to know the training time for the 101B stage to calculate the total FLOPs.","26.54","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. Params (billion) = 101, Tensor Parallel Size = 4, Pipeline Parallel Size = 4, Data Parallel Size = 12, Number of GPUs = 2160, Batch Size per GPU = 165, teraFLOP/s = 165, FLOPs Utilization = 52.88%","The context provides the throughput data for the final 101B training stage in Table 2. The throughput is given as 165 teraFLOP/s, and the number of GPUs used is 2160. To estimate the total computational work performed during this stage, we need to calculate the total FLOPs. First, we convert the throughput to FLOPs per second: 165 teraFLOP/s * 2160 GPUs = 356,400 teraFLOP/s. Then, we need to know the training time for the 101B stage to calculate the total FLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not directly provide the top-1 accuracy on ImageNet for AlexNet 2012. However, it is mentioned in the context that AlexNet has much lower accuracy compared to ResNet, and a figure is referenced that plots top-1 accuracy on ImageNet for various models including AlexNet. The exact top-1 accuracy value for AlexNet is not provided in the given snippets.","is_blank","percent","[""schwartz2019""]","is_blank","Figure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top): model parameters (in million), FPO (in billions), top-1 accuracy on ImageNet. (4a): Different models: AlexNet [20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17].","The context does not directly provide the top-1 accuracy on ImageNet for AlexNet 2012. However, it is mentioned in the context that AlexNet has much lower accuracy compared to ResNet, and a figure is referenced that plots top-1 accuracy on ImageNet for various models including AlexNet. The exact top-1 accuracy value for AlexNet is not provided in the given snippets."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 GPUs. Thus, the total number of GPUs is 24 servers * 8 GPUs/server = 192 GPUs. The answer_unit is 'GPUs', which matches the expected format.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 GPUs. Thus, the total number of GPUs is 24 servers * 8 GPUs/server = 192 GPUs. The answer_unit is 'GPUs', which matches the expected format."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides an estimate of CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) as 626,155 pounds, which is compared to the lifetime emissions of five US cars. It also provides a comparison of various activities and their CO2 emissions, including the average American lifetime emissions of 36,156 pounds. Using this information, we can calculate the equivalent number of average American lifetimes for the NAS CO2 emissions.","17.3","lifetimes","[""strubell2019"", ""luccioni2025c""]","is_blank","Table 1 in [strubell2019] lists estimated CO2 emissions from training common NLP models, including 'Transformer (big) w/ neural architecture search' as 626,155 pounds CO2e, and 'American life, avg, 1 year' as 36,156 pounds CO2e.","The context provides an estimate of CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) as 626,155 pounds, which is compared to the lifetime emissions of five US cars. It also provides a comparison of various activities and their CO2 emissions, including the average American lifetime emissions of 36,156 pounds. Using this information, we can calculate the equivalent number of average American lifetimes for the NAS CO2 emissions."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context snippet from [ref_id=schwartz2019] states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the number of GPU hours required.","25000","hours","[""schwartz2019""]","is_blank","FAIR's RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context snippet from [ref_id=schwartz2019] states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the number of GPU hours required."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems are estimated to avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This directly supports the answer.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems are estimated to avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This directly supports the answer."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions Megatron-LM as a framework used for efficient large-scale language model training on GPU clusters. This is directly related to deploying large language models across multiple GPUs and nodes.","Megatron-LM","is_blank","[""shen2024"", ""fernandez2025""]","is_blank","Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–15, 2021.","The context mentions Megatron-LM as a framework used for efficient large-scale language model training on GPU clusters. This is directly related to deploying large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table B2) that compares the energy consumption of various models when deployed on a single node versus two nodes. For the Llama 3.1 70B model, the energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes. The ratio of energy consumption on two nodes to one node is given as 1.95, indicating the factor by which energy consumption increased.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2: Model Duration (s) Energy consumed (Wh) single double ratio Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","The context provides a table (Table B2) that compares the energy consumption of various models when deployed on a single node versus two nodes. For the Llama 3.1 70B model, the energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes. The ratio of energy consumption on two nodes to one node is given as 1.95, indicating the factor by which energy consumption increased."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide information on water consumption per ChatGPT user session in 2023. The available data is for 2025 and focuses on energy consumption and projected water usage for GPT-4o. Therefore, the answer is 'is_blank'.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide information on water consumption per ChatGPT user session in 2023. The available data is for 2025 and focuses on energy consumption and projected water usage for GPT-4o. Therefore, the answer is 'is_blank'."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. It also states that these values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). To find the number of homes for 700 million daily queries, we can directly use the given comparison.","35000","homes","[""jegham2025""]","is_blank","GPT-4o’s annual energy consumption is projected to be between 391,509 MWh and 463,269 MWh, exceeding the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","The context provides that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. It also states that these values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). To find the number of homes for 700 million daily queries, we can directly use the given comparison."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippet from [ref_id=luccioni2025a] mentions that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This directly supports the answer to the question.","34","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context snippet from [ref_id=luccioni2025a] mentions that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This directly supports the answer to the question."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This directly supports the answer that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This directly supports the answer that traditional models did not achieve accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context provides the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run. The PUE is a measure of datacenter efficiency. According to the text, the PUE for the Iowa datacenter is given as 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context provides the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run. The PUE is a measure of datacenter efficiency. According to the text, the PUE for the Iowa datacenter is given as 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides an estimate of the total number of GPT-4o queries in 2025 based on a monthly prompt growth rate and usage patterns. The analysis yields a total of approximately 772 billion GPT-4o queries in 2025. The answer_unit is 'queries', which is a count and not a unit like 'kW' or 'USD', so the numeric value is provided directly.","772000000000","queries","[""jegham2025""]","is_blank","GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context provides an estimate of the total number of GPT-4o queries in 2025 based on a monthly prompt growth rate and usage patterns. The analysis yields a total of approximately 772 billion GPT-4o queries in 2025. The answer_unit is 'queries', which is a count and not a unit like 'kW' or 'USD', so the numeric value is provided directly."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is directly stated in Table 3. The table lists the average scores of various models, including JetMoE-8B, on the OpenLLM Leaderboard. According to the table, JetMoE-8B achieved an average score of 53.0.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is directly stated in Table 3. The table lists the average scores of various models, including JetMoE-8B, on the OpenLLM Leaderboard. According to the table, JetMoE-8B achieved an average score of 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022. Therefore, the answer cannot be determined based on the provided context.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022. Therefore, the answer cannot be determined based on the provided context."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. The AI Act's current open-source exemption from reporting obligations should be removed.","0","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context indicates that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. The AI Act's current open-source exemption from reporting obligations should be removed."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context provides information about the US national datacenter average Power Usage Effectiveness (PUE) in 2020. According to the provided snippet from [ref_id=patterson2021], the US national datacenter average PUE in 2020 was 1.59.","1.59","PUE","[""patterson2021""]","is_blank","The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.","The context provides information about the US national datacenter average Power Usage Effectiveness (PUE) in 2020. According to the provided snippet from [ref_id=patterson2021], the US national datacenter average PUE in 2020 was 1.59."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a figure (Figure 1) analyzing the KV Cache size growth for different models, including the OPT-2.7B model, in response to increasing batch sizes on an AWS g4dn.xlarge instance. It is stated that when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit is 'GB', so the answer_value should be a number in GB.","5.312","GB","[""kim2025""]","is_blank","For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context provides a figure (Figure 1) analyzing the KV Cache size growth for different models, including the OPT-2.7B model, in response to increasing batch sizes on an AWS g4dn.xlarge instance. It is stated that when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit is 'GB', so the answer_value should be a number in GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides a clear percentage range for R&D staff costs (including equity) as a fraction of total amortized model development costs for the four notable models studied in-depth: GPT-3, OPT-175B, GPT-4, and Gemini Ultra. The answer_unit is 'percent', so the answer_value should be a numeric range in percent.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context provides a clear percentage range for R&D staff costs (including equity) as a fraction of total amortized model development costs for the four notable models studied in-depth: GPT-3, OPT-175B, GPT-4, and Gemini Ultra. The answer_unit is 'percent', so the answer_value should be a numeric range in percent."
"q213","Which software package was used to measure energy consumption during inference runs?","The context snippets mention that the CodeCarbon package was used to measure energy consumption during inference runs. This is supported by multiple references, including [ref_id=zschache2025] and [ref_id=morrison2025].","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://github.com/mlco2/codecarbon"", ""https://mlco2.github.io/codecarbon/methodology.html#power-usage""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context snippets mention that the CodeCarbon package was used to measure energy consumption during inference runs. This is supported by multiple references, including [ref_id=zschache2025] and [ref_id=morrison2025]."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides an analysis of 100 news articles discussing ChatGPT energy consumption. It states that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search. The answer_unit is 'percent', so the answer_value should be a number representing the percentage.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","The context provides an analysis of 100 news articles discussing ChatGPT energy consumption. It states that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search. The answer_unit is 'percent', so the answer_value should be a number representing the percentage."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions a function called 'Compute Time Calibration Function (CTCF)' that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The 'answer_unit' is 'is_blank', indicating that the answer should be the exact identifier/name from the context.","Compute Time Calibration Function","is_blank","[""kim2025""]","is_blank","The Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions a function called 'Compute Time Calibration Function (CTCF)' that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The 'answer_unit' is 'is_blank', indicating that the answer should be the exact identifier/name from the context."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that 'as the number of shards increases, the energy per output token also increases' for LLaMA 65B, which supports the statement that increasing the number of GPU shards increased the energy cost per response.","1","is_blank","[""samsi2024""]","is_blank","C. Energy per Decoded Token: LLaMA 65B: as the number of shards increases, the energy per output token also increases","The context states that 'as the number of shards increases, the energy per output token also increases' for LLaMA 65B, which supports the statement that increasing the number of GPU shards increased the energy cost per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU is assumed to be 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Therefore, the mass of a single H100 GPU can be calculated, and subsequently, the water consumption for mining rare earth materials for one H100 can be estimated.","0.0022","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","The context provides that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU is assumed to be 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Therefore, the mass of a single H100 GPU can be calculated, and subsequently, the water consumption for mining rare earth materials for one H100 can be estimated."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This implies that under current EU rules, open-source general-purpose AI models are not required to report their energy consumption to authorities unless they pose a systemic risk.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4].","The context states that open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This implies that under current EU rules, open-source general-purpose AI models are not required to report their energy consumption to authorities unless they pose a systemic risk."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippet from [ref_id=luccioni2025a] states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This directly supports the answer and matches the expected 'percent' unit.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","The context snippet from [ref_id=luccioni2025a] states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This directly supports the answer and matches the expected 'percent' unit."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The total public health cost of U.S. data centers in 2023 is directly stated in the context as $6.7 billion. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD.","6.7","USD","[""han2024""]","is_blank","Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 and projection in 2028. In 2023, the total public health cost is $6.67 billion.","The total public health cost of U.S. data centers in 2023 is directly stated in the context as $6.7 billion. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh, and that of o3 is 12.222 Wh. To find the factor by which o3's energy consumption is greater, we divide o3's consumption by GPT-4.1 nano's consumption: 12.222 / 0.827 = 14.78. The answer_unit is 'multiplier', so we provide the numeric value.","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). GPT-4.1 nano: 0.827 ± 0.094 Wh, o3: 12.222 ± 1.082 Wh for Energy Consumption(10k input-1.5k output)(Wh)","The energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh, and that of o3 is 12.222 Wh. To find the factor by which o3's energy consumption is greater, we divide o3's consumption by GPT-4.1 nano's consumption: 12.222 / 0.827 = 14.78. The answer_unit is 'multiplier', so we provide the numeric value."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context provides a detailed evaluation of Mélange's performance across different datasets and SLOs. For the short-context Arena dataset with a 120ms SLO, Mélange achieved a cost reduction in the range of 15-77% compared to single-GPU-type baselines. The answer_unit is 'percent', so the answer_value is expressed as a numeric range.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context provides a detailed evaluation of Mélange's performance across different datasets and SLOs. For the short-context Arena dataset with a 120ms SLO, Mélange achieved a cost reduction in the range of 15-77% compared to single-GPU-type baselines. The answer_unit is 'percent', so the answer_value is expressed as a numeric range."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and other well-known LLMs. According to this table, the net tCO2e for FLM-101B is 26, which is the total estimated net carbon emissions for its pre-training.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b) FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26","The context provides a table (Table 3) that summarizes the carbon footprint statistics of FLM-101B and other well-known LLMs. According to this table, the net tCO2e for FLM-101B is 26, which is the total estimated net carbon emissions for its pre-training."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","To find the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU, we need to first determine the throughput. According to Table IV, the throughput for A40-48GB is 1.01 queries/sec. The execution time is the inverse of throughput, so it is 1/1.01 seconds. The answer_unit is 'seconds', so we express the answer_value as a number in seconds.","0.99","seconds","[""xia2024""]","is_blank","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","To find the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU, we need to first determine the throughput. According to Table IV, the throughput for A40-48GB is 1.01 queries/sec. The execution time is the inverse of throughput, so it is 1/1.01 seconds. The answer_unit is 'seconds', so we express the answer_value as a number in seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and that 'disadvantaged communities bear a disproportionate share' of the health costs. This directly contradicts the statement that public health costs are evenly distributed.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value.","The context clearly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and that 'disadvantaged communities bear a disproportionate share' of the health costs. This directly contradicts the statement that public health costs are evenly distributed."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet from [ref_id=wu2021b] states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].' This directly supports the statement in the question, indicating that as of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context snippet from [ref_id=wu2021b] states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].' This directly supports the statement in the question, indicating that as of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama, an open-source platform, was used to apply 4-bit quantization and support local deployment of large language models. This matches the requirement specified in the question.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context mentions that Ollama, an open-source platform, was used to apply 4-bit quantization and support local deployment of large language models. This matches the requirement specified in the question."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the authors used an independent S3 storage provider, Backblaze (B2), to store datasets. This is because the spot VMs can be terminated at any time, and proprietary cloud storage cannot be relied upon. The datasets are accessed on-demand via shards in thetar format with the WebDataset library.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","When we run our experiments in a multi-cloud environment on spot instances, we cannot plug in proprietary cloud storage or wait for the dataset to download, as the instances can be terminated any-time. To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].","The context mentions that the authors used an independent S3 storage provider, Backblaze (B2), to store datasets. This is because the spot VMs can be terminated at any time, and proprietary cloud storage cannot be relied upon. The datasets are accessed on-demand via shards in thetar format with the WebDataset library."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context snippets indicate that a strong correlation between inference energy consumption and model runtime was found, suggesting that execution time can serve as a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption. The answer_unit 'is_blank' requires a simple '1' for True or '0' for False.","1","is_blank","[""zschache2025""]","is_blank","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context snippets indicate that a strong correlation between inference energy consumption and model runtime was found, suggesting that execution time can serve as a practical proxy for energy usage. This implies a nearly linear relationship between runtime and energy consumption. The answer_unit 'is_blank' requires a simple '1' for True or '0' for False."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context snippet from [ref_id=morrison2025] mentions that the Artificial Intelligence Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey on 1 Feb 2024, as referenced in the provided URL.","Edward J. Markey","is_blank","[""ebert2024"", ""morrison2025""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","The bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context snippet from [ref_id=morrison2025] mentions that the Artificial Intelligence Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey on 1 Feb 2024, as referenced in the provided URL."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed in Table 1. The table provides specifications and prices for various GPUs, including the H100. The price is given as $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price
is estimated using the relative complete system cost against H100.

Table 1: H100, H20, and TPU v6e specifications.","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed in Table 1. The table provides specifications and prices for various GPUs, including the H100. The price is given as $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context snippets provide information about the lifespan of GPUs in AI data centers. According to [ref_id=morrison2025], internally, a 4-year lifespan is assumed for GPUs. This directly supports the answer to the question about the estimated average GPU lifetime before retirement in AI data centers.","4","years","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context snippets provide information about the lifespan of GPUs in AI data centers. According to [ref_id=morrison2025], internally, a 4-year lifespan is assumed for GPUs. This directly supports the answer to the question about the estimated average GPU lifetime before retirement in AI data centers."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA models. For LLaMA-13B, it specifies that 2 V100 32GB GPUs are required.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","The context provides a table (Table II) that lists the bare minimum hardware requirements for different LLaMA models. For LLaMA-13B, it specifies that 2 V100 32GB GPUs are required."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides the reported GHG emissions from the pre-training process for Google's Gemma family of language models as 1247.61 tons CO2e. This value is compared to the 'five cars' estimate, which is 284 metric tons CO2e. The answer is derived directly from the given context, and the unit is tons CO2e (tCO2e) as specified in the additional info.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number...","The context provides the reported GHG emissions from the pre-training process for Google's Gemma family of language models as 1247.61 tons CO2e. This value is compared to the 'five cars' estimate, which is 284 metric tons CO2e. The answer is derived directly from the given context, and the unit is tons CO2e (tCO2e) as specified in the additional info."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context snippet from [ref_id=strubell2019] reports that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the GPU model is slightly different (GTX 1080 vs GTX 1080 Ti), the training time is directly provided.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context snippet from [ref_id=strubell2019] reports that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the GPU model is slightly different (GTX 1080 vs GTX 1080 Ti), the training time is directly provided."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh, which is directly used as the answer_value. The answer_unit is L/kWh, matching the required format.","3.1","L/kWh","[""li2025b""]","is_blank","The U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh, which is directly used as the answer_value. The answer_unit is L/kWh, matching the required format."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides information about the PUE of Google's hyperscale data centers. According to Figure 1 in the context, Google's hyperscale data centers had a PUE of 1.10 in 2021. The 'answer_unit' is 'PUE', which is a ratio and does not require a unit conversion.","1.1","PUE","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","The context provides information about the PUE of Google's hyperscale data centers. According to Figure 1 in the context, Google's hyperscale data centers had a PUE of 1.10 in 2021. The 'answer_unit' is 'PUE', which is a ratio and does not require a unit conversion."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that 'Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.' This directly supports the answer to the question about the percent reduction in carbon footprint customers can expect by moving workloads from on-premises data centers to AWS in North America.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that 'Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.' This directly supports the answer to the question about the percent reduction in carbon footprint customers can expect by moving workloads from on-premises data centers to AWS in North America."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a specific cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is given as $3460, which directly matches the expected unit 'USD'.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context provides a specific cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is given as $3460, which directly matches the expected unit 'USD'."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context snippet from [ref_id=chung2025] states that GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter. This directly supports the answer to the question about the percentage of total provisioned power that GPUs account for.","[50,70]","percent","[""chung2025""]","is_blank","• GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The context snippet from [ref_id=chung2025] states that GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter. This directly supports the answer to the question about the percentage of total provisioned power that GPUs account for."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context snippet from [ref_id=shen2024] states that JetMoE-8B was trained using 30,000 H100 GPU hours. However, it does not directly provide the number of H100 GPUs used. Assuming the training duration was the given 30,000 hours, we can infer that the number of H100 GPUs used was not directly stated but can be related to the given hours. Another snippet from [ref_id=morrison2025] mentions that each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server. If we assume JetMoE-8B's training infrastructure is similar, we can estimate the total number of H100 GPUs.","96","H100 GPUs","[""shen2024"", ""morrison2025""]","is_blank","JetMoE-8B is trained with ... 30,000 H100 GPU hours. [ref_id=shen2024]; Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server [ref_id=morrison2025]","The context snippet from [ref_id=shen2024] states that JetMoE-8B was trained using 30,000 H100 GPU hours. However, it does not directly provide the number of H100 GPUs used. Assuming the training duration was the given 30,000 hours, we can infer that the number of H100 GPUs used was not directly stated but can be related to the given hours. Another snippet from [ref_id=morrison2025] mentions that each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server. If we assume JetMoE-8B's training infrastructure is similar, we can estimate the total number of H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides information about the average GPU power for a single node during OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W. The answer_unit is 'Watts', so the answer_value should be a number in Watts.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context provides information about the average GPU power for a single node during OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W. The answer_unit is 'Watts', so the answer_value should be a number in Watts."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a comparison of CO2e emissions for various activities, including human life. According to the context, 'Human life, avg, 1 year' is estimated to emit 11,023 pounds of CO2e. The question asks for the estimated CO2e emissions for an average human life in one year globally, and the answer_unit is 'lbs', which matches the unit provided in the context.","11023","lbs","[""strubell2019""]","is_blank","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. 1 ... Human life, avg, 1 year 11,023","The context provides a comparison of CO2e emissions for various activities, including human life. According to the context, 'Human life, avg, 1 year' is estimated to emit 11,023 pounds of CO2e. The question asks for the estimated CO2e emissions for an average human life in one year globally, and the answer_unit is 'lbs', which matches the unit provided in the context."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 when compared to the V100 across various metrics. Since latency is inversely related to throughput, a 1.25 times increase in latency corresponds to a 1/1.25 = 0.8 times the original throughput, or approximately 80% of the V100 throughput. Thus, the speedup in inference throughput is the inverse of the latency increase, which is 1.25.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 when compared to the V100 across various metrics. Since latency is inversely related to throughput, a 1.25 times increase in latency corresponds to a 1/1.25 = 0.8 times the original throughput, or approximately 80% of the V100 throughput. Thus, the speedup in inference throughput is the inverse of the latency increase, which is 1.25."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides the energy consumption of a single short GPT-4o query as 0.42 Wh (±0.13 Wh). The answer_unit is 'Wh', so the answer_value is directly extracted as a number in that unit.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context provides the energy consumption of a single short GPT-4o query as 0.42 Wh (±0.13 Wh). The answer_unit is 'Wh', so the answer_value is directly extracted as a number in that unit."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, with a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The percentage increase is calculated as (($2.699 - $0.71) / $0.71) * 100 = 280%.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, with a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The percentage increase is calculated as (($2.699 - $0.71) / $0.71) * 100 = 280%."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context suggests that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly supports the answer to the question about the most energy-efficient GPU architecture for such models.","V100","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context suggests that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly supports the answer to the question about the most energy-efficient GPU architecture for such models."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context from [ref_id=schwartz2019] mentions that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice.' This indicates that Green AI involves providing the cost, referred to as 'computational price tag,' associated with models. The term 'computational price tag' can be interpreted as the financial or resource cost.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context from [ref_id=schwartz2019] mentions that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice.' This indicates that Green AI involves providing the cost, referred to as 'computational price tag,' associated with models. The term 'computational price tag' can be interpreted as the financial or resource cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context from the 2025 paper states that 'AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.' This directly provides the total amount of electronic waste generated worldwide in 2022.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context from the 2025 paper states that 'AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.' This directly provides the total amount of electronic waste generated worldwide in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per accelerator for TPU v2 is 208 Watts, and for V100 GPU is 289 Watts. The difference is calculated as 289 - 208 = 81 Watts.","81","Watts","[""patterson2021""]","is_blank","Table 4 in [patterson2021] provides the measured system average power per accelerator, including memory, network interface, fans, and host CPU, which is 208 Watts for TPU v2 and 289 Watts for V100 GPU.","The average system power per accelerator for TPU v2 is 208 Watts, and for V100 GPU is 289 Watts. The difference is calculated as 289 - 208 = 81 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context snippet from [li2025b] states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer_unit is 'liters', so the answer_value should be a number in liters.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context snippet from [li2025b] states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer_unit is 'liters', so the answer_value should be a number in liters."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The 'answer_unit' is 'multiplier', so the answer should be a number representing the multiplier.","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The 'answer_unit' is 'multiplier', so the answer should be a number representing the multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides a DEA analysis result where o3-mini achieved the highest cross-efficiency score. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825)","The context provides a DEA analysis result where o3-mini achieved the highest cross-efficiency score. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context snippet from [ref_id=wu2021b] states that 'current averages of less than 3 years for cell phones', which directly supports the statement that smartphones currently average lifetimes of less than 3 years.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context snippet from [ref_id=wu2021b] states that 'current averages of less than 3 years for cell phones', which directly supports the statement that smartphones currently average lifetimes of less than 3 years."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV's per-GPU speedup is 'almost linear' with values (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively. This supports the statement that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","1","is_blank","[""erben2023""]","is_blank","CV's per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that CV's per-GPU speedup is 'almost linear' with values (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively. This supports the statement that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context window size for the FLM-101B model is specified in the document [li2025a] as 2,048 tokens. This information is directly extracted from the text describing the model's configurations.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context window size for the FLM-101B model is specified in the document [li2025a] as 2,048 tokens. This information is directly extracted from the text describing the model's configurations."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLMs consume significantly less power than the GPU's TDP because LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU when batch size is not small. This supports the statement that LLMs have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive is incorrect, it is actually characterized by low compute-intensity.","1","is_blank","[""chung2025""]","is_blank","Figure 5 and Figure 10 show the power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models on A100 and H100 GPUs. It can be seen that the LLM's power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model's power consumption is close to the maximum.","The context states that LLMs consume significantly less power than the GPU's TDP because LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources. In contrast, diffusion models are significantly more compute-intensive and consume nearly the maximum power of the GPU when batch size is not small. This supports the statement that LLMs have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive is incorrect, it is actually characterized by low compute-intensity."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides a table showing Amazon's representation by the numbers for People Managers globally in 2023, where 52.3% are women.","52.3","percent","[""amazon2023""]","is_blank","Other Gender Men Women
Gender—Global Gender—U.S. U.S. Race/Ethnicity
Amazon Representation by the Numbers* † Other Native American and NHOPI+ (2021 and 2022)
Latino/e+ Black+ Asian+ White+
Multiracial Native American and Alaskan+ (2023)
NHOPI+ (2023)
Amazon Workforce (All Levels)
43.1%56.8%
44.1%55.7%
30.8%69.2%
31.4%68.5%
31.5%68.4%
46.9%52.9%
45.5%54.3%
46.6%53.2%
44.3%55.6%
31.1%68.8%
48
31.7%68.3%
31.6%68.3%
Learn more about our Board diversity data in our Sustainability Reporting Framework Summary 
20.0% 53.3%12.1%11.1%
2021
2022
2023
2021
2022
2023
People Managers
Board Diversity
Gender Race/Ethnicity
2021
2022
2023
2021
2022
2023
0.3% 1.7% 2.6%
2021
2022
2023
2021
2022
2023
16.6% 31.9%26.1%23.5%
0.6% 1.8% 1.8%
0.9% 1.1%  1.5%  0.8%
0.1 % 1.0% 3.0%
0.3% 1.0% 2.6%
0.4% 2.3%  0.7%  0.4%
0.1 %
0.1 %
0.2 %0.2 %
0.1 % 0.2 %
0.2 %0.1 %
0.1 %
0.0 %
0.1 %
2 91
0.1 %
14.4% 29.1%31.3%25.8%
23.0% 50.6%12.3% 12.3%
13.5% 30.7%28.6%23.6%
22.8% 51.3%11.4% 11.9%","The context provides a table showing Amazon's representation by the numbers for People Managers globally in 2023, where 52.3% are women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides the percentage range of total amortized cost attributed to computing hardware for four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) when excluding equity. According to the text, when equity is excluded, computing hardware costs rise to 61–76% of the total amortized cost. This matches the expected format of a percentage range.","[61, 76]","percent","[""cottier2024""]","is_blank","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context provides the percentage range of total amortized cost attributed to computing hardware for four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) when excluding equity. According to the text, when equity is excluded, computing hardware costs rise to 61–76% of the total amortized cost. This matches the expected format of a percentage range."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context snippets indicate that while some performance metrics like precision and recall generally exhibit minor increases after optimization, metrics like accuracy and F1 score are slightly lower after optimization. This suggests that accuracy and F1 scores did not always improve after optimization.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context snippets indicate that while some performance metrics like precision and recall generally exhibit minor increases after optimization, metrics like accuracy and F1 score are slightly lower after optimization. This suggests that accuracy and F1 scores did not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as given by the U.S. Environmental Protection Agency (EPA). The formula CO2e = 0.954pt is used to convert power to estimated CO2 emissions, where the factor 0.954 is derived from the EPA's data on average CO2 produced per kilowatt-hour. Thus, the average CO2 produced is 0.954 lbs/kWh.","0.954","lbs/kWh","[""strubell2019""]","is_blank","CO2e = 0.954pt (2)","The context provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S. as given by the U.S. Environmental Protection Agency (EPA). The formula CO2e = 0.954pt is used to convert power to estimated CO2 emissions, where the factor 0.954 is derived from the EPA's data on average CO2 produced per kilowatt-hour. Thus, the average CO2 produced is 0.954 lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a study's projection for the global AI's electricity consumption in 2027. The answer_unit is 'TWh', so the answer_value should be a numeric range in TWh. The context clearly supports the answer with a specific range.","[85,134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7], whereas a more aggressive estimate by the U.S. data center energy report projects that AI servers’ electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1].","The context provides a study's projection for the global AI's electricity consumption in 2027. The answer_unit is 'TWh', so the answer_value should be a numeric range in TWh. The context clearly supports the answer with a specific range."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context provides information about Amazon's delivery statistics via electric vehicles (EVs) in various regions, including Europe. According to the context, in 2023, Amazon delivered 150 million packages via EVs in Europe. The answer_unit is 'packages', which matches the context.","150","packages","[""amazon2023""]","is_blank","Europe
• We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles.
• We delivered 150 million packages via EVs.","The context provides information about Amazon's delivery statistics via electric vehicles (EVs) in various regions, including Europe. According to the context, in 2023, Amazon delivered 150 million packages via EVs in Europe. The answer_unit is 'packages', which matches the context."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation processed 3000 requests, with each request having 128 input tokens and 512 output tokens. Thus, the total number of tokens processed is 3000 * (128 + 512). The answer_unit is 'tokens', so we calculate the total number of tokens.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload evaluation processed 3000 requests, with each request having 128 input tokens and 512 output tokens. Thus, the total number of tokens processed is 3000 * (128 + 512). The answer_unit is 'tokens', so we calculate the total number of tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not directly state that the AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. However, it mentions that the AI Act contains dedicated sustainability rules and transparency requirements for providers of high-risk and general-purpose AI models. The closest relevant information is that the AI Act requires reporting of energy consumption during the development phase of general-purpose AI models.","0","is_blank","[""ebert2024"", ""luccioni2025c""]","is_blank","The AI Act requires providers of general-purpose AI models to maintain up-to-date technical documentation, including the reporting of energy consumption during the model's development phase [ref_id=ebert2024].","The context does not directly state that the AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. However, it mentions that the AI Act contains dedicated sustainability rules and transparency requirements for providers of high-risk and general-purpose AI models. The closest relevant information is that the AI Act requires reporting of energy consumption during the development phase of general-purpose AI models."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context provides a figure (Fig. 3) showing the CO2 emissions decrease for DenseNet 201 in various regions, including West US. According to the figure, the maximum potential percentage reduction in CO2 emissions for DenseNet 201 in West US is approximately 80%. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","80","percent","[""dodge2022""]","is_blank","Fig. 3. What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context provides a figure (Fig. 3) showing the CO2 emissions decrease for DenseNet 201 in various regions, including West US. According to the figure, the maximum potential percentage reduction in CO2 emissions for DenseNet 201 in West US is approximately 80%. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The study from 2024 provides a comparison of energy consumption for different tasks. It states that the mean energy consumption for text classification is 0.002 kWh per 1,000 inferences, while for image generation it is 2.907 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is calculated as 2.907 / 0.002 = 1453.5, which is described as 'over 1450 times' in the context.","1450","times","[""luccioni2024""]","is_blank","Table 2 from Luccioni et al. (2024) shows the mean energy consumption for text classification as 0.002 kWh and for image generation as 2.907 kWh per 1,000 inferences. The text states that the different models examined can vary by a factor of over 1450 in terms of energy required.","The study from 2024 provides a comparison of energy consumption for different tasks. It states that the mean energy consumption for text classification is 0.002 kWh per 1,000 inferences, while for image generation it is 2.907 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is calculated as 2.907 / 0.002 = 1453.5, which is described as 'over 1450 times' in the context."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This directly supports the answer that traditional models did not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This directly supports the answer that traditional models did not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table listing Amazon Renewable Energy Projects by country as of January 2024. The table includes the number of projects and total MW capacity for each country. For the United States, it lists 244 projects.","244","projects","[""amazon2023""]","is_blank","Project Location | Number of Projects | Total MW Capacity; United States | 244 | 17,706","The context provides a table listing Amazon Renewable Energy Projects by country as of January 2024. The table includes the number of projects and total MW capacity for each country. For the United States, it lists 244 projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides a table from a research paper by Strubell et al. (2019) that compares the percent of energy sourced from different sources for major cloud compute providers, including Amazon-AWS. According to the table, Amazon-AWS covered 17% of its power usage with renewable energy in an unspecified year. Although the exact year is not mentioned in the table, it is the only relevant information available in the context.","17","percent","[""strubell2019""]","is_blank","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, China and Germany (Burger, 2019). Amazon-AWS: 17% renewable energy.","The context provides a table from a research paper by Strubell et al. (2019) that compares the percent of energy sourced from different sources for major cloud compute providers, including Amazon-AWS. According to the table, Amazon-AWS covered 17% of its power usage with renewable energy in an unspecified year. Although the exact year is not mentioned in the table, it is the only relevant information available in the context."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend measuring AI energy consumption at the 'cumulative server level' to balance accuracy and feasibility. This is directly supported by the context provided in the document 'ebert2024'. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name from the context.","cumulative server level","is_blank","[""ebert2024""]","is_blank","Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.","The authors recommend measuring AI energy consumption at the 'cumulative server level' to balance accuracy and feasibility. This is directly supported by the context provided in the document 'ebert2024'. The 'answer_unit' is 'is_blank', indicating that the answer should be an identifier or name from the context."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides a table (Table 1) that details the electricity consumption of different components when training a BERT-base model on a single NVIDIA TITAN X GPU. The GPU accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context provides a table (Table 1) that details the electricity consumption of different components when training a BERT-base model on a single NVIDIA TITAN X GPU. The GPU accounts for 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","","","NVIDIA A100-80GB GPUs","[]","is_blank","is_blank",""
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context provides a figure (Figure 8) that illustrates the operational energy footprint reduction over a two-year period. The figure shows a 28.5% improvement in operational power footprint reduction.","28.5","percent","[""wu2021a""]","is_blank","Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context provides a figure (Figure 8) that illustrates the operational energy footprint reduction over a two-year period. The figure shows a 28.5% improvement in operational power footprint reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide information on the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. Therefore, the answer cannot be determined based on the provided context.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. Therefore, the answer cannot be determined based on the provided context."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides an estimate of the hardware acquisition cost to train GPT-4. According to the document 'cottier2024', the estimated upfront hardware acquisition cost to train GPT-4 is $800M. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context provides an estimate of the hardware acquisition cost to train GPT-4. According to the document 'cottier2024', the estimated upfront hardware acquisition cost to train GPT-4 is $800M. The answer_unit is 'USD', so the answer_value should be expressed as a number in USD."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context defines 'Sustainable AI' as a field that aims to use AI in climate-positive applications and improve the sustainability of AI approaches themselves, not just the former.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The context defines 'Sustainable AI' as a field that aims to use AI in climate-positive applications and improve the sustainability of AI approaches themselves, not just the former."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a figure (Fig. 13) that shows the projected maximum batch size of Mixtral for different GPUs, including the A100-40GB. The ground truth maximum batch size for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU is directly obtained from this figure.","10","samples","[""xia2024""]","is_blank","Fig. 13. Projected maximum batch size of Mixtral for different GPUs.","The context provides a figure (Fig. 13) that shows the projected maximum batch size of Mixtral for different GPUs, including the A100-40GB. The ground truth maximum batch size for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU is directly obtained from this figure."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context clearly states that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation when restoring requests, whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared. It can be seen that when the server is overloaded, Swapping consistently consumes less energy.","The context clearly states that when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation when restoring requests, whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context provides information from Google's 2024 environmental report, which states that Google reported a 48% increase in GHG emissions since 2019. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]","The context provides information from Google's 2024 environmental report, which states that Google reported a 48% increase in GHG emissions since 2019. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context does not provide McKinsey projections for data center electricity consumption in 2030. However, it mentions projections from other sources. The closest relevant information is from [ref_id=fernandez2025] and [ref_id=jegham2025], which state that data centers are estimated to consume between 9.1% and 11.7% of the total US energy demand by 2030.","[9.1,11.7]","percent","[""fernandez2025"", ""jegham2025""]","is_blank","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).","The context does not provide McKinsey projections for data center electricity consumption in 2030. However, it mentions projections from other sources. The closest relevant information is from [ref_id=fernandez2025] and [ref_id=jegham2025], which state that data centers are estimated to consume between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides a figure (Fig. 4) showing the CO2 emissions decrease for the 'Pause and Resume' optimization for the 6B parameter transformer. The figure indicates that the maximum potential emissions saving is up to about 25%. The answer_unit is 'percent', so the answer_value should be a number representing the percentage.","25","percent","[""dodge2022""]","is_blank","Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? ... for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context provides a figure (Fig. 4) showing the CO2 emissions decrease for the 'Pause and Resume' optimization for the 6B parameter transformer. The figure indicates that the maximum potential emissions saving is up to about 25%. The answer_unit is 'percent', so the answer_value should be a number representing the percentage."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. The 'answer_unit' is 'percent', so the answer_value should be a numeric value representing the percentage.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. The 'answer_unit' is 'percent', so the answer_value should be a numeric value representing the percentage."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from [ref_id=luccioni2025b] mentions a seminal 2019 article by Strubell et al. that quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions. The answer_unit is 'lbs CO2e', which matches the unit provided in the context.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context snippet from [ref_id=luccioni2025b] mentions a seminal 2019 article by Strubell et al. that quantified the carbon footprint of training BERT, a large language model, as reaching 626,155 pounds of CO2 emissions. The answer_unit is 'lbs CO2e', which matches the unit provided in the context."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides the estimated training energy of GPT-3 as 1287 MWh, which directly matches the expected unit of MWh specified in the additional info.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context provides the estimated training energy of GPT-3 as 1287 MWh, which directly matches the expected unit of MWh specified in the additional info."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context clearly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and is a prime target for optimization to enhance performance. This directly supports the statement that the MoE layer is often targeted when enhancing performance of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context clearly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and is a prime target for optimization to enhance performance. This directly supports the statement that the MoE layer is often targeted when enhancing performance of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For Mixtral with a dense setup on the Hellaswag (CS) dataset, the maximum batch size is given as 2.","2","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE. Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For Mixtral with a dense setup on the Hellaswag (CS) dataset, the maximum batch size is given as 2."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This directly supports the answer.","1","is_blank","[""erben2023""]","is_blank","For high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This directly supports the answer."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide information on the land area occupied by new AI data centers globally in 2022. Therefore, the answer cannot be determined based on the provided context snippets.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide information on the land area occupied by new AI data centers globally in 2022. Therefore, the answer cannot be determined based on the provided context snippets."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The 2024 study compares task-specific and general-purpose models and provides the CO2eq emissions for the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries. The answer is directly obtained from the context, and the unit is specified as 'g CO2eq'.","0.32","g CO2eq","[""luccioni2024""]","is_blank","For instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The 2024 study compares task-specific and general-purpose models and provides the CO2eq emissions for the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries. The answer is directly obtained from the context, and the unit is specified as 'g CO2eq'."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context from [ref_id=dodge2022] provides a figure (Fig. 1) showing the carbon emissions from training BERT on 8 V100 GPUs for 36 hours across 16 different regions. The emissions range from approximately 7k grams to 26k grams between the most and least efficient regions.","[7,26]","grams","[""dodge2022""]","is_blank","Fig. 1 from [ref_id=dodge2022]: Carbon emissions from training BERT on 8 V100 GPUs for 36 hours across 16 regions, ranging from 7k grams to 26k grams.","The context from [ref_id=dodge2022] provides a figure (Fig. 1) showing the carbon emissions from training BERT on 8 V100 GPUs for 36 hours across 16 different regions. The emissions range from approximately 7k grams to 26k grams between the most and least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context clearly states that the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. The 'answer_unit' is 'year', so the answer should be expressed as a year.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context clearly states that the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. The 'answer_unit' is 'year', so the answer should be expressed as a year."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table (Table 2 in [morrison2025]) that lists the estimated water consumption for training various OLMo models. For OLMo 60M, the water consumption is equivalent to 5 days of water usage for one person in the US.","5","days","[""morrison2025""]","is_blank","Table 2: ... OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table (Table 2 in [morrison2025]) that lists the estimated water consumption for training various OLMo models. For OLMo 60M, the water consumption is equivalent to 5 days of water usage for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","","","liters of freshwater","[]","is_blank","is_blank",""
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context suggests that adding compute resources to accelerate the MoE layers is a way to further reduce the cost of fine-tuning LLMs, implying that it can potentially decrease costs. However, the question asks if it can increase costs. The context does not directly support the idea that adding compute resources increases costs; instead, it implies the opposite. Therefore, the answer should reflect the inverse of the statement based on the context provided.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context suggests that adding compute resources to accelerate the MoE layers is a way to further reduce the cost of fine-tuning LLMs, implying that it can potentially decrease costs. However, the question asks if it can increase costs. The context does not directly support the idea that adding compute resources increases costs; instead, it implies the opposite. Therefore, the answer should reflect the inverse of the statement based on the context provided."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is directly stated in the context as 40 MkWh. To convert this to kWh, we need to multiply by 1,000,000 since 1 MkWh = 1,000,000 kWh. Thus, 40 MkWh equals 40,000,000 kWh.","40000000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO2e, and their formulas are the same as (Patterson et al. 2021). Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b) FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26","The total energy consumption for training the FLM-101B model is directly stated in the context as 40 MkWh. To convert this to kWh, we need to multiply by 1,000,000 since 1 MkWh = 1,000,000 kWh. Thus, 40 MkWh equals 40,000,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides projections for the energy consumption of U.S. data centers by 2030, estimating they could consume between 9.1% and 11.7% of the total U.S. energy demand. However, it does not directly provide the total public health burden valuation for 2030. The closest relevant information is found in [ref_id=han2024], which discusses the projected public health costs for 2028, not 2030.","20,000,000,000","USD","[""han2024"", ""fernandez2025""]","is_blank","According to [ref_id=han2024], 'the total public health costs could reach more than $20 billion' by 2028. [ref_id=fernandez2025] mentions projections for 2030 energy consumption but does not provide a direct valuation of public health burden.","The context provides projections for the energy consumption of U.S. data centers by 2030, estimating they could consume between 9.1% and 11.7% of the total U.S. energy demand. However, it does not directly provide the total public health burden valuation for 2030. The closest relevant information is found in [ref_id=han2024], which discusses the projected public health costs for 2028, not 2030."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. Although the question asks about the GSM8K dataset, the cost for fine-tuning on MATH dataset is given, and GSM8K is mentioned as one of the datasets used. The cost for fine-tuning Mixtral with an NVIDIA A40-48GB GPU is listed as $32.7.","32.7","USD","[""xia2024""]","is_blank","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. Although the question asks about the GSM8K dataset, the cost for fine-tuning on MATH dataset is given, and GSM8K is mentioned as one of the datasets used. The cost for fine-tuning Mixtral with an NVIDIA A40-48GB GPU is listed as $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides a table (TABLE IV) that lists the estimated cost of fine-tuning Mixtral on GS with sparse MoE. For a NVIDIA A40-48 GB, the maximum batch size (MBS) is given as 4. This information directly supports the answer to the question about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB.","4","samples","[""xia2024""]","is_blank","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context provides a table (TABLE IV) that lists the estimated cost of fine-tuning Mixtral on GS with sparse MoE. For a NVIDIA A40-48 GB, the maximum batch size (MBS) is given as 4. This information directly supports the answer to the question about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides a figure (Fig. 4) showing the execution time breakdown for Mixtral and Mamba models. For a sparse Mixtral model with a batch size of 10, the execution time breakdown is given. The total execution time is approximately 4.0 seconds (forward) + 6.0 seconds (backward) + 0.5 seconds (optimizer) = 10.5 seconds.","10.5","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown. Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32) 0.0 2.0 4.0 6.0 8.0 Forward Backward Optimizer","The context provides a figure (Fig. 4) showing the execution time breakdown for Mixtral and Mamba models. For a sparse Mixtral model with a batch size of 10, the execution time breakdown is given. The total execution time is approximately 4.0 seconds (forward) + 6.0 seconds (backward) + 0.5 seconds (optimizer) = 10.5 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context snippets suggest that GPU-level power consumption monitoring is a practical and recommended approach for measuring AI energy use because GPUs are the dominant energy consumers in AI workloads and are typically not shared across instances, making it relatively easy to measure their electricity consumption.","1","is_blank","[""dodge2022"", ""chung2025""]","is_blank","Table 1 from [dodge2022] shows [GPU accounts for 74% of total energy consumption], and [chung2025] states 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter.'","The context snippets suggest that GPU-level power consumption monitoring is a practical and recommended approach for measuring AI energy use because GPUs are the dominant energy consumers in AI workloads and are typically not shared across instances, making it relatively easy to measure their electricity consumption."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context mentions that Luccioni et al. proposed using a Life Cycle Assessment approach for evaluating the environmental impacts of AI systems, finding that training accounted for only half of the model's overall emissions. This information is directly related to the question about the percentage of the BLOOM model's overall emissions attributed to training.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach... finding that training accounted for only half of the model's overall emissions.","The context mentions that Luccioni et al. proposed using a Life Cycle Assessment approach for evaluating the environmental impacts of AI systems, finding that training accounted for only half of the model's overall emissions. This information is directly related to the question about the percentage of the BLOOM model's overall emissions attributed to training."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, it specifies that 1 V100 32GB GPU is required.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size | V100 32GB | A100 80GB
Count | Max. Batch size | Count | Max. Batch size
7B | 1 | 64 | 1 | 64","The context provides a table (Table II) that lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, it specifies that 1 V100 32GB GPU is required."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for various data center locations, including Arizona. The table lists the number of requests for 500ml of water for each location.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3's operational water consumption footprint. For Arizona, the '# of Requests for 500ml Water' is 16.7.","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for various data center locations, including Arizona. The table lists the number of requests for 500ml of water for each location."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides a direct estimate of the CO2 emissions for one average US home energy use for a year. According to the snippet from [ref_id=dodge2022], one average US home energy use is estimated to emit 8.30 metric tons of CO2. This value is directly used as the answer_value since the answer_unit is 'metric tons'.","8.30","metric tons","[""dodge2022""]","is_blank","One average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil).","The context provides a direct estimate of the CO2 emissions for one average US home energy use for a year. According to the snippet from [ref_id=dodge2022], one average US home energy use is estimated to emit 8.30 metric tons of CO2. This value is directly used as the answer_value since the answer_unit is 'metric tons'."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides a table (Table 3) that compares the performance of different models on various benchmarks, including GSM8k. JetMoE-8B's score on GSM8k is directly mentioned in this table.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8","The context provides a table (Table 3) that compares the performance of different models on various benchmarks, including GSM8k. JetMoE-8B's score on GSM8k is directly mentioned in this table."
